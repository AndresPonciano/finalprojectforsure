{"title": "Physis: an implicitly parallel programming model for stencil computations on large-scale GPU-accelerated supercomputers\n", "abstract": " This paper proposes a compiler-based programming framework that automatically translates user-written structured grid code into scalable parallel implementation code for GPU-equipped clusters. To enable such automatic translations, we design a small set of declarative constructs that allow the user to express stencil computations in a portable and implicitly parallel manner. Our framework translates the user-written code into actual implementation code in CUDA for GPU acceleration and MPI for node-level parallelization with automatic optimizations such as computation and communication overlapping. We demonstrate the feasibility of such automatic translations by implementing several structured grid applications in our framework. Experimental results on the TSUBAME2. 0 GPU-based supercomputer show that the performance is comparable as hand-written code and good strong and weak scalability up to\u00a0\u2026", "num_citations": "237\n", "authors": ["1836"]}
{"title": "Design and modeling of a non-blocking checkpointing system\n", "abstract": " As the capability and component count of systems increase, the MTBF decreases. Typically, applications tolerate failures with checkpoint/restart to a parallel file system (PFS). While simple, this approach can suffer from contention for PFS resources. Multi-level checkpointing is a promising solution. However, while multi-level checkpointing is successful on today's machines, it is not expected to be sufficient for exascale class machines, which are predicted to have orders of magnitude larger memory sizes and failure rates. Our solution combines the benefits of non-blocking and multi-level checkpointing. In this paper, we present the design of our system and model its performance. Our experiments show that our system can improve efficiency by 1.1 to 2.0x on future machines. Additionally, applications using our checkpointing system can achieve high efficiency even when using a PFS with lower bandwidth.", "num_citations": "134\n", "authors": ["1836"]}
{"title": "An ephemeral burst-buffer file system for scientific applications\n", "abstract": " Burst buffers are becoming an indispensable hardware resource on large-scale supercomputers to buffer the bursty I/O from scientific applications. However, there is a lack of software support for burst buffers to be efficiently shared by applications within a batch-submitted job and recycled across different batch jobs. In addition, burst buffers need to cope with a variety of challenging I/O patterns from data-intensive scientific applications. In this study, we have designed an ephemeral Burst Buffer File System (BurstFS) that supports scalable and efficient aggregation of I/O bandwidth from burst buffers while having the same life cycle as a batch-submitted job. BurstFS features several techniques including scalable metadata indexing, co-located I/O delegation, and server-side read clustering and pipelining. Through extensive tuning and analysis, we have validated that BurstFS has accomplished our design objectives\u00a0\u2026", "num_citations": "92\n", "authors": ["1836"]}
{"title": "Exploration of lossy compression for application-level checkpoint/restart\n", "abstract": " The scale of high performance computing (HPC) systems is exponentially growing, potentially causing prohibitive shrinkage of mean time between failures (MTBF) while the overall increase in the I/O performance of parallel file systems will be far behind the increase in scale. As such, there have been various attempts to decrease the checkpoint overhead, one of which is to employ compression techniques to the checkpoint files. While most of the existing techniques focus on lossless compression, their compression rates and thus effectiveness remain rather limited. Instead, we propose a loss compression technique based on wavelet transformation for checkpoints, and explore its impact to application results. Experimental application of our loss compression technique to a production climate application, NICAM, shows that the overall checkpoint time including compression is reduced by 81%, while relative error\u00a0\u2026", "num_citations": "81\n", "authors": ["1836"]}
{"title": "A user-level infiniband-based file system and checkpoint strategy for burst buffers\n", "abstract": " Checkpoint/Restart is an indispensable fault tolerance technique commonly used by high-performance computing applications that run continuously for hours or days at a time. However, even with state-of-the-art checkpoint/restart techniques, high failure rates at large scale will limit application efficiency. To alleviate the problem, we consider using burst buffers. Burst buffers are dedicated storage resources positioned between the compute nodes and the parallel file system, and this new tier within the storage hierarchy fills the performance gap between node-local storage and parallel file systems. With burst buffers, an application can quickly store checkpoints with increased reliability. In this work, we explore how burst buffers can improve efficiency compared to using only node-local storage. To fully exploit the bandwidth of burst buffers, we develop a user-level Infini Band-based file system (IBIO). We also develop\u00a0\u2026", "num_citations": "69\n", "authors": ["1836"]}
{"title": "Observation and measurements of the production of prompt and non-prompt J\u03c8 mesons in association with a Z boson in pp collisions at s= 8TeV with the ATLAS detector\n", "abstract": " The production of a Z boson in association with a J/\u03c8 meson in proton\u2013proton collisions probes the production mechanisms of quarkonium and heavy flavour in association with vector bosons, and allows studies of multiple parton scattering. Using 20. 3 fb\u2212 1 of data collected with the ATLAS experiment at the LHC in pp collisions at", "num_citations": "53\n", "authors": ["1836"]}
{"title": "Scalable multi-gpu 3-d fft for tsubame 2.0 supercomputer\n", "abstract": " For scalable 3-D FFT computation using multiple GPUs, efficient all-to-all communication between GPUs is the most important factor in good performance. Implementations with point-to-point MPI library functions and CUDA memory copy APIs typically exhibit very large overheads especially for small message sizes in all-to-all communications between many nodes. We propose several schemes to minimize the overheads, including employment of lower-level API of InfiniBand to effectively overlap intra- and inter-node communication, as well as auto-tuning strategies to control scheduling and determine rail assignments. As a result we achieve very good strong scalability as well as good performance, up to 4.8TFLOPS using 256 nodes of TSUBAME 2.0 Supercomputer (768 GPUs) in double precision.", "num_citations": "44\n", "authors": ["1836"]}
{"title": "A model-based algorithm for optimizing i/o intensive applications in clouds using vm-based migration\n", "abstract": " Federated storage resources in geographically distributed environments are becoming viable platforms for data-intensive cloud and grid applications. To improve I/O performance in such environments, we propose a novel model-based I/O performance optimization algorithm for data-intensive applications running on a virtual cluster, which determines virtual machine (VM) migration strategies,i.e., when and where a VM should be migrated, while minimizing the expected value of file access time. We solve this problem as a shortest path problem of a weighted direct acyclic graph (DAG), where the weighted vertex represents a location of a VM and expected file access time from the location, and the weighted edge represents a migration of a VM and time. We construct the DAG from our Markov model which represents the dependency of files. Our simulation-based studies suggest that our proposed algorithm can\u00a0\u2026", "num_citations": "42\n", "authors": ["1836"]}
{"title": "Energy-aware I/O optimization for checkpoint and restart on a NAND flash memory system\n", "abstract": " Both energy efficiency and system reliability are significant concerns towards exa-scale high-performance computing. In such large HPC systems, applications are required to conduct massive I/O operations to local storage devices (eg a NAND flash memory) for scalable checkpoint and restart. However, checkpoint/restart can use a large portion of runtime, and consumes enormous energy by non-I/O subsystems, such as CPU and memory. Thus, energy-aware optimization, including I/O operations to storage, is required for checkpoint/restart. In this paper, we present a profile-based I/O optimization technique for NAND flash memory devices based on Markov model for checkpoint/restart. The results based on performance studies show that our profile lookup approach can save 4.1% of energy consumption in an application execution with checkpoint/restart. Especially, our approach improves the energy consumption\u00a0\u2026", "num_citations": "38\n", "authors": ["1836"]}
{"title": "Entropy-aware I/O pipelining for large-scale deep learning on HPC systems\n", "abstract": " Deep neural networks have recently gained tremendous interest due to their capabilities in a wide variety of application areas such as computer vision and speech recognition. Thus it is important to exploit the unprecedented power of leadership High-Performance Computing (HPC) systems for greater potential of deep learning. While much attention has been paid to leverage the latest processors and accelerators, I/O support also needs to keep up with the growth of computing power for deep neural networks. In this research, we introduce an entropy-aware I/O framework called DeepIO for large-scale deep learning on HPC systems. Its overarching goal is to coordinate the use of memory, communication, and I/O resources for efficient training of datasets. DeepIO features an I/O pipeline that utilizes several novel optimizations: RDMA (Remote Direct Memory Access)-assisted in-situ shuffling, input pipelining, and\u00a0\u2026", "num_citations": "35\n", "authors": ["1836"]}
{"title": "Fmi: Fault tolerant messaging interface for fast and transparent recovery\n", "abstract": " Future supercomputers built with more components will enable larger, higher-fidelity simulations, but at the cost of higher failure rates. Traditional approaches to mitigating failures, such as checkpoint/restart (C/R) to a parallel file system incur large overheads. On future, extreme-scale systems, it is unlikely that traditional C/R will recover a failed application before the next failure occurs. To address this problem, we present the Fault Tolerant Messaging Interface (FMI), which enables extremely low-latency recovery. FMI accomplishes this using a survivable communication runtime coupled with fast, in-memory C/R, and dynamic node allocation. FMI provides message-passing semantics similar to MPI, but applications written using FMI can run through failures. The FMI runtime software handles fault tolerance, including check pointing application state, restarting failed processes, and allocating additional nodes when\u00a0\u2026", "num_citations": "35\n", "authors": ["1836"]}
{"title": "Metakv: A key-value store for metadata management of distributed burst buffers\n", "abstract": " Distributed burst buffers are a promising storage architecture for handling I/O workloads for exascale computing. Their aggregate storage bandwidth grows linearly with system node count. However, although scientific applications can achieve scalable write bandwidth by having each process write to its node-local burst buffer, metadata challenges remain formidable, especially for files shared across many processes. This is due to the need to track and organize file segments across the distributed burst buffers in a global index. Because this global index can be accessed concurrently by thousands or more processes in a scientific application, the scalability of metadata management is a severe performance-limiting factor. In this paper, we propose MetaKV: a key-value store that provides fast and scalable metadata management for HPC metadata workloads on distributed burst buffers. MetaKV complements the\u00a0\u2026", "num_citations": "18\n", "authors": ["1836"]}
{"title": "BurstFS: A distributed burst buffer file system for scientific applications\n", "abstract": " Burst buffers are becoming an indispensable hardware resource on large-scale supercomputers to buffer the bursty I/O from scientific applications. However, there is a lack of software support for burst buffers to be efficiently shared by applications within a batch-submitted job and recycled across different batch jobs. In addition, burst buffers need to cope with a variety of challenging I/O patterns from data-intensive scientific applications. In this study, we have designed an ephemeral Burst Buffer File System (BurstFS) that supports scalable and efficient aggregation of I/O bandwidth from burst buffers while having the same life cycle as a batch-submitted job. BurstFS features several techniques including scalable metadata indexing, co-located I/O delegation, and server-side read clustering and pipelining. Through extensive tuning and analysis, we have validated that BurstFS has accomplished our design objectives, with linear scalability in terms of aggregated I/O bandwidth for parallel writes and reads.", "num_citations": "18\n", "authors": ["1836"]}
{"title": "Latent Fault Detection With Unbalanced Workloads\n", "abstract": " Big data means big datacenters, comprised of hundreds or thousands of machines. With so many machines, failures are commonplace. Failure detection is crucial: undetected failures may lead to data loss and outages. Recent health monitoring approaches use anomaly detection to forecast failures\u2013anomalous machines are considered to be at risk of future failures. Our previous work focused on detecting latent faults in large web services, which are often characterized by scale-out architecture where load is dynamically balanced. We proposed a robust and unsupervised latent fault detector for such systems, with statistical bounds on the rate of false positives. That detector, however, is unsuitable for applications without dynamic load balancing, such as statically-balanced key-value stores, Hadoop jobs, and supercomputer applications. We describe an improved latent fault detection method for unbalanced workloads. It retains the advantages of our previous methods: it is unsupervised, robust to changes, and statistically sound. Moreover, the statistical bounds for the new method scale better with the number of machines, and so dramatically reduce the number of measurements needed. Preliminary evaluation on supercomputer logs shows that the new method is able to correctly predict some failures, while our previous methods completely fail in this setting.", "num_citations": "9\n", "authors": ["1836"]}
{"title": "Direct-fuse: Removing the middleman for high-performance fuse file system support\n", "abstract": " Developing a file system is a challenging task, especially a kernel-level file system. User-level file systems alleviate the burden and development complexity associated with kernel-level implementations. The Filesystem in Userspace (FUSE) is a widely used tool that allows non-privileged users to develop file systems in user space. When a FUSE file system is mounted, it runs as a user-level process. Application programs and FUSE file system processes are bridged through FUSE kernel module. However, as the FUSE kernel module transfers requests between an application program and a file system process, the overheads in a FUSE file system call from crossing the user-kernel boundary is non-trivial. The overheads contain user-kernel mode switches, context switches, and additional memory copies. In this paper, we describe our Direct-FUSE framework that supports multiple FUSE file systems as well as other\u00a0\u2026", "num_citations": "8\n", "authors": ["1836"]}
{"title": "TSUBAME2. 0: The first petascale supercomputer in Japan and the greatest production in the world\n", "abstract": " TSUBAME2. 0, successor to TSUBAME1. 0 that superseded the Earth Simulator as Japan's fastest supercomputer in Spring 2006, became Japan's first muti-petascale supercomputer in history. TSUBAME2. 0 embodies various innovative, forward-looking architectural properties as well as software features, such as extensive use of GPUs, highly scalable and optical high-bandwidth node and network design, along with massive utilization of silicone I/O technologies such as solid state drives (SSD). TSUBAME1. 0/1.2 and 2.0 are supercomputing clusters, which consist of a large number of the usual processors such as Intel compatible CPUs. TSUBAME2. 0 includes a storage system with a capacity of 7.1 petabytes (PB), which is six times larger than that of TSUBAME1. 0. TSUBAME2. 0 provides both Linux and Windows operating systems on its compute nodes. While the typical usage of computing power of\u00a0\u2026", "num_citations": "8\n", "authors": ["1836"]}
{"title": "Model-based optimization for data-intensive application on virtual cluster\n", "abstract": " We propose a model-based optimization algorithm that determines virtual machine(VM) migration strategies, i.e., which VMs should be migrated to which nodes, while minimizing I/O access costs. We solve this problem as a shortest path problem of a direct acyclic graph which minimizes overall data access costs of target file accesses. Our simulation-based studies suggest that the proposed algorithm can achieve higher performance than simple techniques, such as ones that never migrate VMs or always migrate VMs onto the nodes that hold target files.", "num_citations": "7\n", "authors": ["1836"]}
{"title": "Cloudbb: Scalable i/o accelerator for shared cloud storage\n", "abstract": " Current shared cloud storage cannot provide sufficient I/O throughput for data-intensive HPC applications. Moreover, the consistency policy used in most shared cloud storage can cause parallel I/O applications to fail due to unexpected file inconsistencies. In order to resolve these problems, we propose a novel fast, scalable and fault tolerant filesystem called CloudBB (Cloud-based Burst Buffer). Unlike conventional filesystems, CloudBB creates an on-demand two-level hierarchical storage system and caches popular files to accelerate I/O performance. Since CloudBB supports multiple metadata servers, CloudBB is also highly scalable. In addition, by using file replication, failure detection and recovery techniques, CloudBB is resilient to failures. Furthermore, we implement CloudBB by using FUSE so that existing applications can run seamlessly and benefit from all of the CloudBB's capabilities without code\u00a0\u2026", "num_citations": "5\n", "authors": ["1836"]}
{"title": "Fault tolerance assistant (fta): An exception handling programming model for mpi applications\n", "abstract": " Future high-performance computing systems may face frequent failures with their rapid increase in scale and complexity. Resilience to faults has become a major challenge for large-scale applications running on supercomputers, which demands fault tolerance support for prevalent MPI applications. Among failure scenarios, process failures are one of the most severe issues as they usually lead to termination of applications. However, the widely used MPI implementations do not provide mechanisms for fault tolerance. We propose FTA-MPI (Fault Tolerance Assistant MPI), a programming model that provides support for failure detection, failure notification and recovery. Specifically, FTA-MPI exploits a try/catch model that enables failure localization and transparent recovery of process failures in MPI applications. We demonstrate FTA-MPI with synthetic applications and a molecular dynamics code CoMD, and show that FTA-MPI provides high programmability for users and enables convenient and flexible recovery of process failures.", "num_citations": "5\n", "authors": ["1836"]}
{"title": "Fault tolerance assistant (FTA): an exception handling approach for MPI programs\n", "abstract": " We propose FTA, a programming model that provides failure localization and transparent recovery of process failures in MPI applications.", "num_citations": "5\n", "authors": ["1836"]}
{"title": "Design and modeling of non-blocking checkpoint system\n", "abstract": " In purpose to minimize the overhead of checkpointing, we propose a non-blocking checkpointing system, based on writing checkpoints to the parallel file system in the background. Our preliminary evaluation shows that our non-blocking checkpointing system is highly efficient. It incurs less than 1% overhead for CPU-bound applications.", "num_citations": "4\n", "authors": ["1836"]}
{"title": "Accelerating big data infrastructure and applications (ongoing collaboration)\n", "abstract": " High-performance computing (HPC) systems are increasingly being used for data-intensive, or \"Big Data\", workloads. However, since traditional HPC workloads are compute-intensive, the HPC-Big Data convergence has created many challenges with optimizing data movement and processing on modern supercomputers. Our collaborative work addresses these challenges using a three-pronged approach: (i) measuring and modeling extreme-scale I/O workloads, (ii) designing a low-latency, scalable, on-demand burst-buffer solution, and (iii) optimizing graph algorithms for processing Big Data workloads. We describe the three areas of our collaboration and report on their respective developments.", "num_citations": "3\n", "authors": ["1836"]}
{"title": "Search for the Standard Model Higgs boson produced in association with top quarks and decaying into $$\\varvec {b\\bar {b}} $$ bb\u00af in $$\\varvec {pp} $$ pp collisions at $$\\sqrt\u00a0\u2026\n", "abstract": " A search for the Standard Model Higgs boson produced in association with a top-quark pair,, is presented. The analysis uses 20.3 fb\u2212 1 of pp collision data at, collected with the ATLAS detector at the Large Hadron Collider during 2012. The search is designed for the decay mode and uses events containing one or two electrons or muons. In order to improve the sensitivity of the search, events are categorised according to their jet and b-tagged jet multiplicities. A neural network is used to discriminate between signal and background events, the latter being dominated by+ jets production. In the single-lepton channel, variables calculated using a matrix element method are included as inputs to the neural network to improve discrimination of the irreducible background. No significant excess of events above the background expectation is found and an observed (expected) limit of 3.4 (2.2) times the\u00a0\u2026", "num_citations": "3\n", "authors": ["1836"]}
{"title": "Explorations of Data Swapping on Burst Buffer\n", "abstract": " Burst buffers have been widely deployed in many supercomputers to absorb bursty I/O and accelerate I/O performance. Previous work has shown that with burst buffer systems, I/O operations from computer nodes can be greatly accelerated. While the lack of data swapping supports on burst buffer leads to under-utilization and application failure issues. In addition, the effects of data replacement algorithms on application performance and the suitability of each algorithm for the target application are unclear. In this paper, we address these challenges by simulating data swapping on burst buffers with different data replacement strategies. Trace logs from a set of real-world HPC applications are used with different data replacement algorithms to show the behavior of representative HPC applications. From the results, we found that most HPC applications can still achieve full performance when using a buffer size that is\u00a0\u2026", "num_citations": "2\n", "authors": ["1836"]}
{"title": "Multi-client DeepIO for large-scale deep learning on HPC systems\n", "abstract": " Deep neural networks (DNNs) have gained tremendous interest due to their potential for solving complex problems. With the growth of computation power of processors and accelerators, such as on leadership High-Performance Computing (HPC) systems, larger datasets can be trained more efficiently with DNNs. However, not many efforts have worked to improve I/O support for DNNs to match the increasing computation power. Typically, on HPC systems, a training dataset is stored on a parallel file system or node-local storage devices. However, not all HPC clusters have node-local storage (eg, SSD), and large mini-batch sizes stress the read performance of parallel file systems since the large datasets cannot fit in file system caches. To make things worse, large mini-batches have been widely used in DNN training for speeding up training and maintaining accuracy [3\u20135, 7]. Thus, it is a challenge for DNNs with large mini-batches to achieve high training performance on HPC systems, particularly those with GPUs.In prior work, we proposed DeepIO [8] to mitigate the I/O pressure when loading datasets from parallel file systems. DeepIO is designed to support the I/O behavior of TensorFlow [2] and leverages on-node memory and InfiniBand between nodes to assist the mini-batch generation. TensorFlow is a popular machine learning framework that supports splitting a larger mini-batch into multiple small pieces or fetching multiple small mini-batches simultaneously to serve different workers, which is critical for achieving high training throughput. However, a key limitation of DeepIO is that it does not support multiple training workers on a single\u00a0\u2026", "num_citations": "2\n", "authors": ["1836"]}
{"title": "Burst SSD Buffer: Checkpoint Strategy at Extreme Scale\n", "abstract": " Checkpointing is an indispensable fault tolerance technique, commonly used by HPC applications that run continuously for hours or days at a time. However, when checkpointing extreme scale systems, the bursty nature of the I/O pattern of checkpointing overburdens file systems and also causes huge overhead to be added to an application\u2019s runtime. In order to alleviate the overhead and achieve fast checkpoint/restart, we propose a highly-resilient mini-SSD-based burst buffer system, and explore a checkpoint strategy on the system based on our checkpointing model.", "num_citations": "2\n", "authors": ["1836"]}
{"title": "Evaluation of skeletal maturation by using a computed X-ray densitometry method-Standard values of normal Japanese from the age of 6 to 20 years\n", "abstract": " CiNii \u8ad6\u6587 - Evaluation of skeletal maturation by using a computed X-ray densitometry method-Standard values of normal Japanese from the age of 6 to 20 years- CiNii \u56fd\u7acb\u60c5\u5831\u5b66 \u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb \u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005 \u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 Evaluation of skeletal maturation by using a computed X-ray densitometry method-Standard values of normal Japanese from the age of 6 to 20 years- SATO K \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 SATO K \u53ce\u9332\u520a\u884c\u7269 J Jpn Orthodentic Soc J Jpn Orthodentic Soc 52, 291-300, 1993 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 Bone Mineral Density and Bone Metabolic Markers in Children with Hyperthyroidism Before and During Treatment ASAKURA Yumi , \u2026", "num_citations": "2\n", "authors": ["1836"]}
{"title": "Erratum to: Search for supersymmetry in final states with two same-sign or three leptons and jets using 36 fb \u22121 of s = 13 TeV pp collision data with the ATLAS\u00a0\u2026\n", "abstract": " One correction is made to the figure 4e of the paper. The reported cross-sections, both the experimental upper limits and the theoretical prediction, corresponded only to a fiducial region of the phase space with at least two prompt same-sign electrons or muons before detector simulation (pT> 10 GeV,| \u03b7|< 2.8). The updated figure 4e instead presents the results without this requirement, ie in the full phase space as it should, given the y-axis label of the plot. The reported sensitivity of the analysis, ie the ability to exclude top squark masses below 700GeV in this scenario, is however unchanged since the same requirements were applied to the analysis and to the theory predictions.", "num_citations": "1\n", "authors": ["1836"]}
{"title": "Cache management with extended interest for information-centric networking\n", "abstract": " In Information-centric Networking (ICN), users can obtain contents from caches in ICN routers. However, how to use caches efficiently is one of challenges. One existing scheme to obtain caches uses a caching algorithm reducing cache overlaps among routers, and users can obtain caches by referring to records that routers have about caches. However, this scheme has two problems. First, routers which forward Interests towards a cache are limited. Second, this scheme depends on information from an ISP on forwarding Interests. In this paper, we propose Cache-aware routing scheme Referring Distance for contents managed by extended Interest for ICN (CaRD), which enables all routers to obtain the closest content without information from an ISP. In CaRD, routers record hop counts to a server and a cache as passing information of contents. Routers compare these hop counts, and then send request packets\u00a0\u2026", "num_citations": "1\n", "authors": ["1836"]}