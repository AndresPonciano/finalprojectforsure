{"title": "Detecting similar Java classes using tree algorithms\n", "abstract": " Similarity analysis of source code is helpful during development to provide, for instance, better support for code reuse. Consider a development environment that analyzes code while typing and that suggests similar code examples or existing implementations from a source code repository. Mining software repositories by means of similarity measures enables and enforces reusing existing code and reduces the developing effort needed by creating a shared knowledge base of code fragments. In information retrieval similarity measures are often used to find documents similar to a given query document. This paper extends this idea to source code repositories. It introduces our approach to detect similar Java classes in software projects using tree similarity algorithms. We show how our approach allows to find similar Java classes based on an evaluation of three tree-based similarity measures in the context of five\u00a0\u2026", "num_citations": "131\n", "authors": ["113"]}
{"title": "Using source code metrics to predict change-prone java interfaces\n", "abstract": " Recent empirical studies have investigated the use of source code metrics to predict the change- and defect-proneness of source code files and classes. While results showed strong correlations and good predictive power of these metrics, they do not distinguish between interface, abstract or concrete classes. In particular, interfaces declare contracts that are meant to remain stable during the evolution of a software system while the implementation in concrete classes is more likely to change. This paper aims at investigating to which extent the existing source code metrics can be used for predicting change-prone Java interfaces. We empirically investigate the correlation between metrics and the number of fine-grained source code changes in interfaces of ten Java open-source systems. Then, we evaluate the metrics to calculate models for predicting change-prone Java interfaces. Our results show that the external\u00a0\u2026", "num_citations": "124\n", "authors": ["113"]}
{"title": "Analyzing the evolution of web services using fine-grained changes\n", "abstract": " In the service-oriented paradigm web service interfaces are considered contracts between web service subscribers and providers. However, these interfaces are continuously evolving over time to satisfy changes in the requirements and to fix bugs. Changes in a web service interface typically affect the systems of its subscribers. Therefore, it is essential for subscribers to recognize which types of changes occur in a web service interface in order to analyze the impact on his/her systems. In this paper we propose a tool called WSDLDiff to extract fine-grained changes from subsequent versions of a web service interface defined in WSDL. In contrast to existing approaches, WSDLDiff takes into account the syntax of WSDL and extracts the WSDL elements affected by changes and the types of changes. With WSDLDiff we performed a study aimed at analyzing the evolution of web services using the fine-grained changes\u00a0\u2026", "num_citations": "100\n", "authors": ["113"]}
{"title": "Improving defect prediction using temporal features and non linear models\n", "abstract": " Predicting the defects in the next release of a large software system is a very valuable asset for the project manger to plan her resources. In this paper we argue that temporal features (or aspects) of the data are central to prediction performance. We also argue that the use of non-linear models, as opposed to traditional regression, is necessary to uncover some of the hidden interrelationships between the features and the defects and maintain the accuracy of the prediction in some cases.", "num_citations": "99\n", "authors": ["113"]}
{"title": "A manual categorization of android app development issues on stack overflow\n", "abstract": " While many tutorials, code examples, and documentation about Android APIs exist, developers still face various problems with the implementation of Android Apps. Many of these issues are discussed on Q&A-sites, such as Stack Overflow. In this paper we present a manual categorization of 450 Android related posts of Stack Overflow concerning their question and problem types. The idea is to find dependencies between certain problems and question types to get better insights into issues of Android App development. The categorization is developed using card sorting with three experienced Android App developers. An initial approach to automate the classification of Stack Overflow posts using Lucene is also presented. The study highlights that the most common question types are 'How to?' and 'What is the problem?'. The problems that are discussed most often are related to 'User Interface' and 'Core Elements'\u00a0\u2026", "num_citations": "49\n", "authors": ["113"]}
{"title": "Synonym suggestion for tags on stack overflow\n", "abstract": " The amount of diverse tags used to classify posts on Stack Overflow increased in the last years to more than 38,000 tags. Many of these tags have the same or similar meaning. Stack Overflow provides an approach to reduce the amount of tags by allowing privileged users to manually create synonyms. However, currently exist only 2,765 synonym-pairs on Stack Overflow that is quite low compared to the total number of tags. To comprehend how synonym-pairs are built, we manually analyzed the tags and how the synonyms could be created automatically. Based on our findings, we then present TSST, a tag synonym suggestion tool, that outputs a ranked list of possible synonyms for each input tag. We first evaluated TSST with the 2,765 approved synonym-pairs of Stack Overflow. For 88.4% of the tags TSST finds the correct synonyms, for 72.2% the correct synonym is within the top 10 suggestions. In addition, we\u00a0\u2026", "num_citations": "34\n", "authors": ["113"]}
{"title": "Refactoring fat interfaces using a genetic algorithm\n", "abstract": " Recent studies have shown that the violation of the Interface Segregation Principle (ISP) is critical for maintaining and evolving software systems. Fat interfaces (i.e., interfaces violating the ISP) change more frequently and degrade the quality of the components coupled to them. According to the ISP the interfaces' design should force no client to depend on methods it does not invoke. Fat interfaces should be split into smaller interfaces exposing only the methods invoked by groups of clients. However, applying the ISP is a challenging task when fat interfaces are invoked differently by many clients. In this paper, we formulate the problem of applying the ISP as a multi-objective clustering problem and we propose a genetic algorithm to solve it. We evaluate the capability of the proposed genetic algorithm with 42,318 public Java APIs whose clients' usage has been mined from the Maven repository. The results of this\u00a0\u2026", "num_citations": "23\n", "authors": ["113"]}
{"title": "Grouping android tag synonyms on stack overflow\n", "abstract": " On Stack Overflow, more than 38,000 diverse tags are used to classify posts. The Stack Overflow community provides tag synonyms to reduce the number of tags that have the same or similar meaning. In our previous research, we used those synonym pairs to derive a number of strategies to create tag synonyms automatically.", "num_citations": "22\n", "authors": ["113"]}
{"title": "Improving fact extraction of framework-based software systems\n", "abstract": " Modern software frameworks provide a set of common and prefabricated software artifacts that support engineers in developing large-scale software systems. Framework-related information can be implemented in source code, comments or configuration files, but in the latter two cases, current reverse engineering approaches miss important facts reducing the quality of subsequent analysis tasks. We introduce a generic fact extraction approach for framework-based systems by combining traditional parsing with lexical pattern matching to obtain framework-specific facts from all three sources. We evaluate our approach with an industrial software application that was built using the Avalon/Phoenix framework. In particular we give examples to point out the benefits of considering framework-related information and reflect experiences made during the case study.", "num_citations": "22\n", "authors": ["113"]}
{"title": "Using run-time data for program comprehension\n", "abstract": " Traditional approaches for program comprehension use static program analysis or dynamic program analysis in the form of execution traces. Our approach, however, makes use of runtime-data such as parameter and object values. Compared to traditional program comprehension techniques, this approach enables fundamentally new ways of program analysis which we have not seen so far. Reflection analysis which allows engineers to understand programs making use of reflective (dynamic) method invocations is one such analysis. Another is object tracing which allows engineers to trace and track the use of a given instance of a class within the program to be understood. In this paper we present these techniques along with a case study to which we have applied them.", "num_citations": "22\n", "authors": ["113"]}
{"title": "Extracting timed automata from Java methods\n", "abstract": " The verification of the time behavior in distributed, multi-threaded programs is challenging, mainly because modern programming languages only provide means to represent time without a proper semantics. Current approaches to extract time models from source code represent time only as a sequence of events or require developers to manually provide a formal model of the time behavior. This makes it difficult for developers to verify various aspects of their systems, such as timeouts, delays and periodicity of the execution. In this paper, we introduce a definition of the time semantics of the Java programming language. Based on the semantics, we present an approach to automatically extract timed automata and their time constraints from the Java methods source code. First, we detect Java statements which involve time, from which we then extract the timed automata that are directly amenable to the verification of\u00a0\u2026", "num_citations": "14\n", "authors": ["113"]}
{"title": "Co-evolution analysis of production and test code by learning association rules of changes\n", "abstract": " Many modern software systems come with automated tests. While these tests help to maintain code quality by providing early feedback after modifications, they also need to be maintained. In this paper, we replicate a recent pattern mining experiment to find patterns on how production and test code co-evolve over time. Understanding co-evolution patterns may directly affect the quality of tests and thus the quality of the whole system. The analysis takes into account fine grained changes in both types of code. Since the full list of fine grained changes cannot be perceived, association rules are learned from the history to extract co-change patterns. We analyzed the occurrence of 6 patterns throughout almost 2500 versions of a Java system and found that patterns are present, but supported by weaker links than in previously reported. Hence we experimented with weighting methods and investigated the composition of\u00a0\u2026", "num_citations": "12\n", "authors": ["113"]}
{"title": "Extracting dynamic dependencies between web services using vector clocks\n", "abstract": " Service Oriented Architecture (SOA) enables organizations to react to requirement changes in an agile manner and to foster the reuse of existing services. However, the dynamic nature of service oriented systems and their agility bear the challenge of properly understanding such systems. In particular, understanding the dependencies among services is a non trivial task, especially if service oriented systems are distributed over several hosts belonging to different departments of an organization. In this paper, we propose an approach to extract dynamic dependencies among web services. The approach is based on the vector clocks, originally conceived and used to order events in a distributed environment. We use the vector clocks to order service executions and to infer causal dependencies among services. We show the feasibility of the approach by implementing it into the Apache CXF framework and\u00a0\u2026", "num_citations": "12\n", "authors": ["113"]}
{"title": "Can I depend on you? Mapping the dependency and quality landscape of ROS packages\n", "abstract": " Since its beginnings ten years ago, the Robot Operating System (ROS) has created a huge community of developers and researchers and is now the most widespread open-source framework for robotics development. It is used in research, prototyping but also in commercial products and supports a wide range of robotic platforms, sensors and highlevel data processing functions. While for a research platform, quality of the software developed with it is typically of lower importance, ROS is gradually moving towards industrial applications making software quality a premier topic. In this paper, we want to gain insights on how ROS is used in practice, how high the quality of the ROS packages and applications is, and where potential pitfalls in the use of ROS lie. To achieve this, we have analyzed several thousands of open-source ROS packages found on GitHub and Bitbucket for their quality and their interdependencies\u00a0\u2026", "num_citations": "9\n", "authors": ["113"]}
{"title": "A genetic algorithm to find the adequate granularity for service interfaces\n", "abstract": " The relevance of the service interfaces' granularity and its architectural impact have been widely investigated in literature. Existing studies show that the granularity of a service interface, in terms of exposed operations, should reflect their clients' usage. This idea has been formalized in the Consumer-Driven Contracts pattern (CDC). However, to the best of our knowledge, no studies propose techniques to assist providers in finding the right granularity and in easing the adoption of the CDC pattern. In this paper, we propose a genetic algorithm that mines the clients' usage of service operations and suggests Fa\u00e7ade services whose granularity reflect the usage of each different type of clients. These services can be deployed on top of the original service and they become contracts for the different types of clients satisfying the CDC pattern. A first study shows that the genetic algorithm is capable of finding Fa\u00e7ade\u00a0\u2026", "num_citations": "8\n", "authors": ["113"]}
{"title": "Leveraging machine learning for software redocumentation\n", "abstract": " Source code comments contain key information about the underlying software system. Many redocumentation approaches, however, cannot exploit this valuable source of information. This is mainly due to the fact that not all comments have the same goals and target audience and can therefore only be used selectively for redocumentation. Performing a required classification manually, e.g. in the form of heuristic rules, is usually time-consuming and error-prone and strongly dependent on programming languages and guidelines of concrete software systems. By leveraging machine learning, it should be possible to classify comments and thus transfer valuable information from the source code into documentation with less effort but the same quality. We applied different machine learning techniques to a COBOL legacy system and compared the results with industry-strength heuristic classification. As a result, we\u00a0\u2026", "num_citations": "7\n", "authors": ["113"]}
{"title": "Towards model checking security of real time Java software\n", "abstract": " More and more software libraries and applications in high-performance computing and distributed systems are coded using the Java programming language. The correctness of such pieces of code w.r.t. a given set of security policies often depends on the correct handling of timing between concurrent or recurrent events. Model-checking has proven to be an effective tool for verifying the correctness of software. In spite of the growing importance of this application area of formal methods, though, no approach exists that targets the problem of verifying the correctness of real-time software w.r.t. timed specifications. The few existing works focus on very different problems, such as schedulability analysis of Java tasks. In this paper we present an approach combining rule-based static analysis together with symbolic execution of Java code to extract networks of timed automata from existing software and then use Uppaal\u00a0\u2026", "num_citations": "7\n", "authors": ["113"]}
{"title": "Towards a weighted voting system for Q&A sites\n", "abstract": " Q&A sites have become popular to share and look for valuable knowledge. Users can easily and quickly access high quality answers to common questions. The main mechanism to label good answers is to count the votes per answer. This mechanism, however, does not consider whether other answers were present at the time when a vote is given. Consequently, good answers that were given later are likely to receive less votes than they would have received if given earlier. In this paper we present a Weighted Votes (WV) metric that gives different weights to the votes depending on how many answers were present when the vote is performed. The idea behind WV is to emphasize the answer that receives most of the votes when most of the answers were already posted. Mining the Stack Overflow data dump we show that the WV metric is able to highlight between 4.07% and 10.82% answers that differ from the\u00a0\u2026", "num_citations": "7\n", "authors": ["113"]}
{"title": "Using vector clocks to monitor dependencies among services at runtime\n", "abstract": " Service-Oriented Architecture (SOA) enable organizations to react to requirement changes in an agile manner and to foster the reuse of existing services. However, the dynamic nature of Service-Oriented Systems and their agility bear the challenge of properly understanding such systems. In particular, understanding the dependencies among services is a non trivial task, especially if service-oriented systems are distributed over several hosts and/or using different SOA technologies.", "num_citations": "6\n", "authors": ["113"]}
{"title": "Modeling time in Java programs for automatic error detection\n", "abstract": " Modern programming languages, such as Java, represent time as integer variables, called timestamps. Timestamps allow developers to tacitly model incorrect time values resulting in a program failure because any negative value or every positive value is not necessarily a valid time representation. Current approaches to automatically detect errors in programs, such as Randoop and FindBugs, cannot detect such errors because they treat timestamps as normal integer variables and test them with random values verifying if the program throws an exception. In this paper, we present an approach that considers the time semantics of the Java language to systematically detect time related errors in Java programs. With the formal time semantics, our approach determines which integer variables handle time and which statements use or alter their values. Based on this information, it translates these statements into an\u00a0\u2026", "num_citations": "5\n", "authors": ["113"]}
{"title": "Exploring visual comparison of multivariate runtime statistics\n", "abstract": " To understand program behavior or find performance bottlenecks in their software, developers need tools to efficiently compare runtime statistics collected across multiple executions. As there is a variety of useful metrics, a good visualization needs to be able to handle multivariate data and highlight the most important differences between multiple versions. We identify three scenarios for the comparison of executionrelevant changes, and explore possible visualizations of the gathered multivariate runtime statistics.", "num_citations": "4\n", "authors": ["113"]}
{"title": "Can ROS be used securely in industry? Red teaming ROS-Industrial\n", "abstract": " With its growing use in industry, ROS is rapidly becoming a standard in robotics. While developments in ROS 2 show promise, the slow adoption cycles in industry will push widespread ROS 2 industrial adoption years from now. ROS will prevail in the meantime which raises the question: can ROS be used securely for industrial use cases even though its origins didn't consider it? The present study analyzes this question experimentally by performing a targeted offensive security exercise in a synthetic industrial use case involving ROS-Industrial and ROS packages. Our exercise results in four groups of attacks which manage to compromise the ROS computational graph, and all except one take control of most robotic endpoints at desire. To the best of our knowledge and given our setup, results do not favour the secure use of ROS in industry today, however, we managed to confirm that the security of certain robotic endpoints hold and remain optimistic about securing ROS industrial deployments.", "num_citations": "3\n", "authors": ["113"]}
{"title": "Diffviz: A diff algorithm independent visualization tool for edit scripts\n", "abstract": " A number of approaches and tools exist that extract and visualize the changes between two versions of a file and thereby help developers to understand them. DiffViz is an interactive visualization tool that visualizes the changes independent from the differencing algorithm. It supports, but is not limited to, a granularity on the level of abstract syntax trees. Furthermore, it provides several new features, such as node matching and the mini-map, to navigate and analyze the changes. A demo of the installation and example usage of the tool is available here: https://youtu.be/RF93ey9GYoc.", "num_citations": "3\n", "authors": ["113"]}
{"title": "XVIZIT: Visualizing cognitive units in spreadsheets\n", "abstract": " Spreadsheets can be large and complex and their maintenance and comprehension difficult to end-users. Large numbers of cells, complex formulae and missing documentation can impede the understanding of a spreadsheet. Comprehension assesses different levels of a spreadsheet according to a specific maintenance task, ranging from single formulae over sets of cells to complex structural patterns. These levels of abstraction are subsumed under the term cognitive unit. XVIZIT helps end-users in maintaining and comprehending spreadsheets. It guides them through a spreadsheet model: Roles of cells and sheets, similar patterns and various concepts of modularity can be explored. It uses modularization algorithms to provide conceptional decompositions of a spreadsheet model, such as equivalence classes or data modules. XVIZIT's slice visualizations ease the evaluation of corrective modifications by\u00a0\u2026", "num_citations": "3\n", "authors": ["113"]}
{"title": "A Cryptography-Powered Infrastructure to Ensure the Integrity of Robot Workflows\n", "abstract": " With the growing popularity of robots, the development of robot applications is subject to an ever increasing number of additional requirements from eg, safety, legal and ethical sides. The certification of an application for compliance to such requirements is an essential step in the development of a robot program. However, at this point in time it must be ensured that the integrity of this program is preserved meaning that no intentional or unintentional modifications happen to the program until the robot executes it. Based on the abstraction of robot programs as workflows we present in this work a cryptography-powered distributed infrastructure for the preservation of robot workflows. A client composes a robot program and once it is accepted a separate entity provides a digital signature for the workflow and its parameters which can be verified by the robot before executing it. We demonstrate a real-world implementation of this infrastructure using a mobile manipulator and its software stack. We also provide an outlook on the integration of this work into our larger undertaking to provide a distributed ledger-based compliant robot application development environment. View Full-Text", "num_citations": "2\n", "authors": ["113"]}
{"title": "Guest editorial: reverse engineering\n", "abstract": " Reverse engineering aims at obtaining high-level representations of software systems from existing low-level artifacts, such as binaries, source code, execution traces or historical information. Reverse engineering methods and technologies play an important role in many software engineering tasks and quite often are the only way to get an understanding of large and complex software systems. Given for example the task to understand certain aspects of the test code in large software systems, such as Eclipse. How would you figure out the answers to the following questions:\u2013How are the tests in Eclipse organized and where is the test code located?\u2013What is tested by a unit test, test plug-in, and a whole test suite\u2014what is not?\u2013What was the reason for running these tests?\u2013What influences the test execution environment?", "num_citations": "2\n", "authors": ["113"]}
{"title": "AWPS\u2013an architecture for pro-active web performance management\n", "abstract": " The growing demand for quality and performance has become a discriminating factor in the field of software applications. Specifically in the area of web applications, performance has become a key factor for success creating the need for new types of performance evaluation models and methods capable of representing the dynamic characteristics of web environments.               In this paper we will recall seminal work in this area and present AWPS, a tool for automatic web performance simulation and prediction. AWPS is capable of (a) automatically creating a web performance simulation and (b) conducting trend analysis of the system under test. The operation and usage of this tool is demonstrated in a case study of a two-tier architecture system.", "num_citations": "2\n", "authors": ["113"]}
{"title": "ARCo: Architecture Recovery in Context\n", "abstract": " Software architecture recovery is an activity which is carried out in contexts such as software production, forensic computing, computer security and education. Each context is determined by an ambit, a set of resources and situations that affect a group of stakeholders. There are multiple techniques, methods and frameworks to recover the architecture of a software product, however, they assume that architecture recovery is exclusive of the software production context. This assumption makes its application difficult in other contexts, because the differences between ambits, resources, situations or interests of the stakeholders of the process are not taken into account. We introduce ARCo, a framework for the recovery and analysis of architectural views, shaped by a conceptual descriptive system and a set of instrumental elements of operational type. To define ARCo, we analyze the approaches of architecture recovery using the pattern matching technique. We evaluate ARCo through a single case study in different contexts; one was the context of education to support the teaching and learning process, and the other, the context of software development to recovery the documentation of a software product. ARCo is made up of a conceptual model, some guidelines, an architecture recovery methodology and a query mechanism. ARCo allowed to recover the architecture of software products in different contexts, attending to the particular needs of each one of them. We conclude that ARCo offer a new approach for software architecture recovery, since it involves the context where the process is carried on, and hide its complexity from stakeholders.", "num_citations": "1\n", "authors": ["113"]}
{"title": "IT-Application Behaviour Analysis: Predicting Critical System States on OpenStack using Monitoring Performance Data and Log Files.\n", "abstract": " Recent studies have proposed several ways to optimize the stability of IT-services with an extensive portfolio of processual, reactive or proactive approaches. The goal of this paper is to combine monitored performance data, such as CPU utilization, with discrete data from log files in a joint model to predict critical system states. We propose a systematic method to derive mathematical prediction models, which we experimentally test using a downsized clone of a real life contract management system as a testbed. First, this testbed is used for data acquisition under variable and fully controllable system loads. Next, based on the monitored performance metrics and log file data, we train models (logistic regression and decision trees) that unify both, numeric and textual, data types in a single incident forecasting model. We focus on 1) investigating different cases to identify an appropriate prediction time window, allowing to prepare countermeasures by considering prediction accuracy and 2) identifying variables that appear more likely than others in the predictive models.", "num_citations": "1\n", "authors": ["113"]}
{"title": "Visualizing Evolution and Performance Metrics on Method-Level as Multivariate Data.\n", "abstract": " Visualizing the evolution of software metrics helps understanding the project progress of a software development team with respect to code quality and related characteristics. Blending this information with performance information can provide relevant insights into crucial changes in execution behavior and their respective context from code changes. We interpret this composition of evolution and performance metrics as multivariate data and map it to a fine-grained method level. This is the basis for investigating a multivariate visualization approach consisting of a visually enriched tabular representation that provides the method-level details for all the metrics across time, a projection view that shows clusters and outliers among the methods on a higher-level of abstraction, and a timeline view to find relevant temporal changes. Interactions connect the views and allow the users to explore the data step by step.", "num_citations": "1\n", "authors": ["113"]}
{"title": "Automatic repair of timestamp comparisons\n", "abstract": " Automated program repair has the potential to reduce the developers' effort to fix errors in their code. In particular, modern programming languages, such as Java, C, and C\\#, represent time as integer variables that suffer from integer overflow, introducing subtle errors that are hard to discover and repair. Recent researches on automated program repair rely on test cases to discover failures to correct, making them suitable only for regression errors. We propose a new strategy to automatically repair programs that suffer from timestamp overflows that are manifested in comparison expressions. It unifies the benefits of static analysis and automatic program repair avoiding dependency on testing to identify and correct defected code. Our approach performs an abstract analysis over the time domain of a program using a Time Type System to identify the problematic comparison expressions. The repairing strategy rewrites\u00a0\u2026", "num_citations": "1\n", "authors": ["113"]}
{"title": "Analyzing the impact of external and internal cohesion on the change-proneness of web apis\n", "abstract": " Several metrics have been proposed in literature to highlight change-prone software components in order to ease their maintainability. However, to the best of our knowledge, no such studies exist for web APIs (ie, APIs exposed and accessible via networks) whose popularity has grown considerably over the last years. Web APIs are considered contracts between providers and consumers and stability is a key quality attribute of them. We present a qualitative and quantitative study of the change-proneness of web APIs with low external and internal cohesion. First, we report on an online survey to investigate the maintenance scenarios that cause changes to web APIs. Then, we define an internal cohesion metric and analyze its correlation with the changes performed in ten well known WSDL APIs.", "num_citations": "1\n", "authors": ["113"]}