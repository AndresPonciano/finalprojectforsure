{"title": "A reconfigurable fabric for accelerating large-scale datacenter services\n", "abstract": " Datacenter workloads demand high computational capabilities, flexibility, power efficiency, and low cost. It is challenging to improve all of these factors simultaneously. To advance datacenter capabilities beyond what commodity server designs can provide, we have designed and built a composable, reconfigurable fabric to accelerate portions of large-scale software services. Each instantiation of the fabric consists of a 6\u251c\u00f98 2-D torus of high-end Stratix V FPGAs embedded into a half-rack of 48 machines. One FPGA is placed into each server, accessible through PCIe, and wired directly to other FPGAs with pairs of 10 Gb SAS cables. In this paper, we describe a medium-scale deployment of this fabric on a bed of 1,632 servers, and measure its efficacy in accelerating the Bing web search engine. We describe the requirements and architecture of the system, detail the critical engineering challenges and solutions\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1184\n", "authors": ["35"]}
{"title": "Efficient path profiling\n", "abstract": " A path profile determines how many times each acyclic path in a routine executes. This type of profiling subsumes the more common basic block and edge profiling, which only approximate path frequencies. Path profiles have many potential uses in program performance tuning, profile-directed compilation, and software test coverage. This paper describes a new algorithm for path profiling. This simple, fast algorithm selects and places profile instrumentation to minimize run-time overhead. Instrumented programs run with overhead comparable to the best previous profiling techniques. On the SPEC95 benchmarks, path profiling overhead averaged 31%, as compared to 16% for efficient edge profiling. Path profiling also identifies longer paths than a previous technique, which predicted paths from edge profiles (average of 88, versus 34 instructions). Moreover, profiling shows that the SPEC95 train input datasets\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "922\n", "authors": ["35"]}
{"title": "Mining specifications\n", "abstract": " Program verification is a promising approach to improving program quality, because it can search all possible program executions for specific errors. However, the need to formally describe correct behavior or errors is a major barrier to the widespread adoption of program verification, since programmers historically have been reluctant to write formal specifications. Automating the process of formulating specifications would remove a barrier to program verification and enhance its practicality.This paper describes specification mining, a machine learning approach to discovering formal specifications of the protocols that code must obey when interacting with an application program interface or abstract data type. Starting from the assumption that a working program is well enough debugged to reveal strong hints of correct protocols, our tool infers a specification by observing program execution and concisely\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "904\n", "authors": ["35"]}
{"title": "Optimally profiling and tracing programs\n", "abstract": " This paper describes algorithms for inserting monitoring code to profile and trace programs. These algorithms greatly reduce the cost of measuring programs with respect to the commonly used technique of placing code in each basic block. Program profiling counts the number of times each basic block in a program executes. Instruction tracing records the sequence of basic blocks traversed in a program execution. The algorithms optimize the placement of counting/tracing code with respect to the expected or measured frequency of each block or edge in a program's control-flow graph. We have implemented the algorithms in a profiling/tracing tool, and they substantially reduce the overhead of profiling and tracing.We also define and study the hierarchy of profiling problems. These  problems have two dimensions: what is profiled (i.e., vertices (basic blocks) or edges in a control-flow graph) and where the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "758\n", "authors": ["35"]}
{"title": "Software and the Concurrency Revolution: Leveraging the full power of multicore processors demands new tools and new thinking from the software industry.\n", "abstract": " Concurrency has long been touted as the \"next big thing\" and \"the way of the future,\" but for the past 30 years, mainstream software development has been able to ignore it. Our parallel future has finally arrived: new machines will be parallel machines, and this will require major changes in the way we develop software. The introductory article in this issue describes the hardware imperatives behind this shift in computer architecture from uniprocessors to multicore processors, also known as CMPs.", "num_citations": "734\n", "authors": ["35"]}
{"title": "Exploiting hardware performance counters with flow and context sensitive profiling\n", "abstract": " A program profile attributes run-time costs to portions of a program's execution. Most profiling systems suffer from two major deficiencies: first, they only apportion simple metrics, such as execution frequency or elapsed time to static, syntactic units, such as procedures or statements; second, they aggressively reduce the volume of information collected and reported, although aggregation can hide striking differences in program behavior.This paper addresses both concerns by exploiting the hardware counters available in most modern processors and by incorporating two concepts from data flow analysis--flow and context sensitivity--to report more context for measurements. This paper extends our previous work on efficient path profiling to flow sensitive profiling, which associates hardware performance metrics with a path through a procedure. In addition, it describes a data structure, the calling context tree, that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "644\n", "authors": ["35"]}
{"title": "Transactional memory\n", "abstract": " The advent of multicore processors has renewed interest in the idea of incorporating transactions into the programming model used to write parallel programs. This approach, known as transactional memory, offers an alternative, and hopefully better, way to coordinate concurrent threads. The ACI (atomicity, consistency, isolation) properties of transactions provide a foundation to ensure that concurrent reads and writes of shared data do not produce inconsistent or incorrect results. At a higher level, a computation wrapped in a transaction executes atomically - either it completes successfully and commits its result in its entirety or it aborts. In addition, isolation ensures the transaction produces the same result as if no other transactions were executing concurrently. Although transactions are not a parallel programming panacea, they shift much of the burden of synchronizing and coordinating parallel computations from\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "622\n", "authors": ["35"]}
{"title": "EEL: Machine-independent executable editing\n", "abstract": " EEL (Executable Editing Library) is a library for building tools to analyze and modify an executable (compiled) program. The systems and languages communities have built many tools for error detection, fault isolation, architecture translation, performance measurement, simulation, and optimization using this approach of modifying executables. Currently, however, tools of this sort are difficult and time-consuming to write and are usually closely tied to a particular machine and operating system. EEL supports a machine-and system-independent editing model that enables tool builders to modify an executable without being aware of the details of the underlying architecture or operating system or being concerned with the consequences of deleting instructions or adding foreign code.", "num_citations": "600\n", "authors": ["35"]}
{"title": "Transactional memory\n", "abstract": " NOTE \u0393\u00fc\u00e2 A New Edition of This Title is Available: Transactional Memory, Second Edition The advent of multicore processors has renewed interest in the idea of incorporating transactions into the programming model used to write parallel programs. This approach, known as transactional memory, offers an alternative, and hopefully better, way to coordinate concurrent threads. The ACI (atomicity, consistency, isolation) properties of transactions provide a foundation to ensure that concurrent reads and writes of shared data do not produce inconsistent or incorrect results. At a higher level, a computation wrapped in a transaction executes atomically \u0393\u00c7\u00f4 either it completes successfully and commits its result in its entirety or it aborts. In addition, isolation ensures the transaction produces the same result as if no other transactions were executing concurrently. Although transactions are not a parallel programming panacea\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "598\n", "authors": ["35"]}
{"title": "Tempest and Typhoon: User-level shared memory\n", "abstract": " Future parallel computers must efficiently execute not only hand-coded applications but also programs written in high-level, parallel programming languages. Today's machines limit these programs to a single communication paradigm, either message-passing or shared-memory, which results in uneven performance. This paper addresses this problem by defining an interface, Tempest, that exposes low-level communication and memory-system mechanisms so programmers and compilers can customize policies for a given application. Typhoon is a proposed hardware platform that implements these mechanisms with a fully-programmable, user-level processor in the network interface. We demonstrate the utility of Tempest with two examples. First, the Stache protocol uses Tempest's finegrain access control mechanisms to manage part of a processor's local memory as a large, fully-associative cache for remote data\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "538\n", "authors": ["35"]}
{"title": "The wisconsin wind tunnel: Virtual prototyping of parallel computers\n", "abstract": " We have developed a new technique for evaluating cache coherent, shared-memory computers. The Wisconsin Wind Tunnel (WWT) runs a parallel shared-memory program on a parallel computer (CM-5) and uses execution-driven, distributed, discrete-event simulation to accurately calculate program execution time. WWT is a virtual prototype that exploits similarities between the system under design (the target) and an existing evaluation platform (the host). The host directly executes all target program instructions and memory references that hit in the target cache. WWT's shared memory uses the CM-5 memory's error-correcting code (ECC) as valid bits for a fine-grained extension of shared virtual memory. Only memory references that miss in the target cache trap to WWT, which simulates a cache-coherence protocol. WWT correctly interleaves target machine events and calculates target program execution time\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "503\n", "authors": ["35"]}
{"title": "Cache-conscious structure layout\n", "abstract": " Hardware trends have produced an increasing disparity between processor speeds and memory access times. While a variety of techniques for tolerating or reducing memory latency have been proposed, these are rarely successful for pointer-manipulating programs. This paper explores a complementary approach that attacks the source (poor reference locality) of the problem rather than its manifestation (memory latency). It demonstrates that careful data organization and layout provides an essential mechanism to improve the cache locality of pointer-manipulating programs and consequently, their performance. It explores two placement techniques---clustering and coloring---that improve cache performance by increasing a pointer structure's spatial and temporal locality, and by reducing cache-conflicts. To reduce the cost of applying these techniques, this paper discusses two strategies---cache-conscious\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "477\n", "authors": ["35"]}
{"title": "Whole program paths\n", "abstract": " Whole program paths (WPP) are a new approach to capturing and representing a program's dynamic---actually executed---control flow. Unlike other path profiling techniques, which record intraprocedural or acyclic paths, WPPs produce a single, compact description of a program's entire control flow, including loop iteration and interprocedural paths.This paper explains how to collect and represent WPPs. It also shows how to use WPPs to find hot subpaths, which are the heavily executed sequences of code that should be the focus of performance tuning and compiler optimization.", "num_citations": "442\n", "authors": ["35"]}
{"title": "Branch prediction for free\n", "abstract": " Many compilers rely on branch prediction to improve program performance by identifying frequently executed regions and by aiding in scheduling instructions.Profile-based predictors require a time-consuming and inconvenient compile-profile-compile cycle in order to make predictions. We present a program-based branch predictor that performs well for a large and diverse set of programs written in C and Fortran. In addition to using natural loop analysis to predict branches that control the iteration of loops, we focus on heuristics for predicting non-loop branches, which dominate the dynamic branch count of many programs. The heuristics are simple and require little program analysis, yet they are effective in terms of coverage and miss rate. Although program-based prediction does not equal the accuracy of profile-based prediction, we believe it reaches a sufficiently high level to be useful. Additional type and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "437\n", "authors": ["35"]}
{"title": "Personal data mining\n", "abstract": " Personal data mining mechanisms and methods are employed to identify relevant information that otherwise would likely remain undiscovered. Users supply personal data that can be analyzed in conjunction with data associated with a plurality of other users to provide useful information that can improve business operations and/or quality of life. Personal data can be mined alone or in conjunction with third party data to identify correlations amongst the data and associated users. Applications or services can interact with such data and present it to users in a myriad of manners, for instance as notifications of opportunities.", "num_citations": "418\n", "authors": ["35"]}
{"title": "Virtual entertainment\n", "abstract": " Systems and methods that provide for a virtual reality entertainment system that supplies immersive entertainment and creates a sensation for a user similar to having guests in a remote location to be physically present as virtual guests. Such virtual reality entertainment system can supply a graphic and/or audio; wherein interconnected computers, video and audio processing devices, supply a live interaction between a user and a guest (s). Although guests are only present virtually (eg, electronically present with other objects/user within the environment) such virtual invitation enables a user and guests to concurrently experience the entertainment together (eg, a live sporting event, spectator game). In a related aspect, the subject innovation can implement holographic avatars, and a plurality of communication interfaces, to imitate (and/or transform) a relationship between the user and the virtual guests/surrounding\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "407\n", "authors": ["35"]}
{"title": "Singularity: rethinking the software stack\n", "abstract": " Every operating system embodies a collection of design decisions. Many of the decisions behind today's most popular operating systems have remained unchanged, even as hardware and software have evolved. Operating systems form the foundation of almost every software stack, so inadequacies in present systems have a pervasive impact. This paper describes the efforts of the Singularity project to re-examine these design choices in light of advances in programming languages and verification tools. Singularity systems incorporate three key architectural features: software-isolated processes for protection of programs and system services, contract-based channels for communication, and manifest-based programs for verification of system properties. We describe this foundation in detail and sketch the ongoing research in experimental systems that build upon it.", "num_citations": "383\n", "authors": ["35"]}
{"title": "Fine-grain access control for distributed shared memory\n", "abstract": " This paper discusses implementations of fine-grain memory access control, which selectively restricts reads and writes to cache-block-sized memory regions. Fine-grain access control forms the basis of efficient cache-coherent shared memory. This paper focuses on low-cost implementations that require little or no additional hardware. These techniques permit efficient implementation of shared memory on a wide range of parallel systems, thereby providing shared-memory codes with a portability previously limited to message passing.", "num_citations": "380\n", "authors": ["35"]}
{"title": "The use of program profiling for software maintenance with applications to the year 2000 problem\n", "abstract": " This paper describes new techniques to help with testing and debugging, using information obtained from path profiling. A path profiler instruments a program so that the number of times each different loop-free path executes is accumulated during an execution run. With such an instrumented program, each run of the program generates a path spectrum for the execution\u0393\u00c7\u00f6a distribution of the paths that were executed during that run. A path spectrum is a finite, easily obtainable characterization of a program's execution on a dataset, and provides a behavior signature for a run of the program.             Our techniques are based on the idea of comparing path spectra from different runs of the program. When different runs produce different spectra, the spectral differences can be used to identify paths in the program along which control diverges in the two runs. By choosing input datasets to hold all factors constant\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "372\n", "authors": ["35"]}
{"title": "Language support for fast and reliable message-based communication in Singularity OS\n", "abstract": " Message-based communication offers the potential benefits of providing stronger specification and cleaner separation between components. Compared with shared-memory interactions, message passing has the potential disadvantages of more expensive data exchange (no direct sharing) and more complicated programming. In this paper we report on the language, verification, and run-time system features that make messages practical as the sole means of communication between processes in the Singularity operating system. We show that using advanced programming language and verification techniques, it is possible to provide and enforce strong system-wide invariants that enable efficient communication and low-overhead software-based process isolation. Furthermore, specifications on communication channels help in detecting programmer mistakes early---namely at compile-time---thereby reducing the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "370\n", "authors": ["35"]}
{"title": "Join-idle-queue: A novel load balancing algorithm for dynamically scalable web services\n", "abstract": " The prevalence of dynamic-content web services, exemplified by search and online social networking, has motivated an increasingly wide web-facing front end. Horizontal scaling in the Cloud is favored for its elasticity, and distributed design of load balancers is highly desirable. Existing algorithms with a centralized design, such as Join-the-Shortest-Queue (JSQ), incur high communication overhead for distributed dispatchers.We propose a novel class of algorithms called Join-Idle-Queue (JIQ) for distributed load balancing in large systems. Unlike algorithms such as Power-of-Two, the JIQ algorithm incurs no communication overhead between the dispatchers and processors at job arrivals. We analyze the JIQ algorithm in the large system limit and find that it effectively results in a reduced system load, which produces 30-fold reduction in queueing overhead compared to Power-of-Two at medium to high load. An\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "364\n", "authors": ["35"]}
{"title": "Detecting conflicts between structure accesses\n", "abstract": " Two references to a record structure conflict if they access the same field and at least one modifies the location. Because structures can be connected by pointers, deciding if two statements conflict requires knowledge of the possible aliases for the locations that they access. This paper describes a dataflow computation that produces a conservative description of the aliases visible at any point in a program. The data structure that records aliases is an alias graph. It also labels instances of structures so that the objects referenced at different points in a program can be compared. This paper shows how alias graphs can be used to detect potential conflicts.", "num_citations": "359\n", "authors": ["35"]}
{"title": "Cache-conscious structure definition\n", "abstract": " A program's cache performance can be improved by changing the organization and layout of its data---even complex, pointer-based data structures. Previous techniques improved the cache performance of these structures by arranging distinct instances to increase reference locality. These techniques produced significant performance improvements, but worked best for small structures that could be packed into a cache block. This paper extends that work by concentrating on the internal organization of fields in a data structure. It describes two techniques---structure splitting and field reordering---that improve the cache behavior of structures larger than a cache block. For structures comparable in size to a cache block, structure splitting can increase the number of hot fields that can be placed in a cache block. In five Java programs, structure splitting reduced cache miss rates 10--27% and improved performance 6\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "335\n", "authors": ["35"]}
{"title": "Hardware architecture for cloud services\n", "abstract": " The claimed subject matter provides systems and/or methods that facilitate dynamically allocating resources (eg, hardware, software,...) supported by a third party service provider. The third party service provider can support any number of services that can be concurrently requested by several clients without user perception of degraded computing performance as compared to conventional systems/techniques due to improved connectivity and mitigated latencies. An interface component can receive a request from a client device. Further, a dynamic allocation component can apportion resources (eg, hardware resources) supported by the third party service provider to process and respond to the request based at least in part upon subscription data. Moreover, a user state evaluator can determine a state associated with a user and/or the client device; the state can be utilized by the dynamic allocation component to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "328\n", "authors": ["35"]}
{"title": "Rewriting executable files to measure program behavior\n", "abstract": " Inserting instrumentation code in a program is an effective technique for detecting, recording, and measuring many aspects of a program's performance. Instrumentation code can be added at any stage of the compilation process by specially\u0393\u00c7\u00c9modified system tools such as a compiler or linker or by new tools from a measurement system. For several reasons, adding instrumentation code after the compilation process\u0393\u00c7\u00f6by rewriting the executable file\u0393\u00c7\u00f6presents fewer complications and leads to more complete measurements. This paper describes the difficulties in adding code to executable files that arose in developing the profiling and tracing tools qp and qpt. The techniques used by these tools to instrument programs on MIPS and SPARC processors are applicable in other instrumentation systems running on many processors and operating systems. In addition, many difficulties could have been avoided with minor\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "278\n", "authors": ["35"]}
{"title": "Marketplace for cloud services resources\n", "abstract": " The claimed subject matter provides systems and/or methods that facilitate dynamically allocating resources (eg, hardware, software,...) supported by a third party service provider. The third party service provider can support any number of services that can be concurrently requested by several clients without user perception of degraded computing performance as compared to conventional systems/techniques due to improved connectivity and mitigated latencies. An interface component can receive a request from a client device. Further, a dynamic allocation component can apportion resources (eg, hardware resources) supported by the third party service provider to process and respond to the request based at least in part upon subscription data. Moreover, a user state evaluator can determine a state associated with a user and/or the client device; the state can be utilized by the dynamic allocation component to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "276\n", "authors": ["35"]}
{"title": "Static branch frequency and program profile analysis\n", "abstract": " Program profiles identify frequently executed portions of a program, which are the places at which optimizations offer programmers and compilers the greatest benefit. Compilers, however, infrequently exploit program profiles, because profiling a program requires a programmer to instrument and run the program. An attractive alternative is for the compiler to statically estimate program profiles. This paper presents several new techniques for static branch prediction and profiling. The first technique combines multiple predictions of a branch's outcome into a prediction of the probability that the branch is taken. Another technique uses these predictions to estimate the relative execution frequency (ie, profile) of basic blocks and control-flow edges within a procedure. A third algorithm uses local frequency estimates to predict the global frequency of calls, procedure invocations, and basic block and control-flow edge\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "260\n", "authors": ["35"]}
{"title": "Efficient program tracing\n", "abstract": " A program trace lists the addresses of instructions executed and data referenced during a program's execution. Earlier approaches to collecting program traces, including abstract execution and optimal control tracing, are reviewed. Two tracing systems based on these techniques are presented. Results collected when using the later systems on several programs show significant reductions in the cost of collecting traces. Reduction in trace file sizes are also significant.< >", "num_citations": "248\n", "authors": ["35"]}
{"title": "Cooperative shared memory: Software and hardware for scalable multiprocessors\n", "abstract": " We believe the paucity of massively parallel, shared-memory machines follows from the lack of a shared-memory programming performance model that can inform programmers of the cost of operations (so they can avoid expensive ones) and can tell hardware designers which cases are common (so they can build simple hardware to optimize them). Cooperative shared memory, our approach to shared-memory design, addresses this problem. Our initial implementation of cooperative shared memory uses a simple programming model, called Check-In/Check-Out (CICO), in conjunction with even simpler hardware, called Dir1SW. In CICO, programs bracket uses of shared data with a check_in directive terminating the expected use of the   data. A cooperative prefetch directive helps hide communication latency. Dir1SW is a minimal directory protocol that adds little complexity to message-passing hardware, but\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "233\n", "authors": ["35"]}
{"title": "Wisconsin Wind Tunnel II: a fast, portable parallel architecture simulator\n", "abstract": " To analyze new parallel computers, developers must rapidly simulate designs running realistic workloads. Historically, direct execution and a parallel host have accelerated simulations, although these techniques have typically lacked portability. Through four key operations, the Wisconsin Wind Tunnel II can easily run simulations on Sparc platforms ranging from a workstation cluster to an asymmetric multiprocessor.", "num_citations": "226\n", "authors": ["35"]}
{"title": "Application-specific protocols for user-level shared memory\n", "abstract": " Recent distributed shared memory (DSM) systems and proposed shared-memory machines have implemented some or all of their cache coherence protocols in software. One way to exploit the flexibility of this software is to tailor a coherence protocol to match an application's communication patterns and memory semantics. This paper presents evidence that this approach can lead to large performance improvements. It shows that application-specific protocols substantially improved the performance of three application programs-appbt, em3d, and barnes-over carefully tuned transparent shared memory implementations. The speed-ups were obtained on Blizzard, a fine-grained DSM system running on a 32-node Thinking Machines CM-5.< >", "num_citations": "220\n", "authors": ["35"]}
{"title": "Design decisions in SPUR\n", "abstract": " First Page of the Article", "num_citations": "216\n", "authors": ["35"]}
{"title": "Abstract execution: A technique for efficiently tracing programs\n", "abstract": " Many areas of computer performance analysis require detailed traces of events that occur during a program's execution. Collecting traces is expensive. The additional code required to record events greatly slows a program's execution. In addition, the resulting trace files can grow unmanageably large. This paper describes a technique called abstract execution that alleviates both problems. Abstract execution records a small set of events during the traced program's execution. These events serve as input to an abstract version of the program that generates a full trace by re\u0393\u00c7\u00c9executing selected portions of the original program. This process greatly reduces both the cost of tracing the original program and the size of the trace files. The cost of regenerating a trace is insignificant in comparison to the cost of applications that use it. This paper also describes a system called AE that implements Abstract Execution. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "213\n", "authors": ["35"]}
{"title": "Using generational garbage collection to implement cache-conscious data placement\n", "abstract": " The cost of accessing main memory is increasing. Machine designers have tried to mitigate the consequences of the processor and memory technology trends underlying this increasing gap with a variety of techniques to reduce or tolerate memory latency. These techniques, unfortunately, are only occasionally successful for pointer-manipulating programs. Recent research has demonstrated the value of a complementary approach, in which pointer-based data structures are reorganized to improve cache locality.This paper studies a technique for using a generational garbage collector to reorganize data structures to produce a cache-conscious data layout, in which objects with high temporal affinity are placed next to each other, so that they are likely to reside in the same cache block. The paper explains how to collect, with low overhead, real-time profiling information about data access patterns in object-oriented\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "209\n", "authors": ["35"]}
{"title": "Improving data-flow analysis with path profiles\n", "abstract": " Data-flow analysis computes its solutions over the paths in a control-flow graph. These paths---whether feasible or infeasible, heavily or rarely executed---contribute equally to a solution. However, programs execute only a small fraction of their potential paths and, moreover, programs' execution time and cost is concentrated in a far smaller subset of hot paths. This paper describes a new approach to analyzing and optimizing programs, which improves the precision of data flow analysis along hot paths. Our technique identifies and duplicates hot paths, creating a hot path graph in which these paths are isolated. After flow analysis, the graph is reduced to eliminate unnecessary duplicates of unprofitable paths. In experiments on SPEC95 benchmarks, path qualification identified 2--112 times more non-local constants (weighted dynamically) than the Wegman-Zadek conditional constant algorithm, which translated into\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "203\n", "authors": ["35"]}
{"title": "Using cohort scheduling to enhance server performance\n", "abstract": " A server is a program that manages access to a shared resource, such as a database, mail store, file system, web site, etc. A server receives a stream of requests, processes each, and produces a stream of results. These systems make information and resources available to remote clients and facilitate concurrent access to and sharing of data and resources. Server performance is important, as it determines the latency to access a resource and constrains a server\u0393\u00c7\u00d6s ability to handle multiple requests. Commercial servers, such as databases, are the focus of considerable research to improve the underlying hardware, algorithms and data structures, and efficiency of server code.Much of this effort focuses on systems\u0393\u00c7\u00d6 memory hierarchy, which until recently meant disk accesses, but now includes processor caches. This shift reflects the deleterious effects of the expanding processor-memory gap, which is caused by\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "197\n", "authors": ["35"]}
{"title": "Resource standardization in an off-premise environment\n", "abstract": " A computing paradigm where information can be aggregated from multiple services/programs within a \u0393\u00c7\u00ffcloud-based\u0393\u00c7\u00d6environment is provided. Thus, the system can provide a uniform interface that can combine computational tasks across the multiple services/programs. Thus, the innovation takes advantage of the computing device being a \u0393\u00c7\u00ffthin client\u0393\u00c7\u00d6which affords greater user comfort to a user without sacrificing data processing capabilities. Accordingly, the mechanisms are disclosed that standardize and/or normalize data across the resources within the cloud.", "num_citations": "176\n", "authors": ["35"]}
{"title": "Transactional memory\n", "abstract": " Is TM the answer for improving parallel programming?", "num_citations": "167\n", "authors": ["35"]}
{"title": "Fast out-of-order processor simulation using memoization\n", "abstract": " Our new out-of-order processor simulatol; FastSim, uses two innovations to speed up simulation 8--15 times (vs. Wisconsin SimpleScalar) with no loss in simulation accuracy. First, FastSim uses speculative direct-execution to accelerate the functional emulation of speculatively executed program code. Second, it uses a variation on memoization---a well-known technique in programming language implementation---to cache microarchitecture states and the resulting simulator actions, and then \"fast forwards\" the simulation the next time a cached state is reached. Fast-forwarding accelerates simulation by an order of magnitude, while producing exactly the same, cycle-accurate result as conventional simulation.", "num_citations": "165\n", "authors": ["35"]}
{"title": "Orleans: cloud computing for everyone\n", "abstract": " Cloud computing is a new computing paradigm, combining diverse client devices--PCs, smartphones, sensors, single-function, and embedded--with computation and data storage in the cloud. As with every advance in computing, programming is a fundamental challenge, as the cloud is a concurrent, distributed system running on unreliable hardware and networks.", "num_citations": "164\n", "authors": ["35"]}
{"title": "Efficient support for irregular applications on distributed-memory machines\n", "abstract": " Irregular computation problems underlie many important scientific applications. Although these problems are computationally expensive, and so would seem appropriate for parallel machines, their irregular and unpredictable run-time behavior makes this type of parallel program difficult to write and adversely affects run-time performance.", "num_citations": "163\n", "authors": ["35"]}
{"title": "Rights management in a cloud\n", "abstract": " Innovative aspects provided herein pertain to digital rights management (DRM) and/or enforcement in conjunction with remote network clouds and services. Digital rights management licenses/rights/policies can be applied to personal files to facilitate worry free remote storage and/or file sharing. These rights can be identity-centric rather than machine centric, thereby facilitating access and usage from any network device anywhere. Various mechanisms are also disclosed to deter assorted uses of content and/or encourage rights acquisition as an alterative or in addition to technologically prohibitive means. Additionally, a system and method are provided that can afford a frictionless marketplace for file distribution, wherein content is protected and freely distributed and identity-centric rights can be purchased to access the content.", "num_citations": "148\n", "authors": ["35"]}
{"title": "Data structure partitioning with garbage collection to optimize cache utilization\n", "abstract": " Fields which are individually addressable data elements in data structures are reordered to improve the efficiency of cache line access. Temporal data regarding the referencing of such fields is obtained, and a tool is used to construct a field affinity graph of temporal access affinities between the fields. Nodes in the graph represent fields, and edges between the nodes are weighted to indicate field affinity. A first pass greedy algorithm attempts to combine high affinity fields in the same cache line or block. Constraints are used to reject invalid combinations of fields. Data structures such as class are partitioned into heavily referenced and less heavily referenced portions. The partitioning is based on profile information about field access counts with indirect addressing used to reference the less heavily referenced partitioned class. A class co-location scheme is used to ensure that temporally correlated classes are\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "143\n", "authors": ["35"]}
{"title": "Making pointer-based data structures cache conscious\n", "abstract": " To narrow the widening gap between processor and memory performance, the authors propose improving the cache locality of pointer-manipulating programs and bolstering performance by careful placement of structure elements. It is concluded that considering past trends and future technology, it seems clear that the processor-memory performance gap will continue to increase and software will continue to grow larger and more complex. Although cache-conscious algorithms and data structures are the first and perhaps best place to attack this performance problem, the complexity of software design and an increasing tendency to build large software systems by assembling smaller components does not favor a focused, integrated approach. We propose another, more incremental approach of cache-conscious data layout, which uses techniques such as clustering, coloring, and compression to enhance data\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "127\n", "authors": ["35"]}
{"title": "Spim s20: A mips r2000 simulator\n", "abstract": " SPIM S20 is a simulator that runs programs for the MIPS R2000/R3000 RISC computers.\" SPIM can read and immediately execute files containing assembly language or MIPS executable files. SPIM is a self-contained system for running these programs and contains a debugger and interface to a few operating system services.The architecture of the MIPS computers is simple and regular, which makes it easy to learn and understand. The processor contains 32 general-purpose registers and a well-designed instruction set that make it a propitious target for generating code in a compiler. However, the obvious question is: why use a simulator when many people have workstations that contain a hardware, and hence significantly faster, implementation of this computer? One reason is that these workstations are not available to most undergraduates since they are used for research. Another reason is that these machine\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "119\n", "authors": ["35"]}
{"title": "Cache metadata for implementing bounded transactional memory\n", "abstract": " Various technologies and techniques are disclosed for providing a bounded transactional memory application that accesses cache metadata in a cache of a central processing unit. When performing a transactional read from the bounded transactional memory application, a cache line metadata transaction-read bit is set. When performing a transactional write from the bounded transactional memory application, a cache line metadata transaction-write bit is set and a conditional store is performed. At commit time, if any lines marked with the transaction-read bit or the transaction-write bit were evicted or invalidated, all speculatively written lines are discarded. The application can also interrogate a cache line metadata eviction summary to determine whether a transaction is doomed and then take an appropriate action.", "num_citations": "116\n", "authors": ["35"]}
{"title": "Remote provisioning of information technology\n", "abstract": " Remote provisioning of an IT network and/or associated services is provided. Hardware, software, service and/or expertise can be moved from on-premise to a remote location (eg, central, distributed...). Accordingly, at least a large degree computation can be moved to the center to exploit economies of scale, among other things. In such an architecture, computational resources (eg, data storage, computation power, cache...) can be pooled, and entities can subscribe to a particular level of resources related to a private entity IT network.", "num_citations": "115\n", "authors": ["35"]}
{"title": "A reconfigurable fabric for accelerating large-scale datacenter services\n", "abstract": " To advance datacenter capabilities beyond what commodity server designs can provide, the authors designed and built a composable, reconfigurable fabric to accelerate large-scale software services. Each instantiation of the fabric consists of a 6 x 8 2D torus of high-end field-programmable gate arrays (FPGAs) embedded into a half-rack of 48 servers. The authors deployed the reconfigurable fabric in a bed of 1,632 servers and FPGAs in a production datacenter and successfully used it to accelerate the ranking portion of the Bing Web search engine by nearly a factor of two.", "num_citations": "110\n", "authors": ["35"]}
{"title": "Spending Moore's dividend\n", "abstract": " Multicore computers shift the burden of software performance from chip designers and processor architects to software developers.", "num_citations": "109\n", "authors": ["35"]}
{"title": "Recommendation system that identifies a valuable user action by mining data supplied by a plurality of users to find a correlation that suggests one or more actions for\u252c\u00e1\u0393\u00c7\u00aa\n", "abstract": " Personal data mining mechanisms and methods are employed to identify relevant information that otherwise would likely remain undiscovered. Users supply personal data that can be analyzed in conjunction with data associated with a plurality of other users to provide useful information that can improve business operations and/or quality of life. Personal data can be mined alone or in conjunction with third party data to identify correlations amongst the data and associated users. Applications or services can interact with such data and present it to users in a myriad of manners, for instance as notifications of opportunities.", "num_citations": "106\n", "authors": ["35"]}
{"title": "Whole program path profiling\n", "abstract": " A program is instrumented to record acyclic paths during execution of the program. A whole program path is produced from the record and provides a complete compact record of a program's entire control flow. It includes a record of crossing loop boundaries and procedure boundaries to provide a complete picture of the program's dynamic behavior. A string compression algorithm that constructs a context-free grammar is used to compress the path trace and uncover its regular structure. Heavily executed subpaths are easily identified from the representation by traversing the whole program path to find hot subpaths according to input parameters of minimum and maximum path lengths and a minimum cost.", "num_citations": "105\n", "authors": ["35"]}
{"title": "C**: A large-grain, object-oriented, data-parallel programming language\n", "abstract": " C** is a new data-parallel programming language based on a new computation model called largegrain data parallelism. C** overcomes many disadvantages of existing data-parallel languages, yet retains their distinctive and advantageous programming style and deterministic behavior. This style makes data parallelism well-suited for massively-parallel computation. Large-grain data parallelism enhances data parallelism by permitting a wider range of algorithms to be expressed naturally.             C** is an object-oriented programming language that inherits data abstraction features from C++. Existing scientific programming languages do not provide modern programming facilities such as operator extensibility, abstract datatypes, or object-oriented programming. C**- and its sequential subset C++-support modern programming practices and enable a single language to be used for all parts of large, complex\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "103\n", "authors": ["35"]}
{"title": "Sealing OS processes to improve dependability and safety\n", "abstract": " In most modern operating systems, a process is a hardware-protected abstraction for isolating code and data. This protection, however, is selective. Many common mechanisms---dynamic code loading, run-time code generation, shared memory, and intrusive system APIs---make the barrier between processes very permeable. This paper argues that this traditional open process architecture exacerbates the dependability and security weaknesses of modern systems. As a remedy, this paper proposes a sealed process architecture, which prohibits dynamic code loading, self-modifying code, shared memory, and limits the scope of the process API. This paper describes the implementation of the sealed process architecture in the Singularity operating system, discusses its merits and drawbacks, and evaluates its effectiveness. Some benefits of this sealed process architecture are: improved program analysis by tools\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "97\n", "authors": ["35"]}
{"title": "Managing memory faults\n", "abstract": " Embodiments are described for managing memory faults. An example system can include a memory controller module to manage memory cells and report memory faults. An error buffer module can store memory fault information received from the memory controller. A notification module can be in communication with the error buffer module. The notification module may generate a notification of a memory fault in a memory access operation. A system software module can provide services and manage executing programs on a processor. In addition, the system software module can receive the notifications of the memory fault for the memory access operation. A notification handler may be activated by an interrupt when the notification of the memory fault in the memory access operation is received.", "num_citations": "96\n", "authors": ["35"]}
{"title": "Deconstructing process isolation\n", "abstract": " Most operating systems enforce process isolation through hardware protection mechanisms such as memory segmentation, page mapping, and differentiated user and kernel instructions. Singularity is a new operating system that uses software mechanisms to enforce process isolation. A software isolated process (SIP) is a process whose boundaries are established by language safety rules and enforced by static type checking. SIPs provide a low cost isolation mechanism that provides failure isolation and fast inter-process communication. To compare the performance of Singularity's SIPs against traditional isolation techniques, we implemented an optional hardware isolation mechanism. Protection domains are hardware-enforced address spaces, which can contain one or more SIPs. Domains can either run at the kernel's privilege level or be fully isolated from the kernel and run at the normal application privilege\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "96\n", "authors": ["35"]}
{"title": "Restructuring symbolic programs for concurrent execution on multiprocessors\n", "abstract": " CURARE, the program restructurer described in this dissertation, automatically transforms a sequential Lisp program into an equivalent concurrent program that executes on a multiprocessor. CURARE first analyzes a program to find its control and data dependences. CURARE uses a new data-dependence algorithm, which finds and classifies these dependences. Dependences constrain the programs concurrent execution because, in general, two conflicting statements cannot execute in a different order without affecting the programs result. A restructerer must know all dependences in order to preserve them. However, not all dependences are essential to produce the programs result. CURARE attempts to transform the program so it computes its result with fewer conflicts. CURARE then examines loops in a program to find those that are unconstrained or lightly constrained by dependences. Loops that are suitable for concurrent execution are changed to execute on a set of concurrent server processes. These servers execute single loop iterations and therefore need to be extremely inexpensive to invoke. Restructured programs execute significantly faster than the original sequential programs. This improvement is large enough to attract programmers to a multiprocessor, particularly since it requires little effort on their part. Although restructured programs may not make optimal use of a multiprocessors parallelism, they make good use of a programmers time.Descriptors:", "num_citations": "94\n", "authors": ["35"]}
{"title": "Evaluation of the SPUR Lisp architecture\n", "abstract": " The SPUR microprocessor has a 40-bit tagged architecture designed to improve its performance for Lisp programs. Although SPUR includes just a small set of enhancements to the Berkeley RISC-II architecture, simulation results show that with a 150-ns cycle time SPUR will run Common Lisp programs at least as fast as a Symbolies 3600 or a DEC VAX 8600. This paper explains SPUR's instruction set architecture and provides measurements of how certain components of the architecture perform.", "num_citations": "94\n", "authors": ["35"]}
{"title": "Zeta: Scheduling interactive services with partial execution\n", "abstract": " This paper presents a scheduling model for a class of interactive services in which requests are time bounded and lower result quality can be traded for shorter execution time. These applications include web search engines, finance servers, and other interactive, on-line services. We develop an efficient scheduling algorithm, Zeta, that allocates processor time among service requests to maximize the quality and minimize the variance of the response.", "num_citations": "83\n", "authors": ["35"]}
{"title": "Teapot: Language support for writing memory coherence protocols\n", "abstract": " Recent shared-memory parallel computer systems offer the exciting possibility of customizing memory coherence protocols to fit an application's semantics and sharing patterns. Custom protocols have been used to achieve message-passing performance---while retaining the convenient programming model of a global address space---and to implement high-level language constructs. Unfortunately, coherence protocols written in a conventional language such as C are difficult to write, debug, understand, or modify. This paper describes Teapot, a small, domain-specific language for writing coherence protocols. Teapot uses continuations to help reduce the complexity of writing protocols. Simple static analysis in the Teapot compiler eliminates much of the overhead of continuations and results in protocols that run nearly as fast as hand-written C code. A Teapot specification can be compiled both to an executable\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "82\n", "authors": ["35"]}
{"title": "Imagining the future: Thoughts on computing\n", "abstract": " New and compelling ideas are transforming the future of computing, bringing about a plethora of changes that have significant implications for our profession and our society and raising some profound technical questions. This Web extra video interview features Dan Reed of Microsoft giving us a sense of how new cloud architectures and cloud capabilities will begin to move computer science education, research, and thinking in whole new directions.", "num_citations": "79\n", "authors": ["35"]}
{"title": "Spim: A mips32 simulator\n", "abstract": " Microsoft Research Formerly: Professor, Computer Sciences Department, University of Wisconsin-Madison spim is a self-contained simulator that will run MIPS32 assembly language programs. It reads and executes assembly language programs written for this processor. spim also provides a simple debugger and minimal set of operating system services. spim does not execute binary (compiled) programs. spim implements almost the entire MIPS32 assembler-extended instruction set.(It omits most floating point comparisons and rounding modes and the memory system page tables.) The MIPS architecture has several variants that differ in various ways (eg, the MIPS64 architecture supports 64-bit integers and addresses), which means that spim will not run programs compiled for all types of MIPS processors. MIPS compilers also generate a number of assembler directives that spim cannot process. These directives\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "76\n", "authors": ["35"]}
{"title": "Facile: A language and compiler for high-performance processor simulators\n", "abstract": " Architectural simulators are essential tools for computer architecture and systems research and development. Simulators, however, are becoming frustratingly slow, because they must now model increasingly complex micro-architectures running realistic workloads. Previously, we developed a technique called fast-forwarding, which applied partial evaluation and mermoization to improve the performance of detailed architectural simulations by as much as an order of magnitude [14]. While writing a detailed processor simulator is difficult, implementing fast-forwarding is even more complex. This paper describes Facile, a domain-specific language for writing detailed, accurate micro-architecture simulators. Architectural descriptions written in Facile can be compiled, using partial evaluation techniques, into fast-forwarding simulators that achieve significant performance improvements with far less programmer effort\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "71\n", "authors": ["35"]}
{"title": "Loop-level parallelism in numeric and symbolic programs\n", "abstract": " A new technique for estimating and understanding the speed improvement that can result from executing a program on a parallel computer is described. The technique requires no additional programming and minimal effort by a program's author. The analysis begins by tracing a sequential program. A parallelism analyzer uses information from the trace to simulate parallel execution of the program. In addition to predicting parallel performance, the parallelism analyzer measures many aspects of a program's dynamic behavior. Measurements of six substantial programs are presented. These results indicate that the three symbolic programs differ substantially from the numeric programs and, as a consequence, cannot be automatically parallelized with the same compilation techniques.< >", "num_citations": "70\n", "authors": ["35"]}
{"title": "SIMD parallelization of applications that traverse irregular data structures\n", "abstract": " Fine-grained data parallelism is increasingly common in mainstream processors in the form of longer vectors and on-chip GPUs. This paper develops support for exploiting such data parallelism for a class of non-numeric, non-graphic applications, which perform computations while traversing many independent, irregular data structures. While the traversal of any one irregular data structure does not give opportunity for parallelization, traversing a set of these does. However, mapping such parallelism to SIMD units is nontrivial and not addressed in prior work. We address this problem by developing an intermediate language for specifying such traversals, followed by a run-time scheduler that maps traversals to SIMD units. A key idea in our run-time scheme is converting branches to arithmetic operations, which then allows us to use SIMD hardware. In order to make our approach fast, we demonstrate several\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "68\n", "authors": ["35"]}
{"title": "A concurrent copying garbage collector for languages that distinguish (im) mutable data\n", "abstract": " This paper describes the design and implementation of a concurrent compacting garbage collector for languages that distinguish mutable data from immutable data (eg, ML) as well for languages that manipulate only immutable data (eg, pure functional languages such as Haskell). The collector runs on shared-memory parallel computers and requires minimal mutator/collector synchronization. No special hardware or operating system support is required.", "num_citations": "68\n", "authors": ["35"]}
{"title": "Register allocation in the SPUR Lisp compiler\n", "abstract": " Register allocation is an important component of most compilers, particularly those for RISC machines. The SPUR Lisp compiler uses a sophisticated, graph-coloring algorithm developed by Fredrick Chow [Chow84]. This paper describes the algorithm and the techniques used to implement it efficiently and evaluates its performance on several large programs. The allocator successfully assigned most temporaries and local variables to registers in a wide variety of functions. Its execution cost is moderate.", "num_citations": "68\n", "authors": ["35"]}
{"title": "Mechanisms for cooperative shared memory\n", "abstract": " This paper explores the complexity of implementing directory protocols by examining their mechanisms primitive operations on directories, caches, and network interfaces. We compare the following protocols: Dir1B, Dir4B, Dir4NB, DirnNB[2], Dir1SW[9] and an improved version of Dir1SW (Dir1SW+). The comparison shows that the mechanisms and mechanism sequencing of Dir1SW and Dir1SW+ are simpler than those for other protocols.   We also compare protocol performance by running eight benchmarks on 32 processor systems. Simulations show that Dir1SW+s performance is comparable to more complex directory protocols. The significant disparity in hardware complexity and the small difference in performance argue that Dir1SW+ may be a more effective use of resources. The small performance difference is attributable to two factors: the low degree of sharing in the benchmarks and Check- In/Check-Out\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "67\n", "authors": ["35"]}
{"title": "Data structure partitioning to optimize cache utilization\n", "abstract": " Fields which are individually addressable data elements in data structures are reordered to improve the efficiency of cache line access. Temporal data regarding the referencing of such fields is obtained, and a tool is used to construct a field affinity graph of temporal access affinities between the fields. Nodes in the graph represent fields, and edges between the nodes are weighted to indicate field affinity. A first pass greedy algorithm attempts to combine high affinity fields in the same cache line or block. Constraints are used to reject invalid combinations of fields. Data structures such as class are partitioned into heavily referenced and less heavily referenced portions. The partitioning is based on profile information about field access counts with indirect addressing used to reference the less heavily referenced partitioned class. A class co-location scheme is used to ensure that temporally correlated classes are\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "66\n", "authors": ["35"]}
{"title": "Self-describing artifacts and application abstractions\n", "abstract": " Described herein is at least one implementation employing multiple self-describing software artifacts persisted on one or more computer-storage media of a software-based computer. In this implementation, each artifact is representative of at least part of the software components (eg, load modules, processes, applications, and operating system components) of the computing system and each artifact is described by at least one associated \u0393\u00c7\u00a3manifest,\u0393\u00c7\u00a5 which include metadata declarative descriptions of the associated artifact.", "num_citations": "64\n", "authors": ["35"]}
{"title": "LCM: Memory system support for parallel language implementation\n", "abstract": " Higher-level parallel programming languages can be difficult to implement efficiently on parallel machines. This paper shows how a flexible, compiler-controlled memory system can help achieve good performance for language constructs that previously appeared too costly to be practical.", "num_citations": "62\n", "authors": ["35"]}
{"title": "SPUR: a VLSI multiprocessor workstation\n", "abstract": " SPUR (Symbolic Processing Using RISCs) is a workstation for conducting parallel processing research. SPUR contains 6 to 12 high-performance homogeneous processors connected with a shared bus. The number of processors is large enough to permit parallel processing experiments, but small enough to allow packaging as a personal workstation. The restricted processor count also allows us to build powerful RISC processors, which include support for Lisp and IEEE floating-point, at reasonable cost. This paper presents a specification of SPUR and the results of some early architectural experiments. SPUR features include a large virtually-tagged cache, address translation without a translation buffer, LISP support with datatype tags but without microcode, multiple cache consistency in hardware, and an IEEE floating-point coprocessor without microcode.", "num_citations": "61\n", "authors": ["35"]}
{"title": "Using paths to measure, explain, and enhance program behavior\n", "abstract": " What happens when a computer program runs? The answer can be frustratingly elusive, as anyone who has debugged or tuned a program knows. As it runs, a program overwrites its previous state, which might have provided a clue as to how the program got to the point at which it computed the wrong answer or otherwise failed. This all-too-common experience is symptomatic of a more general problem: the difficulty of accurately and efficiently capturing and analyzing the sequence of events that occur when a program executes. Program paths offer an insight into a program's dynamic behavior that is difficult to achieve any other way. Unlike simpler measures such as program profiles, which aggregate information to reduce the cost of collecting or storing data, paths capture some of the usually invisible dynamic sequencing of statements. The article exploits the insight that program statements do not execute in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "60\n", "authors": ["35"]}
{"title": "Cache metadata identifiers for isolation and sharing\n", "abstract": " Various technologies and techniques are disclosed for providing software accessible metadata on a cache of a central processing unit. A multiprocessor has at least one central processing unit. The central processing unit has a cache with cache lines that are augmented by cache metadata. The cache metadata includes software-controlled metadata identifiers that allow multiple logical processors to share the cache metadata. The metadata identifiers and cache metadata can then be used to accelerate various operations. For example, parallel computations can be accelerated using cache metadata and metadata identifiers. As another example, nested computations can be accelerated using metadata identifiers and cache metadata. As yet another example, transactional memory applications that include parallelism within transactions or that include nested transactions can be also accelerated using cache\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "58\n", "authors": ["35"]}
{"title": "Protocol-based data-race detection\n", "abstract": " Distributed Shared-Memory (DSM) computers, which partition physical memory among a collection of workstationlike computing nodes, are now a common way to implement parallel machines. Recently, there has been much interest in DSM machines that use software, instead of hardware, to implement coherence protocols to manage data replication and cache coherence. Software offers many advantages, not the least of which is the possibility of adding significant functionality-such as race detection-to a protocol. This paper describes a new, transparent, protocol-based technique for automatically detecting data races on-the-fly. An implementation of this approach in a DSM system running on a Thinking Machines CM-5 found data races in two of a set of five shared-memory benchmarks. Monitored applications had slowdowns ranging from O-3 on 32 nodes.", "num_citations": "55\n", "authors": ["35"]}
{"title": "Tempest: A substrate for portable parallel programs\n", "abstract": " The paper describes Tempest, a collection of mechanisms for communication and synchronization in parallel programs. With these mechanisms, authors of compilers, libraries, and application programs can exploit-across a wide range of hardware platforms-the best of shared memory, message passing, and hybrid combinations of the two. Because Tempest provides mechanisms, not policies, programmers can tailor communication to a program's sharing pattern and semantics, rather than restructuring the program to run with the limited communication options offered by existing parallel machines. And since the mechanisms are easily supported on different machines. Tempest provides a portable interface across platforms. The paper describes the Tempest mechanisms, briefly explains how they are used, outlines several implementations on both custom and stock hardware, and presents preliminary performance\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "55\n", "authors": ["35"]}
{"title": "Method and system for performing a task on a computer\n", "abstract": " A method and system for performing a task on a computer is provided, in which the procedure is organized into multiple stages. Each stage of the task has an associated sub-task. Requests for the procedure are represented by \u0393\u00c7\u00a3work packets\u0393\u00c7\u00a5 that stored in a holding area at each stage, such as a stack or a queue, until it is advantageous for a processor to execute them. Each work packet contains data and/or instructions for performing the sub-task of the stage. When a processor is available, it finds a stage having unexecuted work packets and executes a batch of work packets by repeatedly performing the sub-task of the stage. This repeated execution of a sub-task allows a processor to maximize its native time-saving mechanisms, such as cache. The invention may advantageously be used as an alternative to conventional thread-based programming.", "num_citations": "54\n", "authors": ["35"]}
{"title": "Broad new OS research: challenges and opportunities.\n", "abstract": " Contemporary software systems are beset by problems that create challenges and opportunities for broad new OS research. To illustrate, we describe five areas where broad OS research could significantly improve the current user experience. These areas are dependability, security, system configuration, system extension, and multi-processor programming. In each area we explore how contemporary systems fall short. Where we have thought of possible solutions, we offer directions for future research. Finally, we describe Singularity, a research project at Microsoft Research that is building a new operating system to explore four of these challenges. Singularity incorporates three specific design decisions in order to increase system dependability and improve system security, configuration, and extension. These design decisions include the adoption of an abstract instruction set as part of the system binary interface, a unified extension architecture for both the OS and applications, and a first-class application abstraction.", "num_citations": "54\n", "authors": ["35"]}
{"title": "Transformations for virtual guest representation\n", "abstract": " Systems and methods that provide for a virtual reality entertainment system that supplies immersive entertainment and creates a sensation for a user similar to having guests in a remote location to be physically present as virtual guests. Such virtual reality entertainment system can supply a graphic and/or audio; wherein interconnected computers, video and audio processing devices, supply a live interaction between a user and a guest (s). Although guests are only present virtually (eg, electronically present with other objects/user within the environment) such virtual invitation enables a user and guests to concurrently experience the entertainment together (eg, a live sporting event, spectator game). In a related aspect, the subject innovation can implement holographic avatars, and a plurality of communication interfaces, to imitate (and/or transform) a relationship between the user and the virtual guests/surrounding\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "52\n", "authors": ["35"]}
{"title": "Teapot: A domain-specific language for writing cache coherence protocols\n", "abstract": " In this paper, we describe Teapot, a domain-specific language for writing cache coherence protocols. Cache coherence is of concern when parallel and distributed systems make local replicas of shared data to improve scalability and performance. In both distributed shared memory systems and distributed file systems, a coherence protocol maintains agreement among the replicated copies as the underlying data are modified by programs running on the system. Cache coherence protocols are notoriously difficult to implement, debug, and maintain. Moreover, protocols are not off-the-shelf, reusable components, because their details depend on the requirements of the system under consideration. The complexity of engineering coherence protocols can discourage users from experimenting with new, potentially more efficient protocols. We have designed and implemented Teapot, a domain-specific language that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "52\n", "authors": ["35"]}
{"title": "Reflective program generation with patterns\n", "abstract": " Runtime reflection facilities, as present in Java and .NET, are powerful mechanisms for inspecting existing code and metadata, as well as generating new code and metadata on the fly. Such power does come at a high price though. The runtime reflection support in Java and .NET imposes a cost on all programs, whether they use reflection or not, simply by the necessity of keeping all metadata around and the inability to optimize code because of future possible code changes. A second---often overlooked---cost is the difficulty of writing correct reflection code to inspect or emit new metadata and code and the risk that the emitted code is not well-formed. In this paper we examine a subclass of problems that can be addressed using a simpler mechanism than runtime reflection, which we call compile-time reflection. We argue for a high-level construct called a transform that allows programmers to write inspection and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "51\n", "authors": ["35"]}
{"title": "Data center system that accommodates episodic computation\n", "abstract": " A data center system is described which includes multiple data centers powered by multiple power sources, including any combination of renewable power sources and on-grid utility power sources. The data center system also includes a management system for managing execution of computational tasks by moving data components associated with the computational tasks within the data center system, in lieu of, or in addition to, moving power itself. The movement of data components can involve performing pre-computation or delayed computation on data components within any data center, as well as moving data components between data centers. The management system also includes a price determination module for determining prices for performing the computational tasks based on different pricing models. The data center system also includes a \u0393\u00c7\u00a3stripped down\u0393\u00c7\u00a5 architecture to complement its use in the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "50\n", "authors": ["35"]}
{"title": "Optimizing communication in HPF programs on fine-grain distributed shared memory\n", "abstract": " Unlike compiler-generated message-passing code, the coherence mechanisms in shared-memory systems work equally well for regular and irregular programs. In many programs, however compile-time information about data accesses would permit data to be transferred more efficiently---if the underlying shared-memory system offered suitable primitives. This paper demonstrates that cooperation between a compiler and a memory coherence protocol can improve the performance of High Performance Fortran (HPF) programs running on fine-grain distributed shared memory system up to a factor of 2, while retaining the versatility and portability of shared memory. As a consequence, shared memory's performance becomes competitive with message passing for regular applications, while not affecting (or in some cases, even improving) its large advantage for irregular codes. This paper describes the design of our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "50\n", "authors": ["35"]}
{"title": "Cache considerations for multiprocessor programmers\n", "abstract": " Although caches in most computers are invisible to programmers, they significantly affect program performance. This is particularly true for cache-coherent, shared-memory multiprocessors. This article presents recent research into the performance of parallel programs and its implications for programmers who may know little about caches.", "num_citations": "50\n", "authors": ["35"]}
{"title": "Migrating data to new cloud\n", "abstract": " The claimed subject matter provides a system and/or a method that facilitates preserving and maintaining data and/or services associated with a network service. The network service can be any collection of resources that are maintained by a party (eg, third-party, off-site, etc.) and accessible by an identified user over a network (eg, WAN, Internet, etc.). An interface component can receive a termination notification related to the network service. An executor component can relocate at least a portion of one of data and a service associated with the terminated network service to a disparate replacement network service in order to preserve any services and/or data related therewith.", "num_citations": "49\n", "authors": ["35"]}
{"title": "Wisconsin architectural research tool set\n", "abstract": " Figure 1.1 shows how the address space is divided into a conventional host memory space and an object memory s ace de endinl~ on the settm\u252c\u00fa of the most sil~ nificant bit (~ SB) oFthe viFtual address. For object memSry references, addresses are in objfft: o~.. etformat. The 32-bit object specifies the base of an~ illocated region in the object memory. The 32-bit offletlocates a word in the object. An ALLOC (allocation) request reserves a memory region and returns the object name. A FREE request liberates an object.", "num_citations": "49\n", "authors": ["35"]}
{"title": "Restructuring Lisp programs for concurrent execution\n", "abstract": " This paper describes the techniques that the program transformation system CURARE uses to restructure Lisp programs for concurrent execution in multiprocessor Lisp systems and discusses the problems inherent in producing concurrent programs in a flexible and dynamic programming language such as Lisp.", "num_citations": "49\n", "authors": ["35"]}
{"title": "Energy-aware server management\n", "abstract": " The described implementations relate to energy-aware server management. One implementation involves an adaptive control unit configured to manage energy usage in a server farm by transitioning individual servers between active and inactive states while maintaining response times for the server farm at a predefined level.", "num_citations": "48\n", "authors": ["35"]}
{"title": "Virtual machine for operating N-core application on M-core processor\n", "abstract": " A virtual machine is instantiated on an M-core processor, and an N-core application is instantiated on the virtual machine such that the virtual machine emulates an N-core processor to the N-core application. Thus, the virtual machine hides difference between the N cores expected by the application and the M cores available from the processor.", "num_citations": "44\n", "authors": ["35"]}
{"title": "Field reordering to optimize cache utilization\n", "abstract": " Fields which are individually addressable data elements in data structures are reordered to improve the efficiency of cache line access. Temporal data regarding the referencing of such fields is obtained, and a tool is used to construct a field affinity graph of temporal access affinities between the fields. Nodes in the graph represent fields, and edges between the nodes are weighted to indicate field affinity. A first pass greedy algorithm attempts to combine high affinity fields in the same cache line or block. Constraints are used to reject invalid combinations of fields. The constraints may be provided by program analysis, programmer, or actual dynamically generated.", "num_citations": "44\n", "authors": ["35"]}
{"title": "Implementing fine-grain distributed shared memory on commodity smp workstations\n", "abstract": " This paper reports our experience implementing the Blizzard fine-grain distributed shared memory system on a network of unmodified dual-processor workstations running a commercial operating system. The paper describes and measures: three fine-grain access control mechanisms (optimized software, commodity hardware, and custom hardware); a low-latency, user-level communication layer; kernel support in a commercial operating system; and techniques to exploit multiprocessor (SMP) nodes. The results show that a network of workstations can effectively support fine-grain shared memory, but that high performance requires either custom network hardware or custom coherence protocols.", "num_citations": "43\n", "authors": ["35"]}
{"title": "Experience with a Language for Writing Coherence Protocols.\n", "abstract": " In this paper we describe our experience with Teapot [7], a domain-specific language for writing cache coherence protocols. Cache coherence is of concern when parallel and distributed computing systems make local replicas of shared data to improve scalability and performance. In both distributed shared memory systems and distributed file systems, a coherence protocol maintains agreement among the replicated copies as the underlying data are modified by programs running on the system.Cache coherence protocols are notoriously difficult to implement, debug, and maintain. Unfortunately, protocols are not off-the-shelf items, as their details depend on the requirements of the system under consideration. This paper presents case studies detailing the successes and shortcomings of using Teapot for writing coherence protocols in two systems. The first system, loosely coherent memory (LCM)[16], implements a particular type of distributed shared memory suitable for dataparallel programming. The second system, the xFS distributed file system [9], implements a highperformance, serverless file system.", "num_citations": "39\n", "authors": ["35"]}
{"title": "State replication\n", "abstract": " The claimed subject matter provides systems and/or methods that facilitate replicating a state associated with a client, user, service, application, and the like. A third party service provider can support any number of services that can be concurrently requested by several clients without user perception of degraded computing performance as compared to conventional systems/techniques due to improved connectivity and mitigated latencies. A replication component can generate replicas of states associated with requested services. Further, the replicas can facilitate seamlessly interacting with the third party service provider (eg, while transitioning between client devices). Additionally, by providing replicas of the state related information, differing third party service providers can effectuate services based upon a request from a client without regenerating the state.", "num_citations": "37\n", "authors": ["35"]}
{"title": "A reconfigurable fabric for accelerating large-scale datacenter services\n", "abstract": " Datacenter workloads demand high computational capabilities, flexibility, power efficiency, and low cost. It is challenging to improve all of these factors simultaneously. To advance datacenter capabilities beyond what commodity server designs can provide, we designed and built a composable, reconfigurable hardware fabric based on field programmable gate arrays (FPGA). Each server in the fabric contains one FPGA, and all FPGAs within a 48-server rack are interconnected over a low-latency, high-bandwidth network. We describe a medium-scale deployment of this fabric on a bed of 1632 servers, and measure its effectiveness in accelerating the ranking component of the Bing web search engine. We describe the requirements and architecture of the system, detail the critical engineering challenges and solutions needed to make the system robust in the presence of failures, and measure the performance\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "36\n", "authors": ["35"]}
{"title": "Improving pointer-based codes through cache-conscious data placement\n", "abstract": " Processor and memory technology trends show a continual increase in the cost of accessing main memory. Machine designers have tried to mitigate the e\u2229\u00bc\u00e9ect of this trend through hardware and software prefetching, multiple levels of cache, non~ blocking caches, dynamic instruction scheduling, speculative execution, etc.These techniques, unfortunately, have only been partially successful for pointer-manipulating programs. This paper explores the complementary approach of redesigning and reorganizing data structures to improve cache locality. Pointer-based structures allow data to be placed in arbitrary locations in memory, and consequently in a cache. This freedom enables a programmer to improve performance by applying techniques such as clustering, compression, and coloring. To reduce the cost and complexity of applying these techniques, this paper also presents two semiautomatic techniques for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "36\n", "authors": ["35"]}
{"title": "Compiling for shared-memory and message-passing computers\n", "abstract": " Many parallel languages presume a shared address space in which any portion of a computation can access any datum. Some parallel computers directly support this abstraction with hardware shared memory. Other computers provide distinct (per-processor) address spaces and communication mechanisms on which software can construct a shared address space. Since programmers have difficulty explicitly managing address spaces, there is considerable interest in compiler support for shared address spaces on the widely available message-passing computers. At first glance, it might appear that hardware-implemented shared memory is unquestionably a better base on which to implement a language. This paper argues, however, that compiler-implemented shared memory, despite its short-comings,  has the potential to exploit more effectively the resources in a parallel computer. Hardware designers need to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "35\n", "authors": ["35"]}
{"title": "Sirocco: Cost-effective fine-grain distributed shared memory\n", "abstract": " Software fine-grain distributed shared memory (FGDSM) provides a simplified shared-memory programming interface with minimal or no hardware support. Originally software FGDSMs targeted uniprocessor-node parallel machines. This paper presents Sirocco, a family of software FGDSMs implemented on a network of low-cost SMPs. Sirocco takes full advantage of SMP nodes by implementing inter-node sharing directly in hardware and overlapping computation with protocol execution. To maintain correct shared-memory semantics, however SMP nodes require mechanisms to guarantee atomic coherence operations. Multiple SMP processors may also result in contention for shared resources and reduce performance. SMP nodes also impact the cost trade-off. While SMPs typically charge higher price-premiums, for a given system size SMP nodes substantially reduce networking hardware requirement as\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["35"]}
{"title": "Dynamic environment evaluation and service adjustment based on multiple user profiles including data classification and information sharing with authorized other users\n", "abstract": " An intelligent personalized agent monitors, regulates, and advises a user in decision-making processes for efficiency or safety concerns. The agent monitors an environment and present characteristics of a user and analyzes such information in view of stored preferences specific to one of multiple profiles of the user. Based on the analysis, the agent can suggest or automatically implement a solution to a given issue or problem. In addition, the agent can identify another potential issue that requires attention and suggests or implements action accordingly. Furthermore, the agent can communicate with other users or devices by providing and acquiring information to assist in future decisions. All aspects of environment observation, decision assistance, and external communication can be flexibly limited or allowed as desired by the user.", "num_citations": "31\n", "authors": ["35"]}
{"title": "Efficient logging in non-volatile memory by exploiting coherency protocols\n", "abstract": " Non-volatile memory technologies such as PCM, ReRAM and STT-RAM allow data to be saved to persistent storage significantly faster than hard drives or SSDs. Many of the use cases for non-volatile memory requires persistent logging since it enables a set of operations to execute in an atomic manner. However, a logging protocol must handle reordering, which causes a write to reach the non-volatile memory before a previous write operation.   In this paper, we show that reordering results from two parts of the system: the out-of-order execution in the CPU and the cache coherence protocol. By carefully considering the properties of these reorderings, we present a logging protocol that requires only one round trip to non-volatile memory while avoiding expensive computations, thus increasing performance. We also show how the logging protocol can be extended to building a durable set (hash map) that also\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "30\n", "authors": ["35"]}
{"title": "Assemblers, linkers, and the SPIM simulator\n", "abstract": " Encoding instructions as binary numbers is natural and efficient for computers. Humans, however, have a great deal of difficulty understanding and manipulating these numbers. People read and write symbols (words) much better than long sequences of digits. Chapter 3 showed that we need not choose between numbers and words because computer instructions can be represented in many ways. Humans can write and read symbols, and computers can execute the equivalent binary numbers. This appendix describes the process by which a human-readable program is translated into a form that a computer can execute, provides a few hints about writing assembly programs, and explains how to run these programs on SPIM, a simulator that executes MIPS programs. Unix, Windows, and DOS versions of the SPIM simulator are available through www. mkp. com/cod2e. htm.Assembly language is the symbolic representation of a computer\u0393\u00c7\u00d6s binary encoding\u0393\u00c7\u00f6machine language. Assembly language is more readable than machine language because it uses symbols instead of bits. The symbols in assembly language name commonly occurring bit patterns, such as opcodes and register specifiers, so people can read and remember them. In addition, assembly language permits programmers to use labels to identify and name particular memory words that hold instructions or data.", "num_citations": "30\n", "authors": ["35"]}
{"title": "Compiler-directed shared-memory communication for iterative parallel applications\n", "abstract": " Many scientific applications are iterative and specify repetitive communication patterns. This paper shows how a parallel-language compiler and a predictive cache-coherence protocol in a distributed shared memory system together can implement shared-memory communication efficiently for applications with unpredictable but repetitive communication patterns. The compiler uses static analysis to identify program points where potentially repetitive communication occurs. At runtime, the protocol builds a communication schedule in one iteration and uses the schedule to pre-send data in subsequent iterations. This paper contains measurements of three iterative applications (including adaptive programs with unstructured data accesses) that show that a predictive protocol increases the number of shared-data requests satisfied locally, thus reducing the remote data access latency and total execution time.", "num_citations": "30\n", "authors": ["35"]}
{"title": "CICO: A Practical Shared Memory Programming Performance Model\n", "abstract": " A programming performance model provides a programmer with feedback on the cost of program operations and is a necessary basis to write efficient programs. Many sharedmemory performance models do not accurately capture the cost of interprocessor communication caused by non-local memory references, particularly in computers with caches. This paper describes a simple and practical programming performance model-called check-in, check-out (CICO)\u0393\u00c7\u00f4for cache-coherent, shared-memory parallel computers. CICO consists of two components. The first is a collection of annotations that a programmer adds to a pro-gram to elucidate the communication arising from shared-memory references. The second is a model that calculates the communication cost of these annotations. An annotation's cost models the cost of the memory references that it summarizes and serves as a metric to compare alternative\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "30\n", "authors": ["35"]}
{"title": "Using tracing and dynamic slicing to tune compilers\n", "abstract": " Performance tuning improves a compiler's performance by detecting errors and missed opportunities in its analysis, optimization, and code generation stages. Normally, a compiler's author tunes it by examining the generated code to find suboptimal code sequences. This paper describes a collection of tools, called compiler auditors, that assist a compiler writer by partially mechanizing the process of finding suboptimal code sequences. Although these code sequences do not always exhibit compiler bugs, they frequently illustrate problems in a compiler. Experiments show that auditors effectively find suboptimal code, even in a high-quality, commercial compiler.After writing a high-quality compiler, its authors improve it with the time-consuming and tedious process of examining generated assembly code to find inefficient code sequences that could run faster or consume less space. These sequences direct a compiler\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "30\n", "authors": ["35"]}
{"title": "Object-oriented recovery for non-volatile memory\n", "abstract": " New non-volatile memory (NVM) technologies enable direct, durable storage of data in an application's heap. Durable, randomly accessible memory facilitates the construction of applications that do not lose data at system shutdown or power failure. Existing NVM programming frameworks provide mechanisms to consistently capture a running application's state. They do not, however, fully support object-oriented languages or ensure that the persistent heap is consistent with the environment when the application is restarted.   In this paper, we propose a new NVM language extension and runtime system that supports object-oriented NVM programming and avoids the pitfalls of prior approaches. At the heart of our technique is object reconstruction, which transparently restores and reconstructs a persistent object's state during program restart. It is implemented in NVMReconstruction, a Clang/LLVM extension and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["35"]}
{"title": "Data normalization\n", "abstract": " A computing paradigm where information can be aggregated from multiple services/programs within a \u0393\u00c7\u00ffcloud-based\u0393\u00c7\u00d6environment is provided. Thus, the system can provide a uniform interface that can combine computational tasks across the multiple services/programs. Thus, the innovation takes advantage of the computing device being a \u0393\u00c7\u00ffthin client\u0393\u00c7\u00d6which affords greater user comfort to a user without sacrificing data processing capabilities. Accordingly, the mechanisms are disclosed that standardize and/or normalize data across the resources within the cloud.", "num_citations": "29\n", "authors": ["35"]}
{"title": "Programs follow paths\n", "abstract": " Program paths\u0393\u00c7\u00f6sequences of executed basic blocks\u0393\u00c7\u00f6have proven to be an effective way to capture a program\u0393\u00c7\u00d6s elusive dynamic behavior. This paper shows how paths and path spectra compactly and precisely record many aspects of programs\u0393\u00c7\u00d6 execution-time control flow behavior and explores applications of these paths in computer architecture, compilers, debugging, program testing, and software maintenance.", "num_citations": "28\n", "authors": ["35"]}
{"title": "Instruction scheduling and executable editing\n", "abstract": " Modern microprocessors offer more instruction-level parallelism than most programs and compilers can currently exploit. The resulting disparity between a machine's peak and actual performance, while frustrating for computer architects and chip manufacturers, opens the exciting possibility of low-cost instrumentation for measurement, simulation, or emulation. Instrumentation code that executes in previously unused processor cycles is effectively hidden. On two superscalar SPARC processors, a simple, local scheduler hid an average of 13% of the overhead cost of profiling instrumentation in the SPECINT benchmarks and an average of 33% of the profiling cost in the SPECFP benchmarks.", "num_citations": "28\n", "authors": ["35"]}
{"title": "Cache metadata for accelerating software transactional memory\n", "abstract": " Various technologies and techniques are disclosed for providing a hardware accelerated software transactional memory application. The software transactional memory application has access to metadata in a cache of a central processing unit that can be used to improve the operation of the STM system. For example, open read barrier filtering is provided that uses an opened-for-read bit that is contained in the metadata to avoid redundant open read processing. Similarly, redundant read log validation can be avoided using the metadata. For example, upon entering commit processing for a particular transaction, a get-evictions instruction in an instruction set architecture of the central processing unit is invoked. A retry operation can be optimized using the metadata. The particular transaction is aborted at a current point and put to sleep. The corresponding cache line metadata in the metadata are marked\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["35"]}
{"title": "The singularity system\n", "abstract": " Safe, modern programming languages let Microsoft rethink the architectural trade-offs in its experimental operating system.", "num_citations": "26\n", "authors": ["35"]}
{"title": "State reflection\n", "abstract": " The claimed subject matter provides systems and/or methods that facilitate replicating a state associated with a client, user, service, application, and the like. A third party service provider can support any number of services that can be concurrently requested by several clients without user perception of degraded computing performance as compared to conventional systems/techniques due to improved connectivity and mitigated latencies. A replication component can generate replicas of states associated with requested services. Further, the replicas can facilitate seamlessly interacting with the third party service provider (eg, while transitioning between client devices). Additionally, by providing replicas of the state related information, differing third party service providers can effectuate services based upon a request from a client without regenerating the state.", "num_citations": "26\n", "authors": ["35"]}
{"title": "Employing tags for machine learning\n", "abstract": " Systems and methods that analyze aggregated tagging behavior of users, and evaluate such tagging trends to identify criteria for taxonomy applications. Initially, existence of a possible trend of tagging data based on collective user behavior is determined. Subsequently, tagging trends can be examined to identify that a predetermined convergence criteria has in fact been met, and/or establish such criteria for taxonomy applications. Machine learning systems (implicitly as well as explicitly trained) can be supplied to facilitate determining the trends and the convergence criteria.", "num_citations": "26\n", "authors": ["35"]}
{"title": "Cache-conscious data structures: design and implementation\n", "abstract": " The increasingly expensive cost of accessing memory provides an opportunity to significantly improve the performance of computer programs by redesigning their data structures to use processor memory caches more effectively. This thesis explores principles for designing cache-conscious data structures, such as clustering, coloring, and compression. These techniques improve the spatial and temporal locality of pointer-based data structures. To formalize the design process, this thesis provides an analytic framework that quantifies the performance of cache-conscious pointer structures.", "num_citations": "26\n", "authors": ["35"]}
{"title": "Multiprocessing extensions in SPUR Lisp\n", "abstract": " The authors describe their multiprocessing extensions to Common Lisp. They have added a few simple, expressive features on which one can build high-level constructs. These consist of a multithreading mechanism, primitives for communication and synchronization (mailboxes and signals), and a feature called futures. A few examples clarify how the primitives work and demonstrate their expressiveness. When Spur Lisp is ported to and optimized on the Spur workstation (a shared memory multiprocessor), programmers can use it to make symbolic programs parallel.< >", "num_citations": "25\n", "authors": ["35"]}
{"title": "The cloud will change everything\n", "abstract": " Cloud computing is fast on its way to becoming a meaningless, oversold marketing slogan. In the midst of this hype, it is easy to overlook the fundamental change that is occurring. Computation, which used to be confined to the machine beside your desk, is increasingly centralized in vast shared facilities and at the same time liberated by battery-powered, wireless devices. Performance, security, and reliability are no longer problems that can be considered in isolation -- the wires and software connecting pieces offer more challenges and opportunities than components themselves. The eXtreme Computing Group (XCG) in Microsoft Research is taking a holistic approach to research in this area, by bring together researchers and developers with expertise in data center design, computer architecture, operating systems, computer security, programming language, mobile computation, and user interfaces to tackle the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["35"]}
{"title": "Orleans: A framework for cloud computing\n", "abstract": " Client+ cloud computing is a disruptive, new computing platform, combining diverse client devices\u0393\u00c7\u00f4PCs, smartphones, sensors, and single-function and embedded devices\u0393\u00c7\u00f4with the unlimited, on-demand computation and data storage offered by cloud computing services such as Amazon\u0393\u00c7\u00d6s AWS or Microsoft\u0393\u00c7\u00d6s Windows Azure. As with every advance in computing, programming is a fundamental challenge as client+ cloud computing combines many difficult aspects of software development.Orleans is a software framework for building client+ cloud applications. Orleans encourages use of simple concurrency patterns that are easy to understand and implement correctly, building on an actor-like model with declarative specification of persistence, replication, and consistency and using lightweight transactions to support the development of reliable and scalable client+ cloud software.", "num_citations": "24\n", "authors": ["35"]}
{"title": "Influential digital rights management\n", "abstract": " Innovative aspects provided herein pertain to digital rights management (DRM) and/or enforcement in conjunction with remote network clouds and services. Digital rights management licenses/rights/policies can be applied to personal files to facilitate worry free remote storage and/or file sharing. These rights can be identity-centric rather than machine centric, thereby facilitating access and usage from any network device anywhere. Various mechanisms are also disclosed to deter assorted uses of content and/or encourage rights acquisition as an alternative or in addition to technologically prohibitive means. Additionally, a system and method are provided that can afford a frictionless marketplace for file distribution, wherein content is protected and freely distributed and identity-centric rights can be purchased to access the content.", "num_citations": "24\n", "authors": ["35"]}
{"title": "Storm Watch: A Tool for Visualizing Memory System Protocols\n", "abstract": " Recent research has offered programmers increased options for programming parallel computers by exposing system policies (e.g., memory coherence protocols) or by providing several programming paradigms (e.g. message passing and shared memory) on the same platform. Increased flexibility can lead to higher performance, but it is also a double-edged sword that demands a programmer understand his or her application and system at a more fundamental level. Our system, Tempest, allows a programmer to select or implement communication and memory coherence policies that fit an application's communication patterns. With it, we have achieved substantial performance gains without making major changes in programs. However, the process of selecting, designing, and implementing coherence protocols is difficult and time consuming, without tools to supply detailed information about an application's\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["35"]}
{"title": "Determination of optimized location for services and data\n", "abstract": " The claimed subject matter provides a system and/or a method that facilitates preserving and maintaining data and/or services associated with a network service. The network service can be any collection of resources that are maintained by a party (eg, third-party, off-site, etc.) and accessible by an identified user over a network (eg, WAN, Internet, etc.). An interface component can receive a termination notification related to the network service. An executor component can relocate at least a portion of one of data and a service associated with the terminated network service to a disparate replacement network service in order to preserve any services and/or data related therewith.", "num_citations": "23\n", "authors": ["35"]}
{"title": "When computers decide: European recommendations on machine-learned automated decision making\n", "abstract": " Technological innovation is rarely smooth and continuous. Years of slow, incremental research and development precede the seemingly instantaneous introduction, adoption, and deployment of a new technology, which in turn disrupts long-standing interpersonal, societal, political, and economic relationships. We have reached such a point with machine learning (ML) systems. This white paper is focused on machine learning because ML is the fundamental technology underlying a broad array of emerging products and services that are popularly grouped under the rubric of Artificial Intelligence (AI). The social and economic concerns raised for decades about Artificial Intelligence (AI) thus are now questions and concerns about ML, including whether artificially intelligent machines and robots will rapidly surpass, supplant and displace humans socially and especially economically [3][4][5].From a technical perspective, machine learning systems, in contrast to explicitly written programs, are \u0393\u00c7\u00a3trained,\u0393\u00c7\u00a5 by exposing them to a large number of examples and rewarding them for drawing appropriate distinctions and making correct decisions, much in the same way as human beings learn. This distinction, while it may seem esoteric, has far-reaching consequences for our ability to understand the behaviour of these systems and for our confidence that they will behave in an appropriate manner.", "num_citations": "22\n", "authors": ["35"]}
{"title": "Fine-grain checkpointing with in-cache-line logging\n", "abstract": " Non-Volatile Memory offers the possibility of implementing high-performance, durable data structures. However, achieving performance comparable to well-designed data structures in non-persistent (transient) memory is difficult, primarily because of the cost of ensuring the order in which memory writes reach NVM.\\@ Often, this requires flushing data to NVM and waiting a full memory round-trip time. In this paper, we introduce two new techniques: Fine-Grained Checkpointing, which ensures a consistent, quickly recoverable data structure in NVM after a system failure, and In-Cache-Line Logging, an undo-logging technique that enables recovery of earlier state without requiring cache-line flushes in the normal case. We implemented these techniques in the Masstree data structure, making it persistent and demonstrating the ease of applying them to a highly optimized system and their low (5.9-15.4%) runtime\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["35"]}
{"title": "Programming model to detect deadlocks in concurrent programs\n", "abstract": " Described are embodiments for developing a message-passing application program. The program is constructed using stages having a plurality of asynchronous functions, or operations. The operations communicate with other operations of other message-passing programs in a distributed computing environment. The operations also communicate with other operations on other stages of the message-passing application. In order to reduce deadlock errors, a behavioral type signature is appended to the declaration of each operation of the message-passing application program. The behavioral type signature specifies behavioral properties for each operation, such as when an operation should send a message to another operation. A type checker utilizes typing rules and the behavioral type signature to extract an implementation model of each function. The type checker then compares the implementation model to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["35"]}
{"title": "Compiling Lisp programs for parallel execution\n", "abstract": " Curare, the program restructurer described in this paper automatically transforms a sequential Lisp program into an equivalent concurrent program that runs on a multiprocessor. Data dependences constrain the program's concurrent execution because, in general, two conflicting statements cannot execute in a different order without affecting the program's result. Not all dependences are essential to produce the program's result. Curare attempts to transform the program so it computes its result with fewer conflicts. An optimized program will execute with less synchronization and more concurrency. Curare then examines loops in a program to find those that are unconstrained or lightly constrained by dependences. By necessity, Curare treats recursive functions as loops and does not limit itself to explicit program loops. Recursive functions offer several advantages over explicit loops since they provide a convenient\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["35"]}
{"title": "Singularity design motivation\n", "abstract": " Singularity is a cross-discipline research project in Microsoft Research building a managed code operating system. This technical report describes the motivation and priorities for Singularity. Other technical reports describe the abstractions and implementations of Singularity features.", "num_citations": "20\n", "authors": ["35"]}
{"title": "Dynamic program parallelization\n", "abstract": " Static program analysis limits the performance improvements possible from compile-time parallelization.  Dynamic program parallelization shifts a portion of the analysis from complie-time to run-time, thereby enabling optimizations whose static detection is overly expensive or impossible.  Lambda tagging and heap resolution are two new techniques for finding loop and non-loop parallelism in imperative, sequential languages with first-class procedures and destructive heap operations (e.g., ML and Scheme). Lambda tagging annotates procedures during compilation with a tag that describes the side effects that a procedure's application may cause.  During program execution, the program refines and examines tags  to  identify computations that may safely execute in parallel.  Heap resolution uses reference counts to dynamically detect potential heap aliases and to coordinate parallel access to shared structures\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["35"]}
{"title": "Secured routines: Language-based construction of trusted execution environments\n", "abstract": " Trusted Execution Environments (TEEs), such as Intel SGX\u0393\u00c7\u00d6s enclave, use hardware to ensure the confidentiality and integrity of operations on sensitive data. While the technology is widely available, the complexity of its programming model and its performance overhead have limited adoption. TEEs provide a new and valuable hardware functionality that has no obvious analogue in programming languages, which means that developers must manually partition their application into trusted and untrusted components.", "num_citations": "19\n", "authors": ["35"]}
{"title": "Using lightweight procedures to improve instruction cache performance\n", "abstract": " Instruction cache performance is widely recognized as a critical com\u252c\u2557 ponent of the overall performance of a program; especially so in the case of large applications like database servers. In this report, we present a technique for identifying repeated blocks of instructions in a program executable, and (2) converting these repeated code blocks into lightweight procedures (ie LWpr0cs).The use of LWprocs reduces the static code size of a program, and can potentially reduce the working set size of the process, at the cost of increasing its dynamic instruction count. However, the tradeoff seems to be in favor of the reduction in working set size for most programs. Even with a simple model of program structure and a straightforward technique for generating LWprocs, we find performance improvements between 3% to 9% for programs in the SPECINT95 suite. However, the technique sometimes leads to slowdowns\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "18\n", "authors": ["35"]}
{"title": "Secure and stable hosting of third-party extensions to web services\n", "abstract": " Described herein are one or more computer operating environments that include a standard set of web services via a communications network (eg, the Internet) and a mechanism for extending the standard set of web services to execute one or more extended web services. Since these extended web services may be produced by an unconfirmed or untrusted source (eg, a third-party software developer), the described computer operating environments isolate the extended web services from the standard set of web services and from the communication network. Furthermore, each extended web service is an isolated process (isoproc) with a limited ability to communicate with other services. In particular, each isoproc's ability to communicate is limited to only associated defined communication channels over which it has express permission to communicate.", "num_citations": "17\n", "authors": ["35"]}
{"title": "Using managed runtime systems to tolerate holes in wearable memories\n", "abstract": " New memory technologies, such as phase-change memory (PCM), promise denser and cheaper main memory, and are expected to displace DRAM. However, many of them experience permanent failures far more quickly than DRAM. DRAM mechanisms that handle permanent failures rely on very low failure rates and, if directly applied to PCM, are extremely inefficient: Discarding a page when the first line fails wastes 98% of the memory. This paper proposes low complexity cooperative software and hardware that handle failure rates as high as 50%. Our approach makes error handling transparent to the application by using the memory abstraction offered by managed languages. Once hardware error correction for a memory line is exhausted, rather than discarding the entire page, the hardware communicates the failed line to a failure-aware OS and runtime. The runtime ensures memory allocations never use\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["35"]}
{"title": "Fast and portable parallel architecture simulators: Wisconsin Wind Tunnel II\n", "abstract": " Analysis of future parallel computers requires rapid simulation of target designs running realistic workloads. These simulations have been accelerated by two techniques: direct execution and the use of a parallel host. Historically, these techniques have been considered to lack portability. We identify four key operations necessary to make these simulations portable. This allows us to run the Wisconsin Wind Tunnel II (WWT II) readily on a wide range of SPARC platforms from a workstation cluster to a symmetric multiprocessor (SMP).", "num_citations": "17\n", "authors": ["35"]}
{"title": "Memory manager with enhanced application metadata\n", "abstract": " A memory management system is described herein that receives information from applications describing how memory is being used and that allows an application host to exert more control over application requests for using memory. The system provides an application memory management application-programming interface (API) that allows the application to specify more information about memory allocations that is helpful for managing memory later. The system also provides an ability to statically and/or dynamically analyze legacy applications to give applications that are not modified to work with the system some ability to participate in more effective memory management. The system provides application host changes to leverage the information provided by applications and to manage memory more effectively using the information and hooks into the application's use of memory. Thus, the system provides a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["35"]}
{"title": "Software accessible cache metadata\n", "abstract": " Various technologies and techniques are disclosed for providing software accessible metadata on a cache of a central processing unit. The metadata can include at least some bits for each virtual address, at least some bits for each cache line, and at least some bits for the cache overall. An instruction set architecture on the central processing unit is provided that includes additional instructions for interacting with the metadata. New side effects that are introduced into an operation of the central processing unit by a presence of the metadata and the additional instructions. The metadata can be accessed by at least one software program to facilitate an operation of the software program.", "num_citations": "16\n", "authors": ["35"]}
{"title": "Cachier: A tool for automatically inserting CICO annotations\n", "abstract": " Shared memory in a parallel computer provides programmers with the valuable abstraction of a shared address space--through which any part of a computation can access any datum Although uniform access simplifies programming, it also hides communication, which can lead to inefficient programs The check-in, check-out (CICO) performance model for cache-coherent, shared-memory parallel computers helps a programmer identify the communication underlying memory references and account for its cost CICO consists of annotations that a programmer can use to elucidate communication and a model that attributes costs to these annotations The annotations can also serve as directives to a memory system to improve program performance Inserting CICO annotations requires reasoning about the dynamic cache behavior of a program, which is not always easy This paper describes Cachier, a tool that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["35"]}
{"title": "A comparison of microcode, assembly code, and high-level languages on the VAX-11 and RISC I\n", "abstract": " This paper compares the performance of three implementations of simplified pattern matching instruction. The fastest version was microprogrammed on a VAX-11/750, the next fastest was written for a Reduced Instruction Set. Computer (RISC I) developed at Berkeley, and the slowest was pro grammed on the VAX. The paper presents execution times for the microcode and for high-level language and machine language versions of the instruction.", "num_citations": "16\n", "authors": ["35"]}
{"title": "Machine learning system for analyzing and establishing tagging trends based on convergence criteria\n", "abstract": " Systems and methods that analyze aggregated tagging behavior of users, and evaluate such tagging trends to identify criteria for taxonomy applications. Initially, existence of a possible trend of tagging data based on collective user behavior is determined. Subsequently, tagging trends can be examined to identify that a predetermined convergence criteria has in fact been met, and/or establish such criteria for taxonomy applications. Machine learning systems (implicitly as well as explicitly trained) can be supplied to facilitate determining the trends and the convergence criteria.", "num_citations": "15\n", "authors": ["35"]}
{"title": "Persona: A high-performance bioinformatics framework\n", "abstract": " Next-generation genome sequencing technology has reached a point at which it is becoming cost-effective to sequence all patients. Biobanks and researchers are faced with an oncoming deluge of genomic data, whose processing requires new and scalable bioinformatics architectures and systems. Processing raw genetic sequence data is computationally expensive and datasets are large. Current software systems can require many hours to process a single genome and generally run only on a single computer. Common file formats are monolithic and row-oriented, a barrier to distributed computation.", "num_citations": "14\n", "authors": ["35"]}
{"title": "\u0393\u00c7\u00ffSPUR Lisp: Deisgn and Implementation\n", "abstract": " This document describes SPUR. Lisp, a Common Lisp superset designed and implemented at UC Berkeley. Function calling sequences, system data structures, memory management policies, etc. are all described in detail. Reasons for the more important decisions are given. SPUR. Lisp is implemented on BARB, a software simulator for SPUR hardware. In addition to describing the design of SPUR. Lisp, this paper provides documentation for the BARB simulator, the SPUR. Lisp compiler, and associated tools.", "num_citations": "14\n", "authors": ["35"]}
{"title": "Kernel interface with categorized kernel objects\n", "abstract": " Described herein are one or more implementations that separate kernel interfaces functions into those that act on kernel objects owned by a process and accessed exclusively by that process\u0393\u00c7\u00f6described herein as local kernel objects\u0393\u00c7\u00f6from access to kernel objects owned by a process and accessible by other active processes.", "num_citations": "12\n", "authors": ["35"]}
{"title": "Inter-process interference elimination\n", "abstract": " Described herein is an implementation of a technology for the construction, identification, and/or optimization of operating-system processes. At least one implementation, described herein, constructs an operating-system process having the contents as defined by a process manifest. Once constructed, the operating-system process is unalterable.", "num_citations": "12\n", "authors": ["35"]}
{"title": "Guardian angel\n", "abstract": " An intelligent personalized agent monitors, regulates, and advises a user in decision-making processes for efficiency or safety concerns. The agent monitors an environment and present characteristics of a user and analyzes such information in view of stored preferences specific to one of multiple profiles of the user. Based on the analysis, the agent can suggest or automatically implement a solution to a given issue or problem. In addition, the agent can identify another potential issue that requires attention and suggests or implements action accordingly. Furthermore, the agent can communicate with other users or devices by providing and acquiring information to assist in future decisions. All aspects of environment observation, decision assistance, and external communication can be flexibly limited or allowed as desired by the user.", "num_citations": "12\n", "authors": ["35"]}
{"title": "User-de ned Reductions for E cient Communication in Data-Parallel Languages\n", "abstract": " Data-parallel languages typically include reduction operations for specifying combining and communication. However, most languages limit reductions to prede ned arithmetic and logical operators. User-de ned reductions generalize reductions in two dimensions: they allow the programmer to specify complex combining operations (such as building a list of multiple values), and they extend naturally to user-de ned data types. This paper demonstrates that user-de ned reductions in a data-parallel language o er an e cient, high-level interface to commonly occurring communication patterns in scienti c applications. It describes the design and implementation of user-de ned reductions in the data-parallel language C** with simple message passing support. It also shows how reductions capture important communication patterns in a variety of applications e ciently.", "num_citations": "12\n", "authors": ["35"]}
{"title": "Inter-disciplinary research challenges in computer systems for the 2020s\n", "abstract": " The broad landscape of new technologies currently being explored makes the current times very exciting for computer systems research. The community is actively researching an extensive set of topics, ranging from the small (eg, energy-independent embedded devices) to the large (eg, brain-scale deep learning), simultaneously addressing technology discontinuities (End of Moore\u0393\u00c7\u00d6s Law and Energy Wall), new challenges in security and privacy, and the rise of artificial intelligence (AI).While industry is applying some of these technologies, its efforts are necessarily focused on only a few areas, and on relatively short-term horizons. This offers academic researchers the opportunity to attack the problems with a broader and longer-term view. Further, in recent times, the computer systems community has started to pay increasing attention to non-performance measures, such as security, complexity, and power. To make progress in this multi-objective world, the composition of research teams needs to change. Teams have to become inter-disciplinary, enabling the flow of ideas across computing fields. 1", "num_citations": "11\n", "authors": ["35"]}
{"title": "Operating system with corrective action service and isolation\n", "abstract": " The claimed subject matter provides a system and/or a method that facilitates re-locating a web application associated with a network service utilizing a portion of serialized data. The network service can be any collection of resources that are maintained by a party (eg, third-party, off-site, etc.) and accessible by an identified user over a network (eg, WAN, Internet, etc.). A receiver component can receive a request for initiating and execution of a process that is maintained by the network service. A servicing component can analyze representations of multiple processes within the network service and determines whether to enable initiation and execution of the process based at least in part upon the analysis.", "num_citations": "11\n", "authors": ["35"]}
{"title": "Parallel programming in C**: A large-grain data-parallel programming language\n", "abstract": " Parallel Programming in C**: A Large-Grain Data-Parallel Programming Language - Infoscience English Fran\u251c\u00baais login Home > Parallel Programming in C**: A Large-Grain Data-Parallel Programming Language Infoscience Information Usage statistics Files Parallel Programming in C**: A Large-Grain Data-Parallel Programming Language Larus, James R. ; Richards, Brad ; Viswanathan, Guhan Published in: Parallel Programming Using C++, 297-342 Year: 1996 Publisher: MIT Press ISBN: 978-0-262-73118-8 Laboratories: UPLARUS Record appears in: Scientific production and competences > I&C - School of Computer and Communication Sciences > IINFCOM > UPLARUS - Prof. Larus Group Peer-reviewed publications Work outside EPFL Book chapters Published URL Export as: BibTeX | MARC | MARCXML | DC | EndNote | NLM | RefWorks | RIS View as: MARC | MARCXML | DC Add to your basket: Back to -12-\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["35"]}
{"title": "Providing Services in a System having a Hardware Acceleration Plane and a Software Plane\n", "abstract": " A service mapping component (SMC) is described herein for allocating services to hardware acceleration components in a data processing system based on different kinds of triggering events. The data processing system is characterized by a hardware acceleration plane that is made up of the hardware acceleration components, together with a software plane that is made up of a plurality of software-driven host components. The SMC is configured to select, in response to a triggering event, at least one hardware acceleration component in the hardware plane to perform a service, based on at least one mapping consideration and based on availability information. Each host component in the software plane is then configured to access the service on one or more of the selected hardware acceleration component (s) via an associated local hardware acceleration component, or via some other route.", "num_citations": "10\n", "authors": ["35"]}
{"title": "Parallelism in numeric and symbolic programs\n", "abstract": " Parallelism in Numeric and Symbolic Programs Toggle navigation Login Toggle navigation View Item MINDS@UW Home MINDS@UW Madison College of Letters and Science, University of Wisconsin\u0393\u00c7\u00f4Madison Department of Computer Sciences, UW-Madison CS Technical Reports View Item MINDS@UW Home MINDS@UW Madison College of Letters and Science, University of Wisconsin\u0393\u00c7\u00f4Madison Department of Computer Sciences, UW-Madison CS Technical Reports View Item Parallelism in Numeric and Symbolic Programs Thumbnail File(s) TR929.pdf (2.233Mb) Date 1990 Author Larus, James R Publisher University of Wisconsin-Madison Department of Computer Sciences Metadata Show full item record Permanent Link http://digital.library.wisc.edu/1793/59288 Part of CS Technical Reports Contact Us | Send Feedback Search MINDS@UW This Collection Browse All of MINDS@UWCommunities & By Issue | \u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["35"]}
{"title": "Operating-system process construction\n", "abstract": " Described herein is an implementation of a technology for the construction, identity, and/or optimization of operating-system processes. At least one implementation, described herein, constructs an operating-system process having the contents as defined by a process manifest. Once constructed, the operating-system process is unalterable.", "num_citations": "9\n", "authors": ["35"]}
{"title": "Operating system process identification\n", "abstract": " Described herein is an implementation of a technology for the construction, identification, and/or optimization of operating-system processes. At least one implementation, described herein, constructs an operating-system process having the contents as defined by a process manifest. Once constructed, the operating-system process is unalterable.", "num_citations": "9\n", "authors": ["35"]}
{"title": "Multicore computing and scientific discovery\n", "abstract": " Multicore Computing and Scientific Discovery - Infoscience English Fran\u251c\u00baais login Home > Multicore Computing and Scientific Discovery Infoscience Information Usage statistics Files Multicore Computing and Scientific Discovery Larus, James ; Gannon, Dennis Published in: The Fourth Paradigm: Data-Intensive Scientific Discovery, 125-130 Year: 2009 Publisher: Microsoft Research Laboratories: UPLARUS Record appears in: Scientific production and competences > I&C - School of Computer and Communication Sciences > IINFCOM > UPLARUS - Prof. Larus Group Peer-reviewed publications Work outside EPFL Book chapters Published Export as: BibTeX | MARC | MARCXML | DC | EndNote | NLM | RefWorks | RIS View as: MARC | MARCXML | DC Add to your basket: Back to search Record created 2013-12-23, last modified 2020-04-20 Rate this document: 1 2 3 4 5 (Not yet reviewed) Add to personal basket as , \u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["35"]}
{"title": "Behavioral types for structured asynchronous programming\n", "abstract": " Behavioral Types for Structured Asynchronous Programming - Infoscience English Fran\u251c\u00baais login Home > Behavioral Types for Structured Asynchronous Programming Infoscience Information Usage statistics Files Behavioral Types for Structured Asynchronous Programming Larus, James R. ; Rajamani, Sriram K. ; Rehof, Jakob Year: 2001 Publisher: Microsoft Research Laboratories: UPLARUS Record appears in: Scientific production and competences > I&C - School of Computer and Communication Sciences > IINFCOM > UPLARUS - Prof. Larus Group Peer-reviewed publications Work outside EPFL Technical Reports Published Export as: BibTeX | MARC | MARCXML | DC | EndNote | NLM | RefWorks | RIS View as: MARC | MARCXML | DC Add to your basket: Back to search Record created 2013-12-23, last modified 2020-04-20 Rate this document: 1 2 3 4 5 (Not yet reviewed) Add to personal basket Export as , , , -\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["35"]}
{"title": "Remote network operating system\n", "abstract": " The claimed subject matter provides a system and/or a method that facilitates re-locating a web application associated with a network service utilizing a portion of serialized data. The network service can be any collection of resources that are maintained by a party (eg, third-party, off-site, etc.) and accessible by an identified user over a network (eg, WAN, Internet, etc.). A receiver component can receive a request for initiating and execution of a process that is maintained by the network service. A servicing component can analyze representations of multiple processes within the network service and determines whether to enable initiation and execution of the process based at least in part upon the analysis.", "num_citations": "8\n", "authors": ["35"]}
{"title": "Features for multiprocessing in SPUR Lisp\n", "abstract": " This paper describes simple extensions to Common Lisp for concurrent computation on multiprocessors. Functions for process creation, communication, and synchronization are described. Multiple threads of control are created with process objects. Communication and synchronization are managed using mailbozes. Signals provide asynchronous communication between processes. SPUR. Lisp includes future and delay values, which were first introduced in Multilisp [6]. These features provide a flexible and efficient basis on which higher-level multiprocessing abstractions can be implemented and studied.", "num_citations": "8\n", "authors": ["35"]}
{"title": "Scheduling execution requests to allow partial results\n", "abstract": " The subject disclosure is directed towards scheduling requests using quality values that are defined for partial responses to the requests. For each request in a queue, an associated processing time is determined using a system load and/or the quality values. The associated processing time is less than or equal to a service demand, which represents an amount of time to produce a complete response.", "num_citations": "7\n", "authors": ["35"]}
{"title": "Estimating the potential parallelism in programs\n", "abstract": " Estimating the Potential Parallelism in Programs - Infoscience English Fran\u251c\u00baais login Home > Estimating the Potential Parallelism in Programs Infoscience Information Usage statistics Files Estimating the Potential Parallelism in Programs Larus, James R. Published in: 4th Workshop on Languages and Compilers for Parallel Computing, 331-349 Year: 1991 Publisher: MIT Press Laboratories: UPLARUS Record appears in: Scientific production and competences > I&C - School of Computer and Communication Sciences > IINFCOM > UPLARUS - Prof. Larus Group Peer-reviewed publications Work outside EPFL Conference Papers Published Export as: BibTeX | MARC | MARCXML | DC | EndNote | NLM | RefWorks | RIS View as: MARC | MARCXML | DC Add to your basket: Back to search Record created 2013-12-23, last modified 2020-04-20 Rate this document: 1 2 3 4 5 (Not yet reviewed) Add to personal basket as , , \u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["35"]}
{"title": "Exact data dependence analysis using data access descriptors\n", "abstract": " Exact Data Dependence Analysis Using Data Access Descriptors (Extended Abstract) - Infoscience English Fran\u251c\u00baais login Home > Exact Data Dependence Analysis Using Data Access Descriptors (Extended Abstract) Infoscience Information Usage statistics Files Exact Data Dependence Analysis Using Data Access Descriptors (Extended Abstract) Huelsbergen, Lorenz ; Hahn, Douglas ; Larus, James Published in: 1990 International Conference on Parallel Processing, II, 290-291 Year: 1990 Publisher: Pennsylvania State University Laboratories: UPLARUS Record appears in: Scientific production and competences > I&C - School of Computer and Communication Sciences > IINFCOM > UPLARUS - Prof. Larus Group Peer-reviewed publications Work outside EPFL Conference Papers Published Export as: BibTeX | MARC | MARCXML | DC | EndNote | NLM | RefWorks | RIS View as: MARC | MARCXML | DC Add to : --\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["35"]}
{"title": "Parallel decision tree processor architecture\n", "abstract": " A decision tree multi-processor system includes a plurality of decision tree processors that access a common feature vector and execute one or more decision trees with respect to the common feature vector. A related method includes providing a common feature vector to a plurality of decision tree processors implemented within an on-chip decision tree scoring system, and executing, by the plurality of decision tree processors, a plurality off decision trees, by reference to the common feature vector. A related decision tree-walking system includes feature storage that stores a common feature vector and a plurality of decision tree processors that access the common feature vector from the feature storage and execute a plurality of decision trees by comparing threshold values of the decision trees to feature values within the common feature vector.", "num_citations": "6\n", "authors": ["35"]}
{"title": "Decision tree threshold coding\n", "abstract": " Disclosed herein are systems and methods for coding decision trees, such as for execution on a decision tree scorer system. A computing system determines, for a particular feature of a plurality of features included in one or more decision trees, a list of unique threshold values associated with the particular feature in the one or more decision trees. The computing system determines a plurality of threshold index values for the list of unique threshold values, and represents the one or more decision trees such that decision nodes of the one or more decision trees associated with the particular feature include ones of the threshold index values.", "num_citations": "6\n", "authors": ["35"]}
{"title": "Fine-grained parallel traversals of irregular data structures\n", "abstract": " Fine-grain data parallelism is increasingly common in mainstream processors in the form of long vectors and on-chip GPUs. This paper develops compiler and runtime support to exploit such data parallelism for non-numeric, non-graphic, irregular parallel tasks that perform simple computations while traversing many independent, irregular data structures, like trees and graphs. We vectorize the traversal of trees and graphs by treating a set of irregular data structures as a parallel control-flow graph and compiling the traversal into a domain-specific bytecodes. We produce a SIMD interpreter for these bytecodes, so each lane of a SIMD unit traverses one irregular data structure. Despite the overhead of interpretation, we demonstrate significant increases in single-core performance over optimized baselines.", "num_citations": "6\n", "authors": ["35"]}
{"title": "Spim\n", "abstract": " SPIM S20 is a simulator that runs programs for the MIPS R2000/R3000 RISC computers. \u251c\u00fb SPIM can read and immediately execute files containing assembly language or MIPS executable files. SPIM is a self-contained system for running these programs and contains a debugger and interface to a few operating system services.The architecture of the MIPS computers is simple and regular, which makes it easy to learn and understand. The processor contains 32 general-purpose registers and a well-designed instruction set that make it a propitious target for generating code in a compiler. However, the obvious question is: why use a simulator when many people have workstations that contain a hardware, and hence significantly faster, implementation of this computer? One reason is that these workstations are not generally available. Another reason is that these machine will not persist for many years because of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["35"]}
{"title": "Parallel computer research in the Wisconsin Wind Tunnel project\n", "abstract": " The paper summarizes the Wisconsin Wind Tunnel Project\u0393\u00c7\u00d6s research into parallel computer design and methods. Our principal design contributions\u0393\u00c7\u00f6Cooperative", "num_citations": "6\n", "authors": ["35"]}
{"title": "The Wisconsin Wind Tunnel Project: An Annotated Bibliography\n", "abstract": " This document lists contributors to the Wisconsin Wind Tunnel Project, gives a brief description of the project, and presents references and abstracts to its principal papers, including how to obtain them online.", "num_citations": "6\n", "authors": ["35"]}
{"title": "Cache Considerations for Programmers of Multiprocessors\n", "abstract": " Although caches in computers are invisible to programmers, the significantly affect programs  performance.  This is particularly true for multiprocessors.  This paper presents results from recent computer architecture research about the performance of parallel programs and discuss their implications for programmers who may know little or nothing about caches.", "num_citations": "6\n", "authors": ["35"]}
{"title": "Decision tree processors\n", "abstract": " Disclosed herein are systems, on-chip processors, and methods for executing decision trees. Decision tree circuitry retrieves a plurality of decision trees, which include feature locations and threshold values. A subset of the decision nodes includes next node data. The decision tree circuitry executes the decision nodes and determines next decision nodes to be retrieved and executed based on outcomes of the execution of the decision nodes. First outcomes of decision tree node executions result in determining the next decision nodes of the plurality of decision nodes based on the next node data. Second outcomes of the decision tree node executions result in determining the next decision nodes that are adjacent to currently executing nodes of the plurality of decision nodes.", "num_citations": "5\n", "authors": ["35"]}
{"title": "Tools for cacheconscious data structures\n", "abstract": " The speed of microprocessors has increased 60% per year for almost two decades. Yet, over the same period, the time to access main memory decreased at only 10% per year [32]. The unfortunate, but inevitable, consequence of these trends is a large, and ever-increasing, processor-memory gap. Memory caches are the ubiquitous hardware solution [50, 43]. In the beginning, a single cache sufficed, but the increasing gap (now almost two orders of magnitude) requires a hierarchy of caches, which causes further disparity in memory-access costs. Even so, many programs\u0393\u00c7\u00d6 performance is dominated by memory references. Moreover, high and variable memory access costs undercut the fundamental random-access memory (RAM) model that most programmers use to understand and design data structures and algorithms. Many hardware and software techniques\u0393\u00c7\u00f6such as prefetching [29, 9, 26, 38], multithreading\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["35"]}
{"title": "CURARE. Restructuring Lisp Programs for Concurrent Execution\n", "abstract": " This paper describes the techniques used by CURARE, a program transformer, to restructure Lisp programs for concurrent execution in shared-memory multi-processor Lisp systems. CURARE tries to eliminate control and data-dependencies that prevent concurrent execution of the invocations of recursive functions. CURARE also inserts a variety of synchronization devices to ensure that unremovable dependencies do not impair execution of a program. The product of this process is semantically equivalent to the original Lisp program, but executes faster on a multiprocessor than would the original program.Descriptors:", "num_citations": "5\n", "authors": ["35"]}
{"title": "On the Performance of Courier Remote Procedure Calls Under 4.1 c BSD\n", "abstract": " Courier is a remote procedure call standard developed by Xerox. This paper reports measurements of Courier's performance under 4.1 c BSD Unix running on VAX-11/780s and on Sun personel workstations. The cost of a remote procedure call is many times that of a local call. However, Courier's performance could be greatly improved by using a simpler proto-col and better Ethernet interfaces.", "num_citations": "5\n", "authors": ["35"]}
{"title": "Lisp extensions for multiprocessing\n", "abstract": " Extensions to Common Lisp for concurrent computation on multiprocessors are discussed. Functions for process creation, communication, and synchronization are described. Process objects create multiple threads of control. Processes are lightweight so that programmers can use them to take advantage of fine-grained parallelism. Communication and synchronization are managed with mailboxes. Signals allow processes to communicate using asynchronous interrupts. These constructs are used to implement several higher-level multiprocessing abstractions. These include structured processes, a parallel tree search, and dataflow computation.<>", "num_citations": "4\n", "authors": ["35"]}
{"title": "Detailed heap profiling\n", "abstract": " Modern software systems heavily use the memory heap. As systems grow more complex and compute with increasing amounts of data, it can be difficult for developers to understand how their programs actually use the bytes that they allocate on the heap and whether improvements are possible. To answer this question of heap usage efficiency, we have built a new, detailed heap profiler called Memoro. Memoro uses a combination of static instrumentation, subroutine interception, and runtime data collection to build a clear picture of exactly when and where a program performs heap allocation, and crucially how it actually uses that memory. Memoro also introduces a new visualization application that can distill collected data into scores and visual cues that allow developers to quickly pinpoint and eliminate inefficient heap usage in their software. Our evaluation and experience with several applications\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["35"]}
{"title": "Singularity\n", "abstract": " Singularity - Infoscience English Fran\u251c\u00baais login Home > Singularity Infoscience Information Usage statistics Files Singularity Larus, James ; Hunt, Galen ; Tarditi, David Published in: MSDN Magazine, 21, 7, 176 Year: 2006 Publisher: Microsoft Laboratories: UPLARUS Record appears in: Scientific production and competences > I&C - School of Computer and Communication Sciences > IINFCOM > UPLARUS - Prof. Larus Group Peer-reviewed publications Work outside EPFL Journal Articles Published URL Export as: BibTeX | MARC | MARCXML | DC | EndNote | NLM | RefWorks | RIS View as: MARC | MARCXML | DC Add to your basket: Back to search Record created 2013-12-23, last modified 2020-07-30 External link: Download fulltext URL Rate this document: 1 2 3 4 5 (Not yet reviewed) Add to personal basket Export as BibTeX, MARC, MARCXML, DC, EndNote, NLM, RefWorks About Infoscience | Contact | | \u252c\u2310 -\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["35"]}
{"title": "User-defined reductions for communication in data-parallel languages\n", "abstract": " Parallel programming and parallel computers, have been a gleam in the eye of computer science for three or four decades. Rapid advances in semiconductor technology have led to high-performance, low-cost microprocessors that are appropriate components for a parallel machine. Unfortunately, this progress has left parallel software far behind. The difficulty of programming parallel computers is now, by far, the largest obstacle to their widespread use.Improved parallel programming languages could reduce the difficulty of programming parallel computers by making programs less error prone and less machine specific. One promising approach is data-parallel languages, such as HPF [9], C*[18], or NESL [2], which provide high-level abstractions for the three key facets of parallel programming: concurrency, synchronization, and communication. In these languages, programmers express parallelism by invoking a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["35"]}
{"title": "Abacus: Precise side-channel analysis\n", "abstract": " Side-channel attacks allow adversaries to infer sensitive information from non-functional characteristics. Prior side-channel detection work is able to identify numerous potential vulnerabilities. However, in practice, many such vulnerabilities leak a negligible amount of sensitive information, and thus developers are often reluctant to address them. Existing tools do not provide information to evaluate a leak\u0393\u00c7\u00d6s severity, such as the number of leaked bits.To address this issue, we propose a new program analysis method to precisely quantify the leaked information in a single-trace attack through side-channels. It can identify covert information flows in programs that expose confidential information and can reason about security flaws that would otherwise be difficult, if not impossible, for a developer to find. We model an attacker\u0393\u00c7\u00d6s observation of each leakage site as a constraint. We use symbolic execution to generate these\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["35"]}
{"title": "Programming multicore computers: Technical perspective\n", "abstract": " Programming multicore computers: technical perspective Page 1 76 COMMUNICATIONS OF THE ACM | MAY 2015 | VOL. 58 | NO. 5 and must disallow optimizations that might adversely affect a program\u0393\u00c7\u00d6s result. This paper argues the restructurings and annotations, when performed by developers, are not difficult and should be part of every programmer\u0393\u00c7\u00d6s repertoire for modern computers. These changes include transforming an array of structures into a structure of arrays, blocking loops to increase data reuse, annotating parallel loops, and adopting more parallel algorithms. Conceptually, none of these changes is difficult to understand\u0393\u00c7\u00f6although finding a new algorithm may be challenging. However, these modifications can introduce errors into a program and can be complex to apply to a large application, where a data structure may be shared by many routines. Of course, program optimization in general can have , \u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["35"]}
{"title": "Look up! your future is in the cloud\n", "abstract": " The \"Cloud\" is a wonderfully expansive phrase used to denote computation and data storage centralized in a large datacenter and elastically accessed across a network. The concept is not new; web sites and business servers have run in datacenters for a long time. These, however, were specialized applications, outside of the mainstream of desktop programs. The past few years has seen enormous change as the mainstream shifts from a single computer to mobile devices and clusters of computers. Three factors are driving this change. 1) Mobile computing, where apps run on a size- and power-constrained device and would be far less interesting without backend systems to augment computation and storage capacity. 2) Big data, which uses clusters of computers to extract valuable information from vast amounts of unstructured data. 3) Inexpensive, elastic computing, pioneered by Amazon Web Services, which\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["35"]}
{"title": "Keynote address I: Programming the cloud\n", "abstract": " Client+ cloud computing is a disruptive, new computing platform, combining diverse client devices-PCs, smartphones, sensors, and single-function and embedded devices-with the unlimited, on-demand computation and data storage offered by cloud computing services such as Amazon's AWS or Microsoft's Windows Azure. As with every advance in computing, programming is a fundamental challenge as client+ cloud computing combines many difficult aspects of software development. Systems built for this world are inherently parallel and distributed, run on unreliable hardware, and must be continually available-a challenging programming model for even the most skilled programmers. How then do ordinary programmers develop software for the Cloud? This talk presents one answer, Orleansis a software framework for building client+ cloud applications. Orleans encourages use of simple concurrency patterns\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["35"]}
{"title": "Optimally Profiling and Tracing Programs\n", "abstract": " This paper presents two algorithms for inserting monitoring code to profile and trace programs. These algorithms greatly reduce the cost of measuring programs. Profiling, which counts the number of times each basic block in a program executes, is widely used to measure instruction set utilization of computers, identify program bottlenecks, and estimate program execution times for code optimization. Instruction traces are the basis for trace-driven simulation and analysis and are used also in trace-driven debugging. The profiling algorithm instruments a program for profiling by choosing a placement of counters that is optimized\u0393\u00c7\u00f6and frequently optimal\u0393\u00c7\u00f6with respect to the expected or measured execution frequency of each basic block and branch in the program. The tracing algorithm instruments a program to obtain a subsequence of the basic block trace\u0393\u00c7\u00f6whose length is optimized with respect to the program's\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["35"]}
{"title": "Classy: A method for efficiently compiling Smalltalk\n", "abstract": " Classy: A Method for Efficiently Compiling Smalltalk - Infoscience English Fran\u251c\u00baais login Home > Classy: A Method for Efficiently Compiling Smalltalk Infoscience Information Usage statistics Files Classy: A Method for Efficiently Compiling Smalltalk Larus, James ; Bush, William Year: 1983 Publisher: Computer Science Division (EECS), University of California at Berkeley Laboratories: UPLARUS Record appears in: Scientific production and competences > I&C - School of Computer and Communication Sciences > IINFCOM > UPLARUS - Prof. Larus Group Peer-reviewed publications Work outside EPFL Technical Reports Published Export as: BibTeX | MARC | MARCXML | DC | EndNote | NLM | RefWorks | RIS View as: MARC | MARCXML | DC Add to your basket: Back to search Record created 2013-12-23, last modified 2020-04-20 Rate this document: 1 2 3 4 5 (Not yet reviewed) Add to personal basket Export as , , , , \u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["35"]}
{"title": "Parlez-Vous Franz? An Informal Introduction to Interfacing Foreign Functions to Franz LISP\n", "abstract": " Parlez-Vous Franz? An Informal Introduction to Interfacing Foreign Functions to Franz LISP - Infoscience English Fran\u251c\u00baais login Home > Parlez-Vous Franz? An Informal Introduction to Interfacing Foreign Functions to Franz LISP Infoscience Information Usage statistics Files Parlez-Vous Franz? An Informal Introduction to Interfacing Foreign Functions to Franz LISP Larus, James Year: 1983 Publisher: Center for Pure and Applied Mathematics, Univ. of California, Berkeley Laboratories: UPLARUS Record appears in: Scientific production and competences > I&C - School of Computer and Communication Sciences > IINFCOM > UPLARUS - Prof. Larus Group Peer-reviewed publications Work outside EPFL Technical Reports Published Export as: BibTeX | MARC | MARCXML | DC | EndNote | NLM | RefWorks | RIS View as: MARC | MARCXML | DC Add to your basket: Back to search Record created 2013-12-23, last 2020--(\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["35"]}
{"title": "Abacus: A Tool for Precise Side-channel Analysis\n", "abstract": " Side-channel vulnerabilities can leak sensitive in-formation unconsciously. In this paper, we introduce the usage of Abacus. Abacus is a tool that can analyze secret-dependent control-flow and secret-dependent data-access leakages in binary programs. Unlike previous tools that can only identify leakages, it can also estimate the amount of leaked information for each leakage site. Severe vulnerabilities usually leak more information, allowing developers to triage the patching effort for side-channel vulnerabilities. This paper is to help users make use of Abacus and reproduce our previous results. Abacus is available at https://github.com/s3team/Abacus.", "num_citations": "1\n", "authors": ["35"]}
{"title": "Parallel and Scalable Precise Clustering for Homologous Protein Discovery\n", "abstract": " This paper presents a new, parallel implementation of clustering and demonstrates its utility in greatly speeding up the process of identifying homologous proteins. Clustering is a technique to reduce the number of comparison needed to find similar pairs in a set of  elements such as protein sequences. Precise clustering ensures that each pair of similar elements appears together in at least one cluster, so that similarities can be identified by all-to-all comparison in each cluster rather than on the full set. This paper introduces ClusterMerge, a new algorithm for precise clustering that uses transitive relationships among the elements to enable parallel and scalable implementations of this approach. We apply ClusterMerge to the important problem of finding similar amino acid sequences in a collection of proteins. ClusterMerge identifies 99.8% of similar pairs found by a full  comparison, with only half as many operations. More importantly, ClusterMerge is highly amenable to parallel and distributed computation. Our implementation achieves a speedup of 604 on 768 cores (1400 faster than a comparable single-threaded clustering implementation), a strong scaling efficiency of 90%, and a weak scaling efficiency of nearly 100%.", "num_citations": "1\n", "authors": ["35"]}
{"title": "Wok: statistical program slicing in production\n", "abstract": " Dynamic program slicing can significantly reduce the amount of code developers need to inspect by focusing only on program statements relevant to their investigation. However, it is still not ready for production-level use either in terms of runtime or storage efficiency. We propose statistical program slicing, a novel hybrid dynamic-static slicing technique which explores the tradeoffs between runtime overhead, accuracy, and storage costs. Our approach relies on modern hardware support for control-flow tracing and selective heap memory instrumentation distributed across multiple executions combined with static program analysis for data-flow tracking.", "num_citations": "1\n", "authors": ["35"]}
{"title": "Reducing transaction aborts by looking to the future\n", "abstract": " Transactions are widely used in database engines and they becoming increasingly useful as a general synchronization technique for multicore machines [1]. Transactional systems allow a programmer to encapsulate multiple operations inside a transaction. All these operations appear to be executed atomically or not at all.", "num_citations": "1\n", "authors": ["35"]}
{"title": "Technical Perspective: The power of parallelizing computations\n", "abstract": " As computers become parallel, performance-challenged programs must also become parallel. For some algorithms, this is not a great challenge as the underlying problem divides naturally into independent pieces that can be computed concurrently. Other problems are not so lucky. Their constituent computations are tightly interdependent, and a parallel implementation requires considerable coordination and synchronization and may still perform poorly.Recursive algorithms fall into this category. The recursive call is a dependence between the calculations in successive function invocations, which makes it difficult to overlap their executions significantly. There is no general formula for transforming a recursive function for parallel execution. It is necessary to understand the intrinsic structure of the underlying computation and to find a way to preserve the essential relationships while running in parallel.", "num_citations": "1\n", "authors": ["35"]}
{"title": "The real value of testing.\n", "abstract": " The Real Value of Testing Page 1 THE REAL VALUE OF TESTING (OR, WHAT I\u0393\u00c7\u00d6VE LEARNED IN THE PAST DECADE) James Larus Microsoft Research ISSTA, July 22, 2008 Page 2 If Only We Knew Listened\u0393\u00c7\u00aa \u0393\u00c7\u00a3The real value of tests is not that they detect bugs in the code but that they detect inadequacies in the methods, concentration, and skills of those who design and produce the code. \u0393\u00fb\u00bd Tony Hoare, How did software get so reliable without proof?, FME \u0393\u00c7\u00ff96 2 \u0393\u00c7\u00f3 Jim Larus \u0393\u00c7\u00f3 Microsoft Research \u0393\u00c7\u00f3 Page 3 A Bit of History \u0393\u00fb\u00ac I went to MSR on sabbatical, summer 1997 \u0393\u00fb\u00ac Decided to stay, spring 1998 \u0393\u00fb\u00ac Started SPT \u0393\u00c7\u00f4Software Productivity Tools \u0393\u00fb\u00bd Amitabh Srivastava joined to start PPRC \u0393\u00c7\u00f4 Programmer Productivity Research Center \u0393\u00fb\u00bd Yuri Gurevich joined to start FSE \u0393\u00c7\u00f4 Foundations of Software Engineering 3 \u0393\u00c7\u00f3 Jim Larus \u0393\u00c7\u00f3 Microsoft Research \u0393\u00c7\u00f3 Page 4 Why Fervor Around SWE? \u0393\u00fb\u00ac Microsoft struggling to ship Windows 2000 \u0393\u00fb\u00bd \u0393\u00fb\u00bd \u0393\u00fb\u00ac not (\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["35"]}
{"title": "Using the singularity research development kit\n", "abstract": " ASPLOS 2008 Totorial: Using the Singularity Research Development Kit Page 1 Using the Singularity Research Development Kit James Larus & Galen Hunt Microsoft Research ASPLOS \u0393\u00c7\u00ff08 Tutorial March 1, 2008 Page 2 Outline \u0393\u00c7\u00f3Singularity Overview (Jim) \u0393\u00c7\u00f4Rationale & key decisions \u0393\u00c7\u00f4Singularity architecture \u0393\u00c7\u00f3Singularity Details (Galen) Singularity Page 3 Singularity Singularity Project \u0393\u00c7\u00f3 Large Microsoft Research project with goal of more robust and reliable software \u0393\u00c7\u00f4 Galen Hunt, Jim Larus, and many others \u0393\u00c7\u00f3 Started with firm architectural principles \u0393\u00c7\u00f4 software will fail, system should not \u0393\u00c7\u00f4 system should be self-describing \u0393\u00c7\u00f4 verify as many system aspects as possible \u0393\u00c7\u00f3 No single magic bullet \u0393\u00c7\u00f4 mutually reinforcing improvements to languages and compilers, systems, and tools Safe Languages (C#) Verification Tools Improved OS Architecture Page 4 Singularity Key Tenets 1. Use safe programming languages \u0393\u00c7\u00f4 safe \u0393\u00e7\u00c6 () \u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["35"]}
{"title": "Corezilla: Build and tame the multicore beast?\n", "abstract": " Are multi-core SoCs being held back by the lack of adequate system design and software development tools? Multi-cores supply the advantages of flexible software-defined architectures, but support for system optimization, integration and verification is lacking. Are we taking advantage of new compute density, application partitioning and parallelism? What is enabled with multi-core vs. multiprocessing? How do we accommodate symmetric and asymmetric multiprocessing? We need profiling tools, retained legacy code and a deep understanding of hardware / software interaction.", "num_citations": "1\n", "authors": ["35"]}
{"title": "Singularity: Rethinking the Software Stack\n", "abstract": " Singularity: Rethinking the Software Stack Page 1 Singularity: Rethinking the Software Stack James Larus Microsoft Research University of Pennsylvania November 9, 2006 Page 2 Singularity MSR Software Verification and Testing Research \u0393\u00c7\u00f3 SLAM \u0393\u00c7\u00f4 shipping in Windows DDK \u0393\u00c7\u00f3 ESP \u0393\u00c7\u00f4 extensively used in Windows (CSE) \u0393\u00c7\u00f3 Fugue \u0393\u00c7\u00f4 shipping in FxCop \u0393\u00c7\u00f3 SAL (Standard Annotation Language) \u0393\u00c7\u00f4 most MS .h files annotated \u0393\u00c7\u00f4 shipping in VS2005 \u0393\u00c7\u00f3 SpecExplorer \u0393\u00c7\u00f4 widely used within MS Page 3 Singularity Frustration, Despite Progress \u0393\u00c7\u00f3 \u0393\u00c7\u00a3First we bug the software, then we debug it\u0393\u00c7\u00a5 \u0393\u00c7\u00f4 existing code bases written without many tools \u0393\u00c7\u00f3 Specification unavailable \u0393\u00c7\u00f3 Program analysis difficult \u0393\u00c7\u00f4 unsafe language \u0393\u00c7\u00f4 tenuous assumptions (eg, code completely known and immutable) \u0393\u00c7\u00f3 Struggle to incorporate tools in process \u0393\u00c7\u00f3 Verification and testing stuck at low level \u0393\u00c7\u00f4 language features and programming interfaces Page 4 Singularity \u0393\u00c7\u00a3\u0393\u00c7\u00a5 \u0393\u00c7\u00f3 i\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["35"]}
{"title": "Retrospective: tempest and typhoon: user-level shared memory\n", "abstract": " Tempest and Typhoon have emerged as among the most influential contributions of the Wisconsin Wind Tunnel project, a collaborative effort with Prof. Mark D. Hill, several staff members, and a large group of graduate students. This retrospective focuses on the origins of the Tempest and Typhoon ideas and their subsequent evolution.", "num_citations": "1\n", "authors": ["35"]}
{"title": "Analyzing Path Pro les with the Hot Path Browser\n", "abstract": " Measuring program behavior is easy; understanding program behavior is hard. It is not di cult to construct program pro lers using instrumentation libraries such as ATOM SE94], and EEL LS95]. The resultant pro ling tools can accurately and e ciently record many aspects of programs' execution. Such tools generate reams of data but o er little support to the end user in analyzing and understanding this data. Producing useful information that provides insight into a program's behavior remains a di cult task. Without this understanding, consumers of measurement data {programmers, computer architects, compiler writers, etc.{can be distracted by minor moguls and miss important mountains. We have built a tool, called the Hot Path Browser (HPB) for graphically displaying path pro les BL96, ABL97]. Paths provide a concise record of a program's dynamic control ow. Even large complex programs, such as gcc and Microsoft Word, only execute a few tens of thousands of paths in an execution. Moreover, the vast majority of these paths contribute little to the overall execution, which is dominated by a small subset of hot paths. HPB helps the user analyze the path pro le data from one or more runs of a program. Its browser-like windowing interface provides mechanisms for isolating and displaying hot paths at the source level. With a tool such as HPB, a programmer can examine the small number of heavily executed paths, to better understand his or her program's behavior and to nd redundant computation. Similarly, a computer architect or compiler writer can use the tool to relate dynamic metrics, such as cache misses or instructions stalls along a path to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["35"]}
{"title": "Portably Supporting Parallel Programming Languages\n", "abstract": " To find out where software is headed, experts in academia and industry share their vision of software's future. It is a snapshot in time of where we have been and possibly where we are headed. The subjects discussed are: the desktop; software technology; objects; software agents; software engineering; parallel software; and the curriculum. The results suggest a strong polarization within the software community: a chasm exists between academia and industry. It appears that these two groups share radically different views on where software is headed. The impression is the heavy emphasis on programming languages, operating systems and algorithms by the academic group, in contrast to the clear emphasis on standards and market-leading trends by the industrial group. Academics worry about evolutionary or incremental changes to already poorly designed languages and systems, while industrialists race to keep up with revolutionary changes in everything. Academics are looking for better ideas, industrialists for better tools. To an industrial person, things are moving fast-they are revolutionary. To an academic, things are moving too slowly, and in the wrong direction-they are only evolutionary changes which are slave to an installed base.", "num_citations": "1\n", "authors": ["35"]}