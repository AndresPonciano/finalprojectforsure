{"title": "Weibull models\n", "abstract": " A comprehensive perspective on Weibull models The literature on Weibull models is vast, disjointed, andscattered across many different journals. Weibull Models is acomprehensive guide that integrates all the different facets ofWeibull models in a single volume. This book will be of great help to practitioners in reliabilityand other disciplines in the context of modeling data sets usingWeibull models. For researchers interested in these modelingtechniques, exercises at the end of each chapter define potentialtopics for future research. Organized into seven distinct parts, Weibull Models:* Covers model analysis, parameter estimation, model validation, and application* Serves as both a handbook and a research monograph. As ahandbook, it classifies the different models and presents theirproperties. As a research monograph, it unifies the literature andpresents the results in an integrated manner* Intertwines theory and application* Focuses on model identification prior to model parameterestimation* Discusses the usefulness of the Weibull Probability plot (WPP) in the model selection to model a given data set* Highlights the use of Weibull models in reliability theory Filled with in-depth analysis, Weibull Models pulls together themost relevant information on this topic to give everyone fromreliability engineers to applied statisticians involved withreliability and survival analysis a clear look at what Weibullmodels can offer.", "num_citations": "1289\n", "authors": ["445"]}
{"title": "Stochastic ageing and dependence for reliability\n", "abstract": " Ageing and dependence are two important characteristics in reliability and survival analysis, and they affect significantly the decision people make with regard to maintenance, repair/replacement, price setting, warranties, medical studies, and other areas. There are many papers published at different technical levels. This book aims at providing a state-of-the-art review of the subject so the interested readers may have a panoramic view of the theory and applications of the two areas. This book serves as reference book for professors and researchers involved in reliability and survival analysis. Students with basic probability and statistics knowledge interested in applications will also find the book useful. From the reviews:\" This book is an important addition to several well-written books on reliability theory and concepts. The book fulfills the authors aim in providing a comprehensive treatment of both ageing and dependence concepts, with emphasis on reliability and survival analysis.... Overall, I recommend this book to all serious-minded researchers, practitioners, teachers, and graduate students who would like to have the most up-to-date information available on the subject of stochastic ageing and dependencies in reliability. It also serves as reference book on reliability and survival analysis and is a good addition to one's statistical or reliability library.\" Suprasad V. Amari, International Journal of Performability Engineering, Vol. 3, No. 2, April 2007", "num_citations": "670\n", "authors": ["445"]}
{"title": "A modified Weibull distribution\n", "abstract": " A new lifetime distribution capable of modeling a bathtub-shaped hazard-rate function is proposed. The proposed model is derived as a limiting case of the Beta Integrated Model and has both the Weibull distribution and Type 1 extreme value distribution as special cases. The model can be considered as another useful 3-parameter generalization of the Weibull distribution. An advantage of the model is that the model parameters can be estimated easily based on a Weibull probability paper (WPP) plot that serves as a tool for model identification. Model characterization based on the WPP plot is studied. A numerical example is provided and comparison with another Weibull extension, the exponentiated Weibull, is also discussed. The proposed model compares well with other competing models to fit data that exhibits a bathtub-shaped hazard-rate function.", "num_citations": "544\n", "authors": ["445"]}
{"title": "A modified Weibull extension with bathtub-shaped failure rate function\n", "abstract": " Models with bathtub-shaped failure rate function are useful in reliability analysis, and particularly in reliability related decision making and cost analysis. The traditional Weibull distribution is, however, unable to model the complete lifetime of systems with a bathtub-shaped failure rate function. In this paper, a new model, which is useful for modeling this type of failure rate function, is presented. The model can also be seen as a generalization of the Weibull distribution. Parameter estimation methods are studied for this new distribution. Examples and results of comparison are shown to illustrate the applicability of this new model.", "num_citations": "510\n", "authors": ["445"]}
{"title": "Fuzzy assessment of FMEA for engine systems\n", "abstract": " When performing failure mode and effects analysis (FMEA) for quality assurance and reliability improvement, interdependencies among various failure modes with uncertain and imprecise information are very difficult to be incorporated for failure analysis. Consequently, the validity of the results may be questionable. This paper presents a fuzzy-logic-based method for FMEA to address this issue. A platform for a fuzzy expert assessment is integrated with the proposed system to overcome the potential difficulty in sharing information among experts from various disciplines. The FMEA of diesel engine's turbocharger system is presented to illustrate the feasibility of such techniques.", "num_citations": "509\n", "authors": ["445"]}
{"title": "Reliability analysis using an additive Weibull model with bathtub-shaped failure rate function\n", "abstract": " Lifetime distributions for many components usually have a bathtub-shaped failure rate in practice. However, there are very few practical models to model this type of failure rate function. In this paper we study a simple model based on adding two Weibull survival functions. Some simplifications of the model are also presented. The graphical estimation technique based on the conventional Weibull plot is demonstrated to be useful in this case.", "num_citations": "488\n", "authors": ["445"]}
{"title": "An integrated approach to innovative product development using Kano\u2019s model and QFD\n", "abstract": " Under rapidly changing and highly competitive circumstances, the timely design, development and marketing of new products or services with creative and innovative features are essential for a company\u2019s survival. In order to capture and retain market share, customer requirements and expectations should be met and exceeded through product innovation. For effective new\u2010product\u2010development project management, a systematic approach to understanding customer requirements and further embedding them into the future product is desirable. This paper analyses the notion of customer satisfaction based on the Kano model and points to the importance of product innovation in exceeding customer satisfaction. It further proposes an integrated process model for innovative product development by incorporating Kano\u2019s model and the quality function deployment (QFD) technique. Analyses suggest that the proposed\u00a0\u2026", "num_citations": "477\n", "authors": ["445"]}
{"title": "Stochastic modelling and analysis of degradation for highly reliable products\n", "abstract": " Degradation models have become an important analytic tool for complex systems. During the last two decades, a number of degradation models have been developed to capture the degradation dynamics of a system and aid the subsequent decision\u2010makings. This paper is aimed at providing a summary of the state of the arts in the field, and discussing some further research issues from both analytical and practical point of view. In this paper, degradation models are classified into three classes, that is, stochastic process models, general path models, and other models beyond these two classes. A review on the three classes is given with emphasis on the class of stochastic process models. A comprehensive comparison between stochastic process models and general path models is given to expound the pros and cons of these two methods. Applications of degradation models in degradation test planning and burn\u00a0\u2026", "num_citations": "447\n", "authors": ["445"]}
{"title": "A comparative study of neural network and Box-Jenkins ARIMA modeling in time series prediction\n", "abstract": " This paper aims to investigate suitable time series models for repairable system failure analysis. A comparative study of the Box-Jenkins autoregressive integrated moving average (ARIMA) models and the artificial neural network models in predicting failures are carried out. The neural network architectures evaluated are the multi-layer feed-forward network and the recurrent network. Simulation results on a set of compressor failures showed that in modeling the stochastic nature of reliability data, both the ARIMA and the recurrent neural network (RNN) models outperform the feed-forward model; in terms of lower predictive errors and higher percentage of correct reversal detection. However, both models perform better with short term forecasting. The effect of varying the damped feedback weights in the recurrent net is also investigated and it was found that RNN at the optimal weighting factor gives satisfactory\u00a0\u2026", "num_citations": "356\n", "authors": ["445"]}
{"title": "Improving on the Six Sigma paradigm\n", "abstract": " Since its inception more than a decade ago, six sigma as a quality improvement framework has been gaining increasing attention and acceptance in industry. Thus performance in both manufacturing and service operations can now be calibrated in terms of \u201csigma level\u201d, and companies eager to impress customers have begun to label themselves \u201csix sigma organizations\u201d. In this paper, a realistic view is taken of the six sigma framework, with an examination of the basis of six sigma and its long-term potential. It is argued that in the dynamic business environment of the twenty-first century, a forwardlooking organization should aim beyond the six sigma benchmark; thus additional requirements are recommended to fortify the common six sigma approach, leading to an \u201ceight-S\u201d paradigm for sustained excellence in performance.", "num_citations": "335\n", "authors": ["445"]}
{"title": "Application of neural networks in forecasting engine systems reliability\n", "abstract": " This paper presents a comparative study of the predictive performances of neural network time series models for forecasting failures and reliability in engine systems. Traditionally, failure data analysis requires specifications of parametric failure distributions and justifications of certain assumptions, which are at times difficult to validate. On the other hand, the time series modeling technique using neural networks provides a promising alternative. Neural network modeling via feed-forward multilayer perceptron (MLP) suffers from local minima problems and long computation time. The radial basis function (RBF) neural network architecture is found to be a viable alternative due to its shorter training time. Illustrative examples using reliability testing and field data showed that the proposed model results in comparable or better predictive performance than traditional MLP model and the linear benchmark based on Box\u00a0\u2026", "num_citations": "289\n", "authors": ["445"]}
{"title": "Bayesian networks in fault diagnosis\n", "abstract": " Fault diagnosis is useful in helping technicians detect, isolate, and identify faults, and troubleshoot. Bayesian network (BN) is a probabilistic graphical model that effectively deals with various uncertainty problems. This model is increasingly utilized in fault diagnosis. This paper presents bibliographical review on use of BNs in fault diagnosis in the last decades with focus on engineering systems. This work also presents general procedure of fault diagnosis modeling with BNs; processes include BN structure modeling, BN parameter modeling, BN inference, fault identification, validation, and verification. The paper provides series of classification schemes for BNs for fault diagnosis, BNs combined with other techniques, and domain of fault diagnosis with BN. This study finally explores current gaps and challenges and several directions for future research.", "num_citations": "271\n", "authors": ["445"]}
{"title": "Computing system reliability: models and analysis\n", "abstract": " Computing systems are of growing importance because of their wide use in many areas including those in safety-critical systems. This book describes the basic models and approaches to the reliability analysis of such systems. An extensive review is provided and models are categorized into different types. Some Markov models are extended to the analysis of some specific computing systems such as combined software and hardware, imperfect debugging processes, failure correlation, multi-state systems, heterogeneous subsystems, etc. One of the aims of the presentation is that based on the sound analysis and simplicity of the approaches, the use of Markov models can be better implemented in the computing system reliability.", "num_citations": "269\n", "authors": ["445"]}
{"title": "The use of ARIMA models for reliability forecasting and analysis\n", "abstract": " This paper investigates the approach to repairable system reliability forecasting based on the Autoregressive Integrated Moving Average (ARIMA) models. This time series technique makes very few assumptions and is very flexible. It is theoretically and statistically sound in its foundation and no a priori postulation of models is required when analysing failure data. An illustrative example on a mechanical system failures is presented. Comparison is also made with the traditional Duane model. It is concluded that ARIMA model is a viable alternative that gives satisfactory results in terms of its predictive performance.", "num_citations": "256\n", "authors": ["445"]}
{"title": "Some effective control chart procedures for reliability monitoring\n", "abstract": " Control charts are widely used for process monitoring in the manufacturing industry. Little research is available on their use to monitor the failure process of components or systems, which is important for equipment performance monitoring. Some Shewhart control charts, especially those for the number of defects, can be used for monitoring the number of failures per fixed interval; however, they are not effective especially when the failure frequency becomes small. A recent control scheme based on the cumulative quantity between observations of defects has been proposed which can be easily adopted to monitor the failure process for exponentially distributed inter-failure time. An investigation of its use for reliability monitoring is presented in this paper and the scheme can be easily extended to monitor inter-failure times that follow other distributions such as the Weibull distribution. Furthermore, the scheme is\u00a0\u2026", "num_citations": "247\n", "authors": ["445"]}
{"title": "A data-driven fault diagnosis methodology in three-phase inverters for PMSM drive systems\n", "abstract": " Permanent magnet synchronous motor and power electronics-based three-phase inverter are the major components in the modern industrial electric drive system, such as electrical actuators in an all-electric subsea Christmas tree. Inverters are the weakest components in the drive system, and power switches are the most vulnerable components in inverters. Fault detection and diagnosis of inverters are extremely necessary for improving drive system reliability. Motivated by solving the uncertainty problem in fault diagnosis of inverters, which is caused by various reasons, such as bias and noise of sensors, this paper proposes a Bayesian network-based data-driven fault diagnosis methodology of three-phase inverters. Two output line-to-line voltages for different fault modes are measured, the signal features are extracted using fast Fourier transform, the dimensions of samples are reduced using principal\u00a0\u2026", "num_citations": "239\n", "authors": ["445"]}
{"title": "A study of project selection and feature weighting for analogy based software cost estimation\n", "abstract": " A number of software cost estimation methods have been presented in literature over the past decades. Analogy based estimation (ABE), which is essentially a case based reasoning (CBR) approach, is one of the most popular techniques. In order to improve the performance of ABE, many previous studies proposed effective approaches to optimize the weights of the project features (feature weighting) in its similarity function. However, ABE is still criticized for the low prediction accuracy, the large memory requirement, and the expensive computation cost. To alleviate these drawbacks, in this paper we propose the project selection technique for ABE (PSABE) which reduces the whole project base into a small subset that consist only of representative projects. Moreover, PSABE is combined with the feature weighting to form FWPSABE for a further improvement of ABE. The proposed methods are validated on four\u00a0\u2026", "num_citations": "231\n", "authors": ["445"]}
{"title": "A real-time fault diagnosis methodology of complex systems using object-oriented Bayesian networks\n", "abstract": " Bayesian network (BN) is a commonly used tool in probabilistic reasoning of uncertainty in industrial processes, but it requires modeling of large and complex systems, in situations such as fault diagnosis and reliability evaluation. Motivated by reduction of the overall complexities of BNs for fault diagnosis, and the reporting of faults that immediately occur, a real-time fault diagnosis methodology of complex systems with repetitive structures is proposed using object-oriented Bayesian networks (OOBNs). The modeling methodology consists of two main phases: an off-line OOBN construction phase and an on-line fault diagnosis phase. In the off-line phase, sensor historical data and expert knowledge are collected and processed to determine the faults and symptoms, and OOBN-based fault diagnosis models are developed subsequently. In the on-line phase, operator experience and sensor real-time data are placed\u00a0\u2026", "num_citations": "200\n", "authors": ["445"]}
{"title": "A systematic comparison of metamodeling techniques for simulation optimization in decision support systems\n", "abstract": " Simulation is a widely applied tool to study and evaluate complex systems. Due to the stochastic and complex nature of real world systems, simulation models for these systems are often difficult to build and time consuming to run. Metamodels are mathematical approximations of simulation models, and have been frequently used to reduce the computational burden associated with running such simulation models. In this paper, we propose to incorporate metamodels into Decision Support Systems to improve its efficiency and enable larger and more complex models to be effectively analyzed with Decision Support Systems. To evaluate the different metamodel types, a systematic comparison is first conducted to analyze the strengths and weaknesses of five popular metamodeling techniques (Artificial Neural Network, Radial Basis Function, Support Vector Regression, Kriging, and Multivariate Adaptive Regression\u00a0\u2026", "num_citations": "189\n", "authors": ["445"]}
{"title": "Cumulative quantity control charts for monitoring production processes\n", "abstract": " Two commonly used statistical quality control charts, the c-chart and u-chart, are unsatisfactory for monitoring high-yield processes with low defect rates. To overcome this difficulty, a new type of control chart called the cumulative quantity control chart (CQC-chart) is introduced in this paper. The CQC-chart can be used no matter whether the process defect rate is low or not, and when the process defect rate is low or moderate the CQC-chart does not have the shortcoming of the c- and u-charts of showing up false alarm signals too frequently. The CQC-chart does not require rational subgrouping of samples (which is necessary for the c- and u-charts), and is appropriate for monitoring automated manufacturing processes.", "num_citations": "188\n", "authors": ["445"]}
{"title": "A study of operational and testing reliability in software reliability analysis\n", "abstract": " Software reliability is an important aspect of any complex equipment today. Software reliability is usually estimated based on reliability models such as nonhomogeneous Poisson process (NHPP) models. Software systems are improving in testing phase, while it normally does not change in operational phase. Depending on whether the reliability is to be predicted for testing phase or operation phase, different measure should be used. In this paper, two different reliability concepts, namely, the operational reliability and the testing reliability, are clarified and studied in detail. These concepts have been mixed up or even misused in some existing literature. Using different reliability concept will lead to different reliability values obtained and it will further lead to different reliability-based decisions made. The difference of the estimated reliabilities is studied and the effect on the optimal release time is investigated.", "num_citations": "180\n", "authors": ["445"]}
{"title": "The implementation of quality function deployment based on linguistic data\n", "abstract": " Quality function deployment (QFD) is a customer-driven quality management and product development system for achieving higher customer satisfaction. The QFD process involves various inputs in the form of linguistic data, e.g., human perception, judgment, and evaluation on importance or relationship strength. Such data are usually ambiguous and uncertain. An aim of this paper is to examine the implementation of QFD under a fuzzy environment and to develop corresponding procedures to deal with the fuzzy data. It presented a process model using linguistic variables, fuzzy arithmetic, and defuzzification techniques. Based on an example, this paper further examined the sensitivity of the ranking of technical characteristics to the defuzzification strategy and the degree of fuzziness of fuzzy numbers. Results indicated that selection of the defuzzification strategy and membership function are important. This\u00a0\u2026", "num_citations": "172\n", "authors": ["445"]}
{"title": "A study of the effect of imperfect debugging on software development cost\n", "abstract": " It is widely recognized that the debugging processes are usually imperfect. Software faults are not completely removed because of the difficulty in locating them or because new faults might be introduced. Hence, it is of great importance to investigate the effect of the imperfect debugging on software development cost, which, in turn, might affect the optimal software release time or operational budget. In this paper, a commonly used cost model is extended to the case of imperfect debugging. Based on this, the effect of imperfect debugging is studied. As the probability of perfect debugging, termed testing level here, is expensive to be increased, but manageable to a certain extent with additional resources, a model incorporating this situation is presented. Moreover, the problem of determining the optimal testing level is considered. This is useful when the decisions regarding the test team composition, testing strategy\u00a0\u2026", "num_citations": "170\n", "authors": ["445"]}
{"title": "Statistical models and control charts for high-quality processes\n", "abstract": " Control charts are widely used in industry to monitor processes that are far from Zero-Defect (ZD), and their use in a near Zero-Defect manufacturing environment poses many problems. This book presents techniques of using control charts for high-quality processes, and some recent findings and applications of statistical control chart techniques for ZD processes are presented. A powerful technique based on counting of the cumulative conforming (CCC) items between two nonconforming ones is discussed in detail. Extensions of the CCC chart are described, as well as applications of cumulative sum and exponentially weighted moving average techniques to CCC-related data, multivariate methods, economic design of control chart procedures, and modeling and analysis of trended but regularly adjusted processes. Many examples, charts, and procedures, are presented throughout the book, and references are provided for those interested in exploring the details. A number of questions and issues are posed for further investigations. Researchers and students may find many ideas in this book useful in their academic work, as a foundation is laid for the exploration of many further theoretical and practical issues.", "num_citations": "164\n", "authors": ["445"]}
{"title": "Fundamentals of robotics: linking perception to action\n", "abstract": " Tomorrow's robots, which includes the humanoid robot, can perform task like tutoring children, working as tour guides, driving humans to and from work, do the family shopping etc. Tomorrow's robots will enhance lives in ways we never dreamed possible. No time to attend the decisive meeting on Asian strategy? Let your robot go for you and make the decisions. Not feeling well enough to go to the clinic? Let Dr Robot come to you, make a diagnosis, and get you the necessary medicine for treatment. No time to coach the soccer team this week? Let the robot do it for you. Tomorrow's robots will be the most exciting and revolutionary things to happen to the world since the invention of the automobile. It will change the way we work, play, think, and live. Because of this, nowadays robotics is one of the most dynamic fields of scientific research. These days, robotics is offered in almost every university in the world. Most mechanical engineering departments offer a similar course at both the undergraduate and graduate levels. And increasingly, many computer and electrical engineering departments are also offering it. This book will guide you, the curious beginner, from yesterday to tomorrow. The book will cover practical knowledge in understanding, developing, and using robots as versatile equipment to automate a variety of industrial processes or tasks. But, the book will also discuss the possibilities we can look forward to when we are capable of creating a vision-guided, learning machine. Readership: Upper-level undergraduates, graduates and researchers in robotics &automated systems, artificial intelligence, machine perception and computer vision.", "num_citations": "163\n", "authors": ["445"]}
{"title": "On the solution of renewal-type integral equations\n", "abstract": " A simple method (here called the RS-method) established for solving renewal-type integral equations based on direct numerical Riemann-Stieltjes integration is presented and evaluated. In almost all situations it has shown surprisingly good results in terms of simplicity, convergence and applicability compared with the other known methods. The RS-method is particularly useful when the probability density function has singularities.", "num_citations": "161\n", "authors": ["445"]}
{"title": "Accelerated degradation test planning using the inverse Gaussian process\n", "abstract": " The IG process models have been shown to be an important family in degradation analysis. In this paper, we are interested in optimal constant-stress accelerated degradation tests (ADTs) planning when the underlying degradation follows the inverse Gaussian (IG) process. We first consider ADT planning for the IG process without random effects. Asymptotic variance of the estimate of a lower quantile is derived, and the objective of the planning is to minimize this variance by properly choosing the testing stresses, and the number of samples allocated to each stress. Next, ADT planning for a random-effects IG process model is considered. We then applied the IG process to fit the stress relaxation data of a component, and use the developed methods to help with the optimal ADT design.", "num_citations": "160\n", "authors": ["445"]}
{"title": "Advanced QFD applications\n", "abstract": " This book focuses on the collection, interpretation, and analysis of the voice of the customers (VOC) and serves as an excellent reference or textbook for learning how to apply QFD. Following this unique approach for capturing the VOC will ensure your product/service meets their needs. Included is a discussion of recent advances in QFD methodology, methods for strategically analyzing and selecting benchmarks, and examples through case studies. Contents:-Introduction to Quality Function Deployment-Decision Making Using the House of Quality-Variability Analysis in QFD-QFD for Service Quality Analysis-The Implementation of QFD-based Linguistic Data-Benchmarking in QFD for Quality Improvement", "num_citations": "153\n", "authors": ["445"]}
{"title": "A dynamic-Bayesian-network-based fault diagnosis methodology considering transient and intermittent faults\n", "abstract": " Transient fault (TF) and intermittent fault (IF) of complex electronic systems are difficult to diagnose. As the performance of electronic products degrades over time, the results of fault diagnosis could be different at different times for the given identical fault symptoms. A dynamic Bayesian network (DBN)-based fault diagnosis methodology in the presence of TF and IF for electronic systems is proposed. DBNs are used to model the dynamic degradation process of electronic products, and Markov chains are used to model the transition relationships of four states, i.e., no fault, TF, IF, and permanent fault. Our fault diagnosis methodology can identify the faulty components and distinguish the fault types. Four fault diagnosis cases of the Genius modular redundancy control system are investigated to demonstrate the application of this methodology.", "num_citations": "152\n", "authors": ["445"]}
{"title": "Robust recurrent neural network modeling for software fault detection and correction prediction\n", "abstract": " Software fault detection and correction processes are related although different, and they should be studied together. A practical approach is to apply software reliability growth models to model fault detection, and fault correction process is assumed to be a delayed process. On the other hand, the artificial neural networks model, as a data-driven approach, tries to model these two processes together with no assumptions. Specifically, feedforward backpropagation networks have shown their advantages over analytical models in fault number predictions. In this paper, the following approach is explored. First, recurrent neural networks are applied to model these two processes together. Within this framework, a systematic networks configuration approach is developed with genetic algorithm according to the prediction performance. In order to provide robust predictions, an extra factor characterizing the dispersion of\u00a0\u2026", "num_citations": "141\n", "authors": ["445"]}
{"title": "A comparative study of the prioritization matrix method and the analytic hierarchy process technique in quality function deployment\n", "abstract": " As a tool to identify customer needs and translate customer requirements into technical responses, the house of quality (HOQ) has become one of the most useful techniques in total quality management, especially in the context of quality function deployment (QFD). Recently, the analytic hierarchy process (AHP) has been recommended for prioritizing the customer voice in HOQ. In this paper, we recommend that AHP be applied not only to the customer voice, but also directly to the technical responses. Since the decision-maker can select either the prioritization matrix method or the AHP method for HOQ applications, an understanding of their advantages and disadvantages becomes important. In this paper, we compare these two methods in terms of practical applications in industry leading to the general conclusion that if time, cost and difficulty are the major concerns in product improvement, the\u00a0\u2026", "num_citations": "140\n", "authors": ["445"]}
{"title": "Planning of step-stress accelerated degradation test\n", "abstract": " Estimating the long term performance of highly reliable products has been a difficult problem as accelerated life testing (ALT), which involves testing at highly elevated stresses, often results in too few failures for drawing useful inferences. To overcome this problem, accelerated degradation testing (ADT) has been proposed as a means to predict performance for highly reliable products. It requires one to identify a performance measure that would exhibit degradation and to monitor it over time. Product reliability can then be inferred from the degradation paths without the need of observing actual failures. Although physical failures are not needed in ADT, one usually defines failure as the first time when the degradation process exceeds a pre-specified threshold, so that the degradation path can be correlated to product reliability. As a result, reliability information of a product is embedded in degradation paths of units\u00a0\u2026", "num_citations": "129\n", "authors": ["445"]}
{"title": "Dealing with subjectivity in early product design phase: A systematic approach to exploit Quality Function Deployment potentials\n", "abstract": " Quality Function Deployment (QFD), as a customer-driven tool, is generally used in the early phase of new or improved products/services design process, and therefore most of the input parameters are highly subjective in nature. The five major input components of the QFD, which are laid in the House of Quality (HOQ), namely, the customer requirement, the technical attribute, the relationship matrix, the correlation matrix, and the benchmarking information, play a central role in determining the success of QFD team. Accurate numerical judgment representations are of high importance for the QFD team to fill in the values of each of those components. In this paper, a generic network model, based on Analytic Network Process (ANP) framework, will be proposed to systematically take into account the interrelationship between and within those components simultaneously and finally derive their relative contribution. In\u00a0\u2026", "num_citations": "128\n", "authors": ["445"]}
{"title": "Availability and reliability of k-out-of-(M+ N): G warm standby systems\n", "abstract": " Redundancy or standby is a technique that has been widely applied to improving system reliability and availability in system design. In most cases, components in standby system are assumed to be statistically identical and independent. However, in many practical applications, not all components in standby can be treated as identical because they have different failure and repair rates. In this paper, one kind of such systems with two categories of components is studied, which is named k-out-of-(M+N):G warm standby system. In the system, one category of the components is of type 1 and the other type 2. There are M type 1 components and N type 2 components. Components of type 1 have a lower failure rate and are preferably repaired if there is one failed. There are r repair facilities available. By using Markov model, the system state transition process can be clearly illustrated, and furthermore, the solutions of\u00a0\u2026", "num_citations": "128\n", "authors": ["445"]}
{"title": "An empirical analysis of data preprocessing for machine learning-based software cost estimation\n", "abstract": " ContextDue to the complex nature of software development process, traditional parametric models and statistical methods often appear to be inadequate to model the increasingly complicated relationship between project development cost and the project features (or cost drivers). Machine learning (ML) methods, with several reported successful applications, have gained popularity for software cost estimation in recent years. Data preprocessing has been claimed by many researchers as a fundamental stage of ML methods; however, very few works have been focused on the effects of data preprocessing techniques.ObjectiveThis study aims for an empirical assessment of the effectiveness of data preprocessing techniques on ML methods in the context of software cost estimation.MethodIn this work, we first conduct a literature survey of the recent publications using data preprocessing techniques, followed by a\u00a0\u2026", "num_citations": "124\n", "authors": ["445"]}
{"title": "A study of service reliability and availability for distributed systems\n", "abstract": " Distributed systems are usually designed and developed to provide certain important services such as in computing and communication systems. In this paper, a general model is presented for a centralized heterogeneous distributed system, which is widely used in distributed system design. Based on this model, the distributed service reliability which is defined as the probability of successfully providing the service in a distributed environment, an important performance measure for this type of systems, is investigated. An application example is used to illustrate the procedure. Furthermore, with the help of the model, various issues such as the release time to achieve a service reliability requirement, and the sensitivity of model parameters are studied. This type of analysis is important in the application of this type of models.", "num_citations": "124\n", "authors": ["445"]}
{"title": "Degradation-based burn-in with preventive maintenance\n", "abstract": " As many products are becoming increasingly more reliable, traditional lifetime-based burn-in approaches that try to fail defective units during the test require a long burn-in duration, and thus are not effective. Therefore, we promote the degradation-based burn-in approach that bases the screening decision on the degradation level of a burnt-in unit. Motivated by the infant mortality faced by many Micro-Electro-Mechanical Systems (MEMSs), this study develops two degradation-based joint burn-in and maintenance models under the age and the block based maintenances, respectively. We assume that the product population comprises a weak and a normal subpopulations. Degradation of the product follows Wiener processes with linear drift, while the weak and the normal subpopulations possess distinct drift parameters. The objective of joint burn-in and maintenance decisions is to minimize the long run average\u00a0\u2026", "num_citations": "123\n", "authors": ["445"]}
{"title": "A study of the modeling and analysis of software fault\u2010detection and fault\u2010correction processes\n", "abstract": " Most of the models for software reliability analysis are based on reliability growth models which deal with the fault detection process. This is done either by assuming that faults are corrected immediately after being detected or the time to correct a fault is not counted. Some models have been developed to relax this assumption. However, unlike the fault\u2010detection process, few published data sets are available to support the modeling and analysis of both the fault detection and removal processes. In this paper, some useful approaches to the modeling of both software fault\u2010detection and fault\u2010correction processes are discussed. Further analysis on the software release time decision that incorporates both a fault\u2010detection model and fault\u2010correction model is also presented. This procedure is easy to use and useful for practical applications. The approach is illustrated with an actual set of data from a software\u00a0\u2026", "num_citations": "123\n", "authors": ["445"]}
{"title": "Availability-based engineering resilience metric and its corresponding evaluation methodology\n", "abstract": " Several resilience metrics have been proposed for engineering systems (e.g., mechanical engineering, civil engineering, critical infrastructure, etc.); however, they are different from one another. Their difference is determined by the performances of the objects of evaluation. This study proposes a new availability-based engineering resilience metric from the perspective of reliability engineering. Resilience is considered an intrinsic ability and an inherent attribute of an engineering system. Engineering system structure and maintenance resources are principal factors that affect resilience, which are integrated into the engineering resilience metric. A corresponding dynamic-Bayesian-network-based evaluation methodology is developed on the basis of the proposed resilience metric. The resilience value of an engineering system can be predicted using the proposed methodology, which provides an implementation\u00a0\u2026", "num_citations": "120\n", "authors": ["445"]}
{"title": "A study of the connectionist models for software reliability prediction\n", "abstract": " When analysing software failure data, many software reliability models are available and in particular, nonhomogeneous Poisson process (NHPP) models are commonly used. However, difficulties posed by the assumptions, their validity, and relevance of these assumptions to the real testing environment have limited their usefulness. The connectionist approach using neural network models are more flexible and with less restrictive assumptions. This model-free technique requires only the failure history as inputs and then develops its own internal model of failure process. Their ability to model nonlinear patterns and learn from the data makes it a valuable alternative methodology for characterising the failure process. In this paper, a modified Elman recurrent neural network in modeling and predicting software failures is investigated. The effects of different feedback weights in the proposed model are also studied. A\u00a0\u2026", "num_citations": "120\n", "authors": ["445"]}
{"title": "A comparative study of nine national quality awards\n", "abstract": " Ever since the Malcolm Baldrige National Quality Award (MBNQA) was established in 1987, many other countries have developed their own version of a national quality award (NQA). These NQAs tend to follow the general framework of the MBNQA with different emphases on criteria items such as leadership, customer focus, resource management and impact on society. This paper is a comparative study of nine major national quality awards (three European, two North American, three Asia Pacific and one South American). It is instructive to note the differences in criteria item emphasis based on a country\u2019s stage of economic development. Multinational companies may find it very useful when their overseas subsidiaries apply for the local NQA following the success of their home companies. Countries that have yet to develop an NQA stand to gain from the comparative information gathered.", "num_citations": "120\n", "authors": ["445"]}
{"title": "Modeling and analysis of software fault detection and correction process by considering time dependency\n", "abstract": " Software reliability modeling & estimation plays a critical role in software development, particularly during the software testing stage. Although there are many research papers on this subject, few of them address the realistic time delays between fault detection and fault correction processes. This paper investigates an approach to incorporate the time dependencies between the fault detection, and fault correction processes, focusing on the parameter estimations of the combined model. Maximum likelihood estimates of combined models are derived from an explicit likelihood formula under various time delay assumptions. Various characteristics of the combined model, like the predictive capability, are also analyzed, and compared with the traditional least squares estimation method. Furthermore, we study a direct, useful application of the proposed model & estimation method to the classical optimal release time\u00a0\u2026", "num_citations": "119\n", "authors": ["445"]}
{"title": "Zero-inflated Poisson model in statistical process control\n", "abstract": " Poisson distribution has often been used for count related data. However, this model does not provide a good fit to actual data when there is a frequent or excessive number of zero counts. For example, in a near zero-defect manufacturing environment, there are many zero-defect counts even for fairly large sample size. In such a situation, the zero-inflated Poisson distribution is more appropriate. In this paper, the use of this distribution is investigated. In particular, various tests of Poisson distribution and zero-inflated Poisson alternative are compared. When the model is used in statistical process control, control limits can then be derived based on the zero-inflated Poisson model when the Poisson distribution is rejected in favour of the zero-inflated Poisson alternative. Furthermore, sensitivity analysis of such a control chart is also presented.", "num_citations": "118\n", "authors": ["445"]}
{"title": "Ch. 3. bathtub-shaped failure rate life distributions\n", "abstract": " The concept of aging is very important in reliability theory.'No aging'means the age of a component has no effect on the distribution of residual lifetime.'Positive aging'describes the situation where residual lifetime tends to decrease, in some probabilistic sense, with increasing age of component. On the other hand,'negative aging'has an opposite effect on the residual lifetime. In reliability, we often characterize a lifetime distribution through the following three functions:", "num_citations": "117\n", "authors": ["445"]}
{"title": "Modeling and analysis of correlated software failures of multiple types\n", "abstract": " Most software reliability models assume independence of successive software runs. It is a strict assumption, and usually not valid in reality. Goseva-Popstojanova & Trivedi (2000) presented an interesting study on failure correlation among successive software runs. In this paper, by extending their results, a software reliability model is developed based on a Markov renewal process for the modeling of the dependence among successive software runs, where more than one type of failure is allowed in general formulation. Meanwhile, the cases of restarting with repair, and without repair, are considered. Although such a model is more complex than the traditional approach based on reliability growth, it incorporates more information about the failures, and system structure. A numerical example is also shown to illustrate the procedure, and provide some comparison.", "num_citations": "115\n", "authors": ["445"]}
{"title": "The Schneidewind software reliability model revisited\n", "abstract": " A software reliability model based on a nonhomogeneous Poisson process (NHPP) was proposed by NF Schneidewind (Sigplan Notices, vol. 10, p. 337, 1975). Since then, many other NHPP models have been suggested and studied by various authors. The authors show that several NHPP models can be derived based on the general assumptions made by Schneidewind. Also, they note that in the original paper, there are several interesting approaches worth further consideration. To mention a few, Schneidewind modelled both the software correction process and the software detection process. Methods of weighted least squares were adopted together with the software reliability forecasting based on previous measurements. In this paper, the Schneidewind model is revisited and some further research result are presented.<>", "num_citations": "114\n", "authors": ["445"]}
{"title": "Optimal testing-resource allocation with genetic algorithm for modular software systems\n", "abstract": " In software testing, an important issue is to allocate the limited testing resources to achieve maximum reliability. There are numerous publications on this issue, but the models are usually developed under the assumption of simple series or parallel modules. For complex system configuration, the optimization problem becomes difficult to solve. In this paper, we present a genetic algorithm for testing-resource allocation problems that can be used when the software systems structure is complex, and also when there are multiple objectives. We consider both system reliability and testing cost in the testing-resource allocation problems. The approach is easily implemented. Some numerical examples are shown to illustrate the applicability of the approach.", "num_citations": "113\n", "authors": ["445"]}
{"title": "Iterative list scheduling for heterogeneous computing\n", "abstract": " Optimal scheduling of parallel applications on distributed computing systems represented by directed acyclic graph (DAG) is NP-complete in the general case. List scheduling is a very popular heuristic method for DAG-based scheduling. However, it is more suited to homogenous distributed computing systems. This paper presents an iterative list scheduling algorithm to deal with scheduling on heterogeneous computing systems. The main idea in this iterative scheduling algorithm is to improve the quality of the schedule in an iterative manner using results from previous iterations. The algorithm first uses the heterogeneous earliest-finish-time (HEFT) algorithm to find an initial schedule and iteratively improves it. Hence the algorithm can potentially produce shorter schedule length. The simulation results show that in the majority of the cases, there is significant improvement to the initial schedule. The algorithm is also\u00a0\u2026", "num_citations": "112\n", "authors": ["445"]}
{"title": "On the performance of geometric charts with estimated control limits\n", "abstract": " The control chart based on the geometric distribution (geometric chart) has been shown to be competitive with p- or np-charts for monitoring the proportion nonconforming, especially for applications in high quality manufacturing environments. However, implementing a geometric chart is often based on the assumption that the in-control proportion nonconforming is known or accurately estimated. For a high quality process, an accurate parameter estimate may require a very large sample size that is seldom available. In this paper we investigate the sample size effect when the proportion nonconforming is estimated. An analytical approximation is derived to compute shift detection probabilities and run length distributions. It is found that the effect on the alarm probability can be significant even with sample sizes as large as 10,000. However, the average run length is only affected mildly unless the sample size is small\u00a0\u2026", "num_citations": "112\n", "authors": ["445"]}
{"title": "Degradation-based maintenance decision using stochastic filtering for systems under imperfect maintenance\n", "abstract": " The notion of imperfect maintenance has spawned a large body of literature, and many imperfect maintenance models have been developed. However, there is very little work on developing suitable imperfect maintenance models for systems outfitted with sensors. Motivated by the practical need of such imperfect maintenance models, the broad objective of this paper is to propose an imperfect maintenance model that is applicable to systems whose sensor information can be modeled by stochastic processes. The proposed imperfect maintenance model is founded on the intuition that maintenance actions will change the rate of deterioration of a system, and that each maintenance action should have a different degree of impact on the rate of deterioration. The corresponding parameter-estimation problem can be divided into two parts: the estimation of fixed model parameters and the estimation of the impact of each\u00a0\u2026", "num_citations": "111\n", "authors": ["445"]}
{"title": "A model for availability analysis of distributed software/hardware systems\n", "abstract": " System availability is a major performance concern in distributed systems design and analysis. A typical kind of application on distributed systems has a homogeneously distributed software/hardware structure. That is, identical copies of distributed application software run on the same type of computers. In this paper, the system availability for this type of system is studied. Such a study is useful when studying optimal testing time or testing resource allocation. We consider both the case of simple two-host system, and also the more general case of multi-host system. A Markov model is developed and equations are derived to obtain the steady-state availability. Both software and hardware failures are considered, assuming that software faults are constantly being identified and removed upon a failure. Although a specific model for software reliability is used for illustration, the approach is a general one. Comparisons\u00a0\u2026", "num_citations": "111\n", "authors": ["445"]}
{"title": "Reliability analysis and optimal version-updating for open source software\n", "abstract": " ContextAlthough reliability is a major concern of most open source projects, research on this problem has attracted attention only recently. In addition, the optimal version-dating for open source software considering its special properties is not yet discussed.ObjectiveIn this paper, the reliability analysis and optimal version-updating for open source software are studied.MethodA modified non-homogeneous Poisson process model is developed for open source software reliability modeling and analysis. Based on this model, optimal version-updating for open source software is investigated as well. In the decision process, the rapid release strategy and the level of reliability are the two most important factors. However, they are essentially contradicting with each other. In order to consider these two conflicting factors simultaneously, a new decision model based on multi-attribute utility theory is proposed.ResultsOur\u00a0\u2026", "num_citations": "109\n", "authors": ["445"]}
{"title": "A study of mutual information based feature selection for case based reasoning in software cost estimation\n", "abstract": " Software cost estimation is one of the most crucial activities in software development process. In the past decades, many methods have been proposed for cost estimation. Case based reasoning (CBR) is one of these techniques. Feature selection is an important preprocessing stage of case based reasoning. Most existing feature selection methods of case based reasoning are \u2018wrappers\u2019 which can usually yield high fitting accuracy at the cost of high computational complexity and low explanation of the selected features. In our study, the mutual information based feature selection (MICBR) is proposed. This approach hybrids both \u2018wrapper\u2019 and \u2018filter\u2019 mechanism which is another kind of feature selector with much lower complexity than wrappers, and the features selected by filters are likely to be generalized to other conditions. The MICBR is then compared with popular feature selectors and the published works. The\u00a0\u2026", "num_citations": "108\n", "authors": ["445"]}
{"title": "On maximum likelihood estimation for a general non-homogeneous Poisson process\n", "abstract": " Non-homogeneous Poisson processes (NHPPs) have been widely used in the study of software reliability. The statistical analysis for NHPPs is of interest to both theoreticians and practitioners. In this paper, maximum likelihood estimation under time-truncated sampling is studied for parametric NHPP software reliability models with bounded mean value functions. It is shown that the maximum likelihood estimators need not be consistent or asymptotically normal. The asymptotic distribution is derived for a specific NHPP model.", "num_citations": "108\n", "authors": ["445"]}
{"title": "Benchmarking in QFD for quality improvement\n", "abstract": " Through listening to the voice of the customer, quality function deployment (QFD) is a systematic methodology for quality improvement and product development. The quality of a product or service is ultimately judged in terms of customer satisfaction. Customer satisfaction benchmarking can help decision makers identify areas for improvement, make strategic decisions, and set targets on desired satisfaction performance. The main purpose of this paper is to study procedures and methods for successful benchmarking in QFD for quality improvement. It discussed the customer satisfaction benchmarking process in QFD and proposed the use of hierarchical benchmarks for strategic competitor selection and decision making. A case study was presented to illustrate the use of this method. This paper may provide a road map to achieve world\u2010class performance through benchmarking in QFD, especially for small to\u00a0\u2026", "num_citations": "105\n", "authors": ["445"]}
{"title": "A service quality framework for web-based information systems\n", "abstract": " High quality customer service is the key to enhancing a company's competitiveness. The Internet provides a dynamic and distributed platform for interactive business applications. There is, however, an increasing need to develop a framework to identify elements of superior Web-based service quality, to measure online customer satisfaction, and to achieve a high level of service quality. This paper proposes a broad framework for web-based service quality measurement. It begins by discussing issues related to measuring Web-based service quality. Variation among the myriad types of users is taken into account in developing the framework. The approach taken is one of developing better measures of marketing constructs. It is argued that the framework provides an effective procedure to assess web-based service quality.", "num_citations": "104\n", "authors": ["445"]}
{"title": "Ranking of customer requirements in a competitive environment\n", "abstract": " Quality function deployment (QFD) has been widely used to translate customer requirements to a product\u2019s technical attributes. Correctly rating the importance of every customer requirement is essential to the QFD process because it will largely affect the final target value of a product\u2019s technical attributes. This paper proposes a new customer requirements ranking method that considers competitors\u2019 information. In today\u2019s competitive environment, there are usually several products to fulfill certain functions. The success of a product depends not only on whether it meets the customer requirements, but also on how it compares with competitor products. Most previous methods focus only on the customer perspective, and ignore the competitive environment. The proposed method considers competition position, current performance and customers\u2019 viewpoint to produce the ratings. In addition, this method uses fuzzy\u00a0\u2026", "num_citations": "102\n", "authors": ["445"]}
{"title": "A control chart for the Gamma distribution as a model of time between events\n", "abstract": " In this paper, control charts for monitoring exponentially distributed time between events (TBE) are studied. In particular, a Gamma chart which monitors the time until the rth event is proposed and investigated. A new method based on a random-shift model for calculating the out-of-control average time to signal (ATS) of the Gamma chart is developed. It is shown to be much more accurate than the conventional method based on a fixed-shift model through comparing with Monte Carlo simulation. A comparison is also made among the exponential, the Gamma and the exponential CUSUM charts, which shows that the Gamma chart is more sensitive than the exponential chart and the performance of a Gamma chart with r\u2009=\u20094 is comparable with that of an exponential CUSUM optimally designed. However, the advantage of the Gamma chart is the ease involved in the design, evaluation and implementation. The use of\u00a0\u2026", "num_citations": "102\n", "authors": ["445"]}
{"title": "A new family of positive quadrant dependent bivariate distributions\n", "abstract": " Positive quadrant dependence (PQD) is a notion of bivariate dependence between two positively dependent random variables. Starting from the uniform representation of the Farlie\u2013Gumbel\u2013Morgenstern bivariate distribution, we derive and study a family of continuous bivariate distributions that possesses the PQD property. In particular, we show that this new parametric family of distributions can be ordered in the so-called \u201cPQD order\u201d.", "num_citations": "102\n", "authors": ["445"]}
{"title": "On the upper truncated Weibull distribution and its reliability implications\n", "abstract": " The characteristics and application of the truncated Weibull distribution are studied in this paper. This distribution is applicable to the situation where the test data are bounded in an interval because of test conditions, cost and other restrictions. An important property of the truncated Weibull distribution is that it can have bathtub-shaped failure rate function. In this paper, the parametric analysis and parameter estimation methods of the distribution are investigated. Both the graphical approach and the maximum likelihood estimation are considered. The applicability of this distribution to modeling lifetime data is illustrated by an example and the results of comparisons to other competitive models in modeling the given data are also presented. Moreover, the possible application of the distribution to modeling component or system failure is discussed.", "num_citations": "101\n", "authors": ["445"]}
{"title": "Some procedures for decision making in controlling high yield processes\n", "abstract": " Statistical process control based on the cumulative counts of conforming items instead of occurrences of non\u2010conformances has proved to be useful for the manufacture of high\u2010quality products. In this paper we study some interesting and useful issues of using cumulative counts in practice. The general problem in the control of high\u2010quality products is first discussed, which leads to the calculation of the probability that a process is out of control when a single non\u2010conforming item is detected. A decision graph is introduced with which we can easily judge whether the process is out of control when a non\u2010conforming item is observed after a number of conforming ones, accompanied by the certainty level of this judgement. Next, using the equivalence of information on one non\u2010conforming item found in a number of inspected items to that on zero non\u2010conforming items in a smaller sample, we give a method for\u00a0\u2026", "num_citations": "101\n", "authors": ["445"]}
{"title": "A value-based preventive maintenance policy for multi-component system with continuously degrading components\n", "abstract": " A dynamic preventive maintenance policy for system with continuously degrading components is investigated in this paper. Different from traditional cost-centric preventive maintenance policy, our maintenance strategy is formulated from the value perspective. Component value is modelled as a function of component reliability distribution. Maintenance action is triggered whenever the system reliability drops below a certain threshold. Our policy mainly consists of two steps: (i) determine which component to maintain; (ii) determine to what degree the component should be maintained. In Step 1, we introduce the yield-cost importance to select the most important component. In Step 2, the optimal maintenance level is obtained by maximizing the net value of the maintenance action. Finally, numerical examples are given to illustrate the proposed policy.", "num_citations": "100\n", "authors": ["445"]}
{"title": "Software failure prediction based on a Markov Bayesian network model\n", "abstract": " Due to the complexity of software products and development processes, software reliability models need to possess the ability of dealing with multiple parameters. Also in order to adapt to the continually refreshed data, they should provide flexibility in model construction in terms of information updating. Existing software reliability models are not flexible in this context. The main reason for this is that there are many static assumptions associated with the models. Bayesian network is a powerful tool for solving this problem, as it exhibits strong ability to adapt in problems involving complex variant factors. In this paper, a software prediction model based on Markov Bayesian networks is developed, and a method to solve the network model is proposed. The use of our model is illustrated with an example.", "num_citations": "100\n", "authors": ["445"]}
{"title": "The use of probability limits for process control based on geometric distribution\n", "abstract": " Control charts based on geometric distribution have shown to be useful when this is a better approximation of the underlying distribution than the Poisson distribution. The traditional c\u2010chart, if used, will cause too many false alarms. It is noted that for geometric distribution, the control limits are based on k times standard deviation which has been used previously, will cause a frequent false alarm, and cannot derive any reasonable lower control limits. Studies the use of probability limits to resolve these problems. Also discusses the use of geometric distribution for process control of high\u2010yield processes.", "num_citations": "100\n", "authors": ["445"]}
{"title": "Semiparametric estimation of gamma processes for deteriorating products\n", "abstract": " This article investigates the semiparametric inference of the simple Gamma-process model and a random-effects variant. Maximum likelihood estimates of the parameters are obtained through the EM algorithm. The bootstrap is used to construct confidence intervals. A simulation study reveals that an estimation based on the full likelihood method is more efficient than the pseudo likelihood method. In addition, a score test is developed to examine the existence of random effects under the semiparametric scenario. A comparison study using a fatigue-crack growth dataset shows that performance of a semiparametric estimation is comparable to the parametric counterpart. This article has supplementary material online.", "num_citations": "96\n", "authors": ["445"]}
{"title": "Statistical techniques for quality\n", "abstract": " When objective decisions are to be made, statistical methods should be used based on any objective information in the form of data collected about a product or process. Statistical techniques such as control charts, process capability indices and design of experiments have been used in the manufacturing industry for many years. There are a number of practical and managerial issues related to the application of statistical techniques in studies aimed at improving process and product quality. This paper is a summary of the thoughts and discussions from a recent Internet conference on this issue. Statistical process control techniques and their role in process improvement are first discussed and some issues related to the interpretation and use of experimental design techniques are also summarised. The focus will be on continuous quality improvement using statistical techniques.", "num_citations": "94\n", "authors": ["445"]}
{"title": "A study of the non-linear adjustment for analogy based software cost estimation\n", "abstract": " Cost estimation is one of the most important but most difficult tasks in software project management. Many methods have been proposed for software cost estimation. Analogy Based Estimation (ABE), which is essentially a case-based reasoning (CBR) approach, is one popular technique. To improve the accuracy of ABE method, several studies have been focusing on the adjustments to the original solutions. However, most published adjustment mechanisms are based on linear forms and are restricted to numerical type of project features. On the other hand, software project datasets often exhibit non-normal characteristics with large proportions of categorical features. To explore the possibilities for a better adjustment mechanism, this paper proposes Artificial Neural Network (ANN) for Non-linear adjustment to ABE (NABE) with the learning ability to approximate complex relationships and incorporating the\u00a0\u2026", "num_citations": "93\n", "authors": ["445"]}
{"title": "Redefining failure rate function for discrete distributions\n", "abstract": " For discrete distribution with reliability function R(k), k = 1, 2,\u2026,[R(k - 1) - R(k)]/R(k - 1) has  been used as the definition of the failure rate function in the literature.  However, this is different from that of the continuous case. This discrete version  has the interpretation of a probability while it is known that a failure rate is not  a probability in the continuous case. This discrete failure rate is bounded, and hence cannot be convex, e.g., it cannot grow linearly. It is not additive for series system while the additivity for series system is a common understanding in practice. In the paper, another definition of discrete failure rate function as In[R(k - 1)/R(k)]  is introduced, and the above-mentioned problems are resolved. On the other hand,  it is shown that the two failure rate definitions have the same monotonicity property. That is, if one is increasing/decreasing, the other is also increasing/decreasing. For other aging concepts, the\u00a0\u2026", "num_citations": "92\n", "authors": ["445"]}
{"title": "On the effect of redundancy for systems with dependent components\n", "abstract": " Parallel redundancy is a common approach to increase system reliability and mean time to failure. When studying systems with redundant components, it is usually assumed that the components are independent; however, this assumption is seldom valid in practice. In the case of dependent components, the effectiveness of adding a component may be quite different from the case of independent components. In this paper we investigate how the degree of correlation affects the increase in the mean lifetime for parallel redundancy when the two components are positively quadrant dependent. A number of bivariate distributions that can be used in the modeling of dependent components are compared. Various bounds are also derived. The results are useful in reliability analysis as well as for designers who are required to take into account the possible dependence among the components.", "num_citations": "91\n", "authors": ["445"]}
{"title": "A study of two estimation approaches for parameters of Weibull distribution based on WPP\n", "abstract": " Least-squares estimation (LSE) based on Weibull probability plot (WPP) is the most basic method for estimating the Weibull parameters. The common procedure of this method is using the least-squares regression of Y on X, i.e. minimizing the sum of squares of the vertical residuals, to fit a straight line to the data points on WPP and then calculate the LS estimators. This method is known to be biased. In the existing literature the least-squares regression of X on Y, i.e. minimizing the sum of squares of the horizontal residuals, has been used by the Weibull researchers. This motivated us to carry out this comparison between the estimators of the two LS regression methods using intensive Monte Carlo simulations. Both complete and censored data are examined. Surprisingly, the result shows that LS Y on X performs better for small, complete samples, while the LS X on Y performs better in other cases in view of bias of\u00a0\u2026", "num_citations": "89\n", "authors": ["445"]}
{"title": "Availability of a periodically inspected system with random repair or replacement times\n", "abstract": " The availability of systems undergoing periodic inspections is studied in this paper. A perfect repair or replacement of a failed system is carried out requiring either a constant or a random length of time. In Model A, the system is assumed to be as good as new on completion of inspection or repair. For Model B, no maintenance or corrective actions are taken at the time of inspection if the system is still working, and the condition of the system is assumed to be the same as that before the inspection. Unlike that studied in a related paper by Sarkar and Sarkar (J. Statist. Plann. Inference 91 (2000) 77.), our model assumes that the periodic inspections take place at fixed time points after repair or replacement in case of failure. Some general results on the instantaneous availability and the steady-state availability for the two models are presented under the assumption of random repair or replacement time.", "num_citations": "89\n", "authors": ["445"]}
{"title": "Failure data analysis with extended Weibull distribution\n", "abstract": " A three-parameter distribution called extended Weibull distribution is investigated in this article. This model is generated by a method of introducing an additional parameter into a family of distributions by Marshall and Olkin . It has two-parameter Weibull distribution as a special case. One of the merits of this distribution is that the hazard-rate can be increasing, decreasing, or initially increasing, then decreasing and eventually increasing. The model characterization based on the Weibull Probability Plot (WPP) is studied in this article. The WPP for actual data set can be concave, convex, or likely S-shaped. A procedure is provided for parameter estimation based on WPP. In addition, the maximum likelihood estimation is also presented. An example is shown to illustrate the procedure and application.", "num_citations": "88\n", "authors": ["445"]}
{"title": "Quality dimensions of Internet search engines\n", "abstract": " Searching for information from the Internet today has been made easier by the                     widely available search engines. They will become more and more important for                     our daily life as our society enters the information age. However, there are                     many search engines and their number is increasing. It is of considerable                     importance for the designer to develop quality search engines and for the users                     to select the most appropriate ones for their use. In fact, most search engines                     are developed mainly for better technical performance and there could be a lack                     of quality attributes from the customers\u2019 perspective. In this paper,                     we first provide a brief review of the most commonly used search engines, with                     the focus on the existing comparative studies of the search engines. Then, the                     quality dimensions of Internet search\u00a0\u2026", "num_citations": "87\n", "authors": ["445"]}
{"title": "A fuzzy TOPSIS and rough set based approach for mechanism analysis of product infant failure\n", "abstract": " Root causes identification of product infant failure is nowadays one of the critical topics in product quality improvements. This paper puts forward a novel technical approach for mechanism analysis of product infant failure based on domain mapping in Axiomatic Design and the quality and reliability data from product lifecycle in the form of relational tree. The proposed method could intelligently decompose the early fault symptoms into root causes of critical functional parameters in function domain, design parameters in physical domain and process parameters in process domain successively. More specifically, both qualitative and quantitative attributes of quality and reliability types are considered for solving the root causes weight computation problem of product infant failure, this approach emphasizes the integrated application of artificial intelligence techniques of Rough Set and fuzzy TOPSIS to compute the\u00a0\u2026", "num_citations": "86\n", "authors": ["445"]}
{"title": "Grid service reliability modeling and optimal task scheduling considering fault recovery\n", "abstract": " There has been quite some research on the development of tools and techniques for grid systems, yet some important issues, e.g., grid service reliability and task scheduling in the grid, have not been sufficiently studied. For some grid services which have large subtasks requiring time-consuming computation, the reliability of grid service could be rather low. To resolve this problem, this paper introduces Local Node Fault Recovery (LNFR) mechanism into grid systems, and presents an in-depth study on grid service reliability modeling and analysis with this kind of fault recovery. To make LNFR mechanism practical, some constraints, i.e. the life times of subtasks, and the numbers of recoveries performed in grid nodes, are introduced; and grid service reliability models under these practical constraints are developed. Based on the proposed grid service reliability model, a multi-objective task scheduling optimization\u00a0\u2026", "num_citations": "86\n", "authors": ["445"]}
{"title": "Bias correction for the least squares estimator of Weibull shape parameter with complete and censored data\n", "abstract": " Estimation of the Weibull shape parameter is important in reliability engineering. However, commonly used methods such as the maximum likelihood estimation (MLE) and the least squares estimation (LSE) are known to be biased. Bias correction methods for MLE have been studied in the literature. This paper investigates the methods for bias correction when model parameters are estimated with LSE based on probability plot. Weibull probability plot is very simple and commonly used by practitioners and hence such a study is useful. The bias of the LS shape parameter estimator for multiple censored data is also examined. It is found that the bias can be modeled as the function of the sample size and the censoring level, and is mainly dependent on the latter. A simple bias function is introduced and bias correcting formulas are proposed for both complete and censored data. Simulation results are also presented\u00a0\u2026", "num_citations": "86\n", "authors": ["445"]}
{"title": "Impact of Six Sigma implementation on stock price performance\n", "abstract": " There have been many reported successful cases of Six Sigma implementation in the past few years. As stock price performance is one of the realistic criteria for business performance, two studies are presented in this paper in this regard. One is stock prices' reaction on the day when Six Sigma activities were made known publicly, and the other is the long-run stock performance of \"Six Sigma companies'. The abnormal returns on the event day are evaluated in terms of three models, namely mean adjusted, market adjusted and market models. A full sample consists of 20 announcements, and two sub-samples with four announcements and 16 announcements each were analysed. The result shows that the abnormal returns are not significant on the event day. A study on six firms shows that, in the long-run, stock performance of Six Sigma companies did not significantly outperform S&P 500. This analysis of\u00a0\u2026", "num_citations": "86\n", "authors": ["445"]}
{"title": "Dynamic availability assessment and optimal component design of multi-state weighted k-out-of-n systems\n", "abstract": " Availability/reliability is a main feature of design and operation of all engineering systems. Recently, availability evaluation of multi-state systems with different structures is at the center of attention due to the wide applications in engineering. In this paper, a dynamic model is developed for the availability assessment of multi-state weighted k-out-of-n systems. Then, in a design optimization problem, the availability and capacity for the components of such systems are optimized by genetic algorithm. In the dynamic model, the probabilities and capacities of components in different states are allowed to be changed over time. For availability assessment, universal generating function and Markov process are adopted. Application of the proposed model is illustrated using a real-world marine transportation system in order to evaluate and compare the presented optimization problems in assessing system availability.", "num_citations": "85\n", "authors": ["445"]}
{"title": "A condition-based maintenance policy for degrading systems with age-and state-dependent operating cost\n", "abstract": " Most of the maintenance policies in existing publications assume that no cost is incurred as long as the system can undertake missions while little consideration has been devoted to the operating cost during system operation. However, in practice, the operating cost increases while the system ages and degrades even if a system is in a functioning state. This paper proposes a maintenance policy for a degrading system with age- and state-dependent operating cost, which increases with system age and degradation levels. Under such a setting, a replacement model is first developed to investigate the optimal preventive replacement policy. The replacement model is then extended to a repair-replacement model, in which imperfect repair is assumed to restore the system to the operating condition. Particularly, the repair model with controllable and uncontrollable repair levels is considered separately. The paper\u00a0\u2026", "num_citations": "84\n", "authors": ["445"]}
{"title": "Sensitivity analysis of release time of software reliability models incorporating testing effort with multiple change-points\n", "abstract": " To accurately model software failure process with software reliability growth models, incorporating testing effort has shown to be important. In fact, testing effort allocation is also a difficult issue, and it directly affects the software release time when a reliability criteria has to be met. However, with an increasing number of parameters involved in these models, the uncertainty of parameters estimated from the failure data could greatly affect the decision. Hence, it is of importance to study the impact of these model parameters. In this paper, sensitivity of the software release time is investigated through various methods, including one-factor-at-a-time approach, design of experiments and global sensitivity analysis. It is shown that the results from the first two methods may not be accurate enough for the case of complex nonlinear model. Global sensitivity analysis performs better due to the consideration of the global\u00a0\u2026", "num_citations": "84\n", "authors": ["445"]}
{"title": "An imperfect maintenance policy for mission-oriented systems subject to degradation and external shocks\n", "abstract": " This paper develops a maintenance model for mission-oriented systems subject to natural degradation and external shocks. For mission-oriented systems which are used to perform safety-critical tasks, maintenance actions need to satisfy a range of constraints such as availability/reliability, maintenance duration and the opportunity of maintenance. Additionally, in developing maintenance policy, one needs to consider the natural degradation due to aging and wearing along with the external shocks due to variations of the operating environment. In this paper, the natural degradation is modeled as a Wiener process and the arrival of random shock as a homogeneous Poisson process. The damage caused by shocks is integrated into the degradation process, according to the cumulative shock model. Improvement factor model is used to characterize the impact of maintenance actions on system restoration. Optimal\u00a0\u2026", "num_citations": "83\n", "authors": ["445"]}
{"title": "Some improvements on adaptive genetic algorithms for reliability-related applications\n", "abstract": " Adaptive genetic algorithms (GAs) have been shown to be able to improve GA performance in reliability-related optimization studies. However, there are different ways to implement adaptive GAs, some of which are even in conflict with each other. In this study, a simple parameter-adjusting method using mean and variance of each generation is introduced. This method is used to compare two of such conflicting adaptive GA methods: GAs with increasing mutation rate and decreasing crossover rate and GAs with decreasing mutation rate and increasing crossover rate. The illustrative examples indicate that adaptive GAs with decreasing mutation rate and increasing crossover rate finally yield better results. Furthermore, a population disturbance method is proposed to avoid local optimum solutions. This idea is similar to exotic migration to a tribal society. To solve the problem of large solution space, a variable\u00a0\u2026", "num_citations": "81\n", "authors": ["445"]}
{"title": "Reliability analysis of grid computing systems\n", "abstract": " Grid computing system is different from conventional distributed computing systems by its focus on large-scale resource sharing, where processors and communication have significant influence on grid computing reliability. Most previous research on conventional small-scale distributed systems ignored the communication time and processing time when studying the distributed program reliability, which is not practical in the analysis of grid computing systems. This paper describes the property of the grid computing systems and presents algorithms to analyze the grid program and system reliability.", "num_citations": "81\n", "authors": ["445"]}
{"title": "Listening to the future voice of the customer using fuzzy trend analysis in QFD\n", "abstract": " We use cookies to improve your website experience. To learn about our use of cookies and how you can manage your cookie settings, please see our Cookie Policy. By closing this message, you are consenting to our use of cookies.", "num_citations": "81\n", "authors": ["445"]}
{"title": "Accident risk assessment in marine transportation via Markov modelling and Markov Chain Monte Carlo simulation\n", "abstract": " There are many technological and environmental safety factors involved in marine accidents. This paper deals with an analytic approach to accident risk modelling when data for analyzing safety factors is limited or unavailable. The purpose of this paper is to propose a simulated accident model for assessing accident risk in marine transportation. The proposed approach is based on Markov modelling and Markov Chain Monte Carlo (MCMC) simulation and it is illustrated using an example from marine transportation. A three-state continuous time Markov model is used to record and estimate marine occurrence rates and probabilities. The MCMC simulation requires the occurrence data of the Markov model to estimate the accident risk. However, it can be used when only a limited amount of information is available. Compared with other models, the approach in this paper is applicable to any type of marine accident or\u00a0\u2026", "num_citations": "79\n", "authors": ["445"]}
{"title": "Weibull distributions and their applications\n", "abstract": " Weibull models are used to describe various types of observed failures of components and phenomena. They are widely used in reliability and survival analysis. In addition to the traditional two-parameter and three-parameter Weibull distributions in the reliability or statistics literature, many other Weibull-related distributions are available. The purpose of this chapter is to give a brief introduction to those models, with the emphasis on models that have the potential for further applications. After introducing the traditional Weibull distribution, some historical development and basic properties are presented. We also discuss estimation problems and hypothesis-testing issues, with the emphasis on graphical methods. Many extensions and generalizations of the basic Weibull distributions are then summarized. Various applications in the reliability context and some Weibull densitydistribution functionempirical modelingestimationfailure rate (FR) graphicalhazard plothistoricalhypothesis testingmomentsreliabilityWeibull probability plotWeibull distributionWeibull derived analysis software are also provided.", "num_citations": "79\n", "authors": ["445"]}
{"title": "A methodology to improve higher education quality using the quality function deployment and analytic hierarchy process\n", "abstract": " In order to formulate an effective strategic plan in a customer-driven education context, it is important to recognize who the customers are and what they want. Using Quality Function Deployment (QFD), this information can be translated into strategies to achieve customer satisfaction. Since the final strategic plan relies heavily on the way QFD is used, this paper will first describe the existing problems in its use and then propose a better way to improve it. In this paper, the customers are divided into two major parties, namely, the internal and the external customer. The internal customer comprises of the lecturers and the students, while the external customer is the employers of the graduates. After collecting the Voice of Customer (VOC), the Analytic Hierarchy Process (AHP) technique was employed to generate the priorities of the VOC for each group of customers. Then, the results were used as the input for\u00a0\u2026", "num_citations": "78\n", "authors": ["445"]}
{"title": "Prioritizing quality characteristics in dynamic quality function deployment\n", "abstract": " Due to the combination of rapid influx of new technology, high pressure on time-to-market and increasing globalization, the number of products that have highly uncertain and dynamic specifications or customer requirements might significantly increase. In order to deal with these inherently volatile products or services, we need to adopt a more pro-active approach in order not to produce an unwanted product or service. Thus, based on the idea of the quality loss function and the zero-one goal programming, an intuitively simple mathematical model is developed to prioritize the quality characteristics (QCs) in the dynamic quality function deployment (QFD). It incorporates a pro-active approach towards providing products and services that meet the future voice of the customer (FVOC). The aim is to determine and prioritize only the \u2018important\u2019 QCs with a greater confidence in meeting the FVOC. It is particularly useful\u00a0\u2026", "num_citations": "78\n", "authors": ["445"]}
{"title": "On some reliability measures and their stochastic orderings for the Topp\u2013Leone distribution\n", "abstract": " Topp\u2013Leone distribution is a continuous unimodal distribution with bounded support (recently rediscovered) which is useful for modelling life-time phenomena. In this paper we study some reliability measures of this distribution such as the hazard rate, mean residual life, reversed hazard rate, expected inactivity time, and their stochastic orderings.", "num_citations": "77\n", "authors": ["445"]}
{"title": "Optimizing product design using the Kano model and QFD\n", "abstract": " In this paper, an approach combining the Kano model and quality function deployment (QFD) is proposed to meet customer requirements in product design. The Kano model provides an effective way to categorizing customer requirements and helps understand the nature of these requirements. The Kano model categories the customer satisfaction into three fields, namely, \"must be\", \"attractive\" and \"exciting\". QFD is a customer-focused product design method. It can transfer customer requirements to products' engineering characteristics. By combining the two methods, we can provide a new way to optimize the product design. The proposed methods can be useful to both practitioners and researchers.", "num_citations": "77\n", "authors": ["445"]}
{"title": "Statistical control of a six sigma process\n", "abstract": " Six Sigma as a methodology for quality improvement is often presented and deployed in terms of the dpmo metric, i.e., defects per million opportunities. As the sigma level of a process improves beyond three, practical interpretation problems could arise when conventional Shewhart control charts are applied during the Control phase of the define-measure-analyze-improve-control framework. In this article, some alternative techniques are described for the monitoring and control of a process that has been successfully improved; the techniques are particularly useful to Six Sigma Black Belts in dealing with high-quality processes. The approach used would thus ensure a smooth transition from a low-sigma process management to maintenance of a high-sigma performance in the closing phase of a Six Sigma project.", "num_citations": "77\n", "authors": ["445"]}
{"title": "A mixture of variational canonical correlation analysis for nonlinear and quality-relevant process monitoring\n", "abstract": " Proper monitoring of quality-related variables in industrial processes is nowadays one of the main worldwide challenges with significant safety and efficiency implications.Variational Bayesian mixture of canonical correlation analysis (VBMCCA)-based process monitoring method was proposed in this paper to predict and diagnose these hard-to-measure quality-related variables simultaneously. Use of Student's t-distribution, rather than Gaussian distribution, in the VBMCCA model makes the proposed process monitoring scheme insensitive to disturbances, measurement noises, and model discrepancies. A sequential perturbation (SP) method together with derived parameter distribution of VBMCCA is employed to approach the uncertainty levels, which is able to provide a confidence interval around the predicted values and give additional control line, rather than just a certain absolute control limit, for process\u00a0\u2026", "num_citations": "76\n", "authors": ["445"]}
{"title": "Uncertainty analysis in software reliability modeling by bayesian analysis with maximum-entropy principle\n", "abstract": " In software reliability modeling, the parameters of the model are typically estimated from the test data of the corresponding component. However, the widely used point estimators are subject to random variations in the data, resulting in uncertainties in these estimated parameters. Ignoring the parameter uncertainty can result in grossly underestimating the uncertainty in the total system reliability. This paper attempts to study and quantify the uncertainties in the software reliability modeling of a single component with correlated parameters and in a large system with numerous components. Another characteristic challenge in software testing and reliability is the lack of available failure data from a single test, which often makes modeling difficult. This lack of data poses a bigger challenge in the uncertainty analysis of the software reliability modeling. To overcome this challenge, this paper proposes utilizing experts'\u00a0\u2026", "num_citations": "76\n", "authors": ["445"]}
{"title": "Multiresponse systems optimization using a goal attainment approach\n", "abstract": " A goal attainment approach to optimize multiresponse systems is presented. This approach aims to identify the settings of control factors to minimize the overall weighted maximal distance measure with respect to individual response targets. Based on a nonlinear programming technique, a sequential quadratic programming algorithm, the method is proved to be robust and can achieve good performance for multiresponse optimization problems with multiple conflicting goals. Moreover, the optimization formulation may include some prior work as special cases by assigning proper response targets and weights. Fewer assumptions are needed when using the approach as compared to other techniques. Furthermore, the decision-maker's preference and the model's predictive ability can easily be incorporated into the weights' adjustment schemes with explicit physical interpretation. The proposed approach is\u00a0\u2026", "num_citations": "76\n", "authors": ["445"]}
{"title": "Design of exponential control charts using a sequential sampling scheme\n", "abstract": " Control charts for monitoring the time between events can be applied in various areas. In this study, we focus on the exponential control chart and consider the phase II problem (when process parameters are known) as well as the phase I problem (when process parameters are unknown). An exponential chart designed with the conventional approach has the disadvantage that the Average Run Length (ARL) value may increase when the process deviates from the nominal state. An ARL-unbiased design approach is therefore proposed for both phase II and phase I exponential charts. A sequential sampling scheme is adopted for the phase I exponential chart. The proposed ARL-unbiased design approach has several advantages over the conventional one, as it provides a self-starting feature and can significantly improve the ARL performance. Specific guidelines are suggested regarding the time to stop updating\u00a0\u2026", "num_citations": "75\n", "authors": ["445"]}
{"title": "Service quality of Internet search engines\n", "abstract": " With the development of information technology, search engines are widely used                     as tools to find useful information from the Internet. However, most search                     engines were developed on the basis of technical requirements and without much                     consideration for the customer\u2019s perspective. The purpose of this                     study is to consider quality issues of Internet search engines and hence help                     the designers to improve quality from the customer\u2019s point of view.                     Based on the service dimensions and factors studied in a previous paper, a                     series of statistical analyses were carried out. Major problems were studied,                     together with suggestions for further improvement. Although each search engine                     has its own strong and weak points, the important quality factors identified in                     this paper should be very helpful not only to the\u00a0\u2026", "num_citations": "75\n", "authors": ["445"]}
{"title": "Software reliability models-past, present and future\n", "abstract": " As an important branch of reliability, software reliability is of growing importance in today\u2019s society because of the widespread application of software systems. In this paper, some important software reliability models arereviewed. Some influential earlier models are reviewed, and some recent ones with the focus on their potential uses and further research prospect are also discussed. It can be noted that as many probability models are being explored, there is a need for model selection, model validation and developing models that can make use of earlier information. Furthermore, the use of models for various decision-making in software development and the effect of various assumptions on these decisions have a great potential to be explored.", "num_citations": "74\n", "authors": ["445"]}
{"title": "Cost analysis for multi-component system with failure interaction under renewing free-replacement warranty\n", "abstract": " In a multi-component system, the assumption of failure independence among components is seldom valid, especially for those complex systems with complicated failure mechanism. For such systems, warranty cost is subject to all the factors including system configuration, quality of each component and the extent of failure dependence among components. In this paper, a model is developed based on renewing free-replacement warranty by considering failure interaction among components. It is assumed that whenever a component (subsystem) fails, it can induce a failure of one or more of the remaining components (subsystems). Cost models for series and parallel system configurations are presented, followed by numerical examples with sensitivity analysis. The results show that, compared with series systems, warranty cost for parallel systems is more sensitive to failure interaction.", "num_citations": "73\n", "authors": ["445"]}
{"title": "An integrated framework for risk response planning under resource constraints in large engineering projects\n", "abstract": " Engineering project managers often face a challenge to allocate tight resources for managing interdependent risks. In this paper, a quantitative framework of analysis for supporting decision making in project risk response planning is developed and studied. The design structure matrix representation is used to capture risk interactions and build a risk propagation model for predicting the global mitigation effects of risk response actions. For exemplification, a genetic algorithm is used as a tool for choosing response actions and allocating budget reserves. An application to a real transportation construction project is also presented. Comparison with a sequential forward selection greedy algorithm shows the superiority of the genetic algorithm search for optimal solutions, and its flexibility for balancing mitigation effects and required budget.", "num_citations": "73\n", "authors": ["445"]}
{"title": "Optimal allocation of minimal & perfect repairs under resource constraints\n", "abstract": " The effect of a repair of a complex system can usually be approximated by the following two types: minimal repair for which the system is restored to its functioning state with minimum effort, or perfect repair for which the system is replaced or repaired to a good-as-new state. When both types of repair are possible, an important problem is to determine the repair policy; that is, the type of repair which should be carried out after a failure. In this paper, an optimal allocation problem is studied for a monotonic failure rate repairable system under some resource constraints. In the first model, the numbers of minimal & perfect repairs are fixed, and the optimal repair policy maximizing the expected system lifetime is studied. In the second model, the total amount of repair resource is fixed, and the costs of each minimal & perfect repair are assumed to be known. The optimal allocation algorithm is derived in this case. Two\u00a0\u2026", "num_citations": "73\n", "authors": ["445"]}
{"title": "Degradation-based burn-in planning under competing risks\n", "abstract": " Motivated by two real-life examples, this article develops a burn-in planning framework with competing risks. Existing approaches to planning burn-in tests are confined to a single failure mode based on the assumption that this failure mode is subject to infant mortality. Considering the prevalence of competing risks and the high reliability of modern products, our framework differentiates between normal and infant mortality failure modes and recommends degradation-based burn-in approaches. This framework is employed to guide the burn-in planning for an electronic device subject to both a degradation-threshold failure, which is an infant mortality mode and can be modeled by a gamma process with random effect, and a catastrophic mode, which is normal and can be represented with a conventional reliability model. Three degradation-based burn-in models are built and the optimal cutoff degradation levels are\u00a0\u2026", "num_citations": "72\n", "authors": ["445"]}
{"title": "Accident analysis model based on Bayesian Network and Evidential Reasoning approach\n", "abstract": " In this paper, an accident analysis model is proposed to develop the cost-efficient safety measures for preventing accidents. The model comprises two parts. In the first part, a quantitative accident analysis model is built by integrating Human Factors Analysis and Classification System (HFACS) with Bayesian Network (BN), which can be utilized to present the corresponding prevention measures. In the second part, the proposed prevention measures are ranked in a cost-effectiveness manner through Best-Fit method and Evidential Reasoning (ER) approach. A case study of vessel collision is analyzed as an illustration. The case study shows that the proposed model can be used to seek out accident causes and rank the derived safety measures from a cost-effectiveness perspective. The proposed model can provide accident investigators with a tool to generate cost-efficient safety intervention strategies.", "num_citations": "70\n", "authors": ["445"]}
{"title": "A quality monitoring and decision\u2010making scheme for automated production processes\n", "abstract": " Traditional attribute control charts are based on the monitoring of the number of nonconforming items in a sample of fixed size. In modern manufacturing processes, since items can be checked automatically, use of samples of a fixed subjective size with traditional charts is not suitable for on\u2010line continuous inspection. Furthermore, the sample size usually has to be large when the process fraction nonconforming is not reasonably high. If a decision is to be made only when a sufficient number of items are manufactured and inspected, many nonconforming items might have been produced. In this paper, a control scheme is presented based on the monitoring of cumulative counts of items inspected. This procedure will limit the number of consecutive nonconforming items to a small value when the process has suddenly deteriorated, can detect a sudden process shift quickly and is suitable for continuous inspection\u00a0\u2026", "num_citations": "70\n", "authors": ["445"]}
{"title": "A Bayesian approach for system reliability analysis with multilevel pass-fail, lifetime and degradation data sets\n", "abstract": " Reliability analysis of complex systems is a critical issue in reliability engineering. Motivated by practical needs, this paper investigates a Bayesian approach for system reliability assessment and prediction with multilevel heterogeneous data sets. Two major imperatives have been handled in the proposed approach, which provides a comprehensive Bayesian framework for the integration of multilevel heterogeneous data sets. In particular, the pass-fail data, lifetime data, and degradation data at different system levels are combined coherently for system reliability analysis. This approach goes beyond the alternatives that deal with solely multilevel pass-fail or lifetime data, and presents a more practical tool for real engineering applications. In addition, the indices for reliability assessment and prediction are constructed coherently within the proposed Bayesian framework. It gives rise to a natural manner of\u00a0\u2026", "num_citations": "68\n", "authors": ["445"]}
{"title": "On optimal setting of control limits for geometric chart\n", "abstract": " Control charts based on geometric distribution have shown to  be very useful in the monitoring of high yield manufacturing processes and other applications. It is well known that the traditional 3-sigma limits will give too many false alarms and the probability limits should be used. This paper shows that the average time to alarm may even increase at the beginning when the process is deteriorated. A new procedure is established for the setting of control limits so that the average run length is maximized when the process is at the normal level. Hence the chart sensitivity can be improved. For the derivation of the control limits in this new procedure, a simple adjustment factor is suggested so that the probability limits can be used after the adjustment.", "num_citations": "67\n", "authors": ["445"]}
{"title": "A systematic methodology to deal with the dynamics of customer needs in Quality Function Deployment\n", "abstract": " In the context of a customer-driven product or service design process, a timely update of customer needs information may not only serve as a useful indicator to observe how things change over time, but it also provides the company a better ground to formulate strategies to meet the future needs of its customer. This paper proposes a systematic methodology to deal with customer needs\u2019 dynamics, in terms of their relative weights, in the QFD. Compared to previous research, its contribution is three-fold. First, it proposes the use of a forecasting technique which is effective to model the dynamics of Analytic Hierarchy Process (AHP) based importance rating. This is owing to the fact that the AHP has been applied very extensively in the QFD and there is, unfortunately, almost no tool to model the dynamics. Second, it describes more comprehensively on how future uncertainty in the weights of customer needs may be\u00a0\u2026", "num_citations": "66\n", "authors": ["445"]}
{"title": "Development of innovative products using Kano's model and quality function deployment\n", "abstract": " Quality and innovation are two main issues involved in  product development. When faced with increasingly intense  competition from both national and international competitors,  organisations usually consider quality and innovation as sources  of competitive advantage. Customer satisfaction can be met and  exceeded by providing customers with innovative products of high  quality. Focusing on the early phases of product development, this  paper suggests the combined use of quality function deployment and  Kano's model. A case example is presented by employing the proposed  method in a World Wide Web page design process. Customer  requirements on Web pages were clearly recognised and deeply  analysed. Also, further technical features were identified for  delivering attractive Web pages. The results from the case example  support the proposed framework as a means of innovative product\u00a0\u2026", "num_citations": "66\n", "authors": ["445"]}
{"title": "Condition-based maintenance for systems with aging and cumulative damage based on proportional hazards model\n", "abstract": " This paper develops a condition-based maintenance (CBM) policy for systems subject to aging and cumulative damage. The cumulative damage is modeled by a continuous degradation process. Different from previous studies which assume that the system fails when the degradation level exceeds a specific threshold, this paper argues that the degradation itself does not directly lead to system failure, but increases the failure risk of the system. Proportional hazards model (PHM) is employed to characterize the joint effect of aging and cumulative damage. CBM models are developed for two cases: one assumes that the distribution parameters of the degradation process are known in advance, while the other assumes that the parameters are unknown and need to be estimated during system operation. In the first case, an optimal maintenance policy is obtained by minimizing the long-run cost rate. For the case with\u00a0\u2026", "num_citations": "65\n", "authors": ["445"]}
{"title": "Dynamic programming for QFD optimization\n", "abstract": " Quality function deployment (QFD) is a useful method in product design and development and its aim is to improve the quality and to better meet customers' needs. Due to cost and other resource constraints, trade\u2010offs are always needed. Many optimization methods have been introduced into the QFD process to maximize customer satisfaction under certain constraints. However, current optimization methods sometimes cannot give practical optimal results and the data needed are hard or costly to get. To overcome these problems, this paper proposes a dynamic programming approach for the optimization problem. We first use an extended House of Quality to gather more information. Next, limited resources are allocated to the technical attributes using dynamic programming. The value of each technical attribute can be determined according to the resources allocated to them. Compared with other optimization\u00a0\u2026", "num_citations": "65\n", "authors": ["445"]}
{"title": "A comparison between software design and code metrics for the prediction of software fault content\n", "abstract": " Software metrics play an important role in measuring the quality of software. It is desirable to predict the quality of software as early as possible, and hence metrics have to be collected early as well. This raises a number of questions that has not been fully answered. In this paper we discuss, prediction of fault content and try to answer what type of metrics should be collected, to what extent design metrics can be used for prediction, and to what degree prediction accuracy can be improved if code metrics are included. Based on a data set collected from a real project, we found that both design and code metrics are correlated with the number of faults. When the metrics are used to build prediction models of the number of faults, the design metrics are as good as the code metrics, little improvement can be achieved if both design metrics and code metrics are used to model the relationship between the number of faults\u00a0\u2026", "num_citations": "65\n", "authors": ["445"]}
{"title": "Modeling and analysis of reliability of multi-release open source software incorporating both fault detection and correction processes\n", "abstract": " Large software systems require regular upgrading that tries to correct the reported faults in previous versions and add some functions to meet new requirements. It is thus necessary to investigate changes in reliability in the face of ongoing releases. However, the current modeling frameworks mostly rely on the idealized assumption that all faults will be removed instantaneously and perfectly. In this paper, the failure processes in testing multi-release software are investigated by taking into consideration the delays in fault repair time based on a proposed time delay model. The model is validated on real test datasets from the software that has been released three times with new features. A comprehensive analysis of optimal release times based on cost-efficiency is also provided, which could help project managers to determine the best time to release the software.", "num_citations": "64\n", "authors": ["445"]}
{"title": "A game-theoretical approach for optimizing maintenance, spares and service capacity in performance contracting\n", "abstract": " Recently the service industry is transitioning from material-based contracting to performance-based contracting. This paradigm shift enables the supplier to maximize the profit by attaining the system performance goal, while the customer is able to lower the asset ownership cost with assured system availability. Prior studies usually focus on a single stakeholder, either the supplier or the customer, in searching for the optimal decisions. Under game-theoretical framework, this paper proposes a multi-party, multi-criteria, and multi-item service delivery mechanism to maximize the utilities of all the stakeholders. The goals are achieved by jointly optimizing the maintenance, the spares inventory, and the repair capacity under the game-theoretical framwork. We prove that the supplier\u2019s actions on parts replacement time, spares stock level and repair cycle times are fully observable to the customer. Hence a first-best solution\u00a0\u2026", "num_citations": "62\n", "authors": ["445"]}
{"title": "Investigations of Human and Organizational Factors in hazardous vapor accidents\n", "abstract": " This paper presents a model to assess the contribution of Human and Organizational Factor (HOF) to accidents. The proposed model is made up of two phases. The first phase is the qualitative analysis of HOF responsible for accidents, which utilizes Human Factors Analysis and Classification System (HFACS) to seek out latent HOFs. The hierarchy of HOFs identified in the first phase provides inputs for the analysis in the second phase, which is a quantitative analysis using Bayesian Network (BN). BN enhances the ability of HFACS by allowing investigators or domain experts to measure the degree of relationships among the HOFs. In order to estimate the conditional probabilities of BN, fuzzy analytical hierarchy process and decomposition method are applied in the model. Case studies show that the model is capable of seeking out critical latent human and organizational errors and carrying out quantitative\u00a0\u2026", "num_citations": "62\n", "authors": ["445"]}
{"title": "A generic data-driven software reliability model with model mining technique\n", "abstract": " Complex systems contain both hardware and software, and software reliability becomes more and more essential in system reliability context. In recent years, data-driven software reliability models (DDSRMs) with multiple-delayed-input single-output (MDISO) architecture have been proposed and studied. For these models, the software failure process is viewed as a time series and it is assumed that a software failure is strongly correlated with the most recent failures. In reality, this assumption may not be valid and hence the model performance would be affected. In this paper, we propose a generic DDSRM with MDISO architecture by relaxing this unrealistic assumption. The proposed model can cater for various failure correlations and existing DDSRMs are special cases of the proposed model. A hybrid genetic algorithm (GA)-based algorithm is developed which adopts the model mining technique to discover the\u00a0\u2026", "num_citations": "62\n", "authors": ["445"]}
{"title": "Optimizing survivability of multi-state systems with multi-level protection by multi-processor genetic algorithm\n", "abstract": " In this paper we consider vulnerable systems which can have different states corresponding to different combinations of available elements composing the system. Each state can be characterized by a performance rate, which is the quantitative measure of a system's ability to perform its task. Both the impact of external factors (stress) and internal causes (failures) affect system survivability, which is determined as probability of meeting a given demand.In order to increase the survivability of the system, a multi-level protection is applied to its subsystems. This means that a subsystem and its inner level of protection are in their turn protected by the protection of an outer level. This double-protected subsystem has its outer protection and so forth. In such systems, the protected subsystems can be destroyed only if all of the levels of their protection are destroyed. Each level of protection can be destroyed only if all of the\u00a0\u2026", "num_citations": "62\n", "authors": ["445"]}
{"title": "Reconnection scaling experiment: A new device for three-dimensional magnetic reconnection studies\n", "abstract": " The reconnection scaling experiment (RSX), a linear device for studying three-dimensional magnetic reconnection in both collisional and collisionless laboratory plasmas, has been constructed at Los Alamos National Laboratory. Advanced experimental features of the RSX that lead to scientific advantages include the use of simple technology (commercial plasma guns) to create plasma and current channels. Physics motivations, design and construction features of the RSX, are presented. Basic plasma parameters that characterize the RSX are shown together with preliminary measurements of visible light emission during the merging of two parallel current channels.", "num_citations": "62\n", "authors": ["445"]}
{"title": "On a general periodic preventive maintenance policy incorporating warranty contracts and system ageing losses\n", "abstract": " In this paper, a general periodic preventive maintenance (PM) policy for a repairable revenue-generating system is developed and studied. We define \u2018ageing losses\u2019 as the difference in revenues generated by an ideal system (no ageing) and a real system that ages over the same period of consideration. It is assumed that preventive maintenance slows the system deterioration process and therefore reduces ageing losses. The proposed model is general in the sense that (1) both the warranty contracts and system ageing losses are incorporated in the maintenance cost modeling and (2) the implementation of PM actions does not have to be strictly periodic. A cost model is developed for the buyer under two decision variables\u2014the calendar time of the first PM and the degree of each PM. Numerical examples are then presented to show the effectiveness of the proposed model. Sensitivity analyses are further\u00a0\u2026", "num_citations": "61\n", "authors": ["445"]}
{"title": "On changing points of mean residual life and failure rate function for some generalized Weibull distributions\n", "abstract": " The failure rate function and mean residual life function are two important characteristics in reliability analysis. Although many papers have studied distributions with bathtub-shaped failure rate and their properties, few have focused on the underlying associations between the mean residual life and failure rate function of these distributions, especially with respect to their changing points. It is known that the change point for mean residual life can be much earlier than that of failure rate function. In fact, the failure rate function should be flat for a long period of time for a distribution to be useful in practice. When the difference between the change points is large, the flat portion tends to be longer. This paper investigates the change points and focuses on the difference of the changing points. The exponentiated Weibull, a modified Weibull, and an extended Weibull distribution, all with bathtub-shaped failure rate function\u00a0\u2026", "num_citations": "60\n", "authors": ["445"]}
{"title": "A two-stage decision procedure for monitoring processes with low fraction nonconforming\n", "abstract": " Decision procedures for monitoring industrial processes can be based on application of control charts. The commonly used p-chart and np-chart are unsatisfactory for monitoring high-quality processes with a low fraction nonconforming. To overcome this difficulty, one may develop models based on the number of items inspected until r (\u2a7e 1) nonconforming items are observed. The cumulative count control chart (CCC-chart) is such an example. Like many other control charts, the CCC-charts suggested in the literature are one-stage control charts in which a decision is made when a signal for out of control appears. A CCC-chart with a small value of r requires less items inspected in order to obtain a signal for out of control, but is less reliable in detecting shifts of p than a CCC-chart with a large value of r (because the standard deviation of the number of items inspected in order to observe the rth nonconforming item\u00a0\u2026", "num_citations": "60\n", "authors": ["445"]}
{"title": "A study of EWMA chart with transformed exponential data\n", "abstract": " The exponentially weighted moving average (EWMA) chart has been shown to be effective in detecting small process shifts and predicting the process level at the next time period. In this paper, a new EWMA chart is proposed to monitor exponentially-distributed time-between-events (TBE) data. The proposed EWMA chart is set up after transforming the TBE data to approximate normal using the double square root (SQRT) transformation. The average run length (ARL) properties of an EWMA chart with transformed exponential data are investigated based on which design procedures are developed. Subsequently, the performance of the EWMA chart with transformed exponential data is compared to that of the X-MR chart, the cumulative quantity control (CQC) chart and the exponential EWMA chart. Furthermore, the robustness of the proposed EWMA chart to Weibull-distributed TBE data is examined, followed by an\u00a0\u2026", "num_citations": "59\n", "authors": ["445"]}
{"title": "A simple goodness-of-fit test for the power-law process, based on the Duane plot\n", "abstract": " The PLP (power-law process) or the Duane model is a simple model that can be used for both reliability growth and reliability deterioration. GOF (goodness-of-fit) tests for the PLP have attracted much attention. However, the practical use of the PLP model is its graphical analysis or the Duane plot, which is a log-log plot of the cumulative number of failures versus time. This has been commonly used for model validation and parameter estimation. When a plot is made, and the coefficient of determination, R/sup 2/, of the regression line is computed, the model can be tested based on this value. This paper introduces a statistical test, based on this simple procedure. The distribution of R/sup 2/ under the PLP hypothesis is shown not to depend on the true model parameters. Hence, it is possible to build a statistical GOF test for the PLP. The critical values of the test depend only on the sample size. Simulations show that\u00a0\u2026", "num_citations": "59\n", "authors": ["445"]}
{"title": "A conditional decision procedure for high yield processes\n", "abstract": " A control chart based on the cumulative count of conforming items between two successive nonconforming ones has been shown to be useful in manufacturing industries, particularly for high-quality processes. However, as the decision is based on a single count value, it is relatively insensitive to process shifts. In this paper a conditional procedure is proposed whereby the sensitivity is improved when the process shift is moderate to large in either direction. In addition, optimal limits are defined in such a way that the average run length becomes maximum when the process average is at the nominal level. The performance of the conditional chart and its optimal limits are investigated and compared with the traditional case. The idea of a conditional procedure is to utilize some of the previous runs when a count value exceeds the limits. This procedure is similar to the supplementary run rules, but the conditional\u00a0\u2026", "num_citations": "59\n", "authors": ["445"]}
{"title": "Cumulative probability control charts for geometric and exponential process characteristics\n", "abstract": " A statistical process control chart called the cumulative probability control chart (CPC-chart) is proposed. The CPC-chart is motivated from two existing statistical control charts, the cumulative count control chart (CCC-chart) and the cumulative quantity control chart (CQC-chart). The CCC- and CQC-charts are effective in monitoring production processes when the defect rate is low and the traditional p - and c -charts do not perform well. In a CPC-chart, the cumulative probability of the geometric or exponential random variable is plotted against the sample number, and hence the actual cumulative probability is indicated on the chart. Apart from maintaining all the favourable features of the CCC- and CQC-charts, the CPC-chart is more flexible and it can resolve a technical plotting inconvenience of the CCC- and CQC-charts.", "num_citations": "59\n", "authors": ["445"]}
{"title": "Element maintenance and allocation for linear consecutively connected systems\n", "abstract": " This article considers optimal maintenance and allocation of elements in a Linear Multi-state Consecutively Connected System (LMCCS), which is important in signal transmission and other network systems. The system consists of N+1 linearly ordered positions (nodes) and fails if the first node (source) is not connected with the final node (sink). The reliability of an LMCCS has been studied in the past but has been restricted to the case when each system element has a constant reliability. In practice, system elements usually fail with increasing failure probability due to aging effects. Furthermore, in order to increase system availability, resources can be put into the maintenance of each element to increase the availability of the element. In this article, a framework is proposed to solve the cost optimal maintenance and allocation strategy of this type of system subject to an availability requirement. A universal generating\u00a0\u2026", "num_citations": "58\n", "authors": ["445"]}
{"title": "Subjective operational reliability assessment of maritime transportation system\n", "abstract": " System reliability assessment is one of the major acts in the operation and maintenance of every industrial and service sector, which also holds true for maritime transportation system. The complexity of the maritime transportation system is a prime obstacle in the evaluation of the operational reliability of the system; mainly due to the fact that statistical data on the important parameters and variables is scarce. This makes the application of fuzzy sets and fuzzy logic a viable option to overcome the data problem with regards to imprecision or vagueness in parameters and variables values. In this paper, the different decisive factors, affecting maritime transportation systems, are modeled in the form of linguistic variables. Techniques such as aggregation, mapping of fuzzy sets using distance measure and fuzzy logic rule base are used to arrive at subjective operational reliability value. The complete procedure is\u00a0\u2026", "num_citations": "58\n", "authors": ["445"]}
{"title": "Spc of a near zero\u2010defect process subject to random shocks\n", "abstract": " In this paper we study the problem of monitoring and control of a type of process in which long series with no non\u2010conformities are observed together with occasional samples containing a large number of non\u2010conformities. We call this a near zero\u2010defect process subject to random shocks. Such processes occur often in practice, and a model is proposed for the identification of real non\u2010random variations of process characteristics. Based on the statistical analysis carried out for this model, a procedure for decision\u2010making in the control of this type of process is suggested, and analysis of some actual cases presented.", "num_citations": "58\n", "authors": ["445"]}
{"title": "Software reliability prediction incorporating information from a similar project\n", "abstract": " Although there are many models for the prediction of software reliability using the failure data collected during testing, the estimation is usually inaccurate, especially at the early stages of the testing phase, and hence many practitioners are hesitant to use software reliability models. On the other hand, the traditional software reliability growth models do not make use of information from earlier or similar projects. For example, software systems today are usually an improvement or modification of an earlier version or at least within the same application domain, which implies that some information should be available from similar projects. In this paper we study some approaches for the estimation of software reliability by incorporating information from a similar project. In particular, we use the Goel\u2013Okumoto model and assume the same value of the fault detection rate. The other parameter is then estimated based on the\u00a0\u2026", "num_citations": "57\n", "authors": ["445"]}
{"title": "Reliability of grid service systems\n", "abstract": " Grid computing system is different from conventional distributed computing systems by its focus on large-scale resource sharing, where remote accessing and information communication have great influence on grid computing. As an important metric for services, the grid service reliability is studied in this paper. There are some earlier studies on the reliability analysis of conventional small-scale distributed systems, which ignored the communication time and processing time when studying the reliability, but it is not practical in the reliability analysis of grid services due to the wide-area property of the network. Thus, this paper presents a model for the grid services and develops an approach to analyze the grid service reliability. Detailed algorithms to compute the grid service reliability are presented and a self-sensing technology is proposed for parameterization and monitoring. A numerical example is used for the\u00a0\u2026", "num_citations": "56\n", "authors": ["445"]}
{"title": "A comparative study of exponential time between events charts\n", "abstract": " Control charts based on time between events (TBE) data have been shown to be useful for monitoring production or failure processes where defect or failure occurrence can be recorded over time. One type of TBE chart is set up with probability control limits, e.g. the Cumulative Quantity Control (CQC) chart and its extension CQC-r chart, which monitors the quantity inspected until the occurrence of a fixed number of defects (r). Another type of TBE chart studied in the literature is to apply the CUSUM and EWMA methods to TBE data directly. In this paper, some exponential TBE charts are compared based on their Average Time to Signal (ATS) performance. Different in-control ATS values are used in order to evaluate its effect on the performance of the TBE charts. Based on the comparison analysis, some recommendations are made as guidelines for employing a proper chart under different situations.", "num_citations": "56\n", "authors": ["445"]}
{"title": "Statistical analysis of a Weibull extension model\n", "abstract": " Recently, Chen (Chen, Z. . A new two-parameter lifetime distribution with bathtub-shape or increasing failure rate function. Statistics & Probability Letters 49:155\u2013161.) proposed a two-parameter model that can be used to model bathtub-shaped failure rate. Although this model has several interesting properties, it does not contain a scale parameter and hence not flexible in modeling real data. A generalized model including the scale parameter has shown to be interesting and it has the traditional Weibull distribution as an asymptotic case. In this article, a detailed analysis of this model is presented. Shapes of the density and failure rate function are studied. The asymptotic confidence intervals for the parameters are also derived from the Fisher information matrix. The likelihood ratio test is applied to test the goodness of fit of Weibull extension model. Some examples are shown to illustrate the application of the model\u00a0\u2026", "num_citations": "56\n", "authors": ["445"]}
{"title": "Optimal structure of multi-state systems with multi-fault coverage\n", "abstract": " Due to imperfect fault coverage, the reliability of redundant systems cannot be enhanced unlimitedly with the increase of redundancy. Thus it is essential to study the optimal structure of redundant systems. This paper considers a multi-state series-parallel system with two types of parallelization: redundancy and work sharing. Different from existing works which consider single-fault coverage, multi-fault coverage is considered in order to adapt to a wider range of fault tolerant mechanisms. For multi-fault coverage, the coverage factor of an element failure in a work sharing group depends on the status of other elements. It is assumed that the uncovered failures in the elements belonging to the group of elements sharing the same task can cause failure of the entire group. The optimal trade-off between the two kinds of parallelization has been studied based on various settings of fault coverage factor. Examples of data\u00a0\u2026", "num_citations": "55\n", "authors": ["445"]}
{"title": "Economic design of cumulative count of conforming charts under inspection by samples\n", "abstract": " This paper deals with the economic issues involved in the design of control charts for high quality manufacturing processes. The cumulative count of conforming (CCC) chart has been shown effective for such processes. However, the applicability of the conventional CCC chart is limited to the case that items are inspected one by one in the order of production. When items are inspected lot by lot or sample by sample without preserving or according to the original production ordering, modification is needed. In this paper, it is proposed to monitor the cumulative number of samples inspected until a nonconforming sample is encountered. An economic model is developed for designing such a generalized CCC chart. Numerical examples are given and sensitivity analysis is performed to demonstrate the use of the economic design model.", "num_citations": "55\n", "authors": ["445"]}
{"title": "A model for correlated failures in N-version programming\n", "abstract": " The multi-version programming technique is a method to increase the reliability of safety critical software. In this technique a number of versions are developed and a voting scheme is used before a final result is provided. In the analysis of this type of systems, a common assumption is the independence of the different versions. However, the different versions are usually interdependent and failures are correlated due to the nature of the product design and development. One version may fail simultaneously with another version because of a common cause. In this paper, a model for these dependent failures is developed and studied. Using the developed model, a reliability function can be easily computed. A method is also proposed to estimate the parameters of the model. Finally, as an application of the developed model, an optimal testing resource allocation problem is formulated and a genetic algorithm is\u00a0\u2026", "num_citations": "55\n", "authors": ["445"]}
{"title": "A comparative study of CCC and CUSUM charts\n", "abstract": " The cumulative count of conforming (CCC) chart is a new type of control chart used for the monitoring of high\u2010quality processes. Instead of counting the number of non\u2010conforming items in samples of fixed size, the cumulative number of conforming items between two non\u2010conforming items is monitored. The CCC chart is convenient to use in a modern manufacturing environment where the product is inspected individually and automatically. The CCC chart has sometimes been confused with the cumulative sum (CUSUM) chart which has been shown to be more sensitive than the traditional Shewhart chart for small process shifts. In this paper the uses of these two types of charts are compared. It shown by numerical illustrations and analytical results that the two charts function in entirely different ways. However, the CUSUM concept can be applied to cumulative counts used in the CCC chart to improve its sensitivity\u00a0\u2026", "num_citations": "54\n", "authors": ["445"]}
{"title": "Improvement detection by control charts for high yield processes\n", "abstract": " Process control charts are routinely used for detecting deterioration of process performance. From management's point of view it is also desirable to be alerted when a process has improved. Investigates the problem of detection of quality improvement in a high yield or low-proportion nonconforming process. The conventional 3\u03c3 lower control limit in an np control chart, which is based on the normal approximation to binomial, is not useful in this case as it often gives a negative lower control limit which is physically meaningless. We establish suitable probability limits based on the binomial distribution, and also the size of the sample required to identify a process improvement at various certainty levels. The procedures are conceptually simple and easy to implement in practice.", "num_citations": "54\n", "authors": ["445"]}
{"title": "On modeling dynamic priorities in the analytic hierarchy process using compositional data analysis\n", "abstract": " In a rapidly changing environment, the priorities derived using the analytic hierarchy process (AHP) approach at one point in time might very likely change in the near future. Thus, in order to adapt to such ever-changing environment, it is of primary importance to be able to follow the change over time as to enable the system to respond differently and continuously over time of its operation. This paper proposes the use of a time-based compositional forecasting method, which is based on the idea of exponential smoothing, to deal with the AHP priority dynamics. The proposed method is particularly useful when there is a limited number of historical data, and might be considered to be more effective and time-efficient compared to that of multivariate time series method. It was also shown that the proposed method provides much greater adaptability in modeling the AHP priorities change over time compared to that of\u00a0\u2026", "num_citations": "53\n", "authors": ["445"]}
{"title": "On the dual reliability systems of (n, f, k) and< n, f, k\n", "abstract": " Abstract The (n, f, k)(< n, f, k>) system consists of n components ordered in a line or a circle and the system fails if and only if there exist at least f failed components or (and) at least k consecutive failed components. This type of models involves two common failure criteria of a system. In this paper, the duality of these systems is obtained and the proofs are given. We also obtain the recursive equations for the system reliability for these models. Furthermore, the finite Markov chain imbedding approach is employed to obtain some more compact reliability formulas for independent and Markov-dependent cases.", "num_citations": "53\n", "authors": ["445"]}
{"title": "Sequential inspection strategy for multiple systems under availability requirement\n", "abstract": " System failures are usually observed during regular maintenance or inspection and this is especially the case for systems in standby or storage, which is common for safety critical systems. A periodic inspection policy is usually adopted. However, during the inspection, a lot of information is gained about the status of the system. Such information should be used in deciding upon the time for the next inspection. Hence sequential inspection is more appropriate, especially when the aging property of the system is unknown, and has to be estimated with the information from inspection. In this paper, a model is developed and sequential inspection strategies are studied in this situation. The focus is on the case when there are multiple systems inspected at the same, but discrete times. We also do not assume a known distribution of the system life time, and the estimation of that is incorporated into the analysis and decision\u00a0\u2026", "num_citations": "53\n", "authors": ["445"]}
{"title": "Availability analysis of periodically inspected systems with random walk model\n", "abstract": " In this paper, the instantaneous availability of a system maintained under periodic inspection is investigated using random walk models. Two cases are considered. In the first model, the system is repaired or modified and it is assumed to be as good as new upon periodic inspection and maintenance. In the second model, the system is not modified after the inspection if the system is still working, and the condition of the system is assumed to be the same as that before the inspection. For both models the failures only can be found through the inspection. Perfect repair or replacement of a failed system is assumed to be carried out, but the time it takes can be constant or of a random length. The relationship between this problem and the random walk model in a two-dimensional plane is described. Several new results are also shown.", "num_citations": "53\n", "authors": ["445"]}
{"title": "Life cycle reliability assessment of new products\u2014A Bayesian model updating approach\n", "abstract": " The rapidly increasing pace and continuously evolving reliability requirements of new products have made life cycle reliability assessment of new products an imperative yet difficult work. While much work has been done to separately estimate reliability of new products in specific stages, a gap exists in carrying out life cycle reliability assessment throughout all life cycle stages. We present a Bayesian model updating approach (BMUA) for life cycle reliability assessment of new products. Novel features of this approach are the development of Bayesian information toolkits by separately including \u201creliability improvement factor\u201d and \u201cinformation fusion factor\u201d, which allow the integration of subjective information in a specific life cycle stage and the transition of integrated information between adjacent life cycle stages. They lead to the unique characteristics of the BMUA in which information generated throughout life cycle\u00a0\u2026", "num_citations": "52\n", "authors": ["445"]}
{"title": "Adaptive ridge regression system for software cost estimating on multi-collinear datasets\n", "abstract": " Cost estimation is one of the most critical activities in software life cycle. In past decades, a number of techniques have been proposed for cost estimation. Linear regression is yet the most frequently applied method in the literature. However, a number of studies point out that linear regression is prone to low prediction accuracy. The low prediction accuracy is due to a number of reasons such as non-linearity and non-normality. One less addressed reason is the multi-collinearities which may lead to unstable regression coefficients. On the other hand, it has been reported that multi-collinearity spreads widely across the software engineering datasets. To tackle this problem and improve regression's accuracy, we propose a holistic problem-solving approach (named adaptive ridge regression system) integrating data transformation, multi-collinearity diagnosis, ridge regression technique and multi-objective optimization\u00a0\u2026", "num_citations": "52\n", "authors": ["445"]}
{"title": "Inspection schemes for general systems\n", "abstract": " An inspection scheme is an integral part of any maintenance policy and we usually have to determine the time points at which the system should be inspected to make sure that the system availability requirement is met satisfactorily. There are many optimal policies for the determination of inspection times. This paper deals with periodic inspection with the emphasis on meeting the availability requirement. Five models for assigning the inspection times are studied. For these models, limiting average availability, long-run inspection rate, instantaneous availability and instantaneous inspection rate, are all derived. The relationships among the models are also discussed. A numerical example is presented to illustrate the differences for the cases of steady-state availability and instantaneous availability.", "num_citations": "51\n", "authors": ["445"]}
{"title": "Efficient estimation of the Weibull shape parameter based on a modified profile likelihood\n", "abstract": " The maximum likelihood estimator of the Weibull shape parameter can be very biased. An estimator based on the modified profile likelihood is proposed and its properties are studied. It is shown that the new estimator is almost unbiased with relative bias being less than 1% in most of situations, and it is much more efficient than the regular MLE. The smaller the sample or the heavier of the censoring, the more efficient is the new estimator relative to the regular MLE.", "num_citations": "51\n", "authors": ["445"]}
{"title": "Availability modeling and cost optimization for the grid resource management system\n", "abstract": " Grid computing is a recently developed technique for complex systems with large-scale resource sharing, wide-area communication, and multi-institutional collaboration. Although the development tools and techniques for the grid have been extensively investigated, the availability of the grid resource management system (RMS) has not been comprehensively studied. In order to contribute to this lacking but important field, this paper first models the grid RMS availability by considering both the failures of resource management (RM) servers and the length limitation of request queues. A hierarchical Markov reward model is implemented to evaluate the grid RMS availability. Based on the availability model, an optimization problem for designing the grid RMS is studied in order to minimize the cost by determining the best number of RM servers. Then, the sensitivity analysis is conducted, and a dynamic switching\u00a0\u2026", "num_citations": "50\n", "authors": ["445"]}
{"title": "Economic design of exponential charts for time between events monitoring\n", "abstract": " The occurrence of defects or non-conformities in manufacturing processes can usually be modeled by a homogeneous Poisson process. However, the process parameter may change over time and it can be monitored with statistical process control techniques. Control charts based on an exponential distribution, called exponential charts in this paper, can be developed to monitor the occurrence rate of such events. For manufacturers, the economic objective of production is very important and has to be optimized. An economic approach is developed in this paper for the design of exponential charts. We compare and contrast the performances of statistical design, economic design and economic\u2013statistical design. The usefulness of the proposed economic design approach is justified. The relationships among these designs are illustrated through numerical examples. In particular, the economic\u2013statistical design\u00a0\u2026", "num_citations": "50\n", "authors": ["445"]}
{"title": "Optimal testing\u2010time allocation for modular systems\n", "abstract": " Software testing is usually a very costly and time\u2010consuming phase in software development. As most software systems are modular, it is of great importance for the management to allocate the limited testing\u2010time among the software modules in an optimal way so that the highest quality and reliability of the complete system can be achieved. In this paper, the problem of optimal testing\u2010time allocation for modular software systems is studied. A generic formulation of the problem is presented based on nonhomogeneous Poisson process models. The aim is to maximize the operational reliability of the software system. Numerical examples are presented to illustrate the optimisation algorithm and the solution. Furthermore, as software reliability growth models consist of a number of parameters, an example of a sensitivity analysis is also shown. Such a sensitivity study is useful as important model parameters can be\u00a0\u2026", "num_citations": "50\n", "authors": ["445"]}
{"title": "Optimal burn-in for repairable products sold with a two-dimensional warranty\n", "abstract": " Warranty data analyses reveal that products sold with two-dimensional warranties may have significant infant mortalities. To deal with this problem, this article proposes and studies a new burn-in modeling approach for repairable products sold with a two-dimensional warranty. More specifically, two types of failures are characterized\u2014i.e., normal and defect failures\u2014and performance and cost-based burn-in models are developed under the non-renewing free-repair warranty policy. The proposed models subsume the special cases of a one-dimensional warranty, allow different failure modes to have distinct accelerated relationships, and take consumer usage heterogeneity into consideration. Under mild assumptions, it is established that the optimal burn-in usage rate should be as high as possible, provided that no extraneous failure modes are introduced. Furthermore, It is shown that the optimal burn-in duration\u00a0\u2026", "num_citations": "49\n", "authors": ["445"]}
{"title": "A study of time\u2010between\u2010events control chart for the monitoring of regularly maintained systems\n", "abstract": " Owing to usage, environment and aging, the condition of a system deteriorates over time. Regular maintenance is often conducted to restore its condition and to prevent failures from occurring. In this kind of a situation, the process is considered to be stable, thus statistical process control charts can be used to monitor the process. The monitoring can help in making a decision on whether further maintenance is worthwhile or whether the system has deteriorated to a state where regular maintenance is no longer effective. When modeling a deteriorating system, lifetime distributions with increasing failure rate are more appropriate. However, for a regularly maintained system, the failure time distribution can be approximated by the exponential distribution with an average failure rate that depends on the maintenance interval. In this paper, we adopt a modification for a time\u2010between\u2010events control chart, i.e. the\u00a0\u2026", "num_citations": "49\n", "authors": ["445"]}
{"title": "Reliability modeling and preventive maintenance of load-sharing systemswith degrading components\n", "abstract": " This article presents certain new approaches to the reliability modeling of systems subject to shared loads. It is assumed that components in the system degrade continuously through an additive impact under load. The reliability assessment of such systems is often complicated by the fact that both the arriving load and the failure of components influence the degradation of the surviving components in a complex manner. The proposed approaches seek to ease this problem, by first deriving the time to prior failures and the arrival of random loads and then determining the number of failed components. Two separate models capable of analyzing system reliability as well as arriving at system maintenance and design decisions are proposed. The first considers a constant load and the other a cumulative load. A numerical example is presented to illustrate the effectiveness of the proposed models.", "num_citations": "48\n", "authors": ["445"]}
{"title": "A condition-based maintenance strategy for heterogeneous populations\n", "abstract": " This paper develops a maintenance strategy, called inspection\u2013replacement policy, to cope with heterogeneous populations. Burn-in is the procedure by which most of the defective products in a heterogeneous population can be identified and removed prior to being placed in service. However, modern manufacturing is so well developed that a defective product is able to function for a long period of time even under aggravated operational conditions. Instead of weeding defective products out via costly burn-in tests, use can be made of them in field operation where maintaining actions will be performed to prevent early in-use failures. The inspection\u2013replacement policy consists of an inspection, conducted in an early stage with the purpose of identifying and replacing defective products, and a preventive replacement, carried out at a later stage to prevent wear-out failures. The preventive-replacement time is\u00a0\u2026", "num_citations": "47\n", "authors": ["445"]}
{"title": "A study of the sensitivity of software release time\n", "abstract": " Software release time determination is of great importance to software developers. In order to predict the amount of testing needed and to estimate the release time at which a certain reliability target is met, probabilistic models can be used to predict the reliability level achieved during the software testing. However, most of the results assume that the model parameters are known which is not true since they have to be estimated based on the information available. In many cases, they are inaccurate because of the small amount of data available. Hence, it is useful to know the sensitivity of software release time with respect to the estimated parameters so that attention can be paid to those parameters that affect the release time significantly. In this paper, the sensitivity issue is studied for a commonly used software reliability model. The study can be used, for example, if an overestimation of a parameter implies an\u00a0\u2026", "num_citations": "47\n", "authors": ["445"]}
{"title": "A study of the sensitivity of\" customer voice\" in QFD analysis\n", "abstract": " Quality Function Deployment (QFD) using the method of House of Quality (HOQ) is a qualitative tool to help the management incorporate customer requirements into the product and process development. In the analysis of HOQ, a small difference of the weights of the customer requirements may affect the result of the HOQ, which then influence subsequent outcomes such as the prioritization and allocation of resources. On the other hand, it is usually difficult, if not impossible, to obtain accurate information about the customer needs. In this paper, a more formal study is carried out to examine the sensitivity related to customer needs. The Analytic Hierarchy Process method (AHP) is used to obtain the relative importance of the customer voice. An example is presented to illustrate how to conduct a sensitivity analysis. In general, the ranking is not sensitive to customer voice as the discrete weightage used for the correlation matrix. Significance: A small change of the weights is shown not to affect the final ranking of the technical responses in the HOQ; hence QFD is a robust method for prioritization. The result suggests that we can spend more time with the relationship metrics and other issues when using QFD.", "num_citations": "47\n", "authors": ["445"]}
{"title": "On the log-power NHPP software reliability model\n", "abstract": " A simple software reliability model, the log-power nonhomogeneous Poisson process (NHPP) model, is studied. The log-power NHPP model has several interesting properties, such as simple graphical interpretations and simple forms of the maximum likelihood estimates for the parameters. The authors assess this model by considering its simplicity, data fitting and predicting ability. They have applied the log-power model to many sets of existing software reliability data. The results show that this model is able to fit different data sets and has a relatively high predicting ability. This, together with its simplicity and the graphical interpretation which provides a useful reliability engineering tool, shows that the log-power model is an applicable software reliability model in practice.< >", "num_citations": "47\n", "authors": ["445"]}
{"title": "On ranking of system components with respect to different improvement actions\n", "abstract": " For a system consisting of a number of components, an effort should in first hand be given to the most important components. However, this usually depends on which improvement action is offered. Some ranking methods of system components are introduced. We suggest to use different rankings for different improvement actions. Also we give a simple example where these rankings differ considerably. It is indicated that the importance measure suggested by Birnbaum can not be directly used, but our new measure is strongly connected to it.", "num_citations": "47\n", "authors": ["445"]}
{"title": "The effects of arrest, reporting to the police, and victim services on intimate partner violence\n", "abstract": " Objectives:To estimate the effects of three types of responses to intimate partner violence: (1) reporting of crime to the police, (2) arresting the suspect, and (3) receiving services from agencies other than the police that assist victims of crime.Methods:We obtained a nationally representative sample of 2,221 victims, using longitudinal records from the area-identified National Crime Victimization Survey from 1996 through 2012. To reduce the threat of nonrandom selection into treatment, we estimated effects using propensity score matched and weighted survival analysis.Results:Victims\u2019 probability of repeat victimization is not related to arrest (hazard ratio, 0.87; 95 percent confidence interval [CI], 0.55 to 1.40; p = .57). In contrast, the reporting of crime to the police is associated with a 34 percent reduction in the risk of repeat victimization (hazard ratio, 0.66; 95 percent CI, 0.53 to 0.82; p < .001), and the use of victim\u00a0\u2026", "num_citations": "45\n", "authors": ["445"]}
{"title": "A heuristic algorithm for reliability modeling and analysis of grid systems\n", "abstract": " Grid computing focuses on large-scale resource sharing. Using a general reliability model for grid computing to relax some impractical assumptions, a heuristic algorithm is presented to evaluate grid program/service reliability. The heuristic algorithm is based on two heuristic criteria that determine the significance of an entity and prune those insignificant ones. Through algorithm analysis, the heuristic algorithm is shown to have a linear complexity. This is much better than the previous algorithms, which are of exponential complexity. Another advantage of the heuristic algorithm is that the running time is controllable by adjusting the parameter of significant level (SL) and significant rate. A regression method is proposed to adjust the SL and predict the running time. Two examples are given", "num_citations": "45\n", "authors": ["445"]}
{"title": "Neural network modeling with confidence bounds: a case study on the solder paste deposition process\n", "abstract": " The formation of reliable solder joints in electronic assemblies is a critical issue in surface mount manufacturing. Stringent control is placed on the solder paste deposition process to minimize soldering defects and achieve high assembly yield. Time series process modeling of the solder paste quality characteristics using neural networks (NN) is a promising approach that complements traditional control charting schemes deployed on-line. We present the study of building a multilayer feedforward neural network for monitoring the solder paste deposition process performance. Modeling via neural networks provides not only useful insights in the process dynamics, it also allows forecasts of future process behavior to be made. Data measurements collected on ball grid array (BGA) and quad flat pack (QFP) packages are used to illustrate the NN technique and the forecast accuracies of the models are summarized\u00a0\u2026", "num_citations": "45\n", "authors": ["445"]}
{"title": "Data transformation for geometrically distributed quality characteristics\n", "abstract": " Recently there has been an increasing interest in techniques of process monitoring involving geometrically distributed quality characteristics, as many types of attribute data are neither binomial nor Poisson distributed. The geometric distribution is particularly useful for monitoring high\u2010quality processes based on cumulative counts of conforming items. However, a geometrically distributed quantity can never be adequately approximated by a normal distribution that is typically used for setting 3\u2010sigma control limits. In this paper, some transformation techniques that are appropriate for geometrically distributed quantities are studied. Since the normal distribution assumption is used in run\u2010rules and advanced process\u2010monitoring techniques such as the cumulative sum or exponentially weighted moving average chart, data transformation is needed. In particular, a double square root transformation which can be\u00a0\u2026", "num_citations": "45\n", "authors": ["445"]}
{"title": "Redundancy allocation for series-parallel warm-standby systems\n", "abstract": " Increasing the reliability of a system is generally achieved by using redundant components, which results in raising the system cost. A great challenge in system design is to improve the system reliability while still meeting the resources limitations, typically the budget constraints. This paper considers the redundancy allocation problem (RAP) for series-parallel systems with warm standby sparing (WSP). WSP is a type of dynamic redundancy that compromises the energy consumption of a hot standby sparing system and the recovery time of a cold standby sparing system. The existing RAP solutions mainly focus on systems with cold and hot standby redundancies. This paper presents two optimization solution methodologies, respectively based on genetic algorithm and integer programming to achieve optimal design of a series-parallel with warm standby redundancy. Both methodologies are illustrated using examples.", "num_citations": "44\n", "authors": ["445"]}
{"title": "Probability analysis of offshore fire by incorporating human and organizational factor\n", "abstract": " In this paper, a probability analysis model of offshore fire is presented using the method of converting fault tree (FT) into Bayesian Network (BN) to incorporate the effect of Human and Organizational Factor (HOF). The multi-phase model allows different methods to be applied to different parts. In the first phase, Fault Tree (FT) is used to model the factors how to contribute to the fire scenarios, which guide the construction of BN model. In the second phase, BN is used to extend the causal chain of basic events to potential HOFs and provide a more precise quantitative links between the event nodes. The distinct advantages of BN making them more suitable than FTs are their ability in explicitly representing the dependencies of events, updating probabilities, and coping with uncertainties, which cannot be considered by FT. Finally, the integration algorithm is demonstrated on four offshore fire scenarios. It clearly shows\u00a0\u2026", "num_citations": "44\n", "authors": ["445"]}
{"title": "On some reliability growth models with simple graphical interpretations\n", "abstract": " Some useful reliability growth models, which have simple graphical interpretations, are studied in this paper. The proposed models are inspired by the Duane model. For each of the models, the plot of the cumulative number of failures against the running time, when a suitable scale is used, will tend to be on a straight line if the model is valid. Otherwise the model should be rejected. The slope of the fitted line and its intercept on the vertical axis will give us the estimates of the parameters. Hence, it provides us with a simple graphical model validation and parameter estimation tool. In particular, we propose a \u201cfirst-model-validation-then-parameter-estimation\u201d approach which will simplify the model validation and parameter estimation problem in software reliability analysis. Numerical analysis of several sets of software failure data are also provided to enlighten the ideas.", "num_citations": "44\n", "authors": ["445"]}
{"title": "Optimizing product design using quantitative quality function deployment: a case study\n", "abstract": " In this paper, we examine the applicability of quality function deployment (QFD) in optimizing product design. QFD is a systematic process that can help businesses to focus on their customers. In recent years, many quantitative QFD methods have been developed. However, only a small number of case studies have been reported on their use. For this research a case study on personal computer design was conducted. It demonstrates that quantitative QFD can be successfully implemented in product design. Some limitations are highlighted. Practical suggestions on implementing quantitative QFD are also discussed. Copyright \u00a9 2007 John Wiley & Sons, Ltd.", "num_citations": "43\n", "authors": ["445"]}
{"title": "On economic design of cumulative count of conforming chart\n", "abstract": " Control charts based on Cumulative Count of Conforming (CCC) have been shown to be useful in high-quality processes. In the design stage of any chart in manufacturing industry, economic factors should be taken into account. In this paper, an economic model for CCC chart design is developed. A simplified algorithm is used to search the optimal setting of the sampling and control parameters. Sensitivity analysis is presented and numerical examples are used to illustrate the procedure. Compared with related studies, the model presented here could produce lower cost and smaller Type II error than the existing approach and the procedure is easy to implement.", "num_citations": "42\n", "authors": ["445"]}
{"title": "Software release time determination based on unbounded NHPP model\n", "abstract": " One of the important applications of software reliability models is the determination of software release time. Most of the existing studies on this topic use models based on nonhomogeneous Poisson process with a bounded mean value function. In this paper, nonhomogeneous Poisson process models with unbounded mean value function is used for the determination of the optimum software release time. Both reliability criteria and cost minimisation are discussed together with the used of combined criteria. Furthermore, we also consider the case of interval estimation for the software release time as point estimation is usually inaccurate.", "num_citations": "42\n", "authors": ["445"]}
{"title": "Economic design of time-between-events control chart system\n", "abstract": " This article presents the economic design of the control chart system consisting of several individual control charts based on time-between-events (TBE) data for monitoring multistage manufacturing processes. The design algorithm considers all the TBE charts within a system in an integrative and optimal manner. Numerical studies show that the proposed design algorithm improves the performance characteristics (in terms of profit) considerably. The proposed control chart system is easy to understand and operate, and thus the floor operators can utilize and understand it as easily as for the traditional system.", "num_citations": "41\n", "authors": ["445"]}
{"title": "Some normal approximations for renewal function of large Weibull shape parameter\n", "abstract": " Weibull renewal function has attracted a lot of attention because the Weibull distribution describes in a relatively simple analytical manner a wide range of realistic behavior and its shape and scale parameters can be readily determined with graphical or statistical procedure. On the other hand, there are no closed form analytical solutions for the Weibull renewal function except for the special case of exponential distribution. Bounds, approximations, and tables are usually used. In this article, some approximations based on Normal approximation of Weibull distribution are studied. Such a procedure, which is different from that in the existing literature, is shown to be good for Weibull renewal function when the shape parameter is of moderate or large size. Series truncation expression and approximation bounds can be obtained as well. Numerical examples and comparisons are shown to illustrate the procedure.", "num_citations": "41\n", "authors": ["445"]}
{"title": "On the increase of system reliability by parallel redundancy\n", "abstract": " Adding parallel redundancy to different components generally yields different system reliability improvements. The effect of such parallel redundancy upon system reliability when applied at various places and in various systems is investigated. The problem of how to choose components for parallel redundancy is studied, and some results are given. Some examples are presented to illustrate the approach.< >", "num_citations": "41\n", "authors": ["445"]}
{"title": "A study of interval analysis for cold-standby system reliability optimization under parameter uncertainty\n", "abstract": " This paper presents a study of interval analysis for solving cold-standby system reliability optimization problems with considering parameter uncertainty. Most works reported in existing literature have been based on the assumption that the probabilistic properties and statistical parameters have a known functional form, which is usually not the case. Very often the parameters are presented in form of an interval-valued number or bounds/tolerance from the engineering design. In this paper, interval analysis is used to incorporate this in the system optimization problems. A definition of interval order relation reflecting decision makers\u2019 preference is proposed for comparing interval numbers. A computational algorithm is developed to evaluate the system reliability and expected mission cost, in which a discrete approximation approach and a technique of interval universal generating function are used. For illustration, an\u00a0\u2026", "num_citations": "40\n", "authors": ["445"]}
{"title": "Cumulative count of conforming chart with variable sampling intervals\n", "abstract": " The control chart with cumulative count of conforming (CCC) items has shown to be useful for process monitoring in automated and discrete manufacturing. It does not require a fixed sample size and can be useful for one-at-a-time data. The current research relies on data from the fixed sampling interval (FSI) scheme. In this paper, we investigate the use of the CCC chart when the variable sampling interval scheme is used. The average time to signal (ATS) is calculated, and its efficiency is compared with that of FSI CCC chart. The use of variable sampling interval scheme may further enhance the cost effectiveness of control chart implementation from a practical point of view.", "num_citations": "40\n", "authors": ["445"]}
{"title": "A Shewhart\u2010like charting technique for high yield processes\n", "abstract": " A Shewhart\u2010like charting technique is developed in this paper to overcome the difficulties the traditional control chart encounters in the control of processes with a very low fraction non\u2010conforming. The technique uses the number of conforming items between two consecutive non\u2010conforming ones to monitor the fraction non\u2010conforming of a process, leading to a chart that is informative and easy to interpret. The approach discussed is especially suitable for real\u2010time and automatic statistical quality control.", "num_citations": "40\n", "authors": ["445"]}
{"title": "Software reliability models\u2014A selected annotated bibliography\n", "abstract": " Many articles on software reliability models have been published in the last two decades. Because of the importance of this area and increasing interest in it, this annotated bibliography of 100 selected publications has been prepared. First, books, edited works and review papers of general interest are summarized and this is followed by separate introductions to various types of models.", "num_citations": "40\n", "authors": ["445"]}
{"title": "An improved self-starting cumulative count of conforming chart for monitoring high-quality processes under group inspection\n", "abstract": " The cumulative count of conforming (CCC) chart as a main statistical process control tool for monitoring high-quality processes has been widely studied. However, its applicability is limited to situations where units of product are inspected sequentially, or item by item. Motivated by real-life problems, this paper proposes an improved control charting technique for high-quality processes under group inspection. It integrates a self-starting feature and an approximately ARL- (average run length) unbiased design. The chart, named the CCCG chart, monitors the cumulative count of conforming samples until a non-conforming one is encountered. The \u2018G\u2019 in the subscript stands for \u2018Group\u2019. The self-starting feature caters to the need in practice to start monitoring a production process as soon as possible. The approximately ARL-unbiased design is developed to significantly improve the chart's sensitivity to process\u00a0\u2026", "num_citations": "39\n", "authors": ["445"]}
{"title": "Analysis of maintenance policies for finite life-cycle multi-state systems\n", "abstract": " Maintenance policies for multi-state systems (MSS) are often analyzed under infinite horizon assumptions. In practice, it is important to consider maintenance policies under a finite horizon because the life cycles of most systems are finite. In this paper, we consider a finite life-cycle MSS that is subject to both degradation and Poisson failures. We study two classes of maintenance policies \u2013 preventive replacements and corrective replacements, and their effectiveness in controlling the customer\u2019s expected discounted maintenance cost (EDMC). For both policies, replacement decisions are modelled via two control parameters \u2013 a threshold on the current system state and a threshold on the residual life cycle, which is measured as the time span from present to the end of life cycle. We derive close-to-explicit forms of the cost models under each of the policy. Methodologies for optimizing the maintenance thresholds are\u00a0\u2026", "num_citations": "39\n", "authors": ["445"]}
{"title": "Defending simple series and parallel systems with imperfect false targets\n", "abstract": " This paper analyzes the optimal distribution of defense resources between protecting the genuine system elements and deploying imperfect false targets (FTs) in simple series and parallel systems. The FTs are not perfect and the attacker can detect a FT with a non-zero probability. Once the attacker has detected certain number of FTs, it ignores them and chooses such number of undetected targets to attack that maximizes the expected damage to the system. The defender decides how many FTs to deploy in order to minimize the expected damage to the system assuming that the attacker uses the most harmful strategy to attack. The expected damage to a series system is proportional to the probability of system destruction. The expected damage to a parallel system can be defined as proportional to the probability that the demand is not met, or as the amount of the unsupplied demand. The paper demonstrates the\u00a0\u2026", "num_citations": "39\n", "authors": ["445"]}
{"title": "Prioritizing processes in initial implementation of statistical process control\n", "abstract": " A production process composed of tens or even hundreds of subprocesses is a common phenomenon in industry. Each of the subprocesses contributes to various aspects of product quality. Ideally, a control chart can be set up on every subprocess to guarantee the quality of the final product. This is not practical however, because of limited human and economic resources, and the management has to decide which subprocesses are to be given higher priorities. In this paper, for the purpose of prioritizing processes in complicated production systems for implementing statistical process control (SPC) schemes, preliminary selection based on statistical and technical criticality of processes is discussed. An analytic hierarchy process approach based on pair-wise comparisons between several factors in deciding the relative criticality of the processes in a hierarchy structure is then studied. The approach can be used in\u00a0\u2026", "num_citations": "39\n", "authors": ["445"]}
{"title": "Two-stage control charts for high yield processes\n", "abstract": " In this paper, a two-stage control chart for monitoring the defective rate of high-yield processes is proposed and studied. The Cumulative Count of Conforming control chart is generalized by using the number of items inspected until two defective items are observed. As this will increase the time to alarm, a two-stage approach combining both schemes is proposed.  The occurrence of a defective within n1 items inspected in the first stage indicates that the process is out of control. If no defective occurs within n1 items inspected, the occurrence of two defectives within the next  n2 - n1 in the second stage also indicates that the process is out of control. The probability of making a false alarm  at the first and second stages are equal to \u03b11 and \u03b12, respectively. This procedure improves the  sensitivity of the control chart in detecting shifting of the process defective rate p when p is at the parts-per-million order of magnitude.", "num_citations": "39\n", "authors": ["445"]}
{"title": "Optimal inspection and replacement policy based on experimental degradation data with covariates\n", "abstract": " In this article, a novel maintenance model is proposed for single-unit systems with an atypical degradation path, whose pattern is influenced by inspections. After each inspection, the system degradation is assumed to instantaneously decrease by a random value. Meanwhile, the degrading rate is elevated due to the inspection. Considering the double effects of inspections, we develop a parameter estimation procedure for such systems from experimental data obtained via accelerated degradation tests with environmental covariates. Next, the inspection and replacement policy is optimized with the objective to minimize the Expected Long-Run Cost Rate (ELRCR). Inspections are assumed to be non-periodically scheduled. A numerical algorithm that combines analytical and simulation methods is presented to evaluate the ELRCR. We then investigate the robustness of maintenance policies for such systems by\u00a0\u2026", "num_citations": "38\n", "authors": ["445"]}
{"title": "A study of reliability of multi\u2010state systems with two performance sharing groups\n", "abstract": " The performance sharing can be widely seen in different kinds of engineering systems, such as meshed power distribution systems and interconnected data transmission systems. This paper presents a study of systems consisting of multi\u2010state units connected as two performance sharing groups, and the suggested methodology can be adapted for the case of three or more performance sharing groups. To be more general, the system unit is allowed to be in one single performance sharing group or both. Each unit has a random demand to satisfy, and the units can transmit capacity with each other given that the total performance transmitted in each performance sharing group does not surpass its maximum transmission capacity. An algorithm based on the universal generating function technique is proposed to evaluate the system reliability and the expected system performance deficiency. Copyright \u00a9 2016 John\u00a0\u2026", "num_citations": "38\n", "authors": ["445"]}
{"title": "Video QoE killer and performance statistics in WebRTC-based video communication\n", "abstract": " In this paper, we investigate session-related performance statistics of a Web-based Real-Time Communication (WebRTC) application called appear.in. We explore the characteristics of these statistics and explore how they may relate to users' Quality of Experience (QoE). More concretely, we have run a series of tests involving two parties and according to different test scenarios, and collected real-time session statistics by means of Google Chrome's WebRTC-internals tool. Despite the fact that the Chrome statistics have a number of limitations, our observations indicate that they are useful for QoE research when these limitations are known and carefully handled when performing post-processing analysis. The results from our initial tests show that a combination of performance indicators measured at the sender's and receiver's end may help to identify severe video freezes (being an important QoE killer) in the\u00a0\u2026", "num_citations": "38\n", "authors": ["445"]}
{"title": "Design and application of exponential chart for monitoring time-between-events data under random process shift\n", "abstract": " Exponential charts based on time-between-events (TBE) data were developed for monitoring high-yield process like the process which has achieved six-sigma quality level and has recently shown to be very useful in manufacturing systems, in reliability and maintenance monitoring, and also in service-related applications in general. This article develops an economic model of the exponential chart (known as TBErandom chart) for monitoring time-between-events data; the design algorithm considers the random characteristic of the process shifts and therefore better reflects the real process conditions. The probability distribution of the random process shift is modeled by a Rayleigh distribution based on the sample data acquired during the operation of the control chart. The design of the proposed control chart scheme is demonstrated, and the properties are compared with that of other exponential charts\u00a0\u2026", "num_citations": "38\n", "authors": ["445"]}
{"title": "A study of genetic algorithm for project selection for analogy based software cost estimation\n", "abstract": " Software cost estimation is critical for software project management. Many approaches have been proposed to estimate the cost with current project by referring to the data collected form past projects. Analogy based estimation (ABE), which is essentially a case-based reasoning (CBR) approach, is one of such techniques. In order to achieve successful results from ABE, many previous studies proposed effective methods to optimize the weights of the features (feature weighting). However ABE is still criticized for the low prediction accuracy, and the sensitivity to the outliers. To alleviate these drawbacks, we introduce the selection of appropriate project subsets (project selection) by genetic algorithm. The promising results of the proposed method and the comparisons against other ABE model and machine learning techniques indicate our method's effectiveness and potential as a candidate method for software cost\u00a0\u2026", "num_citations": "37\n", "authors": ["445"]}
{"title": "An EWMA monitoring scheme with a single auxiliary variable for industrial processes\n", "abstract": " When using control charts to monitor manufacturing processes, Shewhart control chart is known to be useful for detecting transient shifts, while the EWMA and CUSUM charts are useful for detecting persistent shifts. The efficiency of EWMA chart in monitoring location parameter can be improved by using an auxiliary variable that is closely related to the variable of interest. In this paper, an EWMA-type scheme using ratio estimator is developed to further increase the effectiveness of the classical EWMA chart in monitoring the location parameter. The proposed procedure outperforms the classical EWMA and even the mixed EWMA-CUSUM chart, especially when there is a strong positive relationship between the variable of interest and the auxiliary variable. Finally, a real data set is used to show the implementation procedures of the proposed chart.", "num_citations": "36\n", "authors": ["445"]}
{"title": "Optimal resource distribution between protection and redundancy considering the time and uncertainties of attacks\n", "abstract": " This paper presents a study of the problem of resource allocation between increasing protection of components and constructing redundant components in parallel systems subject to intentional threats. The defender aims at minimizing the entire system destruction probability during certain time horizon by using the best resource allocation strategy which is determined by redundant components construction pace. Different from previous works which focus on the static resource allocation strategy, we propose a dynamic resource distribution strategy with geometric construction pace model and show its advantage over constant construction pace. The vulnerability model considering a most probable attack time and uncertainties of attack time estimates is provided and a destruction probability is evaluated to quantitatively define the ability of the system to survive an intentional attack. The random time of intentional\u00a0\u2026", "num_citations": "36\n", "authors": ["445"]}
{"title": "Design of exponential control charts based on average time to signal using a sequential sampling scheme\n", "abstract": " Exponential charts based on time-between-events (TBE) data are widely investigated and applied in various fields. The average time to signal (ATS) is used instead of the average run length to evaluate the performance of TBE charts, since the ATS involves both the number and the time of samples inspected until a signal occurs. An ATS-unbiased exponential control chart is proposed when the in-control parameter is known. Considering the need in practice to start monitoring a production process as soon as possible, a sequential sampling scheme is adopted and the in-control parameter is estimated by an unbiased and consistent estimator. Some specific guidelines to stop updating control limits are obtained from the relationship between the phase I sample size and the actual false alarm rate. Finally, two real examples are given to illustrate the implementation and efficiency of the proposed method.", "num_citations": "36\n", "authors": ["445"]}
{"title": "Reliability evaluation of hard disk drive failures based on counting processes\n", "abstract": " Reliability assessment for hard disk drives (HDDs) is important yet difficult for manufacturers. Motivated by the fact that the particle accumulation in the HDDs, which accounts for most HDD catastrophic failures, is contributed from the internal and external sources, a counting process with two arrival sources is proposed to model the particle cumulative process in HDDs. This model successfully explains the collapse of traditional ALT approaches for accelerated life test data. Parameter estimation and hypothesis tests for the model are developed and illustrated with real data from a HDD test. A simulation study is conducted to examine the accuracy of large sample normal approximations that are used to test existence of the internal and external sources.", "num_citations": "36\n", "authors": ["445"]}
{"title": "A model for integer-valued time series with conditional overdispersion\n", "abstract": " In this paper, a new model, motivated by the weekly dengue cases in Singapore from year 2001 to 2010, is proposed to handle the conditional equidispersion, overdispersion and underdispersion in integer-valued pure time series. It is shown that the INARCH model studied by earlier researchers is a special case. Conditions for weak and strict stationarity of this model are also given in our paper. Some basic properties of this model are shown to be parallel to those of the classical autoregressive model. Three distribution based methods and two non-distribution based methods are presented for parameter estimation. These methods are compared in a simulation study for the conditional overdispersed situation with an integer-valued pure time series of order one. Finally, this model is applied to the motivating example.", "num_citations": "36\n", "authors": ["445"]}
{"title": "Early software reliability prediction with extended ANN model\n", "abstract": " Generally, software reliability models can provide accurate reliability measurement in the later phase of testing. However, predictions in the early phase of software testing are useful as cost-effective and timely feedback. Early prediction is also feasible in practice with information from previous releases or similar projects. Such information has been utilized well for early reliability prediction with NHPP models by assuming the same failure rate between two similar projects. Alternatively, in this paper, we propose to \"reuse\" failure data from past projects/releases with ANN models to improve early reliability for current project/release. To illustrate the proposed approach, two numerical examples are developed. Better prediction performance is observed in early phase of testing compared with original ANN model without failure data reuse. Furthermore, the optimal switching point from proposed approach to original ANN\u00a0\u2026", "num_citations": "36\n", "authors": ["445"]}
{"title": "Adopting Six Sigma in higher education: some issues and challenges\n", "abstract": " This exploratory paper attempts to study the feasibility of applying the Six Sigma framework in higher education. Some fundamental issues and challenges in designing an effective Six Sigma training programme, integrating Define-Measure-Analysis-Improve-Control (DMAIC) methodology and statistical quality engineering education into existing curriculum and the potential applications of six sigma for educational excellence are discussed. Some strategic relevance among Six Sigma, education, Sun Tzu's art of war thinking and Da Vinci's principles exist; and these provide useful strategic insights. From a systems perspective and depending on the context, adopting the Six Sigma framework in higher education is feasible. Understanding the DMAIC methodology and education in statistical quality engineering are important for Six Sigma project success. Using the operational amplifier circuit analysis drawn from the\u00a0\u2026", "num_citations": "36\n", "authors": ["445"]}
{"title": "On a generalized k-out-of-n system and its reliability\n", "abstract": " A consecutive-k-out-of-n system is a system with n components arranged either linearly or circularly, which fails if and only if at least k consecutive components fail. An (n,\u2009f,\u2009k) system further requires that the total number of failed components is less than f for the system to be working. Here we consider a more general system consisting of N modules with the ith module-composed of n  i  components in parallel; the system fails if and only if there exist at least f failed components or at least k consecutive failed modules. In this paper, some formulae for the reliability of such a generalized k-out-of-n system are derived for both the linear and the circular cases. The recursive formulae established here can be easily computed. Many existing results are also shown to be special cases of the results obtained in this paper. Furthermore, we investigate some component importance properties.", "num_citations": "36\n", "authors": ["445"]}
{"title": "Study of a Markov model for a high-quality dependent process\n", "abstract": " For high-quality processes, non-conforming items are seldom observed and the traditional p (or np) charts are not suitable for monitoring the state of the process. A type of chart based on the count of cumulative conforming items has recently been introduced and it is especially useful for automatically collected one-at-a-time data. However, in such a case, it is common that the process characteristics become dependent as items produced one after another are inspected. In this paper, we study the problem of process monitoring when the process is of high quality and measurement values possess a certain serial dependence. The problem of assuming independence is examined and a Markov model for this type of process is studied, upon which suitable control procedures can be developed.", "num_citations": "36\n", "authors": ["445"]}
{"title": "Optimum prioritisation and resource allocation based on fault tree analysis\n", "abstract": " Fault tree analysis (FTA) is a technique widely used in the study of the reliability of industrial systems and to quantify risks associated with potentially hazardous systems. Most of the studies carried out are related to how to construct a fault tree and how to carry out qualitative and quantitative analysis. However, this paper studies an approach for prioritisation and optimum resource allocation by making use of the FTA technique. The basic idea is to develop a simple procedure for the ranking of basic elements in the complex system, so that maximum increase in reliability can be achieved. We compare our approach with the existing basic event importance measures, and show that the simple approach is easy to apply and provides ranking that is similar to other more complicated approaches. In addition, the new ranking approach can be used at the initial stages of fault tree construction as it does not require the whole\u00a0\u2026", "num_citations": "36\n", "authors": ["445"]}
{"title": "Alternative methods for the simultaneous monitoring of simple linear profile parameters\n", "abstract": " In many industrial processes, the quality characteristic of interest has a relation (linear or non-linear) with other supporting variable(s). Simple linear profile is a well-known term used for the quality characteristic, which is linearly associated with another descriptive variable, and the monitoring of simple linear profile parameters (i.e., slope, intercept, and error variance) is known as linear profiling. In the literature, a well-known approach named as EWMA_3\u00a0chart is used for the simultaneous monitoring of intercept, slope, and error variance. This approach is very efficient as compared to EWMA/R, Hotelling T2, and Shewhart_3 charts but it is a tedious method, since distinct pair of control limits require individual charting constant for each parameter. In this study, new methods are designed for the simultaneous monitoring of simple linear profile parameters, which requires single charting constant and have\u00a0\u2026", "num_citations": "35\n", "authors": ["445"]}
{"title": "Optimal designs of the variable sample size and sampling interval X chart when process parameters are estimated\n", "abstract": " The idea of varying the X\u00af chart\u2019s parameters has been explored extensively by many researchers. The variable sample size and sampling interval (VSSI) X\u00af chart is among the adaptive control charts which improves the diagnostic abilities of the standard X\u00af chart for a quick detection of small and moderate shifts in the process mean. The VSSI X\u00af chart is usually investigated under the assumption of known process parameters. In practice, process parameters are rarely known and they need to be estimated from an in-control historical Phase-I dataset. Therefore, in this paper, the Markov chain approach for the VSSI X\u00af chart with estimated parameters is developed to facilitate process monitoring in manufacturing and service industries. The performance of the VSSI X\u00af chart is examined and evaluated when process parameters are estimated and is compared with the case where process parameters are known. The\u00a0\u2026", "num_citations": "35\n", "authors": ["445"]}
{"title": "Optimal defence of single object with imperfect false targets\n", "abstract": " The paper considers an object exposed to external intentional attacks. The defender distributes its resource between deploying false targets and protecting the object. The false targets are not perfect and there is a nonzero probability that a false target can be detected by the attacker. Once the attacker has detected a certain number of false targets, it ignores them and chooses such number of undetected targets to attack that maximizes the probability of the object destruction. The defender decides how many false targets to deploy in order to minimize the probability of the object destruction assuming that the attacker uses the most harmful strategy to attack. The optimal number of false targets and the optimal number of attacked targets are obtained for the case of single and multiple types of the false targets. A methodology of finding the optimal defence strategy under uncertain contest intensity is suggested.", "num_citations": "35\n", "authors": ["445"]}
{"title": "CUSUM chart with transformed exponential data\n", "abstract": " Time Between Events (TBE) charts were proposed to monitor the time between events occur based on exponential distribution, and have been shown to be more effective than monitoring the fraction non conforming directly. In this article, we consider monitoring the TBE data with CUSUM scheme by transformation. The idea behind it is to transform the TBE data to normal, and then apply the CUSUM scheme for the approximate normal data. Several simple transformation methods are examined. The calculation of Average Run Length (ARL) with Markov chain approach is described. Comparative studies on the ARL performance show that the transformed CUSUM is superior to the X-MR (Moving Range) chart with transformation, the Cumulative Quantity Control (CQC) chart, and have comparable performance with exponential CUSUM charts. The design procedures of optimal CUSUM chart are also presented. This\u00a0\u2026", "num_citations": "34\n", "authors": ["445"]}
{"title": "QFD optimization using linear physical programming\n", "abstract": " A new approach to quality function deployment (QFD) optimization is presented. The approach uses the linear physical programming (LPP) technique to maximize overall customer satisfaction in product design. QFD is a customer-focused product design method which translates customer requirements into product engineering characteristics. Because market competition is multidimensional, companies must maximize overall customer satisfaction by optimizing the design of their products. At the same time, all constraints (e.g. product development time, development cost, manufacturing cost, human resource in design and production, etc.) must be taken into consideration. LPP avoids the need to specify an importance weight for each objective in advance. This is an effective way of obtaining optimal results. Following a brief introduction to LPP in QFD, the proposed approach is described. A numerical example is\u00a0\u2026", "num_citations": "34\n", "authors": ["445"]}
{"title": "Optimal control limits for CCC charts in the presence of inspection errors\n", "abstract": " The control chart based on cumulative count of conforming (CCC) items between the occurrence of two non\u2010conforming ones, or the CCC chart, has been shown to be very useful for monitoring high\u2010quality processes. However, as in the implementation of other Shewhart\u2010type control charts, it is usually assumed that the inspection is free of error. This assumption may not be valid and this may have a significant impact on the interpretation of the control chart and the setting of control limits. This paper first investigates the effect of inspection errors and discusses the setting of control limits in such cases. Even if inspection errors are considered, the average time to alarm increases in the beginning when the process deteriorates. Since this is undesirable, the control limits in the presence of inspection errors should be set so as to maximize the average run length when the process is at the normal level. A procedure is\u00a0\u2026", "num_citations": "34\n", "authors": ["445"]}
{"title": "An unpunctual preventive maintenance policy under two-dimensional warranty\n", "abstract": " In the business-to-consumer context, e.g., the automobile industry, preventive maintenance (PM) of warranted items usually relies upon customers to return their items to authorized maintenance centers according to prescribed schedules. However, item owners may be unpunctual, causing actual maintenance instants deviating from scheduled instants. This paper studies the impact of customer unpunctuality on the optimization of PM policy and the resultant warranty expenses. An unpunctual imperfect PM policy, which allows customers to advance or postpone scheduled PM activities in a tolerated range, is proposed for repairable items sold with a two-dimensional warranty. The expected total warranty costs of the unpunctual (and punctual) PM policies are derived under the assumption that customer unpunctuality is governed by a specific probability distribution. The optimization and comparison of the two policies\u00a0\u2026", "num_citations": "33\n", "authors": ["445"]}
{"title": "Underreporting of homicides by police in the United States, 1976-2013\n", "abstract": " To identify sources of error in estimates of the number of homicides committed by police officers in the United States, we examined data from the Supplementary Homicide Report (SHR) and the National Vital Statistics System (NVSS) between 1976 and 2013. Coverage and nonresponse errors were the primary reasons for underreporting in the SHR, with some agencies not participating or failing to submit some monthly reports. Measurement errors were the primary source of underreporting in the NVSS. If police involvement was not mentioned on death certificate, the death was misclassified as a civilian homicide.", "num_citations": "33\n", "authors": ["445"]}
{"title": "Generalized moment-independent importance measures based on Minkowski distance\n", "abstract": " Importance measures have been widely studied and applied in reliability and safety engineering. This paper presents a general formulation of moment-independent importance measures and several commonly discussed importance measures are unified based on Minkowski distance (MD). Moment-independent importance measures can be categorized into three classes of MD importance measures, i.e. probability density function based MD importance measure, cumulative distribution function based MD importance measure and quantile based MD importance measure. Some properties of the proposed MD importance measures are investigated. Several new importance measures are also derived as special cases of the generalized MD importance measures and illustrated with some case studies.", "num_citations": "33\n", "authors": ["445"]}
{"title": "Markov chain Monte Carlo methods for parameter estimation of the modified Weibull distribution\n", "abstract": " In this paper, the Markov chain Monte Carlo (MCMC) method is used to estimate the parameters of a modified Weibull distribution based on a complete sample. While maximum-likelihood estimation (MLE) is the most used method for parameter estimation, MCMC has recently emerged as a good alternative. When applied to parameter estimation, MCMC methods have been shown to be easy to implement computationally, the estimates always exist and are statistically consistent, and their probability intervals are convenient to construct. Details of applying MCMC to parameter estimation for the modified Weibull model are elaborated and a numerical example is presented to illustrate the methods of inference discussed in this paper. To compare MCMC with MLE, a simulation study is provided, and the differences between the estimates obtained by the two algorithms are examined.", "num_citations": "33\n", "authors": ["445"]}
{"title": "Mean residual life and other properties of Weibull related bathtub shape failure rate distributions\n", "abstract": " The two-parameter Weibull distribution is widely used in reliability analysis. Because of its monotonic ageing behaviour, its applicability is hampered in certain reliability situations. Several generalizations and extensions of the Weibull model have been proposed in the literature to overcome this limitation but their properties have not yet been described in a unified manner. In this paper, graphical displays of the mean residual life curves of several families of Weibull related life distributions are given together with their corresponding failure rate functions. The relationship between these two functions are visibly demonstrated. We focus our attention on the Weibull related families that have bathtub or modified bathtub shape failure rates. Important reliability characteristics such as burn-in, change point and flatness of bathtub of these families are examined. Model selection and parameters estimation are also discussed.", "num_citations": "33\n", "authors": ["445"]}
{"title": "A model of storage reliability with possible initial failures\n", "abstract": " Recently, storage reliability has attracted attention because of the increasing demand for high reliability of products in storage in both military and commercial industries. In this paper we study a general storage reliability model for the analysis of storage failure data. It is indicated that the initial failures, which are usually neglected, should be incorporated in the estimation of storage failure probability. Data from the reliability testing before and during the storage should be combined to give more accurate estimates of both initial failure probability and the probability of storage failures. The results are also useful for decision-making concerning the amount of testing to be carried out before storage. A numerical example is also given to illustrate the idea.", "num_citations": "33\n", "authors": ["445"]}
{"title": "Maintenance scheduling for multicomponent systems with hidden failures\n", "abstract": " This paper develops a maintenance policy for a multicomponent system subject to hidden failures. Components of the system are assumed to suffer from hidden failures, which can only be detected at inspection. The objective of the maintenance policy is to determine the inspection intervals for each component such that the long-run cost rate is minimized. Due to the dependence among components, an exact optimal solution is difficult to obtain. Concerned with the intractability of the problem, a heuristic method named \u201cbase interval approach\u201d is adopted to reduce the computational complexity. Performance of the base interval approach is analyzed, and the result shows that the proposed policy can approximate the optimal policy within a small factor. Two numerical examples are presented to illustrate the effectiveness of the policy.", "num_citations": "32\n", "authors": ["445"]}
{"title": "Reliability analysis and optimization of weighted voting systems with continuous states input\n", "abstract": " Weighted voting systems are widely used in many practical fields such as target detection, human organization, pattern recognition, etc. In this paper, a new model for weighted voting systems with continuous state inputs is formulated. We derive the analytical expression for the reliability of the entire system under certain distribution assumptions. A more general Monte Carlo algorithm is also given to numerically analyze the model and evaluate the reliability. This paper further proposes a reliability optimization problem of weighted voting systems under cost constraints. A genetic algorithm is introduced and applied as the optimization technique for the model formulated. A numerical example is then presented to illustrate the ideas.", "num_citations": "32\n", "authors": ["445"]}
{"title": "State probability of a series-parallel repairable system with two-types of failure states\n", "abstract": " This paper presents a method for the analysis of a series-parallel safety-critical system where the system states can be distinguished into failure-safe and failure-dangerous. The method incorporates the Markov chain and universal generating function technique. In the model considered, both periodic inspection and repair (perfect and imperfect) of system elements are taken into account. The system state distributions and the overall system safety function are derived, based on the developed model. The proposed method is applicable to complex systems for analysing state distributions and it is also useful in decision-making such as determining the optimal proof-test interval or repair resource allocation. An illustrative example is given.", "num_citations": "32\n", "authors": ["445"]}
{"title": "Reliability and modeling of systems integrated with firmware and hardware\n", "abstract": " Firmware is embedded software in hardware devices and they play important role for many critical systems' function. Firmware failure rate in operation should be quite lower than the application software which is operating on it. Most of the study on software reliability deals with systems during development, and it is also important to study the integrated system during operation. Complex systems usually have a bathtub-shaped failure rate over the lifecycle of the product. This paper discusses the parametric analysis of model given by Haupt and Sch\u00e4be (1992), exponentiated Weibull distribution and models generated from Pareto and Weibull distribution, as well as their possible application to modeling firmware system failure. In addition, the Safety Integrity Levels (SIL) stipulated in IEC 61508 are taken into account in the modeling since the safety-critical systems in general are firmware-dominated.", "num_citations": "32\n", "authors": ["445"]}
{"title": "Exponential approximation for maintained Weibull distributed component\n", "abstract": " Exponential distribution is widely used in reliability and maintainability studies although it is well known that the constant failure rate assumption may not be valid. The purpose of this paper is to investigate the use of exponential distribution as an approximation. In fact, for components undergoing regular maintenance or replacement, the exponential assumption can be acceptable. In this paper, the exponential approximation for regularly maintained Weibull component is studied. The approximated exponential distribution using the average failure rate is compared with the exact reliability. The asymptotic relative error is derived, which can be used to adjust the exponential approximation when needed. Based on the framework of exponential approximation for Weibull distributed components, the problems of decision\u2010making regarding the optimal maintenance time and spare allocation are also addressed.", "num_citations": "32\n", "authors": ["445"]}
{"title": "Analysis of repairable system failure data using time series models\n", "abstract": " Repairable system reliability analysis is very important to industry and, for complex systems, replacing a failed component is the most commonly used corrective maintenance action as it is an inexpensive way to restore the system to its functional state. However, failure data analysis for repairable system is not an easy task and usually a number of assumptions which are difficult to validate have to be made. Despite the fact that time series models have the advantage of few such assumptions and they have been successfully applied in areas such as chemical processes, manufacturing and economics forecasting, its use in the field of reliability prediction has not been that widespread. In this paper, we examine the usefulness of this powerful technique in predicting system failures. Time series models are statistically and theoretically sound in their foundation and no postulation of models is required when analysing\u00a0\u2026", "num_citations": "32\n", "authors": ["445"]}
{"title": "A practical method for the estimation of software reliability growth in the early stage of testing\n", "abstract": " The traditional approach of reliability prediction using software reliability growth models requires a large number of failures which might not be available at the beginning of the testing. The commonly used maximum likelihood estimates may not even exist or converge to a reasonable value. In this paper, an approach of making use of information from similar projects in order to obtain an early estimation of one model parameter for a current project is studied. As most of the two-parameter reliability growth models contains one parameter related to the number of faults in the software and a reliability growth rate parameter related to the testing efficiency, information from a similar project can used to estimate the reliability growth rate parameter and the limited failure data from initial testing is used to estimate the other parameter. Our case study shows that this approach is very easy to use as the estimation does not\u00a0\u2026", "num_citations": "32\n", "authors": ["445"]}
{"title": "Neighborhood immigrant concentration and violent crime reporting to the police: A multilevel analysis of data from the National Crime Victimization Survey\n", "abstract": " Using data from the Area\u2010Identified National Crime Victimization Survey (NCVS), we provide a national assessment of the impact of neighborhood immigrant concentration on whether violence is reported to the police. By drawing on multiple theoretical perspectives, we outline how the level of violence reporting could be higher or lower in immigrant neighborhoods, as well as how this may depend on individual race/ethnicity and the history of immigration in the county in which immigrant neighborhoods are located. Controlling for both individual\u2010 and neighborhood\u2010level conditions, our findings indicate that within traditional immigrant counties, rates of violence reporting in immigrant neighborhoods are similar to those observed elsewhere. In contrast, within newer immigrant destinations, we observe much lower rates of violence reporting in neighborhoods with a large concentration of immigrants. Our study findings\u00a0\u2026", "num_citations": "31\n", "authors": ["445"]}
{"title": "Utilizing experimental degradation data for warranty cost optimization under imperfect repair\n", "abstract": " Manufacturers usually want to predict the warranty cost for new products under affordable maintenance policies. With insufficient reliability information, experimental degradation tests are commonly conducted to predict the field reliability before the products are put on the market. In this paper, we propose a novel warranty cost optimization framework based on degradation data within a finite warranty period under the assumption of imperfect repairs. The expected number of warranty claims is given in the analytical form. Two sources of uncertainty are considered to estimate the field reliability for more realistic warranty cost prediction: the uncertainty in experimental data and the variation in field conditions. Effects of imperfect repairs are assumed to be random. The warranty cost for a single repair is assumed to be associated with the improvement factor of imperfect repairs. Optimal imperfect repair policy is obtained\u00a0\u2026", "num_citations": "31\n", "authors": ["445"]}
{"title": "A hybrid risks-informed approach for the selection of supplier portfolio\n", "abstract": " In conventional supplier selection approaches, cost consideration is usually emphasised and it renders a vulnerable supply chain with various risks. This article aims to develop a quantitative approach for modelling both supply chain operational risks and disruption risks to support decision-making with regard to order allocation and risk mitigation. We introduce two types of risk evaluation models: value-at-risk (VaR) and conditional value-at-risk (CVaR). Specifically, VaR is used to measure operational risks caused by improper selection and operations of a supplier portfolio to the stochastic demand, which may frequently occur but result in relatively small losses to supply chains; CVaR is used to evaluate disruption risks that are less frequent and tend to cause significant damage. After incorporating risk factors into a probability-based multi-criteria optimisation model, different methods and parameters are compared\u00a0\u2026", "num_citations": "31\n", "authors": ["445"]}
{"title": "Design of gamma charts based on average time to signal\n", "abstract": " Gamma charts for time between events are very useful in the high\u2010quality processes, which monitor the time until the rth event. The average time to signal (ATS) is adopted to evaluate the performance of Gamma charts, because it reflects both the number and the sampling interval of samples inspected until an out\u2010of\u2010control signal occurs. An ATS\u2010unbiased design for Gamma charts with known parameters is proposed based on the hypothesis test of the scale parameter. For the phase I monitoring, a new ATS\u2010unbiased design with unknown parameters is developed, and a sequential sampling scheme is adopted to start process monitoring as soon as possible. Some specific guidelines to stop updating the control limits are suggested from the convergence of the width between control limits with different phase I sample sizes. Finally, a real example is illustrated to demonstrate the implementation of the proposed\u00a0\u2026", "num_citations": "31\n", "authors": ["445"]}
{"title": "A burn-in scheme based on percentiles of the residual life\n", "abstract": " Many commercial products sold with warranties suffer from infant-mortality problems. Burn-in is a common means to mitigate the impact of early failures on warranty costs. After burn-in, the p-percentile function of the residual life (PRL-p function) is of particular interest as, in some cases, there exists a change point at which the PRL-p function reaches a maximum value. When the failure probability during the warranty period is prespecified, this change point naturally gives rise to an optimal burn-in duration. Moreover, the maximal PRL-p represents the maximum allowable warranty period when the expected field return is set at p. We present some properties of this change point, and derive the asymptotic distribution of its parametric maximum likelihood estimator and that of the corresponding PRL-p. The procedure is then applied to estimate a set of desirable burn-in durations and the corresponding warranty periods\u00a0\u2026", "num_citations": "31\n", "authors": ["445"]}
{"title": "Confidence intervals for the scale parameter of the power-law process\n", "abstract": " The power-law process (PLP) is a two-parameter model widely used for modeling repairable system reliability. Results on exact point estimation for both parameters as well as exact interval estimation for the shape parameter are well known. In this paper, we investigate the interval estimation for the scale parameter. Asymptotic confidence intervals are derived using Fisher information matrix and theoretical results by Cocozza-Thivent . The accuracy of the interval estimation for finite samples is studied by simulation methods.", "num_citations": "31\n", "authors": ["445"]}
{"title": "Process capability indices for a regularly adjusted process\n", "abstract": " Process capability indices are widely used in industry to achieve and maintain a high-quality level of manufactured items. The successful application of process capability indices requires the process to be stationary. This is not the case for trended processes with regular adjustment, which are very common in manufacturing processes. An interesting and important question is how to define and interpret the process capability for such a process. In this paper, the traditional process capability indices are modified for these trended processes. Two different sources of process variation, one from the process trend, the other from random variation, are identified and used to develop new process capability indices. Based on these new indices, we show that the capability of trended processes can be improved through periodic adjustment to its maximum achievable value. We also develop the procedures to determine the\u00a0\u2026", "num_citations": "31\n", "authors": ["445"]}
{"title": "Optimal condition-based maintenance policy with delay for systems subject to competing failures under continuous monitoring\n", "abstract": " Delay in maintenance operations occurs for many systems in real engineering applications. Random delays increase the variability in maintenance modeling, making the optimization of maintenance policy more complicated. In this paper, a delayed condition-based maintenance (CBM) problem for systems under continuous monitoring is studied. The system is assumed to be affected by competing degradation failures and fatal shocks. The degradation path is modeled by a gamma process, while random fatal shocks are modeled by a non-homogeneous Poisson process, of which the failure intensity has a change point that depends on the degradation level. It is assumed that when the degradation level reaches the alarm threshold, the maintenance operation delays for a random duration of time before its implementation. The main objective here is to choose an appropriate alarm threshold to minimize the expected\u00a0\u2026", "num_citations": "30\n", "authors": ["445"]}
{"title": "Development of RVM-based multiple-output soft sensors with serial and parallel stacking strategies\n", "abstract": " Soft sensors are the most commonly used tools to predict hard-to-measure variables in industrial processes. However, the presence of a large number of hard-to-measure variables always renders a generic single-output soft-sensor inadequate. This brief proposed two multiple-output soft sensors, the former based on a novel serial stacking relevant vector machine (RVM) models, called RVMS, by transforming multiple-output into single-output problems inspired by stacking generation and the latter based on ensemble multivariable RVM (EMRVM) models that are improved by ensemble learning. To further strengthen the predicted ability, least absolute shrinkage and selection operator and canonical correlation analysis are used to remove irrelevant and redundant information from raw features under the supervision of multivariate targets for RVMS and EMRVM, respectively. The proposed methodologies were first\u00a0\u2026", "num_citations": "29\n", "authors": ["445"]}
{"title": "Applying importance measures to risk analysis in engineering project using a risk network model\n", "abstract": " Risk analysis and prioritization are a key process in project risk management (PRM). Its outcomes serve as input of the risk response planning process where decisions are made. Complexity of projects is characterized by the emergence of phenomena that are difficult to detect and to manage using classical methods. It may disturb risk assessment, on which priorities are further established. This paper aims at using importance measure (IM) techniques in the complex PRM field. This involves modeling the complex project risk network and providing complementary analysis results based on risk IMs accounting for risk interactions. These new project risk indicators allow the manager for a more comprehensive understanding of the risks. An application to a complex engineering project is provided to illustrate this approach to assess both risks and risk interactions, to establish priorities for further decision making.", "num_citations": "29\n", "authors": ["445"]}
{"title": "Outlier identification and robust parameter estimation in a zero-inflated Poisson model\n", "abstract": " The Zero-inflated Poisson distribution has been used in the modeling of count data in different contexts. This model tends to be influenced by outliers because of the excessive occurrence of zeroes, thus outlier identification and robust parameter estimation are important for such distribution. Some outlier identification methods are studied in this paper, and their applications and results are also presented with an example. To eliminate the effect of outliers, two robust parameter estimates are proposed based on the trimmed mean and the Winsorized mean. Simulation results show the robustness of our proposed parameter estimates.", "num_citations": "29\n", "authors": ["445"]}
{"title": "Time-between-events charts for on-line process monitoring\n", "abstract": " With the development of automation and high-quality manufacturing techniques, on-line process monitoring system has become essential for enterprises to ensure product quality and reduce cost. This paper presents an on-line process monitoring system, which provides the users with different charting methods to monitor time-between-events data, and detect process shifts. Some advanced time-between-events control charts, such as cumulative quantity control (CQC) chart, CQC-r chart, exponential EWMA and exponential CUSUM chart are discussed. Comparisons of their performance are carried out based on average time to signal (ATS). The design of an on-line SPC system is described, followed by an application example. The approaches presented in this paper complement other on-line systems and can be integrated into existing systems easily.", "num_citations": "29\n", "authors": ["445"]}
{"title": "Economic design of control chart for trended processes\n", "abstract": " Statistical control charts are widely used in manufacturing industry. However, for processes exhibiting certain trend pattern, which stems mainly from deteriorating elements such as power consumption or tool-wear, traditional control charts have to be interpreted differently. This paper addresses appropriate timing problem of making adjustment to such trended processes through economical design of its control chart. This extends the traditional study on economic design of control chart, which mainly focuses on the setting of control limits. An economic design model is developed and analytical results are presented. An application example is used to illustrate the decision-making and analysis procedure.", "num_citations": "29\n", "authors": ["445"]}
{"title": "Rank-based EWMA procedure for sequentially detecting changes of process location and variability\n", "abstract": " This paper presents a study of a new procedure, which is based on integrating a powerful nonparametric test for the two-sample problem and EWMA control scheme to online sequential monitoring. The proposed procedure, based on individual observation per sample, can be used to monitor the location and the scale parameters of a univariate continuous distribution, simultaneously. An iterative computation procedure is developed for computing the monitoring statistics. A search algorithm for the control limit based on Monte-Carlo simulation and bisection method is derived and a table is provided. The sensitivity analysis on the procedure is studied in detail. Monte-Carlo simulation results show that the proposed procedure is quite robust to nonnormally distributed data, and moreover, it is efficient in detecting various process shifts. A real data example from a chemical reaction process is shown to illustrate the\u00a0\u2026", "num_citations": "28\n", "authors": ["445"]}
{"title": "Software reliability modelling and optimization for multi-release software development processes\n", "abstract": " During the lifespan of large software systems, iterative development procedure is commonly adopted with continuously incremental software versions released to the market. When to release each release plays an important role for balancing the competition in market and the risk of low-quality software. Traditionally, release-time issue is addressed with software reliability models for single version. How to model the reliability of multi-release software development process is our concern now. It is interesting to study the dynamics of software faults during this releasing procedure. Without the loss of generality, a specific iterative software development scenario is considered for our current study, where a software development team develops, tests and releases software version by version. The trend of the remaining number of faults over different versions is of great concern, and a modeling framework is proposed to\u00a0\u2026", "num_citations": "28\n", "authors": ["445"]}
{"title": "Reliability of fault-tolerant systems with parallel task processing\n", "abstract": " The paper considers performance and reliability of fault-tolerant software running on a hardware system that consists of multiple processing units. The software consists of functionally equivalent but independently developed versions that start execution simultaneously. The computational complexity and reliability of different versions are different. The system completes the task execution when the outputs of a pre-specified number of versions coincide. The processing units are characterized by different availability and processing speed. It is assumed that they are able to share the computational burden perfectly and that execution of each version can be fully parallelized.The algorithm based on the universal generating function technique is used for determining the distribution of system task execution time. This algorithm allows analysts to evaluate complex hardware\u2013software reliability and performance indices\u00a0\u2026", "num_citations": "28\n", "authors": ["445"]}
{"title": "Effects of correlation on fraction non-conforming statistical process control procedures\n", "abstract": " High-yield production processes that involve a low fraction non-conforming are becoming more common, and the limitations of the standard control charting procedures for such processes are well known. This paper examines the control procedures based on the conforming unit run lengths applied to near-zero-defect processes in the presence of serial correlation. Using a correlation binomial model, a few control schemes are investigated and control limits are derived. The results reduce to the traditional case when the measurements are independent. However, it is shown that the false alarm rate cannot be reduced to below the amount of serial correlation present in the process.", "num_citations": "28\n", "authors": ["445"]}
{"title": "On some importance measures of system components\n", "abstract": " Importance measures are helpful in finding which components should receive more attention than others in system development. In this paper a general importance measure using the system yield function is suggested. Some new results are obtained.", "num_citations": "28\n", "authors": ["445"]}
{"title": "Rebooting data-driven soft-sensors in process industries: A review of kernel methods\n", "abstract": " Soft-sensors usually assist in dealing with the unavailability of hardware sensors in process industries, thus allowing for less fault occurrence and better control performance. However, nonlinear, non-stationary, ill-data, auto-correlated and co-correlated behaviors in industrial data always make general data-driven methods inadequate, thus resorting to kernel-based methods provide a necessary alternative. This paper gives a systematic review of various state-of-the-art kernel-based methods with applications for data pre-processing, sample selection, variable selection, model construction and reliability analysis of soft-sensors. An integrated review of various kernel-based soft-sensor modeling methods is attempted, including on-line, multi-output, small-data-driven, multi-step-ahead and semi-supervised applications. The discussion is further to provide an overview of achieving hard-to-measure variable prediction\u00a0\u2026", "num_citations": "27\n", "authors": ["445"]}
{"title": "Optimal preventive maintenance strategy for leased equipment under successive usage-based contracts\n", "abstract": " In the context of equipment leasing, maintenance service is usually bundled with the leased equipment and offered by the lessor as an integrated package under a lease contract. The lessor is then responsible to prescribe an effective maintenance policy to keep the equipment operational in an economical way. This paper investigates upgrade and preventive maintenance (PM) strategies for industrial equipment during successive usage-based lease contracts with consideration of a warranty period, from the lessor's perspective. The accelerated failure time model and age reduction model are adopted to capture the effect of usage rate and imperfect PM/upgrade on the equipment reliability, respectively. More importantly, since equipment usage rates may vary across different lease contracts, this study develops an age correspondence framework to characterise usage rate shifts between successive lease periods\u00a0\u2026", "num_citations": "27\n", "authors": ["445"]}
{"title": "Using accelerated life tests data to predict warranty cost under imperfect repair\n", "abstract": " For new products that have not been put on the market, manufacturers usually want to predict the warranty cost to forecast its influence on future profit. In the test phase of new products, accelerated life tests (ALT) are commonly used to predict the lifetime under use condition. In this paper, we present a framework to predict the warranty cost and risk under one-dimensional warranty by analyzing ALT experimental data. Two sources of variability are considered to make inferences of predicted warranty cost: the uncertainty of estimated parameters from ALT data and the variation in field conditions. Under these assumptions, the expected warranty cost and warranty risk is computed by Markov chain Monte Carlo (MCMC) sampling based on the approximated lifetime distribution. We assume that the warranty guarantees imperfect repairs. The framework could be easily repeated for ALT data based on log-location-scale\u00a0\u2026", "num_citations": "27\n", "authors": ["445"]}
{"title": "Weibull distributions\n", "abstract": " The basic Weibull distribution is considered the most fundamental and basic lifetime distribution. Various extensions of the Weibull distribution have been proposed since the 1970s and are useful in the modeling of complex lifetime data that are beyond the capability of the basic Weibull. This article reviews the properties of the basic Weibull distribution and lists the various extensions. It describes the use of Weibull probability plots as a tool for model selection and discusses the parameter estimation and model validation. It concludes with some topics for future research. WIREs Comp Stat 2011 3 282\u2013287 DOI: 10.1002/wics.157 This article is categorized under:  Statistical Models > Model Selection", "num_citations": "27\n", "authors": ["445"]}
{"title": "Robust regression using probability plots for estimating the Weibull shape parameter\n", "abstract": " The Weibull shape parameter is important in reliability estimation as it characterizes the ageing property of the system. Hence, this parameter has to be estimated accurately. This paper presents a study of the efficiency of using robust regression methods over the ordinary least\u2010squares regression method based on a Weibull probability plot. The emphasis is on the estimation of the shape parameter of the two\u2010parameter Weibull distribution. Both the case of small data sets with outliers and the case of data sets with multiple\u2010censoring are considered. Maximum\u2010likelihood estimation is also compared with linear regression methods. Simulation results show that robust regression is an effective method in reducing bias and it performs well in most cases. Copyright \u00a9 2006 John Wiley & Sons, Ltd.", "num_citations": "27\n", "authors": ["445"]}
{"title": "Service quality analysis: case study of a 3PL company\n", "abstract": " Service quality is important to penetrate, build and maintain market share. The objective of this paper is to study the quality of service provided by 3PL companies. Approaches such as SERVQUAL can be used. A case study is carried out on a 3PL company. The results from SERVQUAL show that the customer values reliability such as documentation accuracy, picking accuracy and on-time delivery. Ease of communication and productivity are also regarded as very important. Quadrant analysis and gap analysis indicate that the company performance is up to the customer's expectations for most of the attributes except for on-time delivery, efficient utilisation of the warehouse and the productivity level. Areas for further improvements are also identified. The case study can serve the purpose for similar analysis in other organisations.", "num_citations": "27\n", "authors": ["445"]}
{"title": "An integrated SPC approach for manufacturing processes\n", "abstract": " Control charts have been widely used in the manufacturing industry as the most important statistical process control (SPC) technique. One assumption for successfully applying control charts is that the process is stable to begin with and adjustments are made only at a later point when the process is deemed to be out of control. A situation that often arises is when the process is subject to a slow trend caused by factors such as equipment deterioration, power decline during its consumption, or other reasons. For this type of situation, an integrated monitoring and adjustment approach can be considered and the relevant issues involved are discussed in this paper. This problem of implementing SPC is studied from the point of view of process management, together with the concept of engineering process control. A general approach is then formulated for monitoring processes with trend and subject to regular\u00a0\u2026", "num_citations": "27\n", "authors": ["445"]}
{"title": "Bayes reliability demonstration test plan for series-systems with binomial subsystem data\n", "abstract": " One reason that the Bayesian approach to reliability demonstration has not gained popularity in industry is the difficulty in establishing the prior. The problem becomes more complicated when only subsystem data are available. It has received little attention in the existing literature and this paper makes an attempt to do that. A method is proposed to derive the Bayesian reliability demonstration test plan for series systems with binomial subsystem data. The method makes use of Mann's approximately optimum lower confidence bound model to derive the system prior based on binomial subsystem data. The system Bayesian reliability demonstration test plan can then be derived using existing methods for meeting posterior confidence requirements. The proposed method is easy to apply and no complicated computation is involved in deriving the system prior distribution. It uses objective subsystem test data. No\u00a0\u2026", "num_citations": "27\n", "authors": ["445"]}
{"title": "A study of the exponential smoothing technique in software reliability growth prediction\n", "abstract": " Software reliability models can provide quantitative measures of the reliability of software systems which are of growing importance today. Most of the models are parametric ones which rely on the modelling of the software failure process as a Markov or non\u2010homogeneous Poisson process. It has been noticed that many of them do not give a very accurate prediction of future software failures as the focus is on the fitting of past data. In this paper we study the use of the double exponential smoothing technique to predict software failures. The proposed approach is a non\u2010parametric one and has the ability of providing more accurate prediction compared with traditional parametric models because it gives a higher weight to the most recent failure data for a better prediction of future behaviour. The method is very easy to use and requires a very limited amount of data storage and computational effort. It can be updated\u00a0\u2026", "num_citations": "27\n", "authors": ["445"]}
{"title": "Models and monitoring of zero\u2010inflated processes: the past and current trends\n", "abstract": " As product quality has increased rapidly in recent years, monitoring and control of products have become more and more difficult. The items were produced with zero defects, and zero\u2010inflated distributions are used to fit the defect count data. Recently, many studies were designed for the estimation and monitoring methods based on the zero\u2010inflated distributions. As zero\u2010inflated models are useful in the modeling of high\u2010yield and rare health\u2010related processes, so, the stated study is designed to provide a summary of past and current trends of monitoring methods under the zero\u2010inflated models. Moreover, a review is done on the several zero\u2010inflated models and their applications in different industries. Finally, some future directions are also highlighted to overcome existing unsolved issues.", "num_citations": "26\n", "authors": ["445"]}
{"title": "Stochastic filtering approach for condition-based maintenance considering sensor degradation\n", "abstract": " This paper proposes a condition-based maintenance (CBM) policy for a deteriorating system whose state is monitored by a degraded sensor. In the literature of CBM, it is commonly assumed that inspection of system state is perfect or subject to measurement error. The health condition of the sensor, which is dedicated to inspect the system state, is completely ignored during system operation. However, due to the varying operation environment and aging effect, the sensor itself will suffer a degradation process and its performance deteriorates with time. In the presence of sensor degradation, the Kalman filter is employed in this paper to progressively estimate the system and the sensor state. Since the estimation of system state is subject to uncertainty, maintenance solely based on the estimated state will lead to a suboptimal solution. Instead, predictive reliability is used as a criterion for maintenance decision-making\u00a0\u2026", "num_citations": "26\n", "authors": ["445"]}
{"title": "Fuzzy copula model for wind speed correlation and its application in wind curtailment evaluation\n", "abstract": " Wind parks always produce diverse percentages of their nominal power at the same time, leading to a concern about correlation between wind speeds. The assessments of wind speed correlation have been particularly focused on probabilistic modeling of aleatory uncertainty. However, poor historical data, imprecise parameter estimation and incomplete knowledge of wind speeds lead to another type of uncertainty, possibilistic uncertainty, which requires an explicit analysis. Therefore, a fuzzy copula model is firstly proposed to express the possibilistic uncertainty of wind speed correlation. The advantage of the proposed model is that the copula parameters can be interval numbers, triangular or trapezoidal fuzzy numbers based on the wind speed data and subjective judgment of decision makers. For estimating copula parameters, a complete decision rule and interval estimation method is developed based on\u00a0\u2026", "num_citations": "26\n", "authors": ["445"]}
{"title": "Development of energy and reserve pre-dispatch and re-dispatch models for real-time price risk and reliability assessment\n", "abstract": " In the future, energy framework of European Union and other countries, renewable energy plays an important role tackling the problems of the climate change and security of energy supply. The high penetration of renewable energy sources will increase the burden of system operator for maintaining system reliabilities. However, the current strategy of reliability management developed for conventional power systems and existing electricity market design may not cope with the future challenges the power system faces. The development of smart grid will enable power system scheduling and the electricity market to operate in a shorter time horizon for better integrating renewable energy sources into power systems. This study presents an electricity market scheme including a multi-period energy and reserve pre-dispatch model and an energy re-dispatch model for real time operation, respectively. The multi-period\u00a0\u2026", "num_citations": "26\n", "authors": ["445"]}
{"title": "A novel approach to DSM-based activity sequencing problem\n", "abstract": " Recently, there has been a growing interest in applying the design structure matrix (DSM) for planning projects that consist of many interrelated activities. One important objective of planning is to find an activity sequence so as to minimize the sum of superdiagonal numbers in a DSM. It is known that the problem is NP-complete and is difficult to solve. In this study, we prove several structural properties of the problem, and propose a heuristic for obtaining good feasible solutions. We find that based on the fold operation, a block of activities can be treated as a single activity. A novel hybrid algorithm is then presented for solving large activity sequencing problems. Finally, we perform a number of experiments and show that good solutions can be easily obtained by our approach. Moreover, the improvement achieved by the proposed approach is significant.", "num_citations": "26\n", "authors": ["445"]}
{"title": "Warranty cost analysis for nonrepairable services products\n", "abstract": " Many service products are installed in a complex system that is operated only when the entire system is completed. The time from their installation to commissioning, called a dormant state in this article, may take several years for systems such as complete buildings or aircraft. Warranties for the products may cover the time starting from their installation to a certain time. Warranty cost on replacements for such products is different from normal products without any dormant state. This article analyses the replacement cost for nonrepairable services products from a manufacturer perspective. We consider four warranty policies, which include two types of warranty terms (i.e., nonrenewing or renewing) and two types of replacements (i.e., with preventive replacement or replacement only upon failures). Relationships between the failure patterns at the dormant state and at the operating state are also discussed. Numerical\u00a0\u2026", "num_citations": "26\n", "authors": ["445"]}
{"title": "A statistical method for controlling software defect detection process\n", "abstract": " The quest for solutions to stay competitive in the marketplace brings software industries to take steps to control their products and processes by introducing metrics as part of their quality system and setting triggers for action when their capability limits are exceeded. Institutionalized software inspection and testing, together with previews and postmortems within each phase of the software development life cycle have been widely recognized as a solid foundation for defect prevention. However, there still lacks a systematic control mechanism for these defect detection activities. In this paper, a statistical method is proposed to control the software defect detection process together with further defect prevention analysis. This method is currently being piloted in the Motorola Singapore Software Centre.", "num_citations": "26\n", "authors": ["445"]}
{"title": "A study of a storage reliability estimation problem\n", "abstract": " Storage reliability, which describes the failure or deterioration of items in a dormant state, is considered in this paper. The study presented here is focused on the estimation of the storage reliability after a certain amount of storage time. We start with simple non\u2010parametric estimation of the current reliability and then study the problem of parametric estimation based on a simple Weibull distribution assumption. Both maximum likelihood estimation and graphical techniques are considered in this case. The study is useful for planning a storage environment and making a decision about the maximum length of storage. Furthermore, the information can be used in the design and improvement of products for which the storage is an important part of the product's life cycle. A numerical example is provided to enlighten the idea.", "num_citations": "26\n", "authors": ["445"]}
{"title": "On a generalization of the JM-model\n", "abstract": " The earliest software reliability model suggested by Jelinski-Moranda has suffered much critics. However, new models may be constructed through removing the unrealistic assumptions. Among the assumptions used, the most serious one is that all software faults contribute the same amount to the failure intensity. In this paper we shall discuss a possibility of modifying this model without making this assumption. We comment on some results and discuss a release problem.", "num_citations": "26\n", "authors": ["445"]}
{"title": "Joint maintenance and spare component provisioning policy for k-out-of-n systems\n", "abstract": " This study develops an analytic framework for joint modeling of age-based preventive maintenance and (s, Q) spare component provisioning policy for k-out-of-n:G systems with i.i.d. nonrepairable components. We assume that during system maintenance, instead of discarding the whole system, only failed components are replaced by spares if the on-handspares are sufficient. The objective is to determine a proper preventive maintenance interval and the ordering parameters to minimize the undiscounted long run average cost under a system availability constraint. Our study sheds light on the lumpy demand scenario in the problems of joint modeling of preventive maintenance and spare provisioning.", "num_citations": "25\n", "authors": ["445"]}
{"title": "A bivariate maintenance policy for multi-state repairable systems with monotone process\n", "abstract": " This paper proposes a sequential failure limit maintenance policy for a repairable system. The objective system is assumed to have k+1 states, including one working state and k failure states, and the multiple failure states are classified potentially by features such as failure severity or failure cause. The system deteriorates over time and will be replaced upon the Nth failure. Corrective maintenance is performed immediately upon each of the first (N-1) failures. To avoid the costly failure, preventive maintenance actions will be performed as soon as the system's reliability drops to a critical threshold R. Both preventive maintenance and corrective maintenance are assumed to be imperfect. Increasing and decreasing geometric processes are introduced to characterize the efficiency of preventive maintenance and corrective maintenance. The objective is to derive an optimal maintenance policy (R*,N*) such that the long\u00a0\u2026", "num_citations": "25\n", "authors": ["445"]}
{"title": "Dynamic benchmarking methodology for quality function deployment\n", "abstract": " Purpose \u2013 The purpose of this paper is to provide a methodology to integrate both the dynamics of competitors' performance and the dynamics of customer preference, along with their interaction, into a quality function deployment (QFD) analysis.Design/methodology/approach \u2013 A systematic dynamic benchmarking methodology is proposed with an illustrative example.Findings \u2013 The analytic hierarchy process's (AHP's) relative measurement might serve as a better way to elicit the customer's judgment over time in the QFD, not only in the importance rating part, but also in the competitive benchmarking part. It is also possible to quantitatively model the AHP priorities' change over time, and incorporate it in the QFD decision\u2010making process.Research limitations/implications \u2013 It might take a certain amount of time and efforts to collect the necessary data over time. However, it might be justified considering the improved\u00a0\u2026", "num_citations": "25\n", "authors": ["445"]}
{"title": "More on the mis\u2010specification of the shape parameter with Weibull\u2010to\u2010exponential transformation\n", "abstract": " When lifetimes follow Weibull distribution with known shape parameter, a simple power transformation could be used to transform the data to the case of exponential distribution, which is much easier to analyze. Usually, the shape parameter cannot be known exactly and it is important to investigate the effect of mis\u2010specification of this parameter. In a recent article, it was suggested that the Weibull\u2010to\u2010exponential transformation approach should not be used as the confidence interval for the scale parameter has very poor statistical property. However, it would be of interest to study the use of Weibull\u2010to\u2010exponential transformation when the mean time to failure or reliability is to be estimated, which is a more common question. In this paper, the effect of mis\u2010specification of Weibull shape parameters on these quantities is investigated. For reliability\u2010related quantities such as mean time to failure, percentile lifetime and\u00a0\u2026", "num_citations": "25\n", "authors": ["445"]}
{"title": "Cold-standby redundancy allocation problem with degrading components\n", "abstract": " Components in cold-standby state are usually assumed to be as good as new when they are activated. However, even in a standby environment, the components will suffer from performance degradation. This article presents a study of a redundancy allocation problem (RAP) for cold-standby systems with degrading components. The objective of the RAP is to determine an optimal design configuration of components to maximize system reliability subject to system resource constraints (e.g. cost, weight). As in most cases, it is not possible to obtain a closed-form expression for this problem, and hence, an approximated objective function is presented. A genetic algorithm with dual mutation is developed to solve such a constrained optimization problem. Finally, a numerical example is given to illustrate the proposed solution methodology.", "num_citations": "24\n", "authors": ["445"]}
{"title": "Optimal testing strategies in overlapped design process\n", "abstract": " Testing is an important activity in product development. Past studies, which are developed to determine the optimal scheduling of tests, often focused on single-stage testing of sequential design process. This paper presents an analytical model for the scheduling of tests in overlapped design process, where a downstream stage starts before the completion of upstream testing. We derive optimal stopping rules for upstream and downstream stages\u2019 testing, together with the optimal time elapsed between beginning the upstream tests and beginning the downstream development. We find that the cost function is first convex then concave increasing with respect to upstream testing duration. A one-dimensional search algorithm is then proposed for finding the unique optimum that minimizes the overall cost. Moreover, the impact of different model parameters, such as the problem-solving capacity and opportunity cost, on\u00a0\u2026", "num_citations": "24\n", "authors": ["445"]}
{"title": "Monitoring time-between-events for health management\n", "abstract": " In the history of health management, control chart techniques have been applied to monitor different types of systems, e.g. human health, equipment health, or healthcare systems. However, with the development of modern technology, the traditional control charts, which monitor the number or the proportion of events occurring in a certain sampling interval, are facing more and more obstacles for high quality process monitoring where the defect rate is low. The time-between-events (TBE) chart which monitors the time between successive occurrences of events is especially suitable for this type of data. The word \u00bftime\u00bf is used to represent the attribute or variable data observed between consecutive events of concern. Based on the characteristic of interest, the TBE charts could be classified into attribute charts and variable charts. Original attribute TBE charts which monitor the number of conforming items observed\u00a0\u2026", "num_citations": "24\n", "authors": ["445"]}
{"title": "On the determination of optimum software release time\n", "abstract": " One of the important applications of software reliability models is the determination of software release time. The author presents some software release policies and discusses the problem of determination of optimum test time. Both reliability requirements and cost models are considered in obtaining specific release policies. It is noted that acceptable failure intensity should be used as a reliability goal and optimum release policy should be based on sequential approach. Some other interesting software release policies are also reviewed.<>", "num_citations": "24\n", "authors": ["445"]}
{"title": "Some results on renewal equations\n", "abstract": " In this paper solutions of renewal-type integral equations are studied. It is proved that a recursively spades;efined approximation to the solution has some nice convergence properties. Some simple bounds and other results on the renewal function and the renewal spades;,ensity are obtained.", "num_citations": "24\n", "authors": ["445"]}
{"title": "Adaptive Event-Triggered Observer-Based Output Feedback  Load Frequency Control for Networked Power Systems\n", "abstract": " This article investigates the event-triggered observer-based output feedback load frequency control (LFC) problem for power systems. To reduce the amount of the transmitted signals, a dynamic event-triggered scheme is proposed by adding an exponential term. Moreover, an adaptive event-triggered scheme is proposed to provide a balance between the control performance and the number of the transmitted signals. Under the proposed schemes, a new model is formulated for the observer-based output feedback LFC system via a time-delay system method. By employing the Lyapunov functional method, sufficient conditions are derived for global asymptotical stability and L \u221e  performance. Then, a controller design method is developed. Finally, two examples are given to illustrate the effectiveness of the proposed schemes.", "num_citations": "23\n", "authors": ["445"]}
{"title": "Quantitative risk assessment through hybrid causal logic approach\n", "abstract": " In this paper, a hybrid causal logic (HCL) model is improved by mapping a fuzzy fault tree (FFT) into a Bayesian network (BN). The first step is to substitute an FFT for the traditional FT. The FFT is based on the Takagi\u2013Sugeno model and the translation rules needed to convert the FFT into a BN are derived. The proposed model is demonstrated in a study of a fire hazard on an offshore oil production facility. It is clearly shown that the FFT can be directly converted into a BN and that the parameters of the FFT can be estimated more accurately using the basic inference techniques of a BN. The improved HCL approach is able to both accurately determine how failures cause an undesired problem using FFT and also model non-deterministic cause\u2013effect relationships among system elements using the BN.", "num_citations": "23\n", "authors": ["445"]}
{"title": "Two MEWMA charts for Gumbel's bivariate exponential distribution\n", "abstract": " Data described by the exponential distribution are commonly encountered in manufacturing processes, reliability analysis, and human-service management. Time between events (TBE) charts have been suggested to monitor exponential data. However, existing studies on TBE charts are limited to univariate cases assuming there is only one process characteristic of interest. In this paper, two multivariate exponential weighted moving average (MEWMA) charts are proposed for the simultaneous monitoring of the mean vector of Gumbel's bivariate exponential (GBE) TBE model: One based on the raw observations and the other based on the transformed data. A numerical example is given to illustrate the implementation of the two MEWMA charts. We compare the average run-length performances of the two proposed charts with the following individual TBE charts pairs: the paired individual t charts, the paired\u00a0\u2026", "num_citations": "23\n", "authors": ["445"]}
{"title": "A model for upside-down bathtub-shaped mean residual life and its properties\n", "abstract": " Mean residual life is an important statistic in reliability analysis. Based on a general functional form of the derivative of the mean residual life, we propose a new lifetime distribution with an upside-down bathtub-shaped mean residual life function. The model has its mean residual life function in a simple, closed form so that further analysis based on the mean residual life can be easily carried out. We study the analysis and applications on both the mean residual life function, and the failure rate function of this model. Maximum likelihood method is used for parameter estimation. Numerical examples and comparisons indicate that the new model performs well in modeling lifetime data with bathtub-shaped failure rate functions, and upside-down bathtub-shaped mean residual life function.", "num_citations": "23\n", "authors": ["445"]}
{"title": "Performance-based maintenance of gas turbines for reliable control of degraded power systems\n", "abstract": " Maintenance actions are necessary for ensuring proper operations of control systems under component degradation. However, current condition-based maintenance (CBM) models based on component health indices are not suitable for degraded control systems. Indeed, failures of control systems are only determined by the controller outputs, and the feedback mechanism compensates the control performance loss caused by the component deterioration. Thus, control systems may still operate normally even if the component health indices exceed failure thresholds. This work investigates the CBM model of control systems and employs the reduced control performance as a direct degradation measure for deciding maintenance activities. The reduced control performance depends on the underlying component degradation modelled as a Wiener process and the feedback mechanism. To this aim, the controller\u00a0\u2026", "num_citations": "22\n", "authors": ["445"]}
{"title": "Design and implementation of two CUSUM schemes for simultaneously monitoring the process mean and variance with unknown parameters\n", "abstract": " Designing joint monitoring schemes for the mean and variance of a Gaussian process (normal distribution) using a single combined statistic instead of the traditional approach of using two separate statistics has gained many attention in recent years. Most of the existing one\u2010chart schemes, however, assume that the true process parameters (standards) are known which is usually not practical and will lead to problems because of improper choices of statistic and control limits. In this paper, we propose two CUSUM control schemes that instinctively work well for the joint monitoring in the case of the unknown parameters by correcting the influence of the reference sample on the plotting statistic. We provide the control limits of the proposed control charts for practical implementation and also offer follow\u2010up procedures for post\u2010signal detection of the nature of shifts. We carry out a comprehensive simulation study to\u00a0\u2026", "num_citations": "22\n", "authors": ["445"]}
{"title": "Monitoring of time\u2010between\u2010events with a generalized group runs control chart\n", "abstract": " Control charting methods for time between events (TBE) is important in both manufacturing and nonmanufacturing fields. With the aim to enhance the speed for detecting shifts in the mean TBE, this paper proposes a generalized group runs TBE chart to monitor the mean TBE of a homogenous Poisson failure process. The proposed chart combines a TBE subchart and a generalized group conforming run length subchart. The zero\u2010state and steady\u2010state performances of the proposed chart were evaluated by applying a Markov chain method. Overall, it is found that the proposed chart outperforms the existing TBE charts, such as the T, Tr, EWMA\u2010T, Synth\u2010Tr, and GR\u2010Tr charts. Copyright \u00a9 2015 John Wiley & Sons, Ltd.", "num_citations": "22\n", "authors": ["445"]}
{"title": "Effects of wind speed probabilistic and possibilistic uncertainties on generation system adequacy\n", "abstract": " A random fuzzy model is proposed to express the probabilistic and possibilistic uncertainties of wind speed simultaneously. In this model, wind speed is represented by a random variable following Weibull distribution, indicating the probabilistic uncertainty. The Weibull distribution parameters of wind speed are fuzzy numbers, meaning the possibilistic uncertainty of wind speed. For estimating distribution parameters, a multi-objective optimisation problem is developed based on cumulative probability and probability distributions of wind speed. The proposed model is then combined with traditional generation system adequacy (GSA) evaluation method to investigate the effect of wind speed uncertainties on GSA. To overcome the difficulty in calculating fuzzy GSA indices, sparse grid is utilised to select collocation points and single-index regression is employed to fit the relationship between adequacy indices and\u00a0\u2026", "num_citations": "22\n", "authors": ["445"]}
{"title": "On weighted least squares estimation for the parameters of Weibull distribution\n", "abstract": " The two-parameter Weibull distribution is one of the most widely used life distributions in reliability studies. It has shown to be satisfactory in modeling the phenomena of fatigue and life of many devices such as ball bearings, electric bulbs, capacitors, transistors, motors and automotive radiators. In recent years, a number of modifications of the traditional Weibull distribution have been proposed and applied to model complex failure data sets.", "num_citations": "22\n", "authors": ["445"]}
{"title": "Classifying weak, and strong components using ROC analysis with application to burn-in\n", "abstract": " Any population of components produced might be composed of two sub-populations: weak components are less reliable, and deteriorate faster whereas strong components are more reliable, and deteriorate slower. When selecting an approach to classifying the two sub-populations, one could build a criterion aiming to minimize the expected mis-classification cost due to mis-classifying weak (strong) components as strong (weak). However, in practice, the unit mis-classification cost, such as the cost of mis-classifying a strong component as weak, cannot be estimated precisely. Minimizing the expected mis-classification cost becomes more difficult. This problem is considered in this paper by using ROC (Receiver Operating Characteristic) analysis, which is widely used in the medical decision making community to evaluate the performance of diagnostic tests, and in machine learning to select among categorical\u00a0\u2026", "num_citations": "22\n", "authors": ["445"]}
{"title": "Reliability growth plot\u2014An underutilized tool in reliability analysis\n", "abstract": " System reliability and performance are improved by continuous improvement effort. The study of the increase in reliability as a function of time is the subject of reliability growth. Although the most well-known reliability growth model, the Duane model, is proposed more than thirty years ago, reliability growth analysis has attracted an increasing interest only recently because of the lack of time for testing and the high reliability of improved products leading to very few failures. In this paper we study a practical approach in reliability growth analysis. Based on the graphical plotting of failure data for some selected models, reliability can easily be estimated and predicted. This approach which is the original idea of the Duane model, overcomes the problem of parameter estimation and model validation that is usually complicated. It is especially useful when the model validation has to be done in order to select a suitable\u00a0\u2026", "num_citations": "22\n", "authors": ["445"]}
{"title": "The effectiveness of adding standby redundancy at system and component levels\n", "abstract": " The effect of adding standby redundancy at the system and component levels is studied. Compared with parallel redundancy, standby redundancy is both easier to implement and more essential in the study of maintenance policies. However, standby redundancy at the component level is not always better than that at the system level, whereas it is always better for parallel redundancy. It is shown that for a series (parallel) system, standby redundancy is more effective at the component (system) level. A simple proof is given.< >", "num_citations": "22\n", "authors": ["445"]}
{"title": "Modelling the evolution of ballasted railway track geometry by a two-level piecewise model\n", "abstract": " Accurate prediction and efficient simulation of the evolution of track geometry condition is a prerequisite for planning effective railway track maintenance. In this regard, the degradation and tamping effect should be equipped with proper and efficient probabilistic models. The possible correlation induced by the spatial structure also needs to be taken into account when modelling the track geometry degradation. To address these issues, a two-level piecewise linear model is proposed to model the degradation path. At the first level, the degradation characteristic of each track section is modelled by a piecewise linear model with known break points at the tamping times. At the second level, Autoregressive Moving Average models are used to capture the spatial dependences between the parameters of the regression lines indexed by their locations. To illustrate the model, a comprehensive case study is presented using\u00a0\u2026", "num_citations": "21\n", "authors": ["445"]}
{"title": "Reliability of warm-standby systems subject to imperfect fault coverage\n", "abstract": " This article models and analyzes the reliability of warm-standby systems subject to imperfect fault coverage based on sequential multistate decision diagrams. For warm-standby systems, the standby units have different failure rates before and after they are used to replace the online faulty unit. Furthermore, a component fault may propagate through the system and cause the entire system to fail if the fault is uncovered or undetected due to the imperfect system recovery mechanism. Existing works on warm-standby systems with imperfect fault coverage are restricted to some special cases, such as cases assuming an exponential time-to-failure distribution for all the system components or cases considering only one warm spare unit. The proposed sequential multistate decision diagram\u2013based approach can overcome the limitations of the existing approaches. Examples are given to illustrate its advantages and\u00a0\u2026", "num_citations": "21\n", "authors": ["445"]}
{"title": "Statistical inference for the extreme value distribution under adaptive Type-II progressive censoring schemes\n", "abstract": " Adaptive Type-II progressive censoring schemes have been shown to be useful in striking a balance between statistical estimation efficiency and the time spent on a life-testing experiment. In this article, some general statistical properties of an adaptive Type-II progressive censoring scheme are first investigated. A bias correction procedure is proposed to reduce the bias of the maximum likelihood estimators (MLEs). We then focus on the extreme value distributed lifetimes and derive the Fisher information matrix for the MLEs based on these properties. Four different approaches are proposed to construct confidence intervals for the parameters of the extreme value distribution. Performance of these methods is compared through an extensive Monte Carlo simulation.", "num_citations": "21\n", "authors": ["445"]}
{"title": "Performance distribution of a fault-tolerant system in the presence of failure correlation\n", "abstract": " This paper considers software systems consisting of fault-tolerant components built from functionally equivalent but independently developed modules (versions) characterized by different reliability and execution times. The components are designed using either the N-version programming method or the recovery block scheme. In our general model, we also allow the number of versions that can run simultaneously to be limited because of hardware or computation time constraints. An analytical algorithm and a numerical procedure to evaluate system execution-time distributions are presented. This algorithm takes into account the positive correlation among failures in different versions by introducing common-cause failures (mutually exclusive and independent common causes are considered). Illustrative examples are presented.", "num_citations": "21\n", "authors": ["445"]}
{"title": "On control charts based on the generalized Poisson model\n", "abstract": " The Poisson distribution is widely used to fit count data such as the number of nonconformities in a product unit. However, this distribution fails to fit the defect data in a high-quality manufacturing environment when over-dispersion occurs. In this paper, the generalized Poisson distribution is studied as an alternative distribution. The generalized Poisson distribution is flexible and capable of dealing with over-dispersed data. In particular, the interpretation of parameters is discussed and statistical monitoring procedures for count data that can be modeled by the generalized Poisson distribution are studied. Based on the generalized Poisson distribution, two different procedures are discussed for an effective process monitoring. Sensitivity analyses of the two monitoring procedures are also presented. To validate the use of the generalized Poisson distribution, three statistical tests for testing the Poisson distribution\u00a0\u2026", "num_citations": "21\n", "authors": ["445"]}
{"title": "Multi-state system reliability\n", "abstract": " 208 Multi-state System Reliability levels of efficiency which can be referred to as performance levels. A system that can have a finite number of performance levels is referred to as a multi-state system, eg Brunelle & Kapur (1999), Pourret et al.(1999), Lisnianski & Levitin (2003) and Wu & Chan (2003).", "num_citations": "21\n", "authors": ["445"]}
{"title": "SPC in an automated manufacturing environment\n", "abstract": " Implementation of statistical process control (SPC) in an automated environment requires a number of issues to be addressed. Changes in sample data distribution and statistical properties such as independence will affect the use and interpretation of traditional SPC procedures; changed monitoring and adjustment techniques will influence the subsequent decision-making; at the same time, automation could facilitate implementation of SPC with other control techniques. In this paper, the potential of combining SPC with engineering process control methods is discussed. Comparative analysis between traditional SPC methods and SPC combined with feedback control is presented. Actual and simulated data are used to illustrate the procedure which, because of its self-tuning ability, could greatly reduce system readjustment while most of the advantages of traditional SPC are preserved at the same time.", "num_citations": "21\n", "authors": ["445"]}
{"title": "On a general measure of component importance\n", "abstract": " It is useful in many situations to have some idea of the relative importance of the components in a system. In this paper a general measure of the importance of system components is studied. The measure is obtained by ordering the reduction of the expected yield due to the variation of the life-time distribution of the components. Some results useful in comparison of the importance of components are obtained and some bounds are also derived. Some interesting properties of the importance of modules are also studied.", "num_citations": "21\n", "authors": ["445"]}
{"title": "A shock model for software failures\n", "abstract": " In this paper we present a general shock model for software failures. We give the usually used Jelinski and Moranda model a new interpretation. Some other specific models which may be more realistic are also derived.", "num_citations": "21\n", "authors": ["445"]}
{"title": "Forecasting of mobile subscriptions in Asia pacific using bass diffusion model\n", "abstract": " In today's dynamic world, mobile communication has changed the lifestyle of many people. Forecasting of mobile subscriptions in a country is one of the research areas. In this paper, Bass diffusion model is used to forecast the number of mobile service subscribers in major countries in Asia Pacific. Technically, we compare two forecasting methods: Bass diffusion model and diffusion by analogy. One is popular in research and the other is commonly applied in practice. Two estimation methods for Bass diffusion model are also compared: adaptive nonlinear least square (adaptive NLS) and genetic algorithm (GA). The results show that Bass diffusion model in general performs better than diffusion by analogy based on the in-sample sum of squared errors (SSE) and out-of-sample SSE. On the other hand, adaptive NLS and genetic algorithms are comparable in generating reasonably sound forecasting results", "num_citations": "20\n", "authors": ["445"]}
{"title": "Process monitoring of exponentially distributed characteristics through an optimal normalizing transformation\n", "abstract": " Many process characteristics follow an exponential distribution, and control charts based on such a distribution have attracted a lot of attention. However, traditional control limits may be not appropriate because of the lack of symmetry. In this paper, process monitoring through a normalizing power transformation is studied. The traditional individual measurement control charts can be used based on the transformed data. The properties of this control chart are investigated. A comparison with the chart when using probability limits is also carried out for cases of known and estimated parameters. Without losing much accuracy, even compared with the exact probability limits, the power transformation approach can easily be used to produce charts that can be interpreted when the normality assumption is valid.", "num_citations": "20\n", "authors": ["445"]}
{"title": "Modeling and analysis of the reliability of digital networked control systems considering networked degradations\n", "abstract": " Digital networked control systems are of growing importance in safety-critical systems and perform indispensable function in most complex systems today. Networked degradations such as transmission delay and packet dropout cause such systems to fail to satisfy performance requirements, and eventually affect the overall reliability. It is necessary to get a model to verify and evaluate the system reliability in early design phase, prior to its implementation. However, existing probabilistic models only provide partial descriptions of such coupled networks and control system. In this paper, a new stochastic model represented by linear discrete-time approach is proposed, considering data packet transmissions in both channels: controller-to-actuator and sensor-to-controller. Different from pervious works, the historical behaviors of networked degradations are modeled by multistate Markov chains with uncertainties\u00a0\u2026", "num_citations": "19\n", "authors": ["445"]}
{"title": "ARL-unbiased control charts for the monitoring of exponentially distributed characteristics based on type-II censored samples\n", "abstract": " In this paper, the problem of monitoring process data that can be modelled by exponential distribution is considered when observations are from type-II censoring. Such data are common in many practical inspection environment. An average run length unbiased (ARL-unbiased) control scheme is developed when the in-control scale parameter is known. The performance of the proposed control charts are investigated in terms of the ARL and standard deviation of the run length. The effects of parameter estimation on the proposed control charts are also evaluated. Then, we consider the design of the ARL-unbiased control charts when the in-control scale parameter is estimated. Finally, an example is used to illustrate the implementation of the proposed control charts.", "num_citations": "19\n", "authors": ["445"]}
{"title": "An optimal control procedure based on multivariate synthetic cumulative sum\n", "abstract": " A multivariate synthetic cumulative sum (MSCUSUM) control chart is presented in this study. The MSCUSUM control chart consists of a multivariate cumulative sum (MCUSUM) control chart and a conforming run length (CRL) control chart, and it combines the strengths of both cumulative sum and CRL procedures. Some properties of this new scheme are investigated, and implementation procedure is shown with real data example. Simulation studies are also carried out to show that the MSCUSUM control chart is more efficient than the MCUSUM, synthetic T2 and Hotelling's T2 control charts, for detecting shifts in the process mean vector. The MSCUSUM chart also performs slightly better than the multivariate synthetic exponentially weighted moving average chart in detecting moderate and large shifts. Copyright \u00a9 2013 John Wiley & Sons, Ltd.", "num_citations": "19\n", "authors": ["445"]}
{"title": "Reliability estimation of maritime transportation: A study of two fuzzy reliability models\n", "abstract": " The progressive increase in marine vessel transportation, in recent years, is often a cause of congestion at sea and main cause of highly irregular vessel\u2019s travel time. This greatly affects scheduling of the facilities at the harbor and also related logistics. As a result, the reliability value of any marine vessel becomes a crucial factor in associated decision making. Modeling the uncertainty in estimation of marine vessel reliability has been a research interest for quite some time now. This paper investigates the problem in a different sense and tries to model the uncertainties using expert\u2019s opinions and their imprecise responses. Marine vessel transportation reliability is viewed in an entirely different perspective and framework. This paper initially proposed a transportation reliability estimation procedure considering 12 decision variables divided into three stages. Two realistic models based on fuzzy sets are subsequently\u00a0\u2026", "num_citations": "19\n", "authors": ["445"]}
{"title": "Integrated control chart system for time-between-events monitoring in a multistage manufacturing system\n", "abstract": " This article develops a control chart system consisting of several individual time-between-events (TBE) charts, each of which is used to monitor the time between successive events at different process stages in the manufacturing of a product in a multistage manufacturing system. The design algorithm considers all the TBE charts within a system in an integrative and optimal manner. Numerical studies show that the proposed design algorithm improves the performance characteristics of the chart system significantly and thus the product quality is further guaranteed. The proposed control chart system is easy to understand and operate; thus, the floor operators can utilize and understand it as easily as for the traditional system.", "num_citations": "19\n", "authors": ["445"]}
{"title": "Relative ageing for two parallel systems and related problems\n", "abstract": " Kalashnikov and Rachev [1] have proposed a partial ordering of two life distributions which is equivalent to an increasing hazard (failure rate) ratio, when the ratio exists. The phenomenon of crossing hazards has received considerable attention in recent years. Recently, Sengupta and Deshpande [2) have studied this and two other models of relative ageing. In this paper, we consider the relative ageing properties of two parallel systems with identical but different number of components. We also compare the variances of the two life distributions having the same mean but with increasing hazard ratio. Several examples are given to illustrate the results.", "num_citations": "19\n", "authors": ["445"]}
{"title": "Computer-aided statistical monitoring of automated manufacturing processes\n", "abstract": " Due to the wide use of computer integrated manufacturing system in modern industries, the traditional control charts that are based on sample of fixed number of items are not appropriate. This paper presents some alternative control techniques for automated manufacturing processes, which are based on the plotting of cumulative count of conforming items. The charting and decision making procedures are discussed in the paper. This type of process monitoring techniques can be readily implemented with the help of on-line computers and modern inspection equipment commonly available in automated manufacturing environment.", "num_citations": "19\n", "authors": ["445"]}
{"title": "On the increase of the expected lifetime by parallel redundancy\n", "abstract": " The effectiveness of adding parallel redundancy to a single component in a complex system is studied. It is shown that the effectiveness is strongly dependent on the class of life distributions assumed. The convex ordering approach is used to compare the failure rates of 2 life distributions. It is proved that adding parallel redundancy on an IFR component is less effective than that for a DFR component. It is also shown that for any life distribution, the improvement in expected lifetime also decreases as the number of redundant components increases. The problem of adding parallel redundancy to meet a requirement on mean time to failure is also discussed.", "num_citations": "19\n", "authors": ["445"]}
{"title": "Some total time on test quantities useful for testing constant against bathtub-shaped failure rate distributions\n", "abstract": " The total time on test (TTT) concept and its generalizations are very useful in many areas of reliability engineering. The TTT plot has been widely used for model identification and tests against ageing. In this paper some interesting quantities based on the TTT plot, which can be used for testing exponentiality against BFR (Bathtub-shaped Failure Rate) alternatives are studied. Distributions of these quantities are derived. Some asymptotic results and numerical values are also presented.", "num_citations": "19\n", "authors": ["445"]}
{"title": "Remaining useful life prediction and predictive maintenance strategies for multi-state manufacturing systems considering functional dependence\n", "abstract": " The performance states of the manufacturing equipment and the quality states of the manufactured products are important indicators for the operational state evaluation and maintenance decision of the multi-state system. Further, the performance degradation of manufacturing components shows some dependence on the decline in product quality. However, the traditional remaining useful life (RUL) prediction and maintenance strategy of manufacturing system are limited to the dependence of the manufacturing components performance degradation. Based on the RUL prediction model that considers the components dependence for product quality requirements, a system predictive maintenance method based on the component functional importance is proposed. First, the connotation of degradation mechanism, functional dependence and RUL for manufacturing system is expounded. Second, a mission reliability\u00a0\u2026", "num_citations": "18\n", "authors": ["445"]}
{"title": "Optimal design of a distribution-free quality control scheme for cost-efficient monitoring of unknown location\n", "abstract": " Traditionally, a cost-efficient control chart for monitoring product quality characteristic is designed using prior knowledge regarding the process distribution. In practice, however, the functional form of the underlying process distribution is rarely known a priori. Therefore, the nonparametric (distribution-free) charts have gained more attention in the recent years. These nonparametric schemes are statistically designed either with a fixed in-control average run length or a fixed false alarm rate. Robust and cost-efficient designs of nonparametric control charts especially when the true process location parameter is unknown are not adequately addressed in literature. For this purpose, we develop an economically designed nonparametric control chart for monitoring unknown location parameter. This work is based on the Wilcoxon rank sum (hereafter WRS) statistic. Some exact and approximate procedures for evaluation of\u00a0\u2026", "num_citations": "18\n", "authors": ["445"]}
{"title": "Robust algorithms for economic designing of a nonparametric control chart for abrupt shift in location\n", "abstract": " The existing statistical process control procedures typically rely on the fundamental assumption of a parametric distribution of the quality characteristic. However, when there is a lack of knowledge about the underlying distribution (as full knowledge is not available in practice), the performance of these parametric charts is very likely to be heavily degraded. Motivated by this problem, a one-sided nonparametric monitoring procedure using the single sample sign statistic is proposed for detecting a shift in the location parameter of a continuous distribution. An economic model of the control chart is developed to optimize the sample size, sampling interval, and control limits. Three data-dependent estimation approaches for the unknown parameter are evaluated and discussed. Simulation results exhibit that our proposed procedure generally performs well under a great variety of continuous distributions and hence it is\u00a0\u2026", "num_citations": "18\n", "authors": ["445"]}
{"title": "On MLEs of the parameters of a modified Weibull distribution for progressively type-2 censored samples\n", "abstract": " Lifetimes of modern mechanic or electronic units usually exhibit bathtub-shaped failure rates. An appropriate probability distribution to model such data is the modified Weibull distribution proposed by Lai et al. [15]. This distribution has both the two-parameter Weibull and type-1 extreme value distribution as special cases. It is able to model lifetime data with monotonic and bathtub-shaped failure rates, and thus attracts some interest among researchers because of this property. In this paper, the procedure of obtaining the maximum likelihood estimates (MLEs) of the parameters for progressively type-2 censored and complete samples are studied. Existence and uniqueness of the MLEs are proved.", "num_citations": "18\n", "authors": ["445"]}
{"title": "Nonmonotonic failure rates and mean residual life functions\n", "abstract": " This paper deals with the identification of the shape of the failure rate and the mean residual life function. In this regards we review Glaser's method and present some examples where the expressions for the failure rates are complicated and Glaser's methods can be applied. Glaser's methods are then extended to accomodate more than one turning point and in particular to the case of roller coaster failure rates. The shape of the mean residual life function, in comparison to the failure rate, is then studied for one or more than one turning points. Several illustrative examples are provided.", "num_citations": "18\n", "authors": ["445"]}
{"title": "A study of economic design of control charts for cumulative count of conforming items\n", "abstract": " In this paper the economic design of Cumulative Count of Conforming (CCC) control charts to maintain the current control of fraction nonconforming of a process is studied. CCC chart is an attribute chart for monitoring high quality processes by plotting the cumulative count of conforming items between two nonconforming ones on a suitable chart. A process model is proposed to obtain an appropriate loss function. An alogorithm to search for the optimal setting of the sampling and control parameters is derived. Numerical illustrations of the method and some properties of the optimal economic design are provided.", "num_citations": "18\n", "authors": ["445"]}
{"title": "A positive management orientation for continuous improvement\n", "abstract": " A \"Do It Better Each Time\" strategy has been promoted in quality management literature, in preference to the \"Do It Right First Time\" approach to managing quality. This paper discusses these concepts from the viewpoints of the traditional quality management \"gurus\": Philip B. Crosby, W. Edwards Deming, and Joseph M. Juran, and examines how they relate to current quality management philosophies.", "num_citations": "18\n", "authors": ["445"]}
{"title": "New approach to quality in a near-zero defect environment\n", "abstract": " In today's manufacturing environment, product quality and process productivity have reached a level at which a new quality management philosophy would be useful to maintain an organization's competitive edge. New techniques are also needed as conventional statistical process management procedures would encounter difficulties in their application at this level. In this paper, a \u2018do it better each time\u2019 strategy is advanced to replace the \u2018do it right the first time\u2019 concept. Adaptation of statistical tools for quality monitoring and control in the light of the new philosophy in a near-zero defect environment is also discussed.", "num_citations": "18\n", "authors": ["445"]}
{"title": "Small\u2010batch\u2010size convolutional neural network based fault diagnosis system for nuclear energy production safety with big\u2010data environment\n", "abstract": " In nuclear energy production, with the continuous innovations and challenges in the big data and the industry 4.0 era, to guarantee the operation safety without the fault and failure will become more complex and intelligent. In this paper, a novel optimized convolutional neural network with small\u2010batch\u2010size processing (SCNN) was proposed and assembled in the nuclear fault diagnosis system. Eleven kinds of normal and fault conditions that include the whole 316 simulator sensor features were used to evaluate the performance of the proposed diagnosis system. The application of batch normalization with SCNN significantly optimized the model validation accuracy and loss under 100 epochs compared with normal operation and adding drop\u2010out operation in same condition. Besides, outstanding diagnosis accuracy was highlighted by the comparison of traditional binary and multiple classification methods. This\u00a0\u2026", "num_citations": "17\n", "authors": ["445"]}
{"title": "A unified confidence interval for reliability-related quantities of two-parameter Weibull distribution\n", "abstract": " Statistical inference methods for the Weibull parameters and their functions usually depend on extensive tables, and hence are rather inconvenient for the practical applications. In this paper, we propose a general method for constructing confidence intervals for the Weibull parameters and their functions, which eliminates the need for the extensive tables. The method is applied to obtain confidence intervals for the scale parameter, the mean-time-to-failure, the percentile function, and the reliability function. Monte-Carlo simulation shows that these intervals possess excellent finite sample properties, having coverage probabilities very close to their nominal levels, irrespective of the sample size and the degree of censorship.", "num_citations": "17\n", "authors": ["445"]}
{"title": "Reply to\" On some recent modifications of Weibull distribution\"\n", "abstract": " DISTRIBUTION\u201d for bringing to our attention the paper by Gurvich, et al.[3], of which we were unaware. We came up with the model independently some time during 1997 when Lai, and Murthy were visiting Singapore. Detailed analysis was carried out, and a paper was submitted to IEEE Transactions on Reliability in 1999, which appeared in print in 2003. The motivation for our research was to look at new distributions (derived from the Weibull distribution) with bathtubshaped failure rates. It followed from our joint work on this topic (see, Lai, et al.[1], and Xie, et al.[2]). This is in contrast to Gurvich, et al.[3] where the motivation was in the context of modeling the strength of material.", "num_citations": "17\n", "authors": ["445"]}
{"title": "On modelling reliability growth for software\n", "abstract": " An essential parameter of many important systems in engineering sciences is the reliability of software. In social sciences there is also an increased use of software and the unavailability of it usually causes many practical problems. There are several models suggested in studying software reliability. However, one of the widely used assumptions is that all faults contribute the same amount to the failure probability. In this paper we present some new models in which this assumption is not used. A software fault detection process is modelled by a Markov process with jump intensity which decreases as the number of faults detected increases. The intensity function may for example be a power type function of the number of the remaining faults. A sampling argument suggests that a second order power function may be a good approximation of the reality. We also present some numerical example on estimation model\u00a0\u2026", "num_citations": "17\n", "authors": ["445"]}
{"title": "On optimal upgrade strategy for second-hand multi-component systems sold with warranty\n", "abstract": " Reliability improvement strategies such as upgrade, reconditioning and remanufacturing have been extensively adopted by dealers of second-hand systems to improve the system reliability and reduce the warranty servicing cost. However, most existing studies on this topic do not consider the multi-component structures of complex second-hand systems, and either treat them as black-box systems by ignoring their internal structures or simply deal with individual components. In this paper, a new upgrade model is developed for complex second-hand systems sold with a non-renewing free repair/replacement warranty, by explicitly considering their multi-component configurations. Two types of components, i.e. repairable and non-repairable components, are taken into account. During the upgrade process, non-repairable components can be upgraded only by replacement (if necessary), while repairable ones may be\u00a0\u2026", "num_citations": "16\n", "authors": ["445"]}
{"title": "A study of control chart for monitoring exponentially distributed characteristics based on type\u2010II censored samples\n", "abstract": " In this paper, control charts for monitoring the exponential type\u2010II censoring samples are investigated. Such data are very common in many practical inspection scenarios in reliability context when items are replaced in groups after a period of time. The average time to signal, which involves both the number and the time of samples inspected until a signal occurs, is a good criterion to evaluate the performance of control charts. We propose an average time to signal\u2010unbiased control chart with known parameter and compare the proposed method with the traditional ones. The results indicate the proposed control chart is more sensitive to system deterioration. Then the effects of parameter estimation on the proposed control charts are evaluated. Because the control limits with estimated parameters result in more false alarms, an adjusted control chart with estimated parameters is proposed and the self\u2010starting control\u00a0\u2026", "num_citations": "16\n", "authors": ["445"]}
{"title": "A stochastic em algorithm for progressively censored data analysis\n", "abstract": " Progressive censoring technique is useful in lifetime data analysis. Simple approaches to progressive data analysis are crucial for its widespread adoption by reliability engineers. This study develops an efficient yet easy\u2010to\u2010implement framework for analyzing progressively censored data by making use of the stochastic EM algorithm. On the basis of this framework, we develop specific stochastic EM procedures for several popular lifetime models. These procedures are shown to be very simple. We then demonstrate the applicability and efficiency of the stochastic EM algorithm by a fatigue life data set with proper modification and by a progressively censored data set from a life test on hard disk drives. Copyright \u00a9 2013 John Wiley & Sons, Ltd.", "num_citations": "16\n", "authors": ["445"]}
{"title": "Analyzing highly censored reliability data without exact failure times: an efficient tool for practitioners\n", "abstract": " There are many real-life situations where exact failure times are not available or not easily available for reliability analysis. Motivated by this problem, this work is concerned with the analysis of reliability data without exact failure times. The unavailability of exact failure times has compelled us to develop a simplified version of the maximum likelihood (ML) estimation for reliability parameters and measures. The approach is applicable to single-parameter reliability models including the exponential reliability model and the Weibull reliability model with an assumed or known shape parameter value. In the latter case, it takes advantage of the fact that in many practical situations a reasonable estimate of the Weibull shape parameter is attainable by certain means. This is particularly valuable for reliability assessment of highly censored Weibull data with only a few failures, because in such situations it is highly\u00a0\u2026", "num_citations": "16\n", "authors": ["445"]}
{"title": "Process monitoring with univariate and multivariate c-charts\n", "abstract": " Count events are frequently modeled by Poisson distributions and monitored by the Shewhart c chart. It is shown that the in-control average run length of the Shewhart c chart is deviated greatly from the nominal value. An optimal symmetric c chart is proposed to monitor Poisson data with the closest in-control average run length value to nominal. The symmetric c chart is shown to be competitive to other control charts proposed and can be easily extended to monitor multivariate Poisson counts with positive correlations. A real example is provided to illustrate the effectiveness of the optimal symmetric c chart.", "num_citations": "16\n", "authors": ["445"]}
{"title": "A discrete software reliability growth model with testing effort\n", "abstract": " We propose a discrete software reliability growth model with testing effort. The behaviour of the testing effort is described by a discrete Rayleigh curve. Assuming that the discrete failure intensity to the amount of current testing effort is proportional to the remaining error content, we formulate the model as a non-homogeneous poisson process. Parameters of the model are estimated. We then discuss a release policy based on cost and failure intensity criteria. Numerical results are also presented.", "num_citations": "16\n", "authors": ["445"]}
{"title": "The increase of reliability of k-out-of-n systems through improving a component\n", "abstract": " The effect of improving a component on the increase of system reliability is studied. The order of the increases due to improvement on different components is a natural measure of component importance. For different improvement actions, the rankings of importance may differ considerably. First some general results are given for the k-out-of-n system. Then the effect of parallel redundancy is studied in some detail. An example is given in the Appendix which illustrates the ideas.", "num_citations": "16\n", "authors": ["445"]}
{"title": "Dynamic modeling of general three-state k-out-of-n: G systems: Permanent-based computational results\n", "abstract": " This paper is concerned with dynamic reliability analysis of three-state k-out-of-n: G systems. It is assumed that the components and the systems can be in three states: perfect functioning, partial performance and complete failure. Using the concept of permanent, we study marginal and joint survival functions for the lifetime of two different three-state k-out-of-n: G systems that consist of independent and nonidentical components. Illustrative examples are also provided for the components which follow the Markov degradation process.", "num_citations": "15\n", "authors": ["445"]}
{"title": "A study of N-version programming and its impact on software availability\n", "abstract": " N-version programming is a useful approach to improve the quality of software, especially for safety-critical systems. Positive performance in enhancing software availability is an expected result. In this paper, a software availability model for the study of the impact of N-version programming technique is proposed and investigated. The characteristics of the N-version software system and its operation and failure process are analysed. Based on this analysis, the time-dependent behaviour of the software system, which alternates between online and offline states, is described using a Markov chain. This model derives quantitative measures of software availability. Numerical examples and comparisons are also presented in this paper to directly illustrate N-version programming's positive impact on software availability measures. N-version programming generally provides a positive impact on the system. However, it\u00a0\u2026", "num_citations": "15\n", "authors": ["445"]}
{"title": "System and Bayesian Reliability: Essays in Honor of Professor Richard E. Barlow on His 70th Birthday\n", "abstract": " This volume is a collection of articles on reliability systems and Bayesian reliability analysis. Written by reputable researchers, the articles are self-contained and are linked with literature reviews and new research ideas. The book is dedicated to Emeritus Professor Richard E Barlow, who is well known for his pioneering research on reliability theory and Bayesian reliability analysis.", "num_citations": "15\n", "authors": ["445"]}
{"title": "Control charts for processes subject to random shocks\n", "abstract": " In this paper, a charting technique for controlling processes subject to random shocks is presented. This type of process is common in a high\u2010yield production environment and the conventional Shewhart control charts are not efficient for its monitoring and control. This technique is able to detect process improvement, easy for decision making, and more concise and informative than other methods used for this type of process. In addition, it could provide diagnostic information which is highly useful in practice.", "num_citations": "15\n", "authors": ["445"]}
{"title": "Imperfect preventive maintenance policies with unpunctual execution\n", "abstract": " Traditional maintenance planning problems usually presume that preventive maintenance (PM) policies will be executed exactly as planned. In reality, however, maintainers often deviate from the intended PM policy, resulting in unpunctual PM executions that may reduce maintenance effectiveness. This article studies two imperfect PM policies with unpunctual executions for infinite and finite planning horizons, respectively. Under the former policy, imperfect PM actions are periodically performed and the system is preventively replaced at the last PM instant. The objective is to determine the optimal number of PM actions and associated PM interval so as to minimize the long-run average cost rate. However, the latter policy specifies that a system is subject to periodic PM activities within a finite planning horizon and there is no PM activity at the end of the horizon. The aim is then to identify the optimal number of PM\u00a0\u2026", "num_citations": "14\n", "authors": ["445"]}
{"title": "A comparative study of some EWMA schemes for simultaneous monitoring of mean and variance of a Gaussian process\n", "abstract": " In this paper, we introduce four different combinations of EWMA schemes, each based on a single plotting statistic for simultaneous monitoring of the mean and variance of a Gaussian process. We compare the four schemes and address the problem of adopting the best combining mechanism. We consider that the actual process parameters are unknown and estimated from a reference sample. We take into account the effects of estimation of unknown parameters in designing the proposed schemes. We consider the maximum likelihood estimators based pivot statistics for monitoring both the parameters and combine them into a single statistic through the \u2018max\u2019 and the \u2018distance\u2019 type combining functions. Also, we examine two different adaptive approaches to introduce pivot statistics into the EWMA-structure. Results show that the distance-type schemes outperform the max-type schemes. Generally, the proposed\u00a0\u2026", "num_citations": "14\n", "authors": ["445"]}
{"title": "Performance and reliability improvement of cyber-physical systems subject to degraded communication networks through robust optimization\n", "abstract": " The study of cyber-physical systems (CPSs) is a rapidly developing field because of their broad application in different areas. However, the degradation, i.e., time delay and packet dropout, that originated from the cyber layer-communication networks would deteriorate the performance of the physical layer. Hence, the internal dependence and reliability of such a complex and safety-critical systems must be analyzed. In this paper, we describe a CPS by using time-varying models of its main components and realize it in TrueTime simulator. The Monte Carlo simulation is applied to estimate the system reliability. The parameters of the discrete-time proportional-integral-derivative (PID) controller are tuned to improve the performance and reliability of the CPS. Considering the controller parameters that undergo perturbation when in hardware implementation, we use particle swarm optimization to search for a robust PID\u00a0\u2026", "num_citations": "14\n", "authors": ["445"]}
{"title": "Simultaneously monitoring frequency and magnitude of events based on bivariate gamma distribution\n", "abstract": " The occurrence of an event, for example, some mishaps in manufacturing processes or natural disasters such as floods or earthquakes, is often characterized by its frequency and magnitude. Procedures for simultaneously monitoring the event frequency and the event magnitude usually assume that the frequency and magnitude are two independent variables. However, the dependence between frequency and magnitude is very common in practice. In this paper, a bivariate gamma distribution is considered for modelling the event frequency and the magnitude with certain dependence structure. Based on this bivariate gamma distribution, a multivariate exponentially weighted moving average (MEWMA) procedure is designed for jointly monitoring the shifts in mean values of the frequency and the magnitude of an event. Some comparisons are carried out via Monte-Carlo simulations. The results show that our\u00a0\u2026", "num_citations": "14\n", "authors": ["445"]}
{"title": "An ameliorated improvement factor model for imperfect maintenance and its goodness of fit\n", "abstract": " Maintenance actions can be classified, according to their efficiency, into three categories: perfect maintenance, imperfect maintenance, and minimal maintenance. To date, the literature on imperfect maintenance is voluminous, and many models have been developed to treat imperfect maintenance. Yet, there are two important problems in the community of maintenance that still remain wide open: how to give practical grounds for an imperfect-maintenance model, and how to test the fit of a real dataset to an imperfect-maintenance model. Motivated by these two pending problems, this work develops an imperfect-maintenance model by taking a physically meaningful approach. For the practical implementation of the developed model, we advance two methods, called QMI method and spacing-likelihood algorithm, to estimate involved unknown parameters. The two methods complete each other and are widely\u00a0\u2026", "num_citations": "14\n", "authors": ["445"]}
{"title": "Study of an imputation algorithm for the analysis of interval-censored data\n", "abstract": " In this article, an iterative single-point imputation (SPI) algorithm, called quantile-filling algorithm for the analysis of interval-censored data, is studied. This approach combines the simplicity of the SPI and the iterative thoughts of multiple imputation. The virtual complete data are imputed by conditional quantiles on the intervals. The algorithm convergence is based on the convergence of the moment estimation from the virtual complete data. Simulation studies have been carried out and the results are shown for interval-censored data generated from the Weibull distribution. For the Weibull distribution, complete procedures of the algorithm are shown in closed forms. Furthermore, the algorithm is applicable to the parameter inference with other distributions. From simulation studies, it has been found that the algorithm is feasible and stable. The estimation accuracy is also satisfactory.", "num_citations": "14\n", "authors": ["445"]}
{"title": "On the odd Weibull distribution\n", "abstract": " Based on the idea of evaluating the distribution of the \u2018odds of death\u2019 of a lifetime variable, the odd Weibull distribution proposed by Cooray has recently been shown to be useful for testing the goodness of fit of the Weibull and inverse Weibull distributions. The model is also very versatile in modelling lifetime data in that its failure rate function can be increasing, decreasing, constant, bathtub shaped, and unimodal. In this paper, a detailed parametric characterization of the statistical properties of this distribution is carried out. The shapes of the Weibull probability plot with different model parameters are presented and the graphic parameter estimation steps are iterated. Burn-in and useful period-related issues of the bathtub-shaped failure rate curve are discussed. An application example is shown to illustrate the parameter estimation procedure and the superior fit of the model for some real data to the two-parameter\u00a0\u2026", "num_citations": "14\n", "authors": ["445"]}
{"title": "A two-dimensional probability model for evaluating reliability of piezoelectric micro-actuators\n", "abstract": " In this paper, a probabilistic approach is presented for the evaluation of the reliability of piezoelectric micro-actuators that takes into account the effects of both driving voltage and temperature. Based on the relationships between the lifetime and degradation mechanism of piezoelectric actuators and the electric field strength, as well as the actuator working temperature, a two-dimensional probability model for evaluating reliability of piezoelectric micro-actuators is described. The concept of two-dimensional strength is proposed to incorporate the electric driving voltage and the working temperature of the piezoelectric actuators at a specified lifetime. The lifetime (number of cycles to failure) of piezoelectric actuator, electric load and temperature are considered as the random variables and their probability distributions are discussed. A two-dimensional strength probability distribution function is derived. A two\u00a0\u2026", "num_citations": "14\n", "authors": ["445"]}
{"title": "Software reliability predictions using artificial neural networks\n", "abstract": " Computer-based artificial systems have been widely applied in nearly every field of human activities. Whenever people rely heavily on some product/technique, they want to make sure that it is reliable. However, computer systems are not as reliable as expected, and software has always been a major cause of the problems. With the increasing reliability of hardware and growing complexity of software, the software reliability is a rising concern for both developer and users. Software reliability engineering (SRE) has attracted a lot of interests and research in the software community and software reliability modeling is one major part of SRE research.", "num_citations": "14\n", "authors": ["445"]}
{"title": "On reliability bounds via conditional inequalities\n", "abstract": " In this paper we study an approximation of system reliability using one-step conditioning. It is shown that, without greatly increasing the computational complexity, the conditional method may be used instead of the usual minimal cut and minimal path bounds to obtain more accurate approximations and bounds. We also study the conditions under which the approximations are bounds on the reliability. Some further extensions are also presented.", "num_citations": "14\n", "authors": ["445"]}
{"title": "Statistical process control for low nonconformity processes\n", "abstract": " Statistical process control of high quality products is an important issue in modern quality control applications because of the success of continuous improvement efforts worldwide. The conventional Shewhart control charts based on 3-sigma control limits tend to encounter certain practical and theoretical problems as \u201czero-defect\u201d is approached. In this paper, we describe some general approaches to solving this problem, focusing on the control charts for nonconformities or defects. We suggest that for a moderate nonconformity process, the exact probability limits should be used. For a lower non-conformity process, a \u201cpattern recognition\u201d approach can be applied. Finally, for a near-zero nonconformity process, a modified approach based on the cumulative count of nonconformities can be used.", "num_citations": "14\n", "authors": ["445"]}
{"title": "A new approach for fault diagnosis with full-scope simulator based on state information imaging in nuclear power plant\n", "abstract": " In this paper, a new approach aimed at the Fault Diagnosis with Full-scope Simulator based on the State Information Imaging (FDFSSII) in NPP is proposed. The FDFSSII approach first constructs a series of gray-image which presents the operating transient (included normal and fault condition) according to the real time monitoring data. Furthermore, the Machine Learning (ML) technology is employed to achieve image feature extraction and classification by analyzing and learning from massive amounts of historical and synthetic gray-image data \u2013 the image feature is extracted by the Kernel Principal Component Analysis (KPCA) and classified by the designed classifiers in different learning methods. Finally, diagnosis effect is evaluated by the F1 score. The simulation result shows that the FDFSSII approach has achieved good effect for the fault diagnosis in NPP. Meanwhile, it simplifies the process of nuclear\u00a0\u2026", "num_citations": "13\n", "authors": ["445"]}
{"title": "Approach to integrate fuzzy fault tree with Bayesian network\n", "abstract": " Fuzzy fault tree (FFT) can offer an efficient method of representing the fault causes and handling fuzzy information in the relationships among events. However, FFT cannot incorporate the evidence into the reasoning as Bayesian Network (BN). To overcome the disadvantage of FFT and BN, an approach of integrating FFT with BN is proposed in this paper. Firstly, the FFT technique of Takagi and Sugeno model that can handle uncertainties in the relationships among different events is introduced. Secondly, the translation rules of converting FFT into BN are presented. The integration algorithm is then demonstrated on an offshore fire case study.", "num_citations": "13\n", "authors": ["445"]}
{"title": "A study of analogy based sampling for interval based cost estimation for software project management\n", "abstract": " Software cost estimation is one of the most challenging activities in software project management. Since the software cost estimation affects almost all activities of software project development such as: biding, planning, and budgeting, the accurate estimation is very crucial to the success of software project management. However, due to the inherent uncertainties in the estimation process and other factors, the accurate estimates are often obtained with great difficulties. Therefore, it is safer to generate interval based estimates with a certain probability over them. In the literature, many approaches have been proposed for interval estimation. In this study, we propose a navel method namely Analogy Based Sampling (ABS) and compare ABS against the well established Bootstrapped Analogy Based Estimation (BABE) which is the only existing variant of analogy based method with the capability to generate interval\u00a0\u2026", "num_citations": "13\n", "authors": ["445"]}
{"title": "Weibull model allowing nearly instantaneous failures\n", "abstract": " A generalized Weibull model that allows instantaneous or early failures is modified      so that the model can be expressed as a mixture of the uniform distribution and the Weibull distribution. Properties  of the resulting distribution are derived; in particular, the probability density function,  survival function, and the hazard rate function are obtained. Some selected plots of these  functions are also presented. An R script was written to fit the model parameters. An   application of the modified model is illustrated.", "num_citations": "13\n", "authors": ["445"]}
{"title": "Some analytical and numerical bounds on the renewal function\n", "abstract": " Renewal-type equations are frequently encountered in the study of reliability, warranty analysis, replacement and maintenance policies, and inventory control. Renewal equations usually do not have analytical solutions, and hence, bounds or approximations are very useful. In this article, analytical bounds are studied based on a simple iterative procedure which provides some analytical results and nice convergence properties when the number of iteration increases. Bounds and approximations are also investigated for a recursive algorithm for numerical computation. In addition, some interesting monotonicity properties are introduced and discussed. The approximation error, which is important for determining the stopping rule of the iterative procedure and the numerical algorithm, is also studied.", "num_citations": "13\n", "authors": ["445"]}
{"title": "ON VARIABLE SAMPLE SIZE  CHART FOR PROCESSES WITH DOUBLE ASSIGNABLE CAUSES\n", "abstract": " In statistical process control applications, variable sample size (VSS)  chart  is often used to detect the assignable cause quickly. However, it  is usually assumed that only one assignable cause results in the  out-of-control in the process. There are papers dealing with double  assignable causes for traditional fixed sample size Shewhart chart.  In this paper, we consider double assignable causes to occur with  compound in the process and adopt the Markov chain approach to investigate the statistical properties of VSS   chart. A procedure that can compute the optimal sample sizes is proposed.", "num_citations": "13\n", "authors": ["445"]}
{"title": "Concepts of stochastic dependence in reliability analysis\n", "abstract": " 7.2. 2 The Relative Stringency of the Conditions 7.2. 3 Positive Quadrant Dependent in Expectation 7.2. 4 Associated Random Variables 7.2. 5 Positively Correlated Distributions 7.2. 6 Summary of Interrelationships 7.3 Positive Quadrant Dependent Concept 7.3. 1 Constructions of Positive Quadrant Dependent Bivariate Distributions 7.3. 2 Applications of Positive Quadrant Dependence Concept to Reliability 7.3. 3 Effect of Positive Dependence on the Mean Lifetime of a Parallel System 7.3. 4 Inequality Without Any Aging Assumption 7.4 Families of Bivariate Distributions that are Positive Quadrant Dependent 7.4. 1 Positive Quadrant Dependent Bivariate Distributions with Simple Structures 7.4. 2 Positive Quadrant Dependent Bivariate Distributions with More Complicated Structures", "num_citations": "13\n", "authors": ["445"]}
{"title": "Testing-resource allocation for redundant software systems\n", "abstract": " For many safety critical systems, redundancy is the only acceptable method to achieve high operational reliability as individual modules can hardly be certified to have reached that level. When limited resources are available in the testing of a redundant software system, it is important to allocate the testing-time efficiently so that the maximum reliability of the complete system is achieved. In this paper, this problem is investigated in detail. A general formulation is presented and a specific case is used to illustrate the procedure. The case where individual module reliability requirements are given is also considered.", "num_citations": "13\n", "authors": ["445"]}
{"title": "Robustness of optimum software release policies\n", "abstract": " In the development of software systems, it is important to determine when the software testing can be stopped and when the system can be released. An optimum release time is usually determined by minimizing the expected total cost under a reliability requirement. Usually the optimum release time depends on the unknown parameters in the underlying reliability growth models and these parameters have to be estimated based on collected testing data. Because of the random nature of the software failure process, there are some problems with the stability of the estimates of model parameters. This makes the conventional procedures for the determination of optimum software release time not robust. Although an extensive literature exists on the problem of software release time determination, few papers address the robustness issue. In this paper, the robustness of optimum release time procedures is studied. The\u00a0\u2026", "num_citations": "13\n", "authors": ["445"]}
{"title": "Condition-based maintenance for long-life assets with exposure to operational and environmental risks\n", "abstract": " This paper presents a new condition-based maintenance (CBM) model for long-life assets to address the potential risk caused by the decline of the operating environment. Two types of maintenance are formulated in the CBM model. Minor maintenance can mitigate the operational and environmental risk, and major maintenance can eliminate the accumulated damage within the asset. A continuous-time semi-Markov chain (CTSMC) is used for modeling the aging of the asset as well as the stochastic decline of the operating environment. To optimize the CBM policy in a mathematically tractable manner, we introduce a hypo-exponential approximation approach to match the first four moments of the sojourn time distribution of CTSMC. This approach guarantees a minimum representation of the CTSMC with non-fictitious surrogated Markov chain. The model provides both good mathematical tractability and sufficient\u00a0\u2026", "num_citations": "12\n", "authors": ["445"]}
{"title": "Reliability assessment of system under a generalized cumulative shock model\n", "abstract": " Reliability assessment of system suffering from random shocks is attracting a great deal of attention in recent years. Excluding internal factors such as aging and wear-out, external shocks which lead to sudden changes in the system operation environment are also important causes of system failure. Therefore, efficiently modeling the reliability of such system is an important applied problem. A variety of shock models are developed to model the inter-arrival time between shocks and magnitude of shocks. In a cumulative shock model, the system fails when the cumulative magnitude of damage caused by shocks exceed a threshold. Nevertheless, in the existing literatures, only the magnitude is taken into consideration, while the source of shocks is usually neglected. Using the same distribution to model the magnitude of shocks from different sources is too critical in real practice. To this end, considering a system\u00a0\u2026", "num_citations": "12\n", "authors": ["445"]}
{"title": "Static output feedback stabilization of networked control systems with a parallel-triggered scheme\n", "abstract": " This paper investigates the parallel-triggered static output feedback stabilization problem for linear networked control systems. A new parallel-triggered scheme is proposed by using both the relative error and the absolute error information. The scheme can reduce transmission rate while maintaining the global asymptotical stability. The linear parallel-triggered networked control system is modeled as a time-delay system. By employing Lyapunov stability theory, sufficient conditions are established for the closed-loop system to be globally asymptotically stable in terms of linear matrix inequalities. Moreover, a co-design algorithm is developed to obtain both the optimal trigger parameters and the output feedback controller gain in the sense that the transmission rate is minimized. Finally, two examples are given to illustrate the advantages of the proposed scheme.", "num_citations": "12\n", "authors": ["445"]}
{"title": "Optimal burn-in policy for highly reliable products using inverse Gaussian degradation process\n", "abstract": " Burn-in test is a manufacturing procedure implemented to identify and eliminate units with infant mortality before they are shipped to the customers. The traditional burn-in test, collecting event data over a short period of time, is rather inefficient. This problem can be solved if there is a suitable quality characteristic (QC) whose degradation over time can be related to the lifetime of the product. Optimal burn-in policies have been discussed in the literature assuming that the underlying degradation path follows a Wiener process or a gamma process. However, the degradation paths of many products may be more appropriately modeled by an inverse Gaussian process which exhibits a monotone increasing pattern. Here, motivated by the numerous merits of the inverse Gaussian process, we first propose a mixed inverse Gaussian process to describe the degradation paths of the products. Next, we present a\u00a0\u2026", "num_citations": "12\n", "authors": ["445"]}
{"title": "Objective Bayes analysis of zero-inflated Poisson distribution with application to healthcare data\n", "abstract": " In this article, non-informative priors are investigated for a zero-inflated Poisson distribution with two parameters: the probability of zeros and the mean of the Poisson part. Both the reference prior and the Jeffreys prior are derived and shown to be second-order matching priors when only the mean of the Poisson part is of interest. However, when the probability of zeros is of interest, the reference prior is still a second-order matching prior, whereas the Jeffreys prior is not so. Furthermore, when both parameters are of interest, the reference prior is a unique second-order matching prior. Frequentist coverage probabilities of the posterior confidence sets based on the Jeffreys and reference priors are compared with each other using Monte Carlo simulations and with confidence sets based on the maximum likelihood estimation.", "num_citations": "12\n", "authors": ["445"]}
{"title": "Multi-objective optimization approaches to software release time determination\n", "abstract": " Optimal release time determination is a typical application of software reliability models. In this decision process, reliability and cost are the two important dimensions that are generally considered. Most existing research formulates this decision process as single-objective optimization problems. Although these formulations can greatly reduce the complexity, they can hardly reflect the nature of the decision process, which is essentially a multi-objective optimization problem. More specifically, maximizing reliability and minimizing cost should be achieved simultaneously. Due to this consideration, in this paper, software release time determination is investigated by various multi-objective optimization approaches, including the trade-off analysis, multi-attribute utility theory, and physical programming approach. Numerical examples are provided for illustrative purpose. The applicability and limitations of different multi\u00a0\u2026", "num_citations": "12\n", "authors": ["445"]}
{"title": "A study of lifetime optimization of transportation system\n", "abstract": " The operation process or environment usually has a significant influence on system lifetime. In this paper, a lifetime optimization approach based on linear programming (LP) is proposed to maximize the transportation system lifetime, in which a semi-Markov (SM) model is used to model the operation process. In the proposed method, we first formulate the optimization problem as an LP model that is used to find the optimal transient probability of each state. Then, an analytical method is performed to obtain the corresponding optimal sojourn-time distribution parameters of the SM process. Finally, the proposed approach is applied to a port oil transportation system to show that it can efficiently ensure that the transportation system has a long lifetime.", "num_citations": "12\n", "authors": ["445"]}
{"title": "Optimal software maintenance policy considering unavailable time\n", "abstract": " With the enhancement of hardware and software engineering, the effectiveness and correctness of software is less and less doubted and customers are more aware about whether software services are available or not when needed. Software maintenance is one of the main reasons that make software unavailable and it is often very expensive to perform maintenance tasks. Common approaches of studying software maintenance are to consider it as a static by\u2010product of software operation and only the maintenance cost is covered. In this paper, software maintenance policies are studied with the consideration of unavailable service time. A non\u2010homogeneous continuous Markov chain is adopted for modeling the software operation and maintenance process, and the cost of software unavailability that is brought in by software maintenance is investigated and analyzed for searching the optimal maintenance policy\u00a0\u2026", "num_citations": "12\n", "authors": ["445"]}
{"title": "Bayesian networks modeling for software inspection effectiveness\n", "abstract": " Software inspection has been broadly accepted as a cost effective approach for defect removal during the whole software development lifecycle. To keep inspection under control, it is essential to measure its effectiveness. As human-oriented activity, inspection effectiveness is due to many uncertain factors that make such study a challenging task. Bayesian networks modeling is a powerful approach for the reasoning under uncertainty and it can describe inspection procedure well. With this framework, some extensions have been explored in this paper. The number of remaining defects in the software is proposed to be incorporated into the framework, with expectation to provide more information on the dynamic changing status of the software. In addition, a different approach is adopted to elicit the prior belief of related probability distributions for the network. Sensitivity analysis is developed with the model to locate\u00a0\u2026", "num_citations": "12\n", "authors": ["445"]}
{"title": "A general Weibull model for reliability analysis under different failure criteria-application on anisotropic conductive adhesive joining technology\n", "abstract": " In this paper, a generic four-parameter model has been developed and applied to the anisotropic conductive adhesive (ACA) flip-chip joining technology for electronics packaging applications. The model can also be used to predict any minimum failure cycles if the maximum acceptable failure criterion (in this case, a preset electrical resistance value) is set. The original reliability testing from which the test data was obtained was carried out on flip-chip anisotropically conductive adhesive joints on an FR-4 substrate. In the study, nine types of ACA and one nonconductive film (NCF) were used. In total, nearly 1000 single joints were subjected to reliability tests in terms of temperature cycling between -40/spl deg/C and 125/spl deg/C with a dwell time of 15 min and a ramp rate of 110/spl deg/C/min. The reliability was characterized by single contact resistance measured using the four-probe method during temperature\u00a0\u2026", "num_citations": "12\n", "authors": ["445"]}
{"title": "Transformation approaches for the construction of Weibull prediction interval\n", "abstract": " Two methods of transforming the Weibull data to near normality, namely the Box\u2013Cox method and Kullback\u2013Leibler (KL) information method, are discussed and contrasted. A simple prediction interval (PI) based on the better KL information method is proposed. The asymptotic property of this interval is established. Its small sample behavior is investigated using Monte Carlo simulation. Simulation results show that this simple interval is close to the existing complicated PI where the percentage points of the reference distribution have to be either simulated or approximated. The proposed interval can also be easily adjusted to have the correct asymptotic coverage.", "num_citations": "12\n", "authors": ["445"]}
{"title": "Weibull-related distributions for the modelling of bathtub shaped failure rate functions\n", "abstract": " Weibull distribution is very useful in reliability and survival analysis because of its ability in modelling increasing and decreasing failure rate functions. However, when the failure rate function is of a bathtub shape, a single Weibull distribution will not be sufficient. Bathtub failure rate is a common phenomenon in reliability and it is an important ageing property. The system lifetime cycle can usually be divided into three distinct stages: early life, useful life and wear out period. Models for bathtub shaped failure rate functions are needed while it is also useful for a model to be generalisation of the Weibull distribution. In this paper, some models extending the traditional two-parameter Weibull distribution are presented and discussed. We also review some related models that can be used for the modelling of bathtub-shaped failure rate function.", "num_citations": "12\n", "authors": ["445"]}
{"title": "On the monitoring of trended and regularly adjusted processes\n", "abstract": " Trended and regularly adjusted processes are common in manufacturing industries. Such processes are, for example, related to tool wear, material replenishment or some regular maintenance. When the process has a slow trend or is frequently adjusted, the Shewhart chart can be interpreted in the same way as for a stable process. To facilitate comparison between such a trended and adjusted process to a stable case, and to estimate further the loss of effectiveness when the traditional Shewhart chart is applied to trended and adjusted process, this paper provides a statistical interpretation of traditional Shewhart charts for this type of processes. Formulas are derived for the calculation of alarm rate and average run length (ARL). This study is useful when deciding if a traditional Shewhart chart is sufficient or if a more advanced Statistical Process Control method is necessary. Furthermore, given the in-control\u00a0\u2026", "num_citations": "12\n", "authors": ["445"]}
{"title": "An investigation of the effects of inspection errors on the run-length control charts\n", "abstract": " For continuous inspection schemes in an automated manufacturing environment, a useful alternative to the traditional p or np chart is the Run-Length control chart, which is based on plotting the run lengths (the number of conforming items) between successive nonconforming items. However, its establishment relies on the error-free inspection assumption, which can seldom be met in practice. In this paper, the effects of inspection errors on the Run-Length chart are investigated based on that these errors are assumed known. The actual false alarm probability and the average number inspected (ANI) in the presence of inspection errors are studied. This paper also presents the adjusted control limits for the Run-Length chart, which can provide much closer ANI curves to the ones obtained under error-free inspection.", "num_citations": "12\n", "authors": ["445"]}
{"title": "Prioritizing processes for better implementation of statistical process control techniques\n", "abstract": " Statistical process control (SPC) techniques have been widely used in industry. A problem in the implementation of SPC techniques is that usually there are many processes involved in a production line and high priority has to be given to some of them in the face of constraints of available resources. In this paper, the authors discuss some preliminary studies of techniques for process prioritization in SPC implementation and the related decision making approaches. They first present a simple classification scheme based on the statistical and technical importance of each process. Then the use of the quality function deployment (QFD) technique is discussed. A theoretical approach based on the analytic hierarchy process (AHP) technique is then recommended to solve the exact prioritization problem.", "num_citations": "12\n", "authors": ["445"]}
{"title": "EM algorithms for estimating software reliability based on masked data\n", "abstract": " In this paper, the software reliability estimation from masked data is considered based on superposition nonhomogeneous Poisson process models. The masked data are the system failure data when the exact causes of the failures, i.e., the components that have caused the system failure, may be unknown. The components of a software system may indicate its modules, testing strategies and the types of errors according to the practical situations. In general, the maximum likelihood estimates of parameters are difficult to find when there exist masked data, because the superposition process cannot be decomposed into the ordinary processes. In this study, the EM algorithm is investigated to solve the problem of maximum likelihood estimation. It is shown that the EM algorithm is powerful to deal with the masked data. By applying the EM algorithm, the masked data problem is simplified and is reduced to the common\u00a0\u2026", "num_citations": "12\n", "authors": ["445"]}
{"title": "A dynamic approach to performance analysis and reliability improvement of control systems with degraded components\n", "abstract": " Control systems are among the most important subsystems for their ability to undertake indispensable functions in safety-critical systems. Since many key components of such systems follow different performance degradation paths, therefore it is important to have an approach capable of correctly estimating the performance of control systems containing a variety of degraded components. One solution is to endow an existing estimation approach to equip with a capability to cope with uncertainties and inadequate system specifications. This paper presents a hybrid model capable of improving existing approaches by applying the Laplace transform to the time-varying model of the control system while taking into account the varying behaviors of components over different time slices. Reliability is estimated through an event-based Monte Carlo simulation that does not require knowledge of the exact reliability function\u00a0\u2026", "num_citations": "11\n", "authors": ["445"]}
{"title": "Risk-based software release policy under parameter uncertainty\n", "abstract": " The determination of the optimal release time is a significant problem in the software development process. Most existing research on this problem is based on the assumption that the model parameters are either known or can be accurately estimated. Due to the uncertainties associated with parameter estimation created by the very limited amount of software failure data that is generally available, the accuracy of the optimum release time determined by traditional approaches is questionable. When the mean value of the optimal release time is used, for example, there is only a 50 per cent chance that the reliability target is met at the time of release. In this paper, an optimal software release policy under parameter uncertainty is studied. To take parameter uncertainty into consideration, an optimal risk-based software release time determination approach is introduced. Application examples are given to illustrate this\u00a0\u2026", "num_citations": "11\n", "authors": ["445"]}
{"title": "On the change point of the mean residual life of series and parallel systems\n", "abstract": " This paper considers the mean residual life in series and parallel systems with independent and identically distributed components and obtains relationships between the change points of the mean residual life of systems and that of their components. Compared with the change point for single components, should it exists, the change point for a series system occurs later. For a parallel system, however, the change point is located before that for the components, if it exists at all. Moreover, for both types of systems, the distance between the change points of the mean residual life for systems and for components increases with the number of components. These results are helpful in the determination of optimal burn\u2010in time and related decision making in reliability analysis.", "num_citations": "11\n", "authors": ["445"]}
{"title": "Exploiting symmetry in the reliability analysis of coherent systems\n", "abstract": " Components in a complex system are usually not structurally identical. However, in many cases we may find components that are structurally symmetric, and one should make use of this additional information to simplify reliability analysis. The main purpose of this article is to define and study one such class of systems, namely, those having symmetric components, and to derive some reliability\u2010related properties. \u00a9 1996 John Wiley & Sons, Inc.", "num_citations": "11\n", "authors": ["445"]}
{"title": "A markov process model for software reliability analysis\n", "abstract": " Software reliability is a rapidly developing discipline. In this paper we model the fault\u2010detecting processes by Markov processes with decreasing jump intensity. The intensity function is suggested to be a power function of the number of the remaining faults in the software. The models generalize the software reliability model suggested by Jelinski and Moranda (\u2018Software reliability research\u2019, in W. Freiberger (ed.), Statistical Computer Performance Evaluation, Academic Press, New York, 1972. pp. 465\u2013497). The main advantage of our models is that we do not use the assumption that all software faults correspond to the same failure rate. Preliminary studies suggest that a second\u2010order power function is quite a good approximation. Statistical tests also indicate that this may be the case. Numerical results show that the estimation of the expected time to next failure is both reasonable and decreases relatively stably\u00a0\u2026", "num_citations": "11\n", "authors": ["445"]}
{"title": "Resilient critical infrastructure planning under disruptions considering recovery scheduling\n", "abstract": " Reliable and safe critical infrastructures are crucial for the sustainability of modern societies. To cope with increasing disruptive events such as man-made and natural disasters attacking infrastructures, resilience should be considered as an integrated perspective into the system planning process. This paper presents a p-robust optimization model for infrastructure network planning against spatially localized disruptions. The optimization aims at minimizing the investment costs for system hardening and expansion and the total system costs under nominal operating conditions, while incorporating resilience requirements by the p-robustness constraints. Importantly, instead of only mitigating system vulnerability, the proposed model integrates the arranging of the repair sequence of damaged components under limited repair resources into the preevent system planning. The complexity of the proposed model is\u00a0\u2026", "num_citations": "10\n", "authors": ["445"]}
{"title": "Process capability indices based on the highest density interval\n", "abstract": " For process capability indices (PCIs) of non\u2010normal processes, the natural tolerance is defined as the difference between the 99.865 percentile and the 0.135 percentile of the process characteristic. However, some regions with relatively low probability density may still be included in this natural tolerance, while some regions with relatively high probability density may be excluded for asymmetric distributions. To take into account the asymmetry of process distributions and the asymmetry of tolerances from the viewpoint of probability density, the highest density interval is utilized to define the natural tolerance, and a family of new PCIs based on the highest density interval is proposed to ensure that regions with high probability density are included in the natural tolerance. Some properties of the proposed PCIs and two algorithms to compute the highest density interval are given. A real example is given to show the\u00a0\u2026", "num_citations": "10\n", "authors": ["445"]}
{"title": "Testing effort dependent software FDP and FCP models with consideration of imperfect debugging\n", "abstract": " Software reliability can be enhanced considerably during testing with faults being detected and corrected by testers. The allocation of testing resources, such as man power and CPU hours, during testing phase can largely influence fault detection speed and the time to correct a detected fault. The testing resources allocation is usually depicted by testing effort function, which has been incorporated into software reliability models in some recent papers. Fault correction process (FCP) is usually modeled as a delayed process of fault detection process (FDP). In addition, debugging is usually not perfect and new faults can be introduced during testing. In this paper, flexible testing effort dependent paired models of FDP and FCP are derived with consideration of fault introduction. A real dataset is used to illustrate the application of proposed models.", "num_citations": "10\n", "authors": ["445"]}
{"title": "Optimization of feature weights and number of neighbors for analogy based cost estimation in software project management\n", "abstract": " Software cost estimation affects almost all activities of software project development such as: bidding, planning, and budgeting, thus it is very crucial to the success of software project management. In past decades, many methods have been proposed for cost estimation. Analogy based cost estimation (ABE) is among the most popular techniques due to its conceptual simplicity and empirical competitiveness. In order to improve ABE model, many previous studies have focused on optimizing the feature weights in the similarity function. However, according to some prior studies, the K parameter for the K-nearest neighbor is also essential to the performance of ABE. Nevertheless, few studies attempt to optimize the K number of neighbors and most of them are based on the trial-error scheme. In this study, we propose the genetic algorithm to simultaneously optimize the K parameter and the feature weights for ABE\u00a0\u2026", "num_citations": "10\n", "authors": ["445"]}
{"title": "Early software reliability prediction with ANN models\n", "abstract": " It is well-known that accurate reliability estimates can be obtained by using software reliability models only in the later phase of software testing. However, prediction in the early phase is important for cost-effective and timely management. Also this requirement can be achieved with information from previous releases or similar projects. This basic idea has been implemented with nonhomogeneous Poisson process (NHPP) models by assuming the same testing/debugging environment for similar projects or successive releases. In this paper we study an approach to using past fault-related data with artificial neural network (ANN) models to improve reliability predictions in the early testing phase. Numerical examples are shown with both actual and simulated datasets. Better performance of early prediction is observed compared with original ANN model with no such historical fault-related data incorporated. Also, the\u00a0\u2026", "num_citations": "10\n", "authors": ["445"]}
{"title": "Optimal number of hosts in a distributed system based on cost criteria\n", "abstract": " Redundant or distributed systems are increasingly used in system design so that the required reliability and availability can be easily achieved. However, such an approach requires additional resources that can be very costly. Hence, how to design and test such a system in the most cost-effective way is of concern to the developers. A general cost model and a solution algorithm are presented for the determination of the optimal number of hosts and optimal system debugging time that minimize the total cost while achieving a certain performance objective. During testing, software faults are corrected and the reliability shows an increasing trend, and hence system reliability increases. A general system model is constructed based on a Markov process with software reliability and availability obtained from software reliability growth models. The optimization problem is formulated based on the cost criteria and the\u00a0\u2026", "num_citations": "10\n", "authors": ["445"]}
{"title": "On the estimation error in zero-inflated Poisson model for process control\n", "abstract": " The control chart based on a Poisson distribution has often been used to monitor the number of defects in sampling units. However, many false alarms could be observed due to extra zero counts, especially for high-quality processes. Therefore, some alternatives have been developed to alleviate this problem, one of which is the control chart based on the zero-inflated Poisson distribution. This distribution takes into account the extra zeros present in the data, and yield more accurate results than the Poisson distribution. However, implementing a control chart is often based on the assumption that the parameters are either known or an accurate estimate is available. For a high quality process, an accurate estimate may require a very large sample size, which is seldom available. In this paper the effect of estimation error is investigated. An analytical approximation is derived to compute shift detection probability and\u00a0\u2026", "num_citations": "10\n", "authors": ["445"]}
{"title": "Process monitoring strategies for surface mount manufacturing processes\n", "abstract": " Establishing reliable surface mount assemblies requires robust design and assembly practices, including stringent process control schemes for achieving high yield processes and high quality solder interconnects. Conventional Shewhart-based process control charts prevalent in today's complex surface mount manufacturing processes are found to be inadequate as a result of autocorrelation, high false alarm probability, and inability to detect process deterioration. Hence, new strategies are needed to circumvent the shortcomings of traditional process control techniques. In this article, the adequacy of Shewhart models in a surface mount manufacturing environment is examined and some alternative solutions and strategies for process monitoring are discussed. For modeling solder paste deposition process data, a time series analysis based on neural network models is highly desirable for both\u00a0\u2026", "num_citations": "10\n", "authors": ["445"]}
{"title": "Regression goodness-of-fit Test for Software reliability model validation\n", "abstract": " Software reliability growth models based on nonhomogenerous Poisson process (NHPP) seems to be most commonly used because of their simplicity. These models are commonly used in software reliability engineering practice and most of the decision-makings such as the optimal software release time determination, the optimal testing-resource allocation, etc. are based on the results obtained from the analysis of selected model.However a problem is the model validation and selection. If the selected model does not fit the collected software testing data relatively well, we would expect a low prediction ability of this model and the decision-makings based on the analysis of this model would be far from what is considered to be optimal decision. This article presents a simple method for model validation.", "num_citations": "10\n", "authors": ["445"]}
{"title": "Sensitivity of the relationship matrix in quality function deployment\n", "abstract": " Quality Function Deployment (QFD) adopts a customer-driven approach and provides a structural way to ensure that the final product or service meets customer requirements. As the central part of the house of quality (HOQ), the relationship matrix links customer attributes and technical responses through a two-dimensional diagram. As the weightage used is discrete in nature, the ranking of technical responses is sensitive to the scoring system. The technical priority guides QFD practitioners in making trade-offs for resource allocation and directs the downstream QFD activities. The stability of the technical priority values is, hence, important and desirable. To minimize the variability of technical priority, an optimization model is developed and its optimal solution is employed as the scoring system in the calculation of the final technical importance. Two examples are presented to illustrate the use of the model. Some implementation issues relating to the proposed approach are also discussed.", "num_citations": "10\n", "authors": ["445"]}
{"title": "Planning and optimizing environmental stress screening\n", "abstract": " Environmental stress screening (ESS) is widely used in the electronics industries as a means to remove early failures. It is a process that calls for proper planning as inadequate duration is ineffective while prolonged screening can incur unnecessary cost. This note describes an approach utilizing mathematical programming to ensure that the right amount of screening is in place at each assembly level. The factors considered include the screening cost and desired operational reliability.", "num_citations": "10\n", "authors": ["445"]}
{"title": "System reliability growth analysis using component failure data\n", "abstract": " In this paper the problem of system-level reliability growth estimation using component-level failure data is studied. It is suggested that system failure data should be broken down into component, or subsystem, failure data when the above problems have occurred during the system testing phase. The proposed approach is especially useful when the system is not unchanged over the time, when some subsystems are improved more than others, or when the testing has been concentrated on different components at different time. These situations usually happen in practice and it may also be the case even if the system failure data is provided. Two sets of data are used to illustrate the simple approach; one is a set of component failure data for which all subsystems are available for testing at the same time and for the other set of data, the starting times are different for different subsystems.", "num_citations": "10\n", "authors": ["445"]}
{"title": "Some monitoring procedures related to asymmetry parameter of Azzalini\u2019s skew-normal model\n", "abstract": " \u2022 In the real world, we often observe that the underlying distribution of some Gaussian processes tends to become skewed, when some undesirable assignable cause takes place in the process. Such phenomena are common in the field of manufacturing and in chemical industries, among others, where a process deviates from a normal model and becomes a skew-normal. The Azzalini\u2019s skew-normal (hereafter ASN) distribution is a well-known model for such processes. In other words, we assume that the in-control (hereafter IC) distribution of the process under consideration is normal, that is a special case of the ASN model with asymmetry parameter zero, whereas the out-of-control (hereafter OOC) process distribution is ASN with any nonzero asymmetry parameter. In the ASN model, a change in asymmetry parameter also induces shifts in both the mean and variance, even if, both the location and scale parameters remain invariant. Traditionally, researchers consider a shift either in the mean or in variance or in both the parameters of the normal distribution. Some inference and monitoring issues related to deviation from symmetry are essential problems that are largely overlooked in literature. To this end, we propose various test statistics and design for sequential monitoring schemes for the asymmetry parameter of the ASN model. We examine and compare the performance of various procedures based on an extensive Monte-Carlo experiment. We provide an illustration based on an interesting manufacturing case study. We also offer some concluding remarks and future research problems.", "num_citations": "9\n", "authors": ["445"]}
{"title": "Reliability assessment of system under a generalized run shock model\n", "abstract": " In this paper we are concerned with modelling the reliability of a system subject to external shocks. In a run shock model, the system fails when a sequence of shocks above a threshold arrive in succession. Nevertheless, using a single threshold to measure the severity of a shock is too critical in real practice. To this end, we develop a generalized run shock model with two thresholds. We employ a phase-type distribution to model the damage size and the inter-arrival time of shocks, which is highly versatile and may be used to model many quantitative features of random phenomenon. Furthermore, we use the Markovian property to construct a multi-state system which degrades with the arrival of shocks. We also provide a numerical example to illustrate our results.", "num_citations": "9\n", "authors": ["445"]}
{"title": "Bayesian network-based risk analysis methodology: A case of atmospheric and vacuum distillation unit\n", "abstract": " Chemical and petrochemical accidents, such as fires and explosions, do not happen frequently but have considerable consequences. These accidents compromise not only human safety but also cause significant economic losses and environmental contamination. The increasing complexity of chemical infrastructures increases the requirements of risk prevention. Thus, risk analysis for petrochemical systems is essential in helping analysts find the weakest process in the entire system and be used to strengthen the process and improve safety. Risk analysis has been previously studied; however, traditional methods have limitations. This study proposes a methodology that is based on Bayesian networks by giving a model for system risk analysis. The event is classified into three categories; cause, incident, and accident, according to criticality and thus, the model is analyzed as a three-layered structure. The\u00a0\u2026", "num_citations": "9\n", "authors": ["445"]}
{"title": "A dynamic maintenance strategy for prognostics and health management of degrading systems: Application in locomotive wheel-sets\n", "abstract": " This paper develops a dynamic maintenance strategy for prognostics and health management (PHM) of a degrading system. The system under investigation suffers a continuous degradation process, modeled as a Gamma process. In addition to the degradation process, the system is subject to aging, which contributes to the increase of failure rate. An additive model is employed to describe the impact of degradation level and aging on system failure rate. Inspection is implemented upon the system so as to effectively avoid failure. At inspection, the system will be repaired or replaced in terms of the degradation level. Different from previous studies which assume that repair will always lead to an improvement on system degradation, in our study, however, the effect of repair is twofold. It will reduce the system age to 0 but will increase the degradation level. System reliability is analyzed as a first step to serve for the\u00a0\u2026", "num_citations": "9\n", "authors": ["445"]}
{"title": "Investigating the root causes of major failures of critical components\u2013With a case study of asbestos cement pipes\n", "abstract": " Root cause analysis (RCA) is an essential step in quality and reliability improvement. However, it is not straightforward because most of the time, failure analysis does not always consider the whole system approach. Different applications, scenarios, and objectives require different adaptations and modifications of the approach. In this paper, we present a study of RCA by analyzing major failures of critical components. While many researchers have used and developed various tools and techniques to improve RCA in this aspect, applications of this research in practice still lack. In this study, one of the more established models; Issue-Based Information System (IBIS) using DesignVUE, is implemented in a water utility set up to solve the problem of major failures of Asbestos Cement (AC) pipes. In this work, we present a case study of major failures with serious consequences of pipe failures in a water system\u00a0\u2026", "num_citations": "9\n", "authors": ["445"]}
{"title": "Assessing wind curtailment under different wind capacity considering the possibilistic uncertainty of wind resources\n", "abstract": " For power system decision makers, they should decide a reasonable wind capacity or target penetration level considering the balance of wind curtailment, investment, system reliability and social benefit. Assessing wind curtailment under different wind capacity, considering the wind resource uncertainty, is therefore necessary for utilizing wind power effectively. In this paper, fuzzy wind capacity factor is proposed to analyze and propagate the uncertainty of wind resources based on the wind speed distribution. By investigating the maximal wind power output that can be accepted in candidate system nodes, a fuzzy linear programming problem is proposed to decide the required wind capacity considering the fuzzy wind capacity factor. Based on the cumulative distribution function of wind capacity resulting from large number of simulations, the relationship between wind capacity and wind curtailment is determined\u00a0\u2026", "num_citations": "9\n", "authors": ["445"]}
{"title": "A new framework and application of software reliability estimation based on fault detection and correction processes\n", "abstract": " Software reliability growth modeling plays an important role in software reliability evaluation. To incorporate more information and provide more accurate analysis, modeling software fault detection and correction processes has attracted widespread research attention recently. However, the assumption of the stochastic fault correction time delay brings more difficulties in modeling and estimating the parameters. In practice, other than the grouped fault data, software test records often include some more detailed information, such as the rough time when one fault is detected or corrected. Such semi-grouped dataset contains more information about fault removal processes than commonly used grouped dataset. Using the semi-grouped datasets can improve the accuracy of time delayed models. In this paper, a fault removal modelling framework for software reliability with semi-grouped data is studied and extended into\u00a0\u2026", "num_citations": "9\n", "authors": ["445"]}
{"title": "Violent victimization in new and established Hispanic areas, 2007-2010\n", "abstract": " Hispanic populations in many US communities experienced rapid growth during the past 3 decades. 1 Before 1980, most Hispanics lived in the Southwest and in New York, Florida, and Illinois. From 1980 to 2010, the number of Hispanics living outside of these areas increased from 2.7 million to 13.5 million. Meanwhile, from 2007 to 2010, the overall rate of violence in new Hispanic areas exhibited no statistically significant difference from that in established Hispanic areas.This report is based on data from the Bureau of Justice Statistics\u2019(BJS) area-identified National Crime Victimization Survey (NCVS). It examines violent victimization (rape or sexual assault, robbery, aggravated assault, and simple assault) among blacks, whites, and Hispanics in four types of Hispanic areas:(1) established slow-growth areas,(2) established fast-growth areas,(3) new Hispanic areas, and (4) small Hispanic areas.", "num_citations": "9\n", "authors": ["445"]}
{"title": "Generalized confidence interval for the scale parameter of the power-law process\n", "abstract": " The power-law process is widely used in the analysis of repairable system reliability. In this article, interval estimation for the scale parameter is investigated under some general conditions. A procedure to derive a generalized confidence interval for the scale parameter is presented. We also study the accuracy of the generalized confidence interval by Monte Carlo simulation. Finally, two examples are shown to illustrate the proposed procedure.", "num_citations": "9\n", "authors": ["445"]}
{"title": "Control limits based on the narrowest confidence interval\n", "abstract": " In statistical process control (SPC), if the traditional 3-sigma control limits or probability limits are adopted, some points with relatively high occurrence possibility may be excluded; however, some points with relatively small occurrence possibility may be accepted for asymmetrical or multimodal distributions. Motivated by the highest posterior density credibility interval, we propose control limits based on the narrowest confidence interval to solve the problem, where the narrowest confidence interval denotes the confidence interval with the shortest width. The proposed control limits will not only meet the false alarm requirement, but also ensure that each in-control data point has relatively high occurrence possibility. Some properties of the proposed control limits and its relation to the existing two control limits are presented in the end.", "num_citations": "9\n", "authors": ["445"]}
{"title": "A model of open source software maintenance activities\n", "abstract": " The development of computer networks, especially the Internet, has largely facilitated the communications among software developers and it resulted a thriving of open source software systems (OSS). As more and more open source software systems are deployed by individuals and enterprises, it is vital to study such systems separately from traditional software systems since their development and maintenance processes are fairly different. In this paper, we investigate a type of behaviors of software maintenance of open source software systems and propose an approach that is based on NHPP with Rayleigh function to model the maintenance activities. Using the proposed model, the maintenance events could be estimated with high confidence and this will certainly assistant software maintenance planning. A numerical example using from the data from Apache project is shown to illustrate the application of our\u00a0\u2026", "num_citations": "9\n", "authors": ["445"]}
{"title": "Ch. 28. Software reliability modeling, estimation and analysis\n", "abstract": " Our society has become increasingly dependent on computer systems. Many system failures occur due to latent software defects encountered as the software executes various input combinations during operation. Software failure process is a random process and a systematic approach is needed to predict, measure and manage software failures so that the reliability of software can be quantified and improved.Software reliability is defined as the probability of failure-free operation of a computer program for a specified time in a specified environment. Software reliability parameters, measures and metrics, described by software reliability models (SRM), offer the possibility of evaluating and monitoring software failures quantitatively during the verification phases of a product. The probabilistic models for software failures and their frequency of occurrence can be described and measured by software reliability functions\u00a0\u2026", "num_citations": "9\n", "authors": ["445"]}
{"title": "Dependence and ageing properties of bivariate Lomax distribution\n", "abstract": " The notions of dependence between two variables in a bivariate distributions are useful concepts in reliability theory and lifetime data analysis. Apart from the covariance and the correlation coefficient which are of particular interest as they give a measure of the strength of the dependence, other notions such as \u2018association\u2019 also become relevant in many situations. In this paper, the dependence notions and the correlation of the bivariate Lomax distribution are studied. The maximum and minimum of the correlation coefficient are also obtained. It is shown that the bivariate Lomax distribution has a reasonably wide admissible range that compares well with the Farlie-Gumbel-Morgenstern bivariate distribution with various marginals. Furthermore, some ageing properties of the Lomax distribution are investigated and the condition for monotonic aging are studied for under different bivariate ageing definitions.", "num_citations": "9\n", "authors": ["445"]}
{"title": "On Bayesian software reliability modelling\n", "abstract": " In this expository article we first survey some basic software reliability models and some of their Bayesian extensions. A point processes approach is emphasized and some suggestions are given. Finally, the problem of model selection is discussed.", "num_citations": "9\n", "authors": ["445"]}
{"title": "A note on the Natvig measure\n", "abstract": " In this note the Natvig measure of component importance is studied. This measure was originally suggested by Natvig (1979). Here some new results are obtained.", "num_citations": "9\n", "authors": ["445"]}
{"title": "Planning accelerated reliability tests for mission-oriented systems subject to degradation and shocks\n", "abstract": " This article presents a novel accelerated reliability testing framework for mission-oriented systems. The system to be tested is assumed to suffer from cumulative degradation and traumatic shocks with increasing intensity. We propose a new optimality criterion that minimizes the asymptotic variance of the predicted reliability evaluated at the mission\u2019s end time. Two usage scenarios are considered in this study: one is to assume that systems are brand new at the start of the mission and the other is that systems are randomly selected from used ones under pre-determined policies. Optimal test plans for both scenarios are obtained via delta methods by utilizing the Fisher information. The global optimality of test plans is verified using general equivalence theorems. A revisited example of a carbon-film resistor is presented to illustrate the efficiency and robustness of optimal test plans for both new and randomly aged\u00a0\u2026", "num_citations": "8\n", "authors": ["445"]}
{"title": "An adaptive two-stage Bayesian model averaging approach to planning and analyzing accelerated life tests under model uncertainty\n", "abstract": " Accelerated life testing (ALT) is commonly used to predict the lifetime of a product at its use stress by subjecting test units to elevated stress conditions that accelerate the occurrence of failures. For new products, the selection of an acceleration model for planning optimal ALT plans is challenging due to the absence of historical lifetime data. The misspecification of an ALT model can lead to considerable errors when it is used to predict the product\u2019s life quantiles. This article proposes a two-stage Bayesian approach to constructing ALT plans and predicting lifetime quantiles. At the first stage, the ALT plan is optimized based on the prior information of candidate models under a modified V-optimality criterion that incorporates both asymptotic prediction variance and squared bias. A Bayesian model averaging (BMA) framework is used to derive the posterior model and the posterior distribution for the life quantile of\u00a0\u2026", "num_citations": "8\n", "authors": ["445"]}
{"title": "An approach for reliability demonstration test based on power\u2010law growth model\n", "abstract": " Reliability demonstration test (RDT) is a critical and necessary step before the acceptance of an industrial system. Generally, a RDT focuses on designing a test plan through which one can judge whether the system reliability indices meet specific requirements. There are many established RDT plans, but few have incorporated the reliability growth aspects of the corresponding products. In this paper, we examine a comprehensive test plan that involves information concerning the reliability growth stage. An approach for RDT under the assumption of the power\u2010law model is proposed. It combines data related to the growth stage with those pertaining to the test stage of the product to reduce the cost of the test. Through simulation studies and numerical examples, we illustrate the characteristics of the test plan and significant reduction in test costs through our approach. Copyright \u00a9 2017 John Wiley & Sons, Ltd.", "num_citations": "8\n", "authors": ["445"]}
{"title": "A novel critical infrastructure resilience assessment approach using dynamic Bayesian networks\n", "abstract": " The word resilience originally originates from the Latin word \u201cresiliere\u201d, which means to \u201cbounce back\u201d. The concept has been used in various fields, such as ecology, economics, psychology, and society, with different definitions. In the field of critical infrastructure, although some resilience metrics are proposed, they are totally different from each other, which are determined by the performances of the objects of evaluation. Here we bridge the gap by developing a universal critical infrastructure resilience metric from the perspective of reliability engineering. A dynamic Bayesian networks-based assessment approach is proposed to calculate the resilience value. A series, parallel and voting system is used to demonstrate the application of the developed resilience metric and assessment approach.", "num_citations": "8\n", "authors": ["445"]}
{"title": "Integrated beta model for bathtub-shaped hazard rate data\n", "abstract": " In this paper, we introduce a new parametric distribution for modelling lifetime data with bathtub-shaped hazard rate, derived by representing the cumulative hazard as proportional to a generalized beta density. The distribution has a finite range, useful when there is a maximum possible lifetime, a common scenario when there are additional failure modes leading to failure after a certain time. Reliability and other distributional properties of the distribution are discussed. Parameter issues are also studied. The application of the model is illustrated by fitting the model to several data sets from the literature.", "num_citations": "8\n", "authors": ["445"]}
{"title": "Economic modelling for statistical process control subject to a general quality deterioration\n", "abstract": " The applications of control chart have traditionally focused on the detection of step shifts in process mean. However, changes are usually gradual, not as perfect step shifts. The common consideration of a shift as a step function does not always adequately describe what actually happens in practice. Hence, there is a need for more realistic assumptions to be incorporated. This paper employs a Markov chain approach and provides a way to quantitatively measure the economic performance of control charts in the presence of a more general quality deterioration mechanism. The finite production run is considered in the model as it has become a very important production mode at present and the process failure mechanism is described by geometric distribution. The chart properties, particularly on the issues of the quality deterioration mechanism, are investigated. The findings provide critical insights on the use of step\u00a0\u2026", "num_citations": "8\n", "authors": ["445"]}
{"title": "Reuse of embedded software in small and medium enterprises\n", "abstract": " Software reuse is a commonly adopted approach to improve productivity and quality of software development. However, the study of embedded software reuse, especially for small and medium sized enterprises (SMEs), is rare. This paper presents an empirical study on the effect of embedded software reuse of a SME. The study focused on the relationship between reuse rate against productivity, cost, quality, and time of embedded software projects over a period of three years. The result supports higher modular reuse of embedded software of a SME can improve productivity, cost and quality but not the time to market. This result provides direct evidence to management of SMEs on the benefit of improving reuse rate.", "num_citations": "8\n", "authors": ["445"]}
{"title": "Some statistical models for the monitoring of high-quality processes\n", "abstract": " One important application of statistical models in industry is statistical process control. Many control charts have been developed and used in industry. They are easy to use, but have been developed based on statistical principles. However, for today\u02bcs high-quality processes, traditional control-charting techniques are not applicable in many situations. Research has been going on in the last two decades and new methods have been proposed. This chapter summarizes some of these techniques. High-quality processes are those with very low defect-occurrence rates. Control charts based on the cumulative count of conforming items are recommended for such processes. The use of such charts has opened up new frontiers in the research and applications of statistical control charts in general. In this chapter, several extended or modified statistical models are described. They are useful when the simple and basic geometric distribution is not appropriate or is insufficient. In particular, we present some extended Poisson distribution models that can be used for count data with large numbers of zero counts. We also extend the chart to the case of general time-between-event monitoring; such an extension can be useful in service or reliability monitoring. Traditionally, the exponential distribution is used for the modeling of time-between-events, although other distributions such as the Weibull or gamma distribution can also be used in this context.", "num_citations": "8\n", "authors": ["445"]}
{"title": "Distributed system availability in the case of imperfect debugging process\n", "abstract": " Significance: This paper studies the imperfect debugging process of combined software/hardware systems and presents a general Markov model to analyze the system availability. The approaches and results could help practitioners to consider the common imperfect debugging problem and deal with some important design and scheduling issues.", "num_citations": "8\n", "authors": ["445"]}
{"title": "Statistical monitoring and control of tool wear processes\n", "abstract": " Statistical control charts have been successfully used in industry for monitoring stable processes. However, processes with uncontrollable but acceptable trend are common in practice. One typical example is the wear process of cutting tools. Conventional control charts may not serve the purpose of process monitoring. In this paper, a forecast-based technique using Double Exponential Smoothing is proposed. It eliminates the trend component, and control charts are applied to the residuals. Furthermore, a procedure based on double control lines is suggested and adopted in tool wear process monitoring to integrate statistical and engineering properties for better decision making on tool wear-out. Other than the monitoring of tool wear process, the method can be used for better monitoring of other processes with trend. An actual tool wear data set is used as illustration.", "num_citations": "8\n", "authors": ["445"]}
{"title": "Bayesian planning of step\u2010stress accelerated degradation tests under various optimality criteria\n", "abstract": " Step\u2010stress accelerated degradation testing (SSADT) has become a common approach to predicting lifetime for highly reliable products that are unlikely to fail in a reasonable time under use conditions or even elevated stress conditions. In literature, the planning of SSADT has been widely investigated for stochastic degradation processes, such as Wiener processes and gamma processes. In this paper, we model the optimal SSADT planning problem from a Bayesian perspective and optimize test plans by determining both stress levels and the allocation of inspections. Large\u2010sample approximation is used to derive the asymptotic Bayesian utility functions under 3 planning criteria. A revisited LED lamp example is presented to illustrate our method. The comparison with optimal plans from previous studies demonstrates the necessity of considering the stress levels and inspection allocations simultaneously.", "num_citations": "7\n", "authors": ["445"]}
{"title": "Reliability demonstration test for load-sharing systems with exponential and Weibull components\n", "abstract": " Conducting a Reliability Demonstration Test (RDT) is a crucial step in production. Products are tested under certain schemes to demonstrate whether their reliability indices reach pre-specified thresholds. Test schemes for RDT have been studied in different situations, e.g., lifetime testing, degradation testing and accelerated testing. Systems designed with several structures are also investigated in many RDT plans. Despite the availability of a range of test plans for different systems, RDT planning for load-sharing systems hasn\u2019t yet received the attention it deserves. In this paper, we propose a demonstration method for two specific types of load-sharing systems with components subject to two distributions: exponential and Weibull. Based on the assumptions and interpretations made in several previous works on such load-sharing systems, we set the mean time to failure (MTTF) of the total system as the demonstration target. We represent the MTTF as a summation of mean time between successive component failures. Next, we introduce generalized test statistics for both the underlying distributions. Finally, RDT plans for the two types of systems are established on the basis of these test statistics.", "num_citations": "7\n", "authors": ["445"]}
{"title": "Comprehensive cost oriented predictive maintenance based on mission reliability for a manufacturing system\n", "abstract": " In this paper, a comprehensive cost oriented dynamic predictive maintenance policy based on mission reliability state is developed for a multi-state single-machine manufacturing system. In view of the inherent polymorphism of manufacturing systems (i.e., dynamic production scheduling and performance degradation), the connotation of mission reliability of equipment is defined and modeled based on the processing capacity distribution which integrates multiple fault data. Further, the relationship between mission reliability and performance of equipment is established by using the unavailability as the intermediary. The optimal predictive maintenance policy, the best mission reliability threshold for performing predictive maintenance action, is obtained by minimizing the comprehensive cost which includes processing capacity loss, corrective maintenance cost, predictive maintenance cost and indirect loss caused by\u00a0\u2026", "num_citations": "7\n", "authors": ["445"]}
{"title": "Revealing the dark side of WebRTC statistics collected by Google Chrome\n", "abstract": " Google  Chrome  provides  a  built-in  tool  to  collectreal-time   session-related   performance   statistics   of   Web-basedReal-Time  Communication  (WebRTC).  Although  the  Chromestatistics  have  a  number  of  limitations,  we  believe  that  they  canbe  used  in  studies  of  Quality  of  Experience  (QoE)  aspects  ofWebRTC  services.  In  this  paper,  we  first  reveal  the  limitationsof  the  collected  statistics  and  its  consequences.  We  then  discusshow  to  overcome  these  issues.", "num_citations": "7\n", "authors": ["445"]}
{"title": "Optimizing maritime travel time reliability\n", "abstract": " Travel time reliability optimization problems generally cannot be formulated in direct mathematical form due to complexity of precise information. Classically, qualitative form of objective function and/or constraints does not allow the decision maker to represent it in a typical form of standard optimization model. This article attempts to resolve travel time reliability optimization problem of maritime transportation. The marine vessel\u2019s travel time consists of seven time components. The travel time reliability is considered in a possibilistic manner, and then the reliability optimization problem with budgetary constraints and stage-time limitations is formulated. Next, algorithmic framework for solution of the possibilistic programming has been proposed, followed by a suitable illustration. Finally, this article investigates the parameter sensitivity that has consequences in the marine vessel\u2019s transportation decision making. The\u00a0\u2026", "num_citations": "7\n", "authors": ["445"]}
{"title": "A new methodology to integrate human factors analysis and classification system with Bayesian Network\n", "abstract": " In this paper, a new methodology, which integrates human factors analysis and classification system (HFACS) with Bayesian Network (BN), is proposed to assess the contribution of human and organizational factors in maritime accidents. As a means of making up the lack of quantitative analysis within HFACS, the integration of BN and fuzzy analytical hierarchy process (AHP) have been selected to estimate quantitatively the contribution of human error to the accident. At the same time, the HFACS' 4-level structure provides a systematic guideline in the construction of the BN to model how human errors are related to form a network. Fuzzy AHP and decomposition method are applied to estimate the conditional probabilities of BN, which is more efficient manner and can reduce subjective biases. A case study of ship collision showed that the method is more flexible to seek out the critical latent human and\u00a0\u2026", "num_citations": "7\n", "authors": ["445"]}
{"title": "Bayesian inference approach for probabilistic analogy based software maintenance effort estimation\n", "abstract": " Software maintenance effort estimation is essential for the success of software maintenance process. In the past decades, many methods have been proposed for maintenance effort estimation. However, most existing estimation methods only produce point predictions. Due to the inherent uncertainties and complexities in the maintenance process, the accurate point estimates are often obtained with great difficulties. Therefore some prior studies have been focusing on probabilistic predictions. Analogy Based Estimation (ABE) is one popular point estimation technique. This method is widely accepted due to its conceptual simplicity and empirical competitiveness. However, there is still a lack of probabilistic framework for ABE model. In this study, we first propose a probabilistic framework of ABE (PABE). The predictive PABE is obtained by integrating over its parameter k number of nearest neighbors via Bayesian\u00a0\u2026", "num_citations": "7\n", "authors": ["445"]}
{"title": "Software reliability prediction improvement with prior information incorporated\n", "abstract": " Software reliability growth models (SRGMs) play a central role in reliability prediction, providing critical measurements for decision makings in software testing. Against the immediate fault correction assumption, extensions on traditional NHPP models have been studied by associating fault correction process (FCP) with fault detection process (FDP). However, as they are applied by fitting against historical data, such paired models for both FDP&FCP cannot provide accurate estimations in the early phase of testing, similar to traditional NHPP models. Only in the later phase can accurate estimation be made, but then sometimes it would be too late to be useful. It is necessary to estimate accurately in the early phase for timely decision-makings. Stepping from this point, this paper studied the improvement of reliability prediction by incorporating information from prior similar project or release, within the context of paired FDP&FCP models. Limited by the available datasets, this study is carried out with simulation. The paired modeling framework provides a convenient simulation approach. Improvement in reliability prediction is observed with the analysis on simulated datasets. Also, with a cost model incorporating the factor of fault correctors' number, the optimal release time is discussed and illustrated in a simulated approach.", "num_citations": "7\n", "authors": ["445"]}
{"title": "A reliability model for piezoelectric actuators\n", "abstract": " A probabilistic approach is presented for the evaluation of the reliability of piezoelectric micro-actuators that takes into account the effects of both driving voltage and temperature. Based on the relationships between the lifetime and degradation mechanism of piezoelectric actuators and the electric field strength, as well as the actuator working temperature, a two-dimensional probability model for evaluating reliability of piezoelectric micro-actuators is described. The concept of two-dimensional strength is proposed to incorporate the electric driving voltage and the working temperature of the piezoelectric actuators at a specified lifetime. The lifetime (number of cycles to failure) of piezoelectric actuator, electric load and temperature are considered as the random variables and their probability distributions are discussed. A two-dimensional strength probability distribution function is derived. A two-dimensional\u00a0\u2026", "num_citations": "7\n", "authors": ["445"]}
{"title": "Distribution of runs in a two-stage process monitoring model\n", "abstract": " In statistical process control, procedures similar to double sampling technique can be used to incorporate information from two observations. This leads to the study of the distribution of runs. In this paper, a run is defined as the number of nonconformities observed until an alarm when the two stage procedure is used. The exact distribution of run and the average quantity inspected between alarms are derived and we also obtain some analytical results concerning the expected value of some interesting related quantities.", "num_citations": "7\n", "authors": ["445"]}
{"title": "On some discrete notions of aging\n", "abstract": " This chapter is dedicated to the study of basic notions of aging, such as IFR (Increasing Failure Rate) IFRA (Increasing Failure Rate in Average) and NBU (New Better than Used), when system lifetimes are discrete random variables. As stated by Shaked, Shantikumar and Valdez-Torres, several different definitions are possible for these notions, which are equivalent in the continuous case but not in the discrete case. It is shown that the problem lies in the usual definition of failure rate for discrete distributions. An alternative definition, also known as the second rate of failure, is discussed to solve the above-mentioned problem.", "num_citations": "7\n", "authors": ["445"]}
{"title": "Development and applications of a three-parameter Weibull distribution with load-dependent location and scale parameters\n", "abstract": " In this paper, we first discuss some problems encountered in traditional load-testing data analysis by an illustration. Then a three parameter Weibull distribution model with load-dependent location and scale parameters is proposed to model the relationships between reliability, load and life of components. Using this model, all data can be analyzed together, the total number of parameters to be estimated is reduced greatly, and the results are more suitable for practical use. In particular, the R-L-N equations and strength distribution, which are different from those obtained using conventional methods, can be derived directly.", "num_citations": "7\n", "authors": ["445"]}
{"title": "A Joint Long Short-Term Memory and AdaBoost regression approach with application to remaining useful life estimation\n", "abstract": " Along with wide application of sensors, multi-dimensional time-series data are commonly available for remaining useful life (RUL) estimation. This paper proposes a joint data-driven approach that adapts two models, AdaBoost regression and Long Short-Term Memory (LSTM), to estimate the RUL based on data trajectory extension. In RUL prediction, the data trajectories in the training set contain the data up to the units\u2019 failure while the data trajectories in the testing set do not. Although this fact has a significant negative effect on the accuracy of RUL estimation, it is considered by few literatures. The proposed approach adapts the LSTM to learn the time series dependencies of training data and then extend the trajectories of testing data, aiming at reducing the variance of the lengths of data trajectory between the training and testing sets. Then, the proposed approach adapts the AdaBoost regression to estimate the\u00a0\u2026", "num_citations": "6\n", "authors": ["445"]}
{"title": "Service assurance in 5G networks: A study of joint monitoring and analytics\n", "abstract": " In 5G, network slicing is considered as an important enabler to support various verticals simultaneously, by providing dedicated and isolated slice services over a common infrastructure. Many verticals have specific Service Level Agreement (SLA), which should be guaranteed by mechanisms like service assurance (SA). In 5G, SA is required to collaborate with orchestration and management to automate the slice provisioning process. In this paper, we first propose a hierarchical, distributive, and modular SA architecture to assure the entire slice services from end to end (E2E), based on distributed modules assuring underlying services, functions, and resources. Secondly, a closed loop is formed between assurance and orchestration to enable automation by cooperation between monitoring, analytics, and orchestration. In addition, correlation across layers (infrastructure, network function, network service and E2E\u00a0\u2026", "num_citations": "6\n", "authors": ["445"]}
{"title": "Cost analysis of a piece-wise renewing free replacement warranty policy\n", "abstract": " Nowadays, fierce competition and increasing customer requirements force manufacturers to continuously provide better after-sales services and supports. One such kind of after-sales services is product warranty which has been offered for almost all products in today\u2019s market. In this paper, we study a new warranty policy, called piece-wise renewing free replacement warranty. Under this policy, the whole warranty period is divided into two sub-periods and once an item fails in a specific sub-period, it will be replaced by a new identical one and the warranty period is fully or partially renewed. The expected warranty cost and warranty cycle of this policy are derived from the manufacturer\u2019s perspective. The proposed model is then modified by involving three sub-periods and a failure limit, respectively. In the latter scenario, when the number of item failures over a warranty cycle exceeds a pre-specified threshold, the\u00a0\u2026", "num_citations": "6\n", "authors": ["445"]}
{"title": "Synthetic exponential control charts with unknown parameter\n", "abstract": " The existing synthetic exponential control charts are based on the assumption of known in-control parameter. However, the in-control parameter has to be estimated from a Phase I dataset. In this article, we use the exact probability distribution, especially the percentiles, mean, and standard deviation of the conditional average run length (ARL) to evaluate the effect of parameter estimation on the performance of the Phase II synthetic exponential charts. This approach accounts for the variability in the conditional ARL values of the synthetic chart obtained by different practitioners. Since parameter estimation results in more false alarms than expected, we develop an exact method to design the adjusted synthetic charts with desired conditional in-control performance. Results of known and unknown in-control parameter cases show that the control limit of the conforming run length sub-chart of the synthetic chart should\u00a0\u2026", "num_citations": "6\n", "authors": ["445"]}
{"title": "A univariate procedure for monitoring location and dispersion with ordered categorical data\n", "abstract": " The quality characteristic is usually measured by ordered attribute levels, such as good, general, and poor, which describe different magnitudes of the characteristic. The ordinal levels are determined by a continuous latent variable, the shifts of which are reflected by the observed counts in each level. This article devises a control procedure based on the discrepancy between observed average cumulative counts and their expected ones. Simulation results are shown to demonstrate its superior sensitivity in simultaneously detecting location and dispersion shifts of the latent variable. Flexibility in assigning the weight for each level can allow the chart to be more powerful.", "num_citations": "6\n", "authors": ["445"]}
{"title": "Service assurance architecture in NFV\n", "abstract": " Service Assurance (SA) is a significant part of Network Function Virtualization (NFV) to enable automated and efficient service delivery from end to end (E2E). In NFV, SA should be integrated into the design and development loop from the beginning. However, it sees slower pace than other NFV management and orchestration (MANO) components. Most of present NFV-SA solutions are partial and do not provide a global view to assure services E2E. In this paper, we propose a NFV-SA architecture that on top of existing SA solutions, extends the state of the art by including an E2E scope and customer-facing assurance. The hierarchical, distributive and modular features of this architecture increases efficiency, reduces complexity and communications overhead. Furthermore, it gives operators flexibility to customize their SA framework and differentiate their services. We are also aware of challenges to realize this\u00a0\u2026", "num_citations": "6\n", "authors": ["445"]}
{"title": "Monitoring the shape parameter of a Weibull renewal process\n", "abstract": " This research arose from a challenge faced in real practice\u2014monitoring changes to the Weibull shape parameter. From first-hand experience, we understand that a mechanism for such a purpose is very useful. This article is primarily focused on monitoring the shape parameter of a Weibull renewal process. We derive a novel statistic on the Weibull shape parameter making use of maximum likelihood theory, which is demonstrated to follow an approximately normal distribution. This desirable normality property makes the statistic well suited for use in monitoring the Weibull shape parameter. It also allows for a simple approach to constructing a Shewhart-type control chart, named the Beta chart. The parameter values required to design a Beta chart are provided. A self-starting procedure is also proposed for setting up the Phase I Beta chart. The Average Run Length (ARL) performance of the Beta chart is evaluated\u00a0\u2026", "num_citations": "6\n", "authors": ["445"]}
{"title": "Bayesian analysis for NHPP-based software fault detection and correction processes\n", "abstract": " Many software reliability methods have been proposed for estimating and predicting reliability, and the method based on Bayesian framework is a popular one. However, the present Bayesian approaches are all based on the impractical assumption that the detected faults are corrected immediately with no debugging time delay. In this paper, we derive effective parameter estimation algorithms based on Bayesian framework not only for fault detection process, but also for combined detection and correction processes. To have a better understanding of the estimation performance, a simulation study and a practical example are conducted to investigate the performance of the proposed Bayesian approach.", "num_citations": "6\n", "authors": ["445"]}
{"title": "Modularity's impact on the quality and productivity of embedded software development: a case study in a Hong Kong company\n", "abstract": " Product modularity is starting to be considered an alternative strategic approach to enhancing new product development performance. However, modularity is still new. There is serious conclusive and methodological inconsistency. Experts are still calling for more solid empirical studies with objective data on the effects of product modularity on new product performance. This paper reports a real-life study of the impact of modularity on quality, productivity, cost and time of embedded software development in a small software company. The study is based on first-hand objective data from 30 projects in the company. This study reveals that a higher modular reuse rate enhances productivity and quality and reduces cost of embedded software development. However, it does not shorten the time to market. The results are explained and practical implications for R&D managers discussed based on an interview. Limitations\u00a0\u2026", "num_citations": "6\n", "authors": ["445"]}
{"title": "An empirical study of dynamic incomplete-case nearest neighbor imputation in software quality data\n", "abstract": " Software quality prediction is an important yet difficult problem in software project development and management. Historical datasets can be used to build models for software quality prediction. However, the missing data significantly affects the prediction ability of models in knowledge discovery. Instead of ignoring missing observations, we investigate and improve incomplete-case k-nearest neighbor based imputation. K-nearest neighbor imputation is widely applied but has rarely been improved to have the most appropriate parameter settings for each imputation. This work conducts imputation on four well-known software quality datasets to discover the impact of the new imputation method we proposed. We compare it with mean imputation and other commonly used versions of k-nearest neighbor imputation. The empirical results show that the proposed dynamic incomplete-case nearest neighbor imputation\u00a0\u2026", "num_citations": "6\n", "authors": ["445"]}
{"title": "A study of process monitoring based on inverse Gaussian distribution\n", "abstract": " The inverse Gaussian distribution has considerable applications in describing product life, employee service times, and so on. In this paper, the average run length (ARL) unbiased control charts, which monitor the shape and location parameters of the inverse Gaussian distribution respectively, are proposed when the in-control parameters are known. The effects of parameter estimation on the performance of the proposed control charts are also studied. An ARL-unbiased control chart for the shape parameter with the desired ARL 0, which takes the variability of the parameter estimate into account, is further developed. The performance of the proposed control charts is investigated in terms of the ARL and standard deviation of the run length. Finally, an example is used to illustrate the proposed control charts.", "num_citations": "6\n", "authors": ["445"]}
{"title": "Bi-objective burn-in modeling and optimization\n", "abstract": " This study develops a bi-objective method for burn-in decision makings with a view to achieving an optimal trade-off between the cost and the performance measures. Under the proposed method, a manufacturer specifies the relative importance between the cost and the performance measures. Then a single-objective optimal solution can be obtained through optimizing the weighted combination of these two measures. Based on this method, we build a specific model when the performance objective is the survival probability given a mission time. We prove that the optimal burn-in duration is decreasing in the weight assigned to the normalized cost. Then, we develop an algorithm to populate the Pareto frontier in case the manufacturer has no idea about the relative weight.", "num_citations": "6\n", "authors": ["445"]}
{"title": "Nonparametric estimation of decreasing mean residual life with type II censored data\n", "abstract": " In this paper, a nonparametric method is proposed for the estimation of decreasing mean residual life with type II censored data. This method is based on the comparison between two estimators of the reliability function: the Kaplan-Meier estimator, and an estimator derived from the empirical MRL function. Simulation results indicate that the new approach is able to give good performance, and can outperform some existing parametric methods when censoring is heavy.", "num_citations": "6\n", "authors": ["445"]}
{"title": "On cumulative conforming type of control charts for high quality processes under sampling inspection\n", "abstract": " Control charts for high quality processes based on counting cumulative conforming items have attracted a lot of attention recently. However, such charting procedures usually apply when items are inspected one by one. There are situations that products are inspected sample by sample for practical or economic reasons. In such situations, the application of conventional cumulative conforming type of control charts based on the assumption of geometric or negative binomial distribution is challenged. To solve the related problem, in this paper we propose a new cumulative conforming type of control chart, called CCS (cumulative conforming samples) chart. The ANI (average number of items inspected) performance of CCS chart is examined. The advantage of CCS chart is discussed. Furthermore, the issue of correlation within samples is also addressed. The effect of correlation on the performance of CCS chart is\u00a0\u2026", "num_citations": "6\n", "authors": ["445"]}
{"title": "Sensitivity analysis in optimal software release time problems\n", "abstract": " In the modeling of software reliability and in the decision-making using software reliability models, there are a number of parameters whose values are either assumed or estimated based on early data. These parameters are usually inaccurate and thus it is important to study the sensitivity of the estimation. In this paper, the parameter sensitivity problem is studied from two different points of view. First, in addition to point estimation that is commonly used in data analysis, interval estimation is recommended to quantify the errors in the estimated parameters. Second, with the example of sensitivity analysis for the release time determination problem that usually involves both software reliability model parameters and software cost model parameters, a statistical design of experiments technique is employed. This method can not only estimate the effect of separate change of each model parameter, but also\u00a0\u2026", "num_citations": "6\n", "authors": ["445"]}
{"title": "Concepts and applications of stochastic aging in reliability\n", "abstract": " 9.2 Basic Concepts for Univariate Reliability Classes 9.2. 1 Some Acronyms and the Notions of Aging 9.2. 2 Definitions of Reliability Classes", "num_citations": "6\n", "authors": ["445"]}
{"title": "Design of experiments considering multiple engineering characteristics\n", "abstract": " In an industrial process, other than optimizing the mean of the response, the engineer is often also interested in minimizing the variance of the response. The problem is augmented when it is required to simultaneously optimize more than one response. This paper first highlights some technical and managerial issues faced in the multivariate problem, and briefly reviews the desirability and expected loss approaches. A simple and intuitive approach, which utilizes the criterion of nondominated designs, is then proposed and illustrated with an example. To offer some guidance to the engineer tasked with optimizing a multi-characteristic problem, a useful comparative summary of the discussed approaches is also presented.", "num_citations": "6\n", "authors": ["445"]}
{"title": "A mixed Poisson model and its application to attribute testing data\n", "abstract": " A two-component mixed Poisson distribution useful in modeling and analyzing of certain quality and reliability data is studied in this paper. Its statistical properties, physical interpretations, and some practical issues are discussed. The model generalizes a previously proposed model which describes a perfect process subject to random shocks. The approach of using the model and its application are highlighted using some actual data.", "num_citations": "6\n", "authors": ["445"]}
{"title": "On the increase of the expected system yield due to component improvement\n", "abstract": " By improving a system component, the system yield or gain may be increased. However, an improvement action is worth taking only if the increase of the expected system yield is greater than the cost associated with it. For different improvement actions, the amount of the expected increase will also be different. In this paper we study some common improvement actions at component level and the effect on the system performance. Comparison of the expected system yield due to the improvements will be made.", "num_citations": "6\n", "authors": ["445"]}
{"title": "Testing constant failure rate against some partially monotone alternatives\n", "abstract": " In many practical situations, it is necessary to know whether the failure rate of a unit is partially monotone, e. g. monotone under a part of its life time. Some interesting and useful classes of life distributions with partially monotone failure rate is defined. The total time on test (TTT) concept which is useful for model identification and for tests against aging can also be used for testing exponentiality against these partially monotone alternatives. Some numerical results will also be given.", "num_citations": "6\n", "authors": ["445"]}
{"title": "A study of the extraction of bug judgment and correction times from open source software bug logs\n", "abstract": " Cost and time effective solution for software reliability analysis is becoming the biggest concern for open source software development. The open source software development process is a bug driven development and its life cycle cost is mainly incurred in bug correction process. Bug reports, with its various features, play an important role in improving the quality of software products. Unfortunately, less attention is paid towards extraction of important attributes from bug logs. This paper is focused on the comment feature of bug repositories to extract bug judgment and correction times for the open source software reliability model building. A bug report life cycle model is presented and the method to compute the two time points is proposed. The study used datasets from ten official releases of Apache 2.0 to draw empirical results. The proposition is expected to assist in open source software reliability model building.", "num_citations": "5\n", "authors": ["445"]}
{"title": "Simplified likelihood based goodness-of-fit tests for the Weibull distribution\n", "abstract": " The aim of this paper is to present new likelihood based goodness-of-fit tests for the two-parameter Weibull distribution. These tests consist in nesting the Weibull distribution in three-parameter generalized Weibull families and testing the value of the third parameter by using the Wald, score, and likelihood ratio procedures. We simplify the usual likelihood based tests by getting rid of the nuisance parameters, using three estimation methods. The proposed tests are not asymptotic. A comprehensive comparison study is presented. Among a large range of possible GOF tests, the best ones are identified. The results depend strongly on the shape of the underlying hazard rate.", "num_citations": "5\n", "authors": ["445"]}
{"title": "A shrinkage approach for failure rate estimation of rare events\n", "abstract": " Systems have become more and more reliable due to technological advancement. For highly reliable systems, there are usually very few or even no failures during the testing and operation. On the other hand, given a short operating or testing time, the failure of the system is also rare even the failure rate is relatively high. The classical maximum likelihood estimation approach results in degenerated estimates zero when no failure occurs and hence meaningless. To overcome this problem, we investigate a shrinkage approach for the estimation of the failure rate of rare events from multiple heterogeneous systems provided that some of them have failures. The shrinkage estimator shrinks the MLE toward a predetermined data\u2010dependent point in the parameter space and could be expressed as a weighted average of MLE and the data\u2010dependent point. Examples are shown how the procedure can be implemented\u00a0\u2026", "num_citations": "5\n", "authors": ["445"]}
{"title": "Combinatorial competing failure analysis considering random propagation time\n", "abstract": " Existing reliability models of complex systems have mostly assumed instantaneous failure propagation with a global effect leading to conservative analysis results on system reliability. This paper proposes a new reliability model considering competing failure isolation and failure propagation with random propagation time. The assessment method utilizes the total probability theorem and binary decision diagrams, providing a practical procedure for the reliability analysis of complex systems subject to competing failures and random propagation time. Results can be used in arranging maintenance activities and optimizing resource allocation. The proposed model is validated using a case study.", "num_citations": "5\n", "authors": ["445"]}
{"title": "Analysis of the performance of safety-critical systems with diagnosis and periodic inspection\n", "abstract": " This paper presents a method for analysis of performance indexes of safety-critical systems. It incorporates periodic inspection and repair which occurs just after each time interval into Markov model. This modeling technique is applied to the typical system structures regulated in the standard IEC 61508. Both perfect and imperfect inspections and repairs can be modeled. Through derivation, a variety of important system performance indexes can be obtained in closed form, that include MTTF, MTTFD, MTTFS, average availability, average probability of failure-dangerous, and average probability of failure on demand. The solutions are applied to 1-out-of-2 system structure to illustrate the usefulness of this method in analyzing the system performance, for example, choice of proof-test interval and evaluation on the average probability of failure on demand.", "num_citations": "5\n", "authors": ["445"]}
{"title": "Monitoring inter-arrival times with statistical control charts\n", "abstract": " All processes suffer from two kinds of variations: chance causes and assignable causes. Chance causes are the causes that are inherently present in the process and thus have to be accepted. On the other hand assignable causes, as the name suggests are induced by the system, ie, man, machine, material, etc. The main objective of the control chart is to detect the presence of assignable causes and to inform the user by raising an alarm. Usually the control chart has three lines, referred to as the upper control limit, the lower control limit, and the central line. The chart plots the sample statistic of the quality characteristic, which is to be monitored. The presence of some unusual sources of variation results in a point plotting above or below the upper control limit or the lower control limit, respectively. This warrants investigation and removal of such sources to bring the process back to its original state or if possible to improve it. Failure process monitoring is an important issue for complex or repairable systems. It is also a common problem for a fleet of systems, such as equipment or vehicles of the same type in a company.", "num_citations": "5\n", "authors": ["445"]}
{"title": "An investigation of transformation-based prediction interval for the Weibull median life\n", "abstract": " Statistical inference based on the Weibull distribution, a distribution widely used in reliability and survival analysis, is usually difficult as it often involves numerical computation and approximation. However, this distribution can be transformed to near-normality by a simple power transformation. Based on this transformation, a prediction interval (PI) for its median can be easily constructed through an inverse transformation. The procedure for selecting the best power transformation through minimizing Kullback-Leibler information is described. The property of this transformation-based PI is investigated. Simple correction factors are also proposed. It is shown that the transformation-based PI with corrections performs well, irrespective of the sample size and parameter values. Simulation results show that the new PI generally outperforms the existing PI. Numerical examples are given for illustration.", "num_citations": "5\n", "authors": ["445"]}
{"title": "Reliability by design a tool to reduce time-to-market\n", "abstract": " Reliability of products plays a crucial role in retaining brand loyalty. The conventional approach to reliability analysis resorts to testing the prototype and entails long development time. This is undesirable for electronic products which have very short life cycle. Reliability by design using the stressor-susceptibility interaction model provides a way to address this dilemma. Management no longer have to forgo reliability analysis in the race to shorten time-to-market.", "num_citations": "5\n", "authors": ["445"]}
{"title": "Dynamic event-triggered L\u221e control for networked control systems under deception attacks: a switching method\n", "abstract": " Based on the event-triggered scheme (ETS), the L\u221e control problem is considered for networked control systems subject to stochastic deception attacks. A novel dynamic switching ETS is proposed to reduce the number of transmitted signals. A stochastic model is used for deception attacks, where the system states are corrupted by attackers. Under this framework, the system to be investigated is modeled as a new switched system. By using the constructed Lyapunov function, sufficient criteria are derived to guarantee the exponential mean-square stability and L\u221e performance. Subsequently, the corresponding controller is designed. Finally, the effectiveness of the dynamic ETS is illustrated by using an unmanned aerial vehicle system.", "num_citations": "4\n", "authors": ["445"]}
{"title": "Dynamic random testing with test case clustering and distance-based parameter adjustment\n", "abstract": " ContextSoftware testing is essential in software engineering to improve software reliability. One goal of software testing strategies is to detect faults faster. Dynamic Random Testing (DRT) strategy uses the testing results to guide the selection of test cases, which has shown to be effective in the fault detection process.ObjectivePrevious studies have demonstrated that DRT is greatly affected by the test case classification and the process of adjusting the testing profile. In this paper, we propose Distance-based DRT (D-DRT) strategies, aiming at enhancing the fault detection effectiveness of DRT.MethodD-DRT strategies utilize distance information of inputs into the test case classification and the testing profile adjustment process. The test cases are vectorized based on the input parameters and classified into disjoint subdomains through certain clustering methods. And the distance information of subdomains, along\u00a0\u2026", "num_citations": "4\n", "authors": ["445"]}
{"title": "Resilience dynamics modeling and control for a reconfigurable electronic assembly line under spatio-temporal disruptions\n", "abstract": " This paper proposes a resilience dynamics modeling and control approach for a reconfigurable electronic assembly line under disruptions. A Digital Twin (DT) platform is developed as the basis for resilience analysis, and open reconfigurable architectures (ORAs) are introduced to support reconfiguration of the assembly line. The time-delays of disruptions are identified and used to characterize their spatio-temporal attributes. A systematic method based on max-plus algebra is proposed to model resilience dynamics under disruptions. The resilience control policy used in the DT platform is developed to minimize production losses, and it is tested on a smart phone assembly line, with its effectiveness validated by comparative analysis.", "num_citations": "4\n", "authors": ["445"]}
{"title": "A switching method to event-triggered output feedback control for unmanned aerial vehicles over cognitive radio networks\n", "abstract": " This article investigates the event-triggered output feedback control problem for unmanned aerial vehicle (UAV) systems over cognitive radio (CR) networks. A periodic event-triggered scheme is proposed in the presence of CR networks. By modeling the CR network as an scon-off switch, a new switched time-delay system model is developed for the event-triggered UAV. Based on the new model, the exponential stability and H\u221e performance criteria are derived by using the constructed Lyapunov function. Then, a co-design method is proposed to obtain mode-dependent controller gains and trigger parameters simultaneously. Finally, the proposed scheme is verified by a UAV system.", "num_citations": "4\n", "authors": ["445"]}
{"title": "Dynamic random testing strategy for test case optimization in cloud environment\n", "abstract": " Dynamic Random Testing (DRT) strategy employs feedback mechanism to guide the selection of test cases, which has shown to be effective in fault detection process. Cloud testing is the combination of cloud computing and software testing, in which the parallel mechanism is introduced to handle multiple test tasks simultaneously. The efficiency of DRT can be improved by combining it into cloud environment. However, it faces challenges in cloud testing as its test cases are selected sequentially, which does not consist with the characteristic of parallelism underlying cloud testing. In this paper, we present a cloud-based DRT strategy to adapt DRT in cloud testing, in which both the test case prioritization and resource allocation are considered. The results of the experiments show that the cloud-based DRT can improve the efficiency of original DRT and provide stable fault detection performance enhancement over\u00a0\u2026", "num_citations": "4\n", "authors": ["445"]}
{"title": "A reliability assessment approach for systems with heterogeneous component information\n", "abstract": " Reliability assessment of complex systems is an important yet difficult task. The difficulty arises largely because of heterogeneous component-level data, for example, lifetime data, degradation data, component-level assessment results, and prior information. A method for reliability assessment is developed in this article for systems plagued by degradation and lifetime data. Our framework divides component-level information into two types. These two types are then combined using a systematic approach. This article describes the method along with some application examples that demonstrate the approach capable of overcoming several difficulties associated with conventional reliability assessment approaches.", "num_citations": "4\n", "authors": ["445"]}
{"title": "On the mean residual life of a generalized k-out-of-n system\n", "abstract": " A generalized k-out-of-n system consists of N modules in which the i th module is composed of ni components in parallel. The system failswhen at least f components in the whole system or at least k consecutive modules have failed. In this article, we obtain the mean residual life function of such a generalized k-out-of-n system under different conditions, namely, when the number of components in each module is equal or unequal and when the components of the system are independent or exchangeable.", "num_citations": "4\n", "authors": ["445"]}
{"title": "Adjusted clustering Clarke-Wright Saving Algorithm for two depots-N vehicles\n", "abstract": " In this paper we modeled the distribution of a single type of products, which are distributed from two depots and use N-vehicles. This problem can be modeled using Capacitated Vehicle Routing Problems (CVRP), and the common algorithm to solve that model is Clarke and Wright Saving Algorithm (CWSA). The needed computational time for finding the nearly global optimum of CWSA grows exponentially with the numbers of the existed routes. Therefore, in this paper, we proposed to combine the clustering algorithm with CWSA. Additionally, we consider the largest item in the cluster, which has to be transported, as the starting point to find the optimum solution.", "num_citations": "4\n", "authors": ["445"]}
{"title": "Lower confidence limit for reliability based on grouped data using a quantile-filling algorithm\n", "abstract": " The aim of this paper is to propose an approach to constructing lower confidence limits for a reliability function and investigate the effect of a sampling scheme on the performance of the proposed approach. This is accomplished by using a data-completion algorithm and certain Monte Carlo methods. The data-completion algorithm fills in censored observations with pseudo-complete data while the Monte Carlo methods simulate observations for complicated pivotal quantities. The Birnbaum\u2013Saunders distribution, the lognormal distribution and the Weibull distribution are employed for illustrative purpose. The results of three cases of data-analysis are presented to validate the applicability and effectiveness of the proposed methods. The first case is illustrated through simulated data, and the last two cases are illustrated through two real-data sets.", "num_citations": "4\n", "authors": ["445"]}
{"title": "Weibull distribution\n", "abstract": " In probability theory and statistics, the Weibull distribution is a continuous probability distribution named after Waloddi Weibull who described it in detail in 1951, although it was first identified by Fr\u00e9chet (1927) and first applied by Rosin and Rammler (1933) to describe the size distribution of particles.", "num_citations": "4\n", "authors": ["445"]}
{"title": "Reliability of systems subjected to imperfect fault coverage\n", "abstract": " Due to imperfect fault coverage, the reliability of redundant systems cannot be enhanced unlimitedly with the increase of redundancy. Many works have been done on the reliability modeling and optimization of systems subjected to imperfect fault coverage. The methodologies adopted mainly include combinatorial approach, ordered binary decision diagram and universal generating function. Depending on the type of fault tolerant techniques used, there are mainly three kinds of fault coverage models: (1) element level coverage (ELC). (2) fault level coverage (FLC). and (3) performance-dependent coverage (PDC). This chapter reviews the literatures on the reliability of systems subjected to imperfect fault coverage and shows an extended\u00a0work.", "num_citations": "4\n", "authors": ["445"]}
{"title": "Reliability of Smart Grid Systems with Warm Standby Spares and Imperfect Coverage.\n", "abstract": " This paper models the reliability of a smart grid system with warm standby spares and imperfect fault coverage based on binary decision diagrams (BDD). In order to meet stringent reliability requirement, it is essential for a smart grid system to be designed with fault tolerance. The Warm standby SParing (WSP) is an important fault tolerance technique which compromises the energy consumption and the recovery time. For WSP, the standby units have different failure rates before and after they are used to replace the on-line faulty units. Furthermore a component failure may propagate through the grid and cause the whole system to fail if the failure is uncovered. Existing works on systems with warm standby spares and imperfect fault coverage are restricted to some special cases, such as assuming exponential failure time distribution for all components or only considering one spare. The BDD approach proposed in this paper can overcome the limitations of the existing approaches. Examples are shown to illustrate the application.", "num_citations": "4\n", "authors": ["445"]}
{"title": "A study of safety and reliability of maritime transportation systems\n", "abstract": " Safety is the most important concerns with transportation systems. To provide high quality service, reliability is also essential. Transportation system is a very complex system, with a large number of components that they are built of, and their complicated operating processes. This complexity often poses challenges for evaluation of system reliability, availability and safety. This paper reports some of the research carried out in the past few years with the focus on safety and reliability modeling and analysis of maritime transportation systems. This is a joint collaboration project supported by Singapore\u2019s Agency for Science and Technology and Poland\u2019s Ministry of Science and Technology.", "num_citations": "4\n", "authors": ["445"]}
{"title": "A new method for maritime traffic safety index appraisal\n", "abstract": " In maritime transportation, safety during vessel's navigation at sea depends on technical and human components of the ship-system and the marine environment. The danger to the ship, on board crew and the environment continually change during the entire ship voyage due to numerous unpredictable factors. This paper proposes a new method to assess the maritime traffic safety index of the marine vessel during its journey at sea. The technology recognized as Automatic Identification System (AIS), made a compulsory fitting by the International Maritime Organization (IMO), for ships engaged in international voyages, is used to gather the information needed. This information is then transformed to obtain the various static and dynamic factor values affecting the maritime traffic safety index. Initially, identified factors are grouped under higher level factors to develop a hierarchical structure. Analytic hierarchy process\u00a0\u2026", "num_citations": "4\n", "authors": ["445"]}
{"title": "Using evidential reasoning approach for ship turbine's condition monitoring techniques ranking\n", "abstract": " Ranking and selection of optimal alternative among a range of alternatives have always been a multifaceted task in real-world complex decision analysis problems due to the qualitative nature of the decision criteria's and also information uncertainties like incomplete i.e. missing or imprecise data. This makes ranking process a difficult task in all multi criteria decision making (MCDM) problems. Evidential reasoning approach advocates a general multi level evaluation process for dealing with MCDM problems. This paper reports the application of evidential reasoning approach to ranking and optimal selection among three techniques utilized for monitoring the condition of a turbine on a transportation ship. The problem was attempted earlier in the literature by different fuzzy MCDM methods. However, the assessment of incomplete information was not at all accounted earlier, which is incorporated in this paper. This\u00a0\u2026", "num_citations": "4\n", "authors": ["445"]}
{"title": "Software Reliability Modeling and Analysis\n", "abstract": " Computer systems can be found almost everywhere in our daily life and they are also extensively used in areas such as defense and finance. Unlike hardware reliability which is a well\u2010developed area, software reliability is relatively new and is generally more difficult to insure. In order to improve the reliability of a software product, the software has to be tested extensively prior to its release. Software reliability models are commonly used to monitor the testing process and to measure and predict future reliability of the software. Such study is also useful to determine the release time of the software product. During the past three decades there have been many software reliability models proposed and studied. This chapter reviews some of the models and discusses the applications of the models for decision making in software development.", "num_citations": "4\n", "authors": ["445"]}
{"title": "Detection and correction process modeling considering the time dependency\n", "abstract": " Most of the models for software reliability analysis are based on reliability growth models which deal with the fault detection process only. In this paper, some useful approaches to the modeling of both software fault detection and fault correction processes are discussed. Since the estimation of model parameters in software testing is essential to give accurate prediction and help make the right decision about software release, the problem of estimating the parameters is addressed. Taking into account the dependency between the fault correction process and the fault detection process, a new explicit formula for the likelihood function is derived and the maximum likelihood estimates are obtained under various time delay assumptions. An actual set of data from a software development project is used as an illustrative example. A Monte Carlo simulation is carried out to compare the predictive capability between the\u00a0\u2026", "num_citations": "4\n", "authors": ["445"]}
{"title": "Mean Residual Life\u2014Concepts and Applications in Reliability Analysis\n", "abstract": " The mean residual lifetime (MRL) has been studied by reliabilists, statisticians, survival analysts and others. Many useful results have been derived concerning it. Given a component or a system is of age t, the remaining lifetime after t is random. The expected value of this random residual lifetime is called the mean residual life or mean remaining life. The MRL is often an important criterion for finding an optimal burn-in time for an item. A review of its theory and applications was given by Guess and Proschan (1988). Over the last two decades, many papers have been written on this subject and thus an update has become necessary. Let F be the survival function of an item with a finite first moment \u00b5 and X be the random variable that corresponds to F assuming F (0)= 0. The residual life random variable at age t, denoted by Xt= X\u2212 t| X> t, is simply the remaining lifetime beyond that age. The mean residual life (also\u00a0\u2026", "num_citations": "4\n", "authors": ["445"]}
{"title": "Reliability of Grid Computing Systems\n", "abstract": " Grid computing is a recently developed technique for complex systems with large-scale resource sharing, wide-area program communicating, and multi-institutional organization collaborating etc. Many experts believe that the grid technologies will offer a second chance to fulfill the promises of the Internet (Forster et al., 2002). However, it is difficult to analyze the grid reliability due to its highly heterogeneous and wide-area distributed characteristics.This chapter first presents a brief discussion of the Grid computing system. A general grid reliability model is then constructed. We also present approaches to compute the grid reliability by incorporating various aspects of the grid structure including the resource management system, the network and the integrated software/resources.", "num_citations": "4\n", "authors": ["445"]}
{"title": "On program and file assignment for distributed systems\n", "abstract": " For distributed computing system, the system reliability which is an important quality and performance metric, depends mainly on the allocation of various resources such as the assignment of executed programs and stored files. This paper first develops a general reliability based optimization model followed by an exhaustive search algorithm and a genetic algorithm to solve the model. The steps of the algorithm are described and numerical examples are used to illustrate the advantages and disadvantages of the two algorithms. The genetic algorithm can generally achieve near-optimal results within a limited amount of computational time, and hence recommended for large distributed systems.", "num_citations": "4\n", "authors": ["445"]}
{"title": "Retrospective factorial fitting and reverse design of experiments\n", "abstract": " In the traditional use of design of experiments (DOE), an experiment is carried out where the input factors are varied according to a systematic plan. The data are then analysed in the usual way, but not before all of the required response values are available. A procedure called 'Reverse DOE' is presented for retrospectively selecting and fitting a fractional factorial design in situations where observed data are already available. This takes into account a set of factor effects that need to be estimated and also handles the possibility that certain treatment conditions may not be available or observed. A worked example from the steel industry is presented to illustrate the procedure. The procedure is expected to be useful in the early stages of experimentation and discovery, especially for factor screening purposes.", "num_citations": "4\n", "authors": ["445"]}
{"title": "Control charts based on generalized Poisson model for count data\n", "abstract": " Poisson distribution is widely used to fit attribute data in industry. However, this distribution cannot provide a good fit for defect count data in a high-quality manufacturing environment when over-dispersion, which is common, occurs. In this paper, the generalized Poisson distribution is studied as an alternative model. The generalized Poisson distribution is flexible and able to deal with over-dispersion data. In particular, the interpretation of the parameters is discussed and statistical monitoring procedures for count data that can be modeled by generalized Poisson distribution are studied. Based on the generalized Poisson model, two different procedures are introduced for effective monitoring of this type of processes. Sensitivity analyses of the two monitoring procedures are also presented. To illustrate the use of generalized Poisson model and validate the model, three statistical tests of Poisson distribution against\u00a0\u2026", "num_citations": "4\n", "authors": ["445"]}
{"title": "Reliability growth model for object oriented software system\n", "abstract": " Object oriented design techniques are the most powerful way of developing software in the foreseen future. The techniques help in reducing the cost of development and increase the reliability of the software. We develop a software reliability growth model for an object oriented software system. The model is validated by simulating the error data. The goodness of fit and predictive validity is also tested.", "num_citations": "4\n", "authors": ["445"]}
{"title": "Some new aspects of stress-strength modelling\n", "abstract": " In industrial development it is essential to improve product reliability. In this paper some new methods to model reliability improvement and reliability growth process are suggested. Stress-strength modelling is used in considering some interesting issues of an improvement process resulting in reliability growth. The reliability is the probability that the strength of the product is greater than the working as well as environmental stress. Any improvement of product reliability at the production stage may be considered as the increase of the strength distribution using artificial heredity, as Dr van Otterloo suggested in a recent editorial. The increase in strength may usually be modelled by some changes to the parameters of the strength distribution. Generally the mean strength may be increased to get greater reliability. Also in many practical situations an improvement may be achieved by reducing the strength variation; this\u00a0\u2026", "num_citations": "4\n", "authors": ["445"]}
{"title": "Modeling multivariate degradation processes with time\u2010variant covariates and imperfect maintenance effects\n", "abstract": " This article proposes two types of degradation models that are suitable for describing multivariate degrading systems subject to time\u2010variant covariates and imperfect maintenance activities. A multivariate Wiener process is constructed as a baseline model, on top of which two types of models are developed to meaningfully characterize the time\u2010variant covariates and imperfect maintenance effects. The underlying difference between the two models lies in the way of capturing the influences of covariates and maintenance: The first model reflects these impacts in the degradation rates/paths directly, whereas the second one describes the impacts by modifying the time scales governing the degradation processes. In each model, two particular imperfect maintenance models are presented, which differ in the extent of reduction in degradation level or virtual age. The two degradation models are then compared in certain\u00a0\u2026", "num_citations": "3\n", "authors": ["445"]}
{"title": "Optimal cleaning scheduling for photovoltaic Systems in the field based on electricity generation and dust deposition forecasting\n", "abstract": " This article focuses on cleaning scheduling for photovoltaic (PV) systems in the field. A method to design cleaning schedules for PV strings is proposed to optimize the profit of a PV system over an extended period. Previous studies have usually focused solely on the cleaning frequency on experimental systems. Our method designs specific cleaning schedules for PV systems in the field based on the forecasting of environmental conditions, PV power generation, and dust deposition. We propose that the cleaning decision is an infinite scheduling problem. A problem model in the infinite time horizon is developed and a periodic scheduling method based on forecasting is proposed to transform the infinite process to an optimizable problem. The cleaning schedules are amended with time as the forecasting accuracy for future times increases with time. The performance of our proposed method that operates under\u00a0\u2026", "num_citations": "3\n", "authors": ["445"]}
{"title": "Cost-Efficient Service Assurance with Joint Orchestration in NFV Networks\n", "abstract": " Network Function Virtualization (NFV), as an enabler for 5G and network slicing, introduces new abstraction levels and orchestration to automate the network operations. However, such features make it challenging to assure the performance of network services provisioned with Virtual Network Functions (VNFs) timely and accurately. In this paper, we propose a joint service assurance and orchestration approach to assure the End-to-End (E2E) service in a closed loop. The joint approach integrates performance monitoring, Load Balancing (LB), data analytics, and NFV orchestration. To minimize monitoring cost, a selection discipline is designed to monitor only a subset of E2E flows. Based on a Minimum Deviation (MD) metric, the measured E2E delays are examined to detect Service Level Agreement (SLA) violation and identify the problematic VNF instance(s). The MD metrics are also used in LB that continuously\u00a0\u2026", "num_citations": "3\n", "authors": ["445"]}
{"title": "Network slicing architecture and dependability\n", "abstract": " The next generation of cellular networks known as 5G promises to be a major step in the evolution of communications technology, due to its enhanced technical features and because it is planned for a much wider set of applications and scenarios. Network slicing provides the scalability and flexibility needed to support this vision, by enabling the provision of independent and isolated network segments tailored to specific uses and requirements. The aim of this paper is to present the current status of the slicing architecture and based on that, define policies that assure its dependability. Guaranteeing dependability in network slicing is a top priority since 5G networks will be a critical infrastructure in industry sectors such as energy, health, transport and traditional telecom itself. Moreover, some slices are expected to deliver Ultra Reliable Communication (URC) with reliability requirements above 99.999%. A\u00a0\u2026", "num_citations": "3\n", "authors": ["445"]}
{"title": "Dynamic imperfect condition-based maintenance for systems subject to nonlinear degradation paths\n", "abstract": " This paper presents a maintenance optimization framework for systems suffering from nonlinear continuous degradation. The inspection interval is dynamically determined by the historical system conditions. We model the degradation path as a nonlinear Wiener process with time-varying drift parameter. Techniques to predict remaining useful life (RUL) is utilized to optimize maintenance policy by minimizing the expected cost rate. The effect of imperfect maintenance is assumed to be random in the sense that the maintenance action reduces the systems degradation level by a random proportion described by a beta distribution. Two thresholds on the degradation are determined for the preventive imperfect maintenance and perfect replacement, respectively. We evaluate expected cost rate using Monte Carlo simulation. A dataset from the real-world example is used to provide the pilot parameters as input for the optimization maintenance policies. Afterward, numerical examples are presented to illustrate the proposed method.", "num_citations": "3\n", "authors": ["445"]}
{"title": "An integrated method for estimation with superimposed failure data\n", "abstract": " In this paper, a new integrated algorithm is proposed to estimate the superposed renewal processes (SRP) by considering both failure data configurations for the first few observed failures and inter-failure time asymptotic distribution for rest observed failures. Simulation studies are provided to show that the proposed method performs well. Also, we derive the log-likelihood function of this integrated method for censored data and present some simulation studies to verify the approach.", "num_citations": "3\n", "authors": ["445"]}
{"title": "An infrared spectroscopic study on proton transfer from CH bonds in ionized dimers of cyclic ethers\n", "abstract": " Infrared (IR) spectroscopy of neutral and cationic dimers of tetrahydrofuran (THF) and tetrahydropyran (THP) is carried out to investigate isomerization reactions of these dimers following ionization. In the neutral state, both the THF and THP dimers tend to form the stacking structures bound essentially by dispersion. After ionization, both of these dimer cations form the proton transferred structures, in which the proton of the C\u03b1H bond is intermolecularly transferred and shared between the oxygen atoms. Along with the pseudorotation of the five-member ring, the barrierless proton transfer from CH occurs in the ionized THF dimer, and the dimer finally forms the proton transferred structure. In the case of the THP dimer cation, the barrierless intracluster proton transfer occurs with the conformational change of the proton donor site because the acidic CH site of the THP cation strongly depends on its conformation. These\u00a0\u2026", "num_citations": "3\n", "authors": ["445"]}
{"title": "An integrated experimental and theoretical reaction path search: analyses of the multistage reaction of an ionized diethylether dimer involving isomerization, proton transfer\u00a0\u2026\n", "abstract": " An ionization-induced multistage reaction of an ionized diethylether (DEE) dimer involving isomerization, proton transfer, and dissociation is investigated by combining infrared (IR) spectroscopy, tandem mass spectrometry, and a theoretical reaction path search. The vertically-ionized DEE dimer isomerizes to a hydrogen-bonded cluster of protonated DEE and the [DEE\u2212H] radical through barrierless intermolecular proton transfer from the CH bond of the ionized moiety. This isomerization process is confirmed by IR spectroscopy and the theoretical reaction path search. The multiple dissociation pathways following the isomerization are analyzed by tandem mass spectrometry. The isomerized cluster dissociates stepwise into a [protonated DEE\u2013acetaldehyde (AA)] cluster, protonated DEE, and protonated AA. The structure of the fragment ion is also analyzed by IR spectroscopy. The reaction map of the multistage\u00a0\u2026", "num_citations": "3\n", "authors": ["445"]}
{"title": "Vibrational autoionization of state-selective jet-cooled methanethiol (CH 3 SH) investigated with infrared+ vacuum-ultraviolet photoionization\n", "abstract": " Vibrational autoionization of Rydberg states provides key information about nonadiabatic processes above an ionization threshold. We employed time-of-flight mass detection of CH3SH+ to record vibrational-state selective photo-ionization efficiency (PIE) spectra of jet-cooled methanethiol (CH3SH) on exciting CH3SH to a specific vibrationally excited state with an infrared (IR) laser, followed by excitation with a tunable laser in the vacuum-ultraviolet (VUV) region for ionization. Autoionizing Rydberg states assigned to the ns, np, nd and nf series are identified. When IR light at 2601 (\u03bd3, SH stretching mode) and 2948 cm\u22121 (\u03bd2, CH3 symmetric stretching mode) was employed, the Rydberg series converged to the respective vibrationally excited (\u03bd3 and \u03bd2) states of CH3SH+. When IR light at 3014 cm\u22121 (overlapped \u03bd1/\u03bd9, CH3 antisymmetric stretching and CH2 antisymmetric stretching modes) was employed\u00a0\u2026", "num_citations": "3\n", "authors": ["445"]}
{"title": "Generalized sooner waiting time problems in a sequence of trinary trials\n", "abstract": " Let {\u03be n, n\u2265 1} be a sequence of independent trials with three possible outcomes 0, 1, 2 labeled as failure, success of type I and success of type II, respectively. Suppose that at each time a success of type I (type II) occurs in {\u03be n, n\u2265 1} a random reward of type I (type II) is received. We obtain distributions of the number of trials until either the sum of consecutive rewards of type I is equal to or exceeds the level k 1 or the sum of consecutive rewards of type II is equal to or exceeds the level k 2 under two different schemes.", "num_citations": "3\n", "authors": ["445"]}
{"title": "Circle chart for the monitoring of maxima in periodic processes\n", "abstract": " An important process characteristic is process maxima as it is often an important quality or performance parameter. Such measurements often play a vital role in process analysis and have to be monitored carefully. In this paper, we consider the problem of monitoring process maxima when the process exhibits a kind of periodicity. A circle chart for periodical process maxima with or without seasonal patterns monitoring is developed and studied, by modeling the process maxima using the extreme value distribution. Mean normalization is applied for periodic process monitoring with seasonal patterns to facilitate comparison between different sections and periods. The average cycle run length is proposed to measure charting efficiency. Copyright \u00a9 2014 John Wiley & Sons, Ltd.", "num_citations": "3\n", "authors": ["445"]}
{"title": "The robust redundancy allocation problem of series-parallel systems\n", "abstract": " Redundancy allocation is a technique that has been widely used to improve systems reliability. In this paper, the interval analysis is introduced to represent imprecise component reliabilities, and an order relation is applied in the comparison of interval values. We propose a mathematical model to deal with constrained redundancy optimization of series-parallel systems with interval-valued component reliabilities. The objective function is extended to an interval-valued function in the proposed model, and a dynamic penalty genetic algorithm (GA) is developed to search the best solution and the optimum system reliability. Several numerical examples are given to illustrate the proposed method, and the experimental results have shown that the interval mathematics is an efficient tool to solve the redundancy allocation problem (RAP) of systems with interval-valued component reliabilities.", "num_citations": "3\n", "authors": ["445"]}
{"title": "Circle chart for monitoring of periodic measurements\n", "abstract": " Measurements with a periodicity are common in practice but there have been no specific monitoring control techniques for them. In this paper, we propose a type of control chart that plots measurements around a circle so that information from the same stage of different cycles can be readily compared. Some basic properties of such charts are investigated, and further developments are discussed. The basic circle chart can be applied under various kinds of control chart schemes, and there is potential for further development. Some application examples are also shown in this paper. Copyright \u00a9 2012 John Wiley & Sons, Ltd.", "num_citations": "3\n", "authors": ["445"]}
{"title": "False target vs protection in defending parallel systems against unintentional and intentional impacts\n", "abstract": " This article considers a parallel system exposed to external intentional impacts caused by malicious attacks, and unintentional impacts caused by natural disasters or technological accidents. The defender distributes its resource between the deployment of false targets and the protection of genuine system elements. The deployment of false targets is intended to misinform the attacker so that it is not able to distinguish between the false targets and the genuine system elements, which leads to dissipating the attack resources. Different combinations of unintentional and intentional impacts sequences are considered. The vulnerability of each system element is determined by an attacker-defender and unintentional impact-defender contest success functions. A framework of solving the optimal defense resource distribution which minimizes the overall system vulnerability is suggested. Illustrative examples are presented.", "num_citations": "3\n", "authors": ["445"]}
{"title": "Optimal replacement and protection strategy for parallel systems\n", "abstract": " We consider a parallel system consisting of components with different characteristics. The components can be unavailable due to internal failures or external impacts. Each component has an increasing failure rate and is subjected to external impacts that can occur with fixed frequency. The system reliability is defined as the ability to maintain a specified performance level. It is assumed that the component destruction probability depends on the investment made in protection actions. In order to increase the system reliability, two measures can be taken: (1) increase of the components replacement frequency to improve their availability and (2) improvement of the components protection to reduce the probability of their destruction by the external impacts. The optimal maintenance and protection strategy which minimizes the total cost of maintenance, protection and damage caused by unsupplied demand is\u00a0\u2026", "num_citations": "3\n", "authors": ["445"]}
{"title": "Comparative studies of multi-criteria decision making with application to condition monitoring of a ship turbine\n", "abstract": " Problems in multi-criteria decision making have always been of considerable interest to researchers over the decades, with various techniques and methodologies evolving continuously in this area. The use of fuzzy-set-based approaches has been widely documented in problems where the qualitative or subjective nature of the data, with information ambiguity or data imprecision, is inherent. In recent times, the approach of evidential reasoning, based on the Dempster\u2013Shafer theory which considers uncertainty of a similar nature, has emerged as an alternative. As such, questions have arisen on the merits, and hence the accuracy, of these two kinds of method, i.e. the fuzzy-set-based approach and evidential reasoning, when applied to the considered problem. This paper aims to address some of these questions through a comparative study on the use of fuzzy-set-based methods and evidential reasoning\u00a0\u2026", "num_citations": "3\n", "authors": ["445"]}
{"title": "A general formulation of optimal testing-time allocation for modular systems\n", "abstract": " Software testing is very costly in software development and the testing-time during the module testing phase is limited. Thus, the problem of optimal testing-time allocation is of great importance in practice. In this paper, a general formulation of optimal testing-time allocation for modular systems is proposed. The objective is to maximize the reliability of the entire software system. In our formulation, both architecture-based models and non-homogeneous Poisson process (NHPP) models are used. The assumptions are also highlighted to help practitioners better under the limitations that need attention. A numerical example is provided and sensitivity analysis is conducted to detect significant parameters.", "num_citations": "3\n", "authors": ["445"]}
{"title": "A Model for Reliability Analysis of Repairable Transportation Systems\n", "abstract": " Reliability analysis of complex system is a widely discussed topic, especially for complex products or networked systems. Recently, reliability models are developed for transportation systems which is of growing importance in, for example, maritime industry. In this paper, we study the modeling and analysis of transportation systems that repairable in nature. A semi-Markov process is used to model the system at operation and a multi-state system is constructed to model the system reliability. The impact of repair activity on system reliability is also studied. The approach is illustrated for a bulk cargo transportation system at operation used in earlier studies.[PUBLICATION ABSTRACT]", "num_citations": "3\n", "authors": ["445"]}
{"title": "On),(gf-out-of-)),((nji System and its Reliability\n", "abstract": " The existence of multiple failure criteria is a common situation for complex system, especially for consecutive-k type systems. The),( gf-out-of-)),,((nji: F)(gf> system is one such system. It consists of n components ordered in a line or a circle, while the system fails if, and only if, there exist at least f failed components in the system or at least g failed components among components j j ii, 1,, 1,\u2212+ L. Here the component index has modular n property for the circular case, ie, components i and in+ indicate the same one, gij\u2265\u2212, if ij>; g ijn\u2265+\u2212+ 1, if ij<. Throughout the paper, the independence among components is assumed. In this paper, we present the formulae for the system reliability for the),( gf-out-of-Fnji:)),,((systems. It is given in the forms of recursive equation and product of matrices by means of a probability argument and a two-stage finite Markov chain imbedding approach, respectively. The latter method is emphasized in this paper. Fu (1986) first used the finite Markov chain imbedding approach for studying system reliabilities. Since then numerous authors, eg, Fu and Hu (1987), Chao and Fu (1991), Fu and Koutras (1994), Koutras (1996), Chang, Cui & Hwang (1999, 2000), Cui, Kuo & Xie (2001, 2002), among others, have employed this method in the discussion of system reliabilities. The earlier studies in using this method showed that many important reliability systems can be described by some imbedded finite Markov chains. This approach can usually be employed to obtain compact reliability formulas and this approach can also be used for systems with dependent components.", "num_citations": "3\n", "authors": ["445"]}
{"title": "A simple graphical approach for comparing reliability trends of different units in a fleet\n", "abstract": " Consider a fleet of repairable systems made up of different line replacement units (LRU) in which the maintenance and repair policy is based on fail-replace-repair-use cycles for all types of LRU within the fleet of repairable systems. We model the failure process of a system by a piecewise HPP model which accounts for random age mix in the pool of LRU. Using the model, we propose a graphical approach for comparing reliability trends of different units in a fleet of repairable systems and between different fleets with identical sets of such units. The applicability of the proposed approach is demonstrated using simulated data as well as actual field data.", "num_citations": "3\n", "authors": ["445"]}
{"title": "Interval Estimation in Software Reliability Analysis\n", "abstract": " Software reliability is traditionally estimated by analyzing failure data collected during software testing. There are many software reliability models available, but the estimation of model parameters usually requires a large number of failure data which might not be available. Hence the estimated parameters are not accurate and frequent revisions are needed as more failure data become available. In this paper, we study the use of interval estimation in software reliability prediction. For a commonly used software reliability model, we present the interval estimates of the parameters and their uses for a better planning during reliability testing. Con\ufb01dence limits for the reliability and failure intensity function are presented. The results are useful, for example, in the detennination of software release time which is a difficult problem in practice.", "num_citations": "3\n", "authors": ["445"]}
{"title": "Design and analysis of some fault-tolerance configurations based on a multipath principle\n", "abstract": " In this article, some software fault-tolerance configurations based on a multipath principle are studied. System structural reliability is considered from theoretical and practical points of view. The results can be useful in making decisions related to the design and selection of fault-tolerance configurations in practice. The structures studied here are implementable in many real-time and other systems for which both reliability and computational complexity have to be considered. A system designed using the proposed fault-tolerance principle shows that the approach is feasible and has many advantages.", "num_citations": "3\n", "authors": ["445"]}
{"title": "The generalized linear model\u2010based exponentially weighted moving average and cumulative sum charts for the monitoring of high\u2010quality processes\n", "abstract": " In this industry 4.0 revolution, most of the manufacturing processes are equipped with the digital devices which are continuously recording the data. To monitor the quality of a manufacturing system, variable about number of conforming or nonconforming items is usually used and statistical analysis based on it is further utilized for developing the policies. In this era of sophisticated and modern technology, most of the manufacturing systems are producing near zero\u2010defect items. Such processes are known as high\u2010quality processes, and their dataset consists of excess number of zeros. Generally, the zero excess or near zero\u2010defect dataset is well fitted by the zero\u2010inflated distributions, and the zero\u2010inflated Poisson (ZIP) and zero\u2010inflated Negative Binomial (ZINB) distributions are the most common models. Most of the time, in high\u2010quality processes, few covariates are also measured along with defect counts. Hence\u00a0\u2026", "num_citations": "2\n", "authors": ["445"]}
{"title": "Modeling and monitoring unweighted networks with directed interactions\n", "abstract": " Networks have been widely employed to represent interactive relationships among individual units in complex systems such as the Internet of Things. Assignable causes in systems can lead to abrupt increased or decreased frequency of communications within the corresponding network, which allows us to detect such assignable causes by monitoring the communication level of the network. However, existing statistical process control methods for unweighted networks have scarcely incorporated either the network sparsity or the direction of interactions between two network nodes, i.e., dyadic interaction. Regarding this, we establish a matrix-form model to characterize directional dyadic interactions in time-independent unweighted networks. With inactive dyadic interactions excluded, the proposed procedure of parameter estimation achieves higher consistency with less computational cost than its alternative when\u00a0\u2026", "num_citations": "2\n", "authors": ["445"]}
{"title": "Distribution-free hybrid schemes for process surveillance with application in monitoring chlorine content of water\n", "abstract": " Chlorination is one of the well-known and efficient methods for treating water. The amount of chlorine in drinking water must meet a specific quality standard to avoid health hazards. Hence, it is necessary to monitor the amount of chlorine in drinking water. In this article, we introduce two new statistical process monitoring schemes for monitoring a process to ensure it meets a specific quality standard. We show the application of the proposed schemes in monitoring the chlorine content of compliance water samples efficiently. The proposed methods are nonparametric and require little or no knowledge of the statistical distribution of a process. The new schemes rely on the Max and Distance combining functions to capitalise the individual advantages of the distribution-free Lepage and Cucconi statistics for simultaneous monitoring of location and scale parameters. We employ the p-value approach to construct the\u00a0\u2026", "num_citations": "2\n", "authors": ["445"]}
{"title": "Conformational Landscapes and Infrared Spectra of Gas-phase Interstellar Molecular Clusters [(C3H3N)(CH3OH)n, n = 1\u20134]\n", "abstract": " Acrylonitrile (A) is one of the important interstellar molecules, which is considered closely related to the origin of life. And methanol (M) is one of the commonly used solvents, which is also found in outer space. Herein, we obtained the infrared (IR) spectra of size-selected AMn (n = 1\u20134) clusters in supersonic jet by monitoring their fragments of H+AMn\u20131 (n = 1\u20134) with vacuum ultraviolet single-photon soft ionization/IR-depletion technique. IR spectra of AMn (n = 1\u20134) clusters were recorded in the CH and OH vibration bands in the range of 2700\u20133800 cm\u20131. Spectra of AMn (n = 1\u20134) clusters are similar in the CH stretching regions, while those show significant variations in the OH stretching regions with the increase of methanol molecules. Calculated IR spectra, which were predicted with the B3LYP-D3(BJ)/aug-cc-pVDZ method, were employed to compare with the experimental results. For AM, AM2, and AM3, the\u00a0\u2026", "num_citations": "2\n", "authors": ["445"]}
{"title": "A distance-based dynamic random testing with test case clustering\n", "abstract": " One goal of software testing strategies is to detect faults faster. Dynamic Random Testing (DRT) strategy uses the testing results to guide the selection of test cases, which have shown to be effective in the fault detection process. However, the effectiveness of DRT still can be improved. In this paper, a distance-based DRT (D-DRT) strategy is proposed. The vectorized test cases are partitioned with k-means clustering method to obtain better classification, and the distance information are used to guide the test case selection, then the test cases that are close to failure-causing test cases are more likely to be selected, thus the testing process can be optimized. In the case study, the performance of D-DRT and other testing strategies are compared. The experiment results show that the proposed D-DRT strategy has better fault detection effectiveness than the others without significant increase in computational cost.", "num_citations": "2\n", "authors": ["445"]}
{"title": "Reliability assessment for load\u2010sharing systems with exponential components using statistical expansion as a correction\n", "abstract": " Among recent system models, one specific type of system is generally used to model the dependence among components. Components are connected parallel in such systems as they fail one by one and are supposed to share the system work load. The model is thus referred to as the load\u2010sharing system model. Despite the availability of extensive reliability assessment methods for different systems, load\u2010sharing systems have not received enough attention from the scholars who have studied reliability assessment so far. Load\u2010sharing systems are generally designed for high levels of reliability. Therefore, tests for such systems can be expensive and time consuming. Limitation on resources always leads to small test sample sizes. This increases the difficulties associated with obtaining an accurate and robust system reliability assessment result. This paper proposes a novel assessment method for a certain type\u00a0\u2026", "num_citations": "2\n", "authors": ["445"]}
{"title": "Condition-based maintenance for systems with aging and cumulative\n", "abstract": " This chapter develops a condition-based maintenance (CBM) policy for systems subject to aging and cumulative damage. The cumulative damage is modelled by a continuous degradation process. Different from previous studies which assume that the system fails when the degradation level exceeds a specific threshold, this chapter argues that the degradation itself does not directly lead to system failure, but increases the failure risk of the system. The Proportional Hazards Model is employed to characterize the joint effect of aging and cumulative damage. CBM models are developed for two cases: one assumes that the distribution parameters of the degradation process are known in advance, while the other assumes that the parameters are unknown and need to be estimated during system operation. In the first case, an optimal maintenance policy is obtained by minimizing the long-run cost rate. For the case with unknown parameters, period inspection is adopted to monitor the degradation level of the system and update the distribution parameters. A case study of Asphalt Plug Joint in UK bridge system is employed to illustrate the maintenance policy.", "num_citations": "2\n", "authors": ["445"]}
{"title": "A study on the use of discrete event data for prognostics and health management: discovery of association rules\n", "abstract": " This study addresses prognostics and health management (PHM) for manufacturing machines. Different from previous researches where continuous monitoring is assumed for PHM, we investigate the issue with discrete event data. Various event data are recorded during system operation, which can provide useful information for fault diagnosis and failure prediction. We focus on discovery of association rules based on the industrial discrete data. Apriori algorithm is employed to discover the frequent event groups and identify strong association rules. To accommodate the algorithm, the initial event data is transformed into the form of transactional data as a first step. The obtained association rule estimates the occurrence probability of certain significant events within specified time interval. It is concluded through a case study that the number of frequent event groups and strong association rules increases with the\u00a0\u2026", "num_citations": "2\n", "authors": ["445"]}
{"title": "Applications of Software Reliability Models-Possible Problems and Practical Solutions\n", "abstract": " Software reliability analysis has been an active research area since the beginning of the seventies. There are a number of models suggested and studied. However, none of them has been successfully applied. The main reason is that the models are theoretical and in practice the software testing does not coincide with the assumptions used in the models. In this paper we, by applying a general model to real data, give some insights into the practical problems. Some general topics in software reliability modelling will be discussed.ABSTRACT", "num_citations": "2\n", "authors": ["445"]}
{"title": "A cloud-based dynamic random software testing strategy\n", "abstract": " Cloud testing has emerged as a new technology in corporate world and organization, which generates virtual machines for application demands, and employs resource allocation strategies to dispatch the test tasks. Traditional resource allocation strategies usually concern on static assignment, which lacks dynamic adjustment functions. Dynamic Random Testing (DRT) strategy employs feedback mechanism to guide the selection of test cases, which is shown to be effective and has great potential in theoretical as well as practical terms. Therefore, planting the idea of DRT into cloud testing can allocate the test tasks in a dynamic way. In this paper, a cloud-based dynamic random testing strategy is proposed to enhance the testing process and promote the testing efficiency, in which the virtual resource allocation is in accordance with the testing results. A framework of cloud-based DRT strategy is constructed and\u00a0\u2026", "num_citations": "2\n", "authors": ["445"]}
{"title": "Risk analysis of atmospheric and vacuum distillation unit using Bayesian networks\n", "abstract": " The accidents occurred in chemical plants often regard as low frequency and high consequence. It is necessary to raise the risk analysis for the petrochemical system to help people to find the weakest process in the whole system thus people can strength the process to improve the safety. In this paper, a methodology by using Bayesian Networks (BNs) to give a model for a chemical plant has been raised. According to the harm extend, the methodology classifies the events into three layers, cause, incident, and accident. Then the application of the methodology is illustrated by analyzing an atmospheric and vacuum distillation unit. The model identifies the most possible cause when an accident happened. After that, mutual information and variety of beliefs are calculated in order to find the most sensitive event of an accident. The study gives suggestions to people of identification the most relevant and weakest point\u00a0\u2026", "num_citations": "2\n", "authors": ["445"]}
{"title": "Estimation procedures for grouped data\u2013a comparative study\n", "abstract": " Interval-censored data are very common in the reliability and lifetime data analysis. This paper investigates the performance of different estimation procedures for a special type of interval-censored data, i.e. grouped data, from three widely used lifetime distributions. The approaches considered here include the maximum likelihood estimation, the minimum distance estimation based on chi-square criterion, the moment estimation based on imputation (IM) method and an ad hoc estimation procedure. Although IM-based techniques are extensively used recently, we show that this method is not always effective. It is found that the ad hoc estimation procedure is equivalent to the minimum distance estimation with another distance metric and more effective in the simulation. The procedures of different approaches are presented and their performances are investigated by Monte Carlo simulation for various combinations of\u00a0\u2026", "num_citations": "2\n", "authors": ["445"]}
{"title": "Performance-based burn-in for products sold with warranty\n", "abstract": " To protect users from early failures, manufacturers often use burn-in to screen out weak units and meanwhile providing warranty to customers. Some performance-based burn-in models have been proposed in the literature. However, relations between these indices are not clear. In this study, we look into this problem and reveal the internal relations between these indices. More specifically, we focus on the probability of failure within the warranty period, mean number of failures within the warranty period and the percentile residual life that maximize the warranty period given a specified proportion of field failure. It is found that there are some dual relations between these indices. An illustrative example is used to demonstrate our results.", "num_citations": "2\n", "authors": ["445"]}
{"title": "Service reliability and availability analysis of distributed software systems considering malware attack\n", "abstract": " Distributed systems are widely deployed in industries where high computational capability and low cost are required. Distributed software architecture is very important in the distributed system. However, since most distributed systems are designed based on a network structure, distributed software is very vulnerable to malware attacks. Due to the popularity of distributed system, it is vital to study the effects of malware attack on distributed systems. In this paper, we studied the malware attack behaviors by a Markov process model. The states of the object homogeneous distributed system can be derived by analyzing the Markov model, with which the service reliability and service availability can be further obtained. An illustrative demonstration is also presented.", "num_citations": "2\n", "authors": ["445"]}
{"title": "Intelligence and impact contests in defending a single object with imperfect false targets\n", "abstract": " Introducing false targets is a common approach to protect important facilities. In this paper we assume that the defender has deployed a single object that can be destroyed by the attacker and deployed several false targets in order to misinform the attacker. The false targets are imperfect and can be detected by the attacker independently. In order to detect the false targets the attacker allocates part of his budget into intelligence actions. The optimal resource distribution between target identification/disinformation and attack/ protection efforts is studied for the case of constrained defense and attack resources.", "num_citations": "2\n", "authors": ["445"]}
{"title": "Economic design of exponential chart for monitoring time-between-event data under random process shift\n", "abstract": " Control charts based on time-between-event data have recently shown to be very useful in manufacturing systems, in reliability and maintenance monitoring, and also in service related applications in general. This article develops an economic model of the exponential chart for monitoring time-between-event data under random process shift. The probability density function for the occurrence of process shift follows Rayleigh distribution. The design of the proposed control chart scheme has been demonstrated. A comparison between economic design and statistical design is conducted. The results of the numerical example show that the exponential chart designed under random process shift is more realistic than the exponential chart designed under fixed process shift.", "num_citations": "2\n", "authors": ["445"]}
{"title": "Concepts and applications of stochastic ageing\n", "abstract": " The concept of ageing is very important in reliability analysis.\u2018No ageing\u2019means that the age of a component has no effect on the distribution of residual lifetime of the component.\u2018Positive ageing\u2019(also known as \u2018averse ageing\u2019) describes the situation where residual lifetime tends to decrease, in some probabilistic sense, with increasing age of a component. This situation is common in reliability engineering as components tend to become worse with time due to increased wear and tear. On the other hand,\u2018negative ageing\u2019has an opposite effect on the residual lifetime.\u2018Negative ageing\u2019is also known as \u2018beneficial ageing\u2019. Although this is less common, when a system undergoes regular testing and improvement, there are cases for which we have reliability growth phenomenon. Though we concentrate on positive ageing in this book, it is being understood that a parallel development of negative ageing can also be\u00a0\u2026", "num_citations": "2\n", "authors": ["445"]}
{"title": "Basic reliability concepts and analysis\n", "abstract": " The density function can be mathematically described as This can be interpreted as the probability that the failure time T will occur between time t and the next interval of operation, The three functions, R (t), F (t) and f (t) are closely related to one another. If any of them is known, all the others can be determined.", "num_citations": "2\n", "authors": ["445"]}
{"title": "On the change points of mean residual life and failure rate for some extended Weibull distributions\n", "abstract": " The lifetime distribution of many complex systems exhibits bathtub-shaped failure rate (BFR). In this paper, we investigate the association between the mean residual life (MRL) and failure rate (FR) of BFR distributions, especially with respect to their change points. Some generalizations of Weibull distribution that can be used to model bathtub-shaped failure rate are used. The sensitivity of the change points to the parameters is discussed. Besides, it is also of interest to investigate for what parameters or what BFR distribution, the flat portion of bathtub curve is long. Although it is not always the case, when the relative difference between the change points is small, the flat portion tends to be longer. Some preliminary studies on this will also be presented.", "num_citations": "2\n", "authors": ["445"]}
{"title": "Fault tree reduction for reliability analysis and improvement\n", "abstract": " Fault tree analysis is a technique widely used in the study of the reliability of complex systems. Most of the studies carried out are related to how to construct a fault tree, and how to determine the top event probability. Fault trees are usually very complex after the construction phase and almost all fault trees can be reduced so that further analysis can be made easier. In this paper some principles of fault tree reduction are presented. By reducing the fault trees to a much more graphically simplified configuration, analysis of the fault trees can be carried out with less effort. Examples are given to demonstrate the usefulness of the proposed reduction technique in both the qualitative and quantitative fault tree analysis. The MOCUS algorithm is for example used as a basis for comparison between non-reduced fault trees and reduced fault trees on the reduction in steps required to obtain the minimal cut sets. Furthermore\u00a0\u2026", "num_citations": "2\n", "authors": ["445"]}
{"title": "Software reliability models for practical applications\n", "abstract": " Reliability is one of the most important aspects of product quality. Software reliability research has been going on worldwide and many software reliability models are proposed and studied. In this paper, we first review some of the important software reliability models and then discuss them with respect to their validity and usefulness. The final part of this paper is aimed to present some simple models for the analysis of software failure data. Unlike the conventional approach, we present our model based on the graphical interpretation and this approach allows us to easily test a few models before detailed study based on a selected model is carried out.", "num_citations": "2\n", "authors": ["445"]}
{"title": "Some new aspects on component importance measures\n", "abstract": " Component importance measures are useful in deciding where to allocate further efforts to increase system performance, particularly system reliability and availability. Which of the components should be judged as important, and hence to be improved at the first hand, depends on the component structural position in the system, the improvement potential of the component and also the improvement action to be taken. In this paper we review some recent advances in measuring the importance of system components. A general methodology that incorporates these features is presented. We will also introduce some concepts concerning availability importance useful for repairable systems.", "num_citations": "2\n", "authors": ["445"]}
{"title": "Reliability analysis of systems with discrete event data using association rules\n", "abstract": " With the popularization of big data, an increasing number of discrete event data have been collected and recorded during system operations. These events are usually stored in the form of event logs, which contain rich information of system operations and have potential applications in fault diagnosis and failure prediction. In manufacturing processes, various levels of correlations exist among the events, which can be used to predict the occurrence of failure events. However, two challenges remain to be solved for effective reliability analysis and failure prediction: (1) how to leverage various information from the event log to predict the occurrence of failure events and (2) how to model the effects of multiple correlations on the prediction. To address these issues, this paper proposes a novel reliability model, which integrates Cox proportional hazards (PHs) regression into survival analysis and association rule mining\u00a0\u2026", "num_citations": "1\n", "authors": ["445"]}
{"title": "Improved resilience measure for component recovery priority in power grids\n", "abstract": " Given the complexity of power grids, the failure of any component may cause large-scale economic losses. Consequently, the quick recovery of power grids after disasters has become a new research direction. Considering the severity of power grid disasters, an improved power grid resilience measure and its corresponding importance measures are proposed. The recovery priority of failed components after a disaster is determined according to the influence of the failed components on the power grid resilience. Finally, based on the data from the 2019 Power Yearbook of each city in Shandong Province, China, the power grid resilience after a disaster is analyzed for two situations, namely, partial components failure and failure of all components. Result shows that the recovery priorities of components with different importance measures vary. The resilience evaluations under different repair conditions prove the\u00a0\u2026", "num_citations": "1\n", "authors": ["445"]}
{"title": "Improved event-triggered control for networked control systems subject to deception attacks\n", "abstract": " This paper investigates the event-triggered control problem for networked control systems subject to deception attacks. An improved event-triggered scheme is proposed to reduce transmission rate by using both the information of the relative error and the past released signals. Under the proposed event-triggered scheme, a new switched time-delay system model is proposed for the event-triggered control systems. Based on the new model, the exponential mean-square stability criteria are derived by using the constructed Lyapunov function. Then, a co-design method is developed to obtain both trigger parameters and mode-dependent controller gains. Finally, the proposed scheme is verified by an unmanned aerial vehicle system.", "num_citations": "1\n", "authors": ["445"]}
{"title": "Performance-oriented risk evaluation and maintenance for multi-asset systems: A Bayesian perspective\n", "abstract": " In this article, we present a risk evaluation and maintenance strategy optimization approach for systems with parallel identical assets subject to continuous deterioration. System performance is defined by the number of functional assets, and the penalty cost is measured by the loss of performance. To overcome the practical challenges of information sparsity, we employ a Bayesian framework to dynamically update unknown parameters in a Wiener degradation model. Order statistics are utilized to describe the failure times of assets and the stepwise incurred performance penalty cost. Furthermore, based on the Bayesian parameter inferences, we propose a short-term value-based replacement policy to minimize the expected cost rate in the current planning horizon. The proposed strategy simultaneously considers the variability of parameter estimators and the inherent uncertainty of the stochastic degradation\u00a0\u2026", "num_citations": "1\n", "authors": ["445"]}
{"title": "Discussion of \u201cVirtual age, is it real?\u201d\n", "abstract": " Discussion of \u201cVirtual age, is it real?\u201d - Archive ouverte HAL Acc\u00e9der directement au contenu Acc\u00e9der directement \u00e0 la navigation Toggle navigation CCSD HAL HAL HALSHS TEL M\u00e9diHAL Liste des portails AUR\u00e9HAL API Data Documentation Episciences.org Episciences.org Revues Documentation Sciencesconf.org Support hal Accueil D\u00e9p\u00f4t Consultation Les derniers d\u00e9p\u00f4ts Par type de publication Par discipline Par ann\u00e9e de publication Par structure de recherche Les portails de l'archive Recherche Documentation hal-03149856, version 1 Article dans une revue Discussion of \u201cVirtual age, is it real?\u201d Laurent Doyen 1 Olivier Gaudoin 1 D\u00e9tails 1 ASAR - Applied Statistics And Reliability - ASAR LJK - Laboratoire Jean Kuntzmann Type de document : Article dans une revue Domaine : Statistiques [stat] / Applications [stat.AP] Liste compl\u00e8te des m\u00e9tadonn\u00e9es Voir https://hal.archives-ouvertes.fr/hal-03149856 : Doyen :\u2026", "num_citations": "1\n", "authors": ["445"]}
{"title": "On dynamically monitoring aggregate warranty claims for early detection of reliability problems\n", "abstract": " Warranty databases managed by most world-leading manufacturers are constantly expanding in the big data era. An important application of warranty databases is to detect unobservable reliability problems that emerge at design and/or manufacturing stages, through modeling and analysis of warranty claims data. Usually, serious reliability problems will result in certain abnormal patterns in warranty claims, which can be captured by appropriate statistical methods. In this article, a dynamic control charting scheme is developed for early detection of reliability problems by monitoring warranty claims one period after another, over the product life cycle. Instead of specifying a constant control limit, we determine the control limits progressively by considering stochastic product sales and non-homogeneous failure processes, simultaneously. The false alarm rate at each time period is controlled at a desired level, based on\u00a0\u2026", "num_citations": "1\n", "authors": ["445"]}
{"title": "A double-sampling SPM scheme for simultaneously monitoring of location and scale shifts and its joint design with maintenance strategies\n", "abstract": " Joint design of SPM (Statistical Process Monitoring) and maintenance strategies has evolved as a popular research topic in industrial engineering. Most existing works only consider location shifts of a process but neglect the effects of scale shifts. Besides, traditional SPM schemes are usually employed based on the single-sampling plan, which might be insensitive to detect quality shifts and perform uneconomically from the cost-saving perspective. To overcome the drawbacks mentioned above, this paper proposes a double-sampling SPM scheme for simultaneously monitoring of location and scale shifts, and then develop a more realistic and effective model for the joint design of SPM and maintenance strategies. First, we propose a new SPM scheme with a double-sampling plan for simultaneously monitoring location and scale shifts. The comparison results indicate that our proposed DS (double-sampling\u00a0\u2026", "num_citations": "1\n", "authors": ["445"]}
{"title": "Flexible Monitoring Methods for High-yield Processes\n", "abstract": " In recent years, advancement in technology brought a revolutionary change in the manufacturing processes. Therefore, manufacturing systems produce a large number of conforming items with a small amount of non-conforming items. The resulting dataset usually contains a large number of zeros with a small number of count observations. It is claimed that the excess number of zeros may cause over-dispersion in the data (ie, when variance exceeds mean), which is not entirely correct. Actually, an excess amount of zeros reduce the mean of a dataset which causes inflation in the dispersion. Hence, modeling and monitoring of the products from high-yield processes have become a challenging task for quality inspectors. From these highly efficient processes, produced items are mostly zero-defect and modeled based on zero-inflated distributions like zero-inflated Poisson (ZIP) and zero-inflated Negative Binomial\u00a0\u2026", "num_citations": "1\n", "authors": ["445"]}
{"title": "An unpunctual preventive maintenance policy for repairable items sold with a two-dimensional warranty\n", "abstract": " The optimization of preventive maintenance (PM) strategies for repairable items sold with warranty contracts has received much attention in the literature. However, the existing research implicitly assumes that maintenance actions within the warranty period are punctual. In practice, it is not uncommon that the actual maintenance instants deviate from the scheduled instants. In this paper, an unpunctual imperfect PM strategy, which allows customers to advance or postpone the scheduled PM actions in a tolerable range, is proposed for repairable items sold with a two-dimensional warranty. The objective of this work is to determine the optimal unpunctual PM strategies under the given warranty period so as to minimize the manufacturers total expected warranty servicing cost. It is shown that the unpunctual PM strategy contains its punctual counterpart as a special case and tends to result in slightly higher warranty servicing cost.", "num_citations": "1\n", "authors": ["445"]}
{"title": "\u201cAdvances in the theory and application of Statistical Process Control\u201d\u2013Celebrate the Quasquicentennial (125th) Birth Anniversary of the Father of Statistical Quality Control\u00a0\u2026\n", "abstract": " Dr Shewhart joined the Inspection Engineering Department of the famous Western Electric Company at the Hawthorne Works, in Cicero, Illinois in 1918. In those days, industrial quality monitoring was restricted to inspecting finished products and removing defective items. 16 May 1924 witnessed a great transformation in Statistical Process Monitoring (SPM). George D. Edwards, superior to Dr Shewhart in the Western Electric Company remembered \u201cDr Shewhart prepared a little memorandum only about a page in length. About a third of that page was given over to a simple diagram which we would all recognize today as a schematic control chart. That diagram, and the short text which preceded and followed it, set forth all of the essential principles and considerations which are involved in what we know today as process quality control\u201d. As we earlier noted in our call for papers for this special issue,\u201cShewhart\u2019s\u00a0\u2026", "num_citations": "1\n", "authors": ["445"]}
{"title": "Reliability and performance modeling for mission-oriented k-out-of-n system under common cause failures\n", "abstract": " This paper proposes a reliability and performance analysis and modeling methodology for mission-oriented k-out-of-n systems. The system is assumed to suffer both independent internal failures and external common cause shocks, of which arrivals are both modeled by Poisson processes. Periodic missions are assigned to the system due to a fixed schedule. A performance measure is introduced based on the mission workload and number of components working in the system. By modeling the failure modes on such systems with a Markov chain model, the defined reliability and performance is given in analytical forms. In a following numerical example, we illustrate the reliability and performance for such systems by the proposed approach.", "num_citations": "1\n", "authors": ["445"]}
{"title": "Monitoring the frequency and magnitude of an event with a ratio chart\n", "abstract": " Control charts for monitoring the event has attracted many researchers' interests and many works have been done in recent years. However, previous works always neglect the dependence between event frequency and event magnitude and this kind of ignorance may make the control chart not suitable in some real applications. In our work, the frequency and magnitude of an event is characterized by a bivariate gamma distribution with a dependence parameter. And the ratio of the event magnitude and event frequency is introduced and the EWMA type of the ratio is considered as the monitoring statistic. The implementation procedure of the introduced chart is discussed with an example.", "num_citations": "1\n", "authors": ["445"]}
{"title": "A study of linear multi-state systems with interval-valued states\n", "abstract": " The paper proposes a new model that generalizes the linear multi-state system with interval-valued states. The system consists of independent linear ordered multi-state components. The states of each component are represented as interval numbers with upper bounds and lower bounds. It means that a component in the interval-valued state can provide arbitrary contribution between the lower bound and the upper bound to the entire system. The system fails, if the sum of the interval-valued performances of all the components does not satisfy the predetermined demand with a maximum bound and a minimum bound. The performance degradation of system components is considered, and it is described as the degradation of the bounds of component's states. Interval universal generation function is introduced to evaluate the system reliability. The components' reliability importance indices are also studied to identifying the most influential components of the new proposed system. An example of a telescope antenna is presented for demonstration.", "num_citations": "1\n", "authors": ["445"]}
{"title": "Rejoinder to \u2018Stochastic modelling and analysis of degradation for highly reliable products\u2019\n", "abstract": " Rejoinder to \u2018Stochastic modelling and analysis of degradation for highly reliable products\u2019 IDEAS home Advanced search Economic literature: papers, articles, software, chapters, books. Authors Institutions Rankings Help/FAQ MyIDEAS More options at page bottom Economic literature Authors Institutions Rankings Help/FAQ MyIDEAS (now with weekly email digests) Advanced (and improved) search Browse Econ Literature Working papers Journals Software components Books Book chapters JEL classification More features Subscribe to new research RePEc Biblio Author registration Economics Virtual Seminar Calendar NEW! FRED data IDEAS home Printed from https://ideas.repec.org/a/wly/apsmbi/v31y2015i1p35-36.html My bibliography Save this article Rejoinder to \u2018Stochastic modelling and analysis of degradation for highly reliable products\u2019 Author & abstract Download 19 Citations Related works & more : Zhi\u2026", "num_citations": "1\n", "authors": ["445"]}
{"title": "Economic design of a nonparametric control chart for shift in location\n", "abstract": " Most economically designed control charts rely on the assumption of normality or some specific process distribution. However, when identifying a specific distribution is not possible or unlikely (as full knowledge is not available in practice), the economic effectiveness of conventional charts is very likely to be heavily discounted. In this paper, we first consider an economic model based on the Duncan-type cost function for designing a nonparametric sign chart for monitoring the location parameter of a univariate process. Numerical results show that the proposed design performs well for various continuous distributions.", "num_citations": "1\n", "authors": ["445"]}
{"title": "Modelling and analysis of transmission delays and packet dropouts on the reliability of digital networked control systems\n", "abstract": " The insertion of communication networks in closed-loop of digital control systems makes the system synthesis and analysis complex. In this paper, we consider the combination of classical reliability, risk and safety evaluation with typical control system through digital networked control systems. The main purpose is to study the influence of transmission delays and packet dropouts on the reliability of digital networked control systems by using a modified statistical method-Monte Carlo simulation. The behaviors of two types of fault are modelled by different statistical distribution. The control strategies we apply for digital control systems are Proportion Integration Differentiation control strategies, generated by Genetic Algorithm. In terms of a realistic system model, we evaluate and compare the influences on the reliability of digital networked control systems with different control strategies from quantitative aspect\u00a0\u2026", "num_citations": "1\n", "authors": ["445"]}
{"title": "Reference ship selection for the on-time performance benchmarking of ships\n", "abstract": " The on-time performance of a ship is one of the major aspects of a ship\u2019s operational reliability. The establishment of proper benchmarking for it would help the ship\u2019s stakeholders to a considerable extent with respect to enhancement of the performance. This paper investigates and suggests a methodology for selection of a ship as a point of reference, so that improvement in the on-time performance of other ships could be achieved. The issue is addressed through comparative studies of ships, exclusively from an on-time performance viewpoint. The paper proposes a structure that would guide the development of a support system for improvement in the operational reliability of a ship. The on-time performance of a ship is expressed in terms of 12 primary attributes signifying the diverse aspects of its operational reliability. Four ships of like characteristics voyaging between Singapore and Japan are considered for\u00a0\u2026", "num_citations": "1\n", "authors": ["445"]}
{"title": "Degradation modeling using stochastic filtering for systems under imperfect maintenance\n", "abstract": " Wiener process with a linear drift has been extensively studied in degradation modeling, mainly due to the existence of an analytical expression of the first hitting time distribution which permits feasible mathematical developments. However, a fundamental problem related to the stationary Wiener process is that it can only describe linearly drifted diffusion processes. This article is devoted to characterizing degradation phenomena with non-stationary Wiener processes. A new treatment is initiated to characterize the efficiency of imperfect maintenance, ie, extending the improvement factor method on the degradation rate function. A stochastic filtering technique is employed to dynamically update the estimate of the degradation rate. A numerical example is given to illustrate the potential applications in real practice. Copyright\u00a9 2013, AIDIC Servizi Srl", "num_citations": "1\n", "authors": ["445"]}
{"title": "An extended quantitative risk analysis model by incorporating human and organizational factors\n", "abstract": " In this paper, a quantitative risk analysis (QRA) model incorporating human and organizational factor is presented by integrating Fault Tree (FT) with Bayesian Network (BN). FT is used to model the factors how to contribute to the final failures. BN extends the causal chain of basic events to potential human and organizational roots and provides a more precise quantitative links between the event nodes. In order to define the conditional probability table of BN, fuzzy Analytical Hierarchy Process (AHP) is integrated with a decomposition method. The fuzzy AHP helps to reduce the subjective biases by avoiding the need to spell out explicit probability values for the variables' states. The decomposition method breaks the complexity by allowing conditioning on each of the parent nodes separately. The new QRA model is demonstrated on an offshore fire case study. By exploiting the advantages of both models, the method\u00a0\u2026", "num_citations": "1\n", "authors": ["445"]}
{"title": "Quantitative risk assessment using hybrid causal logic model\n", "abstract": " This paper presents a hybrid causal logic model, which integrates the traditional Quantitative Risk Assessment (QRA) models with Bayesian Network (BN) incorporating human and organizational factors. The multi-phase model allows different risk assessment methods to be applied to different parts. In the first phase, Event Tree (ET) defines the base scenarios for the source of risk issues. In the second phase, Fault Tree (FT) is used to model the factors how to contributing to the final failures. BN comprise the third phase, which extends the causal chain of basic events to potential human and organizational roots and provide a more precise quantitative links between the event nodes. The new model integrates the power of typical QRA for modeling deterministic causal paths with the flexibility of BN for modeling non-deterministic cause-effect relationships. The integration algorithm is demonstrated on an offshore fire case study. It clearly shows the new model is more flexible and useful than traditional QRA models.", "num_citations": "1\n", "authors": ["445"]}
{"title": "A MEWMA chart for a bivariate exponential distribution\n", "abstract": " Control charts as one of the most well-known statistical process control (SPC) techniques have shown to be effective in process monitoring. Most of the existing studies in the area of the time-between-event (TBE) control charts have been focused on the univariate cases. In this paper, a MEWMA chart is constructed for monitoring the mean vector of the Gumbel's bivariate exponential TBE model. The average run length profile of the proposed chart is studied using simulation. Some guidelines for setting up an optimal MEWMA chart are provided. Finally, a numerical example is given to show the effectiveness of the MEWMA chart.", "num_citations": "1\n", "authors": ["445"]}
{"title": "A Systematic Approach to the Reliability Analysis of an n-Unit Warm Standby System With k-Repair Facility\n", "abstract": " A systematic reliability analysis of n-unit warm standby repairable system with k-repair facility is presented in this paper. Traditional approaches are extended under the following assumptions: (1) the working lifetime, the standby lifetime, and the repair time of failed units are represented as exponential distribution; and (2) the repair of failed units are as good as new after repair. In this paper, a general reliability analysis of an n-unit warm standby repairable system with k-repair facility is presented. Based on previous analysis, the steady-state reliability and the average availability of the system are formulated using the Markov process theory and Laplace transform.", "num_citations": "1\n", "authors": ["445"]}
{"title": "On change point of mean residual life of parallel systems\n", "abstract": " Mean residual life is one of the most important characteristics that can be used to measure the reliability of parallel systems. In literature, mean residual life of parallel systems has been defined in different ways. How are these mean residual life functions related to each other? To answer this question, this paper studies the mean residual life of parallel systems from the point of view of change point, at which the mean residual life function changes its trend. Different definitions of the mean residual life of parallel systems are reviewed and introduced. By comparing the change points of these mean residual life functions graphically, we find that these change points behave in a regular pattern, and all of them are related to the change point for single components in the aspect of location. These results are helpful in the determination of optimal burn-in time.", "num_citations": "1\n", "authors": ["445"]}
{"title": "An integrated design or exponential chart system with independent quality characteristics\n", "abstract": " Exponential chart as one type of the time-between-event chart has been shown to be very useful in manufacturing systems, in reliability and maintenance monitoring, and also in service-related applications in general. This paper proposes an algorithm to design the exponential chart system consisting of several individual exponential charts with independent quality characteristics in an integrated and optimal manner. The numerical example shows that the performance of the overall system as a whole can be considerably improved.\u00a9 2008 ICQR.", "num_citations": "1\n", "authors": ["445"]}
{"title": "Applying statistical design of experiments as a change agent in industry\n", "abstract": " Quality management gurus and statisticians alike have been admonishing that industry should make more use of statistical thinking and statistical tools. In this paper, the experience in the use of statistical tools, specifically Design of Experiments, as a change agent in an industrial setting is described. The usefulness of an overall framework, in this case Design for Six Sigma, is pointed out, as it could bring out conscious thought changes such as the recognition of variation and the need for robustness in design and manufacturing. The\" 5S\", or Start-Secure-Setup-Scrutinize-Share framework can make statistical tools part of an organization's quality improvement roadmap.[PUBLICATION ABSTRACT]", "num_citations": "1\n", "authors": ["445"]}
{"title": "Economic design of integrated time-between-events chart system with independent quality characteristics\n", "abstract": " This article presents the economic design of the integrated control chart system consisting of several individual time-between-events (TBE) charts for monitoring time between successive events in different process stages in a multistage manufacturing system. The design of the integrated chart system has been illustrated through an example. The proposed control chart system is easy to understand and operate, and thus the floor operators can utilize and understand it as easily as for the traditional system.", "num_citations": "1\n", "authors": ["445"]}
{"title": "Weibull Related Distributions\n", "abstract": " The Weibull distribution is one of the best known lifetime distributions. It adequately describes observed failures of many different types of components and phenomena. Over the last three decades, numerous articles have been written on this distribution. Hallinan (1993) presented an insightful review by presenting a number of historical facts, and many forms of this distribution as used by the practitioners and possible confusions and errors that arise due to this non-uniqueness. Johnson et al.(1994) devoted a comprehensive chapter on a systematic study of this distribution. More recently, a monograph written by Murthy et al.(2003) contains nearly every facet concerning the Weibull distribution and its extensions. Lai et al.(2005) also provided a bird\u2019s eye view of this vast subject. Section 2.3. 4 has briefly introduced the Weibull distribution. In this chapter, we angle our discussion towards reliability aspects of the\u00a0\u2026", "num_citations": "1\n", "authors": ["445"]}
{"title": "Failure time data\n", "abstract": " This chapter provides data sets that are known to belong to a particular ageing class. We believe that many readers will welcome having a number of data sets reproduced here as academic staff like to give their students data that is real rather than contrived. Only the numbers are extracted here, and the readers should consult the original source for detailed analysis done. In several cases, the data sets have also been analyzed by other researchers. In Section 11.2, a rough guide on how to select a model from many plausible models such as the Weibull models discussed in Chapter 5. We discuss briefly in Section 11.3 how survival functions and failure rate functions can be estimated from a data set as well as the structure of our data presentation. Sections 11.4\u201311.8 list various data sets according to their ageing classifications. Finally in Section 11.9 we refer the readers to other sources of survival and reliability\u00a0\u2026", "num_citations": "1\n", "authors": ["445"]}
{"title": "An introduction to discrete failure time models\n", "abstract": " An important aspect of lifetime analysis is to find a lifetime distribution that can adequately describe the ageing behavior of the device concerned. Most of the lifetimes are continuous in nature and hence many continuous life distributions have been proposed in the literature. On the other hand, discrete failure data arise in several common situations, for example:\u2022 Reports on field failures are collected weekly, monthly, and the observations are the number of failures, without specification of the failure times;\u2022 A piece of equipment operates in cycles and the experimenter observes the number of cycles successfully completed prior to failure. A frequently referred example is a copier whose life length would be the total number of copies it produces. Another example is the number of on/off cycles of a switch before failure occurs;", "num_citations": "1\n", "authors": ["445"]}
{"title": "Some advanced control charts for monitoring weibull-distributed time between events\n", "abstract": " Time-between-events (TBE) data are available in industries such as manufacturing, maintenance, and even in service. Recently, control charts have been shown to be useful for the time-between-events to detect changes in the statistical distribution, especially the mean change. A common assumption for control chart design is the time between occurrences of events is exponentially-distributed. However, this is valid only when the events occurrence rate is constant. In this paper, a version of exponentially weighted moving average (EWMA) chart is developed for monitoring Weibull-distributed TBE data. The Average Run length (ARL) and Average Time to Signal (ATS) properties are examined, and an example is given for illustration.", "num_citations": "1\n", "authors": ["445"]}
{"title": "WARRANTY COST ANALYSIS OF PRORATA POLICY FOR CONTINUOUS SALES PROCESS CONSIDERING WARRANTY EXECUTION FACTORS\n", "abstract": " \u6b63 Estimation of warranty servicing costs during the product life cycle, in the event of product failure within the warranty period, is of importance to the manufacturer. The warranty costs are usually drawn from a warranty reserve fund created by the manufacturer. Previous research has usually dealt with the unit item or a fixed product lot size in the market and assumed full execution of warranty if a product fails within the warranty period. In this paper, on the basis of investigating the effect of warranty execution, we obtain the estimating warranty costs model for continuous sales process of nonrepairable products under pro-rate warranty policy. The model discounts future warranty costs to their present value by adjusting for expected changes in the general price level and investment growth. From this model, we can obtain the cash flows of warranty reserve costs at any time intervals during the product life cycle. A numerical example is given to illustrate the proposed model.", "num_citations": "1\n", "authors": ["445"]}
{"title": "On the performance of geometric charts with estimated control limits\n", "abstract": " The control chart based on geometric distribution (geometric chart) has been shown to be competitive to p-or np-charts for monitoring proportion nonconforming, especially for applications in high quality manufacturing environment. However, implementing a geometric chart often assumes the process parameter to be known or accurately estimated. For a high quality process, an accurate parameter estimate may require a very large sample size that is seldom available. In this paper we investigate the sample size effect when the process parameter needs to be estimated. It is shown that the estimated control limits create dependence among the monitoring events. Analytical approximation is derived to compute shift detection probabilities and run length distributions. It is found that, when there is no shift in proportion nonconforming, the false alarm probability increases as the sample size decreases and the effect can be significant even with sample size as large as 10,000. However, the in-control average run length is only affected mildly. On the other hand, when there is a process shift, the out-of-control average run length can be significantly affected by the estimated control limits, even with very large sample sizes. In practice, the quantitative results of the paper can be used to determine the minimum number of items required for estimating the control limits of a geometric chart so that certain average run length requirements are met.", "num_citations": "1\n", "authors": ["445"]}
{"title": "Control Charts with Probability Limits\n", "abstract": " When the process characteristic does not follow the normal distribution, the probability limits should be used instead of the traditional 3-sigma limits. In a near zero-defect environment, many problems indicated in Section 1.2 will arise and the traditional control charts with 3-sigma limits cannot be interpreted in the same way as the actual false alarm probability could be very different. Probability limits can be obtained by fixing the false alarm probability at an acceptable level and is hence more appropriate. It is an easy task to compute the exact probability limits when the underlying distribution for process characteristics is known. In this chapter we focus on the need for and uses of probability limits in a high quality environment and for process improvement identification in general. Although the methodology applies to both variable charts as well as attribute charts, we will focus on attribute charts as the\u00a0\u2026", "num_citations": "1\n", "authors": ["445"]}
{"title": "Optimizing environmental stress screening using mathematical programming\n", "abstract": " Environmental stress screening (ESS) is widely used in the electronics industry as a means of removing early failures. It is a process that calls for proper planning because inadequate duration is ineffective and prolonged screening can incur unnecessary costs. This paper describes an approach that uses mathematical programming to ensure that the right amount of screening is in place at each assembly level. Factors considered are screening cost and desired operational reliability.", "num_citations": "1\n", "authors": ["445"]}
{"title": "Fault Content Estimations: A Pragmatic Approach using Design Metrics\n", "abstract": " The identification of fault-prone parts of a system or estimation of the total fault content are important for planning purposes and to increase the quality of the delivered software. This paper presents a pragmatic procedure for metrics selection to include in a prediction model of fault content. The proposed procedure is simple to apply and it is based on correlation analysis between the metrics, fault content and inter-correlation between the different metrics. Multicollinearity is addressed by the selection method, although not formally handled. A prediction model is derived from one data set and evaluated in a second data set. The model performs well, but more analysis is required. Future work also includes evaluating the pragmatic approach based on correlation with other approaches, such as stepwise regression analysis and principal components. The main objective of the study is to raise a number of questions for discussion and further work.", "num_citations": "1\n", "authors": ["445"]}