{"title": "SALSA: Analyzing Logs as StAte Machines.\n", "abstract": " SALSA examines system logs to derive state-machine views of the sytem\u2019s execution, along with controlflow, data-flow models and related statistics. Exploiting SALSA\u2019s derived views and statistics, we can effectively construct higher-level useful analyses. We demonstrate SALSA\u2019s approach by analyzing system logs generated in a Hadoop cluster, and then illustrate SALSA\u2019s value by developing visualization and failure-diagnosis techniques, for three different Hadoop workloads, based on our derived state-machine views and statistics.", "num_citations": "154\n", "authors": ["1770"]}
{"title": "Mochi: Visual Log-Analysis Based Tools for Debugging Hadoop.\n", "abstract": " Mochi, a new visual, log-analysis based debugging tool correlates Hadoop\u2019s behavior in space, time and volume, and extracts a causal, unified control-and dataflow model of Hadoop across the nodes of a cluster. Mochi\u2019s analysis produces visualizations of Hadoop\u2019s behavior using which users can reason about and debug performance issues. We provide examples of Mochi\u2019s value in revealing a Hadoop job\u2019s structure, in optimizing real-world workloads, and in identifying anomalous Hadoop behavior, on the Yahoo! M45 Hadoop cluster.", "num_citations": "97\n", "authors": ["1770"]}
{"title": "Black-Box Problem Diagnosis in Parallel File Systems.\n", "abstract": " We focus on automatically diagnosing different performance problems in parallel file systems by identifying, gathering and analyzing OS-level, black-box performance metrics on every node in the cluster. Our peercomparison diagnosis approach compares the statistical attributes of these metrics across I/O servers, to identify the faulty node. We develop a root-cause analysis procedure that further analyzes the affected metrics to pinpoint the faulty resource (storage or network), and demonstrate that this approach works commonly across stripe-based parallel file systems. We demonstrate our approach for realistic storage and network problems injected into three different file-system benchmarks (dd, IOzone, and Post-Mark), in both PVFS and Lustre clusters.", "num_citations": "85\n", "authors": ["1770"]}
{"title": "Visual, Log-Based Causal Tracing for Performance Debugging of MapReduce Systems.\n", "abstract": " The distributed nature and large scale of MapReduce programs and systems poses two challenges in using existing profiling and debugging tools to understand MapReduce programs. Existing tools produce too much information because of the large scale of MapReduce programs, and they do not expose program behaviors in terms of Maps and Reduces. We have developed a novel non-intrusive log-analysis technique which extracts state-machine views of the control- and data-flows in MapReduce behavior from the native logs of Hadoop MapReduce systems, and it synthesizes these views to create a unified, causal view of MapReduce program behavior. This technique enables us to visualize MapReduce programs in terms of MapReduce-specific behaviors, aiding operators in reasoning about and debugging performance problems in MapReduce systems. We validate our technique and visualizations using a\u00a0\u2026", "num_citations": "77\n", "authors": ["1770"]}
{"title": "A5: Automated Analysis of Adversarial Android Applications\n", "abstract": " Mobile malware is growing-both in overall volume and in number of existing variants-at a pace rapid enough that systematic manual, human analysis is becoming increasingly difficult. As a result, there is a pressing need for techniques and tools that provide automated analysis of mobile malware samples. We present A5, an open source automated system to process Android malware. A5 is a hybrid system combining static and dynamic malware analysis techniques. Android's architecture permits many different paths for malware to react to system events, any of which may result in malicious behavior. Key innovations in A5 consist of novel methods of interacting with mobile malware to better coerce malicious behavior, and in combining both virtual and physical pools of Android platforms to capture behavior that could otherwise be missed. The primary output of A5 is a set of network threat indicators and intrusion\u00a0\u2026", "num_citations": "65\n", "authors": ["1770"]}
{"title": "Performance Troubleshooting in Data Centers: An Annotated Bibliography\n", "abstract": " In the emerging cloud computing era, enterprise data centers host a plethora of web services and applications, including those for e-Commerce, distributed multimedia, and social networks, which jointly, serve many aspects of our daily lives and business. For such applications, lack of availability, reliability, or responsiveness can lead to extensive losses. For instance, on June 29th 2010, Amazon. com experienced three hours of intermittent performance problems as the normally reliable website took minutes to load items, and searches came back without product links. Customers were also unable to place orders. Based on their 2010 quarterly revenues, such downtime could cost Amazon up to $1.75 million per hour, thus making rapid problem resolution critical to its business. In another serious incident, on July 7th, 2010, DBS bank in Singapore suffered a 7-hour outage which crippled its Internet banking systems\u00a0\u2026", "num_citations": "42\n", "authors": ["1770"]}
{"title": "Hadoop framework: impact of data organization on performance\n", "abstract": " Hadoop, based on the popular MapReduce framework, is an open\u2010source distributed computing framework that has been gaining much popularity and usage. It aims to allow programmers to focus on building applications that deals with processing large amount of data, without having to handle other issues when performing parallel computations. However, tuning the performance of Hadoop applications is not an easy task due to the level of abstraction of the framework. In this paper, we present three case studies and some of the challenges and issues that are to be considered in performance tuning when running applications in Hadoop. The focus is mainly on the impact of input data on Hadoop's performance and how they can be tuned. Copyright \u00a9 2011 John Wiley & Sons, Ltd.", "num_citations": "29\n", "authors": ["1770"]}
{"title": "Theia: Visual Signatures for Problem Diagnosis in Large Hadoop Clusters.\n", "abstract": " Diagnosing performance problems in large distributed systems can be daunting as the copious volume of monitoring information available can obscure the root-cause of the problem. Automated diagnosis tools help narrow down the possible root-causes\u2014however, these tools are not perfect thereby motivating the need for visualization tools that allow users to explore their data and gain insight on the root-cause. In this paper we describe Theia, a visualization tool that analyzes application-level logs in a Hadoop cluster, and generates visual signatures of each job's performance. These visual signatures provide compact representations of task durations, task status, and data consumption by jobs. We demonstrate the utility of Theia on real incidents experienced by users on a production Hadoop cluster.", "num_citations": "29\n", "authors": ["1770"]}
{"title": "Lightweight Black-box Failure Detection for Distributed Systems\n", "abstract": " Detecting failures in distributed systems is challenging, as modern datacenters run a variety of applications. Current techniques for detecting failures often require training, have limited scalability, or have results that are hard to interpret. We present LFD, a light-weight technique to quickly detect performance problems in distributed systems using only correlations of OS metrics. LFD is based on our hypothesis of server application behavior, does not require training, and detects failures with complexity linear in the number of nodes, with results that are interpretable by sysadmins. We further show that LFD is versatile, and can diagnose faults in Hadoop MapReduce systems and on multi-tier web request systems, and show how LFD is intuitive to sysadmins.", "num_citations": "24\n", "authors": ["1770"]}
{"title": "Blind Men and the Elephant: Piecing together Hadoop for diagnosis\n", "abstract": " Google\u2019s MapReduce framework enables distributed, data-intensive, parallel applications by decomposing a massive job into smaller (Map and Reduce) tasks and a massive data-set into smaller partitions, such that each task processes a different partition in parallel. However, performance problems in a distributed MapReduce system can be hard to diagnose and to localize to a specific node or a set of nodes. On the other hand, the structure of large number of nodes performing similar tasks naturally affords us opportunities for observing the system from multiple viewpoints. We present a \u201cBlind Men and the Elephant\u201d(BliMeE) framework in which we exploit this structure, and demonstrate how problems in a MapReduce system can be diagnose by corroborating the multiple viewpoints. More specifically, we present algorithms within the BliMeE framework based on OS-level performance counters, on white-box metrics extracted from logs, and on application-level heartbeats. We show that our BliMeE algorithms are able to capture a variety of faults including resource hogs and application hangs, and to localize the fault to subsets of slave nodes in the MapReduce system. In addition, we discuss how the diagnostic algorithms\u2019 outcomes can be further synthesized in a repeated application of the BliMeE approach. We present a simple supervised learning technique which allows us to identify a fault if it has been previously observed.", "num_citations": "18\n", "authors": ["1770"]}
{"title": "Transparent System Call Based Performance Debugging for Cloud Computing.\n", "abstract": " Problem diagnosis and debugging in distributed environments such as the cloud and popular distributed systems frameworks has been a hard problem. We explore an evaluation of a novel way of debugging distributed systems, such as the MapReduce framework, by using system calls. Performance problems in such systems can be hard to diagnose and to localize to a specific node or a set of nodes. Additionally, most debugging systems often rely on forms of instrumentation and signatures that sometimes cannot truthfully represent the state of the system (logs or application traces for example). We focus on evaluating the performance debugging of these frameworks using a low level of abstraction-system calls. By focusing on a small set of system calls, we try to extrapolate meaningful information on the control flow and state of the framework, providing accurate and meaningful automated debugging.", "num_citations": "11\n", "authors": ["1770"]}
{"title": "PETAL: Preset Encoding Table Information Leakage\n", "abstract": " SPDY is an application-layer protocol which multiplexes multiple HTTP requests and compresses HTTP headers over a single TCP connection protected by SSL/TLS encryption. Web applications are ubiquitous, and HTTP headers carry HTTP cookies which often contain sensitive information which can result in loss of privacy if leaked. We perform a security analysis on the proposed compression scheme for the next revision of the SPDY protocol, particularly with respect to the previously disclosed CRIME attack which uses compression-based information leaks. We have identified a new information leakage in the compression scheme of the proposed and previous versions of the SPDY protocol, which we call PETAL1, which exploits the use of a fixed Huffman encoding table and the lack of byte-alignment of encoded characters, and we have identified a way to recover cookies using this information leakage by exploiting the way that multiple HTTP cookies with the same name but different Path attributes are handled by current web browsers. We perform a detailed analysis of the impact of this information leakage, and find that after considering practical issues such as the byte-padded nature of network communications, our hypothesized attack only leaks less than 2-bits of information for 30-character uppercase alphanumeric strings, and does not allow a network attacker to recover meaningful amounts of information despite our discovered information leakage.Acknowledgements: We would like to thank Collin Jackson and Lin-Shung Huang for helpful discussions and suggestions for this paper. We would also like to thank Roberto Peon and\u00a0\u2026", "num_citations": "5\n", "authors": ["1770"]}
{"title": "Log-based Approaches to Characterizing and Diagnosing MapReduce Systems\n", "abstract": " MapReduce programs and systems are large-scale, highly distributed and parallel, consisting of many interdependent Map and Reduce tasks executing simultaneously on potentially large numbers of cluster nodes. They typically process large datasets and run for long durations. Thus, diagnosing failures in MapReduce programs is challenging due to their scale. This renders traditional time-based Service-Level Objectives ineffective. Hence, even detecting whether a MapReduce program is suffering from a performance problem is difficult. Tools for debugging and profiling traditional programs are not suitable for MapReduce programs, as they generate too much information at the scale of MapReduce programs, do not fully expose the distributed interdependencies, and do not expose information at the MapReduce level of abstraction. Hadoop, the open-source implementation of MapReduce, natively generates logs that record the system\u2019s execution, with low overheads. From these logs, we can extract state-machine views of Hadoop\u2019s execution, and we can synthesize these views to create a single unified, causal, distributed control-flow and data-flow view of MapReduce program behavior. This state-machine view enables us to diagnose problems in MapReduce systems. We can also generate visualizations of MapReduce programs in combinations of the time, space, and volume dimensions of their behavior that can aid users in reasoning about and debugging performance problems. We evaluate our diagnosis algorithm based on these state-machine views on synthetically injected faults on Hadoop clusters on Amazon\u2019s EC2 infrastructure\u00a0\u2026", "num_citations": "3\n", "authors": ["1770"]}