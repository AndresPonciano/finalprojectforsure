{"title": "Empirical validation of object-oriented metrics on open source software for fault prediction\n", "abstract": " Open source software systems are becoming increasingly important these days. Many companies are investing in open source projects and lots of them are also using such software in their own work. But, because open source software is often developed with a different management style than the industrial ones, the quality and reliability of the code needs to be studied. Hence, the characteristics of the source code of these projects need to be measured to obtain more information about it. This paper describes how we calculated the object-oriented metrics given by Chidamber and Kemerer to illustrate how fault-proneness detection of the source code of the open source Web and e-mail suite called Mozilla can be carried out. We checked the values obtained against the number of bugs found in its bug database - called Bugzilla - using regression and machine learning methods to validate the usefulness of these\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1101\n", "authors": ["64"]}
{"title": "Lecture notes in computer science (including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics): Preface\n", "abstract": " The 20th ICPR (International Conference on Pattern Recognition) Conference took place in Istanbul, Turkey, during August 23\u0393\u00c7\u00f426, 2010. For the first time in the ICPR history, several scientific contests (http://www. icpr2010. org/contests. php) were organized in parallel to the conference main tracks. The purpose of these contests was to provide a setting where participants would have the opportunity to evaluate their algorithms using publicly available datasets and standard performance assessment methodologies, disseminate their results, and discuss technical topics in an atmosphere that fosters active exchange of ideas. Members from all segments of the pattern recognition community were invited to submit contest proposals for review.", "num_citations": "170\n", "authors": ["64"]}
{"title": "Mining design patterns from C++ source code\n", "abstract": " Design patterns are micro architectures that have proved to be reliable, easy-to implement and robust. There is a need in science and industry for recognizing these patterns. We present a new method for discovering design patterns in the source code. This method provides a precise specification of how the patterns work by describing basic structural information like inheritance, composition, aggregation and association, and as an indispensable part, by defining call delegation, object creation and operation overriding. We introduce a new XML-based language, the Design Pattern Markup Language (DPML), which provides an easy way for the users to modify pattern descriptions to suit their needs, or even to define their own patterns or just classes in certain relations they wish to find. We tested our method on four open-source systems, and found it effective in discovering design pattern instances.", "num_citations": "161\n", "authors": ["64"]}
{"title": "A probabilistic software quality model\n", "abstract": " In order to take the right decisions in estimating the costs and risks of a software change, it is crucial for the developers and managers to be aware of the quality attributes of their software. Maintainability is an important characteristic defined in the ISO/IEC 9126 standard, owing to its direct impact on development costs. Although the standard provides definitions for the quality characteristics, it does not define how they should be computed. Not being tangible notions, these characteristics are hardly expected to be representable by a single number. Existing quality models do not deal with ambiguity coming from subjective interpretations of characteristics, which depend on experience, knowledge, and even intuition of experts. This research aims at providing a probabilistic approach for computing high-level quality characteristics, which integrate expert knowledge, and deal with ambiguity at the same time. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "133\n", "authors": ["64"]}
{"title": "Clone smells in software evolution\n", "abstract": " Although source code cloning (copy&paste programming) represents a significant threat to the maintainability of a software system, problems usually start to arise only when the system evolves. Most of the related research papers tackle the question of finding code clones in one particular version of the software only, leaving the dynamic behavior of the clones out of consideration. Eliminating these clones in large software systems often seems absolutely hopeless, as there might exist several thousands of them. Alternatively, tracking the evolution of individual clones can be used to identify those occurrences that could really cause problems in the future versions. In this paper we present an approach for mapping clones from one particular version of the software to another one, based on a similarity measure. This mapping is used to define conditions under which clones become suspicious (or \"smelly\") compared to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "129\n", "authors": ["64"]}
{"title": "Survey of code-size reduction methods\n", "abstract": " Program code compression is an emerging research activity that is having an impact in several production areas such as networking and embedded systems. This is because the reduced-sized code can have a positive impact on network traffic and embedded system costs such as memory requirements and power consumption. Although code-size reduction is a relatively new research area, numerous publications already exist on it. The methods published usually have different motivations and a variety of application contexts. They may use different principles and their publications often use diverse notations. To our knowledge, there are no publications that present a good overview of this broad range of methods and give a useful assessment. This article surveys twelve methods and several related works appearing in some 50 papers published up to now. We provide extensive assessment criteria for evaluating\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "124\n", "authors": ["64"]}
{"title": "Design pattern mining enhanced by machine learning\n", "abstract": " Design patterns present good solutions to frequently occurring problems in object-oriented software design. Thus their correct application in a system's design may significantly improve its internal quality attributes such as reusability and maintainability. In software maintenance the existence of up-to-date documentation is crucial, so the discovery of as yet unknown design pattern instances can help improve the documentation. Hence a reliable design pattern recognition system is very desirable. However, simpler methods (based on pattern matching) may give imprecise results due to the vague nature of the patterns' structural description. In previous work we presented a pattern matching-based system using the Columbus framework with which we were able to find pattern instances from the source code by considering the patterns' structural descriptions only, and therefore we could not identify false hits and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "115\n", "authors": ["64"]}
{"title": "Modeling class cohesion as mixtures of latent topics\n", "abstract": " The paper proposes a new measure for the cohesion of classes in object-oriented software systems. It is based on the analysis of latent topics embedded in comments and identifiers in source code. The measure, named as maximal weighted entropy, utilizes the latent Dirichlet allocation technique and information entropy measures to quantitatively evaluate the cohesion of classes in software. This paper presents the principles and the technology that stand behind the proposed measure. Two case studies on a large open source software system are presented. They compare the new measure with an extensive set of existing metrics and use them to construct models that predict software faults. The case studies indicate that the novel measure captures different aspects of class cohesion compared to the existing cohesion measures and improves fault prediction for most metrics, which are combined with maximal\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "101\n", "authors": ["64"]}
{"title": "Extracting facts from open source software\n", "abstract": " Open source software systems are becoming increasingly important these days. Many companies are investing in open source projects and lots of them are also using such software in their own work. But because open source software is often developed without proper management, the quality and reliability of the code may be uncertain. The quality of the code needs to be measured and this can be done only with the help of proper tools. We describe a framework called Columbus with which we calculate the object oriented metrics validated by Basili et al. for illustrating how fault-proneness detection from the open source Web and e-mail suite called Mozilla can be done. We also compare the metrics of several versions of Mozilla to see how the predicted fault-proneness of the software system changed during its development. The Columbus framework has been further developed recently with a compiler wrapping\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "85\n", "authors": ["64"]}
{"title": "Bulk fixing coding issues and its effects on software quality: Is it worth refactoring?\n", "abstract": " The quality of a software system is mostly defined by its source code. Software evolves continuously, it gets modified, enhanced, and new requirements always arise. If we do not spend time periodically on improving our source code, it becomes messy and its quality will decrease inevitably. Literature tells us that we can improve the quality of our software product by regularly refactoring it. But does refactoring really increase software quality? Can it happen that a refactoring decreases the quality? Is it possible to recognize the change in quality caused by a single refactoring operation? In our paper, we seek answers to these questions in a case study of refactoring large-scale proprietary software systems. We analyzed the source code of 5 systems, and measured the quality of several revisions for a period of time. We analyzed 2 million lines of code and identified nearly 200 refactoring commits which fixed over 500\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "63\n", "authors": ["64"]}
{"title": "New conceptual coupling and cohesion metrics for object-oriented systems\n", "abstract": " The paper presents two novel conceptual metrics for measuring coupling and cohesion in software systems. Our first metric, Conceptual Coupling between Object classes (CCBO), is based on the well-known CBO coupling metric, while the other metric, Conceptual Lack of Cohesion on Methods (CLCOM5), is based on the LCOM5 cohesion metric. One advantage of the proposed conceptual metrics is that they can be computed in a simpler (and in many cases, programming language independent) way as compared to some of the structural metrics. We empirically studied CCBO and CLCOM5 for predicting fault-proneness of classes in a large open source system and compared these metrics with a host of existing structural and conceptual metrics for the same task. As the result, we found that the proposed conceptual metrics, when used in conjunction, can predict bugs nearly as precisely as the 58 structural metrics\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "63\n", "authors": ["64"]}
{"title": "A cost model based on software maintainability\n", "abstract": " In this paper we present a maintainability based model for estimating the costs of developing source code in its evolution phase. Our model adopts the concept of entropy in thermodynamics, which is used to measure the disorder of a system. In our model, we use maintainability for measuring disorder (i.e. entropy) of the source code of a software system. We evaluated our model on three proprietary and two open source real world software systems implemented in Java, and found that the maintainability of these evolving software is decreasing over time. Furthermore, maintainability and development costs are in exponential relationship with each other. We also found that our model is able to predict future development costs with high accuracy in these systems.", "num_citations": "52\n", "authors": ["64"]}
{"title": "Constructing control flows graphs of binary executable programs at post-link time\n", "abstract": " A method and a system for constructing a control flow graph (CFG, 106) from an executable computer program (104). The solution detects data intermixed with instructions and instruction set changes. The method includes the steps of defining block leader types specifying basic block boundaries in the program (104), building a CFG structure (106) according to the basic blocks found in the program, and adding control flow and addressing information to the CFG (106) by propagating through the basic blocks and internals thereof. The CFG (106) may be then optimised (108) and a compacted executable (112) created as a result.", "num_citations": "51\n", "authors": ["64"]}
{"title": "Myth or reality? analyzing the effect of design patterns on software maintainability\n", "abstract": " Although the belief of utilizing design patterns to create better quality software is fairly widespread, there is relatively little research objectively indicating that their usage is indeed beneficial.             In this paper we try to reveal the connection between design patterns and software maintainability. We analyzed more than 300 revisions of JHotDraw, a Java GUI framework whose design relies heavily on some well-known design patterns. We used our probabilistic quality model for estimating the maintainability and we parsed the javadoc annotations of the source code for gathering the pattern instances.             We found that every introduced pattern instance caused an improvement in the different quality attributes. Moreover, the average design pattern line density showed a very high, 0.89 Pearson correlation with the estimated maintainability values. Although the amount of available empirical data is still very\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "48\n", "authors": ["64"]}
{"title": "Recognizing Design Patterns in C++ programs with the integration of Columbus and Maisa\n", "abstract": " A method for recognizing design patterns from C++ programs is presented. The method consists of two separate phases, analysis and reverse engineering of the C++ code, and architectural pattern matching over the reverse-engineered intermediate code representation. It is shown how the pattern recognition effect can be realized by integrating two specialized software tools, the reverse engineering framework Columbus and the architectural metrics analyzer Maisa. The method and the integrated power of the tool set are illustrated with small experiments.", "num_citations": "45\n", "authors": ["64"]}
{"title": "A public bug database of github projects and its application in bug prediction\n", "abstract": " Detecting defects in software systems is an evergreen topic, since there is no real world software without bugs. Many different bug locating algorithms have been presented recently that can help to detect hidden and newly occurred bugs in software. Papers trying to predict the faulty source code elements or code segments in the system always use experience from the past. In most of the cases these studies construct a database for their own purposes and do not make the gathered data publicly available. Public datasets are rare; however, a well constructed dataset could serve as a benchmark test input. Furthermore, open-source software development is rapidly increasing that also gives an opportunity to work with public data.               In this study we selected 15 Java projects from GitHub to construct a public bug database from. We matched the already known and fixed bugs with the corresponding source\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "41\n", "authors": ["64"]}
{"title": "Towards a benchmark for evaluating design pattern miner tools\n", "abstract": " Recovering design pattern usage in source code is a very difficult task. Several tools are described in the literature for this purpose, but there is little work invested in evaluating them. The main reason for this is the lack of an approved benchmark for these tools. In this paper we present work in progress towards creating a benchmark, called DEEBEE (design pattern evaluation benchmark environment), for evaluating and comparing design pattern miner tools. It is programming language, tool, pattern and software independent, and it is open to the community and freely available. Currently, the benchmark database contains the results of three tools: Columbus (C++), Maisa (C++), and design pattern detection tool (Java). The tools were evaluated on reference implementations of patterns and on open source software (Mozilla, NotePad++, JHotDraw, JRefactory and JUnit). Additionally, instances recovered by\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "38\n", "authors": ["64"]}
{"title": "BugsJS: a benchmark of JavaScript bugs\n", "abstract": " JavaScript is a popular programming language that is also error-prone due to its asynchronous, dynamic, and loosely-typed nature. In recent years, numerous techniques have been proposed for analyzing and testing JavaScript applications. However, our survey of the literature in this area revealed that the proposed techniques are often evaluated on different datasets of programs and bugs. The lack of a commonly used benchmark limits the ability to perform fair and unbiased comparisons for assessing the efficacy of new techniques. To fill this gap, we propose BugsJS, a benchmark of 453 real, manually validated JavaScript bugs from 10 popular JavaScript server-side programs, comprising 444k LOC in total. Each bug is accompanied by its bug report, the test cases that detect it, as well as the patch that fixes it. BugsJS features a rich interface for accessing the faulty and fixed versions of the programs and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "36\n", "authors": ["64"]}
{"title": "Columbus--tool for reverse engineering large object oriented software systems\n", "abstract": " One of the most critical issues in large-scale software development and maintenance is the rapidly growing size and complexity of the software systems. As a result of this rapid growth there is a need to understand the relationships between the different parts of a large system. In this paper we present a reverse engineering framework called Columbus that is able to analyze large C/C++ projects. Columbus supports project handling, data extraction,-representation,-storage and-export. Ecient ltering methods can be used to produce comprehensible diagrams from the extracted information. The flexible architecture of the Columbus system (based on plug-ins) makes it a really versatile and an easily extendible tool for reverse engineering.", "num_citations": "36\n", "authors": ["64"]}
{"title": "FaultBuster: An automatic code smell refactoring toolset\n", "abstract": " One solution to prevent the quality erosion of a software product is to maintain its quality by continuous refac-toring. However, refactoring is not always easy. Developers need to identify the piece of code that should be improved and decide how to rewrite it. Furthermore, refactoring can also be risky; that is, the modified code needs to be re-tested, so developers can see if they broke something. Many IDEs offer a range of refactorings to support so-called automatic refactoring, but tools which are really able to automatically refactor code smells are still under research. In this paper we introduce FaultBuster, a refactoring toolset which is able to support automatic refactoring: identifying the problematic code parts via static code analysis, running automatic algorithms to fix selected code smells, and executing integrated testing tools. In the heart of the toolset lies a refactoring framework to control the analysis and the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "34\n", "authors": ["64"]}
{"title": "Software product quality models\n", "abstract": " Both for software developers and managers it is crucial to have information about different aspects of the quality of their systems. This chapter gives a brief overview about the history of software product quality measurement, focusing on software maintainability, and the existing approaches and high-level models for characterizing software product quality. The most widely accepted and used practical maintainability models and the state-of-the-art works in the subject are introduced. These models play a very important role in software evolution by allowing to estimate future development costs, assess risks, or support management decisions. Based on objective aspects, the implementations of the most popular software maintainability models are compared and evaluated. The evaluation includes the Quality Index, SQALE, SQUALE, SIG, QUAMOCO, and Columbus Quality Model. The chapter presents the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["64"]}
{"title": "Empirical evaluation of software maintainability based on a manually validated refactoring dataset\n", "abstract": " ContextRefactoring is a technique for improving the internal structure of software systems. It has a solid theoretical background while being used in development practice also. However, we lack empirical research results on the real effect of code refactoring and its application.ObjectiveThis paper presents a manually validated subset of a previously published dataset containing the refactorings extracted by the RefFinder tool, code metrics, and maintainability of 7 open-source systems. We found that RefFinder had around 27% overall average precision on the subject systems, thus our manually validated subset has substantial added value. Using the dataset, we studied several aspects of the refactored and non-refactored source code elements (classes and methods), like the differences in their maintainability and source code metrics.MethodWe divided the source code elements into a group containing the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "32\n", "authors": ["64"]}
{"title": "Empirical study on refactoring large-scale industrial systems and its effects on maintainability\n", "abstract": " Software evolves continuously, it gets modified, enhanced, and new requirements always arise. If we do not spend time occasionally on improving our source code, its maintainability will inevitably decrease. The literature tells us that we can improve the maintainability of a software system by regularly refactoring it. But does refactoring really increase software maintainability? Can it happen that refactoring decreases the maintainability? Empirical studies show contradicting answers to these questions and there have been only a few studies which were performed in a large-scale, industrial context. In our paper, we assess these questions in an in vivo context, where we analyzed the source code and measured the maintainability of 6 large-scale, proprietary software systems in their manual refactoring phase. We analyzed 2.5 million lines of code and studied the effects on maintainability of 315 refactoring commits\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "32\n", "authors": ["64"]}
{"title": "A code refactoring dataset and its assessment regarding software maintainability\n", "abstract": " It is very common in various fields that there is a gap between theoretical results and their practical applications. This is true for code refactoring as well, which has a solid theoretical background while being used in development practice at the same time. However, more and more studies suggest that developers perform code refactoring entirely differently than the theory would suggest. Our paper encourages the further investigation of code refactorings in practice by providing an excessive open dataset of source code metrics and applied refactorings through several releases of 7 open-source systems. As a first step of processing this dataset, we examined the quality attributes of the refactored source code classes and the values of source code metrics improved by those refactorings. Our early results show that lower maintainability indeed triggers more code refactorings in practice and these refactorings\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "32\n", "authors": ["64"]}
{"title": "Extracting Facts with Columbus from C++ Code.\n", "abstract": " Fact extraction from software systems is the fundamental building block in the process of understanding the relationships among the system\u0393\u00c7\u00d6s elements. It is evident that in real life situations manual fact extraction must be supported by software tools which are able to analyze the subject system and provide useful information about it in various forms. These forms are most useful if they adhere to prescribed schemas and this way promote tool interoperability. In this work we outline our solution to tool supported fact extraction, which is built upon the reverse engineering framework Columbus and is supported by schemas for the C++ language. We describe the extraction process in detail and show how the extracted facts can be used in practice by processing the schema instances. We also introduce new features of the Columbus system not published previously, which among others include compiler wrapping and source code auditing.", "num_citations": "32\n", "authors": ["64"]}
{"title": "Qualitygate sourceaudit: A tool for assessing the technical quality of software\n", "abstract": " Software systems are evolving continuously in order to fulfill the ever-changing business needs. This endless modification, however, decreases the internal quality of the system over time. This phenomena is called software erosion, which results in higher development, testing, and operational costs. The SourceAudit tool presented in this paper helps managing the technical risks of software deterioration by allowing imme-diate, automatic, and objective assessment of software quality. By monitoring the high-level technical quality of systems it is possible to immediately perform the necessary steps needed to reduce the effects of software erosion, thus reaching higher maintainability and lower costs in the mid and long-term. The tool measures source code maintainability according to the ISO/IEC 25010 based probabilistic software maintainability model called ColumbusQM. It gives a holistic view on software quality\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "30\n", "authors": ["64"]}
{"title": "A drill-down approach for measuring maintainability at source code element level\n", "abstract": " Measuring source code maintainability has always been a challenge for software engineers. To address this problem, a number of metrics-based quality models have been proposed by researchers. Besides expressing source code maintainability in terms of numerical values, these models are also expected to provide explicable results, ie to give a detailed list of source code fragments that should be improved in order to reach higher overall quality.", "num_citations": "27\n", "authors": ["64"]}
{"title": "Evaluating C++ design pattern miner tools\n", "abstract": " Many articles and tools have been proposed over the years for mining design patterns from source code. These tools differ in several aspects, thus their fair comparison is hard. Besides the basic methodology, the main differences are that the tools operate on different representations of the subject system and that the pattern definitions differ as well. In this paper we first provide a common measurement platform for three well-known pattern mining systems, Columbus, Maisa and CrocoPat. Then we compare these tools on four C++ open-source systems: DC++,WinMerge, Jikes and Mozilla. Columbus can discover patterns from the C++ source code itself, while Maisa and CrocoPat require the representation of a software system in a special textual format, so we extended Columbus to provide the common input for the two other tools. We compared these tools in terms of speed, memory consumption and the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["64"]}
{"title": "Performance comparison of query-based techniques for anti-pattern detection\n", "abstract": " ContextProgram queries play an important role in several software evolution tasks like program comprehension, impact analysis, or the automated identification of anti-patterns for complex refactoring operations. A central artifact of these tasks is the reverse engineered program model built up from the source code (usually an Abstract Semantic Graph, ASG), which is traditionally post-processed by dedicated, hand-coded queries.ObjectiveOur paper investigates the costs and benefits of using the popular industrial Eclipse Modeling Framework (EMF) as an underlying representation of program models processed by four different general-purpose model query techniques based on native Java code, OCL evaluation and (incremental) graph pattern matching.MethodWe provide in-depth comparison of these techniques on the source code of 28 Java projects using anti-pattern queries taken from refactoring operations in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["64"]}
{"title": "A public unified bug dataset for java\n", "abstract": " Background: Bug datasets have been created and used by many researchers to build bug prediction models.Aims: In this work we collected existing public bug datasets and unified their contents.Method: We considered 5 public datasets which adhered to all of our criteria. We also downloaded the corresponding source code for each system in the datasets and performed their source code analysis to obtain a common set of source code metrics. This way we produced a unified bug dataset at class and file level that is suitable for further research (eg to be used in the building of new bug prediction models). Furthermore, we compared the metric definitions and values of the different bug datasets.Results: We found that (i) the same metric abbreviation can have different definitions or metrics calculated in the same way can have different names,(ii) in some cases different tools give different values even if the metric\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["64"]}
{"title": "A case study of refactoring large-scale industrial systems to efficiently improve source code quality\n", "abstract": " Refactoring source code has many benefits (e.g. improving maintainability, robustness and source code quality), but it takes time away from other implementation tasks, resulting in developers neglecting refactoring steps during the development process. But what happens when they know that the quality of their source code needs to be improved and they can get the extra time and money to refactor the code? What will they do? What will they consider the most important for improving source code quality? What sort of issues will they address first or last and how will they solve them? In our paper, we look for answers to these questions in a case study of refactoring large-scale industrial systems where developers participated in a project to improve the quality of their software systems. We collected empirical data of over a thousand refactoring patches for 5 systems with over 5 million lines of code in total, and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["64"]}
{"title": "Source code metrics and maintainability: a case study\n", "abstract": " Measuring high level quality attributes of operation-critical IT systems is essential for keeping the maintainability costs under control. International standards and recommendations, like ISO/IEC 9126, give some guidelines regarding the different quality characteristics to be assessed, however, they do not define unambiguously their relationship to the low level quality attributes. The vast majority of existing quality models use source code metrics for measuring low level quality attributes. Although, a lot of researches analyze the relation of source code metrics to other objective measures, only a few studies deal with their expressiveness of subjective feelings of IT professionals. Our research involved 35 IT professionals and manual evaluation results of 570 class methods of an industrial and an open source Java system. Several statistical models have been built to evaluate the relation of low level source code\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["64"]}
{"title": "Anti-pattern detection with model queries: A comparison of approaches\n", "abstract": " Program queries play an important role in several software evolution tasks like program comprehension, impact analysis, or the automated identification of anti-patterns for complex refactoring operations. A central artifact of these tasks is the reverse engineered program model built up from the source code (usually an Semantic Graph, ASG), which is traditionally post-processed by dedicated, hand-coded queries. Our paper investigates the use of the popular industrial Eclipse Modeling Framework (EMF) as an underlying representation of program models processed by three general-purpose model query techniques based on native Java code, local-search and incremental evaluation. We provide in-depth comparison of these techniques on the source code of 17 Java projects using queries taken from refactoring operations in different usage profiles. Our results show that general purpose model queries outperform\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["64"]}
{"title": "Solutions for reverse engineering 4GL applications, recovering the design of a logistical wholesale system\n", "abstract": " Re-engineering a legacy software system to support new, modern technologies instead of old ones is not an easy task, especially for large systems with a complex architecture. The use of reverse engineering tools is crucial for different subtasks of the full process, such as re-documenting the old code or recovering its design. There are many tools available to assist developers, but most of these tools were designed to deal with third generation languages (e.g. Java, C, C++, C#). However, many large systems are developed in higher level languages (e.g. Magic, Informix, ABAP) and current tools are not able to support all the arising problems during re-engineering systems written in fourth generation languages. In this paper we present a project whose main goal is the development of a technologically and functionally renewed medicinal wholesale system. This system is developed in Magic 4GL, and its\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["64"]}
{"title": "Method and a device for abstracting instruction sequences with tail merging\n", "abstract": " A method and a device for abstracting instruction sequences in a computer program. First, a control flow graph of the program is generated and analysed in order to detect multiple occurrences of a same instruction sequence (504, 506). Then, a function including the longest sequence common to at least two instruction sequences from a plurality of sequences having a common instruction sequence of equal or shorter length compared to the longest sequence is created (512). Finally, the original occurrences of the instruction sequences in the plurality of sequences with a reference to a proper position in the newly created function are deleted and a reference to a proper position in the created function inserted instead (514).", "num_citations": "19\n", "authors": ["64"]}
{"title": "Do automatic refactorings improve maintainability? An industrial case study\n", "abstract": " Refactoring is often treated as the main remedy against the unavoidable code erosion happening during software evolution. Studies show that refactoring is indeed an elemental part of the developers' arsenal. However, empirical studies about the impact of refactorings on software maintainability still did not reach a consensus. Moreover, most of these empirical investigations are carried out on open-source projects where distinguishing refactoring operations from other development activities is a challenge in itself. We had a chance to work together with several software development companies in a project where they got extra budget to improve their source code by performing refactoring operations. Taking advantage of this controlled environment, we collected a large amount of data during a refactoring phase where the developers used a (semi)automatic refactoring tool. By measuring the maintainability of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "18\n", "authors": ["64"]}
{"title": "A manually validated code refactoring dataset and its assessment regarding software maintainability\n", "abstract": " Refactoring is a popular technique for improving the internal structure of software systems. It has a solid theoretical background while being used in development practice at the same time. However, we lack empirical research results on the real effect of code refactoring and its ways of application.", "num_citations": "17\n", "authors": ["64"]}
{"title": "Macro impact analysis using macro slicing\n", "abstract": " The expressiveness of the C/C++ preprocessing facility enables the development of highly configurable source code. However, the usage of language constructs like macros also bears the potential of resulting in highly incomprehensible and unmaintainable code, which is due to the flexibility and the \u0393\u00c7\u00a3cryptic\u0393\u00c7\u00a5 nature of the preprocessor language. This could be overcome if suitable analysis tools were available for preprocessor-related issues, however, this is not the case (for instance, none of the modern Integrated Development Environments provides features to efficiently analyze and browse macro usage). A conspicuous problem in software maintenance is the correct (safe and efficient) management of change. In particular, due to the aforementioned reasons, determining efficiently the impact of a change in a specific macro definition is not yet possible. In this paper, we describe a method for the impact analysis of macro definitions, which significantly differs from the previous approaches. We reveal and analyze the dependencies among macro-related program points using the so-called macro slices.", "num_citations": "16\n", "authors": ["64"]}
{"title": "Designing and developing automated refactoring transformations: An experience report\n", "abstract": " There are several challenges which should be kept in mind during the design and development phases of a refactoring tool, and one is that developers have several expectations that are quite hard to satisfy. In this report, we present our experiences of a two-year project where we attempted to create an automatic refactoring tool. In this project, we worked with five software development companies that wanted to improve the maintainability of their products. The project was designed to take into account the expectations of the developers of these companies and consisted of three main stages: a manual refactoring phase, a tool building phase, and an automatic refactoring phase. Throughout these stages we collected the opinions of the developers and faced several challenges on how to automate refactoring transformations, which we present and summarize.", "num_citations": "14\n", "authors": ["64"]}
{"title": "Recognizing antipatterns and analyzing their effects on software maintainability\n", "abstract": " Similarly to design patterns and their inherent extra information about the structure and design of a system, antipatterns \u0393\u00c7\u00f4 or bad code smells \u0393\u00c7\u00f4 can also greatly influence the quality of software. Although the belief that they negatively impact maintainability is widely accepted, there are still relatively few objective results that would support this theory.               In this paper we show our approach of detecting antipatterns in source code by structural analysis and use the results to reveal connections among antipatterns, number of bugs, and maintainability. We studied\u252c\u00e1228 open-source Java based systems and extracted bug-related information for 34 of them from the PROMISE database. For estimating the maintainability, we used the ColumbusQM probabilistic quality model.               We found that there is a statistically significant, 0.55 Spearman correlation between the number of bugs and the number of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["64"]}
{"title": "The impact of version control operations on the quality change of the source code\n", "abstract": " The number of software systems under development and maintenance is rapidly increasing. The quality of a system\u0393\u00c7\u00d6s source code tends to decrease during its lifetime which is a problem because maintaining low quality code consumes a big portion of the available efforts. In this research we investigated one aspect of code change, the version control commit operations (add, update, delete). We studied the impact of these operations on the maintainability of the code. We calculated the ISO/IEC\u252c\u00e19126 quality attributes for thousands of revisions of an industrial and three open-source software systems. We also collected the cardinality of each version control operation type for every investigated revision. Based on these data, we identified that operation Add has a rather positive, while operation Update has a rather negative effect on the quality. On the other hand, for operation Delete we could not find a clear\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["64"]}
{"title": "MAGISTER: Quality assurance of Magic applications for software developers and end users\n", "abstract": " Nowadays there are many tools and methods available for source code quality assurance based on static analysis, but most of these tools focus on traditional software development techniques with 3GL languages. Besides procedural languages, 4GL programming languages such as Magic 4GL and Progress are widely used for application development. All these languages lie outside the main scope of analysis techniques. In this paper we present MAGISTER, which is a quality assurance framework for applications being developed in Magic, a 4GL application development solution created by Magic Software Enterprises. MAGISTER extracts data using static analysis methods from applications being developed in different versions of Magic (v5-9 and uniPaaS). The extracted data (including metrics, rule violations and dependency relations) is presented to the user via a GUI so it can be queried and visualized for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["64"]}
{"title": "Towards a benchmark for evaluating reverse engineering tools\n", "abstract": " In this paper we present work in progress towards implementing a benchmark called BEFRIEND (benchmark for reverse engineering tools working on source code), with which the outputs of reverse engineering tools can be evaluated and compared easily and efficiently. Such tools are e.g. design pattern miners, duplicated code detectors and coding rule violation checkers. BEFRIEND supports different kinds of tool families, programming languages and software systems, and it enables the users to define their own evaluation criteria.", "num_citations": "14\n", "authors": ["64"]}
{"title": "Impact of version history metrics on maintainability\n", "abstract": " In this study we present how some version control history based metrics affect maintainability of the source code. These metrics cover intensity of modifications, code ownership and code aging. We determine the order of source files based on each analyzed metrics, and compare it with their maintainability based order. As a cross-check we perform a comparison test with post-release defects as well. We performed the analysis on 14 versions of 4 well-known open source software systems. The results show high correlation between the version control metrics and relative maintainability indexes, in each case. The comparison with post-release defects also support the results in most of the cases.", "num_citations": "13\n", "authors": ["64"]}
{"title": "Cumulative code churn: Impact on maintainability\n", "abstract": " It is a well-known phenomena that the source code of software systems erodes during development, which results in higher maintenance costs in the long term. But can we somehow narrow down where exactly this erosion happens? Is it possible to infer the future erosion based on past code changes? Do modifications performed on frequently changing code have worse effect on software maintainability than those affecting less frequently modified code? In this study we investigated these questions and the results indicate that code churn indeed increases the pace of code erosion. We calculated cumulative code churn values and maintainability changes for every version control commit operation of three open-source and one proprietary software system. With the help of Wilcoxon rank test we compared the cumulative code churn values of the files in commits resulting maintainability increase with those of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["64"]}
{"title": "Complexity measures in 4GL environment\n", "abstract": " Nowadays, the most popular programming languages are so-called third generation languages, such as Java, C# and C++, but higher level languages are also widely used for application development. Our work was motivated by the need for a quality assurance solution for a fourth generation language (4GL) called Magic. We realized that these very high level languages lie outside the main scope of recent static analysis techniques and researches, even though there is an increasing need for solutions in 4GL environment.               During the development of our quality assurance framework we faced many challenges in adapting metrics from popular 3GLs and defining new ones in 4GL context. Here we present our results and experiments focusing on the complexity of a 4GL system. We found that popular 3GL metrics can be easily adapted based on syntactic structure of a language, however it requires\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["64"]}
{"title": "CSiBE benchmark: One year perspective and plans\n", "abstract": " In this paper we summarize our experiences in designing and running CSiBE, the new code size benchmark for GCC. Since its introduction in 2003, it has been widely used by GCC developers in their daily work to help them keep the size of the generated code as small as possible. We have been making continuous observations on the latest results and informing GCC developers of any problem when necessary. We overview some concrete \u0393\u00c7\u00a3success stories\u0393\u00c7\u00a5 of where GCC benefited from the benchmark. This paper overviews the measurement methodology, providing some information about the test bed, the measuring method, and the hardware/software infrastructure. The new version of CSiBE, launched in May 2004, has been extended with new features such as code performance measurements and a test bed\u0393\u00c7\u00f6four times larger\u0393\u00c7\u00f6with even more versatile programs.", "num_citations": "13\n", "authors": ["64"]}
{"title": "Code ownership: Impact on maintainability\n", "abstract": " Software systems erode during development, which results in high maintenance costs in the long term. Is it possible to narrow down where exactly this erosion happens? Can we infer the future erosion based on past code changes?                 In this paper we investigate code ownership and show that a further step of code quality decrease is more likely to happen due to the changes in source files modified by several developers in the past, compared to files with clear ownership. We estimate the level of code ownership and maintainability changes for every commit of three open-source and one proprietary software systems. With the help of Wilcoxon rank test we compare the ownership values of the files in commits resulting maintainability increase with those of decreasing the maintainability. Three tests out of the four gave strong results and the fourth one did not contradict them either. The conclusion of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["64"]}
{"title": "Characterization of source code defects by data mining conducted on GitHub\n", "abstract": " In software systems the coding errors are unavoidable due to the frequent source changes, the tight deadlines and the inaccurate specifications. Therefore, it is important to have tools that help us in finding these errors. One way of supporting bug prediction is to analyze the characteristics of the previous errors and identify the unknown ones based on these characteristics. This paper aims to characterize the known coding errors.                 Nowadays, the popularity of the source code hosting services like GitHub are increasing rapidly. They provide a variety of services, among which the most important ones are the version and bug tracking systems. Version control systems store all versions of the source code, and bug tracking systems provide a unified interface for reporting errors. Bug reports can be used to identify the wrong and the previously fixed source code parts, thus the bugs can be characterized by\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["64"]}
{"title": "Towards building method level maintainability models based on expert evaluations\n", "abstract": " The maintainability of software systems is getting more and more attention both from researchers and industrial experts. This is due to its direct impact on development costs and reliability of the software.             Many models exist for estimating maintainability by aggregating low level source code metrics. However, very few of them are able to predict the maintainability on method level; even fewer take subjective human opinions into consideration. In this paper we present a new approach to create method level maintainability prediction models based on human surveys using regression techniques.             We performed three different surveys and compared the derived prediction models. Our regression models were built based on approximately 150000 answers of 268 persons. These models were able to estimate the maintainability of methods with a 0.72 correlation and a 0.83 mean absolute error on a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["64"]}
{"title": "Static javascript call graphs: A comparative study\n", "abstract": " The popularity and wide adoption of JavaScript both at the client and server side makes its code analysis more important than ever before. Most of the algorithms for vulnerability analysis, coding issue detection, or type inference rely on the call graph representation of the underlying program. Despite some obvious advantages of dynamic analysis, static algorithms should also be considered for call graph construction as they do not require extensive test beds for programs and their costly execution and tracing. In this paper, we systematically compare five widely adopted static algorithms - implemented by the npm call graph, IBM WALA, Google Closure Compiler, Approximate Call Graph, and Type Analyzer for JavaScript tools - for building JavaScript call graphs on 26 WebKit SunSpider benchmark programs and 6 real-world Node.js modules. We provide a performance analysis as well as a quantitative and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["64"]}
{"title": "Connection between version control operations and quality change of the source code\n", "abstract": " Software erosion is a well-known phenomena, meaning that software quality is continuously decreasing due to the ever-ongoing modifications in the source code. In this research work we investigated this phenomena by studying the impact of version control commit operations (add, update, delete) on the quality of the code. We calculated the ISO/IEC 9126 quality attributes for thousands of revisions of an industrial and three open-source software systems with the help of the Columbus Quality Model. We also collected the cardinality of each version control operation type for every investigated revision. We performed Chisquared tests on contingency tables with rows of quality change and columns of version control operation commit types. We compared the results with random data as well. We identified that the relationship between the version control operations and quality change is quite strong. Great maintainability improvements are mostly caused by commits containing Add operation. Commits containing file updates only tend to have a negative impact on the quality. Deletions have a weak connection with quality, and we could not formulate a general statement.", "num_citations": "11\n", "authors": ["64"]}
{"title": "Modelling and reverse engineering C++ source code\n", "abstract": " This thesis discusses the concepts of reverse engineering C++ source code. Reverse engineering is defined as the process of analyzing a software system to create some kind of representation for it at a higher level of abstraction. In this work we are, among other things, engaged in creating a so-called model representation of the source code. The format of this model is prescribed by a schema whose design is one of the main results of this work. We also present various methods with which the process of reverse engineering C++ source code can be carried out with relative ease. The second part of the thesis deals with some important utilizations of the generated software model. First, we present two methods for recognizing design pattern instances in C++ code. Second, we describe a work carried out to assess the quality of open source software by predicting its fault-proneness.Acknowledgements.", "num_citations": "11\n", "authors": ["64"]}
{"title": "A semi-automatic usability evaluation framework\n", "abstract": " Most of the software maintenance costs come from usability bugs reported after the release and deployment. A usability bug is really subjective, hence there is a large communication overhead between the end user and the developer. Moreover, the reputation of the software development company could be decreased as well. Therefore, proactively testing and maintaining software systems from a usability point of view is unambiguously beneficial.             In this paper we propose a research prototype, the Usability Evaluation Framework. The development of the framework is driven by welldefined requirements. It is built upon a usability model, it calculates usability metrics, it integrates questionnaires and it also ensures several meaningful reports. We have successfully applied the framework to evaluate and to improve the usability of two industrial software systems.", "num_citations": "10\n", "authors": ["64"]}
{"title": "Deep learning in static, metric-based bug prediction\n", "abstract": " Our increasing reliance on software products and the amount of money we spend on creating and maintaining them makes it crucial to find bugs as early and as easily as possible. At the same time, it is not enough to know that we should be paying more attention to bugs; finding them must become a quick and seamless process in order to be actually used by developers. Our proposal is to revitalize static source code metrics \u0393\u00c7\u00f4 among the most easily calculable, while still meaningful predictors \u0393\u00c7\u00f4 and combine them with deep learning \u0393\u00c7\u00f4 among the most promising and generalizable prediction techniques \u0393\u00c7\u00f4 to flag suspicious code segments at the class level. In this paper, we show a detailed methodology of how we adapted deep neural networks to bug prediction, applied them to a large bug dataset (containing 8780 bugged and 38,838 not bugged Java classes), and compared them to multiple \u0393\u00c7\u00a3traditional\u0393\u00c7\u00a5 algorithms\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["64"]}
{"title": "Challenging machine learning algorithms in predicting vulnerable javascript functions\n", "abstract": " The rapid rise of cyber-crime activities and the growing number of devices threatened by them place software security issues in the spotlight. As around 90% of all attacks exploit known types of security issues, finding vulnerable components and applying existing mitigation techniques is a viable practical approach for fighting against cyber-crime. In this paper, we investigate how the state-of-the-art machine learning techniques, including a popular deep learning algorithm, perform in predicting functions with possible security vulnerabilities in JavaScript programs. We applied 8 machine learning algorithms to build prediction models using a new dataset constructed for this research from the vulnerability information in public databases of the Node Security Project and the Snyk platform, and code fixing patches from GitHub. We used static source code metrics as predictors and an extensive grid-search algorithm to find\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["64"]}
{"title": "BEFRIEND-a benchmark for evaluating reverse engineering tools\n", "abstract": " Reverse engineering tools analyze the source code of a software system and produce various results, which usually point back to the original source code. Such tools are eg design pattern miners, duplicated code detectors and coding rule violation checkers. Most of the time these tools present their results in different formats, which makes them very difficult to compare. In this paper, we present work in progress towards implementing a benchmark called BEFRIEND (BEnchmark For Reverse engInEering tools workiNg on source coDe) with which the outputs of reverse engineering tools can be easily and efficiently evaluated and compared. It supports different kinds of tool families, programming languages and software systems, and it enables the users to define their own evaluation criteria. Furthermore, it is a freely available web-application open to the community. We hope that in the future it will be accepted and used by the community members to evaluate and compare their tools with each other.", "num_citations": "9\n", "authors": ["64"]}
{"title": "An automatically created novel bug dataset and its validation in bug prediction\n", "abstract": " Bugs are inescapable during software development due to frequent code changes, tight deadlines, etc.; therefore, it is important to have tools to find these errors. One way of performing bug identification is to analyze the characteristics of buggy source code elements from the past and predict the present ones based on the same characteristics, using e.g. machine learning models. To support model building tasks, code elements and their characteristics are collected in so-called bug datasets which serve as the input for learning.We present the BugHunter Dataset: a novel kind of automatically constructed and freely available bug dataset containing code elements (files, classes, methods) with a wide set of code metrics and bug information. Other available bug datasets follow the traditional approach of gathering the characteristics of all source code elements (buggy and non-buggy) at only one or more pre-selected\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["64"]}
{"title": "Prediction models for performance, power, and energy efficiency of software executed on heterogeneous hardware\n", "abstract": " Heterogeneous computer environments are becoming commonplace so it is increasingly important to understand how and where we could execute a given algorithm the most efficiently. In this paper we propose a methodology that uses both static source code metrics, and dynamic execution time, power, and energy measurements to build gain ratio prediction models. These models are trained on special benchmarks that have both sequential and parallel implementations and can be executed on various computing elements, e.g., on CPUs, GPUs, or FPGAs. After they are built, however, they can be applied to a new system using only the system\u0393\u00c7\u00d6s static source code metrics which are much more easily computable than any dynamic measurement. We found that while estimating a continuous gain ratio is a much harder problem, we could predict the gain category (e.g., \u0393\u00c7\u00a3slight improvement\u0393\u00c7\u00a5 or \u0393\u00c7\u00a3large\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["64"]}
{"title": "A software quality model for RPG\n", "abstract": " The IBM i mainframe was designed to manage business applications for which the reliability and quality is a matter of national security. The RPG programming language is the most frequently used one on this platform. The maintainability of the source code has big influence on the development costs, probably this is the reason why it is one of the most attractive, observed and evaluated quality characteristic of all. For improving or at least preserving the maintainability level of software it is necessary to evaluate it regularly. In this study we present a quality model based on the ISO/IEC 25010 international standard for evaluating the maintainability of software systems written in RPG. As an evaluation step of the quality model we show a case study in which we explain how we integrated the quality model as a continuous quality monitoring tool into the business processes of a mid-size software company which has\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["64"]}
{"title": "A true story of refactoring a large oracle PL/SQL banking system\n", "abstract": " It is common that due to the pressure of business, banking systems evolve and grow fast and even the slightest wrong decision may result in losing control over the codebase in long term. Once it happens, the business will not drive developments any more, but will be constrained by maintenance preoccupations. As easy is to lose control, as hard is to regain it again. Software comprehension and refactoring are the proper means for reestablishing governance over the system, but they require sophisticated tools and methods that help analyzing, understanding and refactoring the codebase. This paper tells a true story about how control has been lost and regained again in case of a real banking system written in PL/SQL programming language.", "num_citations": "8\n", "authors": ["64"]}
{"title": "Visualization of software architecture graphs of java systems: managing propagated low level dependencies\n", "abstract": " The availability of up-to-date documentation of the architecture is crucial for software maintenance tasks, but it is often missing or differs from the implemented architecture. An increasingly popular and feasible way to get a clear picture of the architecture is to reconstruct it from the source code. The result of the reconstruction procedure is a graph with special, architecture-specific properties. Nowadays software systems are typically very large, so the reconstructed architecture contains a lot of details and is really difficult to interpret. It is important therefore to have efficient methods that help in understanding and managing the architecture graph. The purpose of these methods is to try to present the information so that it is comprehensible to the users. Two important methods are selective subtree collapsion and lifting low level dependencies of the system into higher, visible levels. These enable an architect to investigate\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["64"]}
{"title": "Assessment of the Code Refactoring Dataset Regarding the Maintainability of Methods\n", "abstract": " Code refactoring has a solid theoretical background while being used in development practice at the same time. However, previous works found controversial results on the nature of code refactoring activities in practice. Both their application context and impact on code quality needs further examination.               Our paper encourages the investigation of code refactorings in practice by providing an excessive open dataset of source code metrics and applied refactorings through several releases of 7 open-source systems. We already demonstrated the practical value of the dataset by analyzing the quality attributes of the refactored source code classes and the values of source code metrics improved by those refactorings.               In this paper, we have gone one step deeper and explored the effect of code refactorings at the level of methods. We found that similarly to class level, lower maintainability indeed\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["64"]}
{"title": "Bug Forecast: A method for automatic bug prediction\n", "abstract": " In this paper we present an approach and a toolset for automatic bug prediction during software development and maintenance. The toolset extends the Columbus source code quality framework, which is able to integrate into the regular builds, analyze the source code, calculate different quality attributes like product metrics and bad code smells; and monitor the changes of these attributes. The new bug forecast toolset connects to the bug tracking and version control systems and assigns the reported and fixed bugs to the source code classes from the past. It then applies machine learning methods to learn which values of which quality attributes typically characterized buggy classes. Based on this information it is able to predict bugs in current and future versions of the classes.             The toolset was evaluated on an industrial software system developed by a large software company called evosoft. We\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["64"]}
{"title": "Towards portable metrics-based models for software maintenance problems\n", "abstract": " The usage of software metrics for various purposes has become a hot research topic in academia and industry (e.g. detecting design patterns and bad smells, studying change-proneness, quality and maintainability, predicting faults). Most of these topics have one thing in common: they are all using some kind of metrics-based models to achieve their goal. Unfortunately, only few researchers have tested these models on unknown software systems so far. This paper tackles the question, which metrics are suitable for preparing portable models (which can be efficiently applied to unknown software systems). We have assessed several metrics on four large software systems and we found that the well-known RFC and WMC metrics differentiate the analyzed systems fairly well. Consequently, these metrics cannot be used to build portable models, while the CBO, LCOM and LOC metrics behave similarly on all systems\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["64"]}
{"title": "A public unified bug dataset for java and its assessment regarding metrics and bug prediction\n", "abstract": " Bug datasets have been created and used by many researchers to build and validate novel bug prediction models. In this work, our aim is to collect existing public source code metric-based bug datasets and unify their contents. Furthermore, we wish to assess the plethora of collected metrics and the capabilities of the unified bug dataset in bug prediction. We considered 5 public datasets and we downloaded the corresponding source code for each system in the datasets and performed source code analysis to obtain a common set of source code metrics. This way, we produced a unified bug dataset at class and file level as well. We investigated the diversion of metric definitions and values of the different bug datasets. Finally, we used a decision tree algorithm to show the capabilities of the dataset in bug prediction. We found that there are statistically significant differences in the values of the original and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["64"]}
{"title": "Deep-water framework: The Swiss army knife of humans working with machine learning models\n", "abstract": " Working with machine learning models has become an everyday task not only for software engineers, but for a much wider spectrum of researchers and professionals. Training such models involves finding the best learning methods and their best hyper-parameters for a specific task, keeping track of the achieved performance measures, comparing the results visually, etc. If we add feature extraction methods \u0393\u00c7\u00f4 that precede the learning phase and depend on many hyper-parameters themselves \u0393\u00c7\u00f4 into the mixture, like source code embedding that is quite common in the field of software analysis, the task cries out for supporting tools. We propose a framework called Deep-Water that works similarly to a configuration management tool in the area of software engineering. It supports defining arbitrary feature extraction and learning methods for an input dataset and helps in executing all the training tasks with different\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["64"]}
{"title": "Development of a Methodology, Software--Suite and Service for Supporting Software Architecture Reconstruction\n", "abstract": " Having an up-to-date knowledge of the architecture of a software system is of primary importance, since it affects every aspect of software development. It aids under-standing the system, helps defining high level conditions and constraints for making decisions, supports dependency analysis, logical grouping of components, evaluation of high level design, etc. During the evolution of a software, the documentation of its architecture may not be maintained because of the strict deadlines, resulting in an increasing gap between the architectural design and implementation. The national grant project named GOP-1.1.1-07/1-2008-0077 sponsored by the New Hungarian Development Plan, supports the development of appropriate tools for automatic architecture reconstruction and reverse engineering of software systems. The project will result in a complex solution for automatic architecture reconstruction of software\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["64"]}
{"title": "Designing and implementing control flow graph for Magic 4th generation language\n", "abstract": " A good compiler which implements many optimizations during its compilation phases must be able to perform several static analysis techniques such as control flow or data flow analysis. Besides compilers, these techniques are common for static analyzers as well to retrieve information from source code, for example for code auditing, quality assurance or testing purposes. Implementing control flow analysis requires handling many special structures of the target language. In our paper we present our experiences in implementing control flow graph (CFG) construction for a special 4th generation language called Magic. While we were designing and implementing the CFG for this language, we identified differences compared to 3rd generation languages mostly because of the unique programming technique of Magic (eg data access, parallel task execution, events). Our work was motivated by our industrial partner who needed precise static analysis tools (eg for quality assurance or testing purposes) for this language. We believe that our experiences for Magic, as a representative of 4GLs, might be generalized for other languages too.", "num_citations": "4\n", "authors": ["64"]}
{"title": "A retrospective view of software maintenance and reengineering research\u0393\u00c7\u00f4a selection of papers from European Conference on Software Maintenance and Reengineering 2010\n", "abstract": " As a summary of past, current, and future trends in software maintenance and reengineering research, we give in this editorial a retrospective look from the past 14\u0393\u00c7\u00ebyears to now. We provide insight on how software maintenance has evolved and on the most important research topics presented in the series of the European Conference on Software Maintenance and Reengineering. Copyright \u252c\u2310 2011 John Wiley & Sons, Ltd.", "num_citations": "4\n", "authors": ["64"]}
{"title": "Runtime exception detection in java programs using symbolic execution\n", "abstract": " On behalf of the steering and p1 \u0393\u00c7\u00ffOg'l \u0393\u00c7\u00ff\u2229\u00bc\u00e9l11 comniittees, welcome to the 13th S_'y'111-pOI: lLll1L on P1 \u0393\u00c7\u00ffOg'l \u0393\u00c7\u00ff{1111ll1ll1g Lziiigiiages and Software Tools (SPLST13). The series stmtccl in 1989 in Szeged, l-liiiignry, and since then, by tradition, it has been or\u0393\u00c7\u00f6gaiiized every second yeaiv in HLLllgtLl'\u252c\u00ba \u0393\u00c7\u00d6, Finland, and Estonia, T with participants coniiiig from all over Europe. This year, the thirteenth edition of the syiiiposiiim is hark agaiii in Szeged on August 26~\u0393\u00c7\u00ff2T\\2013. The purpose of the Syiiiposirnn on Progrziniming Lziiigriiiges and Software Tools is to provide fl for1_1ni for soi\" tWa1'e scieiitists to present and discuss i'e0eiit reseaiwblies and clevelopineiits in romputer science. The scope of the syniposinin covers ongoing r \u0393\u00c7\u00ffrch related to p1 \u0393\u00c7\u00ff() gI'\u2229\u00bc\u00fc111l11ll1'l \u0393\u00c7\u00ff< 1l1gLl \u0393\u00c7\u00ff<'L \u0393\u00c7\u00ff\u252c\u00f3{6S \u0393\u00c7\u00ffsoftW: ire tools, and methods for SOlvl/\\VHl \u0393\u00c7\u00ff(* developiiient.", "num_citations": "4\n", "authors": ["64"]}
{"title": "A layout independent GUI test automation tool for applications developed in Magic/uniPaaS\n", "abstract": " A good software development process involves thorough testing phases, that are usually expensive, but necessary to deliver a reliable and high quality product. Testing an application via its graphical user interface requires lots of manual work, even if some steps of GUI testing can be automated. Test automation tools are a great help for testers, particularly for regression tests. However these tools still lack some important features and still require manual work to maintain the test cases. For instance, if the layout of a window is changed without affecting the main functionality of the application, all test cases testing the window must be re-recorded again. This hard maintenance work is one of the greatest problems with the regression tests of GUI applications. In our paper we propose an approach to use the GUI information stored in the source code during automatic testing processes to create layout independent test scripts. With this technique, the already recorded tests scripts will be unaffected by minor changes in the GUI. It reduces the maintenance effort of very expensive regression tests where thousands of test cases have to be maintained by testing teams. The idea was motivated by testing an application developed in a fourth generation language, Magic/uniPaaS. In this language the layout of the GUI elements (structure of the window, position and size of controls, etc.) are stored in the code and it can be gathered via static code analysis. We implemented the presented approach for Magic/uniPaaS, and our Magic Test Automation tool is used by our industrial partner who has developed applications in Magic/uniPaaS for more than a decade.", "num_citations": "4\n", "authors": ["64"]}
{"title": "Comparing and Evaluating Design Pattern Miner Tools\n", "abstract": " Several tools are published in the literature which are able to mine design pattern usage from source code. Because a common test database\u0393\u00c7\u00f4a benchmark\u0393\u00c7\u00f4is not available, the accuracy of the tools is difficult to check and measuring any kind of improvements on the tools is also problematic. As an all-in-one solution we have developed a benchmark for evaluating and comparing design pattern miner tools and for ensuring a test database for them.In this paper we present some experiments performed with the benchmark. Two design pattern miner tools\u0393\u00c7\u00f4Columbus and Maisa\u0393\u00c7\u00f4are evaluated and compared. The tools are evaluated on C++ reference implementations of design patterns, on a real software system called NotePad++ and on FormulaManager, which is a software implemented by us to have a test case where the usage of design patterns is well defined and documented. Design pattern instances from NotePad++ recovered by professional software developers are also added to the benchmark.", "num_citations": "4\n", "authors": ["64"]}
{"title": "From C++ refactorings to graph transformations\n", "abstract": " In this paper, we study a metamodel for the C++ programming language. We work out refactorings on the C++ metamodel and present the essentials as graph transformations. The refactorings are demonstrated in terms of the C++ source code and the C++ target code as well. Graph transformations allow to capture refactoring details on a conceptual and easy to understand, but also very precise level. Using this approach we managed to formalize two major aspects of refactorings: the structural changes and the preconditions.", "num_citations": "4\n", "authors": ["64"]}
{"title": "BUGSJS: a benchmark and taxonomy of JavaScript bugs\n", "abstract": " JavaScript is a popular programming language that is also error\u0393\u00c7\u00c9prone due to its asynchronous, dynamic, and loosely typed nature. In recent years, numerous techniques have been proposed for analyzing and testing JavaScript applications. However, our survey of the literature in this area revealed that the proposed techniques are often evaluated on different datasets of programs and bugs. The lack of a commonly used benchmark limits the ability to perform fair and unbiased comparisons for assessing the efficacy of new techniques. To fill this gap, we propose BugsJS, a benchmark of 453 real, manually validated JavaScript bugs from 10 popular JavaScript server\u0393\u00c7\u00c9side programs, comprising 444k lines of code (LOC) in total. Each bug is accompanied by its bug report, the test cases that expose it, as well as the patch that fixes it. We extended BugsJS with a rich web interface for visualizing and dissecting the bugs'\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["64"]}
{"title": "Enhanced bug prediction in javascript programs with hybrid call-graph based invocation metrics\n", "abstract": " Bug prediction aims at finding source code elements in a software system that are likely to contain defects. Being aware of the most error-prone parts of the program, one can efficiently allocate the limited amount of testing and code review resources. Therefore, bug prediction can support software maintenance and evolution to a great extent. In this paper, we propose a function level JavaScript bug prediction model based on static source code metrics with the addition of a hybrid (static and dynamic) code analysis based metric of the number of incoming and outgoing function calls (HNII and HNOI). Our motivation for this is that JavaScript is a highly dynamic scripting language for which static code analysis might be very imprecise; therefore, using a purely static source code features for bug prediction might not be enough. Based on a study where we extracted 824 buggy and 1943 non-buggy functions from the publicly available BugsJS dataset for the ESLint JavaScript project, we can confirm the positive impact of hybrid code metrics on the prediction performance of the ML models. Depending on the ML algorithm, applied hyper-parameters, and target measures we consider, hybrid invocation metrics bring a 2\u0393\u00c7\u00f410% increase in model performances (ie, precision, recall, F-measure). Interestingly, replacing static NOI and NII metrics with their hybrid counterparts HNOI and HNII in itself improves model performances; however, using them all together yields the best results. View Full-Text", "num_citations": "3\n", "authors": ["64"]}
{"title": "Transforming c++ 11 code to c++ 03 to support legacy compilation environments\n", "abstract": " Newer technologies - programming languages, environments, libraries - change very rapidly. However, various internal and external constraints often prevent projects from quickly adopting to these changes. Customers may require specific platform compatibility from a software vendor, for example. In this work, we deal with such an issue in the context of the C++ programming language. Our industrial partner is required to use SDKs that support only older C++ language editions. They, however, would like to allow their developers to use the newest language constructs in their code. To address this problem, we created a source code transformation framework to automatically backport source code written according to the C++11 standard to its functionally equivalent C++03 variant. With our framework developers are free to exploit the latest language features, while production code is still built by using a restricted set\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["64"]}
{"title": "Comparison of static analysis tools for quality measurement of rpg programs\n", "abstract": " The RPG programming language is a popular language employed widely in IBM i mainframes nowadays. Legacy mainframe systems that evolved and survived the past decades usually data intensive and even business critical applications. Recent, state of the art quality assurance tools are mostly focused on popular languages like Java, C++ or Python. In this work we compare two source code based quality management tools for the RPG language. The study is focused on the data obtained using static analysis, which is then aggregated to higher level quality attributes. SourceMeter is a command line tool-chain capable to measure various source attributes like metrics and coding rule violations. SonarQube is a quality management platform with RPG language support. To facilitate the objective comparison, we used the SourceMeter for RPG plugin for SonarQube, which seamlessly integrates into the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["64"]}
{"title": "Challenges of SonarQube Plug-In Maintenance\n", "abstract": " The SONARQUBE TM  platform is a widely used open-source tool for continuous code quality management. It provides an API to extend the platform with plug-ins to upload additional data or to enrich its functionalities. The SourceMeter plug-in for SONARQUBE TM  platform integrates the SourceMeter static source code analyzer tool into the SONARQUBE TM  platform, i.e., uploads the analysis results and extends the GUI to be able to present the new results. The first version of the plug-in was released in 2015 and was compatible with the corresponding SONARQUBE TM  version. However, the platform - and what is more important, its API - have evolved a lot since then, therefore the plug-in had to be adapted to the new API. It was not just a slight adjustment, though, because we had to redesign and reimplement the whole UI and, at the same time, perform significant alterations in other parts of the plug-in as well\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["64"]}
{"title": "Service layer for IDE integration of C/C++ preprocessor related analysis\n", "abstract": " Software development in C/C++ languages is tightly coupled with preprocessor directives. While the use of preprocessor constructs cannot be avoided, current IDE support for developers can still be improved. Early feedback from IDEs about misused macros or conditional compilation has positive effects on developer productivity and code quality as well. In this paper we introduce a service layer for the Visual Studio to make detailed preprocessor information accessible for any type of IDE extensions. The service layer is built upon our previous work on the analysis of directives. We wrap the analyzer tool and provide its functionality through an API. We present the public interface of the service and demonstrate the provided services through small plug-ins implemented using various extension mechanisms. These plug-ins work together to aid the daily work of developers in several ways. We provide (1) an\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["64"]}
{"title": "Advances in Software Product Quality Measurement and its Applications in Software Evolution\n", "abstract": " I clearly remember my first PC on which I typed in my first Pascal program code line. The processor ran at a speed of 33MHz with a Turbo function that could boost it to an impressive 40MHz. The PC had a physical memory of 2MB and a hard disk with a capacity of almost 200MB, Nowadays, PCs typically have several processors running at GHz speed, contain GBs of RAM and can store anything from GBs to TBs of data on their hard drives, I would have never imagined such an incredible change in the computer hardware world, nor that I might become a computer scientist some day. Now I have written my dissertation and I look forward to living through similar, unexpected changes in life.Even though the thesis emphasizes the individual contributions of the author, none of the research work presented here would have been realized without the help of oth ers, Therefore I would like to thank all of those who helped me, one way or another, to get where I am now in my scientific and professional career. First, I would like to thank my supervisor Dr, Rudolf Ferenc for guiding my studies and teaching me many indispensable things about research. Without his positive attitude to researchoriented thinking, I would probably have never even got involved in doing scientific research at all, I would also like to thank Dr, Tibor Gyim\u251c\u2502thy, the head of Software Engineering Department, for supporting my research work. My special thanks goes to Dr, Lajos Jen\u253c\u00e6 F\u251c\u255dl\u251c\u2562p, whom I regard as my second mentor. He motivated and encour aged me at the beginning of my PhD studies. My many thanks also go to my colleagues and article co-authors, namely Dr, Tibor Bakota\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["64"]}
{"title": "Developer support for understanding preprocessor macro expansions\n", "abstract": " In the age of advanced integrated development environments there is a lack of support for understanding preprocessor macros. The preprocessor has proven to be a powerful tool for decades, but the developer is still guided poorly when the debugger stops at a source code line containing macros. The main problem is that the developer sees the original code, while the compiler uses the preprocessed code in the background. Investigating the usually nested macro calls can be a labor intensive tasks, which increases the overall effort spent on development and maintenance. There are several possibilities to help the developer in similar situations, but these are rarely employed since the preprocessor has its own, separate language to be analyzed. We implemented a Visual Studio plug-in (AddIn) that provides hand-on information on macros to increase the productivity of developers during debugging or\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["64"]}
{"title": "Employing Partial Least Squares Regression with Discriminant Analysis for Bug Prediction\n", "abstract": " Forecasting defect proneness of source code has long been a major research concern. Having an estimation of those parts of a software system that most likely contain bugs may help focus testing efforts, reduce costs, and improve product quality. Many prediction models and approaches have been introduced during the past decades that try to forecast bugged code elements based on static source code metrics, change and history metrics, or both. However, there is still no universal best solution to this problem, as most suitable features and models vary from dataset to dataset and depend on the context in which we use them. Therefore, novel approaches and further studies on this topic are highly necessary. In this paper, we employ a chemometric approach - Partial Least Squares with Discriminant Analysis (PLS-DA) - for predicting bug prone Classes in Java programs using static source code metrics. To our best knowledge, PLS-DA has never been used before as a statistical approach in the software maintenance domain for predicting software errors. In addition, we have used rigorous statistical treatments including bootstrap resampling and randomization (permutation) test, and evaluation for representing the software engineering results. We show that our PLS-DA based prediction model achieves superior performances compared to the state-of-the-art approaches (i.e. F-measure of 0.44-0.47 at 90% confidence level) when no data re-sampling applied and comparable to others when applying up-sampling on the largest open bug dataset, while training the model is significantly faster, thus finding optimal parameters is much easier. In terms\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["64"]}
{"title": "Systematic comparison of six open-source Java call graph construction tools\n", "abstract": " Call graphs provide the groundwork for numerous analysis algorithms and tools. However, in practice, their construction may have several ambiguities, especially for object-oriented programming languages like Java. The characteristics of the call graphs\u0393\u00c7\u00f4which are influenced by building requirements such as scalability, efficiency, completeness, and precision\u0393\u00c7\u00f4can greatly affect the output of the algorithms utilizing them. Therefore, it is important for developers to know a well-defined set of criteria based on which they can choose the most appropriate call graph builder tool for their static analysis applications. In this paper, we studied and compared six static call graph creator tools for Java. Our aim was to identify linguistic and technical properties that might induce differences in the generated call graphs besides the obvious differences caused by the various call graph construction algorithms. We evaluated the tools on multiple real-life open-source Java systems and performed a quantitative and qualitative assessment of the resulting graphs. We have shown how different outputs could be generated by the different tools. By manually analyzing the differences found on larger programs, we also found differences that we did not expect based on our preliminary assumptions.", "num_citations": "1\n", "authors": ["64"]}
{"title": "Maintainability of Source Code and its Connection to Version Control History Metrics\n", "abstract": " Recently I found a plain old 650 MB compact disc, burnt at 20th June, 2001. On that disc I have found some materials about the preparation of the application to the Doctoral School of Computer Science in Szeged. I was already a co-author of a few ongoing or already presented articles; some of them were on that disc. However, at that time I made a hard decision: I withdrew, and my professional life changed its direction towards the industry.Ten years later I was thinking about proceeding with the research. I have learned that there was the possibility of individual preparation. It was not necessary to follow the three-year study program, and I decided to renew my research activities in this form, parallel to my work. Most of the research was done on the train between Nagyk\u253c\u255cr\u251c\u2562s (my home town) and Budapest (where my work place is located). Chronologically first, I would like to mention the great co-work with co-authors of the articles done on the field of dynamic slicing of source code. The excellent supervision of Dr. J\u251c\u00ednos Csirik and Dr. Tibor Gyim\u251c\u2502thy helped all of us to be efficient in the research. I really enjoyed the great common work with Dr. \u251c\u00fcrp\u251c\u00edd Besz\u251c\u2310des, Dr. Tam\u251c\u00eds Gergely and Zsolt Mih\u251c\u00edly Szab\u251c\u2502. I gained very useful programming experiences with them.", "num_citations": "1\n", "authors": ["64"]}
{"title": "Adding constraint building mechanisms to a symbolic execution engine developed for detecting runtime errors\n", "abstract": " Most of the runtime failures of a software system can be revealed during test execution only, which has a very high cost. The symbolic execution engine developed at the Software Engineering Department of University of Szeged is able to detect runtime errors (such as null pointer dereference, bad array indexing, division by zero) in Java programs without running the program in real-life environment.                 In this paper we present a constraint system building mechanism which improves the accuracy of the runtime errors found by the symbolic execution engine mentioned above. We extend the original principles of symbolic execution by tracking the dependencies of the symbolic variables and substituting them with concrete values if the built constraint system unambiguously determines their value.                 The extended symbolic execution checker was tested on real-life open-source systems as well.", "num_citations": "1\n", "authors": ["64"]}