{"title": "Adaptive semi-supervised feature selection for cross-modal retrieval\n", "abstract": " In order to exploit the abundant potential information of the unlabeled data and contribute to analyzing the correlation among heterogeneous data, we propose the semi-supervised model named adaptive semi-supervised feature selection for cross-modal retrieval. First, we utilize the semantic regression to strengthen the neighboring relationship between the data with the same semantic. And the correlation between heterogeneous data can be optimized via keeping the pairwise closeness when learning the common latent space. Second, we adopt the graph-based constraint to predict accurate labels for unlabeled data, and it can also keep the geometric structure consistency between the label space and the feature space of heterogeneous data in the common latent space. Finally, an efficient joint optimization algorithm is proposed to update the mapping matrices and the label matrix for unlabeled data\u00a0\u2026", "num_citations": "42\n", "authors": ["1099"]}
{"title": "Task-dependent and query-dependent subspace learning for cross-modal retrieval\n", "abstract": " Most existing cross-modal retrieval approaches learn the same couple of projection matrices for different sub-retrieval tasks (such as, image retrieves text and text retrieves image) and various queries. They ignore the important fact that, different sub-retrieval tasks and queries have unique characteristics themselves in real practice. To tackle the problem, we propose a task-dependent and query-dependent subspace learning approach for cross-modal retrieval. Specifically, we first develop a unified cross-modal learning framework, where task-specific and category-specific subspaces can be learned simultaneously via an efficient iterative optimization. Based on this step, a task-category-projection mapping table is built. Subsequently, an efficient linear classifier is trained to learn a semantic mapping function between multimedia documents and their potential categories. In the online retrieval stage, the task\u00a0\u2026", "num_citations": "8\n", "authors": ["1099"]}
{"title": "MMVG-INF-Etrol@ TRECVID 2019: Activities in Extended Video.\n", "abstract": " We propose a video analysis system detecting activities in surveillance scenarios which wins Trecvid Activities in Extended Video (ActEV1) challenge 2019. For detecting and localizing surveillance events in videos, Argus employs a spatialtemporal activity proposal generation module facilitating object detection and tracking, followed by a sequential classification module to spatially and temporally localize persons and objects involved in the activity. We detail the design challenges and provide our insights and solutions in developing the state-of-the-art surveillance video analysis system.", "num_citations": "7\n", "authors": ["1099"]}
{"title": "Fusion-supervised deep cross-modal hashing\n", "abstract": " Deep hashing has recently received attention in cross-modal retrieval for its impressive advantages. However, existing hashing methods for cross-modal retrieval cannot fully capture the heterogeneous multi-modal correlation and exploit the semantic information. In this paper, we propose a novel Fusion-supervised Deep Cross-modal Hashing (FDCH) approach. Firstly, FDCH learns unified binary codes through a fusion hash network with paired samples as input, which effectively enhances the modeling of the correlation of heterogeneous multi-modal data. Then, these high-quality unified hash codes further supervise the training of the modality-specific hash networks for encoding out-of-sample queries. Meanwhile, both pair-wise similarity information and classification information are embedded in the hash networks under one stream framework, which simultaneously preserves cross-modal similarity and keeps\u00a0\u2026", "num_citations": "5\n", "authors": ["1099"]}
{"title": "Multi-class joint subspace learning for cross-modal retrieval\n", "abstract": " Most existing supervised subspace learning methods use the label information for high-level semantic exploration and learn one couple of common mapping matrices for all classes in the retrieval task. However, there are different semantic distributions among different classes and thus we propose to learn different mapping matrices for different classes in this paper, which facilitates learning more discriminative subspace. In addition, semantic overlap usually exists among different classes, which is reflected through common samples in different classes. Therefore, the multi-class joint subspace learning algorithm (MJSL) is proposed to distinct the different classes and mine the potential shared information of semantic overlap as much as possible. Specifically, the MJSL method considers exploring high-level semantic, keeping pair-wised closeness and selecting optimal features to obtain the most discriminative\u00a0\u2026", "num_citations": "3\n", "authors": ["1099"]}
{"title": "Cross-modal transfer hashing based on coherent projection\n", "abstract": " Due to the low storage and high efficiency, cross-modal hashing drew more and more attention in recent years. However, most existing methods ignore the intrinsic distribution of raw features and the inheritance relationship between raw feature space and hash space. In this paper, we propose the transfer hashing based on coherent projection for large-scale cross-modal retrieval. It preserves the inherent correlation of intrinsic distribution among raw heterogeneous features via a linear cross-modal transfer. In addition, the coherent projection is applied to cooperate with the cross-modal transfer to inherit the correlation from raw features space to hash space straightly. Furthermore, the anchor graph with linear complexity is utilized to further explore the local structure of each modality. And the semantic information is also explored by regressing the semantic labels to hash space. Finally, the succinct iterative algorithm\u00a0\u2026", "num_citations": "2\n", "authors": ["1099"]}
{"title": "Semi-supervised distance consistent cross-modal retrieval\n", "abstract": " Most of existing cross-modal retrieval approaches only exploit labeled data to train coupled projection matrices for supporting retrieval tasks across heterogeneous modalities. However, the valuable information involved in unlabeled data is unfortunately ignored. In this paper, we propose a novel Semi-Supervised Distance Consistent method (SSDC) to solve the problem. Our approach firstly models the initial correlation between different modalities by constructing the pseudo label and corresponding data of unlabeled query. Then our method learns projection matrices by adaptively optimizing the pseudo label of unlabeled data. In this way, SSDC could learn discriminative projection matrices. Experimental results on two publicly available datasets demonstrate the superior performance of the proposed approach.", "num_citations": "2\n", "authors": ["1099"]}
{"title": "Coupled feature selection based semi-supervised modality-dependent cross-modal retrieval\n", "abstract": " With the explosive growth of multimedia data, the information is usually represented in multi-modal version. The cross-modal based applications attracted increasing attention in recent years, and cross-modal retrieval is the popular one of them. In this paper, we propose a semi-supervised modality-dependent cross-modal retrieval method based on coupled feature selection (Semi-CoFe). It is different from most of the previous cross-modal retrieval methods, which usually used only labeled data for training to obtain the projection matrices under the constraint of l2-norm. In details, we propagate the label of cluster centers to unlabeled data via a devised weight matrix and construct the pseudo corresponding heterogeneous data. And then we jointly considered the semantic regression and pair-wised correlation analysis when learning the mapping matrices to keep the semantic consistency and the closeness\u00a0\u2026", "num_citations": "1\n", "authors": ["1099"]}
{"title": "Shandong Normal University in the VTT Tasks at TRECVID 2017.\n", "abstract": " SDNU_MMSys from Shandong Normal University participated the Video to Text (VTT) task in TRECVID 2017 including both Matching and Description Generation sub-tasks. We used our cross-modal retrieval model in the Matching sub-task. In the Description Generation sub-task, we combined the Inception V3 [1] and a two-layer LSTM [2] model to generate the description for each video.", "num_citations": "1\n", "authors": ["1099"]}