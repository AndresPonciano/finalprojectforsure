{"title": "How global is the global biodiversity information facility?\n", "abstract": " There is a concerted global effort to digitize biodiversity occurrence data from herbarium and museum collections that together offer an unparalleled archive of life on Earth over the past few centuries. The Global Biodiversity Information Facility provides the largest single gateway to these data. Since 2004 it has provided a single point of access to specimen data from databases of biological surveys and collections. Biologists now have rapid access to more than 120 million observations, for use in many biological analyses. We investigate the quality and coverage of data digitally available, from the perspective of a biologist seeking distribution data for spatial analysis on a global scale. We present an example of automatic verification of geographic data using distributions from the International Legume Database and Information Service to test empirically, issues of geographic coverage and accuracy. There are over 1/2 million records covering 31% of all Legume species, and 84% of these records pass geographic validation. These data are not yet a global biodiversity resource for all species, or all countries. A user will encounter many biases and gaps in these data which should be understood before data are used or analyzed. The data are notably deficient in many of the world's biodiversity hotspots. The deficiencies in data coverage can be resolved by an increased application of resources to digitize and publish data throughout these most diverse regions. But in the push to provide ever more data online, we should not forget that consistent data quality is of paramount importance if the data are to be useful in capturing a meaningful picture of life\u00a0\u2026", "num_citations": "338\n", "authors": ["1961"]}
{"title": "Qualtiy Measures and the Information Consumer\n", "abstract": " This chapter discusses the proposal for using quality criteria to facilitate information searching. It suggests that the information consumer can be assisted in searching for information by using a consumer-oriented model of quality. This is achieved by presenting the consumer with a set of relevant quality criteria from which they can select those of most importance to them at that present time, and allowing them to state preference values and importance weightings for each criterion. The consumer\u2019s quality profile can then be used to focus an information search onto relevant search domains, and produce a more focused output. The chapter presents our model of quality and shows that quality measures can be used to focus information searches by achieving statistically significant changes in the ordering of the obtained search results.", "num_citations": "57\n", "authors": ["1961"]}
{"title": "Establishing a taxonomy of quality for use in information filtering\n", "abstract": " When searching for information within a distributed heterogeneous environment, it is often difficult to ascertain the quality of the obtained results. Sometimes it may be possible to estimate the quality, but as the amount of available information grows this becomes increasingly difficult and time consuming. One possible solution is to develop a method of using quality as a filter to reduce the amount of irrelevant information that is returned by customising it to a user\u2019s requirements, defined in terms of quality characteristics.               Before this can be done the general term\u2019 quality\u2019 must be explicitly defined, and its characteristics identified. This paper therefore discusses our research into creating a domain-independent taxonomy of quality that can be used to assist in information evaluation and filtering within various information retrieval environments.", "num_citations": "27\n", "authors": ["1961"]}
{"title": "Information assurance in a distributed forensic cluster\n", "abstract": " When digital forensics started in the mid-1980s most of the software used for analysis came from writing and debugging software. Amongst these tools was the UNIX utility \u2018dd\u2019 which was used to create an image of an entire storage device. In the next decade the practice of creating and using \u2018an image\u2019 became established as a fundamental base of what we call \u2018sound forensic practice\u2019. By virtue of its structure, every file within the media was an integrated part of the image and so we were assured that it was wholesome representation of the digital crime scene. In an age of terabyte media \u2018the image\u2019 is becoming increasingly cumbersome to process, simply because of its size. One solution to this lies in the use of distributed systems. However, the data assurance inherent in a single media image file is lost when data is stored in separate files distributed across a system. In this paper we assess current assurance\u00a0\u2026", "num_citations": "17\n", "authors": ["1961"]}
{"title": "Using quality criteria to assist in information searching\n", "abstract": " One of the challenges facing today's information consumer is how to find information that meets their personal needs, within an acceptable time frame, and at an appropriate level of quality. One potential method for assisting these consumers is to employ a personalisable, explicit definition of quality to focus information search results. In this paper we discuss the feasibility of this approach by demonstrating how a consumer-refined definition of quality can be used to drive an information search, initially within a closed-world environment. This paves the way for further research, transferring lessons learned and techniques developed to an open, heterogeneous environment.", "num_citations": "15\n", "authors": ["1961"]}
{"title": "Accessing biodiversity resources in computational environments from workflow applications\n", "abstract": " In the biodiversity world (BDW) project we have created a flexible and extensible Web services-based grid environment for biodiversity researchers to solve problems in biodiversity and analyse biodiversity patterns. In this environment, heterogeneous and globally distributed biodiversity-related resources such as data sets and analytical tools are made available to be accessed and assembled by users into workflows to perform complex scientific experiments. One such experiment is bioclimatic modelling of the geographical distribution of individual species using climate variables in order to predict past and future climate-related changes in species distribution. Data sources and analytical tools required for such analysis of species distribution are widely dispersed, available on heterogeneous platforms, present data in different formats and lack interoperability. The BDW system brings all these disparate units\u00a0\u2026", "num_citations": "14\n", "authors": ["1961"]}
{"title": "A Flexible Quality Framework for Use within Information Retrieval.\n", "abstract": " Within recent years the volume of data readily available to the information consumer has dramatically increased in size. Although plentiful, this information is also of varying levels of quality, with providers ranging from multi-national corporations to specialist societies, and professional individuals to students with limited knowledge. As such, it is becoming increasingly difficult when searching for information to find precisely what is required. The two hurdles that prevent the finding of relevant information are therefore \u2018information overload\u2019and \u2018information quality\u2019.Our proposed solution to this problem consists of the development of a methodology for using quality metrics as an aid to information searching. By providing the consumer with a facility for stating their information requirements in terms of quality, via a set of quality metrics with associated importance weightings, the precision of search results is significantly improved. This method also allows easy manipulation of the search criteria, as both the metric selection and importance weightings can be quickly and easily changed, giving each user the opportunity to experiment with the various quality metrics, and observe how different quality weightings affect the returned results.", "num_citations": "14\n", "authors": ["1961"]}
{"title": "Biodiversity World: a problem-solving environment for analysing biodiversity patterns\n", "abstract": " In the Biodiversity World (BDW) project we have created a flexible and extensible Web services-based grid environment for biodiversity researchers to solve problems in biodiversity and analyse biodiversity patterns. In this environment, heterogeneous and globally distributed biodiversity-related resources such as data sets and analytical tools are made available to be accessed and assembled by users into workflows to perform complex scientific experiments. One such experiment is bioclimatic modelling of the geographical distribution of individual species using climate variables in order to predict past and future climate-related changes in species distribution. Data sources and analytical tools required for such analysis of species distribution are widely dispersed, available on heterogeneous platforms, present data in different formats and lack interoperability. The BDW system brings all these disparate units\u00a0\u2026", "num_citations": "13\n", "authors": ["1961"]}
{"title": "Using multiple quality criteria to focus information search results\n", "abstract": " In recent years the volume of data available to the information consumer has dramatically increased. It is now possible to search for information on an unlimited number of topics across wide range information environments. Although plentiful, this information is also of varying levels of quality, with providers ranging from multi-national corporations to individuals with limited knowledge. As such, it is becoming increasingly difficult to find precisely what is required. The two hurdles that prevent the finding of relevant information are therefore \u2018information overload\u2019and \u2018information quality\u2019. Our proposed solution to this problem consists of the development of a methodology for using quality criteria as an aid to information searching.Within this thesis we present a review of current literature in the area of quality-oriented research. As a consequence of this review it can be seen that although many definitions of quality exist\u00a0\u2026", "num_citations": "9\n", "authors": ["1961"]}
{"title": "Applying the ACPO Guidelines to Building Automation Systems\n", "abstract": " The increasing variety of Internet enabled hardware devices is creating a world of semi-autonomous, interconnected systems capable of control, automation and monitoring of a built environment. Many building automation and control systems that have previously been limited in connectivity, or due to cost only used in commercial environments, are now seeing increased uptake in domestic environments. Such systems may lack the management controls that are in place in commercial environments. The risk to these systems is further increased when they are connected to the Internet to allow control via a web browser or smartphone application. This paper explores the application of traditional digital forensics practices by applying established good practice guidelines to the field of building automation. In particular, we examine the application of the UK Association of Chief Police Officers guidelines for\u00a0\u2026", "num_citations": "7\n", "authors": ["1961"]}
{"title": "Patient-centred approach to focusing online health information search results\n", "abstract": " Over recent years the amount of health related information available via the Internet has greatly increased, and is being provided by a vast array of information providers, from government-owned public bodies to charities and to individuals publishing based on their experiences. This paper addresses the two main challenges to using this information: information overload and information quality. It also proposes a unique solution to these problems that can be used across all medical conditions, by facilitating the personalisation of online information searches, and enabling patient access to a proposed common EPRS in Wales.", "num_citations": "5\n", "authors": ["1961"]}
{"title": "A forensic methodology for analyzing Nintendo 3DS devices\n", "abstract": " Handheld video game consoles have evolved much like their desktop counterparts over the years. The most recent eighth generation of game consoles are now defined not by their ability to interact online using a web browser, but by the social media facilities they now provide. This chapter describes a forensic methodology for analyzing Nintendo 3DS handheld video game consoles, demonstrating their potential for misuse and highlighting areas where evidence may reside. Empirical research has led to the formulation of a detailed methodology that can assist forensic examiners in maximizing evidence extraction while minimizing, if not preventing, the destruction of information.", "num_citations": "4\n", "authors": ["1961"]}
{"title": "Data Quality\u2013What is it, and do we agree\n", "abstract": " Abate et al (1998) 4 categories and 15 dimensions Bovee et al (2001) 4 criteria and 10 components Cykana et al (1996) 6 characteristics Dedeke (2000) 5 dimensions and 28 metrics Eppler (2001) 4 quality levels and 16 criteria Gardyn (1997) 5 dimensions Long & Seko (2002) 5 dimensions and 24 characteristics Matsumura & Shouraboura", "num_citations": "4\n", "authors": ["1961"]}
{"title": "Data quality & agile methods: A BT perspective\n", "abstract": " Assuring the quality of data in organisations is not a new issue, yet it is still important and difficult as it concerns how well data in a given environment meets the needs of its intended user or function. The increasing use of information as a strategic resource by organisations has illustrated the multi-dimensional nature of data quality and the complexity of assuring it. This paper uses Soft Systems Methodology (SSM) to develop a model of data quality that incorporates the viewpoints of different stakeholders. The resulting activities were compared to BT\u00e2\u20ac\u2122s current practices, which lead to business recommendations that would promote data quality whilst using a traditional development methodology. However, BT, like many other organisations, is currently migrating its development approach to consider agile methods. Therefore, this paper also analyses how the development of systems and software may be affected by the change in an organisations chosen development methodology, along with recommendations having also been developed to address data quality using agile methodologies. This work highlights that organisations need to consider different approaches to data quality depending on the development methodology. We therefore argue that a \u00e2\u20ac\u0153one-size-fits-all\u00e2\u20ac  approach to data quality is not appropriate. The work has been influenced by best practices in BT and the modelling and analysis reflect this. However, the approach used and the resulting recommendations are useful to other organisations.", "num_citations": "3\n", "authors": ["1961"]}
{"title": "Authentic Assessments and the External Examiner\n", "abstract": " A key role of the external examiner is to review student work submitted for assessment plus the feedback and grading undertaken on that work by academic staff. The aim of this is to ensure equitability between the assessments of individual students\u2019 achievement and consistency and comparability across courses throughout the program and with commensurate study levels and programs at other institutions, whilst safeguarding academic standards. The variety of assessment-types that an external examiner may review can be diverse. When the primary focus of the work being assessed is tangible, such as with written examinations or assignments, external examiners are able to view student achievements and assessor actions through a lens comparable to that of the original assessors. However, this process cannot adequately capture assessment-types where the only evidence is proxies to the original\u00a0\u2026", "num_citations": "1\n", "authors": ["1961"]}