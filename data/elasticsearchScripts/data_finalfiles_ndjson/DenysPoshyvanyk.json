{"title": "Using the conceptual cohesion of classes for fault prediction in object-oriented systems\n", "abstract": " High cohesion is a desirable property of software as it positively impacts understanding, reuse, and maintenance. Currently proposed measures for cohesion in Object-Oriented (OO) software reflect particular interpretations of cohesion and capture different aspects of it. Existing approaches are largely based on using the structural information from the source code, such as attribute references, in methods to measure cohesion. This paper proposes a new measure for the cohesion of classes in OO software systems based on the analysis of the unstructured information embedded in the source code, such as comments and identifiers. The measure, named the Conceptual Cohesion of Classes (C3), is inspired by the mechanisms used to measure textual coherence in cognitive psychology and computational linguistics. This paper presents the principles and the technology that stand behind the C3 measure. A large\u00a0\u2026", "num_citations": "346\n", "authors": ["1015"]}
{"title": "Combining formal concept analysis with information retrieval for concept location in source code\n", "abstract": " The paper addresses the problem of concept location in source code by presenting an approach which combines formal concept analysis (FCA) and latent semantic indexing (LSI). In the proposed approach, LSI is used to map the concepts expressed in queries written by the programmer to relevant parts of the source code, presented as a ranked list of search results. Given the ranked list of source code elements, our approach selects most relevant attributes from these documents and organizes the results in a concept lattice, generated via FCA. The approach is evaluated in a case study on concept location in the source code of eclipse, an industrial size integrated development environment. The results of the case study show that the proposed approach is effective in organizing different concepts and their relationships present in the subset of the search results. The proposed concept location method outperforms\u00a0\u2026", "num_citations": "325\n", "authors": ["1015"]}
{"title": "Using information retrieval based coupling measures for impact analysis\n", "abstract": " Coupling is an important property of software systems, which directly impacts program comprehension. In addition, the strength of coupling measured between modules in software is often used as a predictor of external software quality attributes such as changeability, ripple effects of changes and fault-proneness. This paper presents a new set of coupling measures for Object-Oriented (OO) software systems measuring conceptual coupling of classes. Conceptual coupling is based on measuring the degree to which the identifiers and comments from different classes relate to each other. This type of relationship, called conceptual coupling, is measured through the use of Information Retrieval (IR) techniques. The proposed measures are different from existing coupling measures and they capture new dimensions of coupling, which are not captured by the existing coupling measures. The paper investigates\u00a0\u2026", "num_citations": "248\n", "authors": ["1015"]}
{"title": "Using latent dirichlet allocation for automatic categorization of software\n", "abstract": " In this paper, we propose a technique called LACT for automatically categorizing software systems in open-source repositories. LACT is based on latent Dirichlet Allocation, an information retrieval method which is used to index and analyze source code documents as mixtures of probabilistic topics. For an initial evaluation, we performed two studies. In the first study, LACT was compared against an existing tool, MUDABlue, for classifying 41 software systems written in C into problem domain categories. The results indicate that LACT can automatically produce meaningful category names and yield classification results comparable to MUDABlue. In the second study, we applied LACT to 43 software systems written in different programming languages such as C/C++, Java, C#, PHP, and Perl. The results indicate that LACT can be used effectively for the automatic categorization of software systems regardless of the\u00a0\u2026", "num_citations": "193\n", "authors": ["1015"]}
{"title": "The conceptual cohesion of classes\n", "abstract": " While often defined in informal ways, software cohesion reflects important properties of modules in a software system. Cohesion measurement has been used for quality assessment, fault proneness prediction, software modularization, etc. Existing approaches to cohesion measurement in object-oriented software are largely based on the structural information of the source code, such as attribute references in methods. These measures reflect particular interpretations of cohesion and try to capture different aspects of cohesion and no single cohesion metric or suite is accepted as standard measurement for cohesion. The paper proposes a new set of measures for the cohesion of individual classes within an OO software system, based on the analysis of the semantic information embedded in the source code, such as comments and identifiers. A case study on open source software is presented, which compares the\u00a0\u2026", "num_citations": "182\n", "authors": ["1015"]}
{"title": "Detecting similar software applications\n", "abstract": " Although popular text search engines allow users to retrieve similar web pages, source code search engines do not have this feature. Detecting similar applications is a notoriously difficult problem, since it implies that similar highlevel requirements and their low-level implementations can be detected and matched automatically for different applications. We created a novel approach for automatically detecting Closely reLated ApplicatioNs (CLAN) that helps users detect similar applications for a given Java application. Our main contributions are an extension to a framework of relevance and a novel algorithm that computes a similarity index between Java applications using the notion of semantic layers that correspond to packages and class hierarchies. We have built CLAN and we conducted an experiment with 33 participants to evaluate CLAN and compare it with the closest competitive approach, MUDABlue. The\u00a0\u2026", "num_citations": "161\n", "authors": ["1015"]}
{"title": "Using relational topic models to capture coupling among classes in object-oriented software systems\n", "abstract": " Coupling metrics capture the degree of interaction and relationships among source code elements in software systems. A vast majority of existing coupling metrics rely on structural information, which captures interactions such as usage relations between classes and methods or execute after associations. However, these metrics lack the ability to identify conceptual dependencies, which, for instance, specify underlying relationships encoded by developers in identifiers and comments of source code classes. We propose a new coupling metric for object-oriented software systems, namely Relational Topic based Coupling (RTC) of classes, which uses Relational Topic Models (RTM), generative probabilistic model, to capture latent topics in source code classes and relationships among them. A case study on thirteen open source software systems is performed to compare the new measure with existing structural and\u00a0\u2026", "num_citations": "131\n", "authors": ["1015"]}
{"title": "Concept location using formal concept analysis and information retrieval\n", "abstract": " The article addresses the problem of concept location in source code by proposing an approach that combines Formal Concept Analysis and Information Retrieval. In the proposed approach, Latent Semantic Indexing, an advanced Information Retrieval approach, is used to map textual descriptions of software features or bug reports to relevant parts of the source code, presented as a ranked list of source code elements. Given the ranked list, the approach selects the most relevant attributes from the best ranked documents, clusters the results, and presents them as a concept lattice, generated using Formal Concept Analysis. The approach is evaluated through a large case study on concept location in the source code on six open-source systems, using several hundred features and bugs. The empirical study focuses on the analysis of various configurations of the generated concept lattices and the results indicate\u00a0\u2026", "num_citations": "124\n", "authors": ["1015"]}
{"title": "Combining textual and structural analysis of software artifacts for traceability link recovery\n", "abstract": " Existing methods for recovering traceability links among software documentation artifacts analyze textual similarities among these artifacts. It may be the case, however, that related documentation elements share little terminology or phrasing. This paper presents a technique for indirectly recovering these traceability links in requirements documentation by combining textual with structural information as we conjecture that related requirements share related source code elements. A preliminary case study indicates that our combined approach improves the precision and recall of recovering relevant links among documents as compared to stand-alone methods based solely on analyzing textual similarities.", "num_citations": "119\n", "authors": ["1015"]}
{"title": "Modeling class cohesion as mixtures of latent topics\n", "abstract": " The paper proposes a new measure for the cohesion of classes in object-oriented software systems. It is based on the analysis of latent topics embedded in comments and identifiers in source code. The measure, named as maximal weighted entropy, utilizes the latent Dirichlet allocation technique and information entropy measures to quantitatively evaluate the cohesion of classes in software. This paper presents the principles and the technology that stand behind the proposed measure. Two case studies on a large open source software system are presented. They compare the new measure with an extensive set of existing metrics and use them to construct models that predict software faults. The case studies indicate that the novel measure captures different aspects of class cohesion compared to the existing cohesion measures and improves fault prediction for most metrics, which are combined with maximal\u00a0\u2026", "num_citations": "104\n", "authors": ["1015"]}
{"title": "Machine learning-based prototyping of graphical user interfaces for mobile apps\n", "abstract": " It is common practice for developers of user-facing software to transform a mock-up of a graphical user interface (GUI) into code. This process takes place both at an application's inception and in an evolutionary context as GUI changes keep pace with evolving features. Unfortunately, this practice is challenging and time-consuming. In this paper, we present an approach that automates this process by enabling accurate prototyping of GUIs via three tasks: detection, classification, and assembly. First, logical components of a GUI are detected from a mock-up artifact using either computer vision techniques or mock-up metadata. Then, software repository mining, automated dynamic analysis, and deep convolutional neural networks are utilized to accurately classify GUI-components into domain-specific types (e.g., toggle-button). Finally, a data-driven, K-nearest-neighbors algorithm generates a suitable hierarchical GUI\u00a0\u2026", "num_citations": "93\n", "authors": ["1015"]}
{"title": "Flat3: Feature location and textual tracing tool\n", "abstract": " Feature location is the process of finding the source code that implements a functional requirement of a software system. It plays an important role in software maintenance activities, but when it is performed manually, it can be challenging and time-consuming, especially for large, long-lived systems. This paper describes a tool called FLAT 3  that integrates textual and dynamic feature location techniques along with feature annotation capabilities and a useful visualization technique, providing a complete suite of tools that allows developers to quickly and easily locate the code that implements a feature and then save these annotations for future use.", "num_citations": "93\n", "authors": ["1015"]}
{"title": "Source code exploration with Google\n", "abstract": " The paper presents a new approach to source code exploration, which is the result of integrating the Google Desktop Search (GDS) engine into the Eclipse development environment. The resulting search engine, named Google Eclipse Search (GES), provides improved searching in Eclipse software projects. The paper advocates for a component-based approach that allows us to develop strong tools, which support various maintenance tasks, by leveraging the strengths of existing frameworks and components. The development effort for such tools is reduced, while customization and flexibility, to fully support user needs, is maintained. GES allows developers to search software projects in a manner similar to searching the Internet or their own desktops. The proposed approach takes advantages of the power of GDS for quick and accurate searching and of Eclipse's extensibility. The paper discusses usage\u00a0\u2026", "num_citations": "83\n", "authors": ["1015"]}
{"title": "An exploratory study on assessing feature location techniques\n", "abstract": " This paper presents an exploratory study of ten feature location techniques that use various combinations of textual, dynamic, and static analyses. Unlike previous studies, the approaches are evaluated in terms of finding multiple relevant methods, not just a single starting point of a feature's implementation. Additionally, a new way of applying textual analysis is introduced by which queries are automatically composed of the identifiers of a method known to be relevant to a feature. Our results show that this new type of query is just as effective as a query formulated by a human. We also provide insights into situations when certain feature location approaches work well and fall short. Our results and observations can be used to guide future research on feature location.", "num_citations": "74\n", "authors": ["1015"]}
{"title": "Using structural and textual information to capture feature coupling in object-oriented software\n", "abstract": " Previous studies have demonstrated the relationship between coupling and external software quality attributes, such as fault-proneness, and the application of coupling to software maintenance tasks, such as impact analysis. These previous studies concentrate on class coupling. However, there is a growing focus on the study of features in software, and features are often implemented across multiple classes, meaning class-level coupling measures are not applicable. We ask the pertinent question, \u201cIs measuring coupling at the feature-level also useful?\u201d We define new feature coupling metrics based on structural and textual source code information and extend the unified framework for coupling measurement to include these new metrics. We also conduct three extensive case studies to evaluate these new metrics and answer this research question. The first study examines the relationship between feature\u00a0\u2026", "num_citations": "66\n", "authors": ["1015"]}
{"title": "Automating performance bottleneck detection using search-based application profiling\n", "abstract": " Application profiling is an important performance analysis technique, when an application under test is analyzed dynamically to determine its space and time complexities and the usage of its instructions. A big and important challenge is to profile nontrivial web applications with large numbers of combinations of their input parameter values. Identifying and understanding particular subsets of inputs leading to performance bottlenecks is mostly manual, intellectually intensive and laborious procedure. We propose a novel approach for automating performance bottleneck detection using search-based input-sensitive application profiling. Our key idea is to use a genetic algorithm as a search heuristic for obtaining combinations of input parameter values that maximizes a fitness function that represents the elapsed execution time of the application. We implemented our approach, coined as Genetic Algorithm-driven\u00a0\u2026", "num_citations": "64\n", "authors": ["1015"]}
{"title": "New conceptual coupling and cohesion metrics for object-oriented systems\n", "abstract": " The paper presents two novel conceptual metrics for measuring coupling and cohesion in software systems. Our first metric, Conceptual Coupling between Object classes (CCBO), is based on the well-known CBO coupling metric, while the other metric, Conceptual Lack of Cohesion on Methods (CLCOM5), is based on the LCOM5 cohesion metric. One advantage of the proposed conceptual metrics is that they can be computed in a simpler (and in many cases, programming language independent) way as compared to some of the structural metrics. We empirically studied CCBO and CLCOM5 for predicting fault-proneness of classes in a large open source system and compared these metrics with a host of existing structural and conceptual metrics for the same task. As the result, we found that the proposed conceptual metrics, when used in conjunction, can predict bugs nearly as precisely as the 58 structural metrics\u00a0\u2026", "num_citations": "64\n", "authors": ["1015"]}
{"title": "Recommending source code examples via api call usages and documentation\n", "abstract": " Online source code repositories contain software projects that already implement certain requirements that developers must fulfill. Programmers can reuse code from these existing projects if they can find relevant code without significant effort. We propose a new method to recommend source code examples to developers by querying against Application Programming Interface (API) calls and their documentations that are fused with structural information about the code. We conducted an empirical evaluation that suggests that our approach is lightweight and accurate.", "num_citations": "64\n", "authors": ["1015"]}
{"title": "A large-scale empirical comparison of static and dynamic test case prioritization techniques\n", "abstract": " The large body of existing research in Test Case Prioritization (TCP) techniques, can be broadly classified into two categories: dynamic techniques (that rely on run-time execution information) and static techniques (that operate directly on source and test code). Absent from this current body of work is a comprehensive study aimed at understanding and evaluating the static approaches and comparing them to dynamic approaches on a large set of projects.", "num_citations": "56\n", "authors": ["1015"]}
{"title": "Automated reporting of GUI design violations for mobile apps\n", "abstract": " The inception of a mobile app often takes form of a mock-up of the Graphical User Interface (GUI), represented as a static image delineating the proper layout and style of GUI widgets that satisfy requirements. Following this initial mock-up, the design artifacts are then handed off to developers whose goal is to accurately implement these GUIs and the desired functionality in code. Given the sizable abstraction gap between mock-ups and code, developers often introduce mistakes related to the GUI that can negatively impact an app's success in highly competitive marketplaces. Moreover, such mistakes are common in the evolutionary context of rapidly changing apps. This leads to the time-consuming and laborious task of design teams verifying that each screen of an app was implemented according to intended design specifications.", "num_citations": "52\n", "authors": ["1015"]}
{"title": "Visualization of CVS repository information\n", "abstract": " Mining software repositories is an important activity during software evolution, as the extracted data is used to support a variety of software maintenance tasks. The key information extracted from these repositories gives a picture of the changes on the software system. To have a complete picture, tailored to the needs of the developer, the extracted data needs to be filtered, aggregated, and presented to the users. In this paper we propose a new visualization for such data, which relies on an existing software visualization front-end, SourceViewer3D (sv3D). The new visualization allows users to define multiple views of the change history data, each view helps answer a set of questions relevant to specific maintenance tasks. Data can be viewed at different granularity (e.g., file, line of text, method, class) and comprehensive views can be defined, which display to the user multiple data types at the same time. Complex\u00a0\u2026", "num_citations": "48\n", "authors": ["1015"]}
{"title": "Mining performance regression inducing code changes in evolving software\n", "abstract": " During software evolution, the source code of a system frequently changes due to bug fixes or new feature requests. Some of these changes may accidentally degrade performance of a newly released software version. A notable problem of regression testing is how to find problematic changes (out of a large number of committed changes) that may be responsible for performance regressions under certain test inputs. We propose a novel recommendation system, coined as PERFIMPACT, for automatically identifying code changes that may potentially be responsible for performance regressions using a combination of search-based input profiling and change impact analysis techniques. PERFIMPACT independently sends the same input values to two releases of the application under test, and uses a genetic algorithm to mine execution traces and explore a large space of input value combinations to find specific\u00a0\u2026", "num_citations": "37\n", "authors": ["1015"]}
{"title": "Forepost: Finding performance problems automatically with feedback-directed learning software testing\n", "abstract": " A goal of performance testing is to find situations when applications unexpectedly exhibit worsened characteristics for certain combinations of input values. A fundamental question of performance testing is how to select a manageable subset of the input data faster in order to automatically find performance bottlenecks in applications. We propose FOREPOST, a novel solution, for automatically finding performance bottlenecks in applications using black-box software testing. Our solution is an adaptive, feedback-directed learning testing system that learns rules from execution traces of applications. Theses rules are then used to automatically select test input data for performance testing. We hypothesize that FOREPOST can find more performance bottlenecks as compared to random testing. We have implemented our solution and applied it to a medium-size industrial application at a major insurance company\u00a0\u2026", "num_citations": "31\n", "authors": ["1015"]}
{"title": "How do static and dynamic test case prioritization techniques perform on modern software systems? An extensive study on GitHub projects\n", "abstract": " Test Case Prioritization (TCP) is an increasingly important regression testing technique for reordering test cases according to a pre-defined goal, particularly as agile practices gain adoption. To better understand these techniques, we perform the first extensive study aimed at empirically evaluating four static TCP techniques, comparing them with state-of-research dynamic TCP techniques across several quality metrics. This study was performed on 58 real-word Java programs encompassing 714 KLoC and results in several notable observations. First, our results across two effectiveness metrics (the Average Percentage of Faults Detected APFD and the cost cognizant APFDc) illustrate that at test-class granularity, these metrics tend to correlate, but this correlation does not hold at test-method granularity. Second, our analysis shows that static techniques can be surprisingly effective, particularly when measured by\u00a0\u2026", "num_citations": "29\n", "authors": ["1015"]}
{"title": "Sanitizing and minimizing databases for software application test outsourcing\n", "abstract": " Testing software applications that use nontrivial databases is increasingly outsourced to test centers in order to achieve lower cost and higher quality. Not only do different data privacy laws prevent organizations from sharing this data with test centers because databases contain sensitive information, but also this situation is aggravated by big data - it is time consuming and difficult to anonymize, distribute, and test with large databases. Deleting data randomly often leads to significantly worsened test coverages and fewer uncovered faults, thereby reducing the quality of software applications. We propose a novel approach for Protecting and mInimizing databases for Software TestIng taSks (PISTIS) that both sanitizes and minimizes a database that comes along with an application. PISTIS uses a weight-based data clustering algorithm that partitions data in the database using information obtained using program\u00a0\u2026", "num_citations": "27\n", "authors": ["1015"]}
{"title": "Discovering flaws in security-focused static analysis tools for android using systematic mutation\n", "abstract": " Mobile application security has been one of the major areas of security research in the last decade. Numerous application analysis tools have been proposed in response to malicious, curious, or vulnerable apps. However, existing tools, and specifically, static analysis tools, trade soundness of the analysis for precision and performance, and are hence soundy. Unfortunately, the specific unsound choices or flaws in the design of these tools are often not known or well-documented, leading to a misplaced confidence among researchers, developers, and users. This paper proposes the Mutation-based soundness evaluation (\u03bcSE) framework, which systematically evaluates Android static analysis tools to discover, document, and fix, flaws, by leveraging the well-founded practice of mutation analysis. We implement \u03bcSE as a semi-automated framework, and apply it to a set of prominent Android static analysis tools that detect private data leaks in apps. As the result of an in-depth analysis of one of the major tools, we discover 13 undocumented flaws. More importantly, we discover that all 13 flaws propagate to tools that inherit the flawed tool. We successfully fix one of the flaws in cooperation with the tool developers. Our results motivate the urgent need for systematic discovery and documentation of unsound choices in soundy tools, and demonstrate the opportunities in leveraging mutation testing in achieving this goal.", "num_citations": "25\n", "authors": ["1015"]}
{"title": "Exemplar: Executable examples archive\n", "abstract": " Searching for applications that are highly relevant to development tasks is challenging because the high-level intent reflected in the descriptions of these tasks doesn't usually match the low-level implementation details of applications. In this demo we show a novel code search engine called Exemplar (EXEcutable exaMPLes ARchive) to bridge this mismatch. Exemplar takes natural-language query that contains high-level concepts (e.g., MIME, data sets) as input, then uses information retrieval and program analysis techniques to retrieve applications that implement these concepts.", "num_citations": "25\n", "authors": ["1015"]}
{"title": "On-device bug reporting for android applications\n", "abstract": " Bugs that surface in mobile applications can be difficult to reproduce and fix due to several confounding factors including the highly GUI-driven nature of mobile apps, varying contextual states, differing platform versions and device fragmentation. It is clear that developers need support in the form of automated tools that allow for more precise reporting of application defects in order to facilitate more efficient and effective bug fixes. In this paper, we present a tool aimed at supporting application testers and developers in the process of On-Device Bug Reporting. Our tool, called ODBR, leverages the uiautomator framework and low-level event stream capture to offer support for recording and replaying a series of input gesture and sensor events that describe a bug in an Android application.", "num_citations": "24\n", "authors": ["1015"]}
{"title": "Detecting and summarizing GUI changes in evolving mobile apps\n", "abstract": " Mobile applications have become a popular software development domain in recent years due in part to a large user base, capable hardware, and accessible platforms. However, mobile developers also face unique challenges, including pressure for frequent releases to keep pace with rapid platform evolution, hardware iteration, and user feedback. Due to this rapid pace of evolution, developers need automated support for documenting the changes made to their apps in order to aid in program comprehension. One of the more challenging types of changes to document in mobile apps are those made to the graphical user interface (GUI) due to its abstract, pixel-based representation. In this paper, we present a fully automated approach, called GCAT, for detecting and summarizing GUI changes during the evolution of mobile apps. GCAT leverages computer vision techniques and natural language generation to\u00a0\u2026", "num_citations": "21\n", "authors": ["1015"]}
{"title": "Creating and evolving software by searching, selecting and synthesizing relevant source code\n", "abstract": " When programmers develop or maintain software, they instinctively sense that there are fragments of code that other developers implemented somewhere, and these code fragments could be reused if found. In this paper, we propose a novel solution that addresses the fundamental questions of searching, selecting, and synthesizing (S 3 ) software based on the analysis of Application Programming Interface (API) calls as units of abstractions that implement high-level concepts (e.g., the API call EncryptData implements a cryptographic concept). This paper outlines the details behind S 3 , analyzes current challenges and describes evaluation plans.", "num_citations": "21\n", "authors": ["1015"]}
{"title": "Enhancing rules for cloud resource provisioning via learned software performance models\n", "abstract": " In cloud computing, stakeholders deploy and run their software applications on a sophisticated infrastructure that is owned and managed by third-party providers. The ability of a given cloud infrastructure to effectively re-allocate resources to applications is referred to as elasticity. To enable elasticity, programmers study the behavior of applications and write scripts that guide the cloud to provision resources for these applications. This is an imprecise, laborious, manual and expensive approach that drastically increases the cost of application deployment and maintenance in the cloud. We propose an approach, coined as Provisioning Resources with Experimental SofTware mOdeling (PRESTO), to automatically learn behavioral models of software applications during performance testing in order to recommend programmers how to improve provisioning strategies that guide the cloud to (de) allocate resources to\u00a0\u2026", "num_citations": "20\n", "authors": ["1015"]}
{"title": "Automated tagging of software projects using bytecode and dependencies (n)\n", "abstract": " Several open and closed source repositories group software systems and libraries to allow members of particular organizations or the open source community to take advantage of them. However, to make this possible, it is necessary to have effective ways of searching and browsing the repositories. Software tagging is the process of assigning terms (i.e., tags or labels) to software assets in order to describe features and internal details, making the task of understanding software easier and potentially browsing and searching through a repository more effective. We present Sally, an automatic software tagging approach that is able to produce meaningful tags for Maven-based software projects by analyzing their bytecode and dependency relations without any special requirements from developers. We compared tags generated by Sally to the ones in two widely used online repositories, and the tags generated by a\u00a0\u2026", "num_citations": "20\n", "authors": ["1015"]}
{"title": "An empirical exploration of regularities in open-source software lexicons\n", "abstract": " The software lexicon is an important source of information during program comprehension activities and it has been in the focus of several recent case studies. Identifiers and comments, which constitute a lexicon in software, encode domain concepts and design decisions made by programmers. The paper presents an exploratory study that investigates regularities in the software lexicons of open-source projects by analyzing distributions of tokens in diverse software artifacts. The study examined source code of 142 systems from different domains, written in 12 different programming languages, as well as bug reports and external documentation. We discover that distributions of lexical tokens in studied artifacts follow the Zipf-Mandelbrot law, which is an empirical law in statistical natural language processing. Furthermore, the study reveals that the Zipf-Mandelbrot law is not confined to program lexicons in object\u00a0\u2026", "num_citations": "19\n", "authors": ["1015"]}
{"title": "A study of data store-based home automation\n", "abstract": " Home automation platforms provide a new level of convenience by enabling consumers to automate various aspects of physical objects in their homes. While the convenience is beneficial, security flaws in the platforms or integrated third-party products can have serious consequences for the integrity of a user's physical environment. In this paper we perform a systematic security evaluation of two popular smart home platforms, Google's Nest platform and Philips Hue, that implement home automation\" routines\"(ie, trigger-action programs involving apps and devices) via manipulation of state variables in a centralized data store. Our semi-automated analysis examines, among other things, platform access control enforcement, the rigor of non-system enforcement procedures, and the potential for misuse of routines. This analysis results in ten key findings with serious security implications. For instance, we demonstrate\u00a0\u2026", "num_citations": "16\n", "authors": ["1015"]}
{"title": "Forepost: A tool for detecting performance problems with feedback-driven learning software testing\n", "abstract": " A goal of performance testing is to find situations when applications unexpectedly exhibit worsened characteristics for certain combinations of input values. A fundamental question of performance testing is how to select a manageable subset of the input data faster to find performance problems in applications automatically. We present a novel tool, FOREPOST, for finding performance problems in applications automatically using black-box software testing. In this paper, we demonstrate how FOREPOST extracts rules from execution traces of applications by using machine learning algorithms, and then uses these rules to select test input data automatically to steer applications towards computationally intensive paths and to find performance problems. FOREPOST is available in our online appendix (http://www.cs.wm.edu/semeru/data/ICSE16-FOREPOST), which contains the tool, source code and demo video.", "num_citations": "16\n", "authors": ["1015"]}
{"title": "Portfolio: a search engine for finding functions and their usages\n", "abstract": " In this demonstration, we present a code search system called Portfolio that retrieves and visualizes relevant functions and their usages. We will show how chains of relevant functions and their usages can be visualized to users in response to their queries.", "num_citations": "16\n", "authors": ["1015"]}
{"title": "Using information retrieval to support software maintenance tasks\n", "abstract": " This paper presents an approach based on information retrieval (IR) techniques for extracting and representing the unstructured information in large software systems such that it can be automatically combined with analysis of program dependencies and execution traces to define new techniques for feature location, impact analysis, and software measurement tasks. We expect that these new techniques will contribute directly to the improvement of design of incremental changes and thus increased software quality and reduction of software maintenance costs. The presented results are based on the author's doctoral dissertation.", "num_citations": "16\n", "authors": ["1015"]}
{"title": "Guigle: A gui search engine for android apps\n", "abstract": " The process of developing a mobile application typically starts with the ideation and conceptualization of its user interface. This concept is then translated into a set of mock-ups to help determine how well the user interface embodies the intended features of the app. After the creation of mock-ups developers then translate it into an app that runs in a mobile device. In this paper we propose an approach, called GUIGLE, that aims to facilitate the process of conceptualizing the user interface of an app through GUI search. GUIGLE indexes GUI images and metadata extracted using automated dynamic analysis on a large corpora of apps extracted from Google Play. To perform a search, our approach uses information from text displayed on a screen, user interface components, the app name, and screen color palettes to retrieve relevant screens given a query. Furthermore, we provide a lightweight query language that\u00a0\u2026", "num_citations": "14\n", "authors": ["1015"]}
{"title": "Learning to identify security-related issues using convolutional neural networks\n", "abstract": " Software security is becoming a high priority for both large companies and start-ups alike due to the increasing potential for harm that vulnerabilities and breaches carry with them. However, attaining robust security assurance while delivering features requires a precarious balancing act in the context of agile development practices. One path forward to help aid development teams in securing their software products is through the design and development of security-focused automation. Ergo, we present a novel approach, called SecureReqNet, for automatically identifying whether issues in software issue tracking systems describe security-related content. Our approach consists of a two-phase neural net architecture that operates purely on the natural language descriptions of issues. The first phase of our approach learns high dimensional word embeddings from hundreds of thousands of vulnerability descriptions\u00a0\u2026", "num_citations": "13\n", "authors": ["1015"]}
{"title": "Towards a natural perspective of smart homes for practical security and safety analyses\n", "abstract": " Designing practical security systems for the smart home is challenging without the knowledge of realistic home usage. This paper describes the design and implementation of H\u03b5lion, a framework that generates natural home automation scenarios by identifying the regularities in user-driven home automation sequences, which are in turn generated from routines created by end-users. Our key hypothesis is that smart home event sequences created by users exhibit inherent semantic patterns, or naturalness that can be modeled and used to generate valid and useful scenarios. To evaluate our approach, we first empirically demonstrate that this naturalness hypothesis holds, with a corpus of 30,518 home automation events, constructed from 273 routines collected from 40 users. We then demonstrate that the scenarios generated by H\u03b5lion seem valid to end-users, through two studies with 16 external evaluators. We\u00a0\u2026", "num_citations": "11\n", "authors": ["1015"]}
{"title": "Improving the effectiveness of traceability link recovery using hierarchical bayesian networks\n", "abstract": " Traceability is a fundamental component of the modern software development process that helps to ensure properly functioning, secure programs. Due to the high cost of manually establishing trace links, researchers have developed automated approaches that draw relationships between pairs of textual software artifacts using similarity measures. However, the effectiveness of such techniques are often limited as they only utilize a single measure of artifact similarity and cannot simultaneously model (implicit and explicit) relationships across groups of diverse development artifacts.", "num_citations": "10\n", "authors": ["1015"]}
{"title": "Assisting developers with license compliance\n", "abstract": " Software licensing determines how open source systems are reused, distributed, and modified from a legal perspective. While it facilitates rapid development, it can present difficulty for developers in understanding due to the legal language of these licenses. Because of misunderstandings, systems can incorporate licensed code in a way that violates the terms of the license. Our research first aimed at understanding the rationale of developers in choosing and changing licenses. We also investigated the problem of traceability of license changes. These two studies are fundamental components for understanding problems that developers face so that we can better support developers with licensing. Subsequently, we present our proposed research plan of ensuring license compliance of a system. Our research incorporates techniques from information retrieval, code search, mining software repositories, and\u00a0\u2026", "num_citations": "10\n", "authors": ["1015"]}
{"title": "Combining conceptual and domain-based couplings to detect database and code dependencies\n", "abstract": " Knowledge of software dependencies plays an important role in program comprehension and other maintenance activities. Traditionally, dependencies are derived by source code analysis, however, such an approach can be difficult to use in multi-tier hybrid software systems, or legacy applications where conventional code analysis tools simply do not work as is. In this paper, we propose a hybrid approach to detecting software dependencies by combining conceptual and domain-based coupling metrics. In recent years, a great deal of research focused on deriving various coupling metrics from these sources of information with the aim of assisting software maintainers. Conceptual metrics specify underlying relationships encoded by developers in identifiers and comments of source code classes whereas domain metrics exploit coupling manifested in domain-level information of software components and it is\u00a0\u2026", "num_citations": "10\n", "authors": ["1015"]}
{"title": "Redacting sensitive information in software artifacts\n", "abstract": " In the past decade, there have been many well-publicized cases of source code leaking from different well-known companies. These leaks pose a serious problem when the source code contains sensitive information encoded in its identifier names and comments. Unfortunately, redacting the sensitive information requires obfuscating the identifiers, which will quickly interfere with program comprehension. Program comprehension is key for programmers in understanding the source code, so sensitive information is often left unredacted.", "num_citations": "8\n", "authors": ["1015"]}
{"title": "Are unreachable methods harmful? results from a controlled experiment\n", "abstract": " In this paper, we present the results of a controlled experiment conducted to assess whether the presence of unreachable methods in source code affects source code comprehensibility and modifiability. A total of 47 undergraduate students at the University of Basilicata participated in this experiment. We divided the participants in two groups. The participants in the first group were asked to comprehend code base containing unreachable methods and implement five change requests in that code base. The participants in the second group were asked to accomplish exactly the same tasks as the participants in the first group, however, the source code provided to them did not contain any unreachable methods. The results of the study indicate that code comprehensibility is significantly higher when source code does not contain unreachable methods. However, we did not observe a statistically significant difference for\u00a0\u2026", "num_citations": "7\n", "authors": ["1015"]}
{"title": "Using information retrieval to support software maintenance tasks\n", "abstract": " Software is comprised of a multitude of artifacts; some of them are intended to be read by the compiler, while many others are intended to be read by developers. During software evolution developers have to maintain large software, often written by others. The user centric information is often expressed in natural language and it is embedded in documentation and source code. External documentation written in natural language (eg, requirements, design documents, user manuals, etc.), the comments, and the identifiers encode to a large degree the domain of the software and capture design decisions, change requests, developer information, etc. This mainly unstructured information is usually larger in size than the source code. Storing and sharing this information is much needed today, when most development teams are distributed geographically and change frequently over time. Given the large amount of textual\u00a0\u2026", "num_citations": "7\n", "authors": ["1015"]}
{"title": "Selected publications\n", "abstract": " MC Ray, R. Bhattacharya, and B. Samanta. Exact solutions for dynamic analysis of composite plates with distributed piezoelectric layers. Computers and Structures, 66: 737\u2013743, 1998. Ray, MC (2003),\" Zeroth-order shear deformation theory for laminated composite plates\", J. Appl. Mech., 70 (3), 374-380.", "num_citations": "7\n", "authors": ["1015"]}
{"title": "Systematic mutation-based evaluation of the soundness of security-focused android static analysis techniques\n", "abstract": " Mobile application security has been a major area of focus for security research over the course of the last decade. Numerous application analysis tools have been proposed in response to malicious, curious, or vulnerable apps. However, existing tools, and specifically, static analysis tools, trade soundness of the analysis for precision and performance and are hence soundy. Unfortunately, the specific unsound choices or flaws in the design of these tools is often not known or well documented, leading to misplaced confidence among researchers, developers, and users. This article describes the Mutation-Based Soundness Evaluation (\u03bcSE) framework, which systematically evaluates Android static analysis tools to discover, document, and fix flaws, by leveraging the well-founded practice of mutation analysis. We implemented \u03bcSE and applied it to a set of prominent Android static analysis tools that detect private\u00a0\u2026", "num_citations": "4\n", "authors": ["1015"]}
{"title": "Security in Centralized Data Store-based Home Automation Platforms: A Systematic Analysis of Nest and Hue\n", "abstract": " Home automation platforms enable consumers to conveniently automate various physical aspects of their homes. However, the security flaws in the platforms or integrated third-party products can have serious security and safety implications for the user\u2019s physical environment. This article describes our systematic security evaluation of two popular smart home platforms, Google\u2019s Nest platform and Philips Hue, which implement home automation \u201croutines\u201d (i.e., trigger-action programs involving apps and devices) via manipulation of state variables in a centralized data store. Our semi-automated analysis examines, among other things, platform access control enforcement, the rigor of non-system enforcement procedures, and the potential for misuse of routines, and it leads to 11 key findings with serious security implications. We combine several of the vulnerabilities we find to demonstrate the first end-to-end instance\u00a0\u2026", "num_citations": "4\n", "authors": ["1015"]}
{"title": "Continuous, Evolutionary and Large-Scale: A New Perspective for Automated Mobile App Testing\n", "abstract": " Mobile app development involves a unique set of challenges including device fragmentation and rapidly evolving platforms, making testing a difficult task. The design space for a comprehensive mobile testing strategy includes features, inputs, potential contextual app states, and large combinations of devices and underlying platforms. Therefore, automated testing is an essential activity of the development process. However, current state of the art of automated testing tools for mobile apps poses limitations that has driven a preference for manual testing in practice. As of today, there is no comprehensive automated solution for mobile testing that overcomes fundamental issues such as automated oracles, history awareness in test cases, or automated evolution of test cases. In this perspective paper we survey the current state of the art in terms of the frameworks, tools, and services available to developers to aid in mobile testing, highlighting present shortcomings. Next, we provide commentary on current key challenges that restrict the possibility of a comprehensive, effective, and practical automated testing solution. Finally, we offer our vision of a comprehensive mobile app testing framework, complete with research agenda, that is succinctly summarized along three principles: Continuous, Evolutionary and Large-scale (CEL).", "num_citations": "4\n", "authors": ["1015"]}
{"title": "Automatically detecting integrity violations in database-centric applications\n", "abstract": " Database-centric applications (DCAs) are widely used by many companies and organizations to perform various control and analytical tasks using large databases. Real-world databases are described by complex schemas that oftentimes contain hundreds of tables consisting of thousands of attributes. However, when software engineers develop DCAs, they may write code that can inadvertently violate the integrity of these databases. Alternatively, business analysts and database administrators can also make errors that lead to integrity violations (semantic bugs). To detect these violations, stakeholders must create assertions that check the validity of the data in the rows of the database tables. Unfortunately, creating assertions is a manual, laborious and error-prone task. Thus, a fundamental problem of testing DCAs is how to find such semantic bugs automatically. We propose a novel solution, namely DACITE, that\u00a0\u2026", "num_citations": "2\n", "authors": ["1015"]}
{"title": "Fixing bug reporting for mobile and GUI-based applications\n", "abstract": " Smartphones and tablets have established themselves as mainstays in the modern computing landscape. It is conceivable that in the near future such devices may supplant laptops and desktops, becoming many users primary means of carrying out typical computer assisted tasks. In turn, this means that mobile applications will continue on a trajectory to becoming more complex, and the primary focus of millions of developers worldwide. In order to properly create and maintain these \u201capps\u201d developers will need support, especially with regard to the prompt confirmation and resolution of bug reports. Unfortunately, current issue tracking systems typically only implement collection of coarse grained natural language descriptions, and lack features to facilitate reporters including important information in their reports. This illustrates the lexical information gap that exists in current bug reporting systems for mobile and GUI\u00a0\u2026", "num_citations": "2\n", "authors": ["1015"]}
{"title": "Evaluating recommended applications\n", "abstract": " Large open source software repositories are polluted with incomplete or inadequately functioning projects having scarce or poor descriptions. Developers often search these repositories to find sample applications containing implementations of relevant features. While relying on software search engines that retrieve germane applications based on direct matches between user queries and words in the descriptions (or source code files), it is difficult to warrant that retrieved applications contain functionality described by their authors in project summaries.", "num_citations": "2\n", "authors": ["1015"]}
{"title": "A Systematic Literature Review on the Use of Deep Learning in Software Engineering Research\n", "abstract": " An increasingly popular set of techniques adopted by software engineering (SE) researchers to automate development tasks are those rooted in the concept of Deep Learning (DL). The popularity of such techniques largely stems from their automated feature engineering capabilities, which aid in modeling software artifacts. However, due to the rapid pace at which DL techniques have been adopted, it is difficult to distill the current successes, failures, and opportunities of the current research landscape. In an effort to bring clarity to this cross-cutting area of work, from its modern inception to the present, this paper presents a systematic literature review of research at the intersection of SE & DL. The review canvases work appearing in the most prominent SE and DL conferences and journals and spans 84 papers across 22 unique SE tasks. We center our analysis around the components of learning, a set of principles that govern the application of machine learning techniques (ML) to a given problem domain, discussing several aspects of the surveyed work at a granular level. The end result of our analysis is a research roadmap that both delineates the foundations of DL techniques applied to SE research, and likely areas of fertile exploration for the future.", "num_citations": "1\n", "authors": ["1015"]}
{"title": "Helion: Enabling a natural perspective of home automation\n", "abstract": " Security researchers have recently discovered significant security and safety issues related to home automation and developed approaches to address them. Such approaches often face design and evaluation challenges which arise from their restricted perspective of home automation that is bounded by the IoT apps they analyze. The challenges of past work can be overcome by relying on a deeper understanding of realistic home automation usage. More specifically, the availability of natural home automation scenarios, i.e., sequences of home automation events that may realistically occur in an end-user's home, could help security researchers design better security/safety systems. This paper presents Helion, a framework for building a natural perspective of home automation. Helion identifies the regularities in user-driven home automation, i.e., from user-driven routines that are increasingly being created by users through intuitive platform UIs. Our intuition for designing Helion is that smart home event sequences created by users exhibit an inherent set of semantic patterns, or naturalness that can be modeled and used to generate valid and useful scenarios. To evaluate our approach, we first empirically demonstrate that this naturalness hypothesis holds, with a corpus of 30,518 home automation events, constructed from 273 routines collected from 40 users. We then demonstrate that the scenarios generated by Helion are reasonable and valid from an end-user perspective, through an evaluation with 16 external evaluators. We further show the usefulness of Helion's scenarios by generating 17 home security/safety policies with significantly\u00a0\u2026", "num_citations": "1\n", "authors": ["1015"]}
{"title": "Can software project maturity be accurately predicted using internal source code metrics?\n", "abstract": " Predicting a level of maturity (LoM) of a software project is important for multiple reasons including planning resource allocation, evaluating the cost, and suggesting delivery dates for software applications. It is not clear how well LoM can be actually predicted \u2013 mixed results are reported that are based on studying small numbers of subject software applications and internal software metrics. Thus, a fundamental problem and question of software engineering is if LoM can be accurately predicted using internal software metrics alone?               We reformulated this problem as a supervised machine learning problem to verify if internal software metrics, collectively, are good predictors of software quality. To answer this question, we conducted a large-scale empirical study with 3,392 open-source projects using six different classifiers. Further, our contribution is that it is the first use of feature selection algorithms to\u00a0\u2026", "num_citations": "1\n", "authors": ["1015"]}