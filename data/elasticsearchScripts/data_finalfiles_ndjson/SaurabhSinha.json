{"title": "Analysis and testing of programs with exception handling constructs\n", "abstract": " Analysis techniques, such as control flow, data flow, and control dependence, are used for a variety of software engineering tasks, including structural and regression testing, dynamic execution profiling, static and dynamic slicing, and program understanding. To be applicable to programs in languages such as Java and C++, these analysis techniques must account for the effects of exception occurrences and exception handling constructs; failure to do so can cause the analysis techniques to compute incorrect results and, thus, limit the usefulness of the applications that use them. This paper discusses the effects of exception handling constructs on several analysis techniques. The paper presents techniques to construct representations for programs with explicit exception occurrences-exceptions that are raised explicitly through throw statements-and exception handling constructs. The paper presents algorithms that\u00a0\u2026", "num_citations": "255\n", "authors": ["1821"]}
{"title": "Interprocedural control dependence\n", "abstract": " Program-dependence information is useful for a variety of applications, such as software testing and maintenance tasks, and code optimization. Properly defined, control and data dependences can be used to identify semantic dependences. To function effectively on whole programs, tools that utilize dependence information require information about interprocedural dependences: dependences that are identified by analyzing the interactions among procedures. Many techniques for computing interprocedural data dependences exist; however, virtually no attention has been paid to interprocedural control dependence. Analysis techniques that fail to account for interprocedural control dependences can suffer unnecessary imprecision and loss of safety. This article presents a definition of  interprocedural control dependence that supports the relationship of control and data dependence to semantic dependence. The\u00a0\u2026", "num_citations": "123\n", "authors": ["1821"]}
{"title": "Understanding myths and realities of test-suite evolution\n", "abstract": " Test suites, once created, rarely remain static. Just like the application they are testing, they evolve throughout their lifetime. Test obsolescence is probably the most known reason for test-suite evolution---test cases cease to work because of changes in the code and must be suitably repaired. Repairing existing test cases manually, however, can be extremely time consuming, especially for large test suites, which has motivated the recent development of automated test-repair techniques. We believe that, for developing effective repair techniques that are applicable in real-world scenarios, a fundamental prerequisite is a thorough understanding of how test cases evolve in practice. Without such knowledge, we risk to develop techniques that may work well for only a small number of tests or, worse, that may not work at all in most realistic cases. Unfortunately, to date there are no studies in the literature that investigate\u00a0\u2026", "num_citations": "122\n", "authors": ["1821"]}
{"title": "An approach to analyzing and testing component-based systems\n", "abstract": " Software testing and maintenance account for as much as two-thirds of the cost of software production. Program analysis techniques o er the potential to automate testing and maintenance tasks, and thereby reduce the cost of these tasks. An emerging paradigm of software development that promises to enhance software productivity and quality composes software from independently-developed components. The nature of component-based software, however, introduces new problems for applying program analysis techniques to testing and maintenance of such software: the providers of software components develop and test the components independently of the applications that use the components; the users of software components analyze and test their applications without access to source code of the components used in those applications. This paper describes issues and challenges in applying analysis and testing techniques to component-based software and presents an approach to analyzing and testing component-based systems.Keywords Component-based system, software components, program analysis, software testing 1 INTRODUCTION Software testing and maintenance account for as much as two-thirds of the cost of software production 22]. Software tools that use program-analysis techniques promise to automate testing and maintenance tasks, and thereby reduce the cost of these tasks and improve software quality. For example, data-ow testing techniques use data-ow information (eg, 1, 11, 21]) to evaluate the adequacy of test suites and to assist in test development (eg, 6, 10]). Data-ow information is also useful for\u00a0\u2026", "num_citations": "112\n", "authors": ["1821"]}
{"title": "Accurate interprocedural null-dereference analysis for Java\n", "abstract": " Null dereference is a commonly occurring defect in Java programs, and many static-analysis tools identify such defects. However, most of the existing tools perform a limited interprocedural analysis. In this paper, we present an interprocedural path-sensitive and context-sensitive analysis for identifying null dereferences. Starting at a dereference statement, our approach performs a backward demand-driven analysis to identify precisely paths along which null values may flow to the dereference. The demand-driven analysis avoids an exhaustive program exploration, which lets it scale to large programs. We present the results of empirical studies conducted using large open-source and commercial products. Our results show that: (1) our approach detects fewer false positives, and significantly more interprocedural true positives, than other commonly used tools; (2) the analysis scales to large subjects; and (3) the\u00a0\u2026", "num_citations": "100\n", "authors": ["1821"]}
{"title": "Analysis of programs with exception-handling constructs\n", "abstract": " Analysis techniques, such as control flow, data flow, and control dependence, are used for a variety of maintenance tasks, including regression testing, dynamic execution profiling, and static and dynamic slicing. To be applicable to programs in languages, such as Java and C++ however, these analysis techniques should, to the extent possible, account for the effects of exception occurrences and exception handling constructs. The paper presents techniques to construct intraprocedural and interprocedural representations on which existing techniques can be performed and demonstrates their applicability to several maintenance tasks.", "num_citations": "94\n", "authors": ["1821"]}
{"title": "Computation of interprocedural control dependence\n", "abstract": " Program dependence information is useful for a variety of software testing and maintenance tasks. Properly defined, control and data dependencies can be used to identify semantic dependencies. To function effectively on whole programs, tools that utilize dependence information require information about interprocedural dependencies: dependencies that exist because of interactions among procedures. Many techniques for computing data and control dependencies exist; however, in our search of the literature we find only one attempt to define and compute interprocedural control dependencies. Unfortunately, that approach can omit important control dependencies, and incorrectly identifies control dependencies for a large class of programs. This paper presents a definition of interprocedural control dependence that supports the relationship of control and data dependence to semantic dependence, an efficient\u00a0\u2026", "num_citations": "91\n", "authors": ["1821"]}
{"title": "Criteria for testing exception-handling constructs in Java programs\n", "abstract": " Exception-handling constructs provide a mechanism for mixing exceptions and a facility for designating protected code by attaching exception handlers to blocks of code. Despite the frequency of their occurrences, the behavior of exception-handling constructs is often the least understood and poorly tested part of a program. The presence of such constructs introduces new structural elements, such as control-flow paths, in a program. To adequately test such programs, these new structural elements must be considered for coverage during structural testing. In this paper, we describe a class of adequacy criteria that can be used to test the behavior of exception-handling constructs. We present a subsumption hierarchy of the criteria, and illustrate the relationship of the criteria to those found in traditional subsumption hierarchies. We describe techniques for generating the testing requirements for the criteria using our\u00a0\u2026", "num_citations": "87\n", "authors": ["1821"]}
{"title": "Automated support for development, maintenance, and testing in the presence of implicit flow control\n", "abstract": " Although object-oriented languages can improve programming practices, their characteristics may introduce new problems for software engineers. One important problem is the presence of implicit control flow caused by exception handling and polymorphism. Implicit control flow causes complex interactions, and can thus complicate software-engineering tasks. To address this problem, we present a systematic and structured approach, for supporting these tasks, based on the static and dynamic analyses of constructs that cause implicit control flow. Our approach provides software engineers with information for supporting and guiding development and maintenance tasks. We also present empirical results to illustrate the potential usefulness of our approach. Our studies show that, for the subjects considered, complex implicit control flow is always present and is generally not adequately exercised.", "num_citations": "65\n", "authors": ["1821"]}
{"title": "Regression testing in the presence of non-code changes\n", "abstract": " Regression testing is an important activity performed to validate modified software, and one of its key tasks is regression test selection (RTS) -- selecting a subset of existing test cases to run on the modified software. Most existing RTS techniques focus on changes made to code components and completely ignore non-code elements, such as configuration files and databases, which can also change and affect the system behavior. To address this issue, we present a new RTS technique that performs accurate test selection in the presence of changes to non-code components. To do this, our technique computes traceability between test cases and the external data accessed by an application, and uses this information to perform RTS in the presence of changes to non-code elements. We present our technique, a prototype implementation of our technique, and a set of preliminary empirical results that illustrate the\u00a0\u2026", "num_citations": "63\n", "authors": ["1821"]}
{"title": "Incremental slicing based on data-dependences types\n", "abstract": " Program slicing is useful for assisting with many software-maintenance tasks. The presence and frequent usage of pointers in languages such as C causes complex data dependences. To function effectively on such programs, slicing techniques must account for pointer-induced data dependences. Existing slicing techniques do not distinguish data dependences based on their types. This paper presents a new slicing technique, in which slices are computed based on types of data dependences. This new slicing technique offers several benefits and can be exploited in different ways, such as identifying subtle data dependences for debugging, computing reduced-size slices quickly for complex programs, and performing incremental slicing. This paper describes an algorithm for incremental slicing that increases the scope of a slice in steps, by incorporating different types of data dependences at each step. The paper\u00a0\u2026", "num_citations": "50\n", "authors": ["1821"]}
{"title": "Classifying data dependences in the presence of pointers for program comprehension, testing, and debugging\n", "abstract": " Understanding data dependences in programs is important for many software-engineering activities, such as program understanding, impact analysis, reverse engineering, and debugging. The presence of pointers can cause subtle and complex data dependences that can be difficult to understand. For example, in languages such as C, an assignment made through a pointer dereference can assign a value to one of several variables, none of which may appear syntactically in that statement. In the first part of this article, we describe two techniques for classifying data dependences in the presence of pointer dereferences. The first technique classifies data dependences based on definition type, use type, and path type. The second technique classifies data dependences based on span. We present empirical results to illustrate the distribution of data-dependence types and spans for a set of real C programs. In the\u00a0\u2026", "num_citations": "43\n", "authors": ["1821"]}
{"title": "Effects of pointers on data dependences\n", "abstract": " This paper presents a technique for computing and classifying data dependences that takes into account the complexities introduced by specific language constructs, such as pointers, arrays and structures. The classification is finer-grained than previously proposed classifications. Moreover unlike previous work, the paper presents empirical results that illustrate the distribution of data dependences for a set of C subjects. The paper also presents a potential application for the proposed classification, program slicing, and a technique that completes slices based on data-dependence types. This technique facilitates the use of slicing for program understanding because a user can either augment a slice incrementally, by incorporating data dependences based on their relevance, or focus on specific kinds of dependences. Finally, the paper presents a case study that shows how the incremental addition of data\u00a0\u2026", "num_citations": "40\n", "authors": ["1821"]}
{"title": "A case study: productivity and quality gains using an object\u2010oriented framework\n", "abstract": " The Neuro\u2010Oncology Information System (NOIS) supports researchers and other personnel throughout the United States engaged in brain tumor research. Graphical user interfaces that allow data input into the NOIS have been evolving over several years.  This paper describes the design and implementation of the NOIS Input Forms as they migrated from a procedural approach to a static object\u2010oriented approach, and finally to a framework approach in which not only static components were reused, but also the patterns of interaction among the components. The paper documents a significant gain in productivity and quality that was realized when using the framework design paradigm. Copyright \u00a9 1999 John Wiley & Sons, Ltd.", "num_citations": "18\n", "authors": ["1821"]}
{"title": "Semantics-based reverse engineering of object-oriented data models\n", "abstract": " We present an algorithm for reverse engineering object-oriented (OO) data models from programs written in weakly-typed languages like Cobol. These models, similar to UML class diagrams, can facilitate a variety of program maintenance and migration activities. Our algorithm is based on a semantic analysis of the program's code, and we provide a bisimulation-based formalization of what it means for an OO data model to be correct for a program.", "num_citations": "17\n", "authors": ["1821"]}
{"title": "Adaptation of automated test scripts\n", "abstract": " Embodiments provide a computerized method for adapting automating test scripts, said method including: utilizing at least one processor to execute computer code that performs the steps of: receiving, at an input device, an original test script created to test an application; utilizing the original test script to test, using the processor, a variant of the application; identifying, using the processor, failures in the original test script when the variant of the application is being tested; and modifying, using the processor, the original test script to overcome the identified failures.", "num_citations": "16\n", "authors": ["1821"]}
{"title": "Execution hijacking: Improving dynamic analysis by flying off course\n", "abstract": " Typically, dynamic-analysis techniques operate on a small subset of all possible program behaviors, which limits their effectiveness and the representativeness of the computed results. To address this issue, a new paradigm is emerging: execution hijacking, consisting of techniques that explore a larger set of program behaviors by forcing executions along specific paths. Although hijacked executions are infeasible for the given inputs, they can still produce feasible behaviors that could be observed under other inputs. In such cases, execution hijacking can improve the effectiveness of dynamic analysis without requiring the (expensive) generation of additional inputs. To evaluate the usefulness of execution hijacking, we defined, implemented, and evaluated several variants of it. Specifically, we performed an empirical study where we assessed whether execution hijacking could improve the effectiveness of a\u00a0\u2026", "num_citations": "16\n", "authors": ["1821"]}
{"title": "Automated test input generation for integration testing of microservice-based web applications\n", "abstract": " Techniques for automated generation of inputs for testing microservice-based applications are provided. In one example, a computer-implemented method comprises: traversing, by a system operatively coupled to a processor, a user interface of a microservices-based application by performing actions on user interface elements of the user interface; and generating, by the system, an aggregated log of user interface event sequences and application program interface call sets based on the traversing. The computer-implemented method also comprises: determining, by the system, respective user interface event sequences that invoke application program interface call sets; and generating, by the system, respective test inputs based on the user interface event sequences that invoke the application program interface call sets.", "num_citations": "13\n", "authors": ["1821"]}
{"title": "Prioritizing resiliency tests of microservices\n", "abstract": " Techniques are provided for automated resiliency testing. In one example, a computer-implemented method comprises analyzing, by a system operatively coupled to a processor, an annotated state transition graph of a user interface of a microservices-based application, wherein the annotated state transition graph has edges annotated with application program interface call subgraphs. The computer-implemented method also comprises generating, by the system, an ordered list of the application program interface call subgraphs based on the analyzing.", "num_citations": "13\n", "authors": ["1821"]}
{"title": "Prioritizing resiliency tests of microservices\n", "abstract": " Techniques for automated resiliency testing systems are provided. In one example, a computer-implemented method comprises traversing, by a system operatively coupled to a processor, an application program interface call subgraph of a microservices-based application in a depth first traversal. The computer-implemented method also comprises, during the traversing, performing, by the system, resiliency testing of parent application program interfaces of the application program interface call subgraph according to a systematic resilience testing algorithm that reduces and/or eliminates redundant resiliency testing of parent application program interfaces.", "num_citations": "12\n", "authors": ["1821"]}
{"title": "Efficient and flexible GUI test execution via test merging\n", "abstract": " As a test suite evolves, it can accumulate redundant tests. To address this problem, many test-suite reduction techniques, based on different measures of redundancy, have been developed. A more subtle problem, that can also cause test-suite bloat and that has not been addressed by existing research, is the accumulation of similar tests. Similar tests are not redundant by any measure; but, they contain many common actions that are executed repeatedly, which over a large test suite, can degrade execution time substantially.", "num_citations": "12\n", "authors": ["1821"]}
{"title": "TestEvol: a tool for analyzing test-suite evolution\n", "abstract": " Test suites, just like the applications they are testing, evolve throughout their lifetime. One of the main reasons for test-suite evolution is test obsolescence: test cases cease to work because of changes in the code and must be suitably repaired. There are several reasons why it is important to achieve a thorough understanding of how test cases evolve in practice. In particular, researchers who investigate automated test repair - an increasingly active research area - can use such understanding to develop more effective repair techniques that can be successfully applied in real-world scenarios. More generally, analyzing testsuite evolution can help testers better understand how test cases are modified during maintenance and improve the test evolution process, an extremely time consuming activity for any nontrivial test suite. Unfortunately, there are no existing tools that facilitate investigation of test evolution. To tackle\u00a0\u2026", "num_citations": "12\n", "authors": ["1821"]}
{"title": "Control-flow analysis of programs with exception-handling constructs\n", "abstract": " Analysis techniques, such as control-flow, data-flow, and control-dependence, are used for a variety of maintenance tasks, including regression testing, dynamic execution proling, and static and dynamic slicing. To be applicable to programs in languages, such as Java and C++ however, these analysis techniques should, to the extent possible, account for the effects of exception occurrences and exception-handling constructs. This paper describes control-flow representations that incorporate the effects on control-flow caused by exception-handling constructs, and presents techniques to construct intraprocedural and interprocedural control-flow representations on which existing analysis techniques can be performed.", "num_citations": "12\n", "authors": ["1821"]}
{"title": "Automated modularization of GUI test cases\n", "abstract": " Test cases that drive an application under test via its graphical user interface (GUI) consist of sequences of steps that perform actions on, or verify the state of, the application user interface. Such tests can be hard to maintain, especially if they are not properly modularized - that is, common steps occur in many test cases, which can make test maintenance cumbersome and expensive. Performing modularization manually can take up considerable human effort. To address this, we present an automated approach for modularizing GUI test cases. Our approach consists of multiple phases. In the first phase, it analyzes individual test cases to partition test steps into candidate subroutines, based on how user-interface elements are accessed in the steps. This phase can analyze the test cases only or also leverage execution traces of the tests, which involves a cost-accuracy tradeoff. In the second phase, the technique\u00a0\u2026", "num_citations": "11\n", "authors": ["1821"]}
{"title": "Subsumption of program entities for efficient coverage and monitoring\n", "abstract": " Program entities such as branches, def-use pairs, and call sequences are used in diverse software-development tasks. Reducing a set of entities to a small representative subset through subsumption saves monitoring overhead, focuses the developer's attention, and provides insights into the complexity of a program. Previous work has solved this problem for entities of the same type, and only for some types. In this paper we introduce a novel and general approach for subsumption of entities of any type based on predicate conditions. We discuss applications of this technique, and address future steps.", "num_citations": "11\n", "authors": ["1821"]}
{"title": "Automated modularization of graphical user interface test cases\n", "abstract": " Methods, systems, and computer program products for automated modularization of GUI test cases are provided herein. A method includes grouping test steps derived from one or more application test cases into multiple candidate sub-routines based on a manner in which one or more user interface elements are accessed in each of the test steps; refining the multiple candidate sub-routines to generate a given set of one or more sub-routines; and refactoring the one or more application test cases by replacing the test steps in the one or more application test cases with one or more calls to the given set of one or more sub-routines.", "num_citations": "10\n", "authors": ["1821"]}
{"title": "Accessing a medical database using WWW-based user interfaces\n", "abstract": " We have implemented a Neuro-Oncolgy Information System (NOIS) for researchers and other personnel engaged in brain-tumor research throughout the United States. In this paper we describe our migration of the implementation of the NOIS user interface from Oracle-based products to Java applets. Our goal in the migration was to design the interface software not only as a set of reusable objects, but also to reuse the patterns of interaction among the components by way of object-oriented frameworks. We describe in detail the design of the user interface classes and the connection objects that let Java applets communicate with the database, and we evaluate our success in meeting our design goal.", "num_citations": "8\n", "authors": ["1821"]}
{"title": "Identifying services from legacy batch applications\n", "abstract": " Transaction processing is a key constituent of the IT workload of commercial enterprises (eg, banks, insurance companies). Even today, in many large enterprises, transaction processing is done by legacy\" batch\" applications, which run offline and process accumulated transactions. Developers acknowledge the presence of multiple loosely coupled pieces of functionality within individual applications. Identifying such pieces of functionality (which we call\" services\") is desirable for the maintenance and evolution of these legacy applications. This is a hard problem, which enterprises grapple with, and one without satisfactory automated solutions.", "num_citations": "5\n", "authors": ["1821"]}
{"title": "Understanding data dependences in the presence of pointers\n", "abstract": " Understanding data dependences in programs is important for many software-engineering activities, such as program understanding, impact analysis, reverse engineering, and debugging. The presence of pointers, arrays, and structures can cause subtle and complex data dependences that can be difficult to understand. For example, in languages such as C, an assignment made through a pointer dereference can assign a value to one of several variables, none of which may appear syntactically in that statement. In the first part of this paper, we describe two techniques for classifying data dependences in the presence of pointer dereferences. The first technique classifies data dependences based on definition type, use type, and path type. The second technique classifies data dependences based on span. We present empirical results to illustrate the distribution of data-dependence types and spans for a set of real C programs. In the second part of the paper, we discuss two applications of the classification techniques. First, we investigate different ways in which the classification can be used to facilitate data-flow testing and verification. We outline an approach that uses types and spans of data dependences to determine the appropriate verification technique for different data dependences; we present empirical results to illustrate the approach. Second, we present a new slicing paradigm that computes slices based on types of data dependences. Based on the new paradigm, we define an incremental slicing technique that computes a slice in multiple steps. We present empirical results to illustrate the sizes of incremental slices and the potential\u00a0\u2026", "num_citations": "4\n", "authors": ["1821"]}
{"title": "Parametric process model inference\n", "abstract": " Legacy applications can be difficult and time-consuming to understand and update due to the lack of modern abstraction mechanisms in legacy languages, as well as the gradual deterioration of code due to repeated maintenance activities. We present an approach for reverse engineering process model abstractions from legacy code. Such a process model can provide a quick initial understanding of an application, and can be a useful starting point for further program exploration. Our approach takes as input a user specification of interesting events, and creates a representation (i.e., a process model) that concisely depicts the occurrences of the events and the possible control-flow among them. The key features of our approach are the use of a logical data model of the program for specifying the events, and graph-projection techniques for creating the process model.", "num_citations": "3\n", "authors": ["1821"]}
{"title": "A framework for understanding data dependences\n", "abstract": " Identifying and understanding data dependences is important for a variety of software-engineering tasks.  The presence of pointers, arrays, and dynamic memory allocation introduces subtle and complex data dependences that may be difficult to understand.  In this paper, we present a refinement of our previously developed classification that also distinguishes the types of memory locations, considers interprocedural data dependences, and further distinguishes such data dependences based on the kinds of interprocedura paths on which they occur.  This new classification enables reasoning about the complexity of data dependences in programs using features such as pointers, arrays, and dynamic memory allocation.  We present an algorithm for computing interprocedural data dependences according to our classification.  To evaluate the classification,  we compute the distribution of data dependences for a set of real C programs  and we discuss how the distribution can be useful in understanding the  characteristics of a program. We also evaluate how alias information  provided by different algorithms, varying in precision, affects the  distribution. Finally, we investigate how the classification can be exploited  to estimate complexity of the data dependences in a program.", "num_citations": "3\n", "authors": ["1821"]}
{"title": "Automatically detecting feature mismatches between mobile application versions on different platforms\n", "abstract": " One embodiment provides a method of identifying discrepancies of an application operating on multiple operating platforms, the method including: utilizing at least one processor to execute computer code that performs the steps of: obtaining a first source code associated with an application of a first operating platform, the first source code including at least one feature of the application; generating, based on the first source code, a first context graph; obtaining a second source code associated with the application of a second operating platform, the second source code including the at least one feature of the application; generating, based on the second source code, a second context graph; comparing the first and second context graph; and identifying, based on the comparison, at least one discrepancy. Other variants and embodiments are broadly contemplated herein.", "num_citations": "2\n", "authors": ["1821"]}
{"title": "Automated test input generation for integration testing of microservice-based web applications\n", "abstract": " Techniques for automated generation of inputs for testing microservice-based applications are provided. In one example, a computer-implemented method traverses a user interface of a microservices-based application by performing actions on user interface elements of the user interface, and generates an aggregated log of user interface event sequences and application program interface call sets based on the traversing. The computer-implemented method also determines respective user interface event sequences that invoke application program interface call sets and generates respective test inputs based on the user interface event sequences that invoke the application program interface call sets.", "num_citations": "1\n", "authors": ["1821"]}
{"title": "Mono2Micro: a practical and effective tool for decomposing monolithic Java applications to microservices\n", "abstract": " In migrating production workloads to cloud, enterprises often face the daunting task of evolving monolithic applications toward a microservice architecture. At IBM, we developed a tool called Mono2Micro to assist with this challenging task. Mono2Micro performs spatio-temporal decomposition, leveraging well-defined business use cases and runtime call relations to create functionally cohesive partitioning of application classes. Our preliminary evaluation of Mono2Micro showed promising results.", "num_citations": "1\n", "authors": ["1821"]}