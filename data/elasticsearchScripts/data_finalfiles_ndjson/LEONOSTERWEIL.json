{"title": "Software processes are software too\n", "abstract": " The major theme of this meeting is the exploration of the importance of process as a vehicle for improving both the quality of software products and the way in which we develop and evolve them. In beginning this exploration it seems important to spend at least a short time examining the nature of process and convincing ourselves that this is indeed a promising vehicle.", "num_citations": "1500\n", "authors": ["366"]}
{"title": "Data flow analysis in software reliability\n", "abstract": " The ways that the methods of data flow analysis can be applied to improve software reliability are described. There is also a review of the basic terminology from graph theory and from data flow analysis in global program optimization. The notation of regular expressions is used to describe actions on data for sets of paths. These expressions provide the basis of a classification scheme for data flow which represents patterns of data flow along paths within subprograms and along paths which cross subprogram boundaries. Fast algorithms, originally introduced for global optimization, are described and it is shown how they can be used to implement the classification scheme. It is then shown how these same algorithms can also be used to detect the presence of data flow anomalies which are symptomatic of programming errors. Finally, some characteristics of and experience with DAVE, a data flow analysis system\u00a0\u2026", "num_citations": "397\n", "authors": ["366"]}
{"title": "Foundations for the Arcadia environment architecture\n", "abstract": " Early software environments have supported a narrow range of activities (programming environments) or else been restricted to a single \u201chard-wired\u201d software development process. The Arcadia research project is investigating the construction of software environments that are tightly integrated, yet flexible and extensible enough to support experimentation with alternative software processes and tools. This has led us to view an environment as being composed of two distinct, cooperating parts. One is the variant part, consisting of process programs and the tools and objects used and defined by those programs. The other is the fixed part, or infrastructure, supporting creation, execution, and change to the constituents of the variant part. The major components of the infrastructure are a process programming language and interpreter, object management system, and user interface management system. Process\u00a0\u2026", "num_citations": "355\n", "authors": ["366"]}
{"title": "ISPW-6 software process example\n", "abstract": " One of the important activities undertaken in conjunction with the 6th International Software Process Workshop (ISPW-6) was a comparison of various solutions to a standard softwarc process modcling example problem. The primary purpose of this effort was to facilitate understanding, comparing, and assessing the various approaches that are being pursued for software process modeling. Secondary goals included communicating the diversity of aspects of process actually encountered in modeling real-world software processes, and providing impetus to efforts to extend the proposed approaches to cope effectively with these process issues.It was determined that these objectives would be best served by soliciting solutions to a standard software process modeling example problem. The use of a standard benchmark problem facilitates comparisons of various modeling approaches. Consequently, a working group\u00a0\u2026", "num_citations": "254\n", "authors": ["366"]}
{"title": "On two problems in the generation of program test paths\n", "abstract": " In this paper we analyze the complexity of algorithms for two problems that arise in automatic test path generation for programs: the problem of building a path through a specified set of program statements and the problem of building a path which satisfies impossible-pairs restrictions on statement pairs. These problems are both reduced to graph traversal problems. We give an efficient algorithm for the first, and show that the second is NP-complete.", "num_citations": "226\n", "authors": ["366"]}
{"title": "DAVE\u2014a validation error detection and documentation system for Fortran programs\n", "abstract": " This paper describes DAVE, a system for analysing Fortran programs. DAVE is capable of detecting the symptoms of a wide variety of errors In programs, as well as assuring the absence of these errors. In addition, DAVE exposes and documents subtle data relations and flows within programs. The central analytic procedure used is a depth first search. DAVE itself is written in Fortran. Its implementation at the University of Colorado and some early experience are described.", "num_citations": "218\n", "authors": ["366"]}
{"title": "Exception handling patterns for process modeling\n", "abstract": " Process modeling allows for analysis and improvement of processes that coordinate multiple people and tools working together to carry out a task. Process modeling typically focuses on the normative process, that is, how the collaboration transpires when everything goes as desired. Unfortunately, real-world processes rarely proceed that smoothly. A more complete analysis of a process requires that the process model also include details about what to do when exceptional situations arise. We have found that, in many cases, there are abstract patterns that capture the relationship between exception handling tasks and the normative process. Just as object-oriented design patterns facilitate the development, documentation, and maintenance of object-oriented programs, we believe that process patterns can facilitate the development, documentation, and maintenance of process models. In this paper, we focus on the\u00a0\u2026", "num_citations": "168\n", "authors": ["366"]}
{"title": "Strategic directions in software quality\n", "abstract": " 2. BACKGROUNDA number of studies have suggested that 50\u201360% of the effort involved in producing large software systems is devoted to quality assessment activities such as testing, and the percentage may be significantly higher for life-critical software systems. The most immediately effective leverage is to be obtained simply by bringing previous research and current practice closer together. Very little of the work produced by the research community is even known to the practitioner community, and a truly minuscule amount of this work has ever been transformed into any sort of effective use. The rare successes in bridging1 This article summarizes the conclusions reached by a panel consisting of Leon Osterweil (Chair), Lori A. Clarke, Richard A. DeMillo, Stuart I. Feldman, Bill McKeeman, Edward F. Miller, and John Salasin that met in Cambridge, MA, June 15\u201316, 1996. In addition to the panelists, Daniel\u00a0\u2026", "num_citations": "163\n", "authors": ["366"]}
{"title": "Cecil: A sequencing constraint language for automatic static analysis generation\n", "abstract": " A flexible and general mechanism for specifying problems relating to the sequencing of events and mechanically translating them into dataflow analysis algorithms capable of solving those problems is presented. Dataflow analysis has been used for quite some time in compiler code optimization. Most static analyzers have been custom-built to search for fixed and often quite limited classes of dataflow conditions. It is shown that the range of sequences for which it is interesting and worthwhile to search in actually quite broad and diverse. A formalism for specifying this diversity of conditions is created. It is shown that these conditions can be modeled essentially as dataflow analysis problems for which effective solutions are known. It is also shown how these solutions can be exploited to serve as the basis for mechanical creation of analyzers for these conditions.< >", "num_citations": "143\n", "authors": ["366"]}
{"title": "Toward objective, systematic design-method comparisons\n", "abstract": " Software design methodologies (SDMs) suggest ways to improve productivity and quality. They are collections of complementary design methods and rules for applying them. A base framework and modeling formalism to help designers compare SDMs and define what design issues different SDMs address, which of their components address similar design issues, and ways to integrate the best characteristics of each to make a cleaner, more comprehensive and flexible SDM are presented. The use of formalism and framework and the evaluation of objectivity and completeness using the type and function frameworks are described.< >", "num_citations": "98\n", "authors": ["366"]}
{"title": "A mechanism for environment integration\n", "abstract": " This paper describes research associated with the development and evaluation of Odin-an environment integration system based on the idea that tools should be integrated around a centralized store of persistent software objects. The paper describes this idea in detail and then presents the Odin architecture, which features such notions as the typing of software objects, composing tools out of modular tool fragments, optimizing the storage and rederivation of software objects, and isolating tool interconnectivity information in a single centralized object. The paper then describes some projects that have used Odin to integrate tools on a large scale. Finally, it discusses the significance of this work and the conclusions that can be drawn about superior software environment architectures.", "num_citations": "97\n", "authors": ["366"]}
{"title": "Toolpack\u2014An experimental software development environment research project\n", "abstract": " This paper discusses the goals and methods of the Toolpack project and in this context discusses the architecture and design of the software system being produced as the focus of the project. Toolpack is presented as an experimental activity in which a large software tool environment is being created for the purpose of general distribution and then careful study and analysis. The paper begins by explaining the motivation for building integrated tool sets. It then proceeds to explain the basic requirements that an integrated system of tools must satisfy in order to be successful and to remain useful both in practice and as an experimental object. The paper then summarizes the tool capabilities that will be incorporated into the environment. It then goes on to present a careful description of the actual architecture of the Toolpack integrated tool system. Finally the Toolpack project experimental plan is presented, and future\u00a0\u2026", "num_citations": "94\n", "authors": ["366"]}
{"title": "Software environment research: Directions for the next five years\n", "abstract": " A two-stage strategy, aimed first at understanding the role and characteristics of software environments and then at construction of experimental systems, distinguishes this near-term research plan.", "num_citations": "84\n", "authors": ["366"]}
{"title": "FLAVERS: A finite state verification technique for software systems\n", "abstract": " Software systems are increasing in size and complexity and, subsequently, are becoming ever more difficult to validate. Finite state verification (FSV) has been gaining credibility and attention as an alternative to testing and to formal verification approaches based on theorem proving. There has recently been a great deal of excitement about the potential for FSV approaches to prove properties about hardware descriptions but, for the most part, these approaches do not scale adequately to handle the complexity usually found in software. In this paper, we describe an FSV approach that creates a compact and conservative, but imprecise, model of the system being analyzed, and then assists the analyst in adding additional details as guided by previous analysis results. This paper describes this approach and a prototype implementation called FLAVERS, presents a detailed example, and then provides some\u00a0\u2026", "num_citations": "72\n", "authors": ["366"]}
{"title": "Experience with an approach to comparing software design methodologies\n", "abstract": " Introduces a systematic and defined process called \"comparison of design methodologies\" (CDM) for objectively comparing software design methodologies (SDMs). We believe that using CDM will lead to detailed, traceable, and objective comparisons. CDM uses process modeling techniques to model SDMs, classify their components, and analyze their procedural aspects. Modeling the SDMs entails decomposing their methods into components and analyzing the structure and functioning of the components. The classification of the components illustrates which components address similar design issues and/or have similar structures. Similar components then may be further modeled to aid in more precisely understanding their similarities and differences. The models of the SDMs are also used as the bases for conjectures and analyses about the differences between the SDMs. This paper describes three\u00a0\u2026", "num_citations": "72\n", "authors": ["366"]}
{"title": "Representing process variation with a process family\n", "abstract": " The formalization of process definitions has been an invaluable aid in many domains. However, noticeable variations in processes start to emerge as precise details are added to process definitions. While each such variation gives rise to a different process, these processes might more usefully be considered as variants of each other, rather than completely different processes. This paper proposes that it is beneficial to regard such an appropriately close set of process variants as a process family. The paper suggests a characterization of what might comprise a process family and introduces a formal approach to defining families based upon this characterization. To illustrate this approach, we describe a case study that demonstrates the different variations we observed in processes that define how dispute resolution is performed at the U.S. National Mediation Board. We demonstrate how our approach\u00a0\u2026", "num_citations": "70\n", "authors": ["366"]}
{"title": "Containment units: A hierarchically composable architecture for adaptive systems\n", "abstract": " Software is increasingly expected to run in a variety of environments. The environments themselves are often dynamically changing when using mobile computers or embedded systems, for example. Network bandwidth, available power, or other physical conditions may change, necessitating the use of alternative algorithms within the software, and changing resource mixes to support the software. We present Containment Units as a software architecture useful for recognizing environmental changes and dynamically reconfiguring software and resource allocations to adapt to those changes. We present examples of Containment Units used within robotics along with the results of actual executions, and the application of static analysis to obtain assurances that those Containment Units can be expected to demonstrate the robustness for which they were designed.", "num_citations": "70\n", "authors": ["366"]}
{"title": "Interprocedural static analysis of sequencing constraints\n", "abstract": " This paper describes a system that automatically performs static interprocedural sequencing analysis from programmable constraint specifications. We describe the algorithms used for interprocedural analysis, relate the problems arising from the analysis of real-world programs, and show how these difficulties were overcome. Finally, we sketch the architecture of our prototype analysis system (called Cesar) and describe our experiences to date with its use, citing performance and error detection characteristics.", "num_citations": "69\n", "authors": ["366"]}
{"title": "Simulating patient flow through an emergency department using process-driven discrete event simulation\n", "abstract": " This paper suggests an architecture for supporting discrete event simulations that is based upon using executable process definitions and separate components for specifying resources. The paper describes the architecture and indicates how it might be used to suggest efficiency improvements for hospital emergency departments (EDs). Preliminary results suggest that the proposed architecture provides considerable ease of use and flexibility for specifying a wider range of simulation problems, thus creating the possibility of carrying out a wide range of comparisons of different approaches to ED improvement. Some early comparisons suggest that the simulations are likely to be of value to the medical community and that the simulation architecture offers useful flexibility.", "num_citations": "68\n", "authors": ["366"]}
{"title": "Analytic webs support the synthesis of ecological data sets\n", "abstract": " A wide variety of data sets produced by individual investigators are now synthesized to address ecological questions that span a range of spatial and temporal scales. It is important to facilitate such syntheses so that \u201cconsumers\u201d of data sets can be confident that both input data sets and synthetic products are reliable. Necessary documentation to ensure the reliability and validation of data sets includes both familiar descriptive metadata and formal documentation of the scientific processes used (i.e., process metadata) to produce usable data sets from collections of raw data. Such documentation is complex and difficult to construct, so it is important to help \u201cproducers\u201d create reliable data sets and to facilitate their creation of required metadata. We describe a formal representation, an \u201canalytic web,\u201d that aids both producers and consumers of data sets by providing complete and precise definitions of scientific\u00a0\u2026", "num_citations": "60\n", "authors": ["366"]}
{"title": "Multilanguage interoperability in distributed systems. Experience report\n", "abstract": " The Q system provides interoperability support for multilingual, heterogeneous component-based software systems. Initial development of Q began in 1988, and was driven by the very pragmatic need for a communication mechanism between a client program written in Ada and a server written in C. The initial design was driven by language features present in C, but not in Ada, or vice-versa. In time our needs and aspirations grew and Q evolved to support other languages, such as C++, Lisp, Prolog, Java, and Tcl. As a result of pervasive usage by the Arcadia SDE research project, usage levels and modes of the Q system grew and so more emphasis was placed upon portability, reliability, and performance. In that context we identified specific ways in which programming language support systems can directly impede effective interoperability. This necessitated extensive changes to both our conceptual model and\u00a0\u2026", "num_citations": "56\n", "authors": ["366"]}
{"title": "Verifying properties of process definitions\n", "abstract": " It seems important that the complex processes that synergize humans and computers to solve widening classes of societal problems be subjected to rigorous analysis.  One approach is to use a process definition language to specify these processes and to then use analysis techniques to evaluate these definitions for important correctness properties.  Because humans demand flexibility in their participation in complex processes, process definition languages must incorporate complicated control structures, such as various concurrency, choice, reactive control, and exception mechanisms.  The underlying complexity of these control abstractions, however, often confounds the users' intuitions as well as complicates any analysis.Thus, the control abstraction complexity in process definition languages presents analysis challenges beyond those posed by traditional programming languages.  This paper explores some of\u00a0\u2026", "num_citations": "50\n", "authors": ["366"]}
{"title": "Unifying microprocess and macroprocess research\n", "abstract": " This paper proposes the unification of two complementary approaches to software process research. The two approaches can be characterized as macroprocess research, focused on phenomenological observations of external behaviors of processes, and microprocess research, focused on the study of the internal details and workings of processes. The paper suggests that it is time to bring these approaches together with the goal of using microprocess methods to provide definitive explanations of observed macroprocess behaviors. The paper suggests that this unification could lead to improved understandings leading to improvements in software development practice. The paper observes that such positive outcomes have resulted when the macro- and micro- approaches have been synthesized in domains such as Economics, Physics, and the Life Sciences.", "num_citations": "48\n", "authors": ["366"]}
{"title": "Arcadia, a software development environment research project\n", "abstract": " The research objectives of the Arcadia project are two-fold: discovery and development of environment architecture principles and creation of novel software development tools, particularly powerful analysis tools, which will function within an environment built upon these architectural principles.Work in the architecture area is concerned with providing the framework to support integration while also supporting the often conflicting goal of extensibility. Thus, this area of research is directed toward achieving external integration by providing a consistent, uniform user interface, while still admitting customization and addition of new tools and interface functions. In an effort to also attain internal integration, research is aimed at developing mechanisms for structuring and managing the tools and data objects that populate a software development environment, while facilitating the insertion of new kinds of tools and new classes of objects.The unifying theme of work in the tools area is support for effective analysis at every stage of a software development project. Research is directed toward tools suitable for analyzing pre-implementation descriptions of software, software itself, and towards the production of testing and debugging tools. In many cases, these tools are specifically tailored for applicability to concurrent, distributed, or real-time software systems.The initial focus of Arcadia research is on creating a prototype environment, embodying the architectural principles, which supports Ada1 software development. This prototype environment is itself being developed in Ada.Arcadia is being developed by a consortium of researchers from the University of\u00a0\u2026", "num_citations": "42\n", "authors": ["366"]}
{"title": "Ensuring reliable datasets for environmental models and forecasts\n", "abstract": " At the dawn of the 21st century, environmental scientists are collecting more data more rapidly than at any time in the past. Nowhere is this change more evident than in the advent of sensor networks able to collect and process (in real time) simultaneous measurements over broad areas and at high sampling rates. At the same time there has been great progress in the development of standards, methods, and tools for data analysis and synthesis, including a new standard for descriptive metadata for ecological datasets (Ecological Metadata Language) and new workflow tools that help scientists to assemble datasets and to diagram, record, and execute analyses. However these developments (important as they are) are not yet sufficient to guarantee the reliability of datasets created by a scientific process \u2014 the complex activity that scientists carry out in order to create a dataset. We define a dataset to be reliable\u00a0\u2026", "num_citations": "41\n", "authors": ["366"]}
{"title": "Experience in using a process language to define scientific workflow and generate dataset provenance\n", "abstract": " This paper describes our experiences in exploring the applicability of software engineering approaches to scientific data management problems. Specifically, this paper describes how process definition languages can be used to expedite production of scientific datasets as well as to generate documentation of their provenance. Our approach uses a process definition language that incorporates powerful semantics to encode scientific processes in the form of a Process Definition Graph (PDG). The paper describes how execution of the PDG-defined process can generate Dataset Derivation Graphs (DDGs), metadata that document how the scientific process developed each of its product datasets. The paper uses an example to show that scientific processes may be complex and to illustrate why some of the more powerful semantic features of the process definition language are useful in supporting clarity and\u00a0\u2026", "num_citations": "39\n", "authors": ["366"]}
{"title": "The right algorithm at the right time: Comparing data flow analysis algorithms for finite state verification\n", "abstract": " Finite-state verification is emerging as an important technology for proving properties about software. In our experience, we have found that analysts have different expectations at different times. When an analyst is in an exploratory mode, initially formulating and verifying properties, analyses usually find inconsistencies because of flaws in the properties or in the software artifacts being analyzed. Once an inconsistency is found, the analyst begins to operate in a fault-finding mode, during which meaningful counter-example traces are needed to help determine the cause of the inconsistency. Eventually, systems become relatively stable, but still require re-verification as evolution occurs. During such periods, the analyst is operating in a maintenance mode and would expect re-verification to usually report consistent results. Although it could be that one algorithm suits all three of these modes of use, the hypothesis\u00a0\u2026", "num_citations": "39\n", "authors": ["366"]}
{"title": "Modeling resources for activity coordination and scheduling\n", "abstract": " Precise specification of resources is important in activity and agent coordination. As the scarcity or abundance of resources can make a considerable difference in how to best coordinate the tasks and actions. That being the case, we propose the use of a resource model. We observe that past work on resource modeling does not meet our needs, as the models tend to be either too informal (as in management resource modeling) to support definitive analysis, or too narrow in scope (as in the case of operating system resource modeling) to support specification of the diverse tasks we have in mind.               In this paper we introduce a general approach and some key concepts in a resource modeling and management system that we have developed. We also describe two experiences we have had in applying our resource system. In one case we have added resource specifications to a process program. In\u00a0\u2026", "num_citations": "39\n", "authors": ["366"]}
{"title": "Verification of communication protocols using data flow analysis\n", "abstract": " In this paper we demonstrate the effectiveness of data flow analysis for verifying requirements of communication protocols. Data flow analysis is a static analysis method for increasing confidence in the correctness of software systems by automatically verifying that a given software artifact (eg, design or code) must behave consistently with a specified requirement. In this case study, we apply the FLAVERS data flow analysis tool to pseudocode designs of the three way handshake connection establishment protocol and of the alternating bit protocol and prove that the behavior of the pseudocode is consistent with protocol behavioral requirement specifications. We show how FLAVERS is a particularly effective because it is computationally inexpensive, requires minimal human interaction, and is a general approach that can be applied incrementally until the desired accuracy is achieved. In addition, we show how\u00a0\u2026", "num_citations": "39\n", "authors": ["366"]}
{"title": "Omega\u2014a data flow analysis tool for the C programming language\n", "abstract": " This paper describes Omega, a prototype system designed to analyze data flow in C programs. Omega is capable of detecting certain types of common programming errors, or assuring their absence. Omega also addresses the problems of analyzing pointer variables.", "num_citations": "38\n", "authors": ["366"]}
{"title": "Resource management for complex, dynamic environments\n", "abstract": " This paper describes an approach to the specification and management of the agents and resources that are required to support the execution of complex systems and processes. The paper suggests that a resource should be viewed as a provider of a set of capabilities that are needed by a system or process, where that set may vary dynamically over time and with circumstances. This view of resources is defined and then made the basis for the framework of an approach to specifying, managing, and allocating resources in the presence of real-world complexity and dynamism. The ROMEO prototype resource management system is presented as an example of how this framework can be instantiated. Some case studies of the use of ROMEO to support system execution are presented and used to evaluate the framework, the ROMEO prototype, and our view of the nature of resources.", "num_citations": "37\n", "authors": ["366"]}
{"title": "Automated support for the enactment of rigorously described software processes\n", "abstract": " There are many advantages to developing rigorously described software processes. Certainly, they provide the basis for improved project visibility, communication, and coordination. If they are sufficiently rigorous they also provide the basis for effective analysis and error detection which can be used to improve processes. Of the many advantages, however, none strikes me as being more important than the opportunity which rigorous process specifications present for directing the coordination of human and computer resources in support of the effective enactment of software processes. A number of researchers, both at this Workshop and elsewhere, have recognized this opportunity and have begun to study ways of taking advantage of it. Most of this work has focussed on the development of software environments in which explicit software process representations are used to coordinate the application of software\u00a0\u2026", "num_citations": "37\n", "authors": ["366"]}
{"title": "Cesar: A static sequencing constraint analyzer\n", "abstract": " This paper relates experience with building and using a programmable sequencing analyzer based on data flow analysis algorithms. An earlier paper described both the motivation for and the specification of Cecil, a powerful language for defining constraints on the sequencing of events and gave an algorithm for mapping the sequencing specifications defined by Cecil to data flow analysis algorithms. In this paper, we sketch the architecture of Cesar, a system for carrying out the analysis of Cecil sequencing constraints, describe the problems arising in the analysis of real-world programs, and indicate how we resolved these problems. Finally, we describe our experience in using Cesar, citing speed and efficiency characteristics of the current implementation, and suggesting the error-detection features and powers of Cesar.", "num_citations": "33\n", "authors": ["366"]}
{"title": "Definition and analysis of election processes\n", "abstract": " This paper shows that process definition and analysis technologies can be used to reason about the vulnerability of election processes with respect to incorrect or fraudulent behaviors by election officials. The Little-JIL language is used to model example election processes, and various election worker fraudulent behaviors. The FLAVERS finite-state verification system is then used to determine whether different combinations of election worker behaviors cause the process to produce incorrect election results or whether protective actions can be used to thwart these threats.", "num_citations": "32\n", "authors": ["366"]}
{"title": "The future of software processes\n", "abstract": " In response to increasing demands being put onto software-intensive systems, software processes will evolve significantly over the next two decades. This paper identifies seven relatively surprise-free trends \u2013 increased emphasis on users and end value; increasing software criticality and need for dependability; increasingly rapid change; increasingly complex systems of systems; increasing needs for COTS, reuse, and legacy software integration; and computational plenty \u2013 and two \u201cwild card\u201d trends: increasing software autonomy and combinations of biology and computing; and discusses their likely influences on software processes between now and 2025. It also discusses limitations to software process improvement, and areas of significant software process research and education needs.", "num_citations": "30\n", "authors": ["366"]}
{"title": "Verification of concurrent software with FLAVERS\n", "abstract": " In this demonstration we give a scenario of how FLAVERS, an implementation of the incremental accuracy improving data flow analysis approach [I], is used to verify event sequence properties of concurrent or distributed software programs.", "num_citations": "30\n", "authors": ["366"]}
{"title": "'Using Data Flow Tools in Software Engineering'.\n", "abstract": " In this paper we propose a generic configuration of tool capabilities. We categorize many of the available tools into a few broad classes, and show how these classes have properties which are nicely complementary. We hypothesize that testing, documentation and verification are three of the most important software production activities and suggest that these activities can be nicely supported by different configurations of representatives of these few tool classes.Descriptors:", "num_citations": "30\n", "authors": ["366"]}
{"title": "Specifying and verifying requirements for election processes\n", "abstract": " In this paper we outline an approach for modeling election processes and then performing rigorous analysis to verify that these process models meet selected behavioral requirements. We briefly outline some high-level requirements that an election process must satisfy and demonstrate how these are refined into a collection of lower-level properties that can be used as the basis for verification. We present a motivating example of an election process modeled using the Little-JIL process definition language, capture the lower-level properties using the PROPEL property elicitation tool, and perform formal analysis to verify that the process model adheres to these properties using the FLAVERS finite-state verifier. We illustrate how this approach can identify errors in the process model when a property is violated.", "num_citations": "29\n", "authors": ["366"]}
{"title": "Logically central, physically distributed control in a process runtime environment\n", "abstract": " An effective process definition language must be powerful, yet clear. It must also have well defined semantics to support powerful and definitive analysis. End users require that a runtime interpreter for the language faithfully implement the semantics used in analysis of process definitions, and that the interpreter be efficient and scalable. In addition to all of the above, the language, and its interpreter must also be readily evolvable. In this paper, we describe the architecture of Juliette, a process execution environment designed to address all of these requirements. We outline the tensions posed by these strong objectives and describe Juliette\u2019s modular approach and its novel distribution strategy, indicating how they address the tensions set by our objectives. While we explain the Juliette architectural approach in the context of the interpretation of Little-JIL, the approach applies to the interpretation of a broad class of process definition languages.", "num_citations": "29\n", "authors": ["366"]}
{"title": "Q: A multi-lingual interprocess communications system for software environment implementation\n", "abstract": " Q is a set of matched C and Ada interfaces designed to support interprocess communication between these two languages. It is a first step toward a more general notion of a multi-lingual interprocess communication model. The Q interfaces are adapted from the remote procedure call RPC interface model. The need for modification was imposed by the unavailability of certain C language features in the Ada language. Q attempts to define an interprocess communication model common to both languages, and a type space common to both languages.Descriptors:", "num_citations": "29\n", "authors": ["366"]}
{"title": "Continuous self-evaluation for the self-improvement of software\n", "abstract": " Software systems are increasingly essential to the operation of all aspects of our society. Clearly the operation of these systems has profound effects on society. Less recognized, but no less important, is the effect that societal activity has on software, placing continual pressure upon software systems to improve. In that software systems function as societal change agents, it follows naturally that the changes wrought by software systems rebound back as pressures for these systems themselves to improve in order to meet more closely the changing requirements of society. These pressures are felt as growing gaps between societal requirements and operational profiles. These gaps serve both as measures of required improvement and as vectors that should be used to direct improvement efforts and measure their success.", "num_citations": "28\n", "authors": ["366"]}
{"title": "Modeling and analyzing faults to improve election process robustness\n", "abstract": " This paper presents an approach for continuous process improvement and illustrates its application to improving the robustness of election processes. In this approach, the Little-JIL process definition language is used to create a precise and detailed model of an election process. Given this process model and a potential undesirable event, or hazard, a fault tree is automatically derived. Fault tree analysis is then used to automatically identify combinations of failures that might allow the selected potential hazard to occur. Once these combinations have been identified, we iteratively improve the process model to increase the robustness of the election process against those combinations that seem the most likely to occur. We demonstrate this approach for the Yolo County election process. We focus our analysis on the ballot counting process and what happens when a discrepancy is found during the count. We identify two single points of failure (SPFs) in this process and propose process modifications that we then show remove these SPFs.", "num_citations": "27\n", "authors": ["366"]}
{"title": "Dynamic resource scheduling in disruption-prone software development environments\n", "abstract": " Good resource scheduling plays a pivotal role in successful software development projects. However, effective resource scheduling is complicated by such disruptions as requirements changes, urgent bug fixing, incorrect or unexpected process execution, and staff turnover. Such disruptions demand immediate attention, but can also impact the stability of other ongoing projects. Dynamic resource rescheduling can help suggest strategies for addressing such potentially disruptive events by suggesting how to balance the need for rapid response and the need for organizational stability. This paper proposes a multi-objective rescheduling method to address the need for software project resource management that is able to suggest strategies for addressing such disruptions. A genetic algorithm is used to support rescheduling computations. Examples used to evaluate this approach suggest that it can support\u00a0\u2026", "num_citations": "27\n", "authors": ["366"]}
{"title": "Some experience with dave: a fortran program analyzer\n", "abstract": " This paper describes DAVE, an automatic program testing aid which performs a static analysis of Fortran programs. DAVE analyzes the data flows both within and across subprogram boundaries of Fortran programs, and is able to detect occurrences of uninitialized and dead variables in such programs. The paper shows how this capability facilitates the detection of a wide variety of errors, many of which are often quite subtle. The central analytic mechanism in DAVE is a depth-first search procedure which enables DAVE to execute efficiently. Some experiences with DAVE are described and evaluated and some future work is projected.", "num_citations": "27\n", "authors": ["366"]}
{"title": "Process support to help novices design software faster and better\n", "abstract": " In earlier work we have argued that formal process definitions can be useful in improving our understanding and performance of software development processes. There has, however, been considerable sentiment that formalized processes cannot capture the creative process of software design. This paper describes our experimentation with the hypothesis that both design speed and design quality can be improved through the use of formalized process definitions. Our experimentation supports this hypothesis.", "num_citations": "26\n", "authors": ["366"]}
{"title": "Comparing design methodologies through process modeling\n", "abstract": " Design Methodology, Software Design, Programming, Power System Reliability, Computer Science, Power System Modeling, Performance Analysis, Large Scale Systems, Software Performance, Software Tools,", "num_citations": "26\n", "authors": ["366"]}
{"title": "A future for software engineering?\n", "abstract": " This paper suggests the need for a software engineering research community conversation about the future that the community would like to have. The paper observes that the research directions the community has taken in the past, dating at least back to the formative NATO Conferences in the late 1960's, have been driven largely by desire to meet the needs of practice. The paper suggests that the community should discuss whether it is now appropriate to balance this problem-solving-oriented research with a stronger complement of curiosity-driven research. This paper does not advocate what that balance should be. Neither does it advocate what curiosity driven research topics should be pursued (although illustrative examples are offered). It does does advocate the need for a community conversation about these questions.", "num_citations": "25\n", "authors": ["366"]}
{"title": "Formally defining coordination processes to support contract negotiation\n", "abstract": " The literature and practice of negotiation and auctions, especially in the burgeoning area of electronic commerce, demonstrate that there is a wide and rapidly growing variety of negotiation and auction processes both in use and proposed. We believe that in the future automated online auctions will become a fundamental building block for contract negotiation carried out electronically. To avoid loss of money or bad decision making, it is important for organizations to have high confidence in the software involved in these activities. We are writing a range of negotiation processes in Little-JIL, an agent coordination language that addresses goals of expressiveness, analyzability, and executability. With Little-JIL, we can express processes involving the coordination of the multiple participants involved in contract negotiation and do so in a syntax that allows an intuitive understanding to non-programmers. Furthermore\u00a0\u2026", "num_citations": "25\n", "authors": ["366"]}
{"title": "A proposed testing and analysis research initiative\n", "abstract": " The steps being taken to develop a US software research agenda and to bridge the gap between testing and analysis researchers and practitioners are discussed. The current software reliability crisis and the obstacles to developing more effective testing and analysis technology and transferring it to industry are reviewed. Nine research areas most likely to yield important results are outlined. A major initiative in testing and analysis to provide the needed resources, an organizing framework, and an impetus to broaden and intensify critical testing and analysis research is discussed.< >", "num_citations": "25\n", "authors": ["366"]}
{"title": "Experience with enactable software process models\n", "abstract": " Models of software life-cycle processes provide a means of reasoning about organizational processes used to develop and maintain software. Clearly, these processes play a substantial role in determining the quality, responsiveness, cost, and schedule of a software system. As a result, improvements to software engineering processes should lead to significant improvement in these characteristics. I see software process modeling as an emerging technique, that has enormous potential for contributing to the improvement of software processes and their corresponding products. This paper'discusses experience in developing and using enactable software process models at the Software Engineering Institute (SEI). Some thoughts on future directions for these efforts are also presented.", "num_citations": "25\n", "authors": ["366"]}
{"title": "A process-object centered view of software environment architecture\n", "abstract": " The essential purpose of a software environment is to provide strong, complete and readily accessible support for such key software processes as development and maintenance. The basis of such support must be a diverse and powerful set of functional capabilities supplied by what has previously been referred to as\" software tools\". Increasingly, however, it is becoming clear that the most challenging part of creating an effective software environment is not the creation of the software tools themselves, but rather the effective integration of those tools and presentation of their capabilities to the user.Further, it is becoming quite clear that a software environment is very much like all other complex pieces of software in that it must be capable of growing, adapting and changing to meet the ever-changing needs of its users. Indeed, as software environments are a relatively new type of software product, it must be expected\u00a0\u2026", "num_citations": "25\n", "authors": ["366"]}
{"title": "Understanding process and the quest for deeper questions in software engineering research\n", "abstract": " This paper provides a brief summary of some overall currents and directions in my research with a particular emphasis on my work in the area of process. Using this perspective as a basis, the paper suggests the importance of using the challenge of grappling with hard technical problems as a basis for searching for deeper questions. The search for deep and enduring problems at the core of our discipline is rewarding for individual researchers and could provide substance, direction, purpose, and credibility for the community.", "num_citations": "24\n", "authors": ["366"]}
{"title": "Specification and static evaluation of sequencing constraints in software\n", "abstract": " This paper presents a flexible and general mechanism for specifying problems relating to the sequencing of events and mechanically translating them into dataflow analysis algorithms capable of solving those problems. Dataflow analysis has been used for quite some time in compiler code optimization. It has recently gained increasing attention as a way of statically checking for the presence or absence of errors and as a way of guiding the test case selection process. Most static analyzers, however, have been custom-built to search for fixed, and often quite limited, classes of dataflow conditions. It is shown that the range of sequences for which it is interesting and worthwhile to search is actually quite broad and diverse. A formalism is created for specifying this diversity of conditions. These conditions can be modeled essentially as dataflow analysis problems for which effective solutions are known and further, these solutions can be exploited to serve as the basis for mechanical creation of analyzers for these conditions. 18 refs.", "num_citations": "24\n", "authors": ["366"]}
{"title": "Desiderata for Languages to be Used in the Defnition of Reference Business Processes.\n", "abstract": " In many modern enterprises, explicit business process definitions facilitate the pursuit of business goals in such ways as best practice reuse, process analysis, process efficiency improvement, and automation. Most real-world business processes are large and complex. Successfully capturing, analysing, and automating these processes requires process definition languages that capture a variety of process aspects with a wealth of details. Most current process modelling languages, such as Business Process Modelling Notation (BPMN), focus on structural control flows among activities while providing inadequate support for other process definition needs. In this paper, we first illustrate these inadequacies through our experiences with a collection of real-world reference business processes from the Australian lending industry. We observe that the most significant inadequacies include lack of resource management, exception handling, process variation, and data flow integration. These identified shortcomings led us to consider the Little-JIL language as a vehicle for defining business processes. Little-JIL addresses the afore-mentioned inadequacies with a number of innovative features. Our investigation concludes that these innovative features are effective in addressing a number of key reference business process definition needs.", "num_citations": "23\n", "authors": ["366"]}
{"title": "An architecture for flexible, evolvable process-driven user-guidance environments\n", "abstract": " Complex toolsets can be difficult to use. User interfaces can help by guiding users through the alternative choices that might be possible at any given time, but this tends to lock users into the fixed interaction models dictated by the user-interface designers. Alternatively, we propose an approach where the tool utilization model is specified by a process, written in a process definition langauge. Our approach incorporates a user-interface specification that describes how the user-interface is to respond to, or reflect, progress through the execution of the process definition. By not tightly binding the user-guidance process, the associated user-interfaces, and the toolset, it is easy to develop alternative processes that provide widely varying levels and styles of guidance and to be responsive to evolution in the processes, user interfaces, or toolset. In this paper, we describe this approach for developing process-driven user\u00a0\u2026", "num_citations": "23\n", "authors": ["366"]}
{"title": "Dynamic scheduling of emergency department resources\n", "abstract": " The processes carried out in a hospital emergency department can be thought of as structures of activities that require resources in order to execute. Costs are reduced when resource levels are kept low, but this can lead to competition for resources and poor system performance. Careful allocation can improve performance by enabling more efficient use of resources. This paper proposes that resource scheduling be done in a series of dynamic reschedulings that use precise, detailed information about emergency department processes and available department resources to improve the quality of scheduling results. Rescheduling is done over a small set of activities, and uses a genetic algorithm. Simulations are used to evaluate this approach, and results indicate that it can be effective.", "num_citations": "22\n", "authors": ["366"]}
{"title": "Analyzing processes for e-government application development: The emergence of process definition languages\n", "abstract": " E-government developers need to clearly understand processes they are automating and ensure that automated processes are defect-free. This paper introduces readers to Process Definition Language (PDL) technology that provides rigor and precision over traditional forms of process documentation. We report our experience documenting license renewal processes for application in the Mass.gov portal. The PDL helped analysts identify inconsistencies and errors in natural language-based documents that were guiding system development. This case provides an initial demonstration of the benefits PDLs can bring to e-government application development. We conclude with a discussion of the current limitations of PDLs and a discussion of computer-based analysis approaches that will likely emerge in the future.", "num_citations": "22\n", "authors": ["366"]}
{"title": "Modeling and managing resource utilization in process, workflow, and activity coordination\n", "abstract": " Specifications of workflow, process, and activity coordination systems focus on the assignment of work to human and software agents and the dataflow required to support those activities. The runtime behaviors of these systems vary widely depending upon the availability of agents and other needed resources. Thus the precise specification of resources needed by, and available to, a system is an important basis for reasoning about and optimizing system behavior. Previous resource models used in management and workflow have lacked the rigor to support powerful reasoning and optimization. Some resource models for operating systems have been quite rigorous, but overly narrow in scope. This paper presents a meta-model for creating precise models of such resource types as humans, tools, computation platforms, and data, and the various associations between these types that are needed to support intelligent allocation of these resources. We also present examples of the use of this meta-model. This paper also describes a prototype resource allocation and management system that implements these approaches. This prototype is designed to be a separable, orthogonal component of a system for execution of processes defined as hierarchies of steps, each of which incorporates a specification of resource requirements.", "num_citations": "22\n", "authors": ["366"]}
{"title": "Clear and precise specification of ecological data management processes and dataset provenance\n", "abstract": " With the availability of powerful computational and communication systems, scientists now readily access large, complicated derived datasets and build on those results to produce, through further processing, yet other derived datasets of interest. The scientific processes used to create such datasets must be clearly documented so that scientists can evaluate their soundness, reproduce the results, and build upon them in responsible and appropriate ways. Here, we present the concept of an  analytic web , which defines the scientific processes employed and details the exact application of those processes in creating derived datasets. The work described here is similar to work often referred to as \u00bfscientific workflow,\u00bf but emphasizes the need for a semantically rich, rigorously defined process definition language. We illustrate the information that comprises an analytic web for a scientific process that measures and\u00a0\u2026", "num_citations": "20\n", "authors": ["366"]}
{"title": "A Prototype Facilitators Dashboard: Assessing and visualizing dialogue quality in online deliberation for education and work\n", "abstract": " The emerging next generation (\" Web 3.0\") of socio-technological tool development is adding additional support for reflecting on and improving the quality of online information, communication, and action coordination. An important opportunity is that online systems can include tools that directly support participants in having higher quality and more skillful engagements. We are evaluating dialogue software features that support participants directly and\" dashboard\" tools that support third parties (mediators, teachers, facilitators, moderators, etc.) in supporting higher quality deliberation. In this paper we will focus on our work in educational settings (college classes) and on our development of a Facilitators Dashboard that visualizes dialogue quality indicators for use as facilitation tools or participant social awareness tools. We are particularly interested in supporting the\" social deliberative skills\" that interlocutors need to build mutual understanding and mutual regard in complex or contentious situations.", "num_citations": "19\n", "authors": ["366"]}
{"title": "Challenges observed in the definition of reference business processes\n", "abstract": " In many modern enterprises, explicit business process definitions facilitate the pursuit of business goals in such ways as best practice reuse, process analysis, process efficiency improvement, and automation. Most real-world business processes are large and complex. Successfully capturing, analysing, and automating these processes requires process definition languages that capture a variety of process aspects with a wealth of details. Most current process modeling languages, such as Business Process Modeling Notation (BPMN), focus on structural control flows among activities while providing inadequate support for other process definition needs. In this paper, we first illustrate these inadequacies through our experiences with a collection of real-world reference business processes from the Australian lending industry. We observe that the most significant inadequacies include lack of resource\u00a0\u2026", "num_citations": "19\n", "authors": ["366"]}
{"title": "Engineering software design processes to guide process execution\n", "abstract": " Using systematic development processes is an important characteristic of any mature engineering discipline. In current software practice, software design methodologies (SDMs) are intended to be used to help design software more systematically. This paper shows, however, that one well-known example of such an SDM, Booch Object-Oriented Design (BOOD), as described in the literature is too imprecise and incomplete to be considered as a fully systematic process for specific projects. To provide more effective and appropriate guidance and control in software design processes, we applied the process programming concept to the design process. Given two different sets of plausible design process requirements, we elaborated two more detailed and precise design processes that are responsive to these requirements. We have also implemented, experimented with, and evaluated a prototype (called Debus\u00a0\u2026", "num_citations": "19\n", "authors": ["366"]}
{"title": "Integrating testing techniques through process programming\n", "abstract": " Integration of multiple testing techniques is required to demonstrate high quality of software. Technique integration has four basic goals: reduced development costs, incremental testing capabilities, extensive error detection, and cost-effective application. We are experimenting with the use of process programming as a mechanism for integrating testing techniques. Having set out to develop a process that provides adequate coverage and comprehensive fault detection, we proposed synergistic use of DATA FLOW testing and RELAY to achieve all four goals. We developed a testing process program much as we would develop a software product from requirements through design to implementation and evaluation. We found process programming to be effective for explicitly integrating the techniques and achieving the desired synergism. Used in this way, process programming also mitigates many of the other\u00a0\u2026", "num_citations": "19\n", "authors": ["366"]}
{"title": "New perspectives on software maintenance processes\n", "abstract": " The policies of the maintenance organization, techniques used, types of changes attempted, times when they are attempted, and nature of the product all shape the maintenance process. The authors suggest that no single fixed maintenance process can meet all these needs, but that all views should be consolidated in a single framework. This is done by using the notions of process programming and process environments to provide a structure within which users can alter tools and processes to support all maintenance needs and approaches in a user-tailorable, dynamically adaptable, and incrementally implementable way. Consideration of the demands and ramifications of process programming leads to the discovery of the importance of process maintenance, dynamic maintenance, process execution history maintenance, and product-related process maintenance. These notions, which broaden and complicate\u00a0\u2026", "num_citations": "18\n", "authors": ["366"]}
{"title": "THE DETECTION OF UNEXECUTABLE PROGRAM PATHS THROUGH STATIC DATA FLOW ANALYSIS 1\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "18\n", "authors": ["366"]}
{"title": "Process technology to facilitate the conduct of science\n", "abstract": " This paper introduces the concept of an analytic web, a synthesis of three complementary views of a scientific process that is intended to facilitate the conduct of science. These three views support the clear, complete, and precise process documentation needed to enable the effective coordination of the activities of geographically dispersed scientists. An analytic web also supports automation of various scientific activities, education of young scientists, and reproducibility of scientific results. Of particular significance, an analytic web is intended to forestall the generation of scientific data that are erroneous or suspect, by using process definitions to prevent incorrect combinations of scientific results. The paper also describes experiences with a tool, SciWalker, designed to evaluate the efficacy of this approach.", "num_citations": "17\n", "authors": ["366"]}
{"title": "Keystone: a federated software environment\n", "abstract": " Individual workstations, based on powerful personal computers, suggest interesting possibilities for in portant advances in software development en vironments. These workstations are capable of supporting most, if not all, of the functional activities of an individual developer including editing, compilation, analysis and testing of small modules or programs, document preparation and personal data management. Workstations also offer significant user inter-face advantages over more traditional technology including high-resolution graphics displays and non-keyboard input (such as a mouse or graphics tablet). The use of personal workstations to support large software development projects, however, requires interconnection via a local area network to make it possible for the individuals working on large projects to communicate, cooperate and share resources. This paper describes a project to construct and evaluate\u00a0\u2026", "num_citations": "17\n", "authors": ["366"]}
{"title": "The detection of anomalous interprocedural data flow\n", "abstract": " In an earlier paper, the authors have defined type 1 and type 2 data flow anomalies to be, respectively, the reference to an undefined variable and the definition of a variable without subsequent reference. It is not difficult to devise search techniques to detect such anomalies when the anomalous data flow is contained in a single procedure. When the data flow crosses procedure boundaries, however, many difficulties may arise. In this paper, we carefully define the conditions under which interprocedural anomalies occur. We also show how algorithms currently used in global program optimization can easily be adapted to yield highly efficient algorithms for the detection of such interprocedural anomalies.", "num_citations": "17\n", "authors": ["366"]}
{"title": "Provenance and quality control in sensor networks\n", "abstract": " Scientists and society increasingly rely on streaming data from electronic sensors to assess, model, and forecast environmental changes. Because analyses of time-series data require uninterrupted data streams or datasets, scientists regularly fill gaps in the data by substituting modeled values. As modeling increases in complexity, the provenance metadata needed to describe and define processes used to model data and create derived datasets quickly exceeds the capacity of individual flags or groups of flags to annotate individual data values. In theory, necessary provenance metadata could be captured in narrative form, but the time and effort required to do so are prohibitive. A system that can capture provenance metadata automatically and allow scientists to query them for useful details is what scientists really need. In this paper we describe a system that uses Little-JIL, a process programming language, to rigorously define modeling and data-derivation processes, and a mathematical graph structure\u2013a Data Derivation Graph (DDG)\u2013that precisely describes execution histories. Our system and approach support understanding the (potentially) different processes used to create data values, reasoning about the soundness of these processes, and helping to ensure that the data processing in sensor networks is reliable and reproducible.", "num_citations": "16\n", "authors": ["366"]}
{"title": "What is software?\n", "abstract": " This paper suggests that there may be other types of software besides computer software. The paper identifies parallels between computer software and such other societal artifacts as laws, processes, recipes, and instructions (e.g. for driving and kitbuilding), and suggests that there are similar parallels in the ways in which these artifacts are built and evolved. The paper suggests that technologies for supporting the automation of computer software development and evolution might facilitate work in these other domains. It also suggests that successful approaches in these other domains could have useful and important application to the domain of computer software. It is further suggested that it might be important for discussions such as these to lead to a deeper understanding of the nature of \u201csoftware\u201d.", "num_citations": "16\n", "authors": ["366"]}
{"title": "Unifying the software process spectrum\n", "abstract": " Software Process Workshop (SPW 2005) was held in Beijing on May 25-27, 2005. This paper introduces the motivation of organizing such a workshop, as well as its theme and paper gathering and review; and summarizes the main content and insights of 11 keynote speeches, 30 regular papers in five sessions of \u201cProcess Content\u201d,\u201cProcess Tools and Metrics\u201d,\u201cProcess Management\u201d,\u201cProcess Representation and Analysis\u201d, and \u201cExperience Reports\u201d, 8 software development support tools demonstration, and the ending panel \u201cWhere Are We Now? Where Should We Go Next?\u201d.", "num_citations": "16\n", "authors": ["366"]}
{"title": "An adaptable generation approach to agenda management\n", "abstract": " As software engineering efforts move to more complex, distributed environments, coordinating the activities of people and tools becomes very important. While groupware systems address user level communication needs and distributed computing technologies address tool level communication needs, few attempts have been made to synthesize the common needs of both. This paper describes our attempt to do exactly that. We describe a framework for generating an agenda management system (AMS) from a specification of the system's requirements. The framework can meet a variety of requirements and produces a customized AMS appropriate for use by both humans and software tools. The framework and generated system support evolution in several ways, allowing existing systems to be extended as requirements change. We also describe our experiences using this approach to create an AMS to support a\u00a0\u2026", "num_citations": "16\n", "authors": ["366"]}
{"title": "Using process definitions to support reasoning about satisfaction of process requirements\n", "abstract": " This paper demonstrates how a precise definition of a software development process can be used to determine whether the process definition satisfies certain of its requirements. The paper presents a definition of a Scrum process written in the Little-JIL process definition language. The definition\u2019s details facilitate understanding of this specific Scrum process (while also suggesting the possibility of many variants of the process). The paper also shows how these process details can support the use of analyzers to draw inferences that can then be compared to requirements specifications. Specifically the paper shows how finite state verification can be used to demonstrate that the process protects the team from requirements changes during a sprint, and how analysis of a fault tree derived from the Little-JIL Scrum definition can demonstrate the presence of a single point of failure in the process, suggesting that\u00a0\u2026", "num_citations": "15\n", "authors": ["366"]}
{"title": "An anytime approach to analyzing software systems\n", "abstract": " Proving that a software system satisfies its requirements is a costly process. This paper discusses the benefits and challenges of structuring the analysis of software as an anytime algorithm. We demonstrate that certain incremental approaches to event sequence analysis that produce partial results are anytime algorithms, and we show how these partial results can be used to optimize the time to complete the full analysis.", "num_citations": "15\n", "authors": ["366"]}
{"title": "Search based risk mitigation planning in project portfolio management\n", "abstract": " Software projects are always facing various risks. These risks should be identified, analyzed, prioritized, mitigated, monitored and controlled. After risks are identified and analyzed, resources must then be devoted to mitigation. However, risk prioritization and mitigation planning are complicated problems. Especially in project portfolio management (PPM), resource contention among projects leads to difficulty in choosing and executing mitigation actions. This paper introduces a search based risk mitigation planning method that is useful in PPM. It integrates the analysis of risks, consideration of available resources, and evaluation of possible effects when taking risk mitigation actions. The method uses a genetic algorithm to search for the risk mitigation plan of optimal value. A case study shows how this method can identify effective risk mitigation plans, thus providing useful decision support for managers.", "num_citations": "14\n", "authors": ["366"]}
{"title": "An approach to modeling and supporting the rework process in refactoring\n", "abstract": " This paper presents the definition of a process for performing rework, and a tool that executes the process in order to support humans seeking help in being sure that they are carrying out rework completely and correctly. The process definition treats rework as the reinstantiation of previously-performed activities in new contexts, which requires the careful specification and management of the values of the artifacts that comprise key process execution history and contextual information. The rework tool exploits access to this information to provide human reworkers with guidance about the rework tasks to be done and with context and history information expected to be useful in guiding superior rework decisions. The paper presents a detailed example of the use of the process and tool in supporting a particular kind of rework, namely the refactoring of the design of an Object-Oriented program.", "num_citations": "14\n", "authors": ["366"]}
{"title": "Engineering software design processes to guide process execution\n", "abstract": " Using systematic development processes is an important characteristic of any mature engineering discipline. In current software practice, software design methodologies (SDMs) are intended to be used to help design software more systematically. This paper explicitly shows, however, that one well-known example of such an SDM, Booch object-oriented design (BOOD), as described in the literature, is far too vague to provide specific guidance to designers, and is too imprecise and incomplete to be considered as a fully systematic process for specific projects. To provide more effective and appropriate guidance and control in software design processes, we applied the process programming concept to the design process. Given two different sets of plausible process requirements, we elaborated two more detailed and precise design processes that are responsive to these requirements. We have also implemented\u00a0\u2026", "num_citations": "13\n", "authors": ["366"]}
{"title": "A metaphor and a conceptual architecture for software development environments\n", "abstract": " A conceptual architecture for software development environments (SDEs) is presented in terms of a new metaphor drawn from business enterprises. A metaphor is employed as the architecture is complex, requiring understanding from several perspectives. The metaphor provides a rich set of familiar concepts that strongly aid in understanding the environment architecture and software production. The metaphor is applicable to individual programming environments, software development environments supporting teams of developers, and to large-scale software production as a whole.             The paper begins by considering three perspectives on SDEs, a function-based view, an objects-and-relations view, and a process-centered view. The process view, being the most encompassing, is held through the remainder of the paper. Three metaphors for organizing and explaining a process-centered\u00a0\u2026", "num_citations": "13\n", "authors": ["366"]}
{"title": "A process-driven tool to support online dispute resolution\n", "abstract": " This demonstration shows a prototype tool that projects an impression of how execution of a formally defined process will facilitate dispute resolution. Tool flexibility supports projecting the look and feel of a range of different processes, facilitating user evaluation of alternatives.", "num_citations": "12\n", "authors": ["366"]}
{"title": "Improving the quality of software quality determination processes\n", "abstract": " Abstract This paper suggests a systematic, orderly, process-based approach to stating software qual- ity objectives and knowing if and when they have been achieved. We suggest that quality in software is a complex, multifaceted array of characteristics, and that it is important to establish specific objectives along various software quality dimensions as requirements for software quality assurance determination processes. We propose that process technology be used to design, code, execute, evaluate, and migrate processes that are demonstrably effective in achieving required software product quality objectives. Recently there have been numerous highly visible efforts to codify the assessment of software processes, and to use assessment results to improve them. In this paper we argue that these efforts function as testplans for software processes. We borrow some of the notions proposed in these efforts\u00a0\u2026", "num_citations": "12\n", "authors": ["366"]}
{"title": "An example of formal specification as an aid to design and development\n", "abstract": " ABSTRACI\u2019In this paper we describe an experiment which supports the hypothesis that formally specifying a system after a prototype has been created can enhance the development process. We conjectured that describing au existing system using a small number of well understood concepts could increase understanding and enhance communication between project members. We performed ao experiment to verify lhis conjecture using software constructed by the Arcadia project: a consortium developing software environment technologies including \u201cprocess programming\u201d[lo]. At the beginning of the experiment, a prototype requirements building syste, m had been developed and was undergoing redesign and extension. Over the course of three months, the developers met with a specification expefl and a formal description was written using PLEASE [ll], au Ada@-based executable specification and design\u00a0\u2026", "num_citations": "12\n", "authors": ["366"]}
{"title": "Software process interpretation and software environments\n", "abstract": " This paper suggests that software engineering must become the study of how to formalize, specify, and automate the complex processes by which software products are synthesized. The author believes that software systems are most profitably thought of as products of large and complex manufacturing and fabrication processes which can be described, automated, and controlled by familiar algorithmic programming techniques. Included in this paper are discussions of programming software engineering processes, the relations of the author's research to other computer science research areas, and the design of a software engineering language. 42 refs., 9 tabs.", "num_citations": "12\n", "authors": ["366"]}
{"title": "Discrete-event simulation and integer linear programming for constraint-aware resource scheduling\n", "abstract": " This paper presents a method for scheduling resources in complex systems that integrate humans with diverse hardware and software components, and for studying the impact of resource schedules on system characteristics. The method uses discrete-event simulation and integer linear programming, and relies on detailed models of the system's processes, specifications of the capabilities of the system's resources, and constraints on the operations of the system and its resources. As a case study, we examine processes involved in the operation of a hospital emergency department, studying the impact staffing policies have on such key quality measures as patient length of stay (LoS), number of handoffs, staff utilization levels, and cost. Our results suggest that physician and nurse utilization levels for clinical tasks of 70% result in a good balance between LoS and cost. Allowing shift lengths to vary and shifts to\u00a0\u2026", "num_citations": "11\n", "authors": ["366"]}
{"title": "A pattern for modeling rework in software development processes\n", "abstract": " It is usual for work completed at one point in a software development process to be revisited, or reworked, at a later point. Such rework is informally understood, but if we hope to support reasoning about, and partial automation of, software development processes, rework be more formally understood. In our experience in designing formalized processes in software development and other domains, we have noticed a recurring process pattern that can be used to model rework quite successfully. This paper presents that pattern, which models rework as procedure invocation in a context that is carefully constructed and managed. We present some scenarios drawn from software engineering in which rework occurs. The paper presents rigorously defined models of these scenarios, and demonstrates the applicability of the pattern in constructing these models.", "num_citations": "11\n", "authors": ["366"]}
{"title": "Formalisms to support the definition of processes\n", "abstract": " This paper emphasizes the importance of defining processes rigorously, completely, clearly, and in detail in order to support the complex projects that are essential to the modern world. The paper argues that such process definitions provide needed structure and context for the development of effective software systems. The centrality of process is argued by enumerating seven key ways in which processes and their definitions are expected to provide important benefits to society. The paper provides an example of a process formalism that makes good progress towards the difficult goal of being simultaneously rigorous, detailed, broad, and clear. Early experience suggests that these four key characteristics of this formalism do indeed seem to help it to support meeting the seven key benefits sought from process definitions. Additional research is suggested in order to gain more insights into needs in the area\u00a0\u2026", "num_citations": "11\n", "authors": ["366"]}
{"title": "Effective resource allocation for process simulation: A position paper\n", "abstract": " We often simulate processes to be able to reason about, forecast, and plan the best utilization of available resources. As process programmers, we define resources to be the agents that carry out tasks, and the tools and other entities required by agents in order for them to be able to complete their assigned work. Specifying these resources rigorously and allocating them efficiently during process simulation or execution is a non trivial problem. In this paper, we present many hard and interesting issues related to resource management and propose some solution approaches. In particular, we talk about an auction based solution approach, which we feel fits well in different types of process simulation.", "num_citations": "11\n", "authors": ["366"]}
{"title": "Artifact-based functional comparison of software processes\n", "abstract": " Sound methods of analysis and comparison of software processes are crucial for such tasks as process understanding, process correctness verification, evolution management, process classification, process improvement, and process selection.", "num_citations": "11\n", "authors": ["366"]}
{"title": "Efficient composite data flow analysis applied to concurrent programs\n", "abstract": " FLAVERS, a tool for verifying properties of concurrent systems, uses composite data flow analysis to incrementally improve the precision of the results of its verifications. Although FLAVERS is one of the few static analysis techniques for concurrent systems that has the potential to handle large scale systems, it sometimes can still be very expensive to use. In this paper we experimentally compare the cost of two versions of this approach for solving composite data flow analysis problems. The first version, product-based, uses the more straightforward approach, and the second, tuple-based, is built around the idea of reducing analysis space requirements at the expense of analysis time. We demonstrate experimentally, by analyzing properties of actual concurrent programs, that the tuple-based version is comparable in time to the product-based version but for large composite data flow problems it requires several\u00a0\u2026", "num_citations": "11\n", "authors": ["366"]}
{"title": "A software lifecycle methodology and tool support\n", "abstract": " This paper describes a system of techniques and tools for aiding in the development and maintenance of software. Improved verification techniques are applied throughout the entire process and management visibility is greatly enhanced. The paper discusses the critical need for improving upon past and present methodology. It presents a proposal for a new production methodology, a verification methodology, and the system architecture for a family of support tools.", "num_citations": "11\n", "authors": ["366"]}
{"title": "Some classes of uniquely 3-colorable graphs\n", "abstract": " In this paper we shall present the concept of a uniquely n-colorable graph, and then introduce a class of graphs which we shall call 6-clique rings. We shall show that 6-clique rings are useful in generating some classes of uniquely 3-colorable graphs. Moreover, we shall demonstrate how the techniques used in producing uniquely 3-colorable graphs from 6-clique rings can be extended to allow the production of other classes of uniquely 3-colorable graphs as well as the production of uniquely n-colorable graphs, where n>3.", "num_citations": "11\n", "authors": ["366"]}
{"title": "Resource scheduling through resource-aware simulation of emergency departments\n", "abstract": " This paper proposes using resource-aware, discrete-event simulation to measure the effects of resource scheduling in hospital emergency departments. Determining staffing and resource allocation is a complex constraint-optimization problem that has significant impact on hospital costs and patient care quality. We developed detailed models of the emergency department process of caring for patients, the resources available to support that process, and the scheduling constraints on the deployment of those resources. We then ran a battery of discrete-event simulations of this process, varying details of process, resource mixes, and scheduling constraints, to analyze the effects of resource availability (e.g., staffing patterns) on patient length of stay. Our simulation approach proved to be particularly adept at supporting the systematic investigation of two issues of particular interest to domain experts: (1) an excessive\u00a0\u2026", "num_citations": "10\n", "authors": ["366"]}
{"title": "A Process Programmer Looks at the Spiral Model: A Tribute to the Deep Insights of Barry W. Boehm.\n", "abstract": " This paper elaborates on implications of Barry W. Boehm\u2019s Spiral Model of software development. The paper notes that the Spiral Model presents a compelling view of software development, evocatively represented by a visual image that appeals strongly to intuition, and notes that the view and image have motivated and justified a range of important views of how software development should be done. This paper enhances and elaborates on the intuitions by supplementing them with a definition of the Spiral Model that is enunciated in terms of a rigorously defined language. The rigorous enunciation and accompanying alternative visual depiction are then used to provide clarification and formalization of some of the clearly-indicated elaborations of the Spiral Model. Both the Waterfall Model of software development and the Scrum agile method are presented as possible elaborations of the Spiral Model. Still other elaborations are indicated. Similarities in the visualizations of these development approaches suggest some underlying similarities in the approaches themselves, suggesting the potential value of effective process visualizations. The breadth of these elaborations is also used to suggest how the Spiral Model seems to provide a strong focus on some of the quintessential aspects of what comprises effective software development.", "num_citations": "10\n", "authors": ["366"]}
{"title": "Structural considerations in defining executable process models\n", "abstract": " This paper examines the question of how to structure the representation of a process in order to assure that the representation is effective in supporting such diverse activities as process understanding, communication among process participants, and process execution. The paper uses the example of a negotiation process to demonstrate that one process structure (which we refer to as the narrative form) seems to be quite effective in supporting understanding and communication, but then indicates that this structure seems problematic in supporting process execution. The paper indicates that a different structure (which we refer to as the role-oriented form) seems much more appropriate and effective in supporting execution, but may be lacking at supporting communication. In addition to serving different purposes, the two structures seem to represent different underlying models\u2013a static process model, and\u00a0\u2026", "num_citations": "10\n", "authors": ["366"]}
{"title": "Applying Little\u2010JIL to describe Process\u2010Agent knowledge and support project planning in SoftPM\n", "abstract": " SoftPM is a toolkit that supports a process\u2010based approach to software project management. It relies upon a software process modeling method based upon the idea of an Organization\u2010Entity to define standard processes and model project processes. The Process\u2010Agent is the core of this modeling method and is a well\u2010defined unit whose role is to encapsulate an Organization\u2010Entity's knowledge, skill etc. The Process\u2010Agent's infrastructure comprises descriptive knowledge, process knowledge and an experience library. The process knowledge is represented by process steps, whose execution determines the behaviors of the Process\u2010Agent. This causes Process\u2010Agent knowledge to be precisely described and well organized. In this paper, Little\u2010JIL, a well\u2010known process modeling language, is used to define a Process\u2010Agent's process knowledge. Benefits for process element knowledge representation arising\u00a0\u2026", "num_citations": "10\n", "authors": ["366"]}
{"title": "Using process definitions to facilitate the specifications of requirements\n", "abstract": " This paper describes early experiences in using process definitions to facilitate requirements specification. The paper emphasizes addressing the constraints posed by the particularly challenging activity of obtaining requirements for digital government systems. The use of precise definitions of processes is suggested as a vehicle for bringing diverse stakeholders together to agree on system requirements in a transparent environment facilitating trust. A user interface prototype is employed to help make the implications of the process definitions concrete. Iterative incremental application of these technologies is leading to new understandings of how to elicit system requirements in the challenging domain of digital government. Experience from the development of an Online Dispute Resolution system for use by the US National Mediation Board illustrates our approach.", "num_citations": "10\n", "authors": ["366"]}
{"title": "Integrating high-level and detailed agent coordination into a layered architecture\n", "abstract": " Coordination, which is the process that an agent reasons about its local actions and the (anticipated) actions of others to try to ensure the community acts in a coherent fashion, is an important issue in multi-agent systems. Coordination is a complicated process that typically consists of several operations: exchanging local information; detecting interactions; deciding whether or not to coordinate; proposing, analyzing, refining and forming commitments; sharing results, and so on. We argue that facets of these different operations can be separated and bundled into two different layers.The lowerlayer pertains to feasibility and implementation operations, i.e., the detailed analysis of candidate tasks and actions, the formation of detailed temporal/resource-specific commitments between agents, and the balancing of non-local and local problem solving activities. In contrast, the upper-layer pertains to domain specific\u00a0\u2026", "num_citations": "10\n", "authors": ["366"]}
{"title": "Research issues in the intersection of hypertext and software development environments\n", "abstract": " In this paper we describe issues of mutual interest and mutual benefit to researchers at the intersection of hypertext and software development environments. On one hand, the domain of software development appears to be a natural candidate for applying hypertext. Hypertext and hypermedia capabilities may be used in the creation, manipulation, examination and evolution of software products. On the other hand, software engineering principles and practices may be used to improve the quality and productivity of hypertext-system development. In particular, software development environments may facilitate the provision of hypertext capabilities for those environments, that would have otherwise required substantial development efforts.", "num_citations": "10\n", "authors": ["366"]}
{"title": "ASSET: A life cycle verification and visibility system\n", "abstract": " This paper describes the Automated Systems and Software Engineering Technology (ASSET) System, a system of techniques and tools aiding in the management and control of product development and maintenance. Improved verification techniques are applied throughout the entire life cycle and management visibility is greatly enhanced. The paper discusses the critical need for improving upon past and present management methodology, and describes the ASSET verification methodology, the ASSET system architecture, and the current ASSET development status.", "num_citations": "10\n", "authors": ["366"]}
{"title": "Using computer simulation to study nurse-to-patient ratios in an emergency department\n", "abstract": " OBJECTIVE:To study the impact of nurse-to-patient ratios on patient length of stay (LOS) in computer simulations of emergency department (ED) care.METHODS:Multiple 24-hour computer simulations of emergency care were used to evaluate the impact of different minimum nurse-to-patient ratios related to ED LOS, which is composed of wait (arrival to bed placement) and bedtime (bed placement to leave bed).RESULTS:Increasing the number of patients per nurse resulted in increased ED LOS. Mean bedtimes in minutes were impacted by nurse-to-patient ratios.CONCLUSIONS:In computer simulation of ED care, increasing the number of patients per nurse resulted in increasing delays in care (ie, increasing bedtime).", "num_citations": "9\n", "authors": ["366"]}
{"title": "Specifying flexible human behavior in interaction-intensive process environments\n", "abstract": " Fast changing business environments characterized by unpredictable variations call for flexible process-aware systems. The BPM community addressed this challenge through various approaches but little focus has been on how to specify (respectively constrain) flexible human involvement: how human process participants may collaborate on a task, how they may obtain a joint decision that drives the process, or how they may communicate out-of-band for clarifying task-vital information. Experience has shown that pure process languages are not necessarily the most appropriate technique for specifying such flexible behavior. Hence selecting appropriate modeling languages and strategies needs thorough investigation. To this end, this paper juxtaposes the capabilities of representative human-centric specification languages hADL and Little-JIL and demonstrate their joint applicability for modeling\u00a0\u2026", "num_citations": "9\n", "authors": ["366"]}
{"title": "Supporting process undo and redo in software engineering decision making\n", "abstract": " This paper presents a provenance-based approach for supporting undo and redo for software engineers. Writing software entails creating and reworking intricately intertwined software artifacts. After discovering a mistake in an earlier-completed task, a developer may wish to redo this task, but without undoing much of the work done since. Unfortunately, state-of-the-practice undo and redo mechanisms force the developer to manually redo the work completed since the mistake. This can cause considerable extra, often error-prone work.", "num_citations": "9\n", "authors": ["366"]}
{"title": "Disruption-driven resource rescheduling in software development processes\n", "abstract": " Real world systems can be thought of as structures of activities that require resources in order to execute. Careful allocation of resources can improve system performance by enabling more efficient use of resources. Resource allocation decisions can be facilitated when process flow and estimates of time and resource requirements are statically determinable. But this information is difficult to be sure of in disruption prone systems, where unexpected events can necessitate process changes and make it difficult or impossible to be sure of time and resource requirements. This paper approaches the problems posed by such disruptions by using a Time Window based INcremental resource Scheduling method (TWINS). We show how to use TWINS to respond to disruptions by doing reactive rescheduling over a relatively small set of activities. This approach uses a genetic algorithm. It is evaluated by using it to\u00a0\u2026", "num_citations": "9\n", "authors": ["366"]}
{"title": "Dynamic scheduling in systems with complex resource allocation requirements\n", "abstract": " Real world systems can be thought of as structures of activities that require resources in order to execute. Costs are reduced when resource levels are kept low, but this can lead to competition for resources that can cause poor system performance. Careful allocation of resources can improve system performance by enabling more efficient use of resources. This paper proposes that resource scheduling be done in a series of dynamic reschedulings that use precise and detailed information about the system and available resources to improve the quality of scheduling results. Each rescheduling is done over a relatively small set of activities, and a genetic algorithm is used to support scheduling computations. Simulations of healthcare systems are used to evaluate this approach. Results indicate that this approach can support effective dynamic rescheduling at affordable costs.", "num_citations": "9\n", "authors": ["366"]}
{"title": "What we learn from the study of ubiquitous processes\n", "abstract": " Software engineering has come to recognise the value of software processes as vehicles for addressing such goals as improved efficiency in software development, the achievement of improved product quality, and better coordination and communication of the members of software teams. Greater clarity, completeness, and precision in defining these processes seem to lead to greater effectiveness in pursuing these goals. The languages and notations used to support definition of such processes seem to be relatively successful in supporting these definitions, but more progress towards better languages and notations seems indicated. This article suggests that processes are also central to the effective pursuit of essential goals in a broad spectrum of other domains of human endeavour. We suggest that the study of how processes are used in those other domains to pursue goals that are essential in those domains\u00a0\u2026", "num_citations": "9\n", "authors": ["366"]}
{"title": "The criticality of modeling formalisms in software design method comparison\n", "abstract": " This paper describes experimentation aimed at making the comparison of software design methodologies (SDM's) more of an exact science. Our aim is to lay the foundations for this more exact science by establishing xed methods and conceptual frameworks that are able to assure that comparison e orts will yield predictable, reproducible results. Earlier papers have proposed the use of a systematic process to compare SDM's. This process assumes that the comparison will be done relative to a xed standard SDM feature classi cation schema, and with the use of a xed formalism for modeling the SDM's. Early experiments with this approach have yielded interesting SDM comparisons, but have raised questions about how sensitive these results might be to the choice of modeling formalism. In this paper we study this sensitivity by varying the choice of modeling formalism. We describe an experiment in which we xa pair of SDM's and then use two di erent formalisms to obtain two di erent comparisons of that pair of SDM's. We then compare the comparisons. Our results suggest that comparison results may be relatively insensitive to di erences in modeling formalisms. This paper also suggests an approach to further experimentation.", "num_citations": "9\n", "authors": ["366"]}
{"title": "Perspectives on refactoring planning and practice: an empirical study\n", "abstract": " Iterative development increasingly seeks to incorporate design modification and continuous refactoring in order to maintain code quality even in highly dynamic environments. However, there does not appear to be consensus on how to do this, especially because research results seem to be inconsistent. This paper presents an empirical study based upon an industry survey of refactoring practices and attitudes. The study explored differences in attitudes about refactoring among participants who played roles in software development, and how these different attitudes affected actual practice. The study found strong agreement among all roles about the importance of refactoring, and agreement about the negative effects upon agility of deferring refactoring. Nevertheless, the survey found that roles had different perspectives on the different kinds of tasks in an agile process. Accordingly, there was no universally\u00a0\u2026", "num_citations": "8\n", "authors": ["366"]}
{"title": "Process definition language support for rapid simulation prototyping\n", "abstract": " This paper suggests how an appropriately designed and architected process definition language can be an effective aid to the rapid generation of simulations, which are, in turn, capable of providing important insights. The paper describes how the features of the Little-JIL process definition language helped in the rapid generation of simulations that shed important new light on the effectiveness of various collusion strategies in influencing the outcomes of various auction approaches. The paper describes how Little-JIL\u2019s approach to modular reuse and its separation of process concerns both turn out to be of particular value in supporting rapid prototyping. The simulation results obtained are themselves interesting, as the paper also suggests that the auction idiom is highly relevant to resource allocation in software development. Thus, the insights gained into the efficacy of various collusion approaches have\u00a0\u2026", "num_citations": "8\n", "authors": ["366"]}
{"title": "Modeling processes to effectively reason about their properties\n", "abstract": " The work of our group has been focused, for nearly 15 years, upon modeling processes sufficiently precisely and completely to support the definitive evaluation of their properties. We have been interested in identifying the characteristics of process languages that are needed in order to support definitions of processes that are simultaneously 1) precise, 2) clear, 3) complete, and 4) executable. Our research has led us through a series of process definition languages that has recently culminated with Little-JIL [31, 4]. In this paper we summarize some of the features of Little-JIL. We indicate the wide range of processes that we have successfully defined using Little-JIL. Finally, we indicate why we have growing confidence that Little-JIL can be used successfully to support both dynamic and static analysis of processes. While we have not yet undertaken dynamic analysis of Little-JIL processes (eg. by simulation), we\u00a0\u2026", "num_citations": "8\n", "authors": ["366"]}
{"title": "Architecting dynamic systems using containment units\n", "abstract": " Software modification can require as much time, human effort, and expense as the original development, so considerable software engineering research has been directed toward identifying ways in which software can be developed to facilitate subsequent change. One highly successful approach is to develop software using modules, or objects, each of which seals within itself decisions or secrets that are key to successfully addressing its requirements. When one of these requirements changes, it should be expected that the need to meet this changed requirement is to be satisfied by making appropriate changes to the module intended to satisfy that requirement. These changes are usually done manually and off-line. In our work we are exploring an approach where software systems make changes to themselves while executing.. Our approach is based on Containment Units, which are modules able to selfdiagnose the need for changes based on their operational characteristics and then to make a limited set of changes aimed at meeting these needs.", "num_citations": "8\n", "authors": ["366"]}
{"title": "Design guidance through the controlled application of constraints\n", "abstract": " Seeks to facilitate the development of high-quality software designs and architectures by using rigorous process definitions to guide the application of the complex structure of relations and constraints that define well-formedness. We identify various types of constraints and demonstrate specific instances of these types. We endorse the value of maintaining the integrity of these constraints by reacting to their violation with diagnostics and remedies. The sheer number and diversity of these constraints, however, indicates the desirability of a mechanism for controlling the scope and effect of their enforcement. Thus, we propose to use proactive process specifications to control the enforcement of and reaction to the various constraints. This results in a process-driven system that supports designers and architects by guiding them through orderly development and rework processes, disciplined by the application of\u00a0\u2026", "num_citations": "8\n", "authors": ["366"]}
{"title": "Jil and little-jil process programming languages\n", "abstract": " We are seeking to develop a process programming language that is successful in meeting some difficult and conflicting objectives. We want the language to be precise, based upon firm semantics, comprehensive, capable of concise exposition on a wide range of process issues, and yet easily conprehensible, using a visual editor for process program development.", "num_citations": "8\n", "authors": ["366"]}
{"title": "Program testing techniques using simulated execution\n", "abstract": " Simulation is proving to be a valuable technique in testing computer programs. By simulating different aspects of a program's execution and structure it is possible to detect errors and sometimes demonstrate the absence of certain errors in the program. This presentation will explore three popular testing methodologies which employ simulation techniques. Each methodology is based upon a different type of simulation of the program. The differences in error detection capability resulting from these different choices of simulated execution will be examined. Finally a method for using the best characteristics of each technique in a general validation system will be presented.", "num_citations": "8\n", "authors": ["366"]}
{"title": "Precise process definitions for activities of daily living: a basis for real-time monitoring and hazard detection\n", "abstract": " The dramatically increasing population of disabled people and adults who are 65 years old and over will increase financial burdens for assisted living care in the United States and more generally on a global basis. To mitigate these costs, increasing numbers of disabled and elderly people (our clientele) will live alone at home. This paper suggests that the safety of such disabled and elderly people might be increased by using precise process definitions of Activities of Daily Living (ADLs) as the basis for guiding and monitoring their activities. We propose to model ADLs using Little-JIL, a language that supports ADL definitions that are distinguished from other ADL definitions in the literature by their use of such features as concurrency, exception handling, reaction control, and channel communication, all of which are important for monitoring ADLs at appropriately low levels of detail.", "num_citations": "7\n", "authors": ["366"]}
{"title": "Categorizing and modeling variation in families of systems: a position paper\n", "abstract": " This paper presents an approach that considers variation in systems and system architectures according to the kind of relation among the variants in the software family. The approach highlights why it is beneficial to consider such different variation relations separately and gives examples of what these relations may be.", "num_citations": "7\n", "authors": ["366"]}
{"title": "Ubiquitous process engineering: Applying software process technology to other domains\n", "abstract": " Software engineering has learned a great deal about how to create clear and precise process definitions, and how to use them to improve final software products. This paper suggests that this knowledge can also be applied to good effect in many other domains where effective application of process technology can lead to superior products and outcomes. The paper offers medical practice and government as two examples of such domains, and indicates how process technology, first developed for application to software development, is being applied with notable success in those areas of endeavor. The paper also notes that some characteristics of these domains are highlighting ways in which current process technology seems to be inadequate, thereby suggesting ways in which this research is adding to the agenda for research in software process.", "num_citations": "7\n", "authors": ["366"]}
{"title": "Requirements-based design guidance: A process-centered consistency management approach\n", "abstract": " In previous work, we have argued that constraints on the relationships between requirements elements and design elements can be used to guide designers in making design decisions. We have also argued the importance of controlling which constraints to apply, when to apply them, and what responses to take on constraint failure in order to make this guidance effective. Our suggestion, to control the application of constraints with a formalized, executable process program, offers the benefit of not applying all constraints at all times and therefore not overwhelming designers with input, most of which is not applicable at all times. Rather, our approach offers the possibility of identifying and exploiting the most appropriate constraints at the most opportune times.In this paper we present results of our experiments in applying this approach to a UML design based on requirements specified as use cases. The paper emphasizes two observed benefits of our approach. First, we note that our approach enhances traceability in that it facilitates control of the execution of constraints. Second, we note that our approach enables the specification of consistency rules for design milestones, thereby guiding designers without sacrificing important latitude and flexibility during the design process.", "num_citations": "7\n", "authors": ["366"]}
{"title": "An alternative to software process languages\n", "abstract": " We argue that years of research into the characteristics needed in an \"ideal\" software process definition language have only demonstrated that such a language, if one were to exist, would be impossibly large and complicated. We suggest that it would be most difficult to write clear and effective process definitions in such a language. Thus, the prospects for continuing with this line of research seem bleak. We suggest that, instead we should model software processes using a variety of modelling formalisms, and we should implement software process execution systems as distributed software systems. These systems should be composed of a heterogeneous mix of process fragments, written in different languages designed to support the gamut of different specialized tasks that we now understand to be necessary.", "num_citations": "7\n", "authors": ["366"]}
{"title": "A STRATEGY FOR INTEGRATING PROGRAM TESTING ANT) ANALYSIS\n", "abstract": " This paper presents a view of how the techniques of static analysis and dynamic program testing can be combined and integrated into a too1 supported methodology which smoothly incorporates the best features of each. The paper is composed of two major components. The first is more general and descriptive. In it the central importance of dynamic testing by means of programmer generated assertions is stressed first, and some remarks about tool support for assertion testing are made, Various weaknesses of dynamic testing are then remarked van, motivating the desirability of using static analysis as well. The general characteristics of static analysis, and especially data flow analysis are described next. Static analysis is then described as a technique for making certain kinds of dynamic testing more efficient and trustworthy. Symbolic execution and formal verification are presented next and described as logical and important com. ponents of the being integrated sys tern described. The second component of the paper deals with TOOLPACK, an integrated ensemble of tools of the types described in the first component. The architecture and high level design of TOT) L, PACK are described, and some implementation plans are presented.", "num_citations": "7\n", "authors": ["366"]}
{"title": "Resource specification for prototyping human-intensive systems\n", "abstract": " Today\u2019s software systems rely heavily on complex resources, such as humans. Human-intensive systems are particularly important in our society, especially in the healthcare, financial, and software development domains. One challenge in developing such systems is that the system design must account for the constraints, capabilities, and allocation policies of their complex resources, particularly the humans. The resources, their capabilities, and their allocation policies and constraints need to be carefully specified, and modeled. Toward the goal of supporting the design of systems that make effective use of such resources, we introduce a resource specification language and a process-aware, discrete-event simulation engine that simulates system executions while adhering to these resource specifications. The simulation supports (1)\u00a0modeling the resources that are used by the system, and the ways in\u00a0\u2026", "num_citations": "6\n", "authors": ["366"]}
{"title": "Refactoring planning and practice in agile software development: an empirical study\n", "abstract": " Agile software engineering increasingly seeks to incorporate design modification and continuous refactoring in order to maintain code quality even in highly dynamic environments. However, there does not currently appear to be an industry-wide consensus on how to do this and research in this area expresses conflicting opinions. This paper presents an empirical study based upon an industry survey aimed at understanding the different ways that refactoring is thought of by the different people carrying out different roles in agile processes and how these different people weigh the importance of refactoring versus other kinds of tasks in the process. The study found good support for the importance of refactoring, but most respondents agreed that deferred refactoring impacts the agility of their process. Thus there was no universally agreed-upon strategy for planning refactoring. The survey findings also indicated that\u00a0\u2026", "num_citations": "6\n", "authors": ["366"]}
{"title": "Engineering of Software: The Continuing Contributions of Leon J. Osterweil\n", "abstract": " Software engineering research can trace its roots to a few highly influential individuals. Among that select group is Leon J. Osterweil, who has been a major force in driving software engineering from its infancy to its modern reality. For more than three decades, Prof. Osterweil's work has fundamentally defined or significantly impacted major directions in software analysis, development tools and environments, and software process--all critical parts of software engineering as it is practiced today. His exceptional contributions to the field have been recognized with numerous awards and honors through his career, including the ACM SIGSOFT Outstanding Research Award, in recognition of his extensive and sustained research impact, and the ACM SIGSOFT Influential Educator Award, in recognition of his career-long achievements as an educator and mentor. In honor of Prof. Osterweil's profound accomplishments, this book was prepared for a special honorary event held during the 2011 International Conference on Software Engineering (ICSE). It contains some of his most important published works to date, together with several new articles written by leading authorities in the field, exploring the broad impact of his work in the past and how it will further impact software engineering research in the future. These papers, part of the core software engineering legacy and now available in one commented volume for the first time, are grouped into three sections: flow analysis for software dependability, the software lifecycle, and software process.", "num_citations": "6\n", "authors": ["366"]}
{"title": "Developing discrete event simulations from rigorous process definitions\n", "abstract": " Mohammad S. Raunak1, Leon J. Osterweil, Alexander Wise Department of Computer Science University of Massachusetts Amherst {raunak, ljo, wise}@ cs. umass. edu", "num_citations": "6\n", "authors": ["366"]}
{"title": "Automatically analyzing software processes: Experience report\n", "abstract": " Sound methods of analysis and comparison of software processes are crucial for such tasks as process understanding, process correctness verification, evolution management, process classification, process improvement, and choosing the appropriate process for a certain project. The purpose of our research is to lay the foundations for a systematic and rigorous comparison of processes by establishing fixed methods and conceptual frameworks that are able to assure that comparison efforts will yield predictable, reproducible results. The analysis framework presented here assumes that the comparison will be done relative to a fixed standard feature classification schema for the processes used, and with the use of a fixed formalism for modeling the processes. The aspect of the system described in this paper is focused on functional analysis of processes according to the predefined comparison topics, well\u00a0\u2026", "num_citations": "6\n", "authors": ["366"]}
{"title": "Provenance Support for Rework.\n", "abstract": " Refactoring is rework of design May or may not be triggered when code is recognized as being untidy There are many different design patterns [Fowler 1999] Separate Query from Modifier Refactoring Splits a method that was both a query and a modifier into two methods", "num_citations": "5\n", "authors": ["366"]}
{"title": "Experimental application of process technology to the creation and adoption of online dispute resolution\n", "abstract": " We report on the development of formal models of alternative dispute resolution processes, the creation of an online dispute resolution system based on this model and initial experimental analysis of this system. Early results suggest that formalizing the negotiation process definition indeed leads to clearer understandings and a greater chance for effective automation.", "num_citations": "5\n", "authors": ["366"]}
{"title": "Process technology for achieving government online dispute resolution\n", "abstract": " We have taken on the challenge of using process technology to help create Government Online Dispute Resolution (GODR) systems. The creation of even straightforward digital government applications has lagged parallel developments in e-commerce, due in part to the stringent requirements we place on digital government and to the requirements for collaboration among all stakeholders our system of government imposes. Our premise is that to meet these requirements and overcome resistance to change we must focus on establishing and maintaining trust in all stakeholders. We envision that the development of digital government systems will be viewed as the design, analysis, implementation, execution, and modification of efficient, effective processes with stringent fairness requirements. We are merging powerful process definition and analysis approaches into participatory design methods to overcome resistance to change in order to create digital government systems that are efficient and effective and also convey a strong sense of fairness leading to a high level of trustworthiness.We are demonstrating our approach through the domain of dispute resolution. Efficient, effective and fair dispute resolution can do much to restore trust in government, even after other transactions may have been problematic. Poorly done dispute resolution can, on the other hand, destroy trust in government.", "num_citations": "5\n", "authors": ["366"]}
{"title": "A framework for relocation in mobile process-centered software development environments\n", "abstract": " This paper addresses the problem of enabling a user of a process-centered SDE, hosted on a high-speed network, to continue working on a detached mobile workstation connected by a lower speed interruptable communications link. The focus of the paper is a process for determining whether and how to allow detachment. The process takes into account a broad range of factors, including the speed and reliability of the mobile link, the relative sizes and speeds of the mobile and networked workstations, the nature and state of the development process, and the importance of the detaching user. The paper presents a formal framework that allows us to specify the various factors, and an objective function that quanti es the inconvenience incurred by the detachment. The process uses the framework to determine which tools and resources to relocate when the user detaches from the network. A detailed example is used to illustrate this process.", "num_citations": "5\n", "authors": ["366"]}
{"title": "A process-modeling based approach to comparing and integrating software design methodologies\n", "abstract": " Experience in using a systematic approach to compare and integrate software design methodologies is reported. A comparison approach based on modeling the design methodologies and classifying their components is discussed. Six design methodologies were composed in the context of using this approach, and assessed in relation to comparable work of other researchers. Experiences with assessments of the comparison approach indicate that it is a powerful tool for analyzing software development methodologies.<>", "num_citations": "5\n", "authors": ["366"]}
{"title": "The architecture of the Arcadia-1 process centered software environment\n", "abstract": " Arcadia is a research project aimed at producing process-centered software environments. The project [3] is organized as a consortium of six universities and corporations, collaboration on parallel research in five major areas of importance to software environments: environment architecture, software process, software object management, user interfaces, and measurement and evaluation. Arcadia will produce Arcadia, 1 a prototype process-centered software environment that integrates componentry from each of these parallel research activities. The integration effort provides experimental evaluation of the separate research activities as well as a working prototype environment.A process-centered environment in one that supports the activities of its users by proactively coordinating the actions of tools and humans in the execution of software processes that are explicitly represented by executable code contained\u00a0\u2026", "num_citations": "5\n", "authors": ["366"]}
{"title": "TRICS: a Testing Tool for C\n", "abstract": " Since validating a software system can amount to as much as 50% of the total life-cycle costs, tools have been developed to automate parts of this process. However, most of these are for Fortran or Cobol. TRICS is an interactive tool which uses data flow analysis to help prepare test data for programs in C. It provides a standard environment for executing tests on the user's choice of a subset of procedures. It retains the results as an aid to integration and regression testing. Pointers, multiple compilation units, multiple procedures, and recursion are treated.", "num_citations": "5\n", "authors": ["366"]}
{"title": "Allegations as aids to static program testing\n", "abstract": " In static program analysis, a program is examined without execution in an attempt to anticipate possible sources of error. Possible errors detected in this way can rarely be considered certain to occur because of the impossibility of infallibly determining the executability of a given program path. Current heuristic systems for making this determination are costly and uncertain. Hence the use of allegations\u2014user supplied unverifiable statements designed to provide answers to executability questions\u2014is suggested.", "num_citations": "5\n", "authors": ["366"]}
{"title": "Generation, composition, and verification of families of human-intensive systems\n", "abstract": " Software products are rarely developed without providing different sets of features to better meet varying user needs, whether through tiered products as part of a product line or different subscription levels for software as a service (SaaS). Software product line approaches for generating and maintaining a family of different variants of software products address such needs for variation quite well. Real-world human-intensive systems (HISs) display similar needs for families of variants. A key contribution of this paper is to show how many of these needs can be rigorously and systematically addressed by adapting established techniques from system and software product line engineering (SPLE).", "num_citations": "4\n", "authors": ["366"]}
{"title": "DTTAS: A Dynamic Testing Tool for Agent-based Systems\n", "abstract": " Software agents are designed to be mobiles, autonomous, proactive, collaborative, operate asynchronously and in parallel, and communicate through message passing instead of method invocation. Since agents are run autonomously and cooperatively with other agents, so they may run correctly by themselves but incorrectly in a community or vice versa. For specific nature of software agents, it is difficult to apply traditional software testing and debugging techniques to agents. Agents require special techniques dedicated for testing. This paper introduces a dynamic testing tool that uses a temporal logic assertion language for detecting run time errors in agents and agent-based systems. The tool evaluates agent behaviors, detects errors related to agent behaviors constraints, errors related to agents interactions, errors related to user requirements and security vulnerabilities of agent-based web applications. Also, in this paper, we introduce the grammar, the syntax and the semantic of the temporal logic assertion language. The dynamic testing tool has been built and tested.", "num_citations": "4\n", "authors": ["366"]}
{"title": "Supporting Undo and Redo in Scientific Data Analysis.\n", "abstract": " Supporting Undo and Redo in Scientific Data Analysis Page 1 Supporting Undo and Redo in Scientific Data Analysis Xiang Zhao , Emery R. Boose , Yuriy Brun , xiang@cs.umass.edu, boose@fas.harvard.edu, brun@cs.umass.edu Barbara Staudt Lerner , Leon J. Osterweil blerner@mtholyoke.edu, ljo@cs.umass.edu University of Massachusetts Amherst Mount Holyoke College Harvard University http://laser.cs.umass.edu Page 2 Scientific Data Analysis Page 3 Undo and Redo in Scientific Data Analysis Page 4 Undo and Redo in Scientific Data Analysis Page 5 Undo and Redo in Scientific Data Analysis Page 6 Undo and Redo in Scientific Data Analysis Page 7 Undo and Redo in Scientific Data Analysis Page 8 Undo and Redo in Scientific Data Analysis Page 9 DDG: Provenance Support for Undo/Redo Sensor Calibration Quality Control Model Based Gap-filling Calibrated Data Quality Controlled Data Gap-filled () : /\u2026", "num_citations": "4\n", "authors": ["366"]}
{"title": "Characterizing process variation (NIER Track)\n", "abstract": " A process model, namely a formal definition of the coordination of agents performing activities using resources and artifacts, can aid understanding of the real-world process it models. Moreover, analysis of the model can suggest improvements to the real-world process. Complex real-world processes, however, exhibit considerable amounts of variation that can be difficult or impossible to represent with a single process model. Such processes can often be modeled better, within the restrictions of a given modeling notation, by a family of models. This paper presents an approach to the formal characterization of some of these process families. A variety of needs for process variation are identified, and suggestions are made about how to meet some of these needs using different approaches. Some mappings of different needs for variability to approaches for meeting them are presented as case studies.", "num_citations": "4\n", "authors": ["366"]}
{"title": "Modeling faults to improve election process robustness\n", "abstract": " This paper presents an approach for continuous process improvement and illustrates its application to improving the robustness of election processes. In this approach, the Little-JIL process definition language is used to create a precise and detailed model of an election process. Given this process model and a potential undesirable event, or hazard, a fault tree is automatically derived. Fault tree analysis is then used to automatically identify combinations of failures that might allow the selected potential hazard to occur. Once these combinations have been identified, we iteratively improve the process model to increase the robustness of the election process against those combinations that seem the most likely to occur.We demonstrate this approach for the Yolo County election process. We focus our analysis on the ballot counting process and what happens when a discrepancy is found during the count. We identify two single points of failure (SPFs) in this process and propose process modifications that we then show remove these SPFs.", "num_citations": "4\n", "authors": ["366"]}
{"title": "Unifying the Software Process Spectrum: International Software Process Workshop, SPW 2005, Beijing, China, May 25-27, 2005 Revised Selected Papers\n", "abstract": " This volume contains papers presented at SPW 2005, the Software Process Workshop held in Beijing, PR China, on May 25-27, 2005, and prepared for final publication. The theme of SPW2005 was \u201cUnifying the Software Process Spectrum.\u201d Software process encompasses all the activities that aim at developing or evolving software products. The expanding role of software and information systems in the world has focused increasing attention on the need for assurances that software systems can be developed at acceptable speed and cost, on a predictable schedule, and in such a way that resulting systems are of acceptably high quality and can be evolved surely and rapidly as usage contexts change. This sharpened focus is creating new challenges and opportunities for software process technology. The increasing pace of software s-tem change requires more lightweight and adaptive processes, while the increasing mission criticality of software systems requires more process predictability and c-trol as well as more explicit attention to business or mission values. Emergent app-cation requirements create a need for ambiguity tolerance. Systems of systems and global development create needs for scalability and multi-collaborator, multi-culture concurrent coordination. COTS products provide powerful capabilities, but their v-dor-determined evolution places significant constraints on software definition, dev-opment, and evolution processes. The recognition of these needs has spawned a considerable amount of software process research across a broad spectrum.", "num_citations": "4\n", "authors": ["366"]}
{"title": "Programming Rework in Software Processes\n", "abstract": " Our long-term research in process programming is based on the hypothesis that software processes can and should be captured accurately and formally, using executable formalisms to support execution, analysis, and understanding. Many process languages have been developed over the years for modeling processes formally. In this paper, we argue that for automated support, we need not a process modeling language, but a process programming language\u2013a language with semantics sufficient to support the execution of process definitions. Specifically, we argue that invocation semantics are required for accurately describing real-world development processes.We use the common phenomenon of rework as an example of a process feature which requires invocation semantics for adequate specification. In this paper, we argue that rework can only be accurately described using invocation semantics borrowed from general purpose programming languages. We argue this in the general case and demonstrate a specific case via an example using Little-JIL, our hierarchical process programming language.", "num_citations": "4\n", "authors": ["366"]}
{"title": "HI-PLAN and Little-JIL: A study of contrast between two process languages\n", "abstract": " A key issue for process languages is balancing the need for technical rigor with this need for ease of use. Little-JIL and HI-PLAN are new powerful, yet clear, process languages that attempt to resolve above two apparently conflicting objectives. This paper evaluates both the languages and provides valuable directions to the next-generation process language design. We describe their design goals and features, present solutions to a well-known benchmark process, ISPW-6 software process example, and analyze the relative strengths and weaknesses of each language through detailed comparisons on a wide variety of issues.", "num_citations": "4\n", "authors": ["366"]}
{"title": "A rigorous approach to resource management in activity coordination\n", "abstract": " System behaviors can be expected to vary widely depending upon the availability, or shortage, of resources that they require. Thus, the precise specification of resources required by, and resources available to, a system is an important basis for being able to reason about, and optimize, system behavior. Previous resource models for such disciplines as management and workflow have lacked the rigor to support powerful reasoning and optimization. Some resource models for operating systems have been quite rigorous, but have generally been overly narrow in scope. This paper argues that it is important to be able to optimize and reason about the broad and complex resource requirements of modern distributed multi agent systems. This entails precisely modeling a wide range of entity types, including humans, tools, computation platforms, and data. The paper presents a metamodeling approach that can be used to create precise models of a wide range of resource types, and provides examples of the use of this metamodel. The paper also describes a prototype resource allocation and management system that implements these approaches. This prototype is designed to be a separable, orthogonal component of a system for supporting the execution of processes defined as hierarchies of steps, each of which incorporates a specification of resource requirements.", "num_citations": "4\n", "authors": ["366"]}
{"title": "Comparing implementation strategies for composite data flow analysis problems\n", "abstract": " FLAVERS, a tool for verifying properties of concurrent systems, uses composite data ow analysis to incrementally improve the precision of the results of its veri cations. Although FLAVERS is one of the few static analysis techniques for concurrent systems that does not have exponential worst case bounds, it sometimes can still be very expensive to use. In this paper we experimentally compare the cost of two approaches for solving composite data ow analysis problems. The rst approach, product-based, is the more straightforward approach, and the second, tuple-based, is built around the idea of reducing analysis space requirements at the expense of analysis time. We demonstrate experimentally, by analyzing properties of actual concurrent programs, that the tuple-based approach is comparable in time to the product-based approach but for large composite data ow problems it requires several orders of magnitude less space. keywords", "num_citations": "4\n", "authors": ["366"]}
{"title": "AIDA: a dynamic analyzer for Ada programs\n", "abstract": " This article presents a dynamic analyzer for Ada programs called AIDA. In software engineering, previous dynamic analyzers have often incorporated first-order logic assertion languages. For dynamic testing of both sequential and concurrent programs, however, temporal logic may be advantageous because it deals with the development of situations over time. AIDA investigates the applicability of temporal logic in building a dynamic analyzer for Ada programs. AIDA is designed to test, debug, and specify programs written in the Ada language. It affects the instrumentation of programs as well as collecting, organizing, and reporting of results of the instrumented program. The instrumentation approach is based on the idea that the intended function of a program can often be specified in terms of assertions or values that must be assumed by variables at certain strategic points in the program. This article describes the\u00a0\u2026", "num_citations": "4\n", "authors": ["366"]}
{"title": "Managing change in software development through process programming\n", "abstract": " Change is pervasive during software development. Change management can be facilitated by software-process programming, which formalizes software products and processes in software-process programs. Toward this end process-programming languages PPLs should include constructs that address specific change-management problems. These include lack of explicit representation for relationships, weak or inflexible constraints on objects and relationships, visibility of implementations, lack of formal representation of processes, and dependence on manual practices. APPLA is a prototype PPL that addresses these problems. APPLA is an extension to Ada, APPLA includes abstract, persistent relations with programmable implementations, relation attributes that may be composite and derived, triggers that react to relation operations, optionally-enforcible predicates on relations, and five composite statements that provide flexible transaction-related capabilities. Relations enable relationships to be represented explicitly and derivation dependencies to be maintained automatically. Relation bodies may implement alternative storage and computation strategies without affecting users of relation specifications. Triggers can automatically propagate data, invoke tools, and perform other change-management tasks. Predicates and the transaction-related statements can be used to support change management in the face of concurrent processes and evolving standards of consistency. Together, these features mitigate many of the problems that complicate change management in software development.Descriptors:", "num_citations": "4\n", "authors": ["366"]}
{"title": "Software maintenance as a programmable process\n", "abstract": " The software maintenance process is a particularly complex part of the software life cycle. It can be viewed from a number of different perspectives and dimensions. The policies and philosophies of the maintenance organization and its management, the techniques available for carrying out maintenance, the types of changes attempted, the points in the development process at which maintenance is attempted, and the nature of the subject product are among the factors playing important roles in shaping and designing a maintenance process. No single fixed maintenance process seems able to meet all software maintenance needs emerging from the different perspectives and dimensions, and nobody has yet consolidated all of those views in a single framework. We believe that consolidating the maintenance activity around the notion of Process Programming provides such a common framework for all software maintenance processes. It provides the conceptual structure for creating processes and support environments in which users are free to alter both tools and process to achieve effective support for the full range of maintenance needs and approaches. Process environments environments which support process programming seem to us to meet the minimum requirements for an ideal environment. They focus on both describing and aiding the process itself in a customizable programmable, user-tailorable, dynamically adaptable, and incrementally implementable fashion.Descriptors:", "num_citations": "4\n", "authors": ["366"]}
{"title": "Be Prepared\n", "abstract": " Be prepared! The motto of the Boy Scouts of America. It is also good advice in general, and for software developers in particular. And good preparation is indeed what we academics try to give the students in our Computer Science (CS) and Software Engineering (SE) degree programs. We prepare our students for the need to master new programming languages in coming decades by teaching them the fundamentals underlying a broad range of programming languages. We prepare our students for the need to produce highly reliable programs by teaching them the principles that underlie effective testing and (generally to a disappointingly lesser extent) static analysis and formal verification. We prepare our students for the need to worry about speed and performance by teaching them about algorithmic complexity and about performance measurement. Many degree programs go farther and attempt to prepare\u00a0\u2026", "num_citations": "3\n", "authors": ["366"]}
{"title": "Specification and analysis of human-intensive system resource-utilization policies\n", "abstract": " Complex, human-intensive systems, such as those used in hospital Emergency Departments, typically require the effective support of many types of resources, each governed by potentially complex utilization policies. Resource utilization policies range from simple, e.g., sickest patient first, to e xtremely complex, responding to changes in system environment, state, and stimuli. Further, policies may at times conflict with each other, requiring conflict resolution strategies that further increase the complexity. Sound policies for the management of these resources are crucial in assuring that these systems achieve their key goals. To help system developers make sound resource management decisions, this paper presents a resource utilization policy specification and analysis framework for complex human-intensive systems. We provide (1) a precise specification language to describe very diverse and potentially complex\u00a0\u2026", "num_citations": "3\n", "authors": ["366"]}
{"title": "The problem of\n", "abstract": " The tem \u201cevangelicalism'is widey used today for many different purposes. It is used by historians of religion or church history to refer to particular trends, doctrines, or behavios in Protestant, especially North Atlantic Christianity, from the 18th century to the present This despite the fact that the phenomenon of \u201cevangelicalism'in the UK, Canada, and the US is very different in the 18th century and today. And then such a widely used term, and one so tied to Anglo-American church history, is now used to identify or describe Christian churches, movements, and groups of people all over the world. In this short paper 1 just want to point out some of the potential problems that may exist in using this tem in E. Asia, in particular in doing the church history of China and Japan I am NOT aging, at least not at this point, that we not use the tem evangelicalism, just thinking about the implications of its use. And l welcome comments\u00a0\u2026", "num_citations": "3\n", "authors": ["366"]}
{"title": "Applying little-JIL to describe process-agent knowledge in SoftPM\n", "abstract": " In a software process modeling method based upon the Organization-Entity capability, the Process-Agent is a well-defined unit whose role is to encapsulate an entity\u2019s knowledge, skill etc. The Process-Agent\u2019s infrastructure comprises descriptive knowledge, process knowledge and an experience library. The process knowledge is represented by process steps, whose execution determines the behaviors of the Process-Agent. This causes Process-Agent knowledge to be precisely described and well organized. In this paper, Little-JIL, a well-known process modeling language, is used to define a Process-Agent\u2019s process knowledge. Benefits for process element knowledge representation arising from Little-JIL\u2019s simplicity, semantic richness, expressiveness, formal and precise yet graphical syntax etc., are described.", "num_citations": "3\n", "authors": ["366"]}
{"title": "Early lessons from the application of process technology to online grievance mediation\n", "abstract": " In this paper, we report early lessons learned from a new project to understand how Online Dispute Resolution (ODR) can improve efficiency, effectiveness and fairness in Government dispute resolution and how ODR systems can gain acceptance.", "num_citations": "3\n", "authors": ["366"]}
{"title": "Architecting processes are key to software quality\n", "abstract": " In this position paper we explain why Architecting Processes, namely processes for using architectures to develop software, should receive more attention. Architectures are viewed as software engineering artifacts that can be used as the basis for developing superior software products. But the development of these products should still be accomplished with the guidance of processes. The processes are themselves software artifacts that should be developed in such a way as to demonstrably assure that they achieve their goals and requirements. Architecting Processes should be developed to demonstrably assure that software products are of high quality. Different processes will have different effectiveness in assuring this. Different architecture definition formalisms will be of differing value in supporting these different processes. The selection of an architecture definition formalism should, therefore, be strongly determined by the choice of Architecting Processes, and the goals and requirements that they must satisfy.", "num_citations": "3\n", "authors": ["366"]}
{"title": "Assuring accuracy and impartiality in software design methodology comparison\n", "abstract": " Software product and process modelling can be used as bases for more accurate, objective, and repeatable comparison of software design methodologies (SDM's). In earlier work we demonstrated the use of a fixed feature comparison process (called CDM), a fixed (but extensible) framework of design methodology features (the Base Framework, BF), and a fixed process/product modelling formalism (based upon HFSP) in supporting such comparisons (X. Song, 1992; X. Song and L.J. Osterweil, 1994). We have obtained both broad, general SDM comparison results and sharp narrowly focussed SDM comparison results and have used them to confirm the power and validity of our approach. In the course of this work we have revised, modified, and extended the BF to improve its scope of applicability and its accuracy. We have also improved CDM by elaborating it down to lower levels of detail, and by being more\u00a0\u2026", "num_citations": "3\n", "authors": ["366"]}
{"title": "An argument for the elimination of roles\n", "abstract": " The role approach currently used in many process models is seriously flawed. It imposes rigid boundaries separating people from the actions they might perform. It also combines a number of concepts that are best kept separate. We propose an alternative to roles based on loosening boundaries and on decomposing the role concept into its constituent elements.", "num_citations": "3\n", "authors": ["366"]}
{"title": "Experiences with process programming\n", "abstract": " Our research has investigated the feasibility and ramifications of process programming. We claim that it is highly advantageous to treat software processes, such as development and maintenance, as actual software which can be specified, designed, coded, executed, tested, and maintained. If software processes are to be coded and executed, it is necessary for them to be coded in a language which can then be compiled, loaded, and executed. Likewise, if software processes are to specified and designed it is necessary to have process specification and design formalisms. We claim that process specification, design, and coding languages and formalisms will strongly resemble languages and formalisms currently used to specify software products, but that there will also be significant differences. We also claim that a properly conceived and developed software support environment can support the development of\u00a0\u2026", "num_citations": "3\n", "authors": ["366"]}
{"title": "Directions for US research and development efforts on software testing and analysis\n", "abstract": " This report sets forth a national research agenda aimed at improving the quality of computer software\u2013an invisible, yet increasingly crucial, element of our national infrastructure. The report is a summary of a meeting of research and industrial leaders, held at San Diego, California, 1-3 August 1989, under the sponsorship of the US Office of Naval Research and Office of Naval Technology. This workshop identified nine promising directions of analysis and testing research, aimed at strengthening the underlying foundations and furthering the exploration of innovative and novel ideas. Most of these research directions show promise of producing technology transferable into effective use in industry within the decade.", "num_citations": "3\n", "authors": ["366"]}
{"title": "A Strategy for Effective Integration of Verification and Testing Techniques.\n", "abstract": " This paper presents an approach to integrating four techniques for testing, analysis and verification into one overall strategy for incrementally raising confidence in software in a cost effective way. The paper summarizes the strengths, weaknesses, and operational characteristics of dynamic testing, static analysis, symbolic execution and formal verification. It uses a detailed example as an illustration. Next the integrated strategy is presented. Finally, there is a discussion of how this strategy can be used to raise confidence in software requirements and design specifications as well as in code, thereby making it applicable throughout the entire software lifecycle. AuthorDescriptors:", "num_citations": "3\n", "authors": ["366"]}
{"title": "Process and workflow\n", "abstract": " Processes govern every aspect of software development and every aspect of application usage. Whether trivial, complex, formal, or ad hoc, processes are pervasive in software engineering. This chapter summarizes a variety of ways in which process models, also referred to as workflows, can be used to achieve significant improvements in a range of different disciplines. The chapter starts with a brief summary of the evolution of this approach over the past century. It then identifies some principal ways in which important advantages can be obtained from exploiting process models and process technology. The chapter goes on to specify key criteria for evaluating process modeling approaches, suggesting the different kinds of modeling approaches that seem particularly effective for supporting different uses. A representative set of examples of current process modeling approaches, and different ways in which\u00a0\u2026", "num_citations": "2\n", "authors": ["366"]}
{"title": "Be gracious\n", "abstract": " In the early 1980\u2019s, when I was a new Department Chair, I was \u201cinvited\u201d by the Dean of my Engineering College, to attend a dinner fund-raising event with some of his most prized donors. I found myself seated at a round table with several other engineers. One of the older of these gentlemen (yes, they were all men\u2026) suggested that we get to know each other by introducing ourselves and describing the sort of work that we do. After hearing from a Civil Engineer, a couple of Electrical Engineers, and an Aeronautical Engineer, I proudly introduced myself and said that I was a Software Engineer. After a couple of seconds of silence, there was a loud guffaw, followed by nervous laughter around the table. There was then some earnest, but rather embarrassed, inquiry by these engineers of tangibles, into why and whether the word \u201cEngineering\u201d might properly be connected with the intangible object,\u201cSoftware\u201d.Fast\u00a0\u2026", "num_citations": "2\n", "authors": ["366"]}
{"title": "What is software? The role of empirical methods in answering the question\n", "abstract": " This paper explores the potentially pivotal role of Empirical Methods in addressing existential questions about the nature of software. Building upon an earlier paper that asked the question \u201cWhat is software?\u201d, this paper suggests that a key way to gain such understanding is to ponder the question of how to determine the size of a software entity. The paper notes that there have been a variety of indirect approaches to measuring software size, such as measuring the amount of time taken to produce software, and measuring the number of lines of code in a software entity. But these assume implicitly that such measures correlate positively with the inherent size of the software entity, broadly construed to include the entire panoply of code and non-code artifacts and their interconnections that comprise this entity. As in the original paper, this paper makes the case that entities such as recipes, laws, and processes\u00a0\u2026", "num_citations": "2\n", "authors": ["366"]}
{"title": "Computational predictors in online social deliberations\n", "abstract": " This research seeks to identify online participants\u2019 disposition and skills. A prototype dashboard and annotation scheme were developed to support facilitators and several computational predictors were identified that show statistically significant correlations with dialogue skills as observed by human annotators.", "num_citations": "2\n", "authors": ["366"]}
{"title": "Context, retrospection, and prospection in healthcare process definitions\n", "abstract": " Carefully defined processes can be effective tools for guiding and coordinating the actions of healthcare professionals. In past work our group has focused on defining such processes precisely and completely in order to support largely static analyses that demonstrate the absence from the processes of defects and vulnerabilities. Now increasingly our group\u2019s focus has been turning to the execution of these processes, using them to provide run-time information to guide process participants. This new focus has made it clear that more thought must be given to how to communicate with participants in order to assure more effective guidance. Our work is suggesting that participants, especially human participants, will require that process-provided guidance be accompanied by context, history, and prospective information if the guidance is to be credible, acceptable, and ultimately useful. A process definition that\u00a0\u2026", "num_citations": "2\n", "authors": ["366"]}
{"title": "Supporting negotiation and dispute resolution with computing and communication technologies\n", "abstract": " Careful study of the application of computer and communications technologies to negotiation and dispute resolution can lead to a more harmonious and functional society, but also to more effective technology for software development and evaluation, and to new facilities for pursuing social science research. Work in this area has initially been spurred on by rapid growth in the number and variety of disputes in the world, which seem to be increasing as the number of parties grows and the variety of their interactions increases. Initial application of software technology to support dispute resolution has shown encouraging success, and suggests a broader research program in which computer science and social science research enrich each other. The resulting mature technological support for dispute resolution should, moreover, have important benefits for software development, a domain in which constructive\u00a0\u2026", "num_citations": "2\n", "authors": ["366"]}
{"title": "Forecast for Reproducible Data: Partly Cloudy\n", "abstract": " MR Nelson's Perspective \u201cBuilding an Open Cloud\u201d(26 June, p. 1656) projects a vision of scientific computing that is enticing in some ways, but worrisome in others. Scientists can benefit considerably from being able to tap vast computing resources without knowing where the data or processing software reside or how these resources are provided. But ignorance of the nature\u2014and more important, the provenance\u2014of such resources creates novel problems for science and scientists.Repeatability, reproducibility, and transparency are the hallmarks of the scientific enterprise. As more and more scientific research in every field is based on elaborate computation (1), it becomes more and more challenging to reproduce the results of these computations (2). This challenge is greatest when computational resources are proprietary and unknown even to the scientists who use them, as is the case for Nelson's \u201cMany\u00a0\u2026", "num_citations": "2\n", "authors": ["366"]}
{"title": "Dispute prevention and dispute resolution in networked health information technology\n", "abstract": " The United States has committed to promote Health Information Technology through Electronic Health Records that will be exchanged among participants in the healthcare enterprise. In anticipation of problems, legal remedies have been established to protect the rights of all participants, especially their rights to protect the privacy of their own information. However, there are certain to be situations where disputes arise among participants that can be resolved without recourse to legal action. This paper pursues the possibility of establishing policies to encourage alternative dispute resolution through online methods integrated into the health information technology processes and lays out areas requiring research.", "num_citations": "2\n", "authors": ["366"]}
{"title": "Using process definitions to drive user interactions with digital government systems\n", "abstract": " This poster is showing that process definitions can be effective in specifying and guiding Online Dispute Resolution (ODR) to facilitate effective interactions and mediation among disputants. Simply specifying these interactions is not sufficient to assure that the mediator and the participants are effectively engaged in performing their roles in such processes. Our project is developing a system to translate process definition specifications into actions that are reflected in the screens that participants and mediators view while participating in these processes. Thus we are demonstrating that, with the help of the system we are building, the process definition can control changes to the user interfaces of the participants involved in the process itself. This should enable the process to keep all users continually informed about the state of the process in which they are participating including the actions that the user needs to perform. This should improve the ability of the process to be an effective aid to ODR mediation.", "num_citations": "2\n", "authors": ["366"]}
{"title": "Modelling and analysis of TCP network dynamics\n", "abstract": " This thesis focuses on the application of feedback control theory to the study of data communication over the Internet. At the heart of this communication lies the transmission control protocol (TCP) which is responsible for reliable and efficient data transfer. Using recently developed fluid models of TCP, we treat its congestion control phase as a feedback system and, through analysis, provide insight on its performance. The contribution of the thesis is twofold. First, it introduces a matrix analysis tool, the matrix field of values, to the stability analysis of congested networks involving arbitrary numbers of heterogeneous TCP-controlled sources and congested links. This tool enables us to derive stability results for buffer-based active queue management (AQM) schemes, revealing the impact that routing plays on stability robustness. This matrix field of values also proves valuable in synthesizing stabilizing source controllers\u00a0\u2026", "num_citations": "2\n", "authors": ["366"]}
{"title": "Automated fault tree analysis for engineering design optimization\n", "abstract": " This paper presents an automated fault tree analysis for engineering design optimization process. Specifically, a novel approach is presented in which Little-JIL, a process programming language, is applied to create a process model of engineering optimization. The process model uses a graphical language in the form of easy-to-understand block diagrams for defining processes that coordinate the activities of autonomous agents and their use of resources during the performance of a task. The use of Little-JIL facilitates agent coordination in the design optimization process and helps to model the order of and the communications between units of sub-processes. The resulting process model is easy to debug and is rigorous for simulation and formal reasoning in engineering design optimization. Furthermore, it enables the development of a clear and precise design optimization process model at different levels of\u00a0\u2026", "num_citations": "2\n", "authors": ["366"]}
{"title": "Using Containment Units for Self Adaptation of Software\n", "abstract": " Software is increasingly expected to run in a variety of environments. The environments themselves are often dynamically changing when using mobile computers or embedded systems, for example. Network bandwidth, available power, or other physical conditions may change, necessitating the use of alternative algorithms within the software, and changing resource mixes to support the software. We present Containment Units as a software architecture useful for recognizing environmental changes and dynamically reconfiguring software and resource allocations to adapt to those changes. We present examples of Containment Units used within robotics along with the results of actual executions, and the application of static analysis to obtain assurances that those Containment Units can be expected to demonstrate the robustness for which they were designed.", "num_citations": "2\n", "authors": ["366"]}
{"title": "What we learn about process specification languages from studying recipes\n", "abstract": " In this paper, we study recipes as examples of process programs. By doing so, we intend to learn about cooking as a process and recipes as specifications written in programming languages. We are looking for some insights into the principles which should be incorporated into computer programming languages, especially languages for the expression of computer software processes. We are also interested in studying how process descriptions are currently effectively communicated between humans in the specific problem domain of cooking recipes. Our work is based upon close study of a modest number of recipes. From this study, we infer the features that a language supporting the expression of recipe programs must have. We suggest that a language that is basically algorithmic can suffice, but only if it also supports concurrency, real time constructs, and data type specification that is more complex than what is available in conventional languages. Such a language should probably also support rule-based function evaluation. This language therefore incorporates a mixture of different programming language paradigms eg. algorithmic, real-time, and rule-based. This suggests that it is necessary to create languages which effectively mix paradigms if we are to cleanly and satisfactorally support the programming of human processes. In that such a language would draw upon innate human capabilities, it would probably also have much wider applicability.Descriptors:", "num_citations": "2\n", "authors": ["366"]}
{"title": "The Toolpack/Ist approach to extensibility in software environments\n", "abstract": " Toolpack is an experimental activity in which a large software tool environment is being created for the purpose of general distribution and then careful study and analysis. This paper discusses the goals, methods. architecture and design of the software system being produced as the focus of the Toolpack project. The paper begins by explaining the motivation for building integrated tool sets. It then explains the basic requirements that an integrated system of tools must satisfy in order to be successful and to remain useful both in practice and as an experimental object. The paper then presents a careful description of the actual architecture of the Toolpack integrated tool system. Finally the Toolpack project experimental plan is presented.", "num_citations": "2\n", "authors": ["366"]}
{"title": "The Toolpack Mathematical Software Development Environment.\n", "abstract": " This paper describes a research project aimed at building and studying prototype environments for Mathematical Software. The actual measurements and experiences that should help solidify knowledge about how to build effective environments. Towards this end some speculative ideas about environment file systems and command languages are presented, along with research plans for effectively evaluating these and other design notions. AuthorDescriptors:", "num_citations": "2\n", "authors": ["366"]}
{"title": "BIGMAC II: A Fortran Language Augmentation Tool\n", "abstract": " This paper describes the motivation, design, implementation, and some preliminary performance characteristics of BIGMAC, a macro definition capability for creating language enhancors and translators. BIGMAC enables the user to specify transformations through STREX, a FORTRAN-like language, which enables the specification of macros which are then used to interpretively alter incoming programs. BIGMAC is specially adapted to the processing of FORTRAN programs. This paper shows how it can be used as a deprocedurizer or flattener, a dialect-to-dialect translator, a portability and version control aid, and a device for creating language enhancements as sophisticated as new control structures and abstract data typesDescriptors:", "num_citations": "2\n", "authors": ["366"]}
{"title": "A Data Base System Designed for Flexibility and Usability from FORTRAN\n", "abstract": " This; paper disxussies the design, implementation and usefulness of a system for the. flexible, creation, accessing, and reformatting of list structured data bases in ANSI FORTRAN programs. The system is portrayed as Being a useful tool in Building prototype systems where frequent design modifications are expected and which, for reasons such as; portability, are to \u0431\u0435 written in FORTRAN.", "num_citations": "2\n", "authors": ["366"]}
{"title": "Automated Input/Output Variable Classification as an Aid to Validation of FORTRAN Programs\n", "abstract": " Certain types of errors in tbe coding of FORTRAN programs can be detected by careful analysis of the input/output usage of the variables in the program. It is easy to distinguish the value giving from the value receiving usages of variables within a statement. It is then easy to identify the input and output variables for statements and basic blocks. It is observed that a program variable must not be used as an input variable unless it has been used earlier in the program as an output variable. Conversely, once a variable has been used as an output variable, it should be expected that it will be used later in the program as an input variable. Algorithms are presented which employ depth-first searching techniques to verify whether input uses and output uses are improperly interspersed. These algorithms can also be used to determine the input and output parameters for entire subprograms. This capability extends the\u00a0\u2026", "num_citations": "2\n", "authors": ["366"]}
{"title": "Process-Model-Driven Guidance to Reduce Surgical Procedure Errors: An Expert Opinion\n", "abstract": " This paper explains how a detailed, precise surgical process model can help reduce errors by fostering better understanding, providing guidance during surgery, helping train newcomers, and by supporting process improvement. It describes the features that a process-modeling language should have in order to support the precise specification of such models.", "num_citations": "1\n", "authors": ["366"]}
{"title": "Your Software Dwells in the House of Tomorrow, Too\n", "abstract": " This is an excerpt from a famous poem much-loved by many parents. It reminds parents that their children, the products of their bodies, will go off to an unknowable future that they will inhabit and influence, eventually without any direct parental guidance and supervision. The wise parent must think about how to prepare his or her children for such a future. But the poet seems also to have something to say to software developers (amazingly enough). For our software, the products of our minds, will also go off to an unknowable future that it will have to inhabit, and might inevitably influence. But in this case too, eventually this will probably have to be without the direct guidance and supervision of the developer of the software. So, it seems important that we software developers also do what we can to prepare these intellectual offspring for their future as well. Like our children, we should expect that the software we\u00a0\u2026", "num_citations": "1\n", "authors": ["366"]}
{"title": "A Comprehensive Framework for Using Iterative Analysis to Improve Human-Intensive Process Security: An Election Example\n", "abstract": " This paper presents an approach for analyzing complex processes, including those involving human agents, hardware devices, and software systems, and illustrates the utility of this approach by analyzing part of a process for holding an election. In the work described here, the Little-JIL process definition language is used to create a precise and detailed model of an election process. Given this process definition, two forms of automated analysis are used to explore the possibility that specified security policies could be undermined. Model checking is first used to identify process execution sequences that violate event-sequence security policies and other properties. After these are addressed, fault-tree analysis is applied to identify when the misperformance of steps might allow security breaches or other undesirable outcomes to occur. The results of these analyses can provide assurance about the process, suggest areas for improvement, and, when applied to a modified process definition, evaluate proposed changes in the process.", "num_citations": "1\n", "authors": ["366"]}
{"title": "System processes are software too\n", "abstract": " This talk explores the application of software engineering tools, technologies, and approaches to developing and continuously improving systems by focusing on the systems' processes. The systems addressed are those that are complex coordinations of the efforts of humans, hardware devices, and software subsystems, where humans are on the \u201cinside\u201d, playing critical roles in the functioning of the system and its processes. The talk suggests that in such cases, the collection of processes that use the system is tantamount to being the system itself, suggesting that improving the system's processes amounts to improving the system. Examples of systems from a variety of different domains that have been addressed and improved in this way will be presented and explored. The talk will suggest some additional untried software engineering ideas that seem promising as vehicles for supporting system development and\u00a0\u2026", "num_citations": "1\n", "authors": ["366"]}
{"title": "371: Using Discrete Event Simulation to Study Patient Length of Stay\n", "abstract": " BackgroundEmergency department (ED) crowding is common. Crowding results in increased patient length of stay (LOS). Minimizing LOS would improve patient flow and reduce crowding.Study ObjectiveTo create a reproducible model of emergency care that supports using discrete event simulation (DES) to study how arrival rate, resource management, process changes, and task prioritization affect patient LOS.MethodsThe distribution of inter-arrival rate between 2 consecutive ED patients was determined during a one-month period at a site with> 100,000 annual visits. Task time distributions (median, 25%, 75% interquartile range) were determined in a convenient sample of ED staff. Arena DES was used to determine patient LOS (arrival to discharge) and contribution margin (CM, net revenue minus direct clinical costs) for 200 patients with 5 patient-acuities, acuity-based resource utilization, arrival rates of 1 to\u00a0\u2026", "num_citations": "1\n", "authors": ["366"]}
{"title": "Exception Handling Patterns for Process Modeling\n", "abstract": " Process modeling allows for analysis and improvement of processes that coordinate multiple people and tools working together to carry out a task. Process modeling typically focuses on the normative process, that is, how the collaboration transpires when everything goes as desired. Unfortunately, real-world processes rarely proceed that smoothly. A more complete analysis of a process requires that the process model also include details about what to do when exceptional situations arise. We have found that, in many cases, there are abstract patterns that capture the relationship between exception handling tasks and the normative process.", "num_citations": "1\n", "authors": ["366"]}
{"title": "Simulating patient flow through an emergency department using process-driven discrete event simulation,\u201d Software Engineering in Health Care (SEHC\n", "abstract": " This paper suggests an architecture for supporting discrete event simulations that is based upon using executable process definitions and separate components for specifying resources. The paper describes the architecture and indicates how it might be used to suggest efficiency improvements for hospital Emergency Departments. Preliminary results suggest that the proposed architecture provides considerable ease of use and flexibility for specifying a wider range of simulation problems, thus creating the possibility of carrying out a wide range of comparisons of different approaches to ED improvement. Some early comparisons suggest that the simulations are likely to be of value to the medical community and that the simulation architecture offers useful flexibility.", "num_citations": "1\n", "authors": ["366"]}
{"title": "Chemotherapy ordering and delivery: Rigorously defining and analyzing a complex process employing software engineering techniques\n", "abstract": " 17514              Background: Software engineering tools offer a novel approach to the definition and analysis of complex and defect-prone healthcare processes such as chemotherapy ordering and delivery. Methods: Software engineering researchers and oncologists, nurses, and pharmacists collaboratively defined the processes necessary to order and deliver a first cycle of adjuvant chemotherapy. Discrete procedure steps were precisely defined employing Little-JIL, a language originally created to define software development processes. Process defects were then sought through finite-state verification (using the PROPEL tool to encode constraints on event sequences for specific properties, followed by analysis employing the FLAVERS finite-state verifier), algorithmically evaluating pathways through the process model to determine if any execution of the process violates a pre-specified system property\u00a0\u2026", "num_citations": "1\n", "authors": ["366"]}
{"title": "Data Provenance and Reliability in Sensor Networks\n", "abstract": " Sensor networks that provide high resolution spatial and temporal measurements are revolutionizing environmental research and will soon support real-time environmental modeling and forecasting (Porter et al. 2005). The reliability of the resulting datasets (including both original measurements and derivative products) can be enhanced in a number of ways. For example, the physical network can be designed to minimize the number of missing or questionable values through inclusion of redundant sensors (so that measured values can be compared) or complementary sensors (so that measured values can be compared to modeled values). The data processing system can be designed to take advantage of such measurements and to support real-time quality control, modeling, and gap filling, as well as critical post-processing tasks such as correction for sensor drift and substitution for missing values.More sophisticated methods are required to ensure that the processes used to create these datasets are in fact reproducible and scientifically sound. In our view this is a general and critical problem in science today that is compounded by ready access to powerful computer tools and networks and is especially acute for complex systems such as sensor networks. The problem can be addressed by (1) developing conceptual methods for defining processes using formal representations with sufficient accuracy and detail to support analysis and execution, and (2) developing cyberinfrastructure (CI) tools based on these definitions that scientists can actually use to help ensure that their analyses are sound and reproducible.", "num_citations": "1\n", "authors": ["366"]}
{"title": "Realizing Online Dispute Resolution in a Federal Agency through Process-Based Methods\n", "abstract": " In this system demonstration, we show a formal process model of a manual Alternative Dispute Resolution process and an Online Dispute Resolution system based on this model.", "num_citations": "1\n", "authors": ["366"]}
{"title": "Impact project panel: \u201cdetermining the impact of software engineering research upon practice\u201d\n", "abstract": " The goal of the Impact project is to study the impact that software engineering research has had upon software development practice. The reasons for doing this include: identifying the sorts of contributions that have had substantial impact, determining the research modalities that have been relatively more successful, and anticipating the directions that software engineering research might most effectively pursue, based upon its history and positioning. Impact project research will be held to the highest standards of scrupulous scholarship. It is expected to be useful to the software engineering research and development communities, as well as to other academic disciplines, government funding agencies and the public at large in helping with the objective assessment of the software engineering community's record of achievement.", "num_citations": "1\n", "authors": ["366"]}
{"title": "Applying real-time scheduling techniques to software processes: A position paper\n", "abstract": " Process and workflow technology have traditionally not allowed for the specification of, nor run-time enforcement of, real-time requirements, despite the fact that time-to-market and other real-time constraints are more stringent than ever. Without specification of timing constraints, process designers cannot effectively reason about real-time constraints on process programs and the efficacy of their process programs in satisfying those constraints. Furthermore, without executable semantics for those timing specifications, such reasoning might not be applicable to the process as actually executed. We seek to support reasoning about the real-time requirements of software processes. In this paper, we describe work in which we have added real-time specications to a process programming language, and in which we have added deadline timers and task scheduling to enforce the real-time requirements of processes.", "num_citations": "1\n", "authors": ["366"]}
{"title": "Flaver: A finite state verification technique for software systems title2\n", "abstract": " Software systems are increasing in size and complexity and, subsequently, are becoming ever more difficult to validate. Finite State Verification (FSV) has been gaining credibility and attention as an alternative to testing and to formal verification approaches based on theorem proving. There has recently been a great deal of excitement about the potential for FSV approaches to prove properties about hardware descriptions but, for the most part, these approaches do not scale adequately to handle the complexity usually found in software. In this paper, we describe an FSV approach that creates a compact and conservative, but imprecise, model of the system being analyzed, and then assists the analyst in adding additional details as guided by previous analysis results. This paper describes this approach and a prototype implementation, called FLAVERS, presents adetailed example, and then provides some\u00a0\u2026", "num_citations": "1\n", "authors": ["366"]}
{"title": "Exploiting hierarchy for planning and scheduling\n", "abstract": " In real-time systems, tasks must satisfy strict timing constraints in order to avoid timing failures. In situations in which task running times are not known a priori, ie, at design time, dynamic scheduling approaches attempt to guarantee, at run time, that tasks will meet their deadlines. Clearly, such guarantees may not always be possible, and so schedulability failures are likely. One option to minimize the effect of such failures is to instead schedule alternative tasks. In this paper, we present and evaluate an approach that exploits hierarchically structured program representations that naturally occur in many application scenarios. Hierarchy allows us to (a) simplify the specification of real-time requirements and (b) specify the alternative tasks to schedule in the event of an impending timing failure. We extend an existing dynamic scheduling algorithm to deal with hierarchical task structures and present the results of our experimentation aimed at determining the efficacy of our approach.", "num_citations": "1\n", "authors": ["366"]}
{"title": "Frameworks for Reasoning about Agent Based System\n", "abstract": " This paper suggests formal frameworks that can be used as the basis for defining, reasoning about, and verifying properties of agent systems. The language, Little-JIL is graphical, yet has precise mathematically defined semantics. It incorporates a wide range of semantics needed to define the subtleties of agent system behaviors. We demonstrate that the semantics of Little-JIL are sufficiently well defined to support the application of static dataflow analysis, enabling the verification of critical properties of the agent systems. This approach is inherently a top-down approach that complements bottom-up approaches to reasoning about system behavior.", "num_citations": "1\n", "authors": ["366"]}
{"title": "Supporting Communication of Heterogeneous Distributed Agents with Agenda Management Systems\n", "abstract": " As software engineering efforts move to more complex, distributed environments, coordinating the activities of people and tools becomes increasingly important. While groupware systems address user level communication needs and distributed computing technologies address tool level communication needs, few attempts have been made to synthesize the common needs of both. This paper presents a framework for generating agenda management systems (AMSs) from specifications of application requirements and describes how such systems address communication needs of heterogeneous agents. The framework supports a variety of application requirements and produces a customized AMS that is based on replicated distributed objects. The framework and generated AMS support evolution in several ways, allowing existing systems to be extended as requirements change. Also discussed are experiences using this approach and lessons learned, primarily from use in supporting a process execution environment and a laboratory coordination environment.", "num_citations": "1\n", "authors": ["366"]}
{"title": "ACM fellows\n", "abstract": " ACM fellows Page 1 18 March 1998/Vol. 41, No. 3 COMMUNICATIONS OF THE ACM The ACM Fellows Program was established by Council in 1993 to recognize and honor out- standing ACM members for their achievements in computer science and information technology and for their significant contributions to the mission of the ACM. The ACM Fellows serve as distinguished colleagues to whom the ACM and its members look for guidance and leadership as the world of information technology evolves. The ACM Council endorsed the establishment of a Fellows Program and provided guidance to the ACM Fellows Committee, taking the view that the program represents a concrete benefit to which any ACM Member might aspire, and provides an important source of role models for existing and prospective ACM Members. The program is managed by an ACM Fellows Committee as part of the general ACM Awards by \u2026", "num_citations": "1\n", "authors": ["366"]}
{"title": "The uses of process modeling: a framework for understanding modeling formalisms\n", "abstract": " There is wide-spread recognition of the urgent need to improve software processes in order to improve the performance of software organizations. Process models are essential in achieving understanding and visibility of processes and are important for other uses including the analysis of processes for improvement. It has been increasingly difficult to compare and evaluate the variety of process modeling formalisms that have appeared in recent years without a clear understanding of precisely for what they will be used. The contribution of this paper is to provide an understanding and a fairly comprehensive catalog of the applications of process modeling for which formalisms may be used. The primary mechanism for doing this is a guided tour of the literature on process modeling supplemented by recent industrial experience. In the paper, basic definitions concerning processes, process descriptions and process modeling are reviewed and then uses of process modeling are surveyed under the following headings: communication among process participants, construction of new processes, control of processes, process\u00b7 analysis, and process support by automation. Comments are offered on paradigms for process modeling formalisms and directions for future work to permit evolution of a discipline of process engineering are given.", "num_citations": "1\n", "authors": ["366"]}
{"title": "Software development environments research projects in the United States\n", "abstract": " Software development environments research projects in the United States | Software engineering environments ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksSoftware engineering environmentsSoftware development environments research projects in the United States chapter Software development environments research projects in the United States Share on Author: Leon J Osterweil profile image Leon Osterweil View Profile Authors Info & Affiliations Publication: Software engineering environmentsJanuary 1991 Pages 115\u2013119", "num_citations": "1\n", "authors": ["366"]}
{"title": "Prototyping a process-centered environment\n", "abstract": " This paper describes an experimental system developed and used as a vehicle for prototyping the Arcadia-1 software development environment. Prototyping is viewed as a knowledge acquisition process and is used to reduce risks in software development by gaining rapid feedback about the suitability of a production system before the system is completed. Prototyping a software development environment is particularly important due to the lack of experience with them. There is an acute need to acquire knowledge about user interaction requirements for software environments. These needs are especially important for the Arcadia project, as it is one of the first attempts to construct a process-centered environment. Our prototyping effort addresses questions about effective interaction with a process-centered environment by simulating how Arcadia-1 would interact with users in a representative range of usage scenarios. We built a prototyping system, called PRODUCER, and used it to generate a variety of prototypes simulating user interactions with Arcadia-1 process programs.Experience with PRODUCER indicates that our approach is effective at risk reduction. The prototypes greatly improved communication with our customer. They confirmed some of our design decisions but also redirected our research efforts as a result of unexpected insight. We also found that prototyping usage scenarios provides conceptual guides and design information for process programmers. Most of the benefits of our prototyping effort derive from developing and interacting with usage scenarios, so our approach is generalizable to other prototyping systems. This\u00a0\u2026", "num_citations": "1\n", "authors": ["366"]}
{"title": "Odin environment integration mechanism\n", "abstract": " A method of integrating software development tools is described. A data centered approach is taken, the software objects residing in a software object (data) repository. Odin can be thought of as an interpreter for a high level command language whose operands are the various software objects in the data repository and whose operators are the various tool fragments used to construct the software tools. Software objects can be such tools as source code, parse trees, symbol tables, flow graphs, documentation, test data, test results, and program structure representations. Discussions of previous research, Odin architecture, design, the Odin request language, the Odin specification language, and an example of Odin implementation are presented. 45 refs., 8 figs.(DWL)", "num_citations": "1\n", "authors": ["366"]}
{"title": "An extensible toolset and environment for the production of mathematical software\n", "abstract": " This is a description of the organization of the Toolpack software development environment. The Toolpack project is an attempt to promote the widespread use of modern software tools by providing a set of high-quality tools and a software environment to integrate and coordinate them. The heart of the Toolpack environment is a tree-structured file system which holds, for each program or subprogram in the software under development, a diverse collection of files which relate to it or are derived from it in some way. These might include such things as multiple versions of the source text; different \u201cviews\u201d of a version, such as its parse tree, symbol table, or flowgraph; test data sets and results of test runs; and instrumented versions, in which code has been inserted to produce traces or to check assertions. Then a software tool is implemented as a program which derives such files from one or more existing files, inserting\u00a0\u2026", "num_citations": "1\n", "authors": ["366"]}
{"title": "Toolpack\u2014An Integrated System of Tools for Mathematical Software Development\n", "abstract": " This paper describes the approach being taken in configuring a set of tool capabilities whose goal is the support of mathematical software development. The TOOLPACK tool set is being designed to support such activities as editing, testing, analysis, formatting, transformation, documentation and porting of code. Tools for realizing most of these functional capabilities already exist, yet TOOLPACK aims to do far more than simply bring them together as a collection of side-by-side individual tools. TOOLPACK seeks to merge these capabilities into a system which is smoothly integrated both internally and from a user\u2019s external point of view. The internal integration approach involves the decomposition of all tools into a more or less standard set of modular \u201ctool fragments\u201d. The external integration approach involves the creation of a command language and a set of conceptual entities which is close to the\u00a0\u2026", "num_citations": "1\n", "authors": ["366"]}
{"title": "A benchmark for evaluating the applicability of software engineering techniques to the improvement of medical processes\n", "abstract": " Problems in health care have gained prominence in recent years. To address such concerns, the software engineering and medical informatics communities have been developing a range of methodologies and tools for reasoning about medical processes. To facilitate the comparison of such methodologies and tools in terms of their applicability to health care, it would be desirable to have a set of medical examples, or benchmarks, that are easily available, described in considerable detail, and carefully characterized in terms of the real-world complexities they capture. This paper presents one such benchmark and discusses a list of desiderata that medical benchmarks can be evaluated against.", "num_citations": "1\n", "authors": ["366"]}