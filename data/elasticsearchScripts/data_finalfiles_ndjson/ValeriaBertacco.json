{"title": "GPU computing gems emerald edition\n", "abstract": " Graphics Processing Units (GPUs) are designed to be parallel - having hundreds of cores versus traditional CPUs. Increasingly, you can leverage GPU power for any computationally-intense operation - not just for graphics. If you're facing the challenge of programming systems to effectively use these massively parallel processors to achieve efficiency and performance goals, GPU Computing Gems provides a wealth of tested, proven GPU techniques. Learn from the leading researchers in concurrent programming, who have gathered their insights and experience in one volume under the guidance of NVIDIA and GPU expert Wen-mei Hwu. Covers the breadth of industry from scientific simulation and electronic design automation to audio / video processing, medical imaging, computer vision, and moreMany examples utilize NVIDIA's CUDA parallel computing architecture, the most widely-adopted GPU programming\u00a0\u2026", "num_citations": "360\n", "authors": ["1888"]}
{"title": "A highly resilient routing algorithm for fault-tolerant NoCs\n", "abstract": " Current trends in technology scaling foreshadow worsening transistor reliability as well as greater numbers of transistors in each system. The combination of these factors will soon make long-term product reliability extremely difficult in complex modern systems such as systems on a chip (SoC) and chip multiprocessor (CMP) designs, where even a single device failure can cause fatal system errors. Resiliency to device failure will be a necessary condition at future technology nodes. In this work, we present a network-on-chip (NoC) routing algorithm to boost the robustness in interconnect networks, by reconfiguring them to avoid faulty components while maintaining connectivity and correct operation. This distributed algorithm can be implemented in hardware with less than 300 gates per network router. Experimental results over a broad range of 2D-mesh and 2D-torus networks demonstrate 99.99% reliability on\u00a0\u2026", "num_citations": "334\n", "authors": ["1888"]}
{"title": "BulletProof: A defect-tolerant CMP switch architecture\n", "abstract": " As silicon technologies move into the nanometer regime, transistor reliability is expected to wane as devices become subject to extreme process variation, particle-induced transient errors, and transistor wear-out. Unless these challenges are addressed, computer vendors can expect low yields and short mean-times-to-failure. In this paper, we examine the challenges of designing complex computing systems in the presence of transient and permanent faults. We select one small aspect of a typical chip multiprocessor (CMP) system to study in detail, a single CMP router switch. To start, we develop a unified model of faults, based on the time-tested bathtub curve. Using this convenient abstraction, we analyze the reliability versus area tradeoff across a wide spectrum of CMP switch designs, ranging from unprotected designs to fully protected designs with online repair and recovery capabilities. Protection is considered\u00a0\u2026", "num_citations": "247\n", "authors": ["1888"]}
{"title": "Vicis: A reliable network for unreliable silicon\n", "abstract": " Process scaling has given designers billions of transistors to work with. As feature sizes near the atomic scale, extensive variation and wearout inevitably make margining uneconomical or impossible. The ElastIC project seeks to address this by creating a large-scale chip-multiprocessor that can self-diagnose, adapt, and heal. Creating large, flexible designs in this environment naturally lends itself to the repetitive nature of network-on-chip (NoC), but the loss of a single link or router will result in complete network failure. In this work we present Vicis, an ElastIC-style NoC that can tolerate the loss of many network components due to wearout induced hard faults. Vicis uses the inherent redundancy in the network and its routers in order to maintain correct operation while incurring a much lower area overhead than previously proposed N-modular redundancy (NMR) based solutions. Each router has a built-in-self-test\u00a0\u2026", "num_citations": "236\n", "authors": ["1888"]}
{"title": "Reliable systems on unreliable fabrics\n", "abstract": " The continued scaling of silicon fabrication technology has led to significant reliability concerns, which are quickly becoming a dominant design challenge. Design integrity is threatened by complexity challenges in the form of immense designs defying complete verification, and physical challenges such as silicon aging and soft errors, which impair correct system operation. The Gigascale Systems Research Center Resilient-System Design Team is addressing these key challenges through synergistic research thrusts, ranging from near-term reliability stress reduction techniques to methods for improving the quality of today's silicon, to longer-term technologies that can detect, recover, and repair faulty systems. These efforts are supported and complemented by an active fault-modeling research effort and a strong focus on functional-verification methodologies. The team's goal is to provide highly effective, low-cost\u00a0\u2026", "num_citations": "233\n", "authors": ["1888"]}
{"title": "The disjunctive decomposition of logic functions\n", "abstract": " We present an algorithm for extracting a disjunctive decomposition from the BDD representation of F. The output of the algorithm is a multiple-level netlist exposing the hierarchical decomposition structure of the function. The algorithm has theoretical quadratic complexity in the size of the input BDD. Experimentally, we were able to decompose most synthesis benchmarks in less than one second of CPU time, and to report on the decomposability of several complex ISCAS combinational benchmarks. We found the final netlist to be often close to the output of more complex dedicated optimization tools.", "num_citations": "205\n", "authors": ["1888"]}
{"title": "Opportunities and challenges for better than worst-case design\n", "abstract": " The progressive trend of fabrication technologies towards the nanometer regime has created a number of new physical design challenges for computer architects. Design complexity, uncertainty in environmental and fabrication conditions, and single-event upsets all conspire to compromise system correctness and reliability. Recently, researchers have begun to advocate a new design strategy called Better Than Worst-Case design that couples a complex core component with a simple reliable checker mechanism. By delegating the responsibility for correctness and reliability of the design to the checker, it becomes possible to build provably correct designs that effectively address the challenges of deep submicron design. In this paper, we present the concepts of Better Than Worst-Case design and high light two exemplary designs: the DIVA checker and Razor logic. We show how this approach to system\u00a0\u2026", "num_citations": "201\n", "authors": ["1888"]}
{"title": "Ultra low-cost defect protection for microprocessor pipelines\n", "abstract": " The sustained push toward smaller and smaller technology sizes has reached a point where device reliability has moved to the forefront of concerns for next-generation designs. Silicon failure mechanisms, such as transistor wearout and manufacturing defects, are a growing challenge that threatens the yield and product lifetime of future systems. In this paper we introduce the BulletProof pipeline, the first ultra low-cost mechanism to protect a microprocessor pipeline and on-chip memory system from silicon defects. To achieve this goal we combine area-frugal on-line testing techniques and system-level checkpointing to provide the same guarantees of reliability found in traditional solutions, but at much lower cost. Our approach utilizes a microarchitectural checkpointing mechanism which creates coarse-grained epochs of execution, during which distributed on-line built in self-test (BIST) mechanisms validate the\u00a0\u2026", "num_citations": "176\n", "authors": ["1888"]}
{"title": "Software-based online detection of hardware defects mechanisms, architectural support, and evaluation\n", "abstract": " As silicon process technology scales deeper into the nanometer regime, hardware defects are becoming more common. Such defects are bound to hinder the correct operation of future processor systems, unless new online techniques become available to detect and to tolerate them while preserving the integrity of software applications running on the system. This paper proposes a new, software-based, defect detection and diagnosis technique. We introduce a novel set of instructions, called access-control extension (ACE), that can access and control the microprocessor's internal state. Special firmware periodically suspends microprocessor execution and uses the ACE instructions to run directed tests on the hardware. When a hardware defect is present, these tests can diagnose and locate it, and then activate system repair through resource reconfiguration. The software nature of our framework makes it flexible\u00a0\u2026", "num_citations": "136\n", "authors": ["1888"]}
{"title": "Fault-based attack of RSA authentication\n", "abstract": " For any computing system to be secure, both hardware and software have to be trusted. If the hardware layer in a secure system is compromised, not only it would be possible to extract secret information about the software, but it would also be extremely hard for the software to detect that an attack is underway. In this work we detail a complete end-to-end fault-attack on a microprocessor system and practically demonstrate how hardware vulnerabilities can be exploited to target secure systems. We developed a theoretical attack to the RSA signature algorithm, and we realized it in practice against an FPGA implementation of the system under attack. To perpetrate the attack, we inject transient faults in the target machine by regulating the voltage supply of the system. Thus, our attack does not require access to the victim system's internal components, but simply proximity to it. The paper makes three important\u00a0\u2026", "num_citations": "127\n", "authors": ["1888"]}
{"title": "Event-driven gate-level simulation with GP-GPUs\n", "abstract": " Logic simulation is a critical component of the design tool flow in modern hardware development efforts. It is used widely--from high-level descriptions down to gate-level ones--to validate several aspects of the design, particularly functional correctness. Despite development houses investing vast resources in the simulation task, particularly at the gate-level, it is still far from achieving the performance demands required to validate complex modern designs.", "num_citations": "123\n", "authors": ["1888"]}
{"title": "Ariadne: Agnostic reconfiguration in a disconnected network environment\n", "abstract": " Extreme transistor technology scaling is causing increasing concerns in device reliability: the expected lifetime of individual transistors in complex chips is quickly decreasing, and the problem is expected to worsen at future technology nodes. With complex designs increasingly relying on Networks-on-Chip (NoCs) for on-chip data transfers, a NoC must continue to operate even in the face of many transistor failures. Specifically, it must be able to reconfigure and reroute packets around faults to enable continued operation, i.e., generate new routing paths to replace the old ones upon a failure. In addition to these reliability requirements, NoCs must maintain low latency and high throughput at very low area budget. In this work, we propose a distributed reconfiguration solution named Ariadne, targeting large, aggressively scaled, unreliable NoCs. Ariadne utilizes up*/down* for fast routing at high bandwidth, and upon\u00a0\u2026", "num_citations": "121\n", "authors": ["1888"]}
{"title": "GPU computing gems emerald edition\n", "abstract": " GPU Computing Gems Emerald Edition offers practical techniques in parallel computing using graphics processing units (GPUs) to enhance scientific research. The first volume in Morgan Kaufmann's Applications of GPU Computing Series, this book offers the latest insights and research in computer vision, electronic design automation, and emerging data-intensive applications. It also covers life sciences, medical imaging, ray tracing and rendering, scientific simulation, signal and audio processing, statistical modeling, video and image processing. This book is intended to help those who are facing the challenge of programming systems to effectively use GPUs to achieve efficiency and performance goals. It offers developers a window into diverse application areas, and the opportunity to gain insights from others' algorithm work that they may apply to their own projects. Readers will learn from the leading researchers in parallel programming, who have gathered their solutions and experience in one volume under the guidance of expert area editors. Each chapter is written to be accessible to researchers from other domains, allowing knowledge to cross-pollinate across the GPU spectrum. Many examples leverage NVIDIA's CUDA parallel computing architecture, the most widely-adopted massively parallel programming solution. The insights and ideas as well as practical hands-on skills in the book can be immediately put to use. Computer programmers, software engineers, hardware engineers, and computer science students will find this volume a helpful resource. For useful source codes discussed throughout the book, the editors invite readers\u00a0\u2026", "num_citations": "119\n", "authors": ["1888"]}
{"title": "A reliable routing architecture and algorithm for NoCs\n", "abstract": " Aggressive transistor scaling continues to drive increasingly complex digital designs. The large number of transistors available today enables the development of chip multiprocessors that include many cores on one die communicating through an on-chip interconnect. As the number of cores increases, scalable communication platforms, such as networks-on-chip (NoCs), have become more popular. However, as the sole communication medium, these interconnects are a single point of failure so that any permanent fault in the NoC can cause the entire system to fail. Compounding the problem, transistors have become increasingly susceptible to wear-out related failures as their critical dimensions shrink. As a result, the on-chip network has become a critically exposed unit that must be protected. To this end, we present Vicis, a fault-tolerant architecture and companion routing protocol that is robust to a large number\u00a0\u2026", "num_citations": "116\n", "authors": ["1888"]}
{"title": "Fixing Design Errors With Counterexamples and Resynthesis\n", "abstract": " In this work we propose a new error-correction framework, called CoRe, which uses counterexamples, or bug traces, generated in verification to automatically correct errors in digital designs. CoRe is powered by two innovative resynthesis techniques, goal-directed search (GDS) and entropy-guided search (EGS), which modify the functionality of internal circuit's nodes to match the desired specification. We evaluate our solution to designs and errors arising during combinational equivalence-checking, as well as simulation-based verification of digital systems. Compared with previously proposed techniques, CoRe is more powerful in that: (1) it can fix a broader range of error types because it does not rely on specific error models; (2) it derives the correct functionality from simulation vectors, hence not requiring golden netlists; and (3) it can be applied to a range of verification flows, including formal and simulation\u00a0\u2026", "num_citations": "106\n", "authors": ["1888"]}
{"title": "Automating Postsilicon Debugging and Repair\n", "abstract": " Modern IC designs have reached unparalleled levels of complexity, resulting in more and more bugs discovered after design tape-out However, so far only very few EDA tools for post-silicon debugging have been reported in the literature. In this work we develop a methodology and new algorithms to automate this debugging process. Key innovations in our technique include support for the physical constraints specific to post-silicon debugging and the ability to repair functional errors through subtle modifications of an existing layout. In addition, our proposed post-silicon debugging methodology (FogClear) can repair some electrical errors while preserving functional correctness. Thus, by automating this traditionally manual debugging process, our contributions promise to reduce engineers' debugging effort. As our empirical results show, we can automatically repair more than 70% of our benchmark designs.", "num_citations": "103\n", "authors": ["1888"]}
{"title": "Power-aware NoCs through routing and topology reconfiguration\n", "abstract": " With the advent of multicore processors and system-on-chip designs, intra-chip communication demands have exacerbated, leading to a growing adoption of scalable networks-on-chip (NoCs) as the interconnect fabric. Today, conventional NoC designs may consume up to 30% of the entire chip's power budget, in large part due to leakage power. In this work, we address this issue by proposing Panthre: our solution deploys power-gating to provide long intervals of uninterrupted sleep to selected units. Packets that would normally use power-gated components are steered away via topology and routing reconfiguration, while Panthre provides low-latency alternate paths to their destinations. The routing reconfiguration operates in a distributed fashion and guarantees that deadlock-free routes are available at all times. At runtime, Panthre adapts to the application's communication patterns by updating its power-gating\u00a0\u2026", "num_citations": "100\n", "authors": ["1888"]}
{"title": "Decision diagrams and pass transistor logic synthesis\n", "abstract": " Since the relative importance of interconnections increases as feature size decreases, standardcell based synthesis becomes less e ective when deep-submicron technologies become available. Intra-cell connectivity can be decreased by the use of macro-cells. In this work we present methods for the automatic generation of macro-cells using pass transistors and domino logic. The synthesis of these cells is based on BDD and ZBDD representations of the logic functions. We address speci c problems associated with the BDD approach level degradation, long paths and the ZBDD approach sneak paths, charge sharing, long paths. We compare performance of the macro-cells approach versus the conventional standard-cell approach based on accurate electrical simulation. This shows that the macro-cells perform well up to a certain complexity of the logic function. Functions of high complexity must be decomposed into smaller logic blocks that can directly be mapped to macro-cells.", "num_citations": "99\n", "authors": ["1888"]}
{"title": "Regaining lost cycles with HotCalls: A fast interface for SGX secure enclaves\n", "abstract": " Intel's SGX secure execution technology allows running computations on secret data using untrusted servers. While recent work showed how to port applications and large-scale computations to run under SGX, the performance implications of using the technology remains an open question. We present the first comprehensive quantitative study to evaluate the performance of SGX. We show that straightforward use of SGX library primitives for calling functions add between 8,200 - 17,000 cycles overhead, compared to 150 cycles of a typical system call. We quantify the performance impact of these library calls and show that in applications with high system calls frequency, such as memcached, openVPN, and lighttpd, which all have high bandwidth network requirements, the performance degradation may be as high as 79%. We investigate the sources of this performance degradation by leveraging a new set of\u00a0\u2026", "num_citations": "93\n", "authors": ["1888"]}
{"title": "High-performance gate-level simulation with GP-GPUs\n", "abstract": " Publisher SummaryThis chapter investigates how the parallelism available in the problem structure can be mapped to that of the execution hardware of GPGPUs. The majority of verification methodologies in industry rely heavily on the use of logic simulation tools. These tools simulate the design's functional behavior at different levels of abstraction, ranging from high-level behavioral to low-level structural gate-level descriptions. A typical simulation will rely on a model of the design, stimulating it with inputs and then checking the outputs for correctness. In a typical digital design flow, a system is first described in a high-level behavioral fashion with a hardware description language (HDL), then it is automatically synthesized to a netlist, consisting of structural logic elements such as logic gates and flip-flops. To ensure that the gate-level design provides the same functionality as the behavioral design, the former must\u00a0\u2026", "num_citations": "91\n", "authors": ["1888"]}
{"title": "Simulation-based signal selection for state restoration in silicon debug\n", "abstract": " Post-silicon validation has become a crucial part of modern integrated circuit design to capture and eliminate functional bugs that escape pre-silicon verification. The most critical roadblock in post-silicon validation is the limited observability of internal signals of a design, since this aspect hinders the ability to diagnose detected bugs. A solution to address this issue leverage trace buffers: these are register buffers embedded into the design with the goal of recording the value of a small number of state elements, over a time interval, triggered by a user-specified event. Due to the trace buffer's area overhead, only a very small fraction of signals can be traced. Thus, the selection of which signals to trace is of paramount importance in post-silicon debugging and diagnosis. Ideally, we would like to select signals enabling the maximum amount of reconstruction of internal signal values. Several signal selection algorithms\u00a0\u2026", "num_citations": "87\n", "authors": ["1888"]}
{"title": "CrashTest: A fast high-fidelity FPGA-based resiliency analysis framework\n", "abstract": " Extreme scaling practices in silicon technology are quickly leading to integrated circuit components with limited reliability, where phenomena such as early-transistor failures, gate-oxide wearout, and transient faults are becoming increasingly common. In order to overcome these issues and develop robust design techniques for large-market silicon ICs, it is necessary to rely on accurate failure analysis frameworks which enable design houses to faithfully evaluate both the impact of a wide range of potential failures and the ability of candidate reliable mechanisms to overcome them. Unfortunately, while failure rates are already growing beyond economically viable limits, no fault analysis framework is yet available that is both accurate and can operate on a complex integrated system. To address this void, we present CrashTest, a fast, high-fidelity and flexible resiliency analysis system. Given a hardware description\u00a0\u2026", "num_citations": "78\n", "authors": ["1888"]}
{"title": "StressTest: an automatic approach to test generation via activity monitors\n", "abstract": " The challenge of verifying a modern microprocessor design is an overwhelming one: Increasingly complex micro-architectures combined with heavy time-to-market pressure have forced microprocessor vendors to employ immense verification teams in the hope of finding the most critical bugs in a timely manner. Unfortunately, too often size doesn't seem to matter for verification teams, as design schedules continue to slip and microprocessors find their way to the marketplace with design errors. In this paper, we describe a simulationbased random test generation tool, called StressTest, that provides assistance in locating hard-to-find corner-case design bugs and performance problems. StressTest is based on a Markov-model-driven random instruction generator with activity monitors. The model is generated from the userspecified template programs and is used to generate the instructions sent to the design under\u00a0\u2026", "num_citations": "75\n", "authors": ["1888"]}
{"title": "Automatic error diagnosis and correction for RTL designs\n", "abstract": " Recent improvements in design verification strive to automate the error-detection process and greatly enhance engineers' ability to detect functional errors. However, the process of diagnosing the cause of these errors and fixing them remains difficult and requires significant ad-hoc manual effort. Our work proposes improvements to this aspect of verification by presenting novel constructs and algorithms to automate the error-repair process at the Register-Transfer Level (RTL), where most development occurs. Our contributions include a new RTL error model and scalable error-repair algorithms. Empirical results show that our solution can diagnose and correct errors in just a handful of minutes even for complex designs o/up to several thousand lines of RTL code in minutes. This demonstrates the superior scalability and efficiency of our approach compared to previous work.", "num_citations": "74\n", "authors": ["1888"]}
{"title": "A distributed and topology-agnostic approach for on-line NoC testing\n", "abstract": " A new distributed on-line test mechanism for NoCs is proposed which scales to large-scale networks with general topologies and routing algorithms. Each router and its links are tested using neighbors in different phases. Only the router under test is in test mode and all other parts of the NoC are in functional mode.", "num_citations": "73\n", "authors": ["1888"]}
{"title": "Reversi: Post-silicon validation system for modern microprocessors\n", "abstract": " Verification remains an integral and crucial phase of todaypsilas microprocessor design and manufacturing process. Unfortunately, with soaring design complexities and decreasing time-to-market windows, todaypsilas verification approaches are incapable of fully validating a microprocessor before its release to the public. Increasingly, post-silicon validation is deployed to detect complex functional bugs in addition to exposing electrical and manufacturing defects. This is due to the significantly higher execution performance offered by post-silicon methods, compared to pre-silicon approaches. Validation in the post-silicon domain is predominantly carried out by executing constrained-random test instruction sequences directly on a hardware prototype. However, to identify errors, the state obtained from executing tests directly in hardware must be compared to the one produced by an architectural simulation of the\u00a0\u2026", "num_citations": "72\n", "authors": ["1888"]}
{"title": "At-speed distributed functional testing to detect logic and delay faults in NoCs\n", "abstract": " In this work, we propose a distributed functional test mechanism for NoCs which scales to large-scale networks with general topologies and routing algorithms. Each router and its links are tested using neighbors in different phases. The router under test is in test mode while all other parts of the NoC are operational. We use triple module redundancy (TMR) for the robustness of all testing components that are added into the switch. Experimental results show that our functional test approach can detect stuck-at, short and delay faults in the routers and links. Our approach achieves 100 percent stuck-at fault coverage for the data path and 85 percent for the control paths including routing logic, FIFO's control path, and the arbiter of a 5 \u00d7 5 router. We also show that our approach is able to detect delay faults in critical control and data paths. Synthesis results show that the area overhead of our test components with TMR\u00a0\u2026", "num_citations": "70\n", "authors": ["1888"]}
{"title": "Microprocessor verification via feedback-adjusted Markov models\n", "abstract": " The challenge of verifying a modern microprocessor design is an overwhelming one: Increasingly complex microarchitectures combined with heavy time-to-market pressure have forced microprocessor vendors to employ immense verification teams in the hope of finding the most critical bugs in a timely manner. Unfortunately, too often, size does not seem to matter in verification, as design schedules continue to slip and microprocessors find their way to the marketplace with design errors. In this paper, we describe a novel closed-loop simulation-based approach to hardware verification and present a tool called StressTest that uses our methods to locate hard-to-find corner-case design bugs and performance problems. StressTest is based on a Markov-model-driven random instruction generator with activity monitors. The model is generated from the user-specified template files and is used to generate the\u00a0\u2026", "num_citations": "67\n", "authors": ["1888"]}
{"title": "Distance-guided hybrid verification with GUIDO\n", "abstract": " Constrained random simulation is a widespread technique used to perform functional verification on complex digital designs, because it can generate simulation vectors at a very high rate. However, the generation of high-coverage tests remains a major challenge even in light of this high performance. In this paper we present Guido, a hybrid verification software that uses formal verification techniques to guide the simulation towards a verification goal. Guido is novel in that 1) it guides the simulation by means of a distance function derived from the circuit structure, and 2) it has a trace sequence controller that monitors and controls the direction of the simulation by striking a balance between random chance and controlled hill-climbing. We present experimental results indicating that Guido can tackle complex designs, including a picoJava microprocessor, and reach a verification goal in far fewer simulation cycles than\u00a0\u2026", "num_citations": "67\n", "authors": ["1888"]}
{"title": "Dacota: Post-silicon validation of the memory subsystem in multi-core designs\n", "abstract": " The number of functional errors escaping design verification and being released into final silicon is growing, due to the increasing complexity and shrinking production schedules of modern processor designs. Recent trends towards chip multiprocessors (CMPs) are exacerbating the problem because of their complex and sometimes non-deterministic memory subsystems, prone to subtle but devastating bugs. This deteriorating situation calls for high-efficiency, high-coverage results in functional validation, results that are be achieved by leveraging the performance of post-silicon validation, that is, those verification tasks that are executed directly on prototype hardware. The orders-of-magnitude faster testing in post-silicon enables designers to achieve much higher coverage before customer release, but only if the limitations of this technology in diagnosis and internal node observability could be overcome. In this\u00a0\u2026", "num_citations": "63\n", "authors": ["1888"]}
{"title": "Shielding against design flaws with field repairable control logic\n", "abstract": " Correctness is a paramount attribute of any microprocessor design; however, without novel technologies to tame the increasing complexity of design verification, the amount of bugs that escape into silicon will only grow in the future. In this paper, we propose a novel hardware patching mechanism that can detect design errors which escaped the verification process, and can correct them directly in the field. We accomplish this goal through a simple field-programmable state matcher, which can identify erroneous configurations in the processor's control state and switch the processor into formally-verified degraded performance mode, once a\" match\" occurs. When the instructions exposing the design flaw are committed, the processor is switched back to normal mode. We show that our approach can detect and correct infrequently-occurring errors with almost no performance impact and has approximately 2% area\u00a0\u2026", "num_citations": "63\n", "authors": ["1888"]}
{"title": "Simulation-based bug trace minimization with BMC-based refinement\n", "abstract": " Finding the cause of a bug can be one of the most time-consuming activities in design verification. This is particularly true in the case of bugs discovered in the context of a random-simulation-based methodology, where bug traces, or counterexamples, may be several hundred thousand cycles long. In this paper, BUg TRAce MINimization (Butramin), which is a bug trace minimizer, is proposed. Butramin considers a bug trace produced by a random simulator or semiformal verification software and produces an equivalent trace of shorter length. Butramin applies a range of minimization techniques, deploying both simulation-based and formal methods, with the objective of producing highly reduced traces that still expose the original bug. Butramin was evaluated on a range of designs, including the publicly available picoJava microprocessor, and bug traces up to one million cycles long. Experiments show that in most\u00a0\u2026", "num_citations": "61\n", "authors": ["1888"]}
{"title": "Machine learning-based anomaly detection for post-silicon bug diagnosis\n", "abstract": " The exponentially growing complexity of modern processors intensifies verification challenges. Traditional pre-silicon verification covers less and less of the design space, resulting in increasing post-silicon validation effort. A critical challenge is the manual debugging of intermittent failures on prototype chips, where multiple executions of a same test do not yield a consistent outcome. We leverage the power of machine learning to support automatic diagnosis of these difficult, inconsistent bugs. During post-silicon validation, lightweight hardware logs a compact measurement of observed signal activity over multiple executions of a same test: some may pass, somemay fail. Our novel algorithm applies anomaly detection techniques similar to those used to detect credit card fraud to identify the approximate cycle of a bug's occurrence and a set of candidate root-cause signals. Compared against other state-of-the-art\u00a0\u2026", "num_citations": "56\n", "authors": ["1888"]}
{"title": "Comprehensive online defect diagnosis in on-chip networks\n", "abstract": " We propose a comprehensive yet low-cost solution for online detection and diagnosis of permanent faults in on-chip networks. Using error syndrome collection and packet/flit-counting techniques, high-resolution defect diagnosis is feasible in both datapath and control logic of the on-chip network without injecting any test traffic or incurring significant performance overhead.", "num_citations": "56\n", "authors": ["1888"]}
{"title": "Bridging pre-silicon verification and post-silicon validation\n", "abstract": " Post-silicon validation is a necessary step in a design's verification process. Pre-silicon techniques such as simulation and emulation are limited in scope and volume as compared to what can be achieved on the silicon itself. Some parts of the verification, such as full-system functional verification, cannot be practically covered with current pre-silicon technologies. This panel brings together experts from industry, academia, and EDA to review the differences and similarities between pre- and post-silicon, discuss how the fundamental aspects of verification are affected by these differences, and explore how the gaps between the two worlds can be bridged.", "num_citations": "55\n", "authors": ["1888"]}
{"title": "uDIREC: unified diagnosis and reconfiguration for frugal bypass of NoC faults\n", "abstract": " As silicon continues to scale, transistor reliability is becoming a major concern. At the same time, increasing transistor counts are causing a rapid shift towards large chip multi-processors (CMP) and system-on-chip (SoC) designs, comprising several cores and IPs communicating via a network-on-chip (NoC). As the sole medium of on-chip communication, a NoC should gracefully tolerate many permanent faults.", "num_citations": "50\n", "authors": ["1888"]}
{"title": "Node mergers in the presence of don't cares\n", "abstract": " SAT sweeping is the process of merging two or more functionally equivalent nodes in a circuit by selecting one of them to represent all the other equivalent nodes. This provides significant advantages in synthesis by reducing circuit size and provides additional flexibility in technology mapping, which could be crucial in post-synthesis optimizations. Furthermore, it is also critical in verification because it can reduce the complexity of the netlist to be analyzed in equivalence checking. Most algorithms available so far for this goal do not exploit observability don't cares (ODCs) for node merging since nodes equivalent up to ODCs do not form an equivalence relation. Although a few recently proposed solutions can exploit ODCs by overcoming this limitation, they constrain their analysis to just a few levels of surrounding logic to avoid prohibitive runtime. We develop an ODC-based node merging algorithm that performs\u00a0\u2026", "num_citations": "49\n", "authors": ["1888"]}
{"title": "A flexible software-based framework for online detection of hardware defects\n", "abstract": " This work proposes a new, software-based, defect detection and diagnosis technique. We introduce a novel set of instructions, called access-control extensions (ACE), that can access and control the microprocessor's internal state. Special firmware periodically suspends microprocessor execution and uses the ACE instructions to run directed tests on the hardware. When a hardware defect is present, these tests can diagnose and locate it, and then activate system repair through resource reconfiguration. The software nature of our framework makes it flexible: testing techniques can be modified/upgraded in the field to trade-off performance with reliability without requiring any change to the hardware. We describe and evaluate different execution models for using the ACE framework. We also describe how the proposed ACE framework can be extended and utilized to improve the quality of post-silicon debugging and\u00a0\u2026", "num_citations": "48\n", "authors": ["1888"]}
{"title": "SAGA: SystemC acceleration on GPU architectures\n", "abstract": " SystemC is a widespread language for HW/SW system simulation and design exploration, and thus a key development platform in embedded system design. However, the growing complexity of SoC designs is having an impact on simulation performance, leading to limited SoC exploration potential, which in turns affects development and verification schedules and time-to-market for new designs. Previous efforts have attempted to parallelize SystemC simulation, targeting both multiprocessors and GPUs. However, for practical designs, those approaches fall far short of satisfactory performance. This paper proposes SAGA, a novel simulation approach that fully exploits the intrinsic parallelism of RTL SystemC descriptions, targeting GPU platforms. By limiting synchronization events with ad-hoc static scheduling and separate independent dataflows, we shows that we can simulate complex SystemC descriptions up to\u00a0\u2026", "num_citations": "47\n", "authors": ["1888"]}
{"title": "ReliNoC: A reliable network for priority-based on-chip communication\n", "abstract": " The reliability of networks-on-chip (NoC) is threatened by low yield and device wearout in aggressively scaled technology nodes. We propose ReliNoC, a network-on-chip architecture which can withstand failures, while maintaining not only basic connectivity, but also quality-of-service support based on packet priorities. Our network leverages a dual physical channel switch architecture which removes the control overhead of virtual channels (VCs) and utilizes the inherent redundancy within the 2-channel switch to provide spares for faulty elements. Experimental results show that ReliNoC provides 1.5 to 3 times better network physical connectivity in presence of several faults, and reduces the latency of both high and low priority traffic by 30 to 50%, compared to a traditional VC architecture. Moreover, it can tolerate up to 50 faults within an 8\u00d78 mesh at only 10 and 40% latency overhead on control and data packets\u00a0\u2026", "num_citations": "46\n", "authors": ["1888"]}
{"title": "Post-silicon verification for cache coherence\n", "abstract": " Modern processor designs are extremely complex and difficult to validate during development, causing a growing portion of the verification effort to shift to post-silicon, after the first few hardware prototypes become available. Extremely slow simulation speeds during pre-silicon verification result in functional errors escaping into silicon, a problem that is further exacerbated by the growing complexity of the memory subsystem in multi-core platforms. In this work we present CoSMa, a novel technology offering high coverage functional post-silicon validation of cache coherence protocols in multi-core systems. It enables the detection and diagnosis of functional errors in the memory subsystem by recording at runtime a compact encoding of the operations occurring at each cache line and checking their correctness at regular intervals. We leverage the systempsilas existing memory resources to store the required activity\u00a0\u2026", "num_citations": "44\n", "authors": ["1888"]}
{"title": "Crashtest'ing swat: Accurate, gate-level evaluation of symptom-based resiliency solutions\n", "abstract": " Current technology scaling is leading to increasingly fragile components, making hardware reliability a primary design consideration. Recently researchers have proposed low-cost reliability solutions that detect hardware faults through software-level symptom monitoring. SWAT (SoftWare Anomaly Treatment), one such solution, demonstrated with microarchitecture-level simulations that symptom-based solutions can provide high fault coverage and a low Silent Data Corruption (SDC) rate. However, more accurate evaluations are needed to validate such solutions for hardware faults in real-world processor designs. In this paper, we evaluate SWAT's symptom-based detectors on gate-level faults using an FPGA-based, full-system prototype. With this platform, we performed a gate-level accurate fault injection campaign of 51,630 fault injections in the OpenSPARC T1 core logic across five SPECInt 2000 benchmarks\u00a0\u2026", "num_citations": "39\n", "authors": ["1888"]}
{"title": "Low-cost protection for SER upsets and silicon defects\n", "abstract": " Extreme transistor scaling trends in silicon technology are soon to reach a point where manufactured systems will suffer from limited device reliability and severely reduced life-time, due to early transistor failures, gate oxide wear-out, manufacturing defects, and radiation-induced soft errors (SER). In this paper we present a low-cost technique to harden a microprocessor pipeline and caches against these reliability threats. Our approach utilizes online built-in self-test (BIST) and microarchitectural checkpointing to detect, diagnose and recover the computation impaired by silicon defects or SER events. The approach works by periodically testing the processor to determine if the system is broken. If so, we reconfigure the processor to avoid using the broken component. A similar mechanism is used to detect SER faults, with the difference that recovery is implemented by re-execution. By utilizing low-cost techniques to\u00a0\u2026", "num_citations": "36\n", "authors": ["1888"]}
{"title": "Engineering trust with semantic guardians\n", "abstract": " The ability to guarantee the functional correctness of digital integrated circuits and, in particular, complex microprocessors, is a key task in the production of secure and trusted systems. Unfortunately, this goal remains today an unfulfilled challenge, as even the most straightforward practical designs are released with latent bugs. Patching techniques can repair some of these escaped bugs, however, they often incur a performance overhead, and most importantly, they can only be deployed after an escaped bug has been exposed at the customer site. In this paper we present a novel approach to guaranteeing correct system operation by deploying a semantic guardian component. The semantic guardian is an additional control logic block which is included in the design, and can switch the microprocessor's mode of operation from its normal, high-performance but error-prone mode, to a secure, formally verified safe\u00a0\u2026", "num_citations": "34\n", "authors": ["1888"]}
{"title": "Deployment of better than worst-case design: Solutions and needs\n", "abstract": " The advent of nanometer feature sizes in silicon fabrication has triggered a number of new design challenges for computer designers. These challenges include design complexity and operation in the presence of environmental and device uncertainty. To make things worse, these new challenges add to the many challenges that designers already face in order to scale system performance while meeting power and reliability budgets. Current design objectives are being met by applying even more engineers and increasing overall design times, an unsustainable trend. This paper overviews a novel design strategy, called better than worst-case design, that addresses these challenges through a methodology based on separating the concerns of performance and reliability by coupling complex design components with simple reliable checker mechanisms. We present the key aspects of better than worst-case design\u00a0\u2026", "num_citations": "33\n", "authors": ["1888"]}
{"title": "Morpheus: A vulnerability-tolerant secure architecture based on ensembles of moving target defenses with churn\n", "abstract": " Attacks often succeed by abusing the gap between program and machine-level semantics--for example, by locating a sensitive pointer, exploiting a bug to overwrite this sensitive data, and hijacking the victim program's execution. In this work, we take secure system design on the offensive by continuously obfuscating information that attackers need but normal programs do not use, such as representation of code and pointers or the exact location of code and data. Our secure hardware architecture, Morpheus, combines two powerful protections: ensembles of moving target defenses and churn. Ensembles of moving target defenses randomize key program values (eg, relocating pointers and encrypting code and pointers) which forces attackers to extensively probe the system prior to an attack. To ensure attack probes fail, the architecture incorporates churn to transparently re-randomize program values underneath\u00a0\u2026", "num_citations": "32\n", "authors": ["1888"]}
{"title": "Post-silicon bug diagnosis with inconsistent executions\n", "abstract": " The complexity of modern chips intensifies verification challenges, and an increasing share of this verification effort is shouldered by post-silicon validation. Focusing on the first silicon prototypes, post-silicon validation poses critical new challenges such as intermittent failures, where multiple executions of a same test do not yield a consistent outcome. These are often due to on-chip asynchronous events and electrical effects, leading to extremely time-consuming, if not unachievable, bug diagnosis and debugging processes. In this work, we propose a methodology called BPS (Bug Positioning System) to support the automatic diagnosis of these difficult bugs. During post-silicon validation, lightweight BPS hardware logs a compact encoding of observed signal activity over multiple executions of the same test: some passing, some failing. Leveraging a novel post-analysis algorithm, BPS uses the logged activity to\u00a0\u2026", "num_citations": "32\n", "authors": ["1888"]}
{"title": "Brisk and limited-impact NoC routing reconfiguration\n", "abstract": " The expected low reliability of the silicon substrate at upcoming technology nodes presents a key challenge for digital system designers. Networks-on-chip (NoCs) are especially concerning because they are often the only communication infrastructure for the chips in which they are deployed. Recently, routing reconfiguration solutions have been proposed to address this problem. However, they come at a high silicon cost, and often require suspending the normal network activity while executing a centralized, resource-hungry reconfiguration algorithm. This paper proposes a novel, fast and minimalistic routing reconfiguration algorithm, called BLINC. BLINC utilizes pre-computed routing metadata to quickly evaluate localized detours upon each fault manifestation. We showcase the efficacy of our algorithm by deploying it in a novel NoC fault detection and reconfiguration solution, where BLINC enables uninterrupted\u00a0\u2026", "num_citations": "31\n", "authors": ["1888"]}
{"title": "Reap what you sow: Spare cells for post-silicon metal fix\n", "abstract": " Post-silicon validation has recently become a major bottleneck in IC design. Several high profile IC designs have been taped-out with latent bugs, and forced the manufacturers to resort to additional design revisions. Such changes can be applied through metal fix; however, this is impractical without carefully pre-placed spare cells. In this work we perform the first comprehensive analysis of the issues related to spare-cell insertion, including the types of spare cells that should be used as well as their placement. In addition, we propose a new technique to measure the heterogeneity among signals and use it to determine spare-cell density. Finally, we integrate our findings into a novel multi-faceted approach that calculates regional demand for spare cells, identifies the most appropriate cell types, and places such cells into the layout. Our approach enables the use of metal fix at a much smaller delay cost, with a\u00a0\u2026", "num_citations": "31\n", "authors": ["1888"]}
{"title": "Microarchitectural power modeling techniques for deep sub-micron microprocessors\n", "abstract": " The need to perform early design studies that combine architectural simulation with power estimation has become critical as power has become a design constraint whose importance has moved to the fore. To satisfy this demand several microarchitectural power simulators have been developed around SimpleScalar, a widely used microarchitectural performance simulator. They have proven to be very useful at providing insights into power/performance trade-offs. However, they are neither parameterized nor technology scalable. In this paper, we propose more accurate parameterized power modeling techniques reflecting the actual technology parameters as well as input switching-events for memory and execution units. Compared to HSPICE, the proposed techniques show 93% and 91% accuracies for those blocks, but with a much faster simulation time. We also propose a more realistic power modeling\u00a0\u2026", "num_citations": "31\n", "authors": ["1888"]}
{"title": "Cycle-based symbolic simulation of gate-level synchronous circuits\n", "abstract": " Symbolic methods are often considered the state-of-the-art technique for validating digital circuits. Due to their complexity and unpredictable run-time behavior, however, their potential is currently limited to small-to-medium circuits. Logic simulation privileges capacity, it is nicely scalable, flexible, and it has a predictable run-time behavior. For this reason, it is the common choice for validating large circuits. Simulation, however, typically visits only a small fraction of the state space: The discovery of bugs heavily relies on the expertise of the designer of the test stimuli.In this paper we consider a symbolic simulation approach to the validation problem. Our objective is to trade-off between formal and numerical methods in order to simulate a circuit with a \u201cvery large number\u201d of input combinations and sequences in parallel. We demonstrate larger capacity with respect to symbolic techniques and better efficiency with\u00a0\u2026", "num_citations": "31\n", "authors": ["1888"]}
{"title": "Post-Silicon and Runtime Verification for Modern Processors\n", "abstract": " The purpose of this book is to survey the state of the art and evolving directions in post-silicon and runtime verification. The authors start by giving an overview of the state of the art in verification, particularly current post-silicon methodologies in use in the industry, both for the domain of processor pipeline design and for memory subsystems. They then dive into the presentation of several new post-silicon verification solutions aimed at boosting the verification coverage of modern processors, dedicating several chapters to this topic. The presentation of runtime verification solutions follows a similar approach. This is an area of processor design that is still in its early stages of exploration and that holds the promise of accomplishing the ultimate goal of achieving complete correctness guarantees for microprocessor-based computation. The authors conclude the book with a look towards the future of late-stage verification and its growing role in the processor life-cycle.", "num_citations": "30\n", "authors": ["1888"]}
{"title": "Scalable Hardware Verification with Symbolic Simulation\n", "abstract": " Scalable Hardware Verification with Symbolic Simulation presents recent advancements in symbolic simulation-based solutions which radically improve scalability. It overviews current verification techniques, both based on logic simulation and formal verification methods, and unveils the inner workings of symbolic simulation. The core of this book focuses on new techniques that narrow the performance gap between the complexity of digital systems and the limited ability to verify them. In particular, it covers a range of solutions that exploit approximation and parametrization methods, including quasi-symbolic simulation, cycle-based symbolic simulation, and parameterizations based on disjoint-support decompositions. In structuring this book, the author\u2019s hope was to provide interesting reading for a broad range of design automation readers. The first two chapters provide an overview of digital systems design and, in particular, verification. Chapter 3 reviews mainstream symbolic techniques in formal verification, dedicating most of its focus to symbolic simulation. The fourth chapter covers the necessary principles of parametric forms and disjoint-support decompositions. Chapters 5 and 6 focus on recent symbolic simulation techniques, and the final chapter addresses key topics needing further research. Scalable Hardware Verification with Symbolic Simulation is for verification engineers and researchers in the design automation field. Highlights: A discussion of the leading hardware verification techniques, including simulation and formal verification solutions Important concepts related to the underlying models and algorithms employed in the field\u00a0\u2026", "num_citations": "30\n", "authors": ["1888"]}
{"title": "Microprocessor and method for detecting faults therein\n", "abstract": " A method for detecting microprocessor hardware faults includes sending at least one input signal to a logic block within the microprocessor, collecting an output response to the input signal from the logic block, and determining whether the output response matches an expected output response of the logic block.", "num_citations": "29\n", "authors": ["1888"]}
{"title": "Viper: virtual pipelines for enhanced reliability\n", "abstract": " The reliability of future processors is threatened by decreasing transistor robustness. Current architectures focus on delivering high performance at low cost; lifetime device reliability is a secondary concern. As the rate of permanent hardware faults increases, robustness will become a first class constraint for even low-cost systems. Current research into reliable architectures has focused on ad-hoc solutions to improve designs without altering their centralized control logic. Unfortunately, this centralized control presents a single point of failure, which limits long-term robustness. To address this issue, we introduce Viper, an architecture built from a redundant collection of fine-grained hardware components. Instructions are perceived as customers that require a sequence of services in order to properly execute. The hardware components vie to perform what services they can, dynamically forming virtual pipelines that\u00a0\u2026", "num_citations": "28\n", "authors": ["1888"]}
{"title": "Post-placement rewiring and rebuffering by exhaustive search for functional symmetries\n", "abstract": " Separate optimizations of logic and layout have been thoroughly studied in the past and are well documented for common benchmarks. However, to be competitive, modern circuit optimizations must use physical and logic information simultaneously. In this work, we propose new algorithms for rewiring and rebuffering - a post-placement optimization that reconnects pins of a given netlist without changing the logic function and gate locations. These techniques are compatible with separate layout and logic optimizations, and appear independent of them. In particular, when the new optimization is applied before or after detailed placement, it approximately doubles the improvement in wirelength. Our contributions are based on exhaustive search for functional symmetries in sub-circuits consisting of several gates. Our graph-based symmetry finding is more comprehensive than previously known algorithms - it detects\u00a0\u2026", "num_citations": "26\n", "authors": ["1888"]}
{"title": "Restoring circuit structure from SAT instances\n", "abstract": " SAT solvers are now frequently used in formal verification, circuit test and other areas of EDA. In many such applications, SAT instances are derived from logic circuits. It is often assumed in the literature that circuit structure is lost when a conversion to CNF clauses is made [9]. We aim to examine this assumption. Specifically we formulate classes of combinational circuits that can be reproduced entirely from their SAT encodings. Using this knowledge, one may be able to apply Circuit-SAT techniques to a wider range of SAT instances and benefit from their improved performance on circuit-derived instances.", "num_citations": "26\n", "authors": ["1888"]}
{"title": "MCjammer: Adaptive verification for multi-core designs\n", "abstract": " The challenge of verification of multi-core and multi-processor designs grows dramatically with each new generation of systems produced today. Validation of memory coherence of such systems, which include multiple levels of cache and complex protocols, constitutes a major fraction of this task. Unfortunately, current tools are incapable of addressing these challenges, allowing bugs, which cause unpredictable software behavior and wrong computation results, to slip into hardware.", "num_citations": "25\n", "authors": ["1888"]}
{"title": "DRAIN: Distributed recovery architecture for inaccessible nodes in multi-core chips\n", "abstract": " As transistor dimensions continue to scale deep into the nanometer regime, silicon reliability is becoming a chief concern. At the same time, transistor counts are scaling up, enabling the design of highly integrated chips with many cores and a complex interconnect fabric, often a network on chip (NoC). Particularly problematic is the case when the accumulation of permanent hardware faults leads to disconnected cores in the system. In order to maintain correct system operation, it is necessary to salvage the data from these isolated nodes.", "num_citations": "24\n", "authors": ["1888"]}
{"title": "Gate-level simulation with GPU computing\n", "abstract": " Functional verification of modern digital designs is a crucial, time-consuming task impacting not only the correctness of the final product, but also its time to market. At the heart of most of today\u2019s verification efforts is logic simulation, used heavily to verify the functional correctness of a design for a broad range of abstraction levels. In mainstream industry verification methodologies, typical setups coordinate the validation effort of a complex digital system by distributing logic simulation tasks among vast server farms for months at a time. Yet, the performance of logic simulation is not sufficient to satisfy the demand, leading to incomplete validation processes, escaped functional bugs, and continuous pressure on the EDA industry to develop faster simulation solutions. In this work we propose GCS, a solution to boost the performance of logic simulation, gate-level simulation in particular, by more than a factor of 10 using\u00a0\u2026", "num_citations": "24\n", "authors": ["1888"]}
{"title": "Human computing for EDA\n", "abstract": " Electronic design automation is a field replete with challenging, and often intractable, problems to be solved over very large instances. As a result, the field of design automation has developed a staggering expertise in approximations, abstractions and heuristics as a means to sidestep the NP-hard nature of these problems. Approximations and heuristics are at heart a natural application of human reasoning. In this work we propose to harness human potential to solve some of these problems. Specifically, we propose FunSAT, a massively multiplayer puzzle game for SAT solving. FunSAT leverages visual pattern recognition skills, abstract perception and intuitive strategy skills of humans to solve complex SAT instances. Players are motivated by the puzzle solving challenges of the game and by its social interaction aspects.", "num_citations": "24\n", "authors": ["1888"]}
{"title": "Heterogeneous memory subsystem for natural graph analytics\n", "abstract": " As graph applications become more popular and diverse, it is important to design efficient hardware architectures that maintain the flexibility of high-level graph programming frameworks. Prior works have identified the memory subsystem of traditional chip multiprocessors (CMPs) as a key source of inefficiency; however, these solutions do not exploit locality that exists in the structure of many real-world graphs. In this work, we target graphs that follow a power-law distribution, for which there is a unique opportunity to significantly boost the overall performance of the memory subsystem. We note that many natural graphs, derived from web, social networks, even biological networks, follow the power law, that is, 20% of the vertices are linked to 80% of the edges. Based on this observation, we propose a novel memory subsystem architecture that leverages this structural graph locality. Our architecture is based on a\u00a0\u2026", "num_citations": "23\n", "authors": ["1888"]}
{"title": "Caspar: Hardware patching for multicore processors\n", "abstract": " Ensuring correctness of execution of complex multi-core processor systems deployed in the field remains to this day an extremely challenging task. The major part of this effort is concentrated on design verification, where different pre- and post-silicon techniques are used to guarantee that devices behave exactly as stated in the specification. Unfortunately, the performance of even state-of-the-art validation tools lags behind the growing complexity of multi-core designs. Therefore, subtle bugs still slip into released components, causing incorrect computational results, or even compromising the security of the end-user systems. In this work we present Caspar - an approach for in-the-field patching of the memory subsystem hardware in multi-core chips. Caspar relies on a checkpointing system, which periodically logs the state of the chip, and a novel error detection and recovery scheme, which uses a simplified mode\u00a0\u2026", "num_citations": "23\n", "authors": ["1888"]}
{"title": "Testudo: Heavyweight security analysis via statistical sampling\n", "abstract": " Heavyweight security analysis systems, such as taint analysis and dynamic type checking, are powerful technologies used to detect security vulnerabilities and software bugs. Traditional software implementations of these systems have high instrumentation overhead and suffer from significant performance impacts. To mitigate these slowdowns, a few hardware-assisted techniques have been recently proposed. However, these solutions incur a large memory overhead and require hardware platform support in the form of tagged memory systems and extended bus designs. Due to these costs and limitations, the deployment of heavyweight security analysis solutions is, as of today, limited to the research lab. In this paper, we describe Testudo, a novel hardware approach to heavyweight security analysis that is based on statistical sampling of a programpsilas dataflow. Our dynamic distributed debugging reduces the\u00a0\u2026", "num_citations": "23\n", "authors": ["1888"]}
{"title": "Using field-repairable control logic to correct design errors in microprocessors\n", "abstract": " Functional correctness is a vital attribute of any hardware design. Unfortunately, due to extremely complex architectures, widespread components, such as microprocessors, are often released with latent bugs. The inability of modern verification tools to handle the fast growth of design complexity exacerbates the problem even further. In this paper, we propose a novel hardware-patching mechanism, called the field-repairable control logic (FRCL), that is designed for in-the-field correction of errors in the design's control logic-the most common type of defects, as our analysis demonstrates. Our solution introduces an additional component in the processor's hardware, a state matcher, that can be programmed to identify erroneous configurations using signals in the critical control state of the processor. Once a flawed configuration is ldquomatched,rdquo the processor switches into a degraded mode, a mode of operation\u00a0\u2026", "num_citations": "23\n", "authors": ["1888"]}
{"title": "Verification through the principle of least astonishment\n", "abstract": " Assessing the correctness of a digital design is a challenging task hampered by extremely large circuit netlists, counterintuitive property descriptions and ill-defined specifications. In this paper we propose a new verification methodology, inspired by the principle of least astonishment. The underlying idea is to provide an automatic assessment of what constitutes\" common behavior\" for a system, and use this to detect any anomaly in the design. Deviant behavior is presented to the verification engineer through intuitive, compact diagrams which lend themselves to quick inspection for correctness. To enable this methodology we introduce Inferno, a new tool which can analyze the results of a logic simulation trace and automatically extract high-level diagrams representing the design's transaction activity across any user-defined interface. In addition, Inferno can automatically generate a checker module corresponding\u00a0\u2026", "num_citations": "22\n", "authors": ["1888"]}
{"title": "Efficient state representation for symbolic simulation\n", "abstract": " Symbolic simulation is attracting increasing interest for the validation of digital circuits. It allows the verification engineer to explore all, or a major portion of the circuit's state space without having to design specific and time-consuming test stimuli. However, the complexity and unpredictable run-time behavior of symbolic simulation have limited its scope to small-to-medium circuits. In this paper, we propose a novel approach to symbolic simulation that reduces the size of the BDDs of the state vector while maintaining an exact representation of the set of states visited. The method exploits the decomposition properties of Boolean functions. By restructuring the next-state functions in their disjoint support components, we gain a better insight in the role of each input variable. Consequently, we can simplify the next-state functions without significantly sacrificing the simulation accuracy. Our experimental results shows that\u00a0\u2026", "num_citations": "22\n", "authors": ["1888"]}
{"title": "Application-aware diagnosis of runtime hardware faults\n", "abstract": " Extreme technology scaling in silicon devices drastically affects reliability, particularly because of runtime failures induced by transistor wearout. Current online testing mechanisms focus on testing all components in a microprocessor, including hardware that has not been exercised, and thus have high performance penalties. We propose a hybrid hardware/software online testing solution where components that are heavily utilized by the software application are tested more thoroughly and frequently. Thus, our online testing approach focuses on the processor units that affect application correctness the most, and it achieves high coverage while incurring minimal performance overhead. We also introduce a new metric, Application-Aware Fault Coverage, measuring a test's capability to detect faults that might have corrupted the state or the output of an application. Test coverage is further improved through the\u00a0\u2026", "num_citations": "21\n", "authors": ["1888"]}
{"title": "Inferno: Streamlining verification with inferred semantics\n", "abstract": " Understanding designers' intentions and accurately verifying a design are major obstacles for verification engineers today. Currently available debugging tools, such as waveform viewers, are unwieldy, often requiring the user to search through millions of cycles of logic simulation data to locate a problem. In this paper, we present Inferno, a novel solution capable of automatically extracting semantic information from a design's interface from simulation information. The semantic structure of an interface's communication protocol is presented to the user as a set of transactions, that is, monolithic communication units that have typically been observed several times during the logic simulation. Transactions can graphically be presented to the user and used as an aid to understand and validate the communication protocol of a design's interface. In addition, approved transactions can also be encoded as assertions\u00a0\u2026", "num_citations": "21\n", "authors": ["1888"]}
{"title": "Random stimulus generation using entropy and XOR constraints\n", "abstract": " Despite the growing research effort in formal verification, constraint-based random simulation remains an integral part of design validation, especially for large design components where formal techniques do not scale. However, stimulating important aspects of a design to uncover bugs often requires the construction of complex constraints to guide stimulus generation. We propose Toggle, a stimulus generation engine, which features (1) an entropy-based coverage analysis to efficiently find portions of the design inadequately sensitized by simulation and (2) a novel strategy to automatically stimulate these portions through a specialized SAT algorithm that uses small randomized XOR constraints. As our experimental results demonstrate, Toggle requires minimal input from the verification engineer, and significantly improves the coverage qualities of the generated stimuli when compared to plain random simulation.", "num_citations": "21\n", "authors": ["1888"]}
{"title": "Boolean function representation based on disjoint-support decompositions\n", "abstract": " The Multi-Level Decomposition Diagrams (MLDDs) of this paper are a canonical representation of Boolean functions expliciting disjoint-support decompositions. MLDDs allow the reduction of memory occupation with respect to traditional ROBDDs by decomposing logic functions recursively into simpler-and more sharable-blocks. The representation is less sensitive to variable ordering, and because of this property, analysis of the MLDD graphs allows at times the identification of better variable orderings. The identification of more terminal cases by Boolean algebra techniques makes it possible to compensate the additional-small-CPU time required to identify the disjoint-support decomposition. We expect the properties of MLDDs to be useful in several contexts, most notably logic synthesis, technology mapping, and sequential hardware verification.", "num_citations": "20\n", "authors": ["1888"]}
{"title": "Bridging pre-and post-silicon debugging with BiPeD\n", "abstract": " The growing complexity of modern chips has caused an increasing share of the verification effort to shift towards post-silicon validation. This phase is challenged by poor observability, limited off-chip bandwidth, and complex, concurrent communication interfaces. Furthermore, pre-silicon verification and post-silicon validation methodologies are very different and share little information between them. As as result, the diagnosis and debugging of postsilicon failures is very much an ad-hoc and time-consuming task that is largely unable to leverage the vast body of design knowledge available in pre-silicon. We propose BiPeD, a novel methodology to identify the exact time and location of post-silicon bugs. During pre-silicon verification, BiPeD learns the correct behavior of a design's communication patterns. In post-silicon, this knowledge is used to detect errors by means of a reconfigurable hardware unit. When an\u00a0\u2026", "num_citations": "18\n", "authors": ["1888"]}
{"title": "SystemC simulation on GP-GPUs: CUDA vs. OpenCL\n", "abstract": " SystemC is a widespread language for developing SoC designs. Unfortunately, most SystemC simulators are based on a strictly sequential scheduler that heavily limits their performance, impacting verification schedules and time-to-market of new designs. Parallelizing SystemC simulation entails a complete re-design of the simulator kernel for the specific target parallel architectures. This paper proposes an automatic methodology to generate a parallel SystemC simulator kernel, exploiting the massive parallelism of GP-GPU architectures. Our solution leverages static scheduling to reduce synchronization overheads. The generated simulator code targets both CUDA and OpenCL libraries, to boost scalability and provide support for multiple GP-GPU architectures. Finally, the paper compares the performance of our solution on CUDA vs. OpenCL platforms, with the goal of investigating advantages and drawbacks\u00a0\u2026", "num_citations": "18\n", "authors": ["1888"]}
{"title": "Safe delay optimization for physical synthesis\n", "abstract": " Physical synthesis is a relatively young field in electronic design automation. Many published optimizations for physical synthesis end up hurting the final result, often by neglecting important physical aspects of the layout, such as long wires or routing congestion. In this work we propose SafeResynth, a safe resynthesis technique, which provides immediately-measurable delay improvement without altering the design's functionality. It can enhance circuit timing without detrimental effects on route length and congestion. When applied to IWLS'05 benchmarks, SafeResynth improves circuit delay by 11% on average after routing, while increasing route length and via count by less than 0.2%. Our resynthesis can also be used in an unsafe mode, akin to more traditional physical synthesis algorithms popular in commercial tools. Applied together, our safe and unsafe transformations achieve 24% average delay\u00a0\u2026", "num_citations": "18\n", "authors": ["1888"]}
{"title": "Highly fault-tolerant NoC routing with application-aware congestion management\n", "abstract": " Silicon devices are becoming less and less reliable as technology moves to smaller feature sizes. As a result, digital systems are increasingly likely to experience permanent failures during their life-time. To overcome this problem, networks-on-chip (NoCs) should be designed to, not only fulfill performance requirements, but also be robust to many fault occurrences. This paper proposes a fault-and application-aware routing framework called FATE: it leverages the diversity of communication patterns in applications for highly faulty NoCs to reduce congestion during execution. To this end, FATE estimates routing demands in applications to balance traffic load among the available resources. We propose a set of novel route-enabling rules that greatly reduce the search for deadlock-free, maximally-connected routes for any faulty 2D mesh topology, by preventing early on the exploration of routing configuration options\u00a0\u2026", "num_citations": "17\n", "authors": ["1888"]}
{"title": "Logic synthesis and circuit customization using extensive external don't-cares\n", "abstract": " Traditional digital circuit synthesis flows start from an HDL behavioral definition and assume that circuit functions are almost completely defined, making don't-care conditions rare. However, recent design methodologies do not always satisfy these assumptions. For instance, third-party IP blocks used in a system-on-chip are often overdesigned for the requirements at hand. By focusing only on the input combinations occurring in a specific application, one could resynthesize the system to greatly reduce its area and power consumption. Therefore we extend modern digital synthesis with a novel technique, called SWEDE, that makes use of extensive external don't-cares. In addition, we utilize such don't-cares present implicitly in existing simulation-based verification environments for circuit customization. Experiments indicate that SWEDE scales to large ICs with half-million input vectors and handles practical cases well.", "num_citations": "17\n", "authors": ["1888"]}
{"title": "Optimizing nonmonotonic interconnect using functional simulation and logic restructuring\n", "abstract": " The relatively poor scaling of interconnect in modern digital circuits necessitates a number of design optimizations, which must typically be iterated several times to meet the specified performance objectives. Such iterations are often due to the difficulty of early delay estimation, particularly before placement. Therefore, effective logic restructuring to reduce interconnect delay has been a major challenge in physical synthesis, a phase during which more accurate delay estimates can be finally gathered. In this paper, we develop a new approach that enhances modern high-performance logic synthesis techniques with flexibility and accuracy in the physical domain. This approach is based on the following: 1) a novel criterion based on path monotonicity, which identifies those interconnects that are amenable to optimization through logic restructuring, and 2) a synthesis algorithm relying on logic simulation and placement\u00a0\u2026", "num_citations": "17\n", "authors": ["1888"]}
{"title": "Checking architectural outputs instruction-by-instruction on acceleration platforms\n", "abstract": " Simulation-based verification is an integral part of a modern microprocessor's design effort. Commonly, several checking techniques are deployed alongside the simulator to detect and localize each functional bug manifestation. Among these, a widespread technique entails comparing a microprocessor design's outputs with a golden model at the architectural granularity, instruction-by-instruction. However, due to exponential growth in design complexity, the performance of software-based simulation falls far short of achieving an acceptable level of coverage, which typically requires billions of simulation cycles. Hence, verification engineers rely on simulation acceleration platforms. Unfortunately, the intrinsic characteristics of these platforms make the adoption of the checking solutions mentioned above a challenging goal: for instance, the lockstep execution of a software checker together with the design's simulation\u00a0\u2026", "num_citations": "16\n", "authors": ["1888"]}
{"title": "System for High-Efficiency Post-Silicon Verification of a Processor\n", "abstract": " A post-silicon validation technique is able to craft randomized executable code, with known final outcomes, as a verification test that is executable on a hardware, such as a prototype microprocessor. A verification device is able to generate the test, in the form of programs, in such a way that at the end of the execution, the initial state of the test hardware is restored. Therefore, the final state of such a reversible program is known a priori. The technique may use a program generation algorithm, agnostic to any particular instruction set on the test hardware. In some examples, that algorithm is executed on the test hardware to generate the verification test, which is then executed on that test hardware. In other examples, the verification test is generated on another processor coupled to the test hardware. In either case, the verification test may contain initial and inverse operations determined from the test hardware.", "num_citations": "15\n", "authors": ["1888"]}
{"title": "Assessing SEU vulnerability via circuit-level timing analysis\n", "abstract": " Recently, there has been a growing concern that, in relation to process technology scaling, the soft-error rate will become a major challenge in designing reliable systems. In this work, we introduce a high-fidelity, high-performance simulation infrastructure for quantifying the derating effects on soft-error rates while considering microarchitectural, timing and logic-related masking, using realistic workloads on a CMP switch design. We use a gate-level model for the CMP switch design, enabling us to inject faults into blocks of combinational logic. We are then able to track logic-related and time-related fault masking, as well as microarchitecturalrelated fault masking, at the architecture level. We find out that for complex designs, logic-and time-related fault masking account for more than 50% of the masked faults. We also observe that only 3-4% of the injected faults propagate an error at the design\u2019s output and cause an error in the application\u2019s execution, resulting in a derating factor of 30. From our experiments, we also demonstrate that soft-error derating effects highly depend on the design\u2019s characteristics and utilization.", "num_citations": "15\n", "authors": ["1888"]}
{"title": "High-radix on-chip networks with low-radix routers\n", "abstract": " Networks-on-chip (NoCs) have become increasingly widespread in recent years due to the extensive integration of many components in modern multicore processors and SoC designs. One of the fundamental tradeoffs in NoC design is the radix of its constituent routers. While high-radix routers enable a richly connected and low diameter network, low-radix routers allow for a small silicon area. Since the NoC consumes a significant portion of the on-chip resources, na\u00efvely deploying an expensive high-radix network is not a practical option. In this work, we present a novel solution to provide high-radix like performance at a cost similar to that of a low-radix network. Our solution leverages the irregularity in runtime communication patterns to provide short low-latency paths between frequently communicating nodes, while infrequently communicating pairs rely on longer paths. To this end, it leverages a flexible topology\u00a0\u2026", "num_citations": "14\n", "authors": ["1888"]}
{"title": "Highly scalable distributed dataflow analysis\n", "abstract": " Dynamic dataflow analyses find software errors by tracking meta-values associated with a program's runtime data. Despite their benefits, the orders-of-magnitude slowdowns that accompany these systems limit their use to the development stage; few users would tolerate such overheads. This work extends dynamic dataflow analyses with a novel sampling system which ensures that runtime slowdowns do not exceed a user-defined threshold. While previous sampling methods are inadequate for dataflow analyses, our technique efficiently reduces the number and size of analyzed dataflows. In doing so, it allows individual users to test large, stochastically chosen sets of a process's dataflows. Large populations can therefore, in aggregate, analyze a larger portion of the program than is possible by any single user running a complete, but slow, analysis. In our experimental evaluation, we show that 1 out of every 10\u00a0\u2026", "num_citations": "14\n", "authors": ["1888"]}
{"title": "STACCATO: disjoint support decompositions from BDDs through symbolic kernels\n", "abstract": " A disjoint support decomposition (DSD) is a representation of a Boolean function F obtained by composing two or more simpler component functions such that the component functions have no common inputs. The decomposition of a function is desirable for several reasons. First, it's a method to obtain a multiple-level implementation of a function. It leads to a partition in simpler blocks that easily results in smaller areas and fewer interconnects. Moreover, it exposes a parallelism in the computation of the function that can be exploited by hardware as well as during simulation. In this paper we present a novel algorithm, STACCATO, that generates a DSD decomposition starting from the BDD of a function. STACCATO is novel because 1) it provides a complete description of each decomposition, that is, it computes the\" kernel\" function K relating the elements of each decomposition, and 2) it has better performance than\u00a0\u2026", "num_citations": "14\n", "authors": ["1888"]}
{"title": "On the use of GP-GPUs for accelerating compute-intensive EDA applications\n", "abstract": " General purpose graphics processing units (GP-GPUs) have recently been explored as a new computing paradigm for accelerating compute-intensive EDA applications. Such massively parallel architectures have been applied in accelerating the simulation of digital designs during several phases of their development - corresponding to different abstraction levels, specifically: (i) gate-level netlist descriptions, (ii) register-transfer level and (iii) transaction-level descriptions. This embedded tutorial presents a comprehensive analysis of the best results obtained by adopting GP-GPUs in all these EDA applications.", "num_citations": "13\n", "authors": ["1888"]}
{"title": "Advances and insights into parallel SAT solving\n", "abstract": " The recent improvements in SAT solving algorithms, driven by the quest to solve increasingly complex problem instances, has produced techniques whose objective is to prune large portions of the search space to converge quickly to a solution (for instance, conflict-driven learning). In particular, solutions have been suggested in this area which attack the problem by attempting to parallelize DPLL-based SAT. However, so far the results have been mixed, which, consequently, have led to a scarcity of research in this space. One of the challenges of this direction is that finding a good partitioning of the space is not straightforward when using a DPLL-based algorithm. Moreover, partitioning the problem limits the benefits that can be ripped from effective learning and good variable ordering heuristics in sequential solvers. In this paper, we propose techniques which improve upon previous approaches, primarily by improving the quality of learning during the search. Our first technique implements a parallel version of recursive learning (RL), which we use as a preprocessor to simplify the initial instance. Unlike previous DPLL-based approaches, RL can be easily partitioned and each processor learns information that is beneficial to solving the problem at hand. Our results indicate that this initial preprocessing can be done efficiently when divided among several processors, thereby boosting the performance of solving the resulting instance. In addition to RL, we explore strategies for improving parallel DPLL-based algorithms by adopting heuristics which have proven effective in the sequential domain, such as VSIDs, and by applying them to shared\u00a0\u2026", "num_citations": "13\n", "authors": ["1888"]}
{"title": "Circuit-aware architectural simulation\n", "abstract": " Architectural simulation has achieved a prominent role in the system design cycle by providing designers the ability to quickly examine a wide variety of design choices. However, the recent trend in system design toward architectures that react to circuit-level phenomena has outstripped the capabilities of traditional cycle-based architectural simulators. In this paper, we present an architectural simulator design that incorporates a circuit modeling capability, permitting architectural-level simulations that react to circuit characteristics (such as latency, energy, or current draw) on a cycle-by-cycle basis. While these additional capabilities slow simulation speed, we show that the careful application of circuit simulation optimizations and simulation sampling techniques permit high levels of detail with sufficient speed to examine entire workloads.", "num_citations": "13\n", "authors": ["1888"]}
{"title": "BugMD: Automatic mismatch diagnosis for bug triaging\n", "abstract": " System-level validation is the most challenging phase of design verification. A common methodology in this context entails simulating the design under validation in lockstep with a high-level golden model, while comparing the architectural state of the two models at regular intervals. However, if a bug is detected, the diagnosis of the problem with this framework is extremely time and resource consuming. To address this challenge, we propose a novel bug triaging solution that collects multiple architectural-level mismatches and employs a classifier to pinpoint buggy design units. We also design and implement an automated synthetic bug injection framework that enables us to generate large datasets for training our classifier models. Experimental results show that our solution is able to correctly identify the source of a bug over 70% of the time in an out-of-order processor model. Furthermore, our solution can identify\u00a0\u2026", "num_citations": "12\n", "authors": ["1888"]}
{"title": "Post-silicon validation of multiprocessor memory consistency\n", "abstract": " Shared-memory chip-multiprocessor (CMP) architectures define memory consistency models that establish the ordering rules for memory operations from multiple threads. Validating the correctness of a CMP's implementation of its memory consistency model requires extensive monitoring and analysis of memory accesses while multiple threads are executing on the CMP. In this paper, we present a low overhead solution for observing, recording and analyzing shared-memory interactions for use in an emulation and/or post-silicon validation environment. Our approach leverages portions of the CMP's own data caches, augmented only by a small amount of hardware logic, to log information relevant to memory accesses. After transferring this information to a central memory location, we deploy our own analysis algorithm to detect any possible memory consistency violations. We build on the property that a violation\u00a0\u2026", "num_citations": "12\n", "authors": ["1888"]}
{"title": "Postplacement rewiring by exhaustive search for functional symmetries\n", "abstract": " We propose two new algorithms for rewiring: a postplacement optimization that reconnects pins of a given netlist without changing the logic function and gate locations. In the first algorithm, we extract small subcircuits consisting of several gates from the design and reconnect pins according to the symmetries of the subcircuits. To enhance the power of symmetry detection, we also propose a graph-based symmetry detector that can identify permutational and phase-shift symmetries on multiple input and output wires, as well as hybrid symmetries, creating abundant opportunities for rewiring. Our second algorithm, called long-range rewiring, is based on reconnecting equivalent pins and can augment the first approach for further optimization. We apply our techniques for wirelength optimization and observe that they provide wirelength reduction comparable to that achieved by detailed placement.", "num_citations": "12\n", "authors": ["1888"]}
{"title": "InVerS: an incremental verification system with circuit similarity metrics and error visualization\n", "abstract": " Dramatic increases in design complexity and advances in IC manufacturing technology affect all aspects of circuit performance and functional correctness. As interconnect increasingly dominates delay and power at the latest technology nodes, much effort is invested in physical synthesis optimizations, posing great challenges in validating the correctness of such optimizations. Common design methodology delays the verification of physical synthesis transformations until the completion of the design phase. However, this approach is not sustainable because the isolation of potential errors becomes extremely challenging in current complex design efforts. In addition, the lack of interoperability between verification and debugging tools greatly limits engineers' productivity. Since the design's functional correctness should not be compromised, considerable resources are dedicated to checking an ensuring correctness\u00a0\u2026", "num_citations": "12\n", "authors": ["1888"]}
{"title": "Collaborative accelerators for in-memory mapreduce on scale-up machines\n", "abstract": " Relying on efficient data analytics platforms is increasingly becoming crucial for both small and large scale datasets. While MapReduce implementations, such as Hadoop and Spark, were originally proposed for petascale processing in scale-out clusters, it has been noted that, today, most data centers processes operate on gigabyte-order or smaller datasets, which are best processed in single high-end scale-up machines. In this context, Phoenix++ is a highly optimized MapReduce framework available for chip-multiprocessor (CMP) scale-up machines. In this paper we observe that Phoenix++ suffers from an inefficient utilization of the memory subsystem, and a serialized execution of the MapReduce stages. To overcome these inefficiencies, we propose CASM, an architecture that equips each core in a CMP design with a dedicated instance of a specialized hardware unit (the CASM accelerators). These units\u00a0\u2026", "num_citations": "11\n", "authors": ["1888"]}
{"title": "Gate-level logic simulator using multiple processor architectures\n", "abstract": " Techniques for simulating operation of a connectivity level description of an integrated circuit design are provided, for example, to simulate logic elements expressed through a netlist description. The techniques utilize a host processor selectively partitioning and optimizing the descriptions of the integrated circuit design for efficient simulation on a parallel processor, more particularly a SIMD processor. The description may be segmented into cluster groups, for example macro-gates, formed of logic elements, where the cluster groups are sized for parallel simulation on the parallel processor. Simulation may occur in an oblivious as well as event-driven manner, depending on the implementation.", "num_citations": "11\n", "authors": ["1888"]}
{"title": "Architecting a reliable CMP switch architecture\n", "abstract": " As silicon technologies move into the nanometer regime, transistor reliability is expected to wane as devices become subject to extreme process variation, particle-induced transient errors, and transistor wear-out. Unless these challenges are addressed, computer vendors can expect low yields and short mean-times-to-failure. In this article, we examine the challenges of designing complex computing systems in the presence of transient and permanent faults. We select one small aspect of a typical chip multiprocessor (CMP) system to study in detail, a single CMP router switch. Our goal is to design a BulletProof CMP switch architecture capable of tolerating significant levels of various types of defects. We first assess the vulnerability of the CMP switch to transient faults. To better understand the impact of these faults, we evaluate our CMP switch designs using circuit-level timing on detailed physical layouts. Our\u00a0\u2026", "num_citations": "11\n", "authors": ["1888"]}
{"title": "Achieving scalable hardware verification with symbolic simulation\n", "abstract": " In recent years, the complexity of digital integrated circuit (IC) designs has grown at a challenging pace. Within this context, proper verification of an IC design has become a central aspect of the development cycle. Logic simulation is the accepted method for verification because of its scalability, although it can only visits a small fraction of the state space. Symbolic simulation is an alternative method that is attracting increasing interest because it can explore a major portion of the circuit's state space without the need of design-specific tests. The limiting factor to the mainstream deployment of this approach has been its complexity and unpredictable run-time behavior.", "num_citations": "11\n", "authors": ["1888"]}