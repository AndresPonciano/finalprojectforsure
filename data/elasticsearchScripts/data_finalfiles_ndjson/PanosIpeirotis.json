{"title": "Duplicate record detection: A survey\n", "abstract": " Often, in the real world, entities have two or more representations in databases. Duplicate records do not share a common key and/or they contain errors that make duplicate matching a difficult task. Errors are introduced as the result of transcription errors, incomplete information, lack of standard formats, or any combination of these factors. In this paper, we present a thorough analysis of the literature on duplicate record detection. We cover similarity metrics that are commonly used to detect similar field entries, and we present an extensive set of duplicate detection algorithms that can detect approximately duplicate records in a database. We also cover multiple techniques for improving the efficiency and scalability of approximate duplicate detection algorithms. We conclude with coverage of existing tools and with a brief discussion of the big open problems in the area", "num_citations": "2482\n", "authors": ["2068"]}
{"title": "Estimating the helpfulness and economic impact of product reviews: Mining text and reviewer characteristics\n", "abstract": " With the rapid growth of the Internet, the ability of users to create and publish content has created active electronic communities that provide a wealth of product information. However, the high volume of reviews that are typically published for a single product makes harder for individuals as well as manufacturers to locate the best reviews and understand the true underlying quality of a product. In this paper, we reexamine the impact of reviews on economic outcomes like product sales and see how different factors affect social outcomes such as their perceived usefulness. Our approach explores multiple aspects of review text, such as subjectivity levels, various measures of readability and extent of spelling errors to identify important text-based features. In addition, we also examine multiple reviewer-level features such as average usefulness of past reviews and the self-disclosed identity measures of reviewers that\u00a0\u2026", "num_citations": "1404\n", "authors": ["2068"]}
{"title": "Quality management on Amazon Mechanical Turk\n", "abstract": " Crowdsourcing services, such as Amazon Mechanical Turk, allow for easy distribution of small tasks to a large number of workers. Unfortunately, since manually verifying the quality of the submitted results is hard, malicious workers often take advantage of the verification difficulty and submit answers of low quality. Currently, most requesters rely on redundancy to identify the correct answers. However, redundancy is not a panacea. Massive redundancy is expensive, increasing significantly the cost of crowdsourced solutions. Therefore, we need techniques that will accurately estimate the quality of the workers, allowing for the rejection and blocking of the low-performing workers and spammers.", "num_citations": "1196\n", "authors": ["2068"]}
{"title": "Deriving the pricing power of product features by mining consumer reviews\n", "abstract": " Increasingly, user-generated product reviews serve as a valuable source of information for customers making product choices online. The existing literature typically incorporates the impact of product reviews on sales based on numeric variables representing the valence and volume of reviews. In this paper, we posit that the information embedded in product reviews cannot be captured by a single scalar value. Rather, we argue that product reviews are multifaceted, and hence the textual content of product reviews is an important determinant of consumers' choices, over and above the valence and volume of reviews. To demonstrate this, we use text mining to incorporate review text in a consumer choice model by decomposing textual reviews into segments describing different product features. We estimate our model based on a unique data set from Amazon containing sales data and consumer review data for two\u00a0\u2026", "num_citations": "982\n", "authors": ["2068"]}
{"title": "Analyzing the Amazon Mechanical Turk Marketplace\n", "abstract": " An associate professor at New York Universitys Stern School of Business uncovers answers about who are the employers in paid crowdsourcing, what tasks they post, and how much they pay.", "num_citations": "796\n", "authors": ["2068"]}
{"title": "Approximate string joins in a database (almost) for free\n", "abstract": " String data is ubiquitous, and its management has taken on particular importance in the past few years. Approximate queries are very important on string data especially for more complex queries involving joins. This is due, for example, to the prevalence of typographical errors in data, and multiple conventions for recording attributes such as name and address. Commercial databases do not support approximate string joins directly, and it is a challenge to implement this functionality efficiently with user-defined functions (UDFs). In this paper, we develop a technique for building approximate string join capabilities on top of commercial databases by exploiting facilities already available in them. At the core, our technique relies on matching short substrings of length \u0430, called \u0430-grams, and taking into account both positions of individual matches and the total number of such matches. Our approach applies to both approximate full string matching and approximate substring matching, with a variety of possible edit distance functions. The approximate string match predicate, with a suitable edit distance threshold, can be mapped into a vanilla relational expression and optimized by conventional relational optimizers. We demonstrate experimentally the benefits of our technique over the direct use of UDFs, using commercial database systems and real data. To study the I/O and CPU behavior of approximate string join algorithms with variations in edit distance and \u0430-gram length, we also describe detailed experiments based on a prototype implementation.Permission to copy without fee all or part of this material is granted provided that the copies are not\u00a0\u2026", "num_citations": "788\n", "authors": ["2068"]}
{"title": "Demographics of Mechanical Turk\n", "abstract": " We present the results of a survey that collected information about the demographics of participants on Amazon Mechanical Turk, together with information about their level of activity and motivation for working on Amazon Mechanical Turk. We find that approximately 50% of the workers come from the United States and 40% come from India. Country of origin tends to change the motivating reasons for workers to participate in the marketplace. Significantly more workers from India participate on Mechanical Turk because the online marketplace is a primary source of income, while in the US most workers consider Mechanical Turk a secondary source of income. While money is a primary motivating reason for workers to participate in the marketplace, workers also cite a variety of other motivating reasons, including entertainment and education.", "num_citations": "764\n", "authors": ["2068"]}
{"title": "Designing ranking systems for hotels on travel search engines by mining user-generated and crowd-sourced content\n", "abstract": " User-generated content on social media platforms and product search engines is changing the way consumers shop for goods online. However, current product search engines fail to effectively leverage information created across diverse social media platforms. Moreover, current ranking algorithms in these product search engines tend to induce consumers to focus on one single product characteristic dimension (e.g., price, star rating). This approach largely ignores consumers' multidimensional preferences for products. In this paper, we propose to generate a ranking system that recommends products that provide, on average, the best value for the consumer's money. The key idea is that products that provide a higher surplus should be ranked higher on the screen in response to consumer queries. We use a unique data set of U.S. hotel reservations made over a three-month period through Travelocity, which we\u00a0\u2026", "num_citations": "678\n", "authors": ["2068"]}
{"title": "Designing novel review ranking systems: Predicting the usefulness and impact of reviews\n", "abstract": " With the rapid growth of the Internet, users' ability to publish content has created active electronic communities that provide a wealth of product information. Consumers naturally gravitate to reading reviews in order to decide whether to buy a product. However, the high volume of reviews that are typically published for a single product makes it harder for individuals to locate the best reviews and understand the true underlying quality of a product based on the reviews. Similarly, the manufacturer of a product needs to identify the reviews that influence the customer base, and examine the content of these reviews. In this paper, we propose two ranking mechanisms for ranking product reviews: a consumer-oriented ranking mechanism ranks the reviews according to their expected helpfulness, and a manufacturer-oriented ranking mechanism ranks the reviews according to their expected effect on sales. Our ranking\u00a0\u2026", "num_citations": "428\n", "authors": ["2068"]}
{"title": "Demographics and Dynamics of Mechanical Turk Workers\n", "abstract": " We present an analysis of the population dynamics and demographics of Amazon Mechanical Turk workers based on the results of the survey that we conducted over a period of 28 months, with more than 85K responses from 40K unique participants. The demographics survey is ongoing (as of November 2017), and the results are available at http://demographics. mturk-tracker. com: we provide an API for researchers to download the survey data. We use techniques from the field of ecology, in particular, the capture-recapture technique, to understand the size and dynamics of the underlying population. We also demonstrate how to model and account for the inherent selection biases in such surveys. Our results indicate that there are more than 100K workers available in Amazon\u00bb s crowdsourcing platform, the participation of the workers in the platform follows a heavy-tailed distribution, and at any given time there\u00a0\u2026", "num_citations": "417\n", "authors": ["2068"]}
{"title": "Show me the money! Deriving the pricing power of product features by mining consumer reviews\n", "abstract": " The increasing pervasiveness of the Internet has dramatically changed the way that consumers shop for goods. Consumer-generated product reviews have become a valuable source of information for customers, who read the reviews and decide whether to buy the product based on the information provided. In this paper, we use techniques that decompose the reviews into segments that evaluate the individual characteristics of a product (eg, image quality and battery life for a digital camera). Then, as a major contribution of this paper, we adapt methods from the econometrics literature, specifically the hedonic regression concept, to estimate:(a) the weight that customers place on each individual product feature,(b) the implicit evaluation score that customers assign to each feature, and (c) how these evaluations affect the revenue for a given product. Towards this goal, we develop a novel hybrid technique combining\u00a0\u2026", "num_citations": "312\n", "authors": ["2068"]}
{"title": "Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection\n", "abstract": " We introduce tools and methodologies to collect high quality, large scale fine-grained computer vision datasets using citizen scientists--crowd annotators who are passionate and knowledgeable about specific domains such as birds or airplanes. We worked with citizen scientists and domain experts to collect NABirds, a new high quality dataset containing 48,562 images of North American birds with 555 categories, part annotations and bounding boxes. We find that citizen scientists are significantly more accurate than Mechanical Turkers at zero cost. We worked with bird experts to measure the quality of popular datasets like CUB-200-2011 and ImageNet and found class label error rates of at least 4%. Nevertheless, we found that learning algorithms are surprisingly robust to annotation errors and this level of training data corruption can lead to an acceptably small increase in test error if the training set has sufficient size. At the same time, we found that an expert-curated high quality test set like NABirds is necessary to accurately measure the performance of fine-grained computer vision systems. We used NABirds to train a publicly available bird recognition service deployed on the web site of the Cornell Lab of Ornithology.", "num_citations": "283\n", "authors": ["2068"]}
{"title": "The global opportunity in online outsourcing\n", "abstract": " Online outsourcing (OO) has become a             promising alternative to traditional employment in today\u2019s             digital era. It has transformed where, when, and how work is             performed. For workers, this form of outsourcing has created             new opportunities to access and compete in global job             markets, from anywhere at any time, as long as they have             computer and Internet access. This study focuses on OO\u2019s             potential as a new and innovative channel for socioeconomic             development for developing country governments and             development practitioners, particularly in terms of youth             employment, services exports, and participation in the             digital economy. OO firms report that the private sector is             currently driving most of the demand, but public sector             demand for OO is a potential source of future growth. In             order to understand the opportunity for developing             countries, this study estimated the current size of the             market and projected its growth, and profiled OO work             through a combination of desk research and structured             interviews with academics, online workers, firms, and             industry analysts to better understand OO\u2019s potential impact             on human capital and employment. The study also conducted             focus group interviews with online workers in Kenya to             gather additional insight into the socioeconomic impacts of             OO, and carried out case studies in Kenya and Nigeria.", "num_citations": "271\n", "authors": ["2068"]}
{"title": "Distributed search over the hidden web: Hierarchical database sampling and selection\n", "abstract": " Publisher SummaryThis chapter presents an algorithm to derive content summaries from \u201cuncooperative\u201d databases by using \u201cfocused query probes,\u201d which adaptively zoom in on and extract documents that are representative of the topic coverage of the databases. The World-Wide Web continues to grow rapidly, which makes exploiting all useful information that is available a standing challenge. Although general search engines, such as Google, crawl and index a large amount of information, they typically ignore valuable data in text databases that are \u201chidden\u201d behind search interfaces and whose contents are not directly available for crawling through hyperlinks. Many valuable text databases on the web have non-crawlable contents that are \u201chidden\u201d behind search interfaces. Metasearchers are helpful tools for searching over many such databases at once through a unified query interface. A critical task for a\u00a0\u2026", "num_citations": "253\n", "authors": ["2068"]}
{"title": "The Dynamics of Micro-task Crowdsourcing: The case of Amazon MTurk\n", "abstract": " Micro-task crowdsourcing is rapidly gaining popularity among research communities and businesses as a means to leverage Human Computation in their daily operations. Unlike any other service, a crowdsourcing platform is in fact a marketplace subject to human factors that affect its performance, both in terms of speed and quality. Indeed, such factors shape the dynamics of the crowdsourcing market. For example, a known behavior of such markets is that increasing the reward of a set of tasks would lead to faster results. However, it is still unclear how different dimensions interact with each other: reward, task type, market competition, requester reputation, etc. In this paper, we adopt a data-driven approach to (A) perform a long-term analysis of a popular micro-task crowdsourcing platform and understand the evolution of its main actors (workers, requesters, and platform).(B) We leverage the main findings of our\u00a0\u2026", "num_citations": "240\n", "authors": ["2068"]}
{"title": "Text joins in an RDBMS for web data integration\n", "abstract": " The integration of data produced and collected across autonomous, heterogeneous web services is an increasingly important and challenging problem. Due to the lack of global identifiers, the same entity (eg, a product) might have different textual representations across databases. Textual data is also often noisy because of transcription errors, incomplete information, and lack of standard formats. A fundamental task during data integration is matching of strings that refer to the same entity. In this paper, we adopt the widely used and established cosine similarity metric from the information retrieval field in order to identify potential string matches across web sources. We then use this similarity metric to characterize this key aspect of data integration as a join between relations on textual attributes, where the similarity of matches exceeds a specified threshold. Computing an exact answer to the text join can be\u00a0\u2026", "num_citations": "234\n", "authors": ["2068"]}
{"title": "Examining the Impact of Ranking on Consumer Behavior and Search Engine Revenue\n", "abstract": " In this paper, we study the effects of three different kinds of search engine rankings on consumer behavior and search engine revenues: direct ranking effect, interaction effect between ranking and product ratings, and personalized ranking effect. We combine a hierarchical Bayesian model estimated on approximately one million online sessions from Travelocity, together with randomized experiments using a real-world hotel search engine application. Our archival data analysis and randomized experiments are consistent in demonstrating the following: (1) A consumer-utility-based ranking mechanism can lead to a significant increase in overall search engine revenue. (2) Significant interplay occurs between search engine ranking and product ratings. An inferior position on the search engine affects \u201chigher-class\u201d hotels more adversely. On the other hand, hotels with a lower customer rating are more likely to benefit\u00a0\u2026", "num_citations": "226\n", "authors": ["2068"]}
{"title": "Probe, count, and classify: Categorizing hidden web databases\n", "abstract": " The contents of many valuable web-accessible databases are only accessible through search interfaces and are hence invisible to traditional web \u201ccrawlers.\u201d Recent studies have estimated the size of this \u201chidden web\u201d to be 500 billion pages, while the size of the \u201ccrawlable\u201d web is only an estimated two billion pages. Recently, commercial web sites have started to manually organize web-accessible databases into Yahoo!-like hierarchical classification schemes. In this paper, we introduce a method for automating this classification process by using a small number of query probes. To classify a database, our algorithm does not retrieve or inspect any documents or pages from the database, but rather just exploits the number of matches that each query probe generates at the database in question. We have conducted an extensive experimental evaluation of our technique over collections of real documents, including\u00a0\u2026", "num_citations": "224\n", "authors": ["2068"]}
{"title": "Opinion mining using econometrics: A case study on reputation systems\n", "abstract": " Deriving the polarity and strength of opinions is an important research topic, attracting significant attention over the last few years. In this work, to measure the strength and polarity of an opinion, we consider the economic context in which the opinion is evaluated, instead of using human annotators or linguistic resources. We rely on the fact that text in on-line systems influences the behavior of humans and this effect can be observed using some easy-to-measure economic variables, such as revenues or product prices. By reversing the logic, we infer the semantic orientation and strength of an opinion by tracing the changes in the associated economic variable. In effect, we use econometrics to identify the \u201ceconomic value of text\u201d and assign a \u201cdollar value\u201d to each opinion phrase, measuring sentiment effectively and without the need for manual labeling. We argue that by interpreting opinions using econometrics, we have the first objective, quantifiable, and contextsensitive evaluation of opinions. We make the discussion concrete by presenting results on the reputation system of Amazon. com. We show that user feedback affects the pricing power of merchants and by measuring their pricing power we can infer the polarity and strength of the underlying feedback postings.", "num_citations": "200\n", "authors": ["2068"]}
{"title": "Answering general time-sensitive queries\n", "abstract": " Time is an important dimension of relevance for a large number of searches, such as over blogs and news archives. So far, research on searching over such collections has largely focused on locating topically similar documents for a query. Unfortunately, topic similarity alone is not always sufficient for document ranking. In this paper, we observe that, for an important class of queries that we call time-sensitive queries, the publication time of the documents in a news archive is important and should be considered in conjunction with the topic similarity to derive the final document ranking. Earlier work has focused on improving retrieval for \u201crecency\u201d queries that target recent documents. We propose a more general framework for handling time-sensitive queries and we automatically identify the important time intervals that are likely to be of interest for a query. Then, we build scoring techniques that seamlessly integrate\u00a0\u2026", "num_citations": "188\n", "authors": ["2068"]}
{"title": "Using q-grams in a DBMS for approximate string processing\n", "abstract": " String data is ubiquitous, and its management has taken on particular importance in the past few years. Approximate queries are very important on string data. This is due, for example, to the prevalence of typographical errors in data, and multiple conventions for recording attributes such as name and address. Commercial databases do not support approximate string queries directly, and it is a challenge to implement this functionality efficiently with user-defined functions (UDFs). In this paper, we develop a technique for building approximate string processing capabilities on top of commercial databases by exploiting facilities already available in them. At the core, our technique relies on generating short substrings of length \u00d5, called \u00d5-grams, and processing them using standard methods available in the DBMS. The proposed technique enables various approximate string processing methods in a DBMS, for example approximate (sub) string selections and joins, and can even be used with a variety of possible edit distance functions. The approximate string match predicate, with a suitable edit distance threshold, can be mapped into a vanilla relational expression and optimized by conventional relational optimizers.", "num_citations": "183\n", "authors": ["2068"]}
{"title": "QProber: A system for automatic classification of hidden-web databases\n", "abstract": " The contents of many valuable Web-accessible databases are only available through search interfaces and are hence invisible to traditional Web \"crawlers.\" Recently, commercial Web sites have started to manually organize Web-accessible databases into Yahoo!-like hierarchical classification schemes. Here we introduce QProber, a modular system that automates this classification process by using a small number of query probes, generated by document classifiers. QProber can use a variety of types of classifiers to generate the probes. To classify a database, QProber does not retrieve or inspect any documents or pages from the database, but rather just exploits the number of matches that each query probe generates at the database in question. We have conducted an extensive experimental evaluation of QProber over collections of real documents, experimenting with different types of document classifiers and\u00a0\u2026", "num_citations": "173\n", "authors": ["2068"]}
{"title": "What\u2019s the right price? Pricing tasks for finishing on time\n", "abstract": " Many practitioners currently use rules of thumb to price tasks on online labor markets. Incorrect pricing leads to task starvation or inefficient use of capital. Formal pricing policies can address these challenges. In this paper we argue that a pricing policy can be based on the trade-off between price and desired completion time. We show how this duality can lead to a better pricing policy for tasks in online labor markets. This paper makes three contributions. First, we devise an algorithm for job pricing using a survival analysis model. We then show that worker arrivals can be modeled as a non-homogeneous Poisson Process (NHPP). Finally using NHPP for worker arrivals and discrete choice models we present an abstract mathematical model that captures the dynamics of the market when full market information is presented to the task requester. This model can be used to predict completion times and pricing policies for both public and private crowds.", "num_citations": "170\n", "authors": ["2068"]}
{"title": "Designing ranking systems for consumer reviews: The impact of review subjectivity on product sales and review quality\n", "abstract": " With the rapid growth of the Internet, users\u2019 ability to publish content has created active electronic communities that provide a wealth of product information. Consumers naturally gravitate to reading reviews in order to decide whether to buy a product. However, the high volume of reviews that are typically published for a single product makes it harder for individuals to locate the best reviews and understand the true underlying quality of a product based on the reviews. Similarly, the manufacturer of a product wants to identify the reviews that influence the customer base, and examine the content of these reviews. In this paper we propose two ranking mechanisms for ranking product reviews: a consumer-oriented ranking mechanism ranks the reviews according to their expected helpfulness, and a manufacturer-oriented ranking mechanism ranks the reviews according to their expected effect on sales. Our ranking mechanism combines econometric analysis with text mining techniques in general, with subjectivity analysis in particular. We show that subjectivity analysis can give useful clues about the helpfulness of a review and about its impact on sales. Our results can have several implications for the market design of online opinion forums.", "num_citations": "157\n", "authors": ["2068"]}
{"title": "Automatic extraction of useful facet hierarchies from text databases\n", "abstract": " Databases of text and text-annotated data constitute a significant fraction of the information available in electronic form. Searching and browsing are the typical ways that users locate items of interest in such databases. Faceted interfaces represent a new powerful paradigm that proved to be a successful complement to keyword searching. Thus far, the identification of the facets was either a manual procedure, or relied on apriori knowledge of the facets that can potentially appear in the underlying collection. In this paper, we present an unsupervised technique for automatic extraction of facets useful for browsing text databases. In particular, we observe, through a pilot study, that facet terms rarely appear in text documents, showing that we need external resources to identify useful facet terms. For this, we first identify important phrases in each document. Then, we expand each phrase with \";context\"; phrases using\u00a0\u2026", "num_citations": "156\n", "authors": ["2068"]}
{"title": "Quizz: targeted crowdsourcing with a billion (potential) users\n", "abstract": " We describe Quizz, a gamified crowdsourcing system that simultaneously assesses the knowledge of users and acquires new knowledge from them. Quizz operates by asking users to complete short quizzes on specific topics; as a user answers the quiz questions, Quizz estimates the user's competence. To acquire new knowledge, Quizz also incorporates questions for which we do not have a known answer; the answers given by competent users provide useful signals for selecting the correct answers for these questions. Quizz actively tries to identify knowledgeable users on the Internet by running advertising campaigns, effectively leveraging the targeting capabilities of existing, publicly available, ad placement services. Quizz quantifies the contributions of the users using information theory and sends feedback to the advertisingsystem about each user. The feedback allows the ad targeting mechanism to further\u00a0\u2026", "num_citations": "155\n", "authors": ["2068"]}
{"title": "The dimensions of reputation in electronic markets\n", "abstract": " In this paper, we analyze how different dimensions of a seller's reputation affect pricing power in electronic markets. Given the interplay between buyers' trust and sellers' pricing power, we use text mining techniques to identify and structure dimensions of importance from feedback posted on reputation systems. By aggregating and scoring these dimensions based on the sentiment they contain, we use them to estimate a series of econometric models associating reputation with price premiums. We find that different dimensions do indeed affect pricing power differentially, and that a negative reputation hurts more than a positive one helps on some dimensions but not on others. We provide evidence that sellers of identical products in electronic markets differentiate themselves based on a distinguishing dimension of strength, and that buyers vary in the relative importance they place on different fulfillment characteristics. We highlight the importance of textual reputation feedback further by demonstrating that it substantially improves the performance of a classifier we have trained to predict future sales. Our results also suggest that online sellers distinguish themselves on specific and varying fulfillment characteristics that resemble the unique selling points highlighted by successful brands. We conclude by providing explicit examples of IT artifacts (buyer and seller tools) that use our interdisciplinary approach to enhance buyer trust and seller efficiency in online environments. This paper is the first study that integrates econometric, text mining and predictive modeling techniques toward a more complete analysis of the information captured by reputation\u00a0\u2026", "num_citations": "131\n", "authors": ["2068"]}
{"title": "Automatic construction of multifaceted browsing interfaces\n", "abstract": " Databases of text and text-annotated data constitute a significant fraction of the information available in electronic form. Searching and browsing are the typical ways that users locate items of interest in such databases. Interfaces that use multifaceted hierarchies represent a new powerful browsing paradigm which has been proven to be a successful complement to keyword searching. Thus far, multifaceted hierarchies have been created manually or semi-automatically, making it difficult to deploy multifaceted interfaces over a large number of databases. We present automatic and scalable methods for creation of multifaceted interfaces. Our methods are integrated with traditional relational databases and can scale well for large databases. Furthermore, we present methods for selecting the best portions of the generated hierarchies when the screen space is not sufficient for displaying all the hierarchy at once. We\u00a0\u2026", "num_citations": "130\n", "authors": ["2068"]}
{"title": "Query by document\n", "abstract": " We are experiencing an unprecedented increase of content contributed by users in forums such as blogs, social networking sites and microblogging services. Such abundance of content complements content on web sites and traditional media forums such as news papers, news and financial streams, and so on. Given such plethora of information there is a pressing need to cross reference information across textual services. For example, commonly we read a news item and we wonder if there are any blogs reporting related content or vice versa.", "num_citations": "127\n", "authors": ["2068"]}
{"title": "Reputation transferability in online labor markets\n", "abstract": " Online workplaces such as oDesk, Amazon Mechanical Turk, and TaskRabbit have been growing in importance over the last few years. In such markets, employers post tasks on which remote contractors work and deliver the product of their work online. As in most online marketplaces, reputation mechanisms play a very important role in facilitating transactions, since they instill trust and are often predictive of the employer\u2019s future satisfaction. However, labor markets are usually highly heterogeneous in terms of available task categories; in such scenarios, past performance may not be an accurate signal of future performance. To account for this natural heterogeneity, in this work, we build models that predict the performance of a worker based on prior, category-specific feedback. Our models assume that each worker has a category-specific quality, which is latent and not directly observable; what is observable\u00a0\u2026", "num_citations": "109\n", "authors": ["2068"]}
{"title": "To search or to crawl? Towards a query optimizer for text-centric tasks\n", "abstract": " Text is ubiquitous and, not surprisingly, many important applications rely on textual data for a variety of tasks. As a notable example, information extraction applications derive structured relations from unstructured text; as another example, focused crawlers explore the web to locate pages about specific topics. Execution plans for text-centric tasks follow two general paradigms for processing a text database: either we can scan, or'crawl,\" the text database or, alternatively, we can exploit search engine indexes and retrieve the documents of interest via carefully crafted queries constructed in task-specific ways. The choice between crawl-and query-based execution plans can have a substantial impact on both execution time and output\" completeness\"(eg, in terms of recall). Nevertheless, this choice is typically ad-hoc and based on heuristics or plain intuition. In this paper, we present fundamental building blocks to\u00a0\u2026", "num_citations": "104\n", "authors": ["2068"]}
{"title": "Managing crowdsourced human computation: A tutorial\n", "abstract": " The tutorial covers an emerging topic of wide interest: Crowdsourcing. Specifically, we cover areas of crowdsourcing related to managing structured and unstructured data in a web-related content. Many researchers and practitioners today see the great opportunity that becomes available through easily-available crowdsourcing platforms. However, most newcomers face the same questions: How can we manage the (noisy) crowds to generate high quality output? How to estimate the quality of the contributors? How can we best structure the tasks? How can we get results in small amounts of time and minimizing the necessary resources? How to setup the incentives? How should such crowdsourcing markets be setup? Their presented material will cover topics from a variety of fields, including computer science, statistics, economics, and psychology. Furthermore, the material will include real-life examples and case\u00a0\u2026", "num_citations": "87\n", "authors": ["2068"]}
{"title": "Cost-Effective Quality Assurance in Crowd Labeling\n", "abstract": " The emergence of online paid micro-crowdsourcing platforms, such as Amazon Mechanical Turk, allows on-demand and at-scale distribution of tasks to human workers around the world. In such settings, online workers come and complete small tasks posted by employers, working for as long or as little as they wish, a process that eliminates the overhead of hiring (and dismissal). This flexibility introduces a different set of inefficiencies: verifying the quality of every submitted piece of work is an expensive operation that often requires the same level of effort as performing the task itself. A number of research challenges arise in such settings. How can we ensure that the submitted work is accurate? What allocation strategies can be employed to make the best use of the available labor force? How can we appropriately assess the performance of individual workers? In this paper, we consider labeling tasks and develop a\u00a0\u2026", "num_citations": "85\n", "authors": ["2068"]}
{"title": "Beat the machine: Challenging workers to find the unknown unknowns\n", "abstract": " We present techniques for gathering data that expose errors of automatic predictive models. In certain common settings, traditional methods for evaluating predictive models tend to miss rare-but-important errors---most importantly, rare cases for which the model is confident of its prediction (but wrong). In this paper we present a system that, in a game-like setting, asks humans to identify cases what will cause the predictive-model-based system to fail. Such techniques are valuable in discovering problematic cases that do not reveal themselves during the normal operation of the system, and may include cases that are rare but catastrophic. We describe the design of the system, including design iterations that did not quite work. In particular, the system incentivizes humans to provide examples that are difficult for the model to handle, by providing a reward proportional to the magnitude of the predictive model's error. The humans are asked to``\\emph {Beat the Machine}''and find cases where the automatic model (``\\emph {the Machine}'') is wrong. Experiments show that the humans using Beat the Machine identify more errors than traditional techniques for discovering errors in from predictive models, and indeed, they identify many more errors where the machine is confident it is correct. Further, the cases the humans identify seem to be not simply outliers, butcoherent areas missed completely by the model. Beat the machine identifies the``unknown unknowns.''", "num_citations": "77\n", "authors": ["2068"]}
{"title": "Text joins for data cleansing and integration in an RDBMS\n", "abstract": " An organization's data records are often noisy because of transcription errors, incomplete information, lack of standard formats for textual data or combinations thereof. A fundamental task in a data cleaning system is matching textual attributes that refer to the same entity (e.g., organization name or address). This matching is effectively performed via the cosine similarity metric from the information retrieval field. For robustness and scalability, these \"text joins\" are best done inside an RDBMS, which is where the data is likely to reside. Unfortunately, computing an exact answer to a text join can be expensive. We propose an approximate, sampling-based text join execution strategy that can be robustly executed in a standard, unmodified RDBMS.", "num_citations": "74\n", "authors": ["2068"]}
{"title": "Beat the Machine: Challenging Humans to Find a Predictive Model\u2019s \u201cUnknown Unknowns\u201d\n", "abstract": " We present techniques for gathering data that expose errors of automatic predictive models. In certain common settings, traditional methods for evaluating predictive models tend to miss rare but important errors\u2014most importantly, cases for which the model is confident of its prediction (but wrong). In this article, we present a system that, in a game-like setting, asks humans to identify cases that will cause the predictive model-based system to fail. Such techniques are valuable in discovering problematic cases that may not reveal themselves during the normal operation of the system and may include cases that are rare but catastrophic. We describe the design of the system, including design iterations that did not quite work. In particular, the system incentivizes humans to provide examples that are difficult for the model to handle by providing a reward proportional to the magnitude of the predictive model's error. The\u00a0\u2026", "num_citations": "72\n", "authors": ["2068"]}
{"title": "Estimating the completion time of crowdsourced tasks using survival analysis models\n", "abstract": " In order to seamlessly integrate a human computation component (eg, Amazon Mechanical Turk) within a larger production system, we need to have some basic understanding of how long it takes to complete a task posted for completion in a crowdsourcing platform. We present an analysis of the completion time of tasks posted on Amazon Mechanical Turk, based on a dataset containing 165,368 HIT groups, with a total of 6,701,406 HITs, from 9,436 requesters, posted over a period of 15 months. We model the completion time as a stochastic process and build a statistical method for predicting the expected time for task completion. We use a survival analysis model based on Cox proportional hazards regression. We present the preliminary results of our work, showing how time-independent variables of posted tasks (eg, type of the task, price of the HIT, day posted, etc) affect completion time. We consider this a first step towards building a comprehensive optimization module that provides recommendations for pricing, posting time, in order to satisfy the constraints of the requester.", "num_citations": "69\n", "authors": ["2068"]}
{"title": "When one sample is not enough: Improving text database selection using shrinkage\n", "abstract": " Database selection is an important step when searching over large numbers of distributed text databases. The database selection task relies on statistical summaries of the database contents, which are not typically exported by databases. Previous research has developed algorithms for constructing an approximate content summary of a text database from a small document sample extracted via querying. Unfortunately, Zipf's law practically guarantees that content summaries built this way for any relatively large database will fail to cover many low-frequency words. Incomplete content summaries might negatively affect the database selection process, especially for short queries with infrequent words. To improve the coverage of approximate content summaries, we build on the observation that topically similar databases tend to have related vocabularies. Therefore, the approximate content summaries of topically\u00a0\u2026", "num_citations": "69\n", "authors": ["2068"]}
{"title": "Towards a theory model for product search\n", "abstract": " With the growing pervasiveness of the Internet, online search for products and services is constantly increasing. Most product search engines are based on adaptations of theoretical models devised for information retrieval. However, the decision mechanism that underlies the process of buying a product is different than the process of locating relevant documents or objects.", "num_citations": "68\n", "authors": ["2068"]}
{"title": "Bonus, Disclosure, and Choice: What Motivates the Creation of High-Quality Paid Reviews?\n", "abstract": " The emergence of online crowdsourcing sites has opened up new channels for third parties and companies to solicit paid reviews from people. In this paper, we investigate 1) how the introduction of monetary payments affects review quality, and 2) the impact of bonus rewards, sponsorship disclosure, and choice freedom on the quality of paid reviews. We conduct a 2\u00d7 2\u00d7 2 between-subjects experiment on Amazon Mechanical Turk. Our results indicate that there are no significant quality differences between paid and unpaid reviews. The quality of paid reviews improves by both the presence of additional performance-contingent rewards and the requirement to add disclosure text about material connections, and deteriorates by the restrictions imposed on the product set to be reviewed. These results have implications for websites and companies who are seeking legitimate reviews for their online products from paid workers.", "num_citations": "65\n", "authors": ["2068"]}
{"title": "The EconoMining project at NYU: Studying the economic value of user-generated content on the internet\n", "abstract": " An important use of the internet today is in providing a platform for consumers to disseminate information about products and services they buy, and share experiences about the merchants with whom they transact. Increasingly, online markets develop into social shopping channels, and facilitate the creation of online communities and social networks. Till date, businesses, government organisations and customers have not fully incorporated such information in their decision making and policy formulation processes, either because the potential value of the intellectual capital or appropriate techniques for measuring that value have not been identified. Increasingly, although, this publicly available digital content has concrete economic value that is often hidden beneath the surface. For example, online product reviews affect the buying behaviour of customers, as well as the volume of sales, positively or\u00a0\u2026", "num_citations": "65\n", "authors": ["2068"]}
{"title": "Modeling and managing changes in text databases\n", "abstract": " Large amounts of (often valuable) information are stored in web-accessible text databases. \u201cMetasearchers\u201d provide unified interfaces to query multiple such databases at once. For efficiency, metasearchers rely on succinct statistical summaries of the database contents to select the best databases for each query. So far, database selection research has largely assumed that databases are static, so the associated statistical summaries do not evolve over time. However, databases are rarely static and the statistical summaries that describe their contents need to be updated periodically to reflect content changes. In this article, we first report the results of a study showing how the content summaries of 152 real web databases evolved over a period of 52 weeks. Then, we show how to use \u201csurvival analysis\u201d techniques in general, and Cox's proportional hazards regression in particular, to model database changes over\u00a0\u2026", "num_citations": "65\n", "authors": ["2068"]}
{"title": "Modeling consumer footprints on search engines: An interplay with social media\n", "abstract": " It is now well understood that social media plays an increasingly important role in consumers\u2019 decision making. However, an overload of social media content in product search engines can hinder consumers from efficiently seeking information. We propose a structural econometric model to understand consumers\u2019 preferences and costs on search engines to improve user experience under unstructured social media. Our model combines an optimal stopping framework with an individual-level random utility choice model and analyzes click behavior in conjunction with purchase choices. Our model accounts for three major constraints in a consumer\u2019s decision-making process: (1) interdependency in decision making for different alternatives, (2) sequential arrival of information revealed by click-throughs, and (3) nonnegligible search cost. Our approach allows us to jointly estimate consumers\u2019 heterogeneous\u00a0\u2026", "num_citations": "59\n", "authors": ["2068"]}
{"title": "Faceted browsing over large databases of text-annotated objects\n", "abstract": " We demonstrate a fully working system for multifaceted browsing over large collections of text-annotated data, such as annotated images, that are stored in relational databases. Typically, such databases can be browsed across multiple facets (by topic, genre, location, and so on) and previous user studies showed that multifaceted interfaces improve substantially the ability of users to identify items of interest in the database. We demonstrate a scalable system that automatically generates multifaceted browsing hierarchies on top of a relational database that stores the underlying text-annotated objects. Our system supports a wide range of ranking alternatives for selecting and displaying the best facets and the best portions of the generated hierarchies, to facilitate browsing. We combine our ranking schemes with Rapid Serial Visual Presentation (RSVP), an advanced visualization technique, which further enhances\u00a0\u2026", "num_citations": "57\n", "authors": ["2068"]}
{"title": "Hiring behavior models for online labor markets\n", "abstract": " In an online labor marketplace employers post jobs, receive freelancer applications and make hiring decisions. These hiring decisions are based on the freelancer's observed (eg, education) and latent (eg, ability) characteristics. Because of the heterogeneity that appears in the observed characteristics, and the existence of latent ones, identifying and hiring the best possible applicant is a very challenging task. In this work we study and model the employer's hiring behavior. We assume that employers are utility maximizers and make rational decisions by hiring the best possible applicant at hand. Based on this premise, we propose a series of probabilistic models that estimate the hiring probability of each applicant. We train and test our models on more than 600,000 job applications obtained by oDesk. com, and we show evidence that the proposed models outperform currently in-use baselines. To get further\u00a0\u2026", "num_citations": "56\n", "authors": ["2068"]}
{"title": "Text joins for data cleansing and integration in a relational database management system\n", "abstract": " An organization's data records are often noisy: because of transcription errors, incomplete information, and lack of standard formats for textual data. A fundamental task during data cleansing and integration is matching strings\u2014perhaps across multiple relations\u2014that refer to the same entity (eg, organization name or address). Furthermore, it is desirable to perform this matching within an RDBMS, which is where the data is likely to reside. In this paper, We adapt the widely used and established cosine similarity metric from the information retrieval field to the relational database context in order to identify potential string matches across relations. We then use this similarity metric to characterize this key aspect of data cleansing and integration as a join between relations on textual attributes, where the similarity of matches exceeds a specified threshold. Computing an exact answer to the text join can be expensive. For\u00a0\u2026", "num_citations": "56\n", "authors": ["2068"]}
{"title": "Managing crowdsourcing workers\n", "abstract": " The emergence of online crowdsourcing services such as Amazon Mechanical Turk, presents us huge opportunities to distribute micro-tasks at an unprecedented rate and scale. Unfortunately, the high verification cost and the unstable employment relationship give rise to opportunistic behaviors of workers, which in turn exposes the requesters to quality risks. Currently, most requesters rely on redundancy to identify the correct answers. However, existing techniques cannot separate the true (unrecoverable) error rates from the (recoverable) biases that some workers exhibit, which would lead to incorrect assessment of worker quality. Furthermore, massive redundancy is expensive, increasing significantly the cost of crowdsourced solutions.In this paper, we present an algorithm that can easily separate the true error rates from the biases. Also, we describe how to seamlessly integrate the existence of \u201cgold\u201d data for learning the quality of workers. Next, we bring up an approach for actively testing worker quality in order to quicky identify spammers or malicious workers. Finally, we present experimental results to demonstrate the performance of our proposed algorithm.", "num_citations": "49\n", "authors": ["2068"]}
{"title": "Ridesharing and the use of public transportation\n", "abstract": " We investigate the effects of mobile-sourced ridesharing via platforms like Uber, Lyft, and Didi Chuxing on the use of public transit systems. Our study uses trip-level data about Uber usage in New York City, turnstile data about subway usage, and trip-level data about taxicab and shared bike usage. We find that on the surface, ridesharing and subway usage are positively correlated. Exploiting a series of exogenous shocks to the system\u2013the closing of subway stations\u2013to better isolate substitution effects, our preliminary results suggest that the average shock results in an increase of over 30% in the use of ridesharing, highlighting the potential for crowd-based systems to serve as infrastructure that helps smooth unexpected supply and demand surges. Our ongoing work studies how these substitution patterns vary with neighborhood socioeconomic indicators, and how substitution towards mobile-hailed ridesharing compares to traditional taxi and bike sharing. We hope to lay a data-driven foundation to better understand how sharing economy alternatives substitute and complement existing and future capital-intensive transit systems, and to provide a more judicious basis for assessing impacts on different population segments.", "num_citations": "48\n", "authors": ["2068"]}
{"title": "Getting more for less: Optimized crowdsourcing with dynamic tasks and goals\n", "abstract": " In crowdsourcing systems, the interests of contributing participants and system stakeholders are often not fully aligned. Participants seek to learn, be entertained, and perform easy tasks, which offer them instant gratification; system stakeholders want users to complete more difficult tasks, which bring higher value to the crowdsourced application. We directly address this problem by presenting techniques that optimize the crowdsourcing process by jointly maximizing the user longevity in the system and the true value that the system derives from user participation.", "num_citations": "46\n", "authors": ["2068"]}
{"title": "Towards a query optimizer for text-centric tasks\n", "abstract": " Text is ubiquitous and, not surprisingly, many important applications rely on textual data for a variety of tasks. As a notable example, information extraction applications derive structured relations from unstructured text; as another example, focused crawlers explore the Web to locate pages about specific topics. Execution plans for text-centric tasks follow two general paradigms for processing a text database: either we can scan, or \u201ccrawl,\u201d the text database or, alternatively, we can exploit search engine indexes and retrieve the documents of interest via carefully crafted queries constructed in task-specific ways. The choice between crawl- and query-based execution plans can have a substantial impact on both execution time and output \u201ccompleteness\u201d (e.g., in terms of recall). Nevertheless, this choice is typically ad hoc and based on heuristics or plain intuition. In this article, we present fundamental building blocks to\u00a0\u2026", "num_citations": "46\n", "authors": ["2068"]}
{"title": "A quality-aware optimizer for information extraction\n", "abstract": " A large amount of structured information is buried in unstructured text. Information extraction systems can extract structured relations from the documents and enable sophisticated, SQL-like queries over unstructured text. Information extraction systems are not perfect and their output has imperfect precision and recall (i.e., contains spurious tuples and misses good tuples). Typically, an extraction system has a set of parameters that can be used as \u201cknobs\u201d to tune the system to be either precision- or recall-oriented. Furthermore, the choice of documents processed by the extraction system also affects the quality of the extracted relation. So far, estimating the output quality of an information extraction task has been an ad hoc procedure, based mainly on heuristics. In this article, we show how to use Receiver Operating Characteristic (ROC) curves to estimate the extraction quality in a statistically robust way and show how\u00a0\u2026", "num_citations": "43\n", "authors": ["2068"]}
{"title": "Modeling query-based access to text databases\n", "abstract": " require access to such databases often resort to querying to extract relevant documents because of two main reasons. First, some text databases on the web are not\" crawlable,\" and hence the only way to retrieve their documents is via querying. Second, applications often require only a small fraction of a database's contents, so retrieving relevant documents via querying is an attractive choice from an efficiency viewpoint, even for crawlable databases. Often an application's query-based strategy starts with a small number of user-provided queries. Then, new queries are extracted--in an application-dependent way--from the documents in the initial query results, and the process iterates. The success of this common type of strategy relies on retrieved documents\" contributing\" new queries. If new documents fail to produce new queries, then the process might stall before all relevant documents are retrieved. In this paper, we develop a graph-based\" reachability\" metric that allows to characterize when an application's query-based strategy will successfully\" reach\" all documents that the application needs. We complement our metric with an efficient sampling-based technique that accurately estimates the reachability associated with a text database and an application's query-based strategy. We report preliminary experiments backing the usefulness of our metric and the accuracy of the associated estimation technique over real text databases and for two applications.", "num_citations": "43\n", "authors": ["2068"]}
{"title": "Trust, but verify: Predicting contribution quality for knowledge base construction and curation\n", "abstract": " The largest publicly available knowledge repositories, such as Wikipedia and Freebase, owe their existence and growth to volunteer contributors around the globe. While the majority of contributions are correct, errors can still creep in, due to editors' carelessness, misunderstanding of the schema, malice, or even lack of accepted ground truth. If left undetected, inaccuracies often degrade the experience of users and the performance of applications that rely on these knowledge repositories. We present a new method, CQUAL, for automatically predicting the quality of contributions submitted to a knowledge base. Significantly expanding upon previous work, our method holistically exploits a variety of signals, including the user's domains of expertise as reflected in her prior contribution history, and the historical accuracy rates of different types of facts. In a large-scale human evaluation, our method exhibits precision of\u00a0\u2026", "num_citations": "40\n", "authors": ["2068"]}
{"title": "The Need for Standardization in Crowdsourcing\n", "abstract": " Crowdsourcing has shown itself to be well-suited for the accomplishment of certain kinds of small tasks, yet many crowdsourceable tasks still require extensive structuring and managerial effort before using a crowd is feasible. We argue that this overhead could be substantially reduced via standardization. In the same way that task standardization enabled the mass production of physical goods, standardization of basic \u201cbuilding block\u201d tasks would make crowdsourcing more scalable. Standardization would make it easier to set prices, spread best practices, build meaningful reputation systems and track quality. All of this would increase the demand for paid crowdsourcing\u2014a development we argue is positive on both efficiency and welfare grounds. Standardization would also allow more complex processes to be built out of simpler tasks while still being able to predict quality, cost and time to completion. Realizing this vision will require interdisciplinary research effort as well as buy-in from online labor platforms.", "num_citations": "38\n", "authors": ["2068"]}
{"title": "Reputation premiums in electronic peer-to-peer markets: Analyzing textual feedback and network structure\n", "abstract": " Web-based systems that establish reputation are central to the viability of many electronic markets. We present theory that identifies the different dimensions of online reputation and characterizes their influence on the pricing power of sellers. We provide evidence that existing, numeric reputation scores conceal important seller-specific dimensions of reputation and we validate our theory further by proposing a new text mining technique that identifies and quantitatively evaluates further dimensions of importance in reputation profiles. We also suggest that the buyer-seller network contains critical reputation information that we can further exploit to improve the design of a reputation mechanism. Our experimental evaluation validates the predictions of our model using a new data set containing over 12,000 transactions for consumer software on Amazon. com's online secondary marketplace. This paper is the first\u00a0\u2026", "num_citations": "36\n", "authors": ["2068"]}
{"title": "Have you done anything like that? Predicting performance using inter-category reputation\n", "abstract": " Online labor markets such as oDesk and Amazon Mechanical Turk have been growing in importance over the last few years. In these markets, employers post tasks on which remote contractors work and deliver the product of their work. As in most online marketplaces, reputation mechanisms play a very important role in facilitating transactions, since they instill trust and are often predictive of the future satisfaction of the employer. However, labor markets are usually highly heterogeneous in terms of available task categories; in such scenarios, past performance may not be a representative signal of future performance. To account for this heterogeneity, in our work, we build models that predict the performance of a worker based on prior, category-specific feedback. Our models assume that each worker has a category-specific quality, which is latent and not directly observable; what is observable, though, is the set of\u00a0\u2026", "num_citations": "32\n", "authors": ["2068"]}
{"title": "System, method and computer accessible medium for determining one or more effects of rankings on consumer behavior\n", "abstract": " Exemplary systems, methods and computer-accessible mediums can be provided which can receive information related to a consumer (s), and determine the search behavior of the consumer (s) based on the information and using a consumer search model that is based on heterogeneous preferences and a search cost model of a second consumer (s).", "num_citations": "31\n", "authors": ["2068"]}
{"title": "SDLIP + STARTS = SDARTS: A protocol and toolkit for metasearching\n", "abstract": " In this paper we describe how we combined SDLIP and STARTS, two comple mentary protocols for searching over distributed document collections. The resulting protocol, which we call SDARTS, is simple yet expressible enough to enable building sophisticated metasearch engines. SDARTS can be viewed as an instantiation of SDLIP with metasearch-specific elements from STARTS. We also report on our experience building three SDARTS-compliant wrappers: for locally available plain-text document collections, for locally available XML document collections, and for external web-accessible collections. These wrappers were developed to be easily customizable for new collections. Our work was developed as part of Columbia University's Digital Libraries Initiative--Phase 2 (DLI2) project, which involves the departments of Computer Science, Medical Informatics, and Electrical Engineering, the Columbia\u00a0\u2026", "num_citations": "31\n", "authors": ["2068"]}
{"title": "Content and Context: Identifying the Impact of Qualitative Information on Consumer Choice\n", "abstract": " Managers and researchers alike suspect that the vast amounts of qualitative information found in blogs, product reviews, real estate listings, news stories, analyst reports and experts\u2019 advice influence consumer behavior. But, do these kinds of qualitative information impact or rather reflect consumer choices? We argue that message content and consumer choice are endogenous, and that non-random selection and the conflation of awareness and persuasion complicate causal estimation of the impact of message content on economic decisions and outcomes. Using data on the transcribed content of 2,397 stock recommendations provided by Jim Cramer on his CNBC show Mad Money from 2005 to 2008, combined with data on Internet search volume, the content of prior news, and prior stock price and trading volume data, we show that selection bias in the stocks Cramer chooses to recommend and prior product awareness on the part of his audience create measurable upward bias in estimates of the impact of Cramer\u2019s advice on stock prices. Using Latent Dirichlet Allocation (LDA) to characterize the topical content of Cramer\u2019s speech and the content of prior news, we show that he is less persuasive when he supports his recommendations with arguments that have themselves been recently mentioned in the news. We argue that the classic sales skill of \u201cknowing what a customer needs to hear\u201d can significantly enhance the influence of qualitative information precisely because what the consumer already knows affects how they evaluate messages. The tools and techniques we develop can be put to practical use in a variety of settings where\u00a0\u2026", "num_citations": "29\n", "authors": ["2068"]}
{"title": "Classification-aware hidden-web text database selection\n", "abstract": " Many valuable text databases on the web have noncrawlable contents that are \u201chidden\u201d behind search interfaces. Metasearchers are helpful tools for searching over multiple such \u201chidden-web\u201d text databases at once through a unified query interface. An important step in the metasearching process is database selection, or determining which databases are the most relevant for a given user query. The state-of-the-art database selection techniques rely on statistical summaries of the database contents, generally including the database vocabulary and associated word frequencies. Unfortunately, hidden-web text databases typically do not export such summaries, so previous research has developed algorithms for constructing approximate content summaries from document samples extracted from the databases via querying. We present a novel \u201cfocused-probing\u201d sampling algorithm that detects the topics covered in\u00a0\u2026", "num_citations": "29\n", "authors": ["2068"]}
{"title": "Automatic classification of text databases through query probing\n", "abstract": " Many text databases on the web are \u201chidden\u201d behind search interfaces, and their documents are only accessible through querying. Traditional search engines typically ignore the contents of such searchonly databases. Recently, Yahoo-like directories have started to manually organize these databases into categories that users can browse to find these valuable resources. We propose a novel strategy to automate the classification of search-only text databases. Our technique starts by training a rule-based document classifier, and then uses the classifier\u2019s rules to generate probing queries. The queries are sent to the text databases, which are then classified based on the number of matches that they produce for each query. We report some initial exploratory experiments that show that our approach is promising to automatically characterize the contents of text databases accessible on the web.", "num_citations": "29\n", "authors": ["2068"]}
{"title": "Crowdsourcing using mechanical turk: quality management and scalability\n", "abstract": " \u25cf When spammer says G, it is 25% G, 25% P, 25% R, 25% X\u25cf When spammer says P, it is 25% G, 25% P, 25% R, 25% X\u25cf When spammer says R, it is 25% G, 25% P, 25% R, 25% X\u25cf When spammer says X, it is 25% G, 25% P, 25% R, 25% X [note: assume equal priors]", "num_citations": "27\n", "authors": ["2068"]}
{"title": "Facilitating Document Annotation using Content and Querying Value\n", "abstract": " A large number of organizations today generate and share textual descriptions of their products, services, and actions. Such collections of textual data contain significant amount of structured information, which remains buried in the unstructured text. While information extraction algorithms facilitate the extraction of structured relations, they are often expensive and inaccurate, especially when operating on top of text that does not contain any instances of the targeted structured information. We present a novel alternative approach that facilitates the generation of the structured metadata by identifying documents that are likely to contain information of interest and this information is going to be subsequently useful for querying the database. Our approach relies on the idea that humans are more likely to add the necessary metadata during creation time, if prompted by the interface; or that it is much easier for humans (and\u00a0\u2026", "num_citations": "26\n", "authors": ["2068"]}
{"title": "Building query optimizers for information extraction: The SQoUT project\n", "abstract": " Text documents often embed data that is structured in nature. This structured data is increasingly exposed using information extraction systems, which generate structured relations from documents, introducing an opportunity to process expressive, structured queries over text databases. This paper discusses our SQoUT1 project, which focuses on processing structured queries over relations extracted from text databases. We show how, in our extraction-based scenario, query processing can be decomposed into a sequence of basic steps: retrieving relevant text documents, extracting relations from the documents, and joining extracted relations for queries involving multiple relations. Each of these steps presents different alternatives and together they form a rich space of possible query execution strategies. We identify execution efficiency and output quality as the two critical properties of a query execution, and\u00a0\u2026", "num_citations": "23\n", "authors": ["2068"]}
{"title": "Creativity on paid crowdsourcing platforms\n", "abstract": " Crowdsourcing platforms are increasingly being harnessed for creative work. The platforms' potential for creative work is clearly identified, but the workers' perspectives on such work have not been extensively documented. In this paper, we uncover what the workers have to say about creative work on paid crowdsourcing platforms. Through a quantitative and qualitative analysis of a questionnaire launched on two different crowdsourcing platforms, our results revealed clear differences between the workers on the platforms in both preferences and prior experience with creative work. We identify common pitfalls with creative work on crowdsourcing platforms, provide recommendations for requesters of creative work, and discuss the meaning of our findings within the broader scope of creativity-oriented research. To the best of our knowledge, we contribute the first extensive worker-oriented study of creative work on\u00a0\u2026", "num_citations": "19\n", "authors": ["2068"]}
{"title": "A system for scalable and reliable technical-skill testing in online labor markets\n", "abstract": " The emergence of online labor platforms, online crowdsourcing sites, and even Massive Open Online Courses (MOOCs), has created an increasing need for reliably evaluating the skills of the participating users (e.g., \u201cdoes a candidate know Java\u201d) in a scalable way. Many platforms already allow job candidates to take online tests to assess their competence in a variety of technical topics. However the existing approaches face many problems. First, cheating is very common in online testing without supervision, as the test questions often \u201cleak\u201d and become easily available online along with the answers. Second, technical-skills, such as programming, require the tests to be frequently updated in order to reflect the current state-of-the-art. Third, there is very limited evaluation of the tests themselves, and how effectively they measure the skill that the users are tested for.In this article we present a platform, which\u00a0\u2026", "num_citations": "19\n", "authors": ["2068"]}
{"title": "The computer is the new sewing machine: benefits and perils of crowdsourcing\n", "abstract": " There is increased participation by the developing world in the global manufacturing marketplace: the sewing machine in Bangladesh can be a means to support an entire family. Crowdsourcing for cognitive tasks consists of asking humans for questions that are otherwise impossible to answer by algorithms, eg, is this image pornographic, are these two addresses the same, what is the translation for this text in French? In the last five years, there has been an exponential growth in the size of the global cognitive marketplace: Amazon. com's Mechanical Turk has an estimated 500,000 active workers in over 100 countries, and there are dozens of other companies in this space. This turns the computer into a modern-day sewing machine, where cognitive work of various levels of difficulty will pay anywhere from 5 to 50 dollars a day. Unlike outsourcing, which usually requires college education, competence at these\u00a0\u2026", "num_citations": "19\n", "authors": ["2068"]}
{"title": "Task-agnostic integration of human and machine intelligence\n", "abstract": " A system combines inputs from human processing and machine processing, and employs machine learning to improve processing of individual tasks based on comparison of human processing results. Once performance of a particular task by machine processing reaches a threshold, the level of human processing used on that task is reduced.", "num_citations": "17\n", "authors": ["2068"]}
{"title": "Using twitter to predict sales: A case study\n", "abstract": " This paper studies the relation between activity on Twitter and sales. While research exists into the relation between Tweets and movie and book sales, this paper shows that the same relations do not hold for products that receive less attention on social media. For such products, classification of Tweets is far more important to determine a relation. Also, for such products advanced statistical relations, in addition to correlation, are required to relate Twitter activity and sales. In a case study that involves Tweets and sales from a company in four countries, the paper shows how, by classifying Tweets, such relations can be identified. In particular, the paper shows evidence that positive Tweets by persons (as opposed to companies) can be used to forecast sales and that peaks in positive Tweets by persons are strongly related to an increase in sales. These results can be used to improve sales forecasts and to increase sales in marketing campaigns.", "num_citations": "17\n", "authors": ["2068"]}
{"title": "Ranked queries over sources with boolean query interfaces without ranking support\n", "abstract": " Many online or local data sources provide powerful querying mechanisms but limited ranking capabilities. For instance, PubMed allows users to submit highly expressive Boolean keyword queries, but ranks the query results by date only. However, a user would typically prefer a ranking by relevance, measured by an Information Retrieval (IR) ranking function. The naive approach would be to submit a disjunctive query with all query keywords, retrieve the returned documents, and then re-rank them. Unfortunately, such an operation would be very expensive due to the large number of results returned by disjunctive queries. In this paper we present algorithms that return the top results for a query, ranked according to an IR-style ranking function, while operating on top of a source with a Boolean query interface with no ranking capabilities (or a ranking capability of no interest to the end user). The algorithms generate a\u00a0\u2026", "num_citations": "16\n", "authors": ["2068"]}
{"title": "Identifying effective crowdsource contributors and high quality contributions\n", "abstract": " Systems and methods are disclosed for targeting effective contributors and identifying high quality contributions. For example, a method may include displaying an advertisement to a potential contributor via an advertising platform, receiving an indication that the potential contributor responded to the advertisement, generating a crowdsourcing exercise that is presented to the contributor, receiving a response (a conversion event) from the contributor to the crowdsourcing exercise, and notifying the advertising platform about the conversion event. As another example, a method may include determining a concept space for a new contribution, obtaining previously correct and incorrect contributions of the contributor in the concept space, and determining an expertise confidence score for the new contribution based on a comparison of the new contribution with the previously correct and incorrect contributions. The\u00a0\u2026", "num_citations": "15\n", "authors": ["2068"]}
{"title": "PERSIVAL demo: Categorizing hidden-Web resources\n", "abstract": " The information available in electronic form continues to grow at an exponential rate and this trend is expected to continue. Although traditional search engines like AltaVista can address common information needs, they ignore the often valuable information that is\" hidden\" behind search interfaces, the so-called\" hidden web.\" Automating the classification of\" hidden web\" resources is challenging, since the contents of these collections are available only by querying, not by traditional crawling. For example, consider the PubMed medical database from the National Library of Medicine, which stores medical bibliographic information and links to full-text journals accessible through the web. This database is accessible through a query interface1. A query to PubMed with keyword\" cancer\" returns 1,313,266 matches, which are high-quality citations to medical articles, stored locally at the PubMed site. The contents of\u00a0\u2026", "num_citations": "15\n", "authors": ["2068"]}
{"title": "Query- vs. Crawling-based Classification of Searchable Web Databases\n", "abstract": " The World-Wide Web is one of the main channels through which people currently exchange information. Unfortunately, this information is not characterized in a way that would make its semantics readily understandable by computers, which complicates building value-added services on top of the existing information. An ambitious effort that aims to facilitate the development of such services is the so-called \u201cSemantic Web.\u201d According to Berners-Lee et al.[1]:", "num_citations": "14\n", "authors": ["2068"]}
{"title": "STEP: A Scalable Testing and Evaluation Platform\n", "abstract": " The emergence of online crowdsourcing sites, online work platforms, and evenMassive Open Online Courses (MOOCs), has created an increasing need for reliably evaluating the skills of the participating users in a scalable way. Many platforms already allow users to take online tests and verify their skills, but the existing approaches face many problems. First of all, cheating is very common in online testing without supervision, as the test questions often\" leak\" and become easily available online together with the answers. Second, technical skills, such as programming, require the tests to be frequently updated in order to reflect the current state-of-the-art. Third, there is very limited evaluation of the tests themselves, and how effectively they measure the skill that the users are tested for. In this paper, we present a Scalable Testing and Evaluation Platform (STEP), that allows continuous generation and evaluation of test questions. STEP leverages already available content, on Question Answering sites such as StackOverflow and re-purposes these questions to generate tests. The system utilizes a crowdsourcing component for the editing of the questions, while it uses automated techniques for identifying promising QA threads that can be successfully re-purposed for testing. This continuous question generation decreases the impact of cheating and also creates questions that are closer to the real problems that the skill holder is expected to solve in real life. STEP also leverages the use of Item Response Theory to evaluate the quality of the questions. We also use external signals about the quality of the workers. These identify the questions that have\u00a0\u2026", "num_citations": "13\n", "authors": ["2068"]}
{"title": "Relevance-based Retrieval on Hidden-Web Text Databases without Ranking Support\n", "abstract": " Many online or local data sources provide powerful querying mechanisms but limited ranking capabilities. For instance, PubMed allows users to submit highly expressive Boolean keyword queries, but ranks the query results by date only. However, a user would typically prefer a ranking by relevance, measured by an information retrieval (IR) ranking function. A naive approach would be to submit a disjunctive query with all query keywords, retrieve all the returned matching documents, and then rerank them. Unfortunately, such an operation would be very expensive due to the large number of results returned by disjunctive queries. In this paper, we present algorithms that return the top results for a query, ranked according to an IR-style ranking function, while operating on top of a source with a Boolean query interface with no ranking capabilities (or a ranking capability of no interest to the end user). The algorithms\u00a0\u2026", "num_citations": "13\n", "authors": ["2068"]}
{"title": "System, method, software arrangement and computer-accessible medium for incorporating qualitative and quantitative information into an economic model\n", "abstract": " A system, method, software arrangement and computer-accessible medium can be provided for incorporating quantitative and qualitative information into an economic model is provided. An exemplary method for analyzing qualitative information associated with a characteristic of at least one entity based on associated quantitative information, includes, obtaining first information which contains at least in part a qualitative information relating to at least one of the at least one entity; determining second information associated with at least one attribute of the characteristic obtained from the first information; obtaining third information which contains at least in part quantitative information associated with at least one of at least one entity; and establishing fourth information as a function of the second information and the third information to determine which of at least one attribute affects the characteristic. For example, an\u00a0\u2026", "num_citations": "13\n", "authors": ["2068"]}
{"title": "Stay Elsewhere? Improving Local Search for Hotels Using Econometric Modeling and Image Classification.\n", "abstract": " One of the common Web searches that have a strong local component is the search for hotel accommodation. Customers try to identify hotels that satisfy particular criteria, such as service, food quality, and so on. Unfortunately, today, the travel search engines provide only rudimentary ranking facilities, typically using a single ranking criterion such as distance from city center, number of stars, price per night, or, more recently, customer reviews. This approach has obvious shortcomings. First, it ignores the multidimensional preferences of the consumer and, second, it largely ignores characteristics related to the location of the hotel, for instance, proximity to the beach or proximity to a downtown shopping area. These location-based features represent important characteristics that influence the desirability of a particular hotel. However, currently there are no established metrics that can isolate the importance of the location characteristics of hotels. In our work, we use the fact that the overall desirability of the hotel is reflected in the price of the rooms; therefore, using hedonic regressions, an established technique from econometrics, we estimate the weight that consumers place on different hotel characteristics. Furthermore, since some location-based characteristics, such as proximity to the beach, are not directly measurable, we use image classification techniques to infer such features from the satellite images of the area. Our technique is validated on a unique panel dataset consisting of 9463 different hotels located in the United States, observed over a period of 5 months. The final outcome of our analysis allows us to compute the \u201cresidual value\u201d of a\u00a0\u2026", "num_citations": "13\n", "authors": ["2068"]}
{"title": "The Impact of Information Disclosure on Stock Market Returns: The Sarbanes-Oxley Act and the Role of Media as an Information Intermediary.\n", "abstract": " The Sarbanes-Oxley (SOX) Act of 2002 is one of the, if not the, most important pieces of legislation affecting corporations traded on the US stock exchanges. While SOX does not explicitly address the issue of information security, the definition of internal control provided by the SEC, combined with the fact that the reporting systems in all firms required to comply with SOX are based on systems that promote information security and integrity does imply that more focus on information security is a necessary compliance requirement. Using a dataset on stock market abnormal returns that runs from the period 2000-2006 and consists of 300 firms, we aim to examine how the stock market reaction varies for 8-K filings and news media releases, and how this reaction has changed since the passage of the SOX Act. We hypothesize that the greater timeliness of the 8-K filings induced by SOX increases and accelerates the quality of their information disclosure and dissemination in the market. Further, we classify news articles into press-and firm-initiated articles and hypothesize that the press-initiated coverage of material events has increased in the post-SOX period. We find that the effect of firm-initiated media coverage had significant negative impact relative to press-initiated coverage on the measures of informativeness suggesting that media played a significant role during the scandal-ridden periods when the firms had poor information environment between 2002 and 2004. We also find that the timeliness of release of media articles determines the level of informativeness, suggesting that media is an information intermediary and its role acts as a substitute\u00a0\u2026", "num_citations": "12\n", "authors": ["2068"]}
{"title": "Classifying and searching hidden-web text databases\n", "abstract": " The World-Wide Web continues to grow rapidly, which makes exploiting all available information a challenge. Search engines such as Google index an unprecedented amount of information, but still do not provide access to valuable content in text databases \u201chidden\u201d behind search interfaces. For example, current search engines largely ignore the contents of the Library of Congress, the US Patent and Trademark database, newspaper archives, and many other valuable sources of information because their contents are not \u201ccrawlable.\u201d However, users should be able to find the information that they need with as little effort as possible, regardless of whether this information is crawlable or not. As a significant step towards this goal, we have designed algorithms that support browsing and searching\u2014the two dominant ways of finding information on the web\u2014over \u201chidden-web\u201d text databases.", "num_citations": "11\n", "authors": ["2068"]}
{"title": "Extending SDARTS: Extracting metadata from web databases and interfacing with the Open Archives Initiative\n", "abstract": " SDARTS is a protocol and toolkit designed to facilitate metasearching. SDARTS combines two complementary existing protocols, SDLIP and STARTS, to define a uniform interface that collections should support for searching and exporting metasearch-related metadata. SDARTS also includes a toolkit with wrappers that are easily customized to make both local and remote document collections SDARTS-compliant. This paper describes two significant ways in which we have extended the SDARTS toolkit. First, we have added a tool that automatically builds rich content summaries for remote web collections bym probing the collections with appropriate queries. These content summaries can then be used by a metasearcher to select over which collections to evaluate a given query. Second, we have enhanced the SDARTS toolkit so that all SDARTS-compliant collections export their metadata under the emerging\u00a0\u2026", "num_citations": "11\n", "authors": ["2068"]}
{"title": "Efficient Filtering on Hidden Document Streams\n", "abstract": " Many online services like Twitter and GNIP offer streaming programming interfaces that allow real-time information filtering based on keyword or other conditions. However, all these services specify strict access constraints, or charge a cost based on the usage. We refer to such streams as``hidden streams''to draw a parallel to the well-studied hidden Web, which similarly restricts access to the contents of a database through a querying interface. At the same time, the users' interest is often captured by complex classification models that, implicitly or explicitly, specify hundreds of keyword-based rules, along with the rules' accuracies. In this paper, we study how to best utilize a constrained streaming access interface to maximize the number of retrieved relevant items, with respect to a classifier, expressed as a set of rules. We consider two problem variants. The static version assumes that the popularity of the keywords is known and constant across time. The dynamic version lifts this assumption, and can be viewed as an exploration-vs.-exploitation problem. We show that both problems are NP-hard, and propose exact and bounded approximation algorithms for various settings, including various access constraint types. We experimentally evaluate our algorithms on real Twitter data.", "num_citations": "10\n", "authors": ["2068"]}
{"title": "Search Less, Find More? Examining Limited Consumer Search with Social Media and Product Search Engines\n", "abstract": " With the proliferation of social media, consumers' cognitive costs during information-seeking can become non-trivial during an online shopping session. We propose a dynamic structural model of limited consumer search thatcombines an optimal stopping framework with an individual-level choice model. We estimate the parameters of the model using a dataset of approximately 1 million online search sessions resulting in bookings in 2117 US hotels. The model allows us to estimate the monetary value of the the search costs incurred by users of product search engines in a social media context. On average, searching an extra page on a search engine costs consumers $39.15 and examining an additional offer within the same page has a cost of $6.24, respectively. A good recommendation saves consumers, on average, $9.38, whereas a bad one costs $18.54. Our policy experiment strongly supports this finding by showing that the quality of ranking can have significant impact on consumers\u2019 search efforts, and customized ranking recommendations tend to polarize the distribution of consumer search intensity. Our model-fit comparison demonstrates that the dynamic search model provides the highest overall predictive power compared to the baseline static models. Our dynamic model indicates that consumers have lower price sensitivity than a static model would have predicted, implying that consumers pay a lot of attention to nonprice factors during an online hotel search.", "num_citations": "10\n", "authors": ["2068"]}
{"title": "Approximate String Joins in a Database (Almost) for Free--Erratum\n", "abstract": " In [GIJ+01a, GIJ+01b] we described how to use q-grams in an RDBMS to perform approximate string joins. We also showed how to implement the approximate join using plain SQL queries. Specifically, we described three filters, count filter, position filter, and length filter, which can be used to execute efficiently the approximate join. The intuition behind the count filter was that strings that are similar have many q-grams in common. In particular, two strings s1 and s2 can have up to max {| s1|,| s2|}+ q\u2212 1 common q-grams. When s1= s2, they have exactly that many q-grams in common. When s1 and s2 are within edit distance k, they share at least (max {| s1|,| s2|}+ q\u2212 1)\u2212 kq q-grams, since kq is the maximum number of q-grams that can be affected by k edit distance operations. We implemented count filter in the HAVING clause of the SQL statement in Figure 1. String pairs without enough q-grams in common are filtered out from the result. Unfortunately, this implementation of the count filter is problematic when kq is greater than or equal to max {| s1|,| s2|}+ q\u2212 1. In this case, two strings can be within edit distance k and still not share any q-grams. In such a case, the SQL statement in Figure 1 will fail to identify s1 and s2 as being within edit distance k, since there will be no q-grams from this string pair to join and count. Hence, in this case the result returned by the Figure 1 query is incomplete and suffers from \u201cfalse negatives,\u201d in contrast to our claim to the contrary in [GIJ+01a, GIJ+01b].In general, the string pairs that are omitted are pairs of short strings. Even when these strings match within small edit distance, the match tends to be meaningless (eg,\u201cIBM\u00a0\u2026", "num_citations": "10\n", "authors": ["2068"]}
{"title": "Methods, systems, and media for identifying errors in predictive models using annotators\n", "abstract": " Methods, systems, and media for identifying errors in predictive models using annotators are provided. In some embodiments, a method for evaluating predictive models in classification systems is provided, the method comprising: causing an input region to be presented to a user, where the input region receives an instance from the user that corresponds to a predictive model; retrieving a classification conducted by the predictive model for the received instance and a confidence value associated with the classification; determining whether the received instance has been incorrectly classified by the predictive model; determining a reward associated with the incorrect classification made by the predictive model in response to determining that the received instance has been incorrectly classified by the predictive model, where the reward is based on the confidence value associated with the classification of the\u00a0\u2026", "num_citations": "9\n", "authors": ["2068"]}
{"title": "Designing ranking systems for consumer reviews: The economic impact of customer sentiment in electronic markets\n", "abstract": " With the rapid growth of the Internet, users\u2019 ability to publish content has created active electronic communities that provide a wealth of product information in the form of product reviews. However, the high volume of reviews that are typically published for a single product also have the potential to make it harder for individuals to evaluate the true underlying quality of the product based on the reviews. In such situations, the numeric data based on the average star rating of a product or on the number of reviews may not convey a lot of information on its own to a prospective buyer, and in fact buyers may naturally gravitate to reading reviews in order to come to a decision regarding the product. We conjecture that the textual content of each review may be playing an important role in influencing consumer purchase decisions and thereby affecting actual sales of the product. Hence, in this paper we investigate the veracity\u00a0\u2026", "num_citations": "7\n", "authors": ["2068"]}
{"title": "Examining the Impact of Search Engine Ranking and Personalization on Consumer Behavior: Combining Bayesian Modeling with Randomized Field Experiments\n", "abstract": " In this paper, we examine how different ranking and personalization mechanisms on product search engines influence consumer online search and purchase behavior. To investigate these effects, we combine archival data analysis with randomized field experiments. Our archival data analysis is based on a unique dataset containing approximately 1 million online sessions from Travelocity over a 3-month period. Using a hierarchical Bayesian model, we first jointly estimate the relationship among consumer click and purchase behavior, and search engine ranking decisions. To evaluate the causal effect of search engine interface on user behavior, we conduct randomized field experiments. The field experiments are based on a real-world hotel search engine application designed and built by us. By manipulating the default ranking method of search results, and by enabling or disabling a variety of personalization features on the hotel search engine website, we are able to empirically identify the causal impact of search engines on consumers\u2019 online click and purchase behavior.The archival data analysis and the randomized experiments are consistent in demonstrating that ranking has a significant effect on consumer click and purchase behavior. We find that hotels with a higher reputation for providing superior services are more adversely affected by an inferior screen position. In addition, a consumer utility-based ranking mechanism yields the highest click and purchase propensities in comparison to existing benchmark systems such as ranking based on price or customer ratings. Our randomized experiments on the impact of active vs. passive\u00a0\u2026", "num_citations": "6\n", "authors": ["2068"]}
{"title": "A report on the human computation workshop (HComp 2009)\n", "abstract": " The first Human Computation Workshop (HComp2009) was held on June 28th, 2009, in Paris, France, collocated with SIGKDD 2009. This report summarizes the workshop, with details of the papers, demos and posters presented. The report also includes common themes, issues, and open questions that came up in the workshop.", "num_citations": "6\n", "authors": ["2068"]}
{"title": "Modeling volatility in prediction markets\n", "abstract": " There is significant experimental evidence that prediction markets are efficient mechanisms for aggregating information and are more accurate in forecasting events than traditional forecasting methods, such as polls. Interpretation of prediction market prices as probabilities has been extensively studied in the literature. However there is little research on the volatility of prediction market prices. Given that volatility is fundamental in estimating significance of price movements, it is important to have a better understanding of the volatility of the contract prices.", "num_citations": "6\n", "authors": ["2068"]}
{"title": "Summarizing and searching hidden-web databases hierarchically using focused probes\n", "abstract": " Many valuable text databases on the web have non-crawlable contents that are \u201chidden\u201d behind search interfaces. Metasearchers are helpful tools for searching over many such databases at once through a unified query interface. A critical task for a metasearcher to process a query efficiently and effectively is the selection of the most promising databases for the query, a task that typically relies on statistical summaries of the database contents. Unfortunately, web-accessible text databases do not generally export content summaries. In this paper, we present an algorithm to derive content summaries from \u201cuncooperative\u201d databases by using \u201cfocused query probes,\u201d which adaptively zoom in on and extract documents that are representative of the topic coverage of the databases. The content summaries that result from this algorithm are efficient to derive and more accurate than those from previously proposed probing techniques for content-summary extraction. We also present a novel database selection algorithm that exploits both the extracted content summaries and a hierarchical classification of the databases, automatically derived during probing, to produce accurate results even for imperfect content summaries. Finally, we evaluate our techniques thoroughly using a variety of databases, including 50 real web-accessible text databases.", "num_citations": "5\n", "authors": ["2068"]}
{"title": "Gender and Race Preferences in Hiring in the Age of Diversity Goals: Evidence from Silicon Valley Tech Firms\n", "abstract": " We study the heterogeneous effects of race and gender on hiring outcomes in the context of organizational diversity efforts. Against the backdrop of increasing scrutiny around diversity issues in tech companies and the concomitant growing response of organizational efforts to increase workforce diversity, we revisit the age-old question of whether race and gender preferences (continue to) exist in hiring decisions. We address this question using two novel, large-scale datasets: Applicant Tracking System data from 8 Silicon Valley firms containing nearly 900k applicants, and a LinkedIn dataset containing 300 million public LinkedIn profiles. Using matched sample analyses and controlling for a rich set of job and applicant attributes found in applicants\u2019 resumes and LinkedIn profiles, we find that women are 9-10% more likely to receive a callback compared to men, whereas Black, Hispanic, and Asian applicants are 8-13% less likely to receive a callback compared to White applicants. These outcome gaps do not cancel-out in the later stages, as female and White applicants are more likely to receive an interview and offer. To further address endogeneity concerns, we perform quasi-experimental analysis involving applicants whose race and gender are ambiguous to the recruiter in the initial application review stage, but are later revealed in the phone screen stage. We find that ambiguity in applicants\u2019 race and gender attenuates the main effects of race and gender on receiving a callback\u2013that is, the outcome gap in callback disappears for applicants whose race and gender are ambiguous to the recruiter. We discuss these results in light of theories of\u00a0\u2026", "num_citations": "4\n", "authors": ["2068"]}
{"title": "Demand-Aware Career Path Recommendations: A Reinforcement Learning Approach\n", "abstract": " A skill\u2019s value depends on dynamic market conditions. To remain marketable, contractors need to keep reskilling themselves continuously. But choosing new skills to learn is an inherently hard task: Contractors have very little information about current and future market conditions, which often results in poor learning choices. Recommendation frameworks could reduce uncertainty in learning choices. However, conventional approaches would likely be inefficient; they would model previous (often poor) observed contractor learning behaviors to provide future career path recommendations while ignoring current market trends. This work proposes a framework that combines reinforcement learning, Bayesian inference, and gradient boosting to provide recommendations on how contractors should behave when choosing new skills to learn. Compared with standard recommender systems, this framework does not learn\u00a0\u2026", "num_citations": "4\n", "authors": ["2068"]}
{"title": "A demo search engine for products\n", "abstract": " Most product search engines today build on models of relevance devised for information retrieval. However, the decision mechanism that underlies the process of buying a product is different than the process of locating relevant documents or objects. We propose a theory model for product search based on expected utility theory from economics. Specifically, we propose a ranking technique in which we rank highest the products that generate the highest surplus, after the purchase. We instantiate our research by building a demo search engine for hotels that takes into account consumer heterogeneous preferences, and also accounts for the varying hotel price. Moreover, we achieve this without explicitly asking the preferences or purchasing histories of individual consumers but by using aggregate demand data. This new ranking system is able to recommend consumers products with\" best value for money\" in a\u00a0\u2026", "num_citations": "4\n", "authors": ["2068"]}
{"title": "Statistical considerations for crowdsourced perceptual ratings of human speech productions\n", "abstract": " Crowdsourcing has become a major tool for scholarly research since its introduction to the academic sphere in 2008. However, unlike in traditional laboratory settings, it is nearly impossible to control the conditions under which workers on crowdsourcing platforms complete tasks. In the study of communication disorders, crowdsourcing has provided a novel solution to the collection of perceptual ratings of human speech production. Such ratings allow researchers to gauge whether a treatment yields meaningful change in how human listeners' perceive disordered speech. This paper will explore some statistical considerations of crowdsourced data with specific focus on collecting perceptual ratings of human speech productions. Random effects models are applied to crowdsourced perceptual ratings collected in both a continuous and binary fashion. A simulation study is conducted to test the reliability of the\u00a0\u2026", "num_citations": "3\n", "authors": ["2068"]}
{"title": "System, method and computer-accessible medium for scalable testing and evaluation\n", "abstract": " An exemplary system, method and computer-accessible medium can be provided that can be used, for example, for evaluating a test question (s) for a test (s), which can include receiving information related to a content (s), mapping the content (s) to a skill (s), and evaluating the content (s) as the test question (s) so as to test an ability of user (s) at the skill (s).", "num_citations": "3\n", "authors": ["2068"]}
{"title": "The Computer is the New Sewing Machine: Benefits and Perils of Crowdsourcing\n", "abstract": " The Computer is the New Sewing Machine: Benefits and Perils of Crowdsourcing Page 1 Panel The Computer is the New Sewing Machine: Benefits and Perils of Crowdsourcing Moderator: Praveen Paritosh Google Panelists/Participants Matt Cooper oDesk Panos Ipeirotis NYU Siddharth Suri Yahoo! Research There is increased participation by the developing world in the global manufacturing marketplace: the sewing machine in Bangladesh can be a means to support an entire family. Crowdsourcing for cognitive tasks consists of asking humans for questions that are otherwise impossible to answer by algorithms, eg, is this image pornographic, are these two addresses the same, what is the translation for this text in French? In the last five years, there has been an exponential growth in the size of the global cognitive marketplace: Amazon.com's Mechanical Turk has an estimated 500,000 active workers in over 100 \u2026", "num_citations": "2\n", "authors": ["2068"]}
{"title": "Towards Automating the Pricing Power of Product Attributes: An Analysis of Online Product Reviews\n", "abstract": " The Internet has had a profound impact on at least two areas of life\u2013the way people shop, and the way they exchange information. Both are relevant to consumer product reviews posted in IT-enabled electronic markets. Online consumer product reviews provide information that can facilitate economic exchange, which is the main purpose of electronic marketplaces. In particular, many users like to learn about the experiences of other customers with a product before purchasing the product. Online product reviews have been shown to influence product sales such as books and movies (Chevalier & Mayzlin 2006). Similarly, the volume of discussion about a product in blogs has recently been shown to correlate with the product's financial performance (Gruhl, Guha, Kumar, Novak & Tomkins 2005). We aim to extend these studies in different ways.At a broader level, we plan to empirically estimate how the textual content\u00a0\u2026", "num_citations": "2\n", "authors": ["2068"]}
{"title": "WWW'18: Proceedings of the 2018 World Wide Web Conference\n", "abstract": " WWW '18: Proceedings of the 2018 World Wide Web Conference - Archive ouverte HAL Acc\u00e9der directement au contenu Acc\u00e9der directement \u00e0 la navigation Toggle navigation CCSD HAL HAL HALSHS TEL M\u00e9diHAL Liste des portails AUR\u00e9HAL API Data Documentation Episciences.org Episciences.org Revues Documentation Sciencesconf.org Support hal Accueil D\u00e9p\u00f4t Consultation Les derniers d\u00e9p\u00f4ts Par type de publication Par discipline Par ann\u00e9e de publication Par structure de recherche Les portails de l'archive Recherche Documentation hal-01907226, version 1 Direction d'ouvrage, Proceedings, Dossier WWW '18: Proceedings of the 2018 World Wide Web Conference Pierre-Antoine Champin 1 Fabien Gandon 2 Lionel M\u00e9dini 1 Mounia Lalmas 3 Panagiotis Ipeirotis 4 D\u00e9tails 1 TWEAK - Traces, Web, Education, Adaptation, Knowledge LIRIS - Laboratoire d'InfoRmatique en Image et Syst\u00e8mes d'information \u2026", "num_citations": "1\n", "authors": ["2068"]}
{"title": "Detecting employee misconduct and malfeasance\n", "abstract": " In the United States financial firms have the regulatory obligation to monitor the communications of their employees (eg, emails, chats, phone calls) in order to detect misconduct. Some forms of misconduct are illegal activities (eg, insider trading, bribery) while others are policy violations (eg, improper security practices, or inappropriate language use). Traditionally, firms have deployed relatively simple rule-based systems for employee surveillance. Such systems generate many false positive alerts and are hard to adapt to the changing environment. Recently, firms have attempted to improve their systems by transitioning from the rule-based techniques to statistical machine learning approaches. However, they still treat the problem of misconduct detection as a single-document classification problem. We present an approach that focuses on actors, connections among actors, and on cases of misconduct. Furthermore, we highlight the importance of having a \u201chuman-in-the-loop\u201d approach, where humans are both guided by and guide the system at the same time, in order to detect malfeasance faster and to adapt to changing environments. We also discuss how humans can play a key role for detecting shortcomings of existing machine-learningbased malfeasance-detection systems. Our multifaceted approach has been developed and tested in real environments within both massive and smaller financial institutions, and we discuss practical constraints and lessons learned. 1", "num_citations": "1\n", "authors": ["2068"]}
{"title": "TOWARDS DESIGNING RANKING SYSTEMS FOR HOTELS ON TRAVEL SEARCH ENGINES: COMBINING TEXT MINING AND IMAGE CLASSIFICATION WITH ECONOMETRICS\n", "abstract": " In this paper, we empirically estimate the economic value of different hotel characteristics, especially the location-based and service-based characteristics given the associated local infrastructure. We build a random coefficients-based structural model taking into consideration the multiple-levels of consumer heterogeneity introduced by different travel contexts and different hotel characteristics. We estimate this econometric model with a unique dataset of hotel reservations located in the US over 3 months and user-generated content data that was processed based on techniques from text mining, image classification, and on-demand annotations. This enables us to infer the economic significance of various hotel characteristics. We then propose to design a new hotel ranking system based on the empirical estimates that take into account the multi-dimensional preferences of customers and imputes consumer surplus from transactions for a given hotel. By doing so, we are able to provide customers with the \u201cbest value for money\u201d hotels. Based on blind tests of users from Amazon Mechanical Turk, we test our ranking system with some benchmark hotel ranking systems. We find that our system performs significantly better than existing ones. This suggests that our inter-disciplinary approach has the potential to improve the quality of hotel search.", "num_citations": "1\n", "authors": ["2068"]}
{"title": "Taxonomy design\n", "abstract": " This chapter discusses the design of taxonomies to be used in dynamic taxonomy systems. Although the only actual requirement of dynamic taxonomies is a multidimensional classification, an organization by facets is normally used.             The first section provides guidelines for the design of DT taxonomies, which include the automatic construction from structured data, and the retrofitting of traditional monodimensional taxonomies.             The second section shows how a faceted taxonomy can be automatically extracted from the infobase itself when objects are textual or are described by textual captions or tags.", "num_citations": "1\n", "authors": ["2068"]}