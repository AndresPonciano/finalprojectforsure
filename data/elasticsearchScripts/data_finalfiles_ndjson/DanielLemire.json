{"title": "Slope One predictors for online rating-based collaborative filtering\n", "abstract": " Rating-based collaborative filtering is the process of predicting how a user would rate a given item from other user ratings. We propose three related slope one schemes with predictors of the form f (x) = x + b, which precompute the average difference between the ratings of one item and another for users who rated both. Slope one algorithms are easy to implement, efficient to query, reasonably accurate, and they support both online queries and dynamic updates, which makes them good candidates for real-world systems. The basic slope one scheme is suggested as a new reference scheme for collaborative filtering. By factoring in items that a user liked separately from items that a user disliked, we achieve results competitive with slower memory-based schemes over the standard benchmark EachMovie and Movielens data sets while better fulfilling the desiderata of CF applications.", "num_citations": "924\n", "authors": ["710"]}
{"title": "Tag-cloud drawing: Algorithms for cloud visualization\n", "abstract": " Tag clouds provide an aggregate of tag-usage statistics. They are typically sent as in-line HTML to browsers. However, display mechanisms suited for ordinary text are not ideal for tags, because font sizes may vary widely on a line. As well, the typical layout does not account for relationships that may be known between tags. This paper presents models and algorithms to improve the display of tag clouds that consist of in-line HTML, as well as algorithms that use nested tables to achieve a more general 2-dimensional layout in which tag relationships are considered. The first algorithms leverage prior work in typesetting and rectangle packing, whereas the second group of algorithms leverage prior work in Electronic Design Automation. Experiments show our algorithms can be efficiently implemented and perform well.", "num_citations": "320\n", "authors": ["710"]}
{"title": "Decoding billions of integers per second through vectorization\n", "abstract": " In many important applications\u2014such as search engines and relational database systems\u2014data are stored in the form of arrays of integers. Encoding and, most importantly, decoding of these arrays consumes considerable CPU\u2009time. Therefore, substantial effort has been made to reduce costs associated with compression and decompression. In particular, researchers have exploited the superscalar nature of modern processors and single\u2010instruction, multiple\u2010data (SIMD) instructions. Nevertheless, we introduce a novel vectorized scheme called SIMD\u2010BP128\u22c6 that improves over previously proposed vectorized approaches. It is nearly twice as fast as the previously fastest schemes on desktop processors (varint\u2010G8IU and PFOR). At the same time, SIMD\u2010BP128\u22c6 saves up to 2\u2009bits/int. For even better compression, we propose another new vectorized scheme (SIMD\u2010FastPFOR) that has a compression ratio within\u00a0\u2026", "num_citations": "276\n", "authors": ["710"]}
{"title": "Faster retrieval with a two-pass dynamic-time-warping lower bound\n", "abstract": " The dynamic time warping (DTW) is a popular similarity measure between time series. The DTW fails to satisfy the triangle inequality and its computation requires quadratic time. Hence, to find closest neighbors quickly, we use bounding techniques. We can avoid most DTW computations with an inexpensive lower bound (LB_Keogh). We compare LB_Keogh with a tighter lower bound (LB_Improved). We find that LB_Improved-based search is faster. As an example, our approach is 2\u20133 times faster over random-walk and shape time series.", "num_citations": "205\n", "authors": ["710"]}
{"title": "Measuring academic influence: Not all citations are equal\n", "abstract": " The importance of a research article is routinely measured by counting how many times it has been cited. However, treating all citations with equal weight ignores the wide variety of functions that citations perform. We want to automatically identify the subset of references in a bibliography that have a central academic influence on the citing paper. For this purpose, we examine the effectiveness of a variety of features for determining the academic influence of a citation. By asking authors to identify the key references in their own work, we created a data set in which citations were labeled according to their academic influence. Using automatic feature selection with supervised machine learning, we found a model for predicting academic influence that achieves good performance on this data set using only four features. The best features, among those we evaluated, were those based on the number of times a reference\u00a0\u2026", "num_citations": "190\n", "authors": ["710"]}
{"title": "RACOFI: A rule-applying collaborative filtering system\n", "abstract": " In this paper 1 we give an overview of the RACOFI (Rule-Applying Collaborative Filtering) multidimensional rating system and its related technologies. This will be exemplified with RACOFI Music, an implemented collaboration agent that assists on-line users in the rating and recommendation of audio (Learning) Objects. It lets users rate contemporary Canadian music in the five dimensions of impression, lyrics, music, originality, and production. The collaborative filtering algorithms ST IP earson, STIN2, and the Per Item Average algorithms are then employed together with RuleML-based rules to recommend music objects that best match user queries. RACOFI has been on-line since", "num_citations": "178\n", "authors": ["710"]}
{"title": "Sorting improves word-aligned bitmap indexes\n", "abstract": " Bitmap indexes must be compressed to reduce input/output costs and minimize CPU usage. To accelerate logical operations (AND, OR, XOR) over bitmaps, we use techniques based on run-length encoding (RLE), such as Word-Aligned Hybrid (WAH) compression. These techniques are sensitive to the order of the rows: a simple lexicographical sort can divide the index size by 9 and make indexes several times faster. We investigate row-reordering heuristics. Simply permuting the columns of the table can increase the sorting efficiency by 40%. Secondary contributions include efficient algorithms to construct and aggregate bitmaps. The effect of word length is also reviewed by constructing 16-bit, 32-bit and 64-bit indexes. Using 64-bit CPUs, we find that 64-bit indexes are slightly faster than 32-bit indexes despite being nearly twice as large.", "num_citations": "159\n", "authors": ["710"]}
{"title": "Better bitmap performance with Roaring bitmaps\n", "abstract": " Bitmap indexes are commonly used in databases and search engines. By exploiting bit\u2010level parallelism, they can significantly accelerate queries. However, they can use much memory, and thus, we might prefer compressed bitmap indexes. Following Oracle's lead, bitmaps are often compressed using run\u2010length encoding (RLE). Building on prior work, we introduce the Roaring compressed bitmap format: it uses packed arrays for compression instead of RLE. We compare it to two high\u2010performance RLE\u2010based bitmap encoding techniques: Word Aligned Hybrid compression scheme and Compressed \u2018n\u2019 Composable Integer Set. On synthetic and real data, we find that Roaring bitmaps (1) often compress significantly better (e.g., 2\u00d7) and (2) are faster than the compressed alternatives (up to 900\u00d7 faster for intersections). Our results challenge the view that RLE\u2010based bitmap compression is best. Copyright \u00a9 2015\u00a0\u2026", "num_citations": "153\n", "authors": ["710"]}
{"title": "Scale and translation invariant collaborative filtering systems\n", "abstract": " Collaborative filtering systems are prediction algorithms over sparse data sets of user preferences. We modify a wide range of state-of-the-art collaborative filtering systems to make them scale and translation invariant and generally improve their accuracy without increasing their computational cost. Using the EachMovie and the Jester data sets, we show that learning-free constant time scale and translation invariant schemes outperforms other learning-free constant time schemes by at least 3% and perform as well as expensive memory-based schemes (within 4%). Over the Jester data set, we show that a scale and translation invariant Eigentaste algorithm outperforms Eigentaste 2.0 by 20%. These results suggest that scale and translation invariance is a desirable property.", "num_citations": "116\n", "authors": ["710"]}
{"title": "Collaborative filtering and inference rules for context\u2010aware learning object recommendation\n", "abstract": " Learning objects strive for reusability in e\u2010Learning to reduce cost and allow personalization of content. We show why learning objects require adapted Information Retrieval systems. In the spirit of the Semantic Web, we discuss the semantic description, discovery, and composition of learning objects. As part of our project, we tag learning objects with both objective (e.g., title, date, and author) and subjective (e.g., quality and relevance) metadata. We present the RACOFI (Rule\u2010Applying Collaborative Filtering) Composer prototype with its novel combination of two libraries and their associated engines: a collaborative filtering system and an inference rule system. We developed RACOFI to generate context\u2010aware recommendation lists. Context is handled by multidimensional predictions produced from a database\u2010driven scalable collaborative filtering algorithm. Rules are then applied to the predictions to customize\u00a0\u2026", "num_citations": "112\n", "authors": ["710"]}
{"title": "Streaming maximum-minimum filter using no more than three comparisons per element\n", "abstract": " The running maximum-minimum (max-min) filter computes the maxima and minima over running windows of size w. This filter has numerous applications in signal processing and time series analysis. We present an easy-to-implement online algorithm requiring no more than 3 comparisons per element, in the worst case. Comparatively, no algorithm is known to compute the running maximum (or minimum) filter in 1.5 comparisons per element, in the worst case. Our algorithm has reduced latency and memory usage.", "num_citations": "99\n", "authors": ["710"]}
{"title": "A better alternative to piecewise linear time series segmentation\n", "abstract": " Time series are difficult to monitor, summarize and predict. Segmentation organizes time series into few intervals having uniform characteristics (flatness, linearity, modality, monotonicity and so on). For scalability, we require fast linear time algorithms. The popular piecewise linear model can determine where the data goes up or down and at what rate. Unfortunately, when the data does not follow a linear model, the computation of the local slope creates overfitting. We propose an adaptive time series model where the polynomial degree of each interval vary (constant, linear and so on). Given a number of regressors, the cost of each interval is its polynomial degree: constant intervals cost 1 regressor, linear intervals cost 2 regressors, and so on. Our goal is to minimize the Euclidean (l2) error for a given model complexity. Experimentally, we investigate the model where intervals can be either constant or linear. Over\u00a0\u2026", "num_citations": "99\n", "authors": ["710"]}
{"title": "Wavelet time entropy, T wave morphology and myocardial ischemia\n", "abstract": " Using wavelets, the authors computed the entropy of the signal at various frequency levels (wavelet time entropy) and, thus, find an optimal measure to differentiate normal states from ischemic ones. This new indicator is independent from the ST segment and yet provide a conclusive detection of the ischemic states.", "num_citations": "98\n", "authors": ["710"]}
{"title": "SIMD Compression and the Intersection of Sorted Integers\n", "abstract": " Sorted lists of integers are commonly used in inverted indexes and database systems. They are often compressed in memory. We can use the single\u2010instruction, multiple data (SIMD) instructions available in common processors to boost the speed of integer compression schemes. Our S4\u2010BP128\u2010D4 scheme uses as little as 0.7\u2009CPU cycles per decoded 32\u2010bit integer while still providing state\u2010of\u2010the\u2010art compression. However, if the subsequent processing of the integers is slow, the effort spent on optimizing decompression speed can be wasted. To show that it does not have to be so, we (1) vectorize and optimize the intersection of posting lists; (2) introduce the SIMD GALLOPING algorithm. We exploit the fact that one SIMD instruction can compare four pairs of 32\u2010bit integers at once. We experiment with two Text REtrieval Conference (TREC) text collections, GOV2 and ClueWeb09 (category B), using logs from\u00a0\u2026", "num_citations": "87\n", "authors": ["710"]}
{"title": "Apache Calcite: A Foundational Framework for Optimized Query Processing Over Heterogeneous Data Sources\n", "abstract": " Apache Calcite is a foundational software framework that provides query processing, optimization, and query language support to many popular open-source data processing systems such as Apache Hive, Apache Storm, Apache Flink, Druid, and MapD. The goal of this paper is to formally introduce Calcite to the broader research community, brie y present its history, and describe its architecture, features, functionality, and patterns for adoption. Calcite's architecture consists of a modular and extensible query optimizer with hundreds of built-in optimization rules, a query processor capable of processing a variety of query languages, an adapter architecture designed for extensibility, and support for heterogeneous data models and stores (relational, semi-structured, streaming, and geospatial). This exible, embeddable, and extensible architecture is what makes Calcite an attractive choice for adoption in big-data\u00a0\u2026", "num_citations": "81\n", "authors": ["710"]}
{"title": "A Call to Arms: Revisiting Database Design\n", "abstract": " Good database design is crucial to obtain a sound, consistent database, and\u2014in turn\u2014good database design methodologies are the best way to achieve the right design. These methodologies are taught to most Computer Science undergraduates, as part of any Introduction to Database class [33]. They can be considered part of the \u201ccanon\u201d, and indeed, the overall approach to database design has been unchanged for years. Moreover, none of the major database research assessments identify database design as a strategic research direction [1, 2, 8]. Should we conclude that database design is a solved problem?", "num_citations": "57\n", "authors": ["710"]}
{"title": "Attribute value reordering for efficient hybrid OLAP\n", "abstract": " The normalization of a data cube is the ordering of the attribute values. For large multidimensional arrays where dense and sparse chunks are stored differently, proper normalization can lead to improved storage efficiency. We show that it is NP-hard to compute an optimal normalization even for 1\u00a0\u00d7\u00a03 chunks, although we find an exact algorithm for 1\u00a0\u00d7\u00a02 chunks. When dimensions are nearly statistically independent, we show that dimension-wise attribute frequency sorting is an optimal normalization and takes time O(dn\u00a0log(n)) for data cubes of size nd. When dimensions are not independent, we propose and evaluate a several heuristics. The hybrid OLAP (HOLAP) storage mechanism is already 19\u201330% more efficient than ROLAP, but normalization can improve it further by 9\u201313% for a total gain of 29\u201344% over ROLAP.", "num_citations": "54\n", "authors": ["710"]}
{"title": "Consistently faster and smaller compressed bitmaps with Roaring\n", "abstract": " Compressed bitmap indexes are used in databases and search engines. Many bitmap compression techniques have been proposed, almost all relying primarily on run\u2010length encoding (RLE). However, on unsorted data, we can get superior performance with a hybrid compression technique that uses both uncompressed bitmaps and packed arrays inside a two\u2010level tree. An instance of this technique, Roaring, has recently been proposed. Due to its good performance, it has been adopted by several production platforms (e.g., Apache Lucene, Apache Spark, Apache Kylin, and Druid). Yet there are cases where run\u2010length\u2010encoded bitmaps are smaller than the original Roaring bitmaps\u2014typically when the data are sorted so that the bitmaps contain long compressible runs. To better handle these cases, we build a new Roaring hybrid that combines uncompressed bitmaps, packed arrays, and RLE\u2010compressed\u00a0\u2026", "num_citations": "46\n", "authors": ["710"]}
{"title": "Faster Population Counts using AVX2 Instructions\n", "abstract": " Counting the number of ones in a binary stream is a common operation in database, information-retrieval, cryptographic and machine-learning applications. Most processors have dedicated instructions to count the number of ones in a word (e.g. popcnt on \u00d764 processors). Maybe surprisingly, we show that a vectorized approach using SIMD instructions can be twice as fast as using the dedicated instructions on recent Intel processors. The benefits can be even greater for applications such as similarity measures (e.g. the Jaccard index) that require additional Boolean operations. Our approach has been adopted by LLVM: it is used by its popular C compiler (Clang).", "num_citations": "40\n", "authors": ["710"]}
{"title": "Time series classification by class-specific Mahalanobis distance measures\n", "abstract": " To classify time series by nearest neighbors, we need to specify or learn one or several distance measures. We consider variations of the Mahalanobis distance measures which rely on the inverse covariance matrix of the data. Unfortunately\u2014for time series data\u2014the covariance matrix has often low rank. To alleviate this problem we can either use a pseudoinverse, covariance shrinking or limit the matrix to its diagonal. We review these alternatives and benchmark them against competitive methods such as the related Large Margin Nearest Neighbor Classification (LMNN) and the Dynamic Time Warping (DTW) distance. As we expected, we find that the DTW is superior, but the Mahalanobis distance measures are one to two orders of magnitude faster. To get best results with Mahalanobis distance measures, we recommend learning one distance measure per class using either covariance shrinking or the\u00a0\u2026", "num_citations": "39\n", "authors": ["710"]}
{"title": "Reordering columns for smaller indexes\n", "abstract": " Column-oriented indexes\u2014such as projection or bitmap indexes\u2014are compressed by run-length encoding to reduce storage and increase speed. Sorting the tables improves compression. On realistic data sets, permuting the columns in the right order before sorting can reduce the number of runs by a factor of two or more. Unfortunately, determining the best column order is NP-hard. For many cases, we prove that the number of runs in table columns is minimized if we sort columns by increasing cardinality. Experimentally, sorting based on Hilbert space-filling curves is poor at minimizing the number of runs.", "num_citations": "39\n", "authors": ["710"]}
{"title": "A General SIMD-based Approach to Accelerating Compression Algorithms\n", "abstract": " Compression algorithms are important for data-oriented tasks, especially in the era of \u201cBig Data.\u201d Modern processors equipped with powerful SIMD instruction sets provide us with an opportunity for achieving better compression performance. Previous research has shown that SIMD-based optimizations can multiply decoding speeds. Following these pioneering studies, we propose a general approach to accelerate compression algorithms. By instantiating the approach, we have developed several novel integer compression algorithms, called Group-Simple, Group-Scheme, Group-AFOR, and Group-PFD, and implemented their corresponding vectorized versions. We evaluate the proposed algorithms on two public TREC datasets, a Wikipedia dataset, and a Twitter dataset. With competitive compression ratios and encoding speeds, our SIMD-based algorithms outperform state-of-the-art nonvectorized algorithms with\u00a0\u2026", "num_citations": "38\n", "authors": ["710"]}
{"title": "Bloofi: Multidimensional Bloom Filters\n", "abstract": " Bloom filters are probabilistic data structures commonly used for approximate membership problems in many areas of Computer Science (networking, distributed systems, databases, etc.). With the increase in data size and distribution of data, problems arise where a large number of Bloom filters are available, and all of them need to be searched for potential matches. As an example, in a federated cloud environment, each cloud provider could encode the information using Bloom filters and share the Bloom filters with a central coordinator. The problem of interest is not only whether a given element is in any of the sets represented by the Bloom filters, but also which of the existing sets contain the given element. This problem cannot be solved by just constructing a Bloom filter on the union of all the sets. Instead, we effectively have a multidimensional Bloom filter problem: given an element, we wish to receive a list of\u00a0\u2026", "num_citations": "38\n", "authors": ["710"]}
{"title": "Roaring Bitmaps: Implementation of an Optimized Software Library\n", "abstract": " Compressed bitmap indexes are used in systems such as Git or Oracle to accelerate queries. They represent sets and often support operations such as unions, intersections, differences, and symmetric differences. Several important systems such as Elasticsearch, Apache Spark, Netflix's Atlas, LinkedIn's Pivot, Metamarkets' Druid, Pilosa, Apache Hive, Apache Tez, Microsoft Visual Studio Team Services, and Apache Kylin rely on a specific type of compressed bitmap index called Roaring. We present an optimized software library written in C implementing Roaring bitmaps: CRoaring. It benefits from several algorithms designed for the single\u2010instruction\u2013multiple\u2010data instructions available on commodity processors. In particular, we present vectorized algorithms to compute the intersection, union, difference, and symmetric difference between arrays. We benchmark the library against a wide range of competitive\u00a0\u2026", "num_citations": "37\n", "authors": ["710"]}
{"title": "Reordering rows for better compression: Beyond the lexicographic order\n", "abstract": " Sorting database tables before compressing them improves the compression rate. Can we do better than the lexicographical order? For minimizing the number of runs in a run-length encoding compression scheme, the best approaches to row-ordering are derived from traveling salesman heuristics, although there is a significant trade-off between running time and compression. A new heuristic, Multiple Lists, which is a variant on Nearest Neighbor that trades off compression for a major running-time speedup, is a good option for very large tables. However, for some compression schemes, it is more important to generate long runs rather than few runs. For this case, another novel heuristic, Vortex, is promising. We find that we can improve run-length encoding up to a factor of 3 whereas we can improve prefix coding by up to 80%: these gains are on top of the gains due to lexicographically sorting the table. We prove\u00a0\u2026", "num_citations": "37\n", "authors": ["710"]}
{"title": "Stream VByte: Faster Byte-Oriented Integer Compression\n", "abstract": " Arrays of integers are often compressed in search engines. Though there are many ways to compress integers, we are interested in the popular byte-oriented integer compression techniques (e.g., VByte or Google's varint-GB). Although not known for their speed, they are appealing due to their simplicity and engineering convenience. Amazon's varint-G8IU is one of the fastest byte-oriented compression technique published so far. It makes judicious use of the powerful single-instruction-multiple-data (SIMD) instructions available in commodity processors. To surpass varint-G8IU, we present Stream VByte, a novel byte-oriented compression technique that separates the control stream from the encoded data. Like varint-G8IU, Stream VByte is well suited for SIMD instructions. We show that Stream VByte decoding can be up to twice as fast as varint-G8IU decoding over real data sets. In this sense, Stream VByte\u00a0\u2026", "num_citations": "36\n", "authors": ["710"]}
{"title": "Analyzing large collections of electronic text using OLAP\n", "abstract": " Computer-assisted reading and analysis of text has various applications in the humanities and social sciences. The increasing size of many electronic text archives has the advantage of a more complete analysis but the disadvantage of taking longer to obtain results. On-Line Analytical Processing is a method used to store and quickly analyze multidimensional data. By storing text analysis information in an OLAP system, a user can obtain solutions to inquiries in a matter of seconds as opposed to minutes, hours, or even days. This analysis is user-driven allowing various users the freedom to pursue their own direction of research.", "num_citations": "34\n", "authors": ["710"]}
{"title": "Vectorized VByte Decoding\n", "abstract": " We consider the ubiquitous technique of VByte compression, which represents each integer as a variable length sequence of bytes. The low 7 bits of each byte encode a portion of the integer, and the high bit of each byte is reserved as a continuation flag. This flag is set to 1 for all bytes except the last, and the decoding of each integer is complete when a byte with a high bit of 0 is encountered. VByte decoding can be a performance bottleneck especially when the unpredictable lengths of the encoded integers cause frequent branch mispredictions. Previous attempts to accelerate VByte decoding using SIMD vector instructions have been disappointing, prodding search engines such as Google to use more complicated but faster-to-decode formats for performance-critical code. Our decoder (Masked VByte) is 2 to 4 times faster than a conventional scalar VByte decoder, making the format once again competitive with regard to speed.", "num_citations": "33\n", "authors": ["710"]}
{"title": "Recursive n-gram hashing is pairwise independent, at best\n", "abstract": " Many applications use sequences of n consecutive symbols (n-grams). Hashing these n-grams can be a performance bottleneck. For more speed, recursive hash families compute hash values by updating previous values. We prove that recursive hash families cannot be more than pairwise independent. While hashing by irreducible polynomials is pairwise independent, our implementations either run in time O (n) or use an exponential amount of memory. As a more scalable alternative, we make hashing by cyclic polynomials pairwise independent by ignoring n-1 bits. Experimentally, we show that hashing by cyclic polynomials is twice as fast as hashing by irreducible polynomials. We also show that randomized Karp\u2013Rabin hash families are not pairwise independent.", "num_citations": "30\n", "authors": ["710"]}
{"title": "Strongly Universal String Hashing is Fast\n", "abstract": " We present fast strongly universal string hashing families: they can process data at a rate of 0.2 CPU cycle per byte. Maybe surprisingly, we find that these families\u2014though they require a large buffer of random numbers\u2014are often faster than popular hash functions with weaker theoretical guarantees. Moreover, conventional wisdom is that hash functions with fewer multiplications are faster. Yet we find that they may fail to be faster due to operation pipelining. We present experimental results on several processors including low-power processors. Our tests include hash functions designed for processors with the carry-less multiplication instruction set. We also prove, using accessible proofs, the strong universality of our families.", "num_citations": "28\n", "authors": ["710"]}
{"title": "Web 2.0 OLAP: From data cubes to tag clouds\n", "abstract": " Increasingly, business projects are ephemeral. New Business Intelligence tools must support ad-lib data sources and quick perusal. Meanwhile, tag clouds are a popular community-driven visualization technique. Hence, we investigate tag-cloud views with support for OLAP operations such as roll-ups, slices, dices, clustering, and drill-downs. As a case study, we implemented an application where users can upload data and immediately navigate through its ad hoc dimensions. To support social networking, views can be easily shared and embedded in other Web sites. Algorithmically, our tag-cloud views are approximate range top-k queries over spontaneous data cubes. We present experimental evidence that iceberg cuboids provide adequate online approximations. We benchmark several browser-oblivious tag-cloud layout optimizations.", "num_citations": "28\n", "authors": ["710"]}
{"title": "Xor Filters: Faster and Smaller Than Bloom and Cuckoo Filters\n", "abstract": " The Bloom filter provides fast approximate set membership while using little memory. Engineers often use these filters to avoid slow operations such as disk or network accesses. As an alternative, a cuckoo filter may need less space than a Bloom filter and it is faster. Chazelle et al. proposed a generalization of the Bloom filter called the Bloomier filter. Dietzfelbinger and Pagh described a variation on the Bloomier filter that can answer approximate membership queries over immutable sets. It has never been tested empirically, to our knowledge. We review an efficient implementation of their approach, which we call the xor filter. We find that xor filters can be faster than Bloom and cuckoo filters while using less memory. We further show that a more compact version of xor filters (xor+) can use even less space than highly compact alternatives (e.g., Golomb-compressed sequences) while providing speeds competitive\u00a0\u2026", "num_citations": "27\n", "authors": ["710"]}
{"title": "Parsing Gigabytes of JSON per Second\n", "abstract": " JavaScript Object Notation or JSON is a ubiquitous data exchange format on the web. Ingesting JSON documents can become a performance bottleneck due to the sheer volume of data. We are thus motivated to make JSON parsing as fast as possible. Despite the maturity of the problem of JSON parsing, we show that substantial speedups are possible. We present the first standard-compliant JSON parser to process gigabytes of data per second on a single core, using commodity processors. We can use a quarter or fewer instructions than a state-of-the-art reference parser like RapidJSON. Unlike other validating parsers, our software (simdjson) makes extensive use of single instruction and multiple data instructions. To ensure reproducibility, simdjson is freely available as open-source software under a liberal license.", "num_citations": "26\n", "authors": ["710"]}
{"title": "A comparison of five probabilistic view-size estimation techniques in OLAP\n", "abstract": " A data warehouse cannot materialize all possible views, hence we must estimate quickly, accurately, and reliably the size of views to determine the best candidates for materialization. Many available techniques for view-size estimation make particular statistical assumptions and their error can be large. Comparatively, unassuming probabilistic techniques are slower, but they estimate accurately and reliability very large view sizes using little memory. We compare five unassuming hashing-based view-size estimation techniques including Stochastic Probabilistic Counting and LogLog Probabilistic Counting. Our experiments show that only Generalized Counting, Gibbons-Tirthapura, and Adaptive Counting provide universally tight estimates irrespective of the sizeof the view; of those, only Adaptive Counting remains constantly fast as we increasethe memory budget.", "num_citations": "26\n", "authors": ["710"]}
{"title": "Wavelet-based relative prefix sum methods for range sum queries in data cubes\n", "abstract": " Data mining and related applications often rely on extensive range sum queries and thus, it is important for these queries to scale well. Range sum queries in data cubes can be achieved in time O(1) using prefix sum aggregates, but prefix sum update costs are proportional to the size of the data cube O(n^d). Using the Relative Prefix Sum (RPS) method, the update costs can be reduced to the root of the size of the data cube (O(n^(d/2)). We present a new family of base b wavelet algorithms further reducing the update costs to O(n^(d/beta)) for beta as large as we want, while preserving constant-time queries. We also show that this approach leads to O(log^d n) query and update methods twice as fast as Haar-based methods. Moreover, since these new methods are pyramidal, they provide incrementally improving estimates.", "num_citations": "24\n", "authors": ["710"]}
{"title": "Compressed bitmap indexes: beyond unions and intersections\n", "abstract": " Compressed bitmap indexes are used to speed up simple aggregate queries in databases. Indeed, set operations like intersections, unions and complements can be represented as logical operations (AND, OR and NOT) that are ideally suited for bitmaps. However, it is less obvious how to apply bitmaps to more advanced queries. For example, we might seek products in a store that meet some, but maybe not all, criteria. Such threshold queries generalize intersections and unions; they are often used in information\u2010retrieval and data\u2010mining applications. We introduce new algorithms that are sometimes three orders of magnitude faster than a na\u00efve approach. Our work shows that bitmap indexes are more broadly applicable than is commonly believed. Copyright \u00a9 2014 John Wiley & Sons, Ltd.", "num_citations": "23\n", "authors": ["710"]}
{"title": "Diversity in open social networks\n", "abstract": " Online communities have become become a crucial ingredient of e-business. Supporting open social networks builds strong brands and provides lasting value to the consumer. One function of the community is to recommend new products and services. Open social networks tend to be resilient, adaptive, and broad, but simplistic recommender systems can be'gamed'by members seeking to promote certain products or services. We argue that the gaming is not the failure of the open social network, but rather of the function used by the recommender. To increase the quality and resilience of recommender systems, and provide the user with genuine and novel discoveries, we have to foster diversity, instead of closing down the social networks. Fortunately, software increases the broadcast capacity of each individual, making dense open social networks possible. Numerically, we show that dense social networks encourage diversity. In business terms, dense social networks support a long tail.", "num_citations": "20\n", "authors": ["710"]}
{"title": "Implementing a rating-based item-to-item recommender system in PHP/SQL\n", "abstract": " User personalization and profiling is key to many succesful Web sites. Consider that there is considerable free content on the Web, but comparatively few tools to help us organize or mine such content for specific purposes. One solution is to ask users to rate resources so that they can help each other find better content: we call this rating-based collaborative filtering. This paper presents a database-driven approach to item-to-item collaborative filtering which is both easy to implement and can support a full range of applications.", "num_citations": "20\n", "authors": ["710"]}
{"title": "Histogram-aware sorting for enhanced word-aligned compression in bitmap indexes\n", "abstract": " Bitmap indexes must be compressed to reduce input/output costs and minimize CPU usage. To accelerate logical operations (AND, OR, XOR) over bitmaps, we use techniques based on run-length encoding (RLE), such as Word-Aligned Hybrid (WAH) compression. These techniques are sensitive to the order of the rows: a simple lexicographical sort can divide the index size by 9 and make indexes several times faster. We investigate reordering heuristics based on computed attribute-value histograms. Simply permuting the columns of the table based on these histograms can increase the sorting efficiency by 40%.", "num_citations": "19\n", "authors": ["710"]}
{"title": "Collaborative OLAP with tag clouds: Web 2.0 OLAP formalism and experimental evaluation\n", "abstract": " Increasingly, business projects are ephemeral. New Business Intelligence tools must support ad-lib data sources and quick perusal. Meanwhile, tag clouds are a popular community-driven visualization technique. Hence, we investigate tag-cloud views with support for OLAP operations such as roll-ups, slices, dices, clustering, and drill-downs. As a case study, we implemented an application where users can upload data and immediately navigate through its ad hoc dimensions. To support social networking, views can be easily shared and embedded in other Web sites. Algorithmically, our tag-cloud views are approximate range top-k queries over spontaneous data cubes. We present experimental evidence that iceberg cuboids provide adequate online approximations. We benchmark several browser-oblivious tag-cloud layout optimizations.", "num_citations": "19\n", "authors": ["710"]}
{"title": "Fast Random Integer Generation in an Interval\n", "abstract": " In simulations, probabilistic algorithms, and statistical tests, we often generate random integers in an interval (e.g., [0,s)). For example, random integers in an interval are essential to the Fisher-Yates random shuffle. Consequently, popular languages such as Java, Python, C++, Swift and Go include ranged random integer generation functions as part of their runtime libraries. Pseudo-random values are usually generated in words of a fixed number of bits (e.g., 32b,\u00a064b) using algorithms such as a linear congruential generator. We need functions to convert such random words to random integers in an interval ([0,s)) without introducing statistical biases. The standard functions in programming languages such as Java involve integer divisions. Unfortunately, division instructions are relatively expensive. We review an unbiased function to generate ranged integers from a source of random words that avoids integer\u00a0\u2026", "num_citations": "18\n", "authors": ["710"]}
{"title": "Full Solution Indexing for top-K Web Service Composition\n", "abstract": " Automated service composition fulfills complex tasks by combining different existing web services together. Unfortunately, optimizing service composition is still a challenging area that needs to be addressed. In this article, we propose a novel relational database approach for automated service composition. All possible service combinations are generated beforehand and stored in a relational database. When a user request comes, our system composes SQL queries to search in the database and return the best Quality of Service (QoS) solutions. We test the performance of the proposed system with a web service challenge data set. Our experimental results demonstrate that this system can always find top-K valid solutions to satisfy user's functional and non-functional requirements.", "num_citations": "18\n", "authors": ["710"]}
{"title": "Scaling up Web service composition with the skyline operator\n", "abstract": " Web service composition enables the provision of existing resources on the web without investing in new infrastructure. However, searching an optimal composition solution with both functional and non-functional requirements is a computationally demanding problem: the time and space requirements may be insufferable due to the high number of available services. To alleviate this problem, we propose the application of a skyline operation to reduce the search space and improve the scalability. We design a system to solve the composition problem with two separate processes. The Graphplan approach finds a solution in a short time, the database approach may take longer time to find a solution, but the solution returned by this approach always has fewer redundant services with a better QoS value. Full Solution Indexing using Database (FSIDB) approach pre-computes all services combinations and store them\u00a0\u2026", "num_citations": "17\n", "authors": ["710"]}
{"title": "Faster 64-bit universal hashing using carry-less multiplications\n", "abstract": " Intel and AMD support the carry-less multiplication (CLMUL) instruction set in their x64 processors. We use CLMUL to implement an almost universal 64-bit hash family (CLHASH). We compare this new family with what might be the fastest almost universal family on x64 processors (VHASH). We find that CLHASH is at least 60\u00a0% faster. We also compare CLHASH with a popular hash function designed for speed (Google\u2019s CityHash). We find that CLHASH is 40\u00a0% faster than CityHash on inputs larger than 64\u00a0bytes and just as fast otherwise.", "num_citations": "17\n", "authors": ["710"]}
{"title": "Changing computational research. The challenges ahead\n", "abstract": " The past year has been an interesting one for those interested in reproducible research. There have been great examples of replicability [1, 2] in research communication, and examples of horrifying failure of reproducibility (as described in [3]) with serious questions being raised on the ability of our current system of research communication to guarantee, or even encourage, that published research be reproducible or replicable.When we launched the call for papers for Open Research Computation in late 2010 we saw a clear need for higher standards. Computational research should stand out as an exemplar of just how reproducible research can be, yet it falls short more often than not. With modern computational tools it is entirely possible to provide packages which allow direct replication of results. It is possible to provide data and code in the form of a functional virtual machine image along with automated tests to\u00a0\u2026", "num_citations": "17\n", "authors": ["710"]}
{"title": "Optimizing druid with roaring bitmaps\n", "abstract": " In the current Big Data era, systems for collecting, storing and efficiently exploiting huge amounts of data are continually introduced, such as Hadoop, Apache Spark, Dremel, etc. Druid is one of theses systems especially designed to manage such data quantities, and allows to perform detailed real-time analysis on terabytes of data within sub-second latencies. One of the important Druid's requirements is fast data filtering. To insure that, Druid makes an extensive use of bitmap indexes. Previously, we introduced a new compressed bitmap index scheme called Roaring bitmap that has shown interesting results when compared to the bitmap compression scheme adopted by Druid: Concise. Since, Roaring bitmap has been integrated to Druid as an indexing solution. In this work, we produce an extensive series of experiments in order to compare Roaring bitmap and Concise time-space performances when used to\u00a0\u2026", "num_citations": "15\n", "authors": ["710"]}
{"title": "Baseline asymmetry, Tau projection, B-field estimation and automatic half-cycle rejection\n", "abstract": " This report assumes that the reader is familiar with THEM Geophysics Inc. THEM system is an innovative EM system appropriate for rough terrain mineral exploration. It was developed over the last 4-5 years and is still considered a prototype.One of the unresolved issue has been the noise level. It is only one of several issues of course, but signal-to-noise ration seems to be a convenient way to measure the quality of an EM system and is of significant commercial interest. Unfortunately, there isn\u2019ta standardized measure of the noise level because processing software and hardware vary greatly from one system to another.", "num_citations": "14\n", "authors": ["710"]}
{"title": "Faster Remainder by Direct Computation: Applications to Compilers and Software Libraries\n", "abstract": " On common processors, integer multiplication is many times faster than integer division. Dividing a numerator n by a divisor d is mathematically equivalent to multiplication by the inverse of the divisor (n/d=n\u22171/d). If the divisor is known in advance, or if repeated integer divisions will be performed with the same divisor, it can be beneficial to substitute a less costly multiplication for an expensive division. Currently, the remainder of the division by a constant is computed from the quotient by a multiplication and a subtraction. However, if just the remainder is desired and the quotient is unneeded, this may be suboptimal. We present a generally applicable algorithm to compute the remainder more directly. Specifically, we use the fractional portion of the product of the numerator and the inverse of the divisor. On this basis, we also present a new and simpler divisibility algorithm to detect nonzero remainders. We also derive\u00a0\u2026", "num_citations": "12\n", "authors": ["710"]}
{"title": "Faster Base64 Encoding and Decoding using AVX2 Instructions\n", "abstract": " Web developers use base64 formats to include images, fonts, sounds, and other resources directly inside HTML, JavaScript, JSON, and XML files. We estimate that billions of base64 messages are decoded every day. We are motivated to improve the efficiency of base64 encoding and decoding. Compared to state-of-the-art implementations, we multiply the speeds of both the encoding (\u2248 10 \u00d70) and the decoding (\u2248 7 \u00d7). We achieve these good results by using the single-instruction-multiple-data instructions available on recent Intel processors (AVX2). Our accelerated software abides by the specification and reports errors when encountering characters outside of the base64 set. It is available online as free software under a liberal license.", "num_citations": "12\n", "authors": ["710"]}
{"title": "The LitOLAP project: Data warehousing with literature\n", "abstract": " The litOLAP project seeks to apply the Business Intelligence techniques of Data Warehousing and OLAP to the domain of text processing (specifically, computer-aided literary studies). A literary data warehouse is similar to a conventional corpus, but its data is stored and organized in multidimensional cubes, in order to promote efficient end-user queries. An initial implementation exists for litOLAP, and emphasis has been placed on cube-storage methods and caching intermediate results for reuse. Work continues on improving the query engine, the ETL process, and the user interfaces.", "num_citations": "12\n", "authors": ["710"]}
{"title": "Une famille d'ondelettes biorthogonales sur l'intervalle obtenue par un sch\u00e9ma d'interpolation it\u00e9rative\n", "abstract": " Nous avons d ej a montr e dans 4] que l'on pouvait obtenir les ondelettes biorthogonales de Cohen, Daubechies et Feauveau en d erivant un certain nombre de fois les fonctions fondamentales de l'interpolation it erative de Lagrange. Nous avons mis au point un nouveau sch ema d'interpolation it erative sur l'intervalle dont les d eriv ees nous permettent de construire des ondelettes sur l'intervalle avec des ltres en nombres rationnels. Nous retrouverons comme cas particulier les ondelettes de l'interpolation des moyennes 5].Abstract. In 4], we showed that we could get the Cohen-Daubechies-Feauveau biorthogonal wavelets by deriving a certain number of times the fundamental functions of the Lagrange itera-1", "num_citations": "12\n", "authors": ["710"]}
{"title": "Full solution indexing using database for qos-aware web service composition\n", "abstract": " Automated service composition can fulfill user request by composing services automatically when no individual services meet the goal. Unfortunately, most of current automated service composition methods are in-memory methods, which are limited by expensive and volatile physical memory. In this work, we develop a relational-database approach for automatic service composition. Possible service combinations are stored in a relational database on persistence disk instead of volatile memory, and for any composition requests, solutions can be obtained by simple SQL queries. We offer three main contributions in this paper. First, pursuing earlier work, we overcome the disadvantages of in-memory composition algorithms, such as volatile and expensive, and provide a solution suitable to cloud environments. Second, compared with other pre-computing composition methods, we use a single SQL query: there is no\u00a0\u2026", "num_citations": "11\n", "authors": ["710"]}
{"title": "Large-scale recommender systems and the netflix prize competition\n", "abstract": " Two major challenges for collaborative filtering problems are scalability and sparseness. Some powerful approaches have been developed to resolve these challenges. Two of them are Matrix Factorization (MF) and Fuzzy C-means (FCM). In this paper we combine the ideas of MF and FCM, and propose a new clustering model\u2014Modified Fuzzy C-means (MFCM). MFCM has better interpretability than MF, and better accuracy than FCM. MFCM also supplies a new perspective on MF models. Two new algorithms are developed to solve this new model. They are applied to the Netflix Prize data set and acquire comparable accuracy with that of MF.", "num_citations": "11\n", "authors": ["710"]}
{"title": "Quasi-monotonic segmentation of state variable behavior for reactive control\n", "abstract": " Real-world agents must react to changing conditions as they execute planned tasks. Conditions are typically monitored through time series representing state variables. While some predicates on these times series only consider one measure at a time, other predicates, sometimes called episodic predicates, consider sets of measures. We consider a special class of episodic predicates based on segmentation of the the measures into quasi-monotonic intervals where each interval is either quasi-increasing, quasi-decreasing, or quasi-flat. While being scale-based, this approach is also computational efficient and results can be computed exactly without need for approximation algorithms. Our approach is compared to linear spline and regression analysis.", "num_citations": "10\n", "authors": ["710"]}
{"title": "Scale-based monotonicity analysis in qualitative modelling with flat segments\n", "abstract": " Qualitative models are often more suitable than classical quantitative models in tasks such as Model-based Diagnosis (MBD), explaining system behavior, and designing novel devices from first principles. Monotonicity is an important feature to leverage when constructing qualitative models. Detecting monotonic pieces robustly and efficiently from sensor or simulation data remains an open problem. This paper presents scale-based monotonicity: the notion that monotonicity can be defined relative to a scale. Real-valued functions defined on a finite set of reals e.g. sensor data or simulation results, can be partitioned into quasi-monotonic segments, i.e. segments monotonic with respect to a scale, in linear time. A novel segmentation algorithm is introduced along with a scale-based definition of \"flatness\".", "num_citations": "9\n", "authors": ["710"]}
{"title": "Xorshift1024*, xorshift1024+, xorshift128+ and xoroshiro128+ fail statistical tests for linearity\n", "abstract": " L\u2019Ecuyer & Simard\u2019s Big\u00a0Crush statistical test suite has revealed statistical flaws in many popular random number generators including Marsaglia\u2019s Xorshift generators. Vigna recently proposed some 64-bit variations on the Xorshift scheme that are further scrambled (i.e., xorshift1024*, xorshift1024+, xorshift128+, xoroshiro128+). Unlike their unscrambled counterparts, they pass Big\u00a0Crush when interleaving blocks of 32\u00a0bits for each 64-bit word (most significant, least significant, most significant, least significant, etc.). We report that these scrambled generators systematically fail Big Crush \u2013 specifically the linear-complexity and matrix-rank tests that detect linearity \u2013 when taking the 32\u00a0lowest-order bits in reverse order from each 64-bit word.", "num_citations": "8\n", "authors": ["710"]}
{"title": "New method for denoising borehole transient electromagnetic data with discrete wavelet transform\n", "abstract": " Discrete wavelet transform (DWT) has been widely used as a useful tool in denoising geophysical data for its outstanding feature of detecting singularities and transients. In this paper, we develop a new strategy of denoising borehole transient electromagnetic (BHTEM) data. The principle idea of the denoising process is keeping those coefficients which are necessary to reconstruct the signal unchanged and setting others to zero. In our case, according to results of modeling, only the first eight detail coefficients are needed. The method has been validated on both synthetic and field BHTEM raw data. Besides the DWT, complementary measures are introduced accordingly. For simple data set, a curve fitting technique is employed to smooth the signal furthermore. For field BHTEM raw data the correlation analysis is carried out to identify and correct distorted transients. The efficiency and reliability of the method are\u00a0\u2026", "num_citations": "8\n", "authors": ["710"]}
{"title": "The universality of iterated hashing over variable-length strings\n", "abstract": " Iterated hash functions process strings recursively, one character at a time. At each iteration, they compute a new hash value from the preceding hash value and the next character. We prove that iterated hashing can be pairwise independent, but never 3-wise independent. We show that it can be almost universal over strings much longer than the number of hash values; we bound the maximal string length given the collision probability.", "num_citations": "8\n", "authors": ["710"]}
{"title": "An optimal linear time algorithm for quasi-monotonic segmentation\n", "abstract": " Monotonicity is a simple yet significant qualitative characteristic. We consider the problem of segmenting a sequence in up to K segments. We want the segments to be as monotonic as possible and to alternate signs. We propose a quality metric for this problem using the l \u221e norm, and we present an optimal linear time algorithm based on a novel formalism. Moreover, given a precomputation in time O(n log n) consisting of a labelling of all extrema, we compute any optimal segmentation in constant time. We compare experimentally its performance to two piecewise linear segmentation heuristics (top-down and bottom-up). We show that our algorithm is faster and more accurate. Applications include pattern recognition and qualitative modelling.", "num_citations": "8\n", "authors": ["710"]}
{"title": "Monotone pieces analysis for qualitative modeling\n", "abstract": " It is a crucial task to build qualitative models of industrial applications for model-based diagnosis. A Model Abstraction procedure is designed to automatically transform a quantitative model into qualitative model. If the data is monotone, the behavior can be easily abstracted using the corners of the bounding rectangle. Hence, many existing model abstraction approaches rely on monotonicity. But it is not a trivial problem to robustly detect monotone pieces from scattered data obtained by numerical simulation or experiments. This paper introduces an approach based on scale-dependent monotonicity: the notion that monotonicity can be defined relative to a scale. Real-valued functions defined on a finite set of reals e.g. simulation results, can be partitioned into quasi-monotone segments. The end points for the monotone segments are used as the initial set of landmarks for qualitative model abstraction. The qualitative model abstraction works as an iteratively refining process starting from the initial landmarks. The monotonicity analysis presented here can be used in constructing many other kinds of qualitative models; it is robust and computationally efficient.", "num_citations": "8\n", "authors": ["710"]}
{"title": "Functional dependencies with null markers\n", "abstract": " Functional dependencies (FDs) are an integral part of database design. However, they are only defined when we exclude null markers. However, we commonly use null markers in practice. To bridge this gap between theory and practice, researchers have proposed definitions of FDs over relations with null markers. Though sound, these definitions lack some qualities that we find desirable. For example, some fail to satisfy Armstrong's axioms\u2014while these axioms are part of the foundation of common database methodologies. We propose a set of properties that any extension of FDs over relations with null markers should possess. We then propose two new extensions having these properties. These extensions attempt to allow null markers where they make sense to practitioners. They both support Armstrong's axioms and provide realizablenull markers: at any time, some or all of the null markers can be\u00a0\u2026", "num_citations": "7\n", "authors": ["710"]}
{"title": "A web service composition method based on compact k2-trees\n", "abstract": " With the advent of cloud computing, a significant number of web services are available on the Internet. Services can be combined together when user's requirements are too complex to be solved by individual services. Since there are many services, searching a solution may require much storage. We propose to apply a compact data structure to represent the web service composition graph. To the best of our knowledge, our work is the first attempt to consider compact structure in solving the web service composition problem. Experimental results show that our method can find a valid solution to the composition problem, meanwhile, it takes less space and shows good scalability when handling a large number of web services.", "num_citations": "6\n", "authors": ["710"]}
{"title": "Fourier analysis of 2-point Hermite interpolatory subdivision schemes\n", "abstract": " Two subdivision schemes with Hermite data on \u2124 are studied. These schemes use 2 or 7 parameters respectively depending on whether Hermite data involve only first derivatives or include second derivatives. For a large region in the parameter space, the schemes are convergent in the space of Schwartz distributions. The Fourier transform of any interpolating function can be computed through products of matrices of order 2 or 3. The Fourier transform is related to a specific system of functional equations whose analytic solution is unique except for a multiplicative constant. The main arguments for these results come from Paley-Wiener-Schwartz theorem on the characterization of the Fourier transforms of distributions with compact support and a theorem of Artzrouni about convergent products of matrices.", "num_citations": "6\n", "authors": ["710"]}
{"title": "Hierarchical bin buffering: Online local moments for dynamic external memory arrays\n", "abstract": " For a massive I/O array of size n, we want to compute the first N local moments, for some constant N. Our simpler algorithms partition the array into consecutive ranges called bins, and apply not only to local-moment queries, but also to algebraic queries. With N buffers of size \u221an, time complexity drops to O(\u221an). A more sophisticated approach uses hierarchical buffering and has a logarithmic time complexity (O(b logb n)), when using N hierarchical buffers of size n/b. Using overlapped bin buffering, we show that only one buffer is needed, as with wavelet-based algorithms, but using much less storage.", "num_citations": "5\n", "authors": ["710"]}
{"title": "Diamond Dicing\n", "abstract": " In OLAP, analysts often select an interesting sample of the data. For example, an analyst might focus on products bringing revenues of at least $100,000, or on shops having sales greater than $400,000. However, current systems do not allow the application of both of these thresholds simultaneously, selecting products and shops satisfying both thresholds. For such purposes, we introduce the diamond cube operator, filling a gap among existing data warehouse operations.Because of the interaction between dimensions the computation of diamond cubes is challenging. We compare and test various algorithms on large data sets of more than 100\u00a0million facts. We find that while it is possible to implement diamonds in SQL, it is inefficient. Indeed, our custom implementation can be a hundred times faster than popular database engines (including a row-store and a column-store).", "num_citations": "4\n", "authors": ["710"]}
{"title": "Pruning attribute values from data cubes with diamond dicing\n", "abstract": " Data stored in a data warehouse are inherently multidimensional, unlike most data-pruning techniques (such as iceberg and top-k queries). However, analysts need to issue multidimensional queries. For example, an analyst may need to select not just the most profitable stores or---separately---the most profitable products, but simultaneous sets of stores and products fulfilling some profitability constraints. To fill this need, we propose a new operator, the diamond dice. Because of the interaction between dimensions, the computation of diamonds is challenging. We present the first diamond-dicing experiments on large data sets. Our external memory algorithm avoids potentially expensive random accesses. Experiments show that we can compute diamond cubes over fact tables containing 100 million facts and 500,000 distinct attribute values in less than an hour using a single-core PC.", "num_citations": "4\n", "authors": ["710"]}
{"title": "Upscaledb: Efficient Integer-Key Compression in a Key-Value Store using SIMD Instructions\n", "abstract": " Compression can sometimes improve performance by making more of the data available to the processors faster. We consider the compression of integer keys in a B+-tree index. For this purpose, systems such as IBM DB2 use variable-byte compression over differentially coded keys. We revisit this problem with various compression alternatives such as Google's VarIntGB, Binary Packing and Frame-of-Reference. In all cases, we describe algorithms that can operate directly on compressed data. Many of our alternatives exploit the single-instruction-multiple-data (SIMD) instructions supported by modern CPUs. We evaluate our techniques in a database environment provided by Upscaledb, a production-quality key-value database. Our best techniques are SIMD accelerated: they simultaneously reduce memory usage while improving single-threaded speeds. In particular, a differentially coded SIMD binary-packing\u00a0\u2026", "num_citations": "3\n", "authors": ["710"]}
{"title": "On the Challenges of Collaborative Data Processing\n", "abstract": " The last 30 years have seen the creation of a variety of electronic collaboration tools for science and business. Some of the best-known collaboration tools support text editing (eg, wikis). Wikipedia\u2019s success shows that large-scale collaboration can produce highly valuable content. Meanwhile much structured data is being collected and made publicly available. We have never had access to more powerful databases and statistical packages. Is large-scale collaborative data analysis now possible? Using a quantitative analysis of Web 2.0 data visualization sites, the authors find evidence that at least moderate open collaboration occurs. The authors then explore some of the limiting factors of collaboration over data.", "num_citations": "3\n", "authors": ["710"]}
{"title": "Unasssuming View-Size Estimation Techniques in OLAP\n", "abstract": " Even if storage was infinite, a data warehouse could not materialize all possible views due to the running time and update requirements. Therefore, it is necessary to estimate quickly, accurately, and reliably the size of views. Many available techniques make particular statistical assumptions and their error can be quite large. Unassuming techniques exist, but typically assume we have independent hashing for which there is no known practical implementation. We adapt an unassuming estimator due to Gibbons and Tirthapura: its theoretical bounds do not make unpractical assumptions. We compare this technique experimentally with stochastic probabilistic counting, LogLog probabilistic counting, and multifractal statistical models. Our experiments show that we can reliably and accurately (within 10%, 19 times out 20) estimate view sizes over large data sets (1.5 GB) within minutes, using almost no memory. However, only Gibbons-Tirthapura provides universally tight estimates irrespective of the size of the view. For large views, probabilistic counting has a small edge in accuracy, whereas the competitive sampling-based method (multifractal) we tested is an order of magnitude faster but can sometimes provide poor estimates (relative error of 100%). In our tests, LogLog probabilistic counting is not competitive. Experimental validation on the US Census 1990 data set and on the Transaction Processing Performance (TPC H) data set is provided.", "num_citations": "3\n", "authors": ["710"]}
{"title": "Collaborative OLAP with tag clouds\n", "abstract": " Increasingly, business projects are ephemeral. New Business Intelligence tools must support ad-lib data sources and quick perusal. Meanwhile, tag clouds are a popular community-driven visualization technique. Hence, we investigate tag-cloud views with support for OLAP operations such as roll-ups, slices, dices, clustering, and drill-downs. As a case study, we implemented an application where users can upload data and immediately navigate through its ad hoc dimensions. To support social networking, views can be easily shared and embedded in other Web sites. Algorithmically, our tag-cloud views are approximate range top-k queries over spontaneous data cubes. We present experimental evidence that iceberg cuboids provide adequate online approximations. We benchmark several browser-oblivious tag-cloud layout optimizations.", "num_citations": "3\n", "authors": ["710"]}
{"title": "Validating UTF-8 In Less Than One Instruction Per Byte\n", "abstract": " The majority of text is stored in UTF\u20108, which must be validated on ingestion. We present the lookupalgorithm, which outperforms UTF\u20108 validation routines used in many libraries and languages by more than 10\u00a0times using commonly available single\u2010instruction\u2010multiple\u2010data instructions. To ensure reproducibility, our work is freely available as open source software.", "num_citations": "2\n", "authors": ["710"]}
{"title": "On Desirable Semantics of Functional Dependencies over Databases with Incomplete Information\n", "abstract": " Codd\u2019s relational model describes just one possible world. To better cope with incomplete information, extended database models allow several possible worlds. Vague tables are one such convenient extended model where attributes accept sets of possible values (eg, the manager is either Jill or Bob). However, conceptual database design in such cases remains an open problem. In particular, there is no canonical definition of functional dependencies (FDs) over possible worlds (eg, each employee has just one manager). We identify several desirable properties that the semantics of such FDs should meet including Armstrong\u2019s axioms, the independence from irrelevant attributes, seamless satisfaction and implied by strong satisfaction. We show that we can define FDs such that they have all our desirable properties over vague tables. However, we also show that no notion of FD can satisfy all our desirable\u00a0\u2026", "num_citations": "2\n", "authors": ["710"]}
{"title": "An analysis tool for the contextual information from field experiments on driving fatigue\n", "abstract": " Elderly drivers will be more present on the road in the next few years. Mobility is fundamental for the elderly because it allows them to maintain an active lifestyle. But the elderly may suffer from cognitive, physical or sensorial decline due to aging. To help them to drive, context-aware systems can assess the status of a driver and warn him or her about hazards. We present a data analysis tool for car driving context information that includes data mining and statistical evaluation algorithms. We applied our system to data collected by sensors into an instrumented vehicle in realistic driving conditions. Results show that our tool is able to store the contextual information collected and to enable an interactive visualization of the data collected. Thanks to this tool, it is easier to share information among the scientists working on the data. Moreover, it makes it convenient to store data in the cloud.", "num_citations": "2\n", "authors": ["710"]}
{"title": "Roaring bitmap: nouveau mod\u00e8le de compression bitmap\n", "abstract": " Les index bitmap sont tr\u00e8s utilis\u00e9s dans les entrep\u00f4ts de donn\u00e9es et moteurs de recherche. Leur capacit\u00e9 \u00e0 ex\u00e9cuter efficacement des op\u00e9rations binaires entre bitmaps am\u00e9liore significativement les temps de r\u00e9ponse des requ\u00eates. Cependant, sur des attributs de hautes cardinalit\u00e9s, ils consomment un espace m\u00e9moire important. Ainsi, plusieurs techniques de compression bitmap ont \u00e9t\u00e9 introduites pour r\u00e9duire l'espace m\u00e9moire occup\u00e9 par ces index, et acc\u00e9l\u00e9rer leurs temps de traitement. Ce papier introduit un nouveau mod\u00e8le de compression bitmap, appel\u00e9 Roaring bitmap. Une comparaison exp\u00e9rimentale, sur des donn\u00e9es r\u00e9elles et synth\u00e9tiques, avec deux autres solutions de compression bitmap connues dans la litt\u00e9rature : WAH (Word Aligned Hybrid compression scheme) et Concise (Compressed \"n\" Composable integer Set), a montr\u00e9 que Roaring bitmap n'utilise que 25% d'espace m\u00e9moire compar\u00e9 \u00e0 WAH et 50% par rapport \u00e0 Concise, tout en acc\u00e9l\u00e9rant significativement les temps de calcul des op\u00e9rations logiques entre bitmaps (jusqu'\u00e0 1100 fois pour les intersections).", "num_citations": "2\n", "authors": ["710"]}
{"title": "Write Good Papers\n", "abstract": " 1. Verbs has to agree with their subjects. 2. Prepositions are not words to end sentences with. 3. And don't start a sentence with a conjunction. 4. It is wrong to ever split an infinitive. 5. Avoid cliches like the plague.(They're old hat) 6. Also, always avoid annoying alliteration. 7. Also too, never, ever use repetitive redundancies. 8. Be more or less specific. 9. Parenthetical remarks (however relevant) are (usually) unnecessary.", "num_citations": "2\n", "authors": ["710"]}
{"title": "Faster Sequential Search with a Two-Pass Dynamic-Time-Warping Lower Bound\n", "abstract": " The Dynamic Time Warping (DTW) is a popular similarity measure between time series. The DTW fails to satisfy the triangle inequality and its computation requires quadratic time. Hence, to find closest neighbors quickly, we use bounding techniques. We can avoid most DTW computations with an inexpensive lower bound (LB_Keogh). We compare LB_Keogh with a tighter lower bound (LB_Improved). We find that LB_Improved-based search is faster for sequential search. As an example, our approach is 3 times faster over random-walk and shape time series. We also review some of the mathematical properties of the DTW. We derive a tight triangle inequality for the DTW. We show that the DTW becomes the l_1 distance when time series are separated by a constant.", "num_citations": "2\n", "authors": ["710"]}
{"title": "White paper sur la compression d'images par la fast wavelet transform\n", "abstract": " La th\u00e9orie des ondelettes est encore tr\u00e8s r\u00e9cente. On estime g\u00e9n\u00e9ralement que l'ing\u00e9nieur fran\u00e7ais Morlet en est l'inventeur (d\u00e9but des ann\u00e9es 1980). Plus tard, un math\u00e9maticien fran\u00e7ais, Meyer, fut le premier \u00e0 donner des assises math\u00e9matiques solides \u00e0 la th\u00e9orie naissante (1986). Cependant, l'histoire retiendra que la math\u00e9maticienne am\u00e9ricaine Daubechies fut la premi\u00e8re \u00e0 fournir des ondelettes of industrial strength (1992).Daubechies a racont\u00e9 qu'elle imagina ses ondelettes alors qu'elle \u00e9tait l'invit\u00e9e des professeurs Deslauriers et Dubuc, de l'Universit\u00e9 de Montr\u00e9al, tous deux sp\u00e9cialistes de l'interpolation it\u00e9rative qui est une des bases de la th\u00e9orie moderne des ondelettes. Comme c'\u00e9tait l'hiver \u00e0 Montr\u00e9al et qu'il faisait tr\u00e8s froid, elle demeura prisonni\u00e8re de sa chambre d'h\u00f4tel durant tout son s\u00e9jour, forc\u00e9e \u00e0 travailler...", "num_citations": "2\n", "authors": ["710"]}
{"title": "Number Parsing at a Gigabyte per Second\n", "abstract": " With disks and networks providing gigabytes per second, parsing decimal numbers from strings becomes a bottleneck. We consider the problem of parsing decimal numbers to the nearest binary floating\u2010point value. The general problem requires variable\u2010precision arithmetic. However, we need at most 17\u00a0digits to represent 64\u2010bit standard floating\u2010point numbers (IEEE\u00a0754). Thus, we can represent the decimal significand with a single 64\u2010bit word. By combining the significand and precomputed tables, we can compute the nearest floating\u2010point number using as few as one or two 64\u2010bit multiplications. Our implementation can be several times faster than conventional functions present in standard C libraries on modern 64\u2010bit systems (Intel, AMD, ARM, and POWER9). Our work is available as open source software used by major systems such as Apache\u00a0Arrow and Yandex\u00a0ClickHouse. The Go standard library has\u00a0\u2026", "num_citations": "1\n", "authors": ["710"]}
{"title": "Base64 encoding and decoding at almost the speed of a memory copy\n", "abstract": " Many common document formats on the Internet are text\u2010only such as email (MIME) and the Web (HTML, JavaScript, JSON, and XML). To include images or executable code in these documents, we first encode them as text using base64. Standard base64 encoding uses 64\u00a0ASCII characters, ie, both lower and upper case Latin letters, digits and two other symbols. We show how we can encode and decode base64 data at nearly the speed of a memory copy (memcpy) on recent Intel processors, as long as the data does not fit in the first\u2010level (L1) cache. We use the single\u2010instruction\u2010multiple\u2010data instruction set AVX\u2010512 available on commodity processors. Our implementation generates several times fewer instructions than previous single\u2010instruction\u2010multiple\u2010data\u2010accelerated base64 codecs. It is also more versatile, as it can be adapted, even at runtime, to any base64 variant by only changing constants.", "num_citations": "1\n", "authors": ["710"]}
{"title": "Regular and almost universal hashing: an efficient implementation\n", "abstract": " Random hashing can provide guarantees regarding the performance of data structures such as hash tables \u2013 even in an adversarial setting. Many existing families of hash functions are universal: given two data objects, the probability that they have the same hash value is low given that we pick hash functions at random. However, universality fails to ensure that all hash functions are well behaved. We might further require regularity: when picking data objects at random they should have a low probability of having the same hash value, for any fixed hash function. We present the efficient implementation of a family of non\u2010cryptographic hash functions (PM+) offering good running times, good memory usage, and distinguishing theoretical guarantees: almost universality and component\u2010wise regularity. On a variety of platforms, our implementations are comparable with the state of the art in performance. On recent Intel\u00a0\u2026", "num_citations": "1\n", "authors": ["710"]}
{"title": "Discovering the smart forests with virtual reality\n", "abstract": " By analogy with the rise of the Internet of Things in cities and the emergence of\" smart cities\", we advocate the concept of a\" smart forest\". Such a constantly and deeply monitored forest can both maximize wood production and help combat climate change. However, as we work toward the acquisition of terabytes of sensor data, using inexpensive Internet-of-Things technology, we face an age-old data warehousing and Big Data challenge: how do we equip the decision makers and the scientists so that they can make sense of this deluge of data? We propose to go beyond conventional geographical information systems (GIS) and to leverage a concurrent technological trend: afordable virtual reality. We discuss upcoming challenges and opportunities.", "num_citations": "1\n", "authors": ["710"]}
{"title": "Nouveaux mod\u00e8les d\u2019index bitmap compress\u00e9s \u00e0 64 bits\n", "abstract": " Les index bitmap sont tr\u00e8s utilis\u00e9s dans les entrep\u00f4ts de donn\u00e9es et moteurs de recherche pour acc\u00e9l\u00e9rer les requ\u00eates d'interrogation. Leurs principaux avantages sont leur forme compacte et leur capacit\u00e9 \u00e0 tirer profit du traitement parall\u00e8le de bits dans les CPU (bit-level parallelism). Dans l'\u00e8re actuelle du Big Data, les collections de donn\u00e9es deviennent de plus en plus volumineuses. Les librairies d'index bitmap compress\u00e9s disponibles \u00e0 ce jour dans la litt\u00e9rature, telles que : Roaring bitmap, WAH ou Concise, ne supportent qu'au plus 4 milliards d'entr\u00e9es et sont tr\u00e8s souvent impraticables dans de tels contextes. Apr\u00e8s avoir constat\u00e9 ce besoin tant dans le milieu industriel que scientifique, nous proposons trois nouveaux mod\u00e8les d'index bitmap compress\u00e9s, bas\u00e9s sur le format de notre pr\u00e9c\u00e9dente contribution Roaring bitmap et qui supportent jusqu'\u00e0 2^64 entr\u00e9es. Des exp\u00e9riences sur des donn\u00e9es synth\u00e9tiques ont \u00e9t\u00e9 mises en oeuvre pour comparer les performances des trois nouvelles propositions avec la solution du moteur de recherche Apache Lucene : OpenBitSet, et d'autres collections Java. Les r\u00e9sultats ont montr\u00e9 que les trois nouvelles techniques ont \u00e9t\u00e9 pr\u00e8s de 300 millions de fois et 1800fois moins volumineuses en consommation m\u00e9moire qu'OpenBitSet et les collections Java, respectivement. Aussi, les trois nouveaux mod\u00e8les ont calcul\u00e9 des op\u00e9rations logiques, jusqu'\u00e0 6 millions de fois et jusqu'\u00e0 63 milles fois plus vite qu'OpenBitSet et les structures Java, respectivement.", "num_citations": "1\n", "authors": ["710"]}
{"title": "Threshold and Symmetric Functions over Bitmaps\n", "abstract": " Bitmap indexes are routinely used to speed up simple aggregate queries in databases. Set operations such as intersections, unions and complements can be represented as logical operations (AND, OR, NOT). However, less is known about the application of bitmap indexes to more advanced queries. We want to extend the applicability of bitmap indexes. As a starting point, we consider symmetric Boolean queries (e.g., threshold functions). For example, we might consider stores as sets of products, and ask for products that are on sale in 2 to 10 stores. Such symmetric Boolean queries generalize intersection, union, and T-occurrence queries. It may not be immediately obvious to an engineer how to use bitmap indexes for symmetric Boolean queries. Yet, maybe surprisingly, we find that the best of our bitmap-based algorithms are competitive with the state-of-the-art algorithms for important special cases (e.g., MergeOpt, MergeSkip, DivideSkip, ScanCount). Moreover, unlike the competing algorithms, the result of our computation is again a bitmap which can be further processed within a bitmap index. We review algorithmic design issues such as the aggregation of many compressed bitmaps. We conclude with a discussion on other advanced queries that bitmap indexes might be able to support efficiently.", "num_citations": "1\n", "authors": ["710"]}
{"title": "Extracting, Transforming and Archiving Scientific Data\n", "abstract": " It is becoming common to archive research datasets that are not only large but also numerous. In addition, their corresponding metadata and the software required to analyse or display them need to be archived. Yet the manual curation of research data can be difficult and expensive, particularly in very large digital repositories, hence the importance of models and tools for automating digital curation tasks. The automation of these tasks faces three major challenges: (1) research data and data sources are highly heterogeneous, (2) future research needs are difficult to anticipate, (3) data is hard to index. To address these problems, we propose the Extract, Transform and Archive (ETA) model for managing and mechanizing the curation of research data. Specifically, we propose a scalable strategy for addressing the research-data problem, ranging from the extraction of legacy data to its long-term storage. We review some existing solutions and propose novel avenues of research.", "num_citations": "1\n", "authors": ["710"]}
{"title": "Tri de la table de faits et compression des index bitmaps avec alignement sur les mots\n", "abstract": " Les index bitmaps sont souvent utilis\u00e9s pour indexer des donn\u00e9es multidimensionnelles. Ils utilisent principalement l\u2019acces s\u00e9quentiel aux donn\u00e9es, tanta l\u2019\u00e9criture qu\u2019a la lecture. Les bitmaps peuvent \u00eatre compress\u00e9s pour r\u00e9duire le co\u00fbt des entr\u00e9es/sorties tout en minimisant l\u2019utilisation du microprocesseur. Les techniques de compression les plus efficaces sont fond\u00e9es sur la compression par plage (run-length encoding) telle que la compression align\u00e9e sur les mots. Ce mode de compression permet d\u2019effectuer rapidement des op\u00e9rations logiques (AND, OR) sur les bitmaps. Cependant, la compression par plage d\u00e9pend de l\u2019ordre des faits. Nous proposons donc d\u2019exploiter le tri de la table de faits afin d\u2019am\u00e9liorer l\u2019efficacit\u00e9 des index bitmaps. Le tri lexicographique et par code de Gray, ainsi que le tri par bloc, sont \u00e9valu\u00e9s. Selon nos r\u00e9sultats exp\u00e9rimentaux, un simple tri lexicographique peut produire un index mieux compress\u00e9 (parfois deux fois plus petit) et qui peut \u00eatre plusieurs fois plus rapide. Le temps requis par le tri apporte un surco\u00fbt au temps total d\u2019indexation, mais ce surco\u00fbt est amorti par le fait que l\u2019indexation d\u2019une table tri\u00e9e est plus rapide. L\u2019ordre des colonnes peut avoir une influence d\u00e9terminante sur l\u2019efficacit\u00e9 du tri lexicographique: il est g\u00e9n\u00e9ralement pr\u00e9f\u00e9rable de placer les colonnes ayant plus de valeurs distinctes au d\u00e9but. Le tri par bloc sans fusion est beaucoup moins efficace qu\u2019un tri complet. De plus, le tri par code de Gray n\u2019est pas sup\u00e9rieur au tri lexicographique dans le cas de la compression align\u00e9e sur les mots.", "num_citations": "1\n", "authors": ["710"]}
{"title": "Recursive hashing and one-pass, one-hash n-gram count estimation\n", "abstract": " Many applications use sequences of n consecutive symbols (n-grams). We review n-gram hashing and prove that recursive hash families are pairwise independent at best. We prove that hashing by irreducible polynomials is pairwise independent whereas hashing by cyclic polynomials is quasi-pairwise independent: we make it pairwise independent by discarding n\u2212 1 bits. One application of hashing is to estimate the number of distinct n-grams, a view-size estimation problem. While view sizes can be estimated by sampling under statistical assumptions, we desire a statistically unassuming algorithm with universally valid accuracy bounds. Most related work has focused on repeatedly hashing the data, which is prohibitive for large data sources. We prove that a one-pass onehash algorithm is sufficient for accurate estimates if the hashing is sufficiently independent. For example, we can improve by a factor of 2 the theoretical bounds on estimation accuracy by replacing pairwise independent hashing by 4-wise independent hashing. We show that recursive random hashing is sufficiently independent in practice. Maybe surprisingly, our experiments showed that hashing by cyclic polynomials, which is only quasi-pairwise independent, sometimes outperformed 10-wise independent hashing while being twice as fast. For comparison, we measured the time to obtain exact n-gram counts using suffix arrays and show that, while we used hardly any storage, we were an order of magnitude faster. The experiments used a large collection of English text from Project Gutenberg as well as synthetic data.", "num_citations": "1\n", "authors": ["710"]}
{"title": "Tag-Cloud Drawing\n", "abstract": " Tag examples from popular Web sites Flickr items are photos. Del. icio. us items are URLs. Gmail items are email messages. Tag called \u201clabel\u201d. From: Mom< mom@ parents. org> Subject: Eat your veggies Promise me, Harry, that you will always eat at least two servings of...: family, advice", "num_citations": "1\n", "authors": ["710"]}
{"title": "One-Pass, One-Hash n-Gram Statistics Estimation\n", "abstract": " In multimedia, text or bioinformatics databases, applications query sequences of n consecutive symbols called n-grams. Estimating the number of distinct n-grams is a view-size estimation problem. While view sizes can be estimated by sampling under statistical assumptions, we desire an unassuming algorithm with universally valid accuracy bounds. Most related work has focused on repeatedly hashing the data, which is prohibitive for large data sources. We prove that a one-pass one-hash algorithm is sufficient for accurate estimates if the hashing is sufficiently independent. To reduce costs further, we investigate recursive random hashing algorithms and show that they are sufficiently independent in practice. We compare our running times with exact counts using suffix arrays and show that, while we use hardly any storage, we are an order of magnitude faster. The approach further is extended to a one-pass/one-hash computation of n-gram entropy and iceberg counts. The experiments use a large collection of English text from the Gutenberg Project as well as synthetic data.", "num_citations": "1\n", "authors": ["710"]}
{"title": "Canadian Semantic Web\n", "abstract": " Canadian Semantic Web is an edited volume based on the first Canadian Web Working Symposium, June 2006, in Quebec, Canada. It is the first edited volume based on this subject. This volume includes, but is not limited to, the following popular topics:\" Trust, Privacy, Security on the Semantic Web\",\" Semantic Grid and Semantic Grid Services\" and\" Semantic Web Mining\".", "num_citations": "1\n", "authors": ["710"]}
{"title": "Wavelet Shrinkage of LINAC III and Protons Synchrotron Booster Transformers by the Haar Transform\n", "abstract": " We have used wavelet shrinkage to reduce by 14% the noise level in the signal of the transformers used in some heavy ions accelerators. The loss of information is minimal compared to other techniques and our approach is non parametric. We provide some source code", "num_citations": "1\n", "authors": ["710"]}
{"title": "Sch\u00e9mas d'interpolation et ondelettes\n", "abstract": " Th\u00e9orie des ondelettes discr\u00e8tes et de l'interpolation it\u00e9rative -- D\u00e9riv\u00e9es de la fonction fondamentale de Deslauriers-Dubuc et les ondelettes B-adiques de Cohen-Daubechies-Feauveau -- Une famille d'ondelettes biorthogonales sur l'intervalle obtenue par un sch\u00e9ma d'interpolation it\u00e9rative dyadique -- Ondelettes non s\u00e9parables dans les r\u00e9gions rectangulaires du plan avec filtres interpolants.", "num_citations": "1\n", "authors": ["710"]}