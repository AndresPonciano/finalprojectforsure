{"title": "CSSV: Towards a realistic tool for statically detecting all buffer overflows in C\n", "abstract": " Erroneous string manipulations are a major source of software defects in C programs yielding vulnerabilities which are exploited by software viruses. We present C String Static Verifyer (CSSV), a tool that statically uncovers all string manipulation errors. Being a conservative tool, it reports all such errors at the expense of sometimes generating false alarms. Fortunately, only a small number of false alarms are reported, thereby proving that statically reducing software vulnerability is achievable. CSSV handles large programs by analyzing each procedure separately. To this end procedure contracts are allowed which are verified by the tool. We implemented a CSSV prototype and used it to verify the absence of errors in real code from EADS Airbus. When applied to another commonly used string intensive application, CSSV uncovered real bugs with very few false alarms.", "num_citations": "342\n", "authors": ["259"]}
{"title": "TVLA: A system for implementing static analyses\n", "abstract": " We present TVLA (Three-Valued-Logic Analyzer). TVLA is a \u201cYACC\u201d-like framework for automatically constructing static-analysis algorithms from an operational semantics, where the operational semantics is specified using logical formulae. TVLA has been implemented in Java and was successfully used to perform shape analysis on programs manipulating linked data structures (singly and doubly linked lists), to prove safety properties of Mobile Ambients, and to verify the partial correctness of several sorting programs.", "num_citations": "303\n", "authors": ["259"]}
{"title": "Data flow analysis: theory and practice\n", "abstract": " Data flow analysis is used to discover information for a wide variety of useful applications, ranging from compiler optimizations to software engineering and verification. Modern compilers apply it to produce performance-maximizing code, and software engineers use it to re-engineer or reverse engineer programs and verify the integrity of their programs.", "num_citations": "269\n", "authors": ["259"]}
{"title": "Verifying properties of binarized deep neural networks\n", "abstract": " Understanding properties of deep neural networks is an important challenge in deep learning. In this paper, we take a step in this direction by proposing a rigorous way of verifying properties of a popular class of neural networks, Binarized Neural Networks, using the well-developed means of Boolean satisfiability. Our main contribution is a construction that creates a representation of a binarized neural network as a Boolean formula. Our encoding is the first exact Boolean representation of a deep neural network. Using this encoding, we leverage the power of modern SAT solvers along with a proposed counterexample-guided search procedure to verify various properties of these networks. A particular focus will be on the critical property of robustness to adversarial perturbations. For this property, our experimental results demonstrate that our approach scales to medium-size deep neural networks used in image classification tasks. To the best of our knowledge, this is the first work on verifying properties of deep neural networks using an exact Boolean encoding of the network.", "num_citations": "141\n", "authors": ["259"]}
{"title": "Ivy: safety verification by interactive generalization\n", "abstract": " Despite several decades of research, the problem of formal verification of infinite-state systems has resisted effective automation. We describe a system---Ivy---for interactively verifying safety of infinite-state systems. Ivy's key principle is that whenever verification fails, Ivy graphically displays a concrete counterexample to induction. The user then interactively guides generalization from this counterexample. This process continues until an inductive invariant is found. Ivy searches for universally quantified invariants, and uses a restricted modeling language. This ensures that all verification conditions can be checked algorithmically. All user interactions are performed using graphical models, easing the user's task. We describe our initial experience with verifying several distributed protocols.", "num_citations": "141\n", "authors": ["259"]}
{"title": "Online detection of effectively callback free objects with applications to smart contracts\n", "abstract": " Callbacks are essential in many programming environments, but drastically complicate program understanding and reasoning because they allow to mutate object's local states by external objects in unexpected fashions, thus breaking modularity. The famous DAO bug in the cryptocurrency framework Ethereum, employed callbacks to steal $150M. We define the notion of Effectively Callback Free (ECF) objects in order to allow callbacks without preventing modular reasoning.   An object is ECF in a given execution trace if there exists an equivalent execution trace without callbacks to this object. An object is ECF if it is ECF in every possible execution trace. We study the decidability of dynamically checking ECF in a given execution trace and statically checking if an object is ECF. We also show that dynamically checking ECF in Ethereum is feasible and can be done online. By running the history of all execution traces\u00a0\u2026", "num_citations": "137\n", "authors": ["259"]}
{"title": "Shape analysis\n", "abstract": " A shape-analysis algorithm statically analyzes a program to determine information about the heap-allocated data structures that the program manipulates. The results can be used to understand or verify programs. They also contain information valuable for debugging, compile-time garbage collection, instruction scheduling, and parallelization.", "num_citations": "124\n", "authors": ["259"]}
{"title": "Cleanness checking of string manipulations in C programs via integer analysis\n", "abstract": " All practical C programs use structures, arrays, and/or strings. At runtime, such objects are mapped into consecutive memory locations, hereafter referred to as buffers. Many software defects are caused by buffer overflow \u2014 unintentional access to memory outside the intended object. String manipulation is a major source of such defects. According to the FUZZ study, they are the cause of most UNIX failures.               We present a new algorithm for statically detecting buffer overflow defects caused by string manipulations in C programs. In many programs, our algorithm is capable of precisely handling destructive memory updates, even in the presence of overlapping pointer variables which reference the same buffer at different offsets. Thus, our algorithm can uncover defects which go undetected by previous works. We reduce the problem of checking string manipulation to that of analyzing integer variables\u00a0\u2026", "num_citations": "111\n", "authors": ["259"]}
{"title": "Heap profiling for space-efficient Java\n", "abstract": " We present a heap-profiling tool for exploring the potential for space savings in Java programs. The output of the tool is used to direct rewriting of application source code in a way that allows more timely garbage collection (GC) of objects, thus saving space. The rewriting can also avoid allocating some objects that are never used. The tool measures the difference between the actual collection time and the potential earliest collection time of objects for a Java application. This time difference indicates potential savings. Then the tool sorts the allocation sites in the application source code according to the accumulated potential space saving for the objects allocated at the sites. A programmer can investigate the source code surrounding the sites with the highest savings to find opportunities for code rewriting that could save space. Our experience shows that in many cases simple code rewriting leads to actual space\u00a0\u2026", "num_citations": "102\n", "authors": ["259"]}
{"title": "Interprocedural shape analysis for recursive programs\n", "abstract": " A shape-analysis algorithm statically analyzes a program to determine information about the heap-allocated data structures that the program manipulates. The results can be used to optimize, understand, debug, or verify programs. Existing algorithms are quite imprecise in the presence of recursive procedure calls. This is unfortunate, since recursion provides a natural way to manipulate linked data structures. We present a novel technique for shape analysis of recursive programs. An algorithm based on our technique has been implemented. It handles programs manipulating linked lists written in a subset of C. The algorithm is significantly more precise than existing algorithms. For example, it can verify the absence of memory leaks in many recursive programs; this is beyond the capabilities of existing algorithms.", "num_citations": "91\n", "authors": ["259"]}
{"title": "Deriving specialized program analyses for certifying component-client conformance\n", "abstract": " We are concerned with the problem of statically certifying (verifying) whether the client of a software component conforms to the component's constraints for correct usage. We show how conformance certification can be efficiently carried out in a staged fashion for certain classes of first-order safety (FOS) specifications, which can express relationship requirements among potentially unbounded collections of runtime objects. In the first stage of the certification process, we systematically derive an abstraction that is used to model the component state during analysis of arbitrary clients. In general, the derived abstraction will utilize first-order predicates, rather than the propositions often used by model checkers. In the second stage, the generated abstraction is incorporated into a static analysis engine to produce a certifier. In the final stage, the resulting certifier is applied to a client to conservatively determine whether the\u00a0\u2026", "num_citations": "90\n", "authors": ["259"]}
{"title": "Checking cleanness in linked lists\n", "abstract": " A new algorithm is presented that automatically uncovers memory errors such as NULL pointers dereference and memory leaks in C programs. The algorithm is conservative, i.e., it can never miss an error but may report \u201cfalse alarms\u201d. When applied to several intricate C programs manipulating singly linked lists, the new algorithm yields more accurate results, does not report any false alarm and usually runs even faster and consumes less space than a less precise algorithm.", "num_citations": "83\n", "authors": ["259"]}
{"title": "Verifying reachability in networks with mutable datapaths\n", "abstract": " Recent work has made great progress in verifying the forwarding correctness of networks [26\u201328, 35]. However, these approaches cannot be used to verify networks containing middleboxes, such as caches and firewalls, whose forwarding behavior depends on previously observed traffic. We explore how to verify reachability properties for networks that include such \u201cmutable datapath\u201d elements, both for the original network and in the presence of failures. The main challenge lies in handling large and complicated networks. We achieve scaling by developing and leveraging the concept of slices, which allow network-wide verification to only require analyzing small portions of the network. We show that with slices the time required to verify an invariant on many production networks is independent of the size of the network itself.", "num_citations": "82\n", "authors": ["259"]}
{"title": "A relational approach to interprocedural shape analysis\n", "abstract": " This paper addresses the verification of properties of imperative programs with recursive procedure calls, heap-allocated storage, and destructive updating of pointer-valued fields \u2013 i.e., interprocedural shape analysis. It presents a way to harness some previously known approaches to interprocedural dataflow analysis \u2013 which in past work have been applied only to much less rich settings \u2013 for interprocedural shape analysis.", "num_citations": "81\n", "authors": ["259"]}
{"title": "Automatic removal of array memory leaks in Java\n", "abstract": " Current garbage collection (GC) techniques do not (and in general cannot) collect all the garbage that a program produces. This may lead to a performance slowdown and to programs running out of memory space.             In this paper, we present a practical algorithm for statically detecting memory leaks occurring in arrays of objects in a garbage collected environment. No previous algorithm exists. The algorithm is conservative, i.e., it never detects a leak on a piece of memory that is subsequently used by the program, although it may fail to identify some leaks. The presence of the detected leaks is exposed to the garbage collector, thus allowing GC to collect more storage.             We have instrumented the Java virtual machine to measure the effect of memory leaks in arrays. Our initial experiments indicate that this problem occurs in many Java applications. Our measurements of heap size show improvement\u00a0\u2026", "num_citations": "75\n", "authors": ["259"]}
{"title": "Paxos made EPR: decidable reasoning about distributed protocols\n", "abstract": " Distributed protocols such as Paxos play an important role in many computer systems. Therefore, a bug in a distributed protocol may have tremendous effects. Accordingly, a lot of effort has been invested in verifying such protocols. However, checking invariants of such protocols is undecidable and hard in practice, as it requires reasoning about an unbounded number of nodes and messages. Moreover, protocol actions and invariants involve both quantifier alternations and higher-order concepts such as set cardinalities and arithmetic.   This paper makes a step towards automatic verification of such protocols. We aim at a technique that can verify correct protocols and identify bugs in incorrect protocols. To this end, we develop a methodology for deductive verification based on effectively propositional logic (EPR)\u2014a decidable fragment of first-order logic (also known as the Bernays-Sch\u00f6nfinkel-Ramsey class). In\u00a0\u2026", "num_citations": "71\n", "authors": ["259"]}
{"title": "Generalizing DPLL to richer logics\n", "abstract": " The DPLL approach to the Boolean satisfiability problem (SAT) is a combination of search for a satisfying assignment and logical deduction, in which each process guides the other. We show that this approach can be generalized to a richer class of theories. In particular, we present an alternative to lazy SMT solvers, in which DPLL is used only to find propositionally satisfying assignments, whose feasibility is checked by a separate theory solver. Here, DPLL is applied directly to the theory. We search in the space of theory structures (for example, numerical assignments) rather than propositional assignments. This makes it possible to use conflict in model search to guide deduction in the theory, much in the way that it guides propositional resolution in DPLL. Some experiments using linear rational arithmetic demonstrate the potential advantages of the approach.", "num_citations": "69\n", "authors": ["259"]}
{"title": "Effectively-propositional reasoning about reachability in linked data structures\n", "abstract": " This paper proposes a novel method of harnessing existing SAT solvers to verify reachability properties of programs that manipulate linked-list data structures. Such properties are essential for proving program termination, correctness of data structure invariants, and other safety properties. Our solution is complete, i.e., a SAT solver produces a counterexample whenever a program does not satisfy its specification. This result is surprising since even first-order theorem provers usually cannot deal with reachability in a complete way, because doing so requires reasoning about transitive closure.               Our result is based on the following ideas: (1) Programmers must write assertions in a restricted logic without quantifier alternation or function symbols. (2) The correctness of many programs can be expressed in such restricted logics, although we explain the tradeoffs. (3) Recent results in descriptive complexity\u00a0\u2026", "num_citations": "61\n", "authors": ["259"]}
{"title": "Partially disjunctive heap abstraction\n", "abstract": " One of the continuing challenges in abstract interpretation is the creation of abstractions that yield analyses that are both tractable and precise enough to prove interesting properties about real-world programs. One source of difficulty is the need to handle programs with different behaviors along different execution paths. Disjunctive (powerset) abstractions capture such distinctions in a natural way. However, in general, powerset abstractions increase space and time costs by an exponential factor. Thus, powerset abstractions are generally perceived as very costly.               In this paper, we partially address this challenge by presenting and empirically evaluating a new heap abstraction. The new heap abstraction works by merging shape descriptors according to a partial isomorphism similarity criteria, resulting in a partially disjunctive abstraction.               We implemented this abstraction in TVLA \u2013 a generic\u00a0\u2026", "num_citations": "60\n", "authors": ["259"]}
{"title": "On effectiveness of GC in Java\n", "abstract": " We study the effectiveness of garbage collection (GC) algorithms by measuring the time difference between the actual collection time of an object and the potential earliest collection time for that object. Our ultimate goal is to use this study in order to develop static analysis techniques that can be used together with GC to allow earlier reclamation of objects. The results may also be used to pinpoint application source code that could be rewritten in a way that would allow more timely GC.", "num_citations": "44\n", "authors": ["259"]}
{"title": "Estimating the impact of heap liveness information on space consumption in Java\n", "abstract": " We study the potential impact of different kinds of liveness information on the space consumption of a program in a garbage collected environment, specifically for Java. The idea is to measure the time difference between the actual time an object is collected by the garbage collector (GC) and the potential earliest time an object could be collected assuming liveness information were available. We focus on the following kinds of liveness information:(i) stack-reference liveness (local reference variable liveness in Java),(ii) global-reference liveness (static reference variable liveness in Java),(iii) heap-reference liveness (instance reference variable liveness or array reference liveness in Java), and (vi) any combination of (i)-(iii). We also provide some insights on the kind of interface between a compiler and GC that could achieve these potential savings. The Java Virtual Machine (JVM) was instrumented to measure\u00a0\u2026", "num_citations": "43\n", "authors": ["259"]}
{"title": "A logic-based approach to data flow analysis problems\n", "abstract": " A new uniform formalism for tracking static properties of programs is presented. The formalism annotates each point in a program with static assertions, i.e., assertions which hold independently of the specific execution path leading to this point.             The novel idea is to use Horn clauses to specify the consistent environments and the meaning of program operations.             The abstract interpretation technique is used for finding conservative approximations to the static assertions and to analyze the accuracy of the approximation.             The formalism is used to specify and solve the problem of tracking pointer equality and allocations in Pascal-like languages. It is shown that the solutions are optimal under some natural assumptions.", "num_citations": "42\n", "authors": ["259"]}
{"title": "Abstraction for shape analysis with fast and precise transformers\n", "abstract": " This paper addresses the problem of proving safety properties of imperative programs manipulating dynamically allocated data structures using destructive pointer updates. We present a new abstraction for linked data structures whose underlying graphs do not contain cycles. The abstraction is simple and allows us to decide reachability between dynamically allocated heap cells.               We present an efficient algorithm that computes the effect of low level heap mutations in the most precise way. The algorithm does not rely on the usage of a theorem prover. In particular, the worst case complexity of computing a single successor abstract state is O(V logV) where V is the number of program variables. The overall number of successor abstract states can be exponential in V. A prototype of the algorithm was implemented and is shown to be fast.               Our method also handles programs with \u201csimple cycles\u00a0\u2026", "num_citations": "40\n", "authors": ["259"]}
{"title": "Install-time vaccination of Windows executables to defend against stack smashing attacks\n", "abstract": " Stack smashing is still one of the most popular techniques for computer system attack. In this work, we present an anti-stack-smashing defense technique for Microsoft Windows systems. Our approach works at install-time, and does not rely on having access to the source-code: The user decides when and which executables to vaccinate. Our technique consists of instrumenting a given executable with a mechanism to detect stack smashing attacks. We developed a prototype implementing our technique and verified that it successfully defends against actual exploit code. We then extended our prototype to vaccinate DLLs, multithreaded applications, and DLLs used by multithreaded applications, which present significant additional complications. We present promising performance results measured on SPEC2000 benchmarks: Vaccinated executables were no more than 8 percent slower than their un-vaccinated\u00a0\u2026", "num_citations": "39\n", "authors": ["259"]}
{"title": "Decidability of inferring inductive invariants\n", "abstract": " Induction is a successful approach for verification of hardware and software systems. A common practice is to model a system using logical formulas, and then use a decision procedure to verify that some logical formula is an inductive safety invariant for the system. A key ingredient in this approach is coming up with the inductive invariant, which is known as invariant inference. This is a major difficulty, and it is often left for humans or addressed by sound but incomplete abstract interpretation. This paper is motivated by the problem of inductive invariants in shape analysis and in distributed protocols. This paper approaches the general problem of inferring first-order inductive invariants by restricting the language L of candidate invariants. Notice that the problem of invariant inference in a restricted language L differs from the safety problem, since a system may be safe and still not have any inductive invariant in L that\u00a0\u2026", "num_citations": "38\n", "authors": ["259"]}
{"title": "Modular reasoning about heap paths via effectively propositional formulas\n", "abstract": " First order logic with transitive closure, and separation logic enable elegant interactive verification of heap-manipulating programs. However, undecidabilty results and high asymptotic complexity of checking validity preclude complete automatic verification of such programs, even when loop invariants and procedure contracts are specified as formulas in these logics. This paper tackles the problem of procedure-modular verification of reachability properties of heap-manipulating programs using efficient decision procedures that are complete: that is, a SAT solver must generate a counterexample whenever a program does not satisfy its specification. By (a) requiring each procedure modifies a fixed set of heap partitions and creates a bounded amount of heap sharing, and (b) restricting program contracts and loop invariants to use only deterministic paths in the heap, we show that heap reachability updates can be\u00a0\u2026", "num_citations": "37\n", "authors": ["259"]}
{"title": "Decidable fragments of many-sorted logic\n", "abstract": " Many natural specifications use types. We investigate the decidability of fragments of many-sorted first-order logic. We identified some decidable fragments and illustrated their usefulness by formalizing specifications considered in the literature. Often the intended interpretations of specifications are finite. We prove that the formulas in these fragments are valid iff they are valid over the finite structures. We extend these results to logics that allow a restricted form of transitive closure.We tried to extend the classical classification of the quantifier prefixes into decidable/undecidable classes to the many-sorted logic. However, our results indicate that a naive extension fails and more subtle classification is needed.", "num_citations": "36\n", "authors": ["259"]}
{"title": "Automatic removal of array memory leaks\n", "abstract": " A method for memory management in execution of a program by a computer having a memory includes identifying in the program an array of array elements. At a given point in the program, a range of the elements is determined within the array such that none of the elements in the array outside the range is alive at the point. Information regarding the determined range is passed to a memory management function, so that memory locations are associated with the array elements, responsive to the determined range.", "num_citations": "36\n", "authors": ["259"]}
{"title": "Compactly representing first-order structures for static analysis\n", "abstract": " A fundamental bottleneck in applying sophisticated static analyses to large programs is the space consumed by abstract program states. This is particularly true when analyzing programs that make extensive use of heap-allocated data. The TVLA Three-Valued Logic Analysis) program analysis and verification system models dynamic allocation precisely by representing program states as first-order structures. In such a representation, a finite collection of predicates is used to define states; the predicates range over a universe of individuals that may evolve\u2014expand and contract\u2014during analysis. Evolving first-order structures can be used to encode a wide variety of analyses, including most analyses whose abstract states are represented by directed graphs or maps. This paper addresses the problem of space consumption in such analyses by describing and evaluating two novel structure representation\u00a0\u2026", "num_citations": "36\n", "authors": ["259"]}
{"title": "Detecting memory errors via static pointer analysis (preliminary experience)\n", "abstract": " Programs which manipulate pointers are hard to debug. Pointer analysis algorithms (originally aimed at optimizing compilers) may provide some remedy by identifying potential errors such as dereferencing NULL pointers by statically analyzing the behavior of programs on all their input data.Our goal is to identify the \"core program analysis techniques\" that can be used when developing realistic tools which detect memory errors at compile time without, generating too many false alarms. Our preliminary experience indicates that the following techniques are necessary: (i) finding aliases between pointers, (ii) flow sensitive techniques that account, for the program control flow constructs. (iii) partial interpretation of conditional statements, (iv) analysis of the relationships between pointers, and sometimes (v) analysis of the underlying data structures manipulated by the C program.We show that a combination of these\u00a0\u2026", "num_citations": "36\n", "authors": ["259"]}
{"title": "Modularity for decidability of deductive verification with applications to distributed systems\n", "abstract": " Proof automation can substantially increase productivity in formal verification of complex systems. However, unpredictablility of automated provers in handling quantified formulas presents a major hurdle to usability of these tools. We propose to solve this problem not by improving the provers, but by using a modular proof methodology that allows us to produce decidable verification conditions. Decidability greatly improves predictability of proof automation, resulting in a more practical verification approach. We apply this methodology to develop verified implementations of distributed protocols, demonstrating its effectiveness.", "num_citations": "33\n", "authors": ["259"]}
{"title": "Decentralizing SDN policies\n", "abstract": " Software-defined networking (SDN) is a new paradigm for operating and managing computer networks. SDN enables logically-centralized control over network devices through a \"controller\" --- software that operates independently of the network hardware. Network operators can run both in-house and third-party SDN programs on top of the controller, e.g., to specify routing and access control policies. In practice, having the controller handle events limits the network scalability. Therefore, the feasibility of SDN depends on the ability to efficiently decentralize network event-handling by installing forwarding rules on the switches. However, installing a rule too early or too late may lead to incorrect behavior, e.g., (1) packets may be forwarded to the wrong destination or incorrectly dropped; (2) packets handled by the switch may hide vital information from the controller, leading to incorrect forwarding behavior. The second\u00a0\u2026", "num_citations": "33\n", "authors": ["259"]}
{"title": "Verifying isolation properties in the presence of middleboxes\n", "abstract": " Great progress has been made recently in verifying the correctness of router forwarding tables. However, these approaches do not work for networks containing middleboxes such as caches and firewalls whose forwarding behavior depends on previously observed traffic. We explore how to verify isolation properties in networks that include such \"dynamic datapath\" elements using model checking. Our work leverages recent advances in SMT solvers, and the main challenge lies in scaling the approach to handle large and complicated networks. While the straightforward application of model checking to this problem can only handle very small networks (if at all), our approach can verify simple realistic invariants on networks containing 30,000 middleboxes in a few minutes.", "num_citations": "33\n", "authors": ["259"]}
{"title": "A dynamic evaluation of the precision of static heap abstractions\n", "abstract": " The quality of a static analysis of heap-manipulating programs is largely determined by its heap abstraction. Object allocation sites are a commonly-used abstraction, but are too coarse for some clients. The goal of this paper is to investigate how various refinements of allocation sites can improve precision. In particular, we consider abstractions that use call stack, object recency, and heap connectivity information. We measure the precision of these abstractions dynamically for four different clients motivated by concurrency and on nine Java programs chosen from the DaCapo benchmark suite. Our dynamic results shed new light on aspects of heap abstractions that matter for precision, which allows us to more effectively navigate the large space of possible heap abstractions", "num_citations": "33\n", "authors": ["259"]}
{"title": "Combining shape analyses by intersecting abstractions\n", "abstract": " We consider the problem of computing the intersection (meet) of heap abstractions. This problem is useful, among other applications, to relate abstract memory states computed by forward analysis with abstract memory states computed by backward analysis. Since dynamically allocated heap objects have no static names, relating objects computed by different analyses cannot be done directly. We show that the problem of computing meet is computationally hard. We describe a constructive formulation of meet based on certain relations between abstract heap objects. The problem of enumerating those relations is reduced to finding constrained matchings in graphs. We implemented the algorithm in the TVLA system and used it to prove temporal heap properties of several small Java programs, and obtained empirical evidence showing the effectiveness of the meet algorithm.", "num_citations": "33\n", "authors": ["259"]}
{"title": "Some complexity results for stateful network verification\n", "abstract": " In modern networks, forwarding of packets often depends on the history of previously transmitted traffic. Such networks contain stateful middleboxes, whose forwarding behavior depends on a mutable internal state. Firewalls and load balancers are typical examples of stateful middleboxes.                 This paper addresses the complexity of verifying safety properties, such as isolation, in networks with finite-state middleboxes. Unfortunately, we show that even in the absence of forwarding loops, reasoning about such networks is undecidable due to interactions between middleboxes connected by unbounded ordered channels. We therefore abstract away channel ordering. This abstraction is sound for safety, and makes the problem decidable. Specifically, we show that safety checking is EXPSPACE-complete in the number of hosts and middleboxes in the network. We further identify two useful subclasses of\u00a0\u2026", "num_citations": "32\n", "authors": ["259"]}
{"title": "Secure serverless computing using dynamic information flow control\n", "abstract": " The rise of serverless computing provides an opportunity to rethink cloud security. We present an approach for securing serverless systems using a novel form of dynamic information flow control (IFC). We show that in serverless applications, the termination channel found in most existing IFC systems can be arbitrarily amplified via multiple concurrent requests, necessitating a stronger termination-sensitive non-interference guarantee, which we achieve using a combination of static labeling of serverless processes and dynamic faceted labeling of persistent data. We describe our implementation of this approach on top of JavaScript for AWS Lambda and OpenWhisk serverless platforms, and present three realistic case studies showing that it can enforce important IFC security properties with low overhead.", "num_citations": "31\n", "authors": ["259"]}
{"title": "Scaling model checking of dataraces using dynamic information\n", "abstract": " Dataraces in multithreaded programs often indicate severe bugs and can cause unexpected behaviors when different thread interleavings are executed. Because dataraces are a cause for concern, many works have dealt with the problem of detecting them. Works based on dynamic techniques either report errors only for dataraces that occur in the current interleaving, which limits their usefulness, or produce many spurious dataraces. Works based on model checking search exhaustively for dataraces and thus can reveal even those that occur in rarely executed paths. However, the applicability of model checking is limited because the large number of thread interleavings in realistic multithreaded programs causes state space explosion. In this work, we combine the two techniques in a hybrid scheme which overcomes these difficulties and enjoys the advantages of both worlds. Our hybrid technique succeeds in\u00a0\u2026", "num_citations": "30\n", "authors": ["259"]}
{"title": "Core-guided minimal correction set and core enumeration\n", "abstract": " A set of constraints is unsatisfiable if there is no solution that satisfies these constraints. To analyse unsatisfiable problems, the user needs to understand where inconsistencies come from and how they can be repaired. Minimal unsatisfiable cores and correction sets are important subsets of constraints that enable such analysis. In this work, we propose a new algorithm for extracting minimal unsatisfiable cores and correction sets simultaneously. Building on top of the relaxation and strengthening framework, we introduce novel techniques for extracting these sets. Our new solver significantly outperforms several state of the art algorithms on common benchmarks when it comes to extracting correction sets and compares favorably on core extraction.", "num_citations": "29\n", "authors": ["259"]}
{"title": "Verification in the age of microservices\n", "abstract": " Many large applications are now built using collections of microservices, each of which is deployed in isolated containers and which interact with each other through the use of remote procedure calls (RPCs). The use of microservices improves scalability--each component of an application can be scaled independently--and deployability. However, such applications are inherently distributed and current tools do not provide mechanisms to reason about and ensure their global behavior. In this paper we argue that recent advances in formal methods and software packet processing pave the path towards building mechanisms that can ensure correctness for such systems, both when they are being built and at runtime. These techniques impose minimal runtime overheads and are amenable to production deployments.", "num_citations": "28\n", "authors": ["259"]}
{"title": "TVLA: A system for generating abstract interpreters\n", "abstract": " TVLA (Three-Valued-Logic Analyzer) is a \u201cYACC\u201d-like framework for automatically constructing abstract interpreters from an operational semantics. The operational semantics is specified as a generic transition system based on first-order logic. TVLA was implemented in Java and successfully used to prove interesting properties of (concurrent) Java programs manipulating dynamically allocated linked data structures.", "num_citations": "28\n", "authors": ["259"]}
{"title": "Customization change impact analysis for ERP professionals via program slicing\n", "abstract": " We describe a new tool that automatically identifies impact of customization changes, ie, how changes affect software behavior. As opposed to existing static analysis tools that aim at aiding programmers or improve performance, our tool is designed for end-users without prior knowledge in programming. We utilize state-of-the-art static analysis algorithms for the programs within an Enterprise Resource Planning system (ERP). Key challenges in analyzing real world ERP programs are their significant size and the interdependency between programs. In particular, we describe and compare three customization change impact analyses for real-world programs, and a balancing algorithm built upon the three independent analyses. This paper presents PanayaImpactAnalysis (PanayaIA), a web on-demand tool, providing ERP professionals a clear view of the impact of a customization change on the system. In addition\u00a0\u2026", "num_citations": "27\n", "authors": ["259"]}
{"title": "Verifying equivalence of spark programs\n", "abstract": " Apache Spark is a popular framework for writing large scale data processing applications. Our long term goal is to develop automatic tools for reasoning about Spark programs. This is challenging because Spark programs combine database-like relational algebraic operations and aggregate operations, corresponding to (nested) loops, with User Defined Functions (UDFs). In this paper, we present a novel SMT-based technique for verifying the equivalence of Spark programs.                 We model Spark as a programming language whose semantics imitates Relational Algebra queries (with aggregations) over bags (multisets) and allows for UDFs expressible in Presburger Arithmetics. We prove that the problem of checking equivalence is undecidable even for programs which use a single aggregation operator. Thus, we present sound techniques for verifying the equivalence of interesting classes of\u00a0\u2026", "num_citations": "23\n", "authors": ["259"]}
{"title": "Decidable fragments of many-sorted logic\n", "abstract": " We investigate the possibility of developing a decidable logic which allows expressing a large variety of real world specifications. The idea is to define a decidable subset of many-sorted (typed) first- order logic. The motivation is that types simplify the complexity of mixed quantifiers when they quantify over different types. We noticed that many real world verification problems can be formalized by quantifying over different types in such a way that the relations between types remain simple.             Our main result is a decidable fragment of many-sorted first-order logic that captures many real world specifications.", "num_citations": "22\n", "authors": ["259"]}
{"title": "Abstract transformers for thread correlation analysis\n", "abstract": " We present a new technique for speeding up static analysis of (shared memory) concurrent programs. We focus on analyses that compute thread correlations: such analyses infer invariants that capture correlations between the local states of different threads (as well as the global state). Such invariants are required for verifying many natural properties of concurrent programs.               Tracking correlations between different thread states, however, is very expensive. A significant factor that makes such analysis expensive is the cost of applying abstract transformers. In this paper, we introduce a technique that exploits the notion of footprints and memoization to compute individual abstract transformers more efficiently.               We have implemented this technique in our concurrent shape analysis framework. We have used this implementation to prove properties of fine-grained concurrent programs with a shared\u00a0\u2026", "num_citations": "20\n", "authors": ["259"]}
{"title": "Building a bridge between pointer aliases and program dependences\n", "abstract": " In this paper we present a surprisingly simple reduction of the program dependence problem to the may-alias problem. While both problems are undecidable, providing a bridge between them has great practical importance. Program dependence information is used extensively in compiler optimizations, automatic program parallelizations, code scheduling in super-scalar machines, and in software engineering tools such as code slicers. When working with languages that support pointers and references, these systems are forced to make very conservative assumptions. This leads to many superfluous program dependences and limits compiler performance and the usability of software engineering tools. Fortunately, there are many algorithms for computing conservative approximations to the may-alias problem. The reduction has the important property of always computing conservative program\u00a0\u2026", "num_citations": "20\n", "authors": ["259"]}
{"title": "Self-stabilization preserving compiler\n", "abstract": " Self-stabilization is an elegant approach for designing fault tolerant systems. A system is considered self-stabilizing if, starting in any state, it converges to the desired behavior. Self-stabilizing algorithms were designed for solving fundamental distributed tasks, such as leader election, token circulation and communication network protocols. The algorithms were expressed using guarded commands or pseudo-code. The realization of these algorithms requires the existence of a (self-stabilizing) infrastructure such as a self-stabilizing microprocessor and a self-stabilizing operating system for their execution. Moreover, the high-level description of the algorithms needs to be converted into machine language of the microprocessor. In this article, we present our design for a self-stabilization preserving compiler. The compiler we designed and implemented transforms programs written in a language similar to the abstract\u00a0\u2026", "num_citations": "18\n", "authors": ["259"]}
{"title": "Optimizing C multithreaded memory management using thread-local storage\n", "abstract": " Dynamic memory management in C programs can be rather costly. Multithreading introduces additional synchronization overhead of C memory management functions (malloc, free). In order to reduce this overhead, we extended Hoard \u2014 a state of the art memory allocator with the ability to allocate thread-local storage. Experimental results using the tool show runtime saving of up to 44% for a set of memory management benchmarks.               To allow transparent usage of thread-local storage, we develop a compile-time algorithm, which conservatively detects allocation sites that can be replaced by thread-local allocations. Our static analysis is sound, i.e., every detected thread-local storage is indeed so, although we may fail to identify opportunities for allocating thread-local storage. Technically, we reduce the problem of estimating thread-local storage to the problem of escape analysis and provide an\u00a0\u2026", "num_citations": "18\n", "authors": ["259"]}
{"title": "Composing concurrency control\n", "abstract": " Concurrency control poses significant challenges when composing computations over multiple data-structures (objects) with different concurrency-control implementations. We formalize the usually desired requirements (serializability, abort-safety, deadlock-safety, and opacity) as well as stronger versions of these properties that enable composition. We show how to compose protocols satisfying these properties so that the resulting combined protocol also satisfies these properties. Our approach generalizes well-known protocols (such as two-phase-locking and two-phase-commit) and leads to new protocols. We apply this theory to show how we can safely compose optimistic and pessimistic concurrency control. For example, we show how we can execute a transaction that accesses two objects, one controlled by an STM and another by locking.", "num_citations": "17\n", "authors": ["259"]}
{"title": "JANUS: exploiting parallelism via hindsight\n", "abstract": " This paper addresses the problem of reducing unnecessary conflicts in optimistic synchronization. Optimistic synchronization must ensure that any two concurrently executing transactions that commit are properly synchronized. Conflict detection is an approximate check for this condition. For efficiency, the traditional approach to conflict detection conservatively checks that the memory locations mutually accessed by two concurrent transactions are accessed only for reading.", "num_citations": "17\n", "authors": ["259"]}
{"title": "Delhi College and Urdu\n", "abstract": " In the generation before the Indian revolt of 1857, the intellectual, literary, and religious life of Delhi was rich and vibrant. In contrast to the crepuscular portrait painted by Spear in Twilight of the Mughals, 1 this was a period marked by renaissance and revival. In literature, this was the age of Gh\u2265 lib, Mo\u2019min, \u0178auq, and Bah\u2265 adur Sh\u2265 h\u25ca afar. 2 In religion, Sh\u2265 h \u2018Abdu\u2019l-\u2018Az\u00b5z and his disciples and descendants sparked a number of religious reform movements. 3 The printing press came to the Mughal capital in this period, and newspapers and printed books began to appear, marking the emergence of a wider reading public. 4 At Delhi College, an educational institution founded in the late 1820s combining a pre-existing madrasa and a new college that taught English, the effort was made, if not to reconcile, at least to juxtapose oriental and western learning, both* Parts of this article have been presented at a workshop at Bonn University in December, 1996, and at the South Asian seminar at the University of Chicago in May, 1998. My thanks to Jamal Malik and CM Naim for their invitations, and to the participants for their questions and comments.", "num_citations": "17\n", "authors": ["259"]}
{"title": "Proving safety of speculative load instructions at compile-time\n", "abstract": " Speculative execution of instructions is one of the primary means for enhancing program performance of superscalar and VLIW machines. One of the pitfalls of such compile-time speculative scheduling of instructions is that it may cause run-time exceptions that did not exist in the original version of the program.             As opposed to run-time hardware or software interception of such exceptions, we suggest that the compiler will analyze and prove the safety of those instructions that are candidates for speculative execution, rejecting the ones that have even a slight chance of causing an exception.             Load (moving a memory operand to a register) instructions are important candidates for speculative execution, since they precondition any follow-on computation on load-store architectures. To enable speculative loads, an algorithmic scheme for proving the safety of such instructions is presented and\u00a0\u2026", "num_citations": "17\n", "authors": ["259"]}
{"title": "Resolving circularity in attribute grammars with applications to data flow analysis (preliminary version)\n", "abstract": " Circular attribute grammars appear in many data flow analysis problems. As one way of making the notion useful, an automatic translation of circular attribute grammars to equivalent non-circular attribute grammars is presented. It is shown that for circular attribute grammars that arise in many data flow analysis problems, the translation does not increase the asymptotic complexity of the semantic equations. Therefore, the translation may be used in conjunction with any evaluator generator to automate the development of efficient data flow analysis algorithms. As a result, the integration of such algorithms with other parts of a compiler becomes easier.", "num_citations": "17\n", "authors": ["259"]}
{"title": "Understanding the behavior of database operations under program control\n", "abstract": " Applications that combine general program logic with persistent databases (eg, three-tier applications) often suffer large performance penalties from poor use of the database. We introduce a program analysis technique that combines information flow in the program with commutativity analysis of its database operations to produce a unified dependency graph for database statements, which provides programmers with a high-level view of how costly database operations are and how they are connected in the program. As an example application of our analysis we describe three optimizations that can be discovered by examining the structure of the dependency graph; each helps remove communication latency from the critical path of a multi-tier system. We implement our technique in a tool for Java applications using JDBC and experimentally validate it using the multi-tier component of the Dacapo benchmark.", "num_citations": "15\n", "authors": ["259"]}
{"title": "Inferring inductive invariants from phase structures\n", "abstract": " Infinite-state systems such as distributed protocols are challenging to verify using interactive theorem provers or automatic verification tools. Of these techniques, deductive verification is highly expressive but requires the user to annotate the system with inductive invariants. To relieve the user from this labor-intensive and challenging task, invariant inference aims to find inductive invariants automatically. Unfortunately, when applied to infinite-state systems such as distributed protocols, existing inference techniques often diverge, which limits their applicability.                 This paper proposes user-guided invariant inference based on phase invariants, which capture the different logical phases of the protocol. Users conveys their intuition by specifying a phase structure, an automaton with edges labeled by program transitions; the tool automatically infers assertions that hold in the automaton\u2019s states, resulting\u00a0\u2026", "num_citations": "14\n", "authors": ["259"]}
{"title": "Effects of interpolation and data resolution on methane emission estimates from rice paddies\n", "abstract": " Rice paddies are an important source of the greenhouse gas methane (CH4). Global methane emission estimates are highly uncertain and do not account for effects of interpolation or data resolution errors. This paper determines such scaling effects for the influence of soil properties on calculated CH4 emissions for the island of Java, Indonesia. The effects of different interpolation techniques, variograms and neighbor optimization were tested for soil properties by cross-validation. Interpolated organic carbon values were not significantly different from the original soil samples, in contrast to interpolated soil iron contents. Interpolation of soil properties coupled to a process-based model on CH4emissions led to a significant change in distribution of calculated CH4 emissions, i.e., the variance decreased. Effects of data resolution were examined by interpolating soil properties to derive data at different data\u00a0\u2026", "num_citations": "13\n", "authors": ["259"]}
{"title": "Clay surface properties by water vapor sorption methods\n", "abstract": " Classification of expansive soils is a required component of geotechnical design to improve the long-term performance of structures. The current methods to classify swelling soils are based primarily on indices like the Atterberg limits, which are indirectly related to clay mineralogy and clay structure. The long-term goal of this research is to explore alternative ways to classify clays and to modernize geotechnical soil classification methods using measurement and analysis techniques based on clay surface properties. Clay surface properties such as specific surface area (SSA) and cation exchange capacity (CEC) are indicative of the clay mineralogy. For expansive clays, interactions of the clay surface with water molecules influence behavior. Consequently, water vapor sorption methods may potentially be used to determine clay surface properties. SSA is the measure of surface area per unit mass (m2/g). SSA measurement methods can be classified into three main categories: physical methods, positive adsorption methods, and negative adsorption methods. The most common techniques are positive sorption methods, where the sorption of molecules on the surface is measured. The sorbates can be either polar (e.g., N2) or non-polar (e.g., Ethylene Glycol Monoethyl Ether (EGME), water vapor). CEC is a measure of exchangeable mineral charge. Common measurement techniques include those based on replacing the cations in the natural exchange complex with a known cation species (e.g., ammonium displacement method) A water vapor sorption isotherm characterizes the relationship between relative humidity (RH) and the equilibrium\u00a0\u2026", "num_citations": "12\n", "authors": ["259"]}
{"title": "Abstraction Refinement for 3-Valued Logic Analysis\n", "abstract": " This paper concerns the question of how to create abstractions that are useful for program analysis. It presents a method that refines an abstraction automatically for analysis problems in which the semantics of statements and the query of interest are expressed using logical formulas. Refinement is carried out by introducing new instrumentation relations (defined via logical formulas over core relations, which capture the basic properties of memory configurations). A tool that incorporates the algorithm has been implemented and applied to several algorithms that manipulate linked lists and binary-search trees. In all but a few cases, Ihe tool is able to demonstrate (i) the partial correctness of the algorithms, and (ii) that the algorithms possess additional properties--e.g., stability or antistability.", "num_citations": "12\n", "authors": ["259"]}
{"title": "Data structure fusion\n", "abstract": " We consider the problem of specifying data structures with complex sharing in a manner that is both declarative and results in provably correct code. In our approach, abstract data types are specified using relational algebra and functional dependencies; a novel fuse operation on relational indexes specifies where the underlying physical data structure representation has sharing. We permit the user to specify different concrete shared representations for relations, and show that the semantics of the relational specification are preserved.", "num_citations": "11\n", "authors": ["259"]}
{"title": "TVLA: A framework for kleene logic based static analyses\n", "abstract": " We present TVLA (Three-Valued-Logic Analyzer). TVLA is a \u201cYACC\u201d-like framework for automatically constructing static-analysis algorithms from an operational semantics, where the operational semantics is specified using logical formulae. TVLA has been implemented in Java and was successfully used to perform shape analysis on programs manipulating linked data structures (singly and doubly linked lists), to prove safety properties of Mobile Ambients, and to verify the partial correctness of several sorting programs.\u2217 tla@ math. tau. ac. il", "num_citations": "11\n", "authors": ["259"]}
{"title": "Programming Languages and Systems\n", "abstract": " This volume contains the 29 papers presented at ESOP 2005, the 14th European Symposium on Programming, which took place in Edinburgh, UK, April 6\u20138, 2005. The ESOP series began in 1986 with the goal of bridging the gap between theory and practice, and the conferences continue to be devoted to explaining fundamental issues in the specification, analysis, and implementation of programming languages and systems. The volume begins with a summary of an invited contribution by Andrew Myers titled \u201cProgramming with Explicit Security Policies,\u201d and continues with the 28 papers selected by the Program Committee from 114 submissions. Each submission was reviewed by at least three referees, and papers were selected during a 10-day electronic discussion phase. I would like to sincerely thank the members of the Program Committee for their thorough reviews and dedicated involvement in the PC\u00a0\u2026", "num_citations": "10\n", "authors": ["259"]}
{"title": "Finding circular attributes in attribute grammars\n", "abstract": " The problem of finding the circular attributes in an grammar is considered. Two algorithms are proposed: the first is polynomial but yields conservative results while the second is exact but is potentially expontial. It is also shown that finding the circular attributes is harder than testing circularity.", "num_citations": "10\n", "authors": ["259"]}
{"title": "Taming callbacks for smart contract modularity\n", "abstract": " Callbacks are an effective programming discipline for implementing event-driven programming, especially in environments like Ethereum which forbid shared global state and concurrency. Callbacks allow a callee to delegate the execution back to the caller. Though effective, they can lead to subtle mistakes principally in open environments where callbacks can be added in a new code. Indeed, several high profile bugs in smart contracts exploit callbacks.  We present the first static technique ensuring modularity in the presence of callbacks and apply it to verify prominent smart contracts. Modularity ensures that external calls to other contracts cannot affect the behavior of the contract. Importantly, modularity is guaranteed without restricting programming.  In general, checking modularity is undecidable\u2014even for programs without loops. This paper describes an effective technique for soundly ensuring modularity\u00a0\u2026", "num_citations": "8\n", "authors": ["259"]}
{"title": "Bounded quantifier instantiation for checking inductive invariants\n", "abstract": " We consider the problem of checking whether a proposed invariant  expressed in first-order logic with quantifier alternation is inductive, i.e. preserved by a piece of code. While the problem is undecidable, modern SMT solvers can sometimes solve it automatically. However, they employ powerful quantifier instantiation methods that may diverge, especially when  is not preserved. A notable difficulty arises due to counterexamples of infinite size.   This paper studies Bounded-Horizon instantiation, a natural method for guaranteeing the termination of SMT solvers. The method bounds the depth of terms used in the quantifier instantiation process. We show that this method is surprisingly powerful for checking quantified invariants in uninterpreted domains. Furthermore, by producing partial models it can help the user diagnose the case when  is not inductive, especially when the underlying reason is the existence of infinite counterexamples.   Our main technical result is that Bounded-Horizon is at least as powerful as instrumentation, which is a manual method to guarantee convergence of the solver by modifying the program so that it admits a purely universal invariant. We show that with a bound of 1 we can simulate a natural class of instrumentations, without the need to modify the code and in a fully automatic way. We also report on a prototype implementation on top of Z3, which we used to verify several examples by Bounded-Horizon of bound 1.", "num_citations": "8\n", "authors": ["259"]}
{"title": "Synthesis of circular compositional program proofs via abduction\n", "abstract": " This paper presents a new technique for synthesizing circular compositional proofs of program correctness. Our technique uses abductive inference to decompose the proof into small lemmas (i.e., compositionality) and proves that each lemma is not the first one to fail (i.e., circularity). Our approach represents lemmas as small program fragments annotated with pre and post-conditions and uses different tools to discharge each different lemma. This approach allows us to combine the strengths of different verifiers and addresses scalability concerns, as each lemma concerns the correctness of small syntactic fragments of the program. We have implemented our proposed technique for generating circular compositional proofs and used four different program analysis tools to discharge the proof subgoals. We evaluate our approach on a collection of synthetic and real-world benchmarks and show that our\u00a0\u2026", "num_citations": "8\n", "authors": ["259"]}
{"title": "Self-stabilization preserving compiler\n", "abstract": " Self-Stabilization is an elegant approach for designing fault tolerant systems. A system is considered self-stabilizing if, starting in any state, it converges to the desired behavior. Self-stabilizing algorithms were designed for solving fundamental distributed tasks, such as leader election, token circulation and communication network protocols. The algorithms were expressed using guarded commands or pseudo-code. The realization of these algorithms requires the existence of (self-stabilizing) infrastructure for their execution such as a self-stabilizing microprocessor and a self-stabilizing operating system. Moreover, the high-level description of the algorithms needs to be converted into machine language of the microprocessor. In this work, we present a design for a self-stabilization preserving compiler designed for programs written in a language similar to the abstract state machine (ASM). The compiler\u00a0\u2026", "num_citations": "8\n", "authors": ["259"]}
{"title": "Constrained image generation using binarized neural networks with decision procedures\n", "abstract": " We consider the problem of binary image generation with given properties. This problem arises in a number of practical applications, including generation of artificial porous medium for an electrode of lithium-ion batteries, for composed materials, etc. A generated image represents a porous medium and, as such, it is subject to two sets of constraints: topological constraints on the structure and process constraints on the physical process over this structure. To perform image generation we need to define a mapping from a porous medium to its physical process parameters. For a given geometry of a porous medium, this mapping can be done by solving a partial differential equation (PDE). However, embedding a PDE solver into the search procedure is computationally expensive. We use a binarized neural network to approximate a PDE solver. This allows us to encode the entire problem as a logical formula\u00a0\u2026", "num_citations": "7\n", "authors": ["259"]}
{"title": "Turning nondeterminism into parallelism\n", "abstract": " Nondeterminism is a useful and prevalent concept in the design and implementation of software systems. An important property of nondeterminism is its latent parallelism: A nondeterministic action can evaluate to multiple behaviors. If at least one of these behaviors does not conflict with concurrent tasks, then there is an admissible execution of the action in parallel with these tasks. Unfortunately, existing implementations of the atomic paradigm - optimistic as well as pessimistic - are unable to fully exhaust the parallelism potential of nondeterministic actions, lacking the means to guide concurrent tasks toward nondeterministic choices that minimize interference. This paper investigates the problem of utilizing parallelism due to nondeterminism. We observe that nondeterminism occurs in many real-world codes. We motivate the need for devising coordination mechanisms that can utilize available nondeterminism. We\u00a0\u2026", "num_citations": "7\n", "authors": ["259"]}
{"title": "Reasoning about lock placements\n", "abstract": " A lock placement describes, for each heap location, which lock guards the location, and under what circumstances. We formalize methods for reasoning about lock placements, making precise the interactions between the program, the heap structure, and the lock placement.", "num_citations": "7\n", "authors": ["259"]}
{"title": "Abstract counterexample-based refinement for powerset domains\n", "abstract": " Counterexample-guided abstraction refinement (CEGAR) is a powerful technique to scale automatic program analysis techniques to large programs. However, so far it has been used primarily for model checking in the context of predicate abstraction. We formalize CEGAR for general powerset domains. If a spurious abstract counterexample needs to be removed through abstraction refinement, there are often several choices, such as which program location(s) to refine, which abstract domain(s) to use at different locations, and which abstract values to compute. We define several plausible preference orderings on abstraction refinements, such as refining as \u201clate\u201d as possible and as \u201ccoarse\u201d as possible. We present generic algorithms for finding refinements that are optimal with respect to the different preference orderings. We also compare the different orderings with respect to desirable properties, including\u00a0\u2026", "num_citations": "7\n", "authors": ["259"]}
{"title": "Complexity and information in invariant inference\n", "abstract": " This paper addresses the complexity of SAT-based invariant inference, a prominent approach to safety verification. We consider the problem of inferring an inductive invariant of polynomial length given a transition system and a safety property. We analyze the complexity of this problem in a black-box model, called the Hoare-query model, which is general enough to capture algorithms such as IC3/PDR and its variants. An algorithm in this model learns about the system's reachable states by querying the validity of Hoare triples.   We show that in general an algorithm in the Hoare-query model requires an exponential number of queries. Our lower bound is information-theoretic and applies even to computationally unrestricted algorithms, showing that no choice of generalization from the partial information obtained in a polynomial number of Hoare queries can lead to an efficient invariant inference procedure in this\u00a0\u2026", "num_citations": "6\n", "authors": ["259"]}
{"title": "Abstract interpretation of stateful networks\n", "abstract": " Modern networks achieve robustness and scalability by maintaining states on their nodes. These nodes are referred to as middleboxes and are essential for network functionality. However, the presence of middleboxes drastically complicates the task of network verification. Previous work showed that the problem is undecidable in general and EXPSPACE-complete when abstracting away the order of packet arrival.               We describe a new algorithm for conservatively checking isolation properties of stateful networks. The asymptotic complexity of the algorithm is polynomial in the size of the network, albeit being exponential in the maximal number of queries of the local state that a middlebox can do, which is often small.               Our algorithm is sound, i.e., it can never miss a violation of safety but may fail to verify some properties. The algorithm performs on-the fly abstract interpretation by (1)\u00a0abstracting\u00a0\u2026", "num_citations": "6\n", "authors": ["259"]}
{"title": "Deriving specialized heap analyses for verifying component-client conformance\n", "abstract": " We are concerned with the problem of verifying (\\certifying\") whether the client of a software component conforms to the component's constraints for correct usage. We focus here on the Concurrent Modi\u017bcation Problem (CMP), a component-client conformance problem that can occur when certain classes de\u017bned by the Java Collections Framework are misused. Since CMP errors arise from relationships among a potentially unbounded number of objects, the problem cannot be de\u017bned as a simple \u017bnite-state property. Instead, we show that it is natural to specify it as a particular mustalias property for an abstract heap manipulated by the component. Computing this must-alias property would seem to be di\u0105cult, since the problem is contained in a class known to be intractable. However, we show that by exploiting the fact that the component's abstract heap is manipulated only through a well-de\u017bned interface, we can systematically derive e\u0105cient and accurate certi\u017bcation algorithms for CMP. For a restricted, but still quite useful class of client programs, we can perform precise interprocedural certi\u017bcation in polynomial time. We then generalize the approach to arbitrary client programs. Although this generalization yields some loss of precision in principle, the results produced by our prototype implementation of the certi\u017bcation algorithm in practice are quite accurate for the suite of benchmark programs studied. We believe the techniques presented here are likely to be applicable to other conformance problems as well.", "num_citations": "6\n", "authors": ["259"]}
{"title": "Ivy: interactive verification of parameterized systems via effectively propositional reasoning\n", "abstract": " \u2022 Sorts: Node, ID\u2022 Constants:\u2022 ring_head: Node\u2022 ring_tail: Node\u2022 zero: ID\u2022 leader_id: ID\u2022 Relations:\u2022 le (ID, ID)\u2013total order on node id\u2019s\u2022 reach (Node, Node)\u2013total order, describes ring topology\u2022 id (Node, ID)\u2013relate a node to its id\u2022 pending (ID, Node)\u2013models pending messages\u2022 leader (Node)\u2013leader (n) means n is the leader", "num_citations": "5\n", "authors": ["259"]}
{"title": "Modularity in lattices: A case study on the correspondence between top-down and bottom-up analysis\n", "abstract": " Interprocedural analyses are compositional when they compute over-approximations of procedures in a bottom-up fashion. These analyses are usually more scalable than top-down analyses, which compute a different procedure summary for every calling context. However, compositional analyses are rare in practice as it is difficult to develop them with enough precision.               We establish a connection between compositional analyses and modular lattices, which require certain associativity between the lattice join and meet operations, and use it to develop a compositional version of the connection analysis by Ghiya and Hendren. Our version is slightly more conservative than the original top-down analysis in order to meet our modularity requirement. When applied to real-world Java programs our analysis scaled much better than the original top-down version: The top-down analysis times out in the\u00a0\u2026", "num_citations": "5\n", "authors": ["259"]}
{"title": "Checking linearizability of encapsulated extended operations\n", "abstract": " Linearizable objects (data-structures) provide operations that appear to execute atomically. Modern mainstream languages provide many linearizable data-structures, simplifying concurrent programming. In practice, however, programmers often find a need to execute a sequence of operations (on linearizable objects) that executes atomically and write extended operations for this purpose. Such extended operations are a common source of atomicity bugs.               This paper focuses on the problem of verifying that a set of extension operations (to a linearizable library) are themselves linearizable. We present several reduction theorems that simplify this verification problem enabling more efficient verification.               We first introduce the notion of an encapsulated extension: this is an extension that (a) does not introduce new shared state (beyond the shared state in the base linearizable library), and (b\u00a0\u2026", "num_citations": "5\n", "authors": ["259"]}
{"title": "Sustainable retrofits in existing buildings analysis of stakeholder types, requirements, and perceptions in decision making, using the house of quality\n", "abstract": " There are multiple reasons why existing building stakeholders are concerned with increasing the sustainability of their buildings from social, environmental, economic, and technical perspectives. In most cases, these building stakeholders have varying and opposing perspectives on how, when, and why a building should be sustainably retrofitted. Several studies indicate that conflicting stakeholder requirements are a main barrier in implementing sustainable retrofits and that the decision is most often made based purely on short-term economic grounds. However, most studies did not take into account the important role that different building stakeholders play in determining the type and extent of any retrofit measures, or develop methodologies to fully enhance the interaction amongst these stakeholders which currently do not exist. This research presents a unique investigation into the challenges and barriers that are involved in meeting the various sustainable retrofit requirements of many different types of stakeholders. In this research, the effect of the stakeholder type and the hierarchy of their respective requirements in the decision to sustainably retrofit an existing U.S. Navy case study building were explored. A House of Quality (HOQ) model was developed through this research that synthesized differences amongst the many stakeholders, integrated their competing objectives, and a new way of analyzing HOQ data was introduced. This research demonstrated that a top-down owner commitment to sustainability was able to align the many competing stakeholder requirements and eliminate barriers that could potentially get in the way of efficient\u00a0\u2026", "num_citations": "5\n", "authors": ["259"]}
{"title": "WIP: Finding bugs automatically in smart contracts with parameterized invariants\n", "abstract": " This WIP paper describes our experience using formal verification to find bugs in smart contracts. Perhaps surprisingly, the most difficult part of the formal verification process is not the verification itself, but specification: that is, expressing the desired properties of the program. In the domain of smart contracts, we have found that the same invariants apply across many different software versions and platforms. This creates a potential network effect where the cost of formal verification drops as the repertoire of reusable invariants grows. We aim to jumpstart this process by:(i) sharing several invariants we have already identified and found to be useful and (ii) suggesting a specification framework that formalizes these invariants as parameterized Hoare triples. The second point addresses the challenge that differences arise between platforms in the exact form of the invariants. By adopting a common specification language, the community will be able to better communicate knowledge about important invariants between projects, and to collaborate in building tools that support reasoning about such invariants. Finally, we present some preliminary results from applying a tool for automatically checking invariants to some sample contracts.", "num_citations": "4\n", "authors": ["259"]}
{"title": "An introduction to data representation synthesis\n", "abstract": " We consider the problem of specifying combinations of data structures with complex sharing in a manner that is declarative and results in provably correct code. In our approach, abstract data types are specified using relational algebra and functional dependencies. We describe a language of decompositions that permits the user to specify different concrete representations for relations, and show that operations on concrete representations soundly implement their relational specification. We also describe an auto-tuner that automatically identifies the best decomposition for a particular workload. It is easy to incorporate data representations synthesized by our compiler into existing systems, leading to code that is simpler, correct by construction, and comparable in performance to the code it replaces.", "num_citations": "4\n", "authors": ["259"]}
{"title": "Intersecting heap abstractions with applications to compile-time memory management\n", "abstract": " We consider the problem of computing the intersection (meet) of heap abstractions, namely the common value of a set of abstract memory stores. This problem proves to have many applications in shape analysis, such as interpreting program conditions, refining abstract configurations, reasoning about procedures, and proving temporal properties of heap-manipulating programs, either via greatest fixed point approximation over the trace semantics, or in a staged manner over the collecting semantics. However, computing the meet of heap abstractions is nontrivial; its definition as the least upper bound of all lower bounds does not lead to an effective algorithm. We describe a constructive formulation of meet that is based on finding certain relations between abstract heap objects. The enumeration of those relations is reduced to finding constrained matchings over bipartite graphs. A simple heuristic is applied in order to reduce computational overhead, and is supposed to behave well for common real-life scenarios. We describe a prototype implementation of the algorithm for proving temporal heap properties via staged analysis. It is applied to obtain information for compile-time garbage collection on several small but interesting Java programs.", "num_citations": "4\n", "authors": ["259"]}
{"title": "Sustainable tourism development: a compilation and analysis of expert views\n", "abstract": " This study presents an in-depth discussion of the topic utilizing a combination of primary (interviews with a panel of experts) and secondary (literature review) sources of information. While the latter was used as a tool to identi@ and highlight contested issues, the telephone interviews with experts were used as the medium to discuss and extract opinion/recommendation/judgment on the highlighted issues. Besides bringing out some insightful points of view and recommendations, this study found universal acceptance over certain issues. It was accepted that sustainable tourism development must be seen as a means of achieving sustainable development, which could mean no tourism at all. All experts were of the opinion that some fundamental political and ideological changes need to be made in order for tourism to effectively contribute towards sustainability and sustainable development.", "num_citations": "4\n", "authors": ["259"]}
{"title": "Online subpath profiling\n", "abstract": " We present an efficient online subpath profiling algorithm, OSP, that reports hot subpaths executed by a program in a given run. The hot subpaths can start at arbitrary basic block boundaries, and their identification is important for code optimization; e.g., to locate program traces in which optimizations could be most fruitful, and to help programmers in identifying performance bottlenecks.               The OSP algorit hm is online in the sense that it reports at any point during execution the hot subpaths as observed so far. It has very low memory and runtime overheads, and exhibits high accuracy in reports for benchmarks such as JLex and FFT. These features make the OSP algorithm potentially attractive for use in just-in-time (JIT) optimizing compilers, in which profiling performance is crucial and it is useful to locate hot subpaths as early as possible.               The OSP algorithm is based on an adaptive sampling\u00a0\u2026", "num_citations": "4\n", "authors": ["259"]}
{"title": "Shape analysis\n", "abstract": " Program Analysis via Graph Reachability Page 1 Shape Analysis Mooly Sagiv Page 2 \u2022 Tel-Aviv University \u2013 D. Amit \u2013 I. Bogudlov \u2013 G. Arnold \u2013 G. Erez \u2013 N. Dor \u2013 T. Lev-Ami \u2013 R. Manevich \u2013 R. Shaham \u2013 A. Rabinovich \u2013 N. Rinetzky \u2013 G. Yorsh \u2013 A. Warshavsky \u2022 Universit\u00e4t des Saarlandes \u2013 J. Bauer \u2013 R. Biber \u2013 R. Wilhelm . . . and also \u2022 University of Wisconsin \u2013 F. DiMaio \u2013 D. Gopan \u2013 A. Loginov \u2013 T. Reps \u2022 IBM Research \u2013 J. Field \u2013 H. Kolodner \u2013 M. Rodeh \u2013 E. Yahav \u2022 Microsoft Research \u2013 J. Berdine \u2013 B. Cook \u2013 G. Ramalingam \u2022 University of Massachusetts \u2013 N. Immerman \u2013 B. Hesse \u2022 Inria \u2013 B. Jeannet Page 3 Shape Analysis [Jones and Muchnick 1981] \u2022 Determine the possible shapes of a dynamically allocated data structure at a given program point Page 4 Programs and Properties \u2022 Dynamically allocated memory \u2022 Recursive data structures \u2022 Recursive procedures \u2022 Concurrency \u2022 Memory safety \u2022 of Data \u2022 \u2022 \u2026", "num_citations": "4\n", "authors": ["259"]}
{"title": "Synthesizing cluster management code for distributed systems\n", "abstract": " Management planes for data-center systems are complicated to develop, test, maintain, and evolve. They routinely grapple with hard combinatorial optimization problems like load balancing, placement, scheduling, rolling upgrades and configuration management. To tackle these problems, developers are left with two bad choices:(i) develop ad-hoc mechanisms for systems to solve these optimization problems, or (ii) use specialized solvers that require steep engineering effort.", "num_citations": "3\n", "authors": ["259"]}
{"title": "Property directed reachability for proving absence of concurrent modification errors\n", "abstract": " We define and implement an interprocedural analysis for automatically checking safety of recursive programs with an unbounded state space. The main idea is to infer modular universally quantified inductive invariants in the form of procedure summaries that are sufficient to prove the safety property. We assume that the effect of the atomic commands of the program can be modeled via effectively propositional logic. We then propose a variant of the IC3/PDR approach for computing universally quantified inductive procedure summaries that overapproximate the behavior of the program.               We show that Java programs that manipulate collections and iterators can be modeled in effectively propositional logic and that the invariants are often universal. This allows us to apply the new analysis to prove the absence of concurrent modification exceptions in Java programs. In order to check the feasibility of our\u00a0\u2026", "num_citations": "3\n", "authors": ["259"]}
{"title": "Lower extremity performance with proprioceptive neuromuscular facilitation\n", "abstract": " De Jarlais LR, Gonstead JR, Lynch TM, Masbruch LL, Schuldt RC, Zorn DA. LOWEREXTREMITY PERFORMANCE WITH PROPRIOCEPTIVE NEUROMUSCULAR FACILITATION. Journal of Undergraduate Kinesiology Research 2007; 2 (2): 32-39. This study observed the effectiveness of proprioceptive neuromuscular facilitation stretching or no stretching prior to performance. Purpose: To determine if PNF stretching has a positive impact on performance of running economy and one repetition maximum leg press test. Methods: The subjects consisted of 16 active college aged students, 8 males and 8 females (mean\u00b1SD, age 20\u00b11.2; height 175.5\u00b18.2 cm; weight 70.9\u00b111.5 kg; body fat% 15.7\u00b18%). The subjects performed a five minute warm up on a treadmill prior to PNF stretching or their specific test. Each subject performed their specific test twice, either with or without stretching. Each test was held at least 48 hours apart in the presence of at least two test technicians. Each test was performed using the same equipment. Results: Subjects in the running economy test showed no significant difference in terms of the 4-min.(M= 23.7 mL/kg/min stretching, M= 23mL/kg/min no stretching) and 8-min.(M= 35.3 mL/kg/min stretching, M= 36.4 mL/kg/min no stretching) intervals. For the subjects in the 1-RM test, there was no significant difference between the stretching and non-stretching trials. Conclusion: Our study\u2019s results would imply that PNF stretching is more beneficial for long duration running economy yet provides no significant benefit for lower extremity maximal strength.", "num_citations": "3\n", "authors": ["259"]}
{"title": "Automating Cluster Management with Weave\n", "abstract": " Modern cluster management systems like Kubernetes and Openstack grapple with hard combinatorial optimization problems: load balancing, placement, scheduling, and configuration. Currently, developers tackle these problems by designing custom application-specific algorithms---an approach that is proving unsustainable, as ad-hoc solutions both perform poorly and introduce overwhelming complexity to the system, making it challenging to add important new features. We propose a radically different architecture, where programmers drive cluster management tasks declaratively, using SQL queries over cluster state stored in a relational database. These queries capture in a natural way both constraints on the cluster configuration as well as optimization objectives. When a cluster reconfiguration is required at runtime, our tool, called Weave, synthesizes an encoding of these queries into an optimization model, which it solves using an off-the-shelf solver. We demonstrate Weave's efficacy by powering three production-grade systems with it: a Kubernetes scheduler, a virtual machine management solution, and a distributed transactional datastore. Using Weave, we expressed complex cluster management policies in under 20 lines of SQL, easily added new features to these existing systems, and significantly improved placement quality and convergence times.", "num_citations": "2\n", "authors": ["259"]}
{"title": "Modular lattices for compositional interprocedural analysis\n", "abstract": " Interprocedural analyses are compositional when they compute over-approximations of procedures in a bottom-up fashion. These analyses are usually more scalable than top-down analyses which compute a different procedure summary for every calling context. However, compositional analyses are rare in practice, because it is difficult to develop such analyses with enough precision. In this paper, we establish a connection between a restricted class of compositional analyses and so called modular lattices, which require certain associativity between the lattice join and meet operations. Our connection provides sufficient conditions for building a compositional analysis that is as precise as a top-down analysis. We developed a compositional version of the connection pointer analysis by Ghiya and Hendren which is slightly more conservative than the original top-down analysis in order to meet our modularity requirement. We implemented and applied our compositional connection analysis to real-world Java programs. As expected, the compositional analysis scales much better than the original topdown version. The top-down analysis times out in the largest two of our five programs, and the loss of precision due to the modularity requirement in the remaining programs ranges only between 2-5%.", "num_citations": "2\n", "authors": ["259"]}
{"title": "POPL'11 program chair's report\n", "abstract": " This document summarizes my experience serving as a POPL\u201911 program chair. It is meant to assist future program chairs. See [1] for POPL\u201911 general chair report. I describe the process and estimate the time devoted for each phase, followed a brief conclusion.", "num_citations": "2\n", "authors": ["259"]}
{"title": "Static thread-escape analysis vis dynamic heap abstractions\n", "abstract": " Static thread-escape analysis requires abstracting the program heap in a manner that separates thread-local objects from thread-shared ones. We present a static thread-escape analysis that employs a novel lightweight heap abstraction. It abstracts the heap using only two partitions: one for definitely thread-local objects and the other for possibly thread-shared ones. Queries to the analysis from different parts of the program require potentially different partitionings for precision and scalability. A key aspect of our approach is the use of a dynamic analysis that observes concrete heaps from program runs to guess the partitioning to use for proving each query. We show the effectiveness of our approach on queries from a static race detector on a suite of multi-threaded Java benchmarks.", "num_citations": "2\n", "authors": ["259"]}
{"title": "Efficiently inferring thread correlations\n", "abstract": " This thesis presents a new analysis for proving properties of fine-grained concurrent programs with a shared, mutable, heap in the presence of an unbounded number of objects and threads. Properties addressed include memory safety, data structure invariants, partial correctness, and linearizability. The new techniques presented enable successful verification of programs that were not handled by previous concurrent shape analysis algorithms. The techniques are presented in an abstract framework we call thread-correlation analysis. Thread-correlation analysis infers invariants that capture the correlations between the local states of different threads and the global state (content of the heap). Inferring such invariants is non-trivial, even for systems with a bounded number of threads. The main reason is the need to reason about a quadratic number of interactions between threads when analyzing a single (atomic) statement, which is a significant performance overhead in practice. This work provides two novel techniques for reducing the cost of applying the abstract transformers.", "num_citations": "2\n", "authors": ["259"]}
{"title": "Efficient memory management for long-lived objects\n", "abstract": " Generational garbage collectors perform well for short-lived objects, but do not deal well with long-lived objects. Existing techniques for long-lived objects, such as pretenuring, eliminate work in the nursery; however, the collector still needs to deal with the long-lived objects in the older generations. We introduce a novel scheme employing regions that avoids both nursery and old generation costs.Our scheme divides the heap into two sub-heaps: a garbage collected heap and a region heap. Most objects are allocated in the garbage collected heap, while long-lived objects are allocated in regions. The scheme maintains reference counts to the regions, and reclaims regions when their count drops to zero. The memory management mechanism is decoupled from the region selection policy. Region selection policies may range from manual to fully automatic. This paper presents a particular realization for our memory\u00a0\u2026", "num_citations": "2\n", "authors": ["259"]}
{"title": "Learning the boundary of inductive invariants\n", "abstract": " We study the complexity of invariant inference and its connections to exact concept learning. We define a condition on invariants and their geometry, called the fence condition, which permits applying theoretical results from exact concept learning to answer open problems in invariant inference theory. The condition requires the invariant's boundary---the states whose Hamming distance from the invariant is one---to be backwards reachable from the bad states in a small number of steps. Using this condition, we obtain the first polynomial complexity result for an interpolation-based invariant inference algorithm, efficiently inferring monotone DNF invariants with access to a SAT solver as an oracle. We further harness Bshouty's seminal result in concept learning to efficiently infer invariants of a larger syntactic class of invariants beyond monotone DNF. Lastly, we consider the robustness of inference under program\u00a0\u2026", "num_citations": "1\n", "authors": ["259"]}
{"title": "Modular Safety Verification for Stateful Networks\n", "abstract": " Modern networks achieve robustness and scalability by maintaining states on their nodes. These nodes are referred to as middleboxes and are essential for network functionality. However, the presence of middleboxes drastically complicates the task of network verification. We describe a new algorithm for conservatively checking the safety of stateful networks. Our algorithm is modular in the sense that it repeatedly analyzes each middlebox separately wrt an intermediate global state view. Its asymptotic complexity is polynomial in the size of the network, albeit being exponential in the maximal number of queries of the local state that a middlebox can do, which is often small. Our algorithm is sound, ie, it can never miss a violation of safety but may fail to verify some properties. The algorithm performs on-the fly abstract interpretation by (1) abstracting away the order of packet processing,(2) abstracting away correlations between states of different middleboxes and channel contents, and (3) representing middlebox states by their effect on each packet separately, rather than taking into account the entire state space. We show that the abstractions do not lose precision when the middlebox may reset in any state. This is encouraging since many real middleboxes reset, eg, after some session timeout is reached or due to hardware failure.", "num_citations": "1\n", "authors": ["259"]}
{"title": "Lexical Analysis\n", "abstract": " We start this chapter by describing the task of lexical analysis. Then we present regular expressions as specifications for this task. Regular expressions can be automatically converted into nondeterministic finite automata, which implement lexical analysis. Nondeterministic finite automata can be made deterministic, which is preferred for implementing lexical analyzers, often called scanners. Another transformation on the resulting deterministic finite automata attempts to reduce the sizes of the automata. These three steps together make up an automatic process generating lexical analyzers from specifications. Another module working in close cooperation with such a finite automaton is the screener. It filters out keywords, comments, etc., and may do some bookkeeping or conversion.", "num_citations": "1\n", "authors": ["259"]}
{"title": "TVLA: User\u2019s manual (working draft)\n", "abstract": " This document is intended as a user\u2019s manual for the TVLA system. The reader should be familiar with the Three-Valued Logic based Analysis framework described in [?] before consulting this manual. The manual is accompanied by an example of the analysis of the reverse function in Figure 1. The original algorithms in the system were designed and implemented by Tal Lev-Ami [?,?].", "num_citations": "1\n", "authors": ["259"]}
{"title": "Graph transformation for concurrency and verification-preface\n", "abstract": " Graph Transformation for Concurrency and Verification - Preface \u2014 University of Twente Research Information Skip to main navigation Skip to search Skip to main content University of Twente Research Information Logo Home Profiles Research Units Projects Research output Datasets Activities Prizes Press / Media Search by expertise, name or affiliation Graph Transformation for Concurrency and Verification - Preface Arend Rensink, Reiko Heckel, Barbara K\u00f6nig Research output: Chapter in Book/Report/Conference proceeding \u203a Conference contribution \u203a Academic Overview Original language Undefined Title of host publication Proceedings of the Workshop on Graph Transformation for Concurrency and Verification (GT-VC) Place of Publication Amsterdam Publisher Elsevier Pages 1-2 Number of pages 2 DOIs https://doi.org/10.1016/j.entcs.. Publication status Published - Jul 2007 Event Workshop on Graph for and \u2026", "num_citations": "1\n", "authors": ["259"]}
{"title": "Lightweight analysis of acyclic unshared lists\n", "abstract": " We describe a simple analysis for tracking properties such as may-aliasing, memory leaks, and disjointness for programs manipulating singly-linked lists. We restrict the set of programs we consider to ones that manipulate acyclic and unshared lists. We benefit from these restrictions in terms of simplicity and efficiency of the algorithm. We demonstrate that most common list-manipulating programs satisfy the above restrictions or can be locally transformed to meet the requirements. Our algorithm successfully answers may-aliasing, memory leak and disjointness queries for these programs. The analysis also allows us to prove interesting summary content properties that relate the contents of a set of input lists to a procedure with the content of lists returned from the procedure.", "num_citations": "1\n", "authors": ["259"]}