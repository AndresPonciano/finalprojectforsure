{"title": "Full functional verification of linked data structures\n", "abstract": " We present the first verification of full functional correctness for a range of linked data structure implementations, including mutable lists, trees, graphs, and hash tables. Specifically, we present the use of the Jahob verification system to verify formal specifications, written in classical higher-order logic, that completely capture the desired behavior of the Java data structure implementations (with the exception of properties involving execution time and/or memory consumption). Given that the desired correctness properties include intractable constructs such as quantifiers, transitive closure, and lambda abstraction, it is a challenge to successfully prove the generated verification conditions. Our Jahob verification system uses integrated reasoning to split each verification condition into a conjunction of simpler subformulas, then apply a diverse collection of specialized decision procedures, first-order theorem provers, and\u00a0\u2026", "num_citations": "240\n", "authors": ["1700"]}
{"title": "Role analysis\n", "abstract": " We present a new role system in which the type (or role) of each object depends on its referencing relationships with other objects, with the role changing as these relationships change. Roles capture important object and data structure properties and provide useful information about how the actions of the program interact with these properties. Our role system enables the programmer to specify the legal aliasing relationships that define the set of roles that objects may play, the roles of procedure parameters and object fields, and the role changes that procedures perform while manipulating objects. We present an interprocedural, compositional, and context-sensitive role analysis algorithm that verifies that a program maintains role constraints.", "num_citations": "159\n", "authors": ["1700"]}
{"title": "Sound compilation of reals\n", "abstract": " Writing accurate numerical software is hard because of many sources of unavoidable uncertainties, including finite numerical precision of implementations. We present a programming model where the user writes a program in a real-valued implementation and specification language that explicitly includes different types of uncertainties. We then present a compilation algorithm that generates a finite-precision implementation that is guaranteed to meet the desired precision with respect to real numbers. Our compilation performs a number of verification steps for different candidate precisions. It generates verification conditions that treat all sources of uncertainties in a unified way and encode reasoning about finite-precision roundoff errors into reasoning about real numbers. Such verification conditions can be used as a standardized format for verifying the precision and the correctness of numerical programs. Due to\u00a0\u2026", "num_citations": "146\n", "authors": ["1700"]}
{"title": "CrystalBall: Predicting and preventing inconsistencies in deployed distributed systems\n", "abstract": " We propose a new approach for developing and deploying distributed systems, in which nodes predict distributed consequences of their actions, and use this information to detect and avoid errors. Each node continuously runs a state exploration algorithm on a recent consistent snapshot of its neighborhood and predicts possible future violations of specified safety properties. We describe a new state exploration algorithm, consequence prediction, which explores causally related chains of events that lead to property violation. This paper describes the design and implementation of this approach, termed CrystalBall. We evaluate CrystalBall on RandTree, BulletPrime, Paxos, and Chord distributed system implementations. We identified new bugs in mature Mace implementations of three systems. Furthermore, we show that if the bug is not corrected during system development, CrystalBall is effective in steering the execution away from inconsistent states at runtime.", "num_citations": "144\n", "authors": ["1700"]}
{"title": "Disjunctive interpolants for Horn-clause verification\n", "abstract": " One of the main challenges in software verification is efficient and precise compositional analysis of programs with procedures and loops. Interpolation methods remains one of the most promising techniques for such verification, and are closely related to solving Horn clause constraints. We introduce a new notion of interpolation, disjunctive interpolation, which solves a more general class of problems in one step compared to previous notions of interpolants, such as tree interpolants or inductive sequences of interpolants. We present algorithms and complexity for construction of disjunctive interpolants, as well as their use within an abstraction-refinement loop. We have implemented Horn clause verification algorithms that use disjunctive interpolants and evaluate them on benchmarks expressed as Horn clauses over the theory of integer linear arithmetic.", "num_citations": "107\n", "authors": ["1700"]}
{"title": "Software verification and graph similarity for automated evaluation of students\u2019 assignments\n", "abstract": " ContextThe number of students enrolled in universities at standard and on-line programming courses is rapidly increasing. This calls for automated evaluation of students assignments.ObjectiveWe aim to develop methods and tools for objective and reliable automated grading that can also provide substantial and comprehensible feedback. Our approach targets introductory programming courses, which have a number of specific features and goals. The benefits are twofold: reducing the workload for teachers, and providing helpful feedback to students in the process of learning.MethodFor sophisticated automated evaluation of students\u2019 programs, our grading framework combines results of three approaches (i) testing, (ii) software verification, and (iii) control flow graph similarity measurement. We present our tools for software verification and control flow graph similarity measurement, which are publicly available\u00a0\u2026", "num_citations": "103\n", "authors": ["1700"]}
{"title": "A verification toolkit for numerical transition systems\n", "abstract": " This paper presents a publicly available toolkit and a benchmark suite for rigorous verification of Integer Numerical Transition Systems (INTS), which can be viewed as control-flow graphs whose edges are annotated by Presburger arithmetic formulas. We present Flata and Eldarica, two verification tools for INTS. The Flata system is based on precise acceleration of the transition relation, while the Eldarica system is based on predicate abstraction with interpolation-based counterexample-driven refinement. The Eldarica verifier uses the Princess theorem prover as a sound and complete interpolating prover for Presburger arithmetic. Both systems can solve several examples for which previous approaches failed, and present a useful baseline for verifying integer programs. The infrastructure is a starting point for rigorous benchmarking, competitions, and standardized communication between tools.", "num_citations": "87\n", "authors": ["1700"]}
{"title": "Modular data structure verification\n", "abstract": " This dissertation describes an approach for automatically verifying data structures, focusing on techniques for automatically proving formulas that arise in such verification. I have implemented this approach with my colleagues in a verification system called Jahob. Jahob verifies properties of Java programs with dynamically allocated data structures. Developers write Jahob specifications in classical higher-order logic (HOL); Jahob reduces the verification problem to deciding the validity of HOL formulas. I present a new method for proving HOL formulas by combining automated reasoning techniques. My method consists of 1) splitting formulas into individual HOL conjuncts, 2) soundly approximating each HOL conjunct with a formula in a more tractable fragment and 3) proving the resulting approximation using a decision procedure or a theorem prover. I present three concrete logics; for each logic I show how to use it to approximate HOL formulas, and how to decide the validity of formulas in this logic. First, I present an approximation of HOL based on a translation to first-order logic, which enables the use of existing resolution-based theorem provers. Second, I present an approximation of HOL based on field constraint analysis, a new technique that enables decision procedures for special classes of graphs (such as monadic second-order logic over trees) to be applied to arbitrary graphs.", "num_citations": "86\n", "authors": ["1700"]}
{"title": "Induction for SMT solvers\n", "abstract": " Satisfiability modulo theory solvers are increasingly being used to solve quantified formulas over structures such as integers and term algebras. Quantifier instantiation combined with ground decision procedure alone is insufficient to prove many formulas of interest in such cases. We present a set of techniques that introduce inductive reasoning into SMT solving algorithms that is sound with respect to the interpretation of structures in SMT-LIB standard. The techniques include inductive strengthening of conjecture to be proven, as well as facility to automatically discover subgoals during an inductive proof, where subgoals themselves can be proven using induction. The techniques have been implemented in CVC4. Our experiments show that the developed techniques have good performance and coverage of a range of inductive reasoning problems. Our experiments also show the impact of different\u00a0\u2026", "num_citations": "84\n", "authors": ["1700"]}
{"title": "Verifying a file system implementation\n", "abstract": " We present a correctness proof for a basic file system implementation. This implementation contains key elements of standard Unix file systems such as inodes and fixed-size disk blocks. We prove the implementation correct by establishing a simulation relation between the specification of the file system (which models the file system as an abstract map from file names to sequences of bytes) and its implementation (which uses fixed-size disk blocks to store the contents of the files). We used the Athena proof system to represent and validate our proof. Our experience indicates that Athena\u2019s use of block-structured natural deduction, support for structural induction and proof abstraction, and seamless integration with high-performance automated theorem provers were essential to our ability to successfully manage a proof of this size.", "num_citations": "83\n", "authors": ["1700"]}
{"title": "Synthesizing Java expressions from free-form queries\n", "abstract": " We present a new code assistance tool for integrated development environments. Our system accepts as input free-form queries containing a mixture of English and Java, and produces Java code expressions that take the query into account and respect syntax, types, and scoping rules of Java, as well as statistical usage patterns. In contrast to solutions based on code search, the results returned by our tool need not directly correspond to any previously seen code fragment. As part of our system we have constructed a probabilistic context free grammar for Java constructs and library invocations, as well as an algorithm that uses a customized natural language processing tool chain to extract information from free-form text queries. We present the results on a number of examples showing that our technique (1) often produces the expected code fragments,(2) tolerates much of the flexibility of natural language, and (3\u00a0\u2026", "num_citations": "80\n", "authors": ["1700"]}
{"title": "Deciding Boolean algebra with Presburger arithmetic\n", "abstract": " We describe an algorithm for deciding the first-order multisorted theory BAPA, which combines Boolean algebras of sets of uninterpreted elements (BA) and Presburger arithmetic operations (PA). BAPA can express the relationship between integer variables and cardinalities of unbounded finite sets, and it supports arbitrary quantification over sets and integers. Our motivation for BAPA is deciding verification conditions that arise in the static analysis of data structure consistency properties. Data structures often use an integer variable to keep track of the number of elements they store; an invariant of such a data structure is that the value of the integer variable is equal to the number of elements stored in the data structure. When the data structure content is represented by a set, the resulting constraints can be captured in BAPA. BAPA formulas with quantifier alternations arise when verifying programs with\u00a0\u2026", "num_citations": "77\n", "authors": ["1700"]}
{"title": "Towards a compiler for reals\n", "abstract": " Numerical software, common in scientific computing or embedded systems, inevitably uses a finite-precision approximation of the real arithmetic in which most algorithms are designed. In many applications, the roundoff errors introduced by finite-precision arithmetic are not the only source of inaccuracy, and measurement and other input errors further increase the uncertainty of the computed results. Adequate tools are needed to help users select suitable data types and evaluate the provided accuracy, especially for safety-critical applications. We present a source-to-source compiler called Rosa that takes as input a real-valued program with error specifications and synthesizes code over an appropriate floating-point or fixed-point data type. The main challenge of such a compiler is a fully automated, sound, and yet accurate-enough numerical error estimation. We introduce a unified technique for bounding roundoff\u00a0\u2026", "num_citations": "75\n", "authors": ["1700"]}
{"title": "Towards efficient satisfiability checking for Boolean Algebra with Presburger Arithmetic\n", "abstract": " Boolean Algebra with Presburger Arithmetic (BAPA) is a decidable logic that combines 1) Boolean algebra of sets of uninterpreted elements (BA) and 2) Presburger arithmetic (PA). BAPA can express relationships between integer variables and cardinalities of unbounded sets. In combination with other decision procedures and theorem provers, BAPA is useful for automatically verifying quantitative properties of data structures. This paper examines QFBAPA, the quantifier-free fragment of BAPA. The computational complexity of QFBAPA satisfiability was previously unknown; previous QFBAPA algorithms have non-deterministic exponential time complexity due to an explosion in the number of introduced integer variables.               This paper shows, for the first time, how to avoid such exponential explosion. We present an algorithm for checking satisfiability of QFBAPA formulas by reducing them to formulas of\u00a0\u2026", "num_citations": "74\n", "authors": ["1700"]}
{"title": "An integrated proof language for imperative programs\n", "abstract": " We present an integrated proof language for guiding the actions of multiple reasoning systems as they work together to prove complex correctness properties of imperative programs. The language operates in the context of a program verification system that uses multiple reasoning systems to discharge generated proof obligations. It is designed to 1) enable developers to resolve key choice points in complex program correctness proofs, thereby enabling automated reasoning systems to successfully prove the desired correctness properties; 2) allow developers to identify key lemmas for the reasoning systems to prove, thereby guiding the reasoning systems to find an effective proof decomposition; 3) enable multiple reasoning systems to work together productively to prove a single correctness property by providing a mechanism that developers can use to divide the property into lemmas, each of which is suitable for\u00a0\u2026", "num_citations": "71\n", "authors": ["1700"]}
{"title": "Generalized typestate checking for data structure consistency\n", "abstract": " We present an analysis to verify abstract set specifications for programs that use object field values to determine the membership of objects in abstract sets. In our approach, each module may encapsulate several data structures and use membership in abstract sets to characterize how objects participate in its data structures. Each module\u2019s specification uses set algebra formulas to characterize the effects of its operations on the abstract sets. The program may define abstract set membership in a variety of ways; arbitrary analyses (potentially with multiple analyses applied to different modules in the same program) may verify the corresponding set specifications. The analysis we present in this paper verifies set specifications by constructing and verifying set algebra formulas whose validity implies the validity of the set specifications.               We have implemented our analysis and annotated several programs\u00a0\u2026", "num_citations": "70\n", "authors": ["1700"]}
{"title": "Using first-order theorem provers in the Jahob data structure verification system\n", "abstract": " This paper presents our integration of efficient resolution-based theorem provers into the Jahob data structure verification system. Our experimental results show that this approach enables Jahob to automatically verify the correctness of a range of complex dynamically instantiable data structures, such as hash tables and search trees, without the need for interactive theorem proving or techniques tailored to individual data structures.               Our primary technical results include: (1) a translation from higher-order logic to first-order logic that enables the application of resolution-based theorem provers and (2) a proof that eliminating type (sort) information in formulas is both sound and complete, even in the presence of a generic equality operator. Our experimental results show that the elimination of type information often dramatically decreases the time required to prove the resulting formulas.               These\u00a0\u2026", "num_citations": "66\n", "authors": ["1700"]}
{"title": "An algorithm for deciding BAPA: Boolean algebra with Presburger arithmetic\n", "abstract": " We describe an algorithm for deciding the first-order multisorted theory BAPA, which combines 1) Boolean algebras of sets of uninterpreted elements (BA) and 2) Presburger arithmetic operations (PA). BAPA can express the relationship between integer variables and cardinalities of a priory unbounded finite sets, and supports arbitrary quantification over sets and integers.             Our motivation for BAPA is deciding verification conditions that arise in the static analysis of data structure consistency properties. Data structures often use an integer variable to keep track of the number of elements they store; an invariant of such a data structure is that the value of the integer variable is equal to the number of elements stored in the data structure. When the data structure content is represented by a set, the resulting constraints can be captured in BAPA. BAPA formulas with quantifier alternations arise when verifying\u00a0\u2026", "num_citations": "66\n", "authors": ["1700"]}
{"title": "Trustworthy numerical computation in scala\n", "abstract": " Modern computing has adopted the floating point type as a default way to describe computations with real numbers. Thanks to dedicated hardware support, such computations are efficient on modern architectures, even in double precision. However, rigorous reasoning about the resulting programs remains difficult. This is in part due to a large gap between the finite floating point representation and the infinite-precision real-number semantics that serves as the developers' mental model. Because programming languages do not provide support for estimating errors, some computations in practice are performed more and some less precisely than needed.", "num_citations": "60\n", "authors": ["1700"]}
{"title": "Modular pluggable analyses for data structure consistency\n", "abstract": " Hob is a program analysis system that enables the focused application of multiple analyses to different modules in the same program. In our approach, each module encapsulates one or more data structures and uses membership in abstract sets to characterize how objects participate in data structures. Each analysis verifies that the implementation of the module 1) preserves important internal data structure consistency properties and 2) correctly implements a set algebra interface that characterizes the effects of operations on the data structure. Collectively, the analyses use the set algebra to 1) characterize how objects participate in multiple data structures and to 2) enable the interanalysis communication required to verify properties that depend on multiple modules analyzed by different analyses. We implemented our system and deployed several pluggable analyses, including a flag analysis plug-in for modules\u00a0\u2026", "num_citations": "57\n", "authors": ["1700"]}
{"title": "Deductive program repair\n", "abstract": " We present an approach to program repair and its application to programs with recursive functions over unbounded data types. Our approach formulates program repair in the framework of deductive synthesis that uses existing program structure as a hint to guide synthesis. We introduce a new specification construct for symbolic tests. We rely on such user-specified tests as well as automatically generated ones to localize the fault and speed up synthesis. Our implementation is able to eliminate errors within seconds from a variety of functional programs, including symbolic computation code and implementations of functional data structures. The resulting programs are formally verified by the Leon system.", "num_citations": "53\n", "authors": ["1700"]}
{"title": "Runtime checking for program verification\n", "abstract": " The process of verifying that a program conforms to its specification is often hampered by errors in both the program and the specification. A runtime checker that can evaluate formal specifications can be useful for quickly identifying such errors. This paper describes our preliminary experience with incorporating run-time checking into the Jahob verification system and discusses some lessons we learned in this process. One of the challenges in building a runtime checker for a program verification system is that the language of invariants and assertions is designed for simplicity of semantics and tractability of proofs, and not for run-time checking. Some of the more challenging constructs include existential and universal quantification, set comprehension, specification variables, and formulas that refer to past program states. In this paper, we describe how we handle these constructs in our runtime checker, and\u00a0\u2026", "num_citations": "47\n", "authors": ["1700"]}
{"title": "Accelerating interpolants\n", "abstract": " We present Counterexample-Guided Accelerated Abstraction Refinement (CEGAAR), a new algorithm for verifying infinite-state transition systems. CEGAAR combines interpolation-based predicate discovery in counterexample-guided predicate abstraction with acceleration technique for computing the transitive closure of loops. CEGAAR applies acceleration to dynamically discovered looping patterns in the unfolding of the transition system, and combines overapproximation with underapproximation. It constructs inductive invariants that rule out an infinite family of spurious counterexamples, alleviating the problem of divergence in predicate abstraction without losing its adaptive nature. We present theoretical and experimental justification for the effectiveness of CEGAAR, showing that inductive interpolants can be computed from classical Craig interpolants and transitive closures of loops. We present an\u00a0\u2026", "num_citations": "44\n", "authors": ["1700"]}
{"title": "Structural subtyping of non-recursive types is decidable\n", "abstract": " We show that the first-order theory of structural subtyping of non-recursive types is decidable, as a consequence of a more general result on the decidability of term powers of decidable theories. Let /spl Sigma/ be a language consisting of function symbol and let /spl Cscr/; (with a finite or infinite domain C) be an L-structure where L is a language consisting of relation symbols. We introduce the notion of /spl Sigma/-term-power of the structure /spl Cscr/; denoted /spl Pscr/;/sub /spl Sigma//(/spl Cscr/;). The domain of /spl Pscr/;/sub /spl Sigma//(/spl Cscr/;) is the set of /spl Sigma/-terms over the set C. /spl Pscr/;/sub /spl Sigma//(/spl Cscr/;) has one term algebra operation for each f /spl isin/ /spl Sigma/, and one relation for each r /spl isin/ L defined by lifting operations of /spl Cscr/; to terms over C. We extend quantifier for term algebras and apply the Feferman-Vaught technique for quantifier elimination in products to obtain\u00a0\u2026", "num_citations": "44\n", "authors": ["1700"]}
{"title": "Generalized typestate checking using set interfaces and pluggable analyses\n", "abstract": " We present a generalization of standard typestate systems in which the typestate of each object is determined by its membership in a collection of abstract typestate sets. This generalization supports typestates that model participation in abstract data types, composite typestates that correspond to membership in multiple sets, and hierarchical typestates. Because membership in typestate sets corresponds directly to participation in data structures, our typestate system characterizes global sharing patterns.In our approach, each module encapsulates a data structure and uses membership in abstract sets to characterize how objects participate in its data structure. Each analysis verifies that the implementation of the module 1) preserves important internal data structure representation invariants and 2) conforms to a specification that uses formulas in a set algebra to characterize the effects of operations on the data\u00a0\u2026", "num_citations": "43\n", "authors": ["1700"]}
{"title": "An overview of the Jahob analysis system: project goals and current status\n", "abstract": " We present an overview of the Jahob system for modular analysis of data structure properties. Jahob uses a subset of Java as the implementation language and annotations with formulas in a subset of Isabelle as the specification language. It uses monadic second-order logic over trees to reason about reachability in linked data structures, the Isabelle theorem prover and Nelson-Oppen style theorem provers to reason about high-level properties and arrays, and a new technique to combine reasoning about constraints on uninterpreted function symbols with other decision procedures. It also incorporates new decision procedures for reasoning about sets with cardinality constraints. The system can infer loop invariants using new symbolic shape analysis. Initial results in the use of our system are promising; we are continuing to develop and evaluate it", "num_citations": "38\n", "authors": ["1700"]}
{"title": "An efficient decision procedure for imperative tree data structures\n", "abstract": " We present a new decidable logic called TREX for expressing constraints about imperative tree data structures. In particular, TREX supports a transitive closure operator that can express reachability constraints, which often appear in data structure invariants. We show that our logic is closed under weakest precondition computation, which enables its use for automated software verification. We further show that satisfiability of formulas in TREX is decidable in NP. The low complexity makes it an attractive alternative to more expensive logics such as monadic second-order logic (MSOL) over trees, which have been traditionally used for reasoning about tree data structures.", "num_citations": "37\n", "authors": ["1700"]}
{"title": "Runtime checking for separation logic\n", "abstract": " Separation logic is a popular approach for specifying properties of recursive mutable data structures. Several existing systems verify a subclass of separation logic specifications using static analysis techniques. Checking data structure specifications during program execution is an alternative to static verification: it can enforce the sophisticated specifications for which static verification fails, and it can help debug incorrect specifications and code by detecting concrete counterexamples to their validity.             This paper presents Separation Logic Invariant ChecKer (SLICK), a runtime checker for separation logic specifications. We show that, although the recursive style of separation logic predicates is well suited for runtime execution, the implicit footprint and existential quantification make efficient runtime checking challenging. To address these challenges we introduce a coloring technique for efficiently\u00a0\u2026", "num_citations": "36\n", "authors": ["1700"]}
{"title": "Classifying and solving horn clauses for verification\n", "abstract": " As a promising direction to overcome difficulties of verification, researchers have recently proposed the use of Horn constraints as intermediate representation. Horn constraints are related to Craig interpolation, which is one of the main techniques used to construct and refine abstractions in verification, and to synthesise inductive loop invariants. We give a classification of the different forms of Craig interpolation problems found in literature, and show that all of them correspond to natural fragments of (recursion-free) Horn constraints. For a logic that has the binary interpolation property, all of these problems are solvable, but have different complexity. In addition to presenting the theoretical classification and solvability results, we present a publicly available collection of benchmarks to evaluate solvers for Horn constraints, categorized according to our classification. The benchmarks are derived from real-world\u00a0\u2026", "num_citations": "35\n", "authors": ["1700"]}
{"title": "Relational analysis of algebraic datatypes\n", "abstract": " We present a technique that enables the use of finite model finding to check the satisfiability of certain formulas whose intended models are infinite. Such formulas arise when using the language of sets and relations to reason about structured values such as algebraic datatypes. The key idea of our technique is to identify a natural syntactic class of formulas in relational logic for which reasoning about infinite structures can be reduced to reasoning about finite structures. As a result, when a formula belongs to this class, we can use existing finite model finding tools to check whether the formula holds in the desired infinite model.", "num_citations": "35\n", "authors": ["1700"]}
{"title": "Hob: A tool for verifying data structure consistency\n", "abstract": " This tool demonstration presents Hob, a system for verifying data structure consistency for programs written in a general-purpose programming language. Our tool enables the focused application of multiple communicating static analyses to different modules in the same program. Using our tool throughout the program development process, we have successfully identified several bugs in both specifications and implementations of programs.", "num_citations": "34\n", "authors": ["1700"]}
{"title": "Crosscutting techniques in program specification and analysis\n", "abstract": " We present three aspect-oriented constructs (formats, scopes, and defaults) that, in combination with a specification language based on abstract sets of objects, enable the modular application of multiple arbitrarily precise (and therefore arbitrarily unscalable) analyses to scalably verify data structure consistency properties in sizable programs. Formats use a form of field introduction to group together the declarations of all of the fields that together comprise a given data structure. Scopes and defaults enable the developer to state certain data structure consistency properties once in a single specification construct that cuts across the preconditions and postconditions of the procedures in the system. Standard approaches, in contrast, scatter and duplicate such properties across the preconditions and postconditions. We have implemented a prototype implementation, specification, analysis, and verification system\u00a0\u2026", "num_citations": "34\n", "authors": ["1700"]}
{"title": "Interactive synthesis using free-form queries\n", "abstract": " We present a new code assistance tool for integrated development environments. Our system accepts free-form queries allowing a mixture of English and Java as an input, and produces Java code fragments that take the query into account and respect syntax, types, and scoping rules of Java as well as statistical usage patterns. The returned results need not have the structure of any previously seen code fragment. As part of our system we have constructed a probabilistic context free grammar for Java constructs and library invocations, as well as an algorithm that uses a customized natural language processing tool chain to extract information from free-form text queries. The evaluation results show that our technique can tolerate much of the flexibility present in natural language, and can also be used to repair incorrect Java expressions that contain useful information about the developer's intent. Our demo video is\u00a0\u2026", "num_citations": "33\n", "authors": ["1700"]}
{"title": "Development and evaluation of LAV: an SMT-based error finding platform\n", "abstract": " We present design and evaluation of LAV, a new open-source tool for statically checking program assertions and errors. LAV integrates into the popular LLVM infrastructure for compilation and analysis. LAV uses symbolic execution to construct a first-order logic formula that models the behavior of each basic blocks. It models the relationships between basic blocks using propositional formulas. By combining these two kinds of formulas LAV generates polynomial-sized verification conditions for loop-free code. It uses underapproximating or overapproximating unrolling to handle loops. LAV can pass generated verification conditions to one of the several SMT solvers: Boolector, MathSAT, Yices, and Z3. Our experiments with small 200 benchmarks suggest that LAV is competitive with related tools, so it can be used as an effective alternative for certain verification tasks. The experience also shows that LAV\u00a0\u2026", "num_citations": "33\n", "authors": ["1700"]}
{"title": "Decision procedures for set-valued fields\n", "abstract": " A central feature of current object-oriented languages is the ability to dynamically instantiate user-defined container data structures such as lists, trees, and hash tables. Implementations of these data structures typically use references to dynamically allocated objects, which complicates reasoning about the resulting program. Reasoning is simplified if data structure operations are specified in terms of abstract sets of objects associated with each data structure. For example, an insertion into a data structure in this approach becomes simply an insertion into a dynamically changing set-valued field of an object, as opposed to a manipulation of a dynamically linked structure attached to the object.In this paper we explore reasoning techniques for programs that manipulate data structures specified using set-valued abstract fields associated with container objects. We compare the expressive power and the complexity of\u00a0\u2026", "num_citations": "33\n", "authors": ["1700"]}
{"title": "Combining theorem proving with static analysis for data structure consistency\n", "abstract": " We describe an approach for combining theorem proving techniques with static analysis to analyze data structure consistency for programs that manipulate heterogeneous data structures. Our system uses interactive theorem proving and shape analysis to verify that data structure implementations conform to set interfaces. A simpler static analysis then uses the verified set interfaces to verify properties that characterize how shared objects participate in multiple data structures. We have successfully applied this technique to several programs and found that theorem proving within circumscribed regions of the program combined with static analysis enables the verification of large-scale program properties.", "num_citations": "32\n", "authors": ["1700"]}
{"title": "Bidirectional evaluation with direct manipulation\n", "abstract": " We present an evaluation update (or simply, update) algorithm for a full-featured functional programming language, which synthesizes program changes based on output changes. Intuitively, the update algorithm retraces the steps of the original evaluation, rewriting the program as needed to reconcile differences between the original and updated output values. Our approach, furthermore, allows expert users to define custom lenses that augment the update algorithm with more advanced or domain-specific program updates.   To demonstrate the utility of evaluation update, we implement the algorithm in Sketch-n-Sketch, a novel direct manipulation programming system for generating HTML documents. In Sketch-n-Sketch, the user writes an ML-style functional program to generate HTML output. When the user directly manipulates the output using a graphical user interface, the update algorithm reconciles the\u00a0\u2026", "num_citations": "30\n", "authors": ["1700"]}
{"title": "Towards complete reasoning about axiomatic specifications\n", "abstract": " To support verification of expressive properties of functional programs, we consider algebraic style specifications that may relate multiple user-defined functions, and compare multiple invocations of a function for different arguments. We present decision procedures for reasoning about such universally quantified properties of functional programs, using local theory extension methodology. We establish new classes of universally quantified formulas whose satisfiability can be checked in a complete way by finite quantifier instantiation. These classes include single-invocation axioms that generalize standard function contracts, but also certain many-invocation axioms, specifying that functions satisfy congruence, injectivity, or monotonicity with respect to abstraction functions, as well as conjunctions of some of these properties. These many-invocation axioms can specify correctness of abstract data type\u00a0\u2026", "num_citations": "29\n", "authors": ["1700"]}
{"title": "Synthesis for regular specifications over unbounded domains\n", "abstract": " Synthesis from declarative specifications is an ambitious automated method for obtaining systems that are correct by construction. Previous work includes synthesis of reactive finite-state systems from linear temporal logic and its fragments. Further recent work focuses on a different application area by doing functional synthesis over unbounded domains, using a modified Presburger arithmetic quantifier elimination algorithm. We present new algorithms for functional synthesis over unbounded domains based on automata-theoretic methods, with advantages in the expressive power and in the efficiency of synthesized code. Our approach synthesizes functions that meet given regular specifications defined over unbounded sequences of input and output bits. Thanks to the translation from weak monadic second-order logic to automata, this approach supports full Presburger arithmetic as well as bitwise operations on\u00a0\u2026", "num_citations": "29\n", "authors": ["1700"]}
{"title": "On our experience with modular pluggable analyses\n", "abstract": " We present a technique that enables the focused application of multiple analyses to different modules in the same program. In our approach, each module encapsulates one or more data structures and uses membership in abstract sets to characterize how objects participate in data structures. Each analysis verifies that the implementation of the module 1) preserves important internal data structure consistency properties and 2) correctly implements an interface that uses formulas in a set algebra to characterize the effects of operations on the encapsulated data structures. Collectively, the analyses use the set algebra to 1) characterize how objects participate in multiple data structures and to 2) enable the inter-analysis communication required to verify properties that depend on multiple modules analyzed by different analyses.We have implemented our system and deployed three pluggable analyses into it: a flag analysis for modules in which abstract set membership is determined by a flag field in each object, a plugin for modules that encapsulate linked data structures such as lists and trees, and an array plugin in which abstract set membership is determined by membership in an array. Our experimental results indicate that our approach makes it possible to effectively combine multiple analyses to verify properties that involve objects shared by multiple modules, with each analysis analyzing only those modules for which it is appropriate.", "num_citations": "25\n", "authors": ["1700"]}
{"title": "Solving quantified linear arithmetic by counterexample-guided instantiation\n", "abstract": " This paper presents a framework to derive instantiation-based decision procedures for satisfiability of quantified formulas in first-order theories, including its correctness, implementation, and evaluation. Using this framework we derive decision procedures for linear real arithmetic and linear integer arithmetic formulas with one quantifier alternation. We discuss extensions of these techniques for handling mixed real and integer arithmetic, and to formulas with arbitrary quantifier alternations. For the latter, we use a novel strategy that handles quantified formulas that are not in prenex normal form, which has advantages with respect to existing approaches. All of these techniques can be integrated within the solving architecture used by typical SMT solvers. Experimental results on standardized benchmarks from model checking, static analysis, and synthesis show that our implementation in the SMT solver cvc4\u00a0\u2026", "num_citations": "24\n", "authors": ["1700"]}
{"title": "Counter-example complete verification for higher-order functions\n", "abstract": " We present a verification procedure for pure higher-order functional Scala programs with parametric types. We show that our procedure is sound for proofs, as well as sound and complete for counter-examples. The procedure reduces the analysis of higher-order programs to checking satisfiability of a sequence of quantifier-free formulas over theories such as algebraic data types, integer linear arithmetic, and uninterpreted function symbols, thus enabling the use of efficient satisfiability modulo theory (SMT) solvers. Our solution supports arbitrary function types and arbitrarily nested anonymous functions (which can be stored in data structures, passed as arguments, returned, and applied). Among the contributions of this work is supporting even those cases when anonymous functions cannot be statically traced back to their definition, ensuring completeness of the approach for finding counter-examples. We provide a\u00a0\u2026", "num_citations": "24\n", "authors": ["1700"]}
{"title": "Speculative linearizability\n", "abstract": " Linearizability is a key design methodology for reasoning about implementations of concurrent abstract data types in both shared memory and message passing systems. It provides the illusion that operations execute sequentially and fault-free, despite the asynchrony and faults inherent to a concurrent system, especially a distributed one. A key property of linearizability is inter-object composability: a system composed of linearizable objects is itself linearizable. However, devising linearizable objects is very difficult, requiring complex algorithms to work correctly under general circumstances, and often resulting in bad average-case behavior. Concurrent algorithm designers therefore resort to speculation: optimizing algorithms to handle common scenarios more efficiently. The outcome are even more complex protocols, for which it is no longer tractable to prove their correctness. To simplify the design of efficient yet\u00a0\u2026", "num_citations": "24\n", "authors": ["1700"]}
{"title": "Predicting and preventing inconsistencies in deployed distributed systems\n", "abstract": " We propose a new approach for developing and deploying distributed systems, in which nodes predict distributed consequences of their actions and use this information to detect and avoid errors. Each node continuously runs a state exploration algorithm on a recent consistent snapshot of its neighborhood and predicts possible future violations of specified safety properties. We describe a new state exploration algorithm, consequence prediction, which explores causally related chains of events that lead to property violation. This article describes the design and implementation of this approach, termed CrystalBall. We evaluate CrystalBall on RandTree, BulletPrime, Paxos, and Chord distributed system implementations. We identified new bugs in mature Mace implementations of three systems. Furthermore, we show that if the bug is not corrected during system development, CrystalBall is effective in steering the\u00a0\u2026", "num_citations": "24\n", "authors": ["1700"]}
{"title": "Programming with enumerable sets of structures\n", "abstract": " We present an efficient, modular, and feature-rich framework for automated generation and validation of complex structures, suitable for tasks that explore a large space of structured values. Our framework is capable of exhaustive, incremental, parallel, and memoized enumeration from not only finite but also infinite domains, while providing fine-grained control over the process. Furthermore, the framework efficiently supports the inverse of enumeration (checking whether a structure can be generated and fast-forwarding to this structure to continue the enumeration) and lazy enumeration (achieving exhaustive testing without generating all structures). The foundation of efficient enumeration lies in both direct access to encoded structures, achieved with well-known and new pairing functions, and dependent enumeration, which embeds constraints into the enumeration to avoid backtracking. Our framework defines an\u00a0\u2026", "num_citations": "23\n", "authors": ["1700"]}
{"title": "Symbolic resource bound inference for functional programs\n", "abstract": " We present an approach for inferring symbolic resource bounds for purely functional programs consisting of recursive functions, algebraic data types and nonlinear arithmetic operations. In our approach, the developer specifies the desired shape of the bound as a program expression containing numerical holes which we refer to as templates. For e.g, time \u2264\u2009a\u2009\u2217 height(tree) + b where a,b are unknowns, is a template that specifies a bound on the execution time. We present a scalable algorithm for computing tight bounds for sequential and parallel execution times by solving for the unknowns in the template. We empirically evaluate our approach on several benchmarks that manipulate complex data structures such as binomial heap, lefitist heap, red-black tree and AVL tree. Our implementation is able to infer hard, nonlinear symbolic time bounds for our benchmarks that are beyond the capability of the\u00a0\u2026", "num_citations": "23\n", "authors": ["1700"]}
{"title": "On the theory of structural subtyping\n", "abstract": " We show that the first-order theory of structural subtyping of non-recursive types is decidable. Let  be a language consisting of function symbols (representing type constructors) and  a decidable structure in the relational language  containing a binary relation .  represents primitive types;  represents a subtype ordering. We introduce the notion of -term-power of , which generalizes the structure arising in structural subtyping. The domain of the -term-power of  is the set of -terms over the set of elements of . We show that the decidability of the first-order theory of  implies the decidability of the first-order theory of the -term-power of . Our decision procedure makes use of quantifier elimination for term algebras and Feferman-Vaught theorem. Our result implies the decidability of the first-order theory of structural subtyping of non-recursive types.", "num_citations": "23\n", "authors": ["1700"]}
{"title": "Existential heap abstraction entailment is undecidable\n", "abstract": " In this paper we study constraints for specifying properties of data structures consisting of linked objects allocated in the heap. Motivated by heap summary graphs in role analysis and shape analysis we introduce the notion of regular graph constraints. A regular graph constraint is a graph representing the heap summary; a heap satisfies a constraint if and only if the heap can be homomorphically mapped to the summary. Regular graph constraints form a very simple and natural fragment of the existential monadic second-order logic over graphs. One of the key problems in a compositional static analysis is proving that procedure preconditions are satisfied at every call site. For role analysis, precondition checking requires determining the validity of implication, i.e., entailment of regular graph constraints.               The central result of this paper is the undecidability of regular graph constraint entailment. The\u00a0\u2026", "num_citations": "23\n", "authors": ["1700"]}
{"title": "Automatic synthesis of out-of-core algorithms\n", "abstract": " We present a system for the automatic synthesis of efficient algorithms specialized for a particular memory hierarchy and a set of storage devices. The developer provides two independent inputs: 1) an algorithm that ignores memory hierarchy and external storage aspects; and 2) a description of the target memory hierarchy, including its topology and parameters. Our system is able to automatically synthesize memory-hierarchy and storage-device-aware algorithms out of those specifications, for tasks such as joins and sorting. The framework is extensible and allows developers to quickly synthesize custom out-of-core algorithms as new storage technologies become available.", "num_citations": "20\n", "authors": ["1700"]}
{"title": "The first-order theory of sets with cardinality constraints is decidable\n", "abstract": " We show that the decidability of the first-order theory of the language that combines Boolean algebras of sets of uninterpreted elements with Presburger arithmetic operations. We thereby disprove a recent conjecture that this theory is undecidable. Our language allows relating the cardinalities of sets to the values of integer variables, and can distinguish finite and infinite sets. We use quantifier elimination to show the decidability and obtain an elementary upper bound on the complexity. Precise program analyses can use our decidability result to verify representation invariants of data structures that use an integer field to represent the number of stored elements.", "num_citations": "20\n", "authors": ["1700"]}
{"title": "Boolean algebra of shape analysis constraints\n", "abstract": " The parametric shape analysis framework of Sagiv, Reps, and Wilhelm [45,46] uses three-valued structures as dataflow lattice elements to represent sets of states at different program points. The recent work of Yorsh, Reps, Sagiv, Wilhelm [48,50] introduces a family of formulas in (classical, two-valued) logic that are isomorphic to three-valued structures [46] and represent the same sets of concrete states.                 In this paper we introduce a larger syntactic class of formulas that has the same expressive power as the formulas in [48]. The formulas in [48] can be viewed as a normal form of the formulas in our syntactic class; we give an algorithm for transforming our formulas to this normal form. Our formulas make it obvious that the constraints are closed under all boolean operations and therefore form a boolean algebra. Our algorithm also gives a reduction of the entailment and the equivalence problems for\u00a0\u2026", "num_citations": "20\n", "authors": ["1700"]}
{"title": "Checking data structure properties orders of magnitude faster\n", "abstract": " Executable formal contracts help verify a program at runtime when static verification fails. However, these contracts may be prohibitively slow to execute, especially when they describe the transformations of data structures. In fact, often an efficient data structure operation with O(log(n)) running time executes in O(n log(n)) when naturally written specifications are executed at run time.               We present a set of techniques that improve the efficiency of run-time checks by orders of magnitude, often recovering the original asymptotic behavior of operations. Our implementation first removes any statically verified parts of checks. Then, it applies a program transformation that changes recursively computed properties into data structure fields, ensuring that properties are evaluated no more than once on a given data structure node. We present evaluation of our techniques on the Leon system for verification of\u00a0\u2026", "num_citations": "19\n", "authors": ["1700"]}
{"title": "On role logic\n", "abstract": " We present role logic, a notation for describing properties of relational structures in shape analysis, databases, and knowledge bases. We construct role logic using the ideas of de Bruijn's notation for lambda calculus, an encoding of first-order logic in lambda calculus, and a simple rule for implicit arguments of unary and binary predicates. The unrestricted version of role logic has the expressive power of first-order logic with transitive closure. Using a syntactic restriction on role logic formulas, we identify a natural fragment RL^2 of role logic. We show that the RL^2 fragment has the same expressive power as two-variable logic with counting C^2 and is therefore decidable. We present a translation of an imperative language into the decidable fragment RL^2, which allows compositional verification of programs that manipulate relational structures. In addition, we show how RL^2 encodes boolean shape analysis constraints and an expressive description logic.", "num_citations": "18\n", "authors": ["1700"]}
{"title": "On spatial conjunction as second-order logic\n", "abstract": " Spatial conjunction is a powerful construct for reasoning about dynamically allocated data structures, as well as concurrent, distributed and mobile computation. While researchers have identified many uses of spatial conjunction, its precise expressive power compared to traditional logical constructs was not previously known. In this paper we establish the expressive power of spatial conjunction. We construct an embedding from first-order logic with spatial conjunction into second-order logic, and more surprisingly, an embedding from full second order logic into first-order logic with spatial conjunction. These embeddings show that the satisfiability of formulas in first-order logic with spatial conjunction is equivalent to the satisfiability of formulas in second-order logic. These results explain the great expressive power of spatial conjunction and can be used to show that adding unrestricted spatial conjunction to a decidable logic leads to an undecidable logic. As one example, we show that adding unrestricted spatial conjunction to two-variable logic leads to undecidability. On the side of decidability, the embedding into second-order logic immediately implies the decidability of first-order logic with a form of spatial conjunction over trees. The embedding into spatial conjunction also has useful consequences: because a restricted form of spatial conjunction in two-variable logic preserves decidability, we obtain that a correspondingly restricted form of second-order quantification in two-variable logic is decidable. The resulting language generalizes the first-order theory of boolean algebra over sets and is useful in reasoning about the contents of data\u00a0\u2026", "num_citations": "17\n", "authors": ["1700"]}
{"title": "Generalized records and spatial conjunction in role logic\n", "abstract": " Role logic is a notation for describing properties of relational structures in shape analysis, databases and knowledge bases. A natural fragment of role logic corresponds to two-variable logic with counting and is therefore decidable.               In this paper, we show how to use role logic to describe open and closed records, as well as the dual of records, inverse records. We observe that the spatial conjunction operation of separation logic naturally models record concatenation. Moreover, we show how to eliminate the spatial conjunction of formulas of quantifier depth one in first-order logic with counting. As a result, allowing spatial conjunction of formulas of quantifier depth one preserves the decidability of two-variable logic with counting. This result applies to the two-variable role logic fragment as well.               The resulting logic smoothly integrates type system and predicate calculus notation and can be\u00a0\u2026", "num_citations": "13\n", "authors": ["1700"]}
{"title": "An update on deductive synthesis and repair in the leon tool\n", "abstract": " We report our progress in scaling deductive synthesis and repair of recursive functional Scala programs in the Leon tool. We describe new techniques, including a more precise mechanism for encoding the space of meaningful candidate programs. Our techniques increase the scope of synthesis by expanding the space of programs we can synthesize and by reducing the synthesis time in many cases. As a new example, we present a run-length encoding function for a list of values, which Leon can now automatically synthesize from specification consisting of the decoding function and the local minimality property of the encoded value.", "num_citations": "12\n", "authors": ["1700"]}
{"title": "Synthesis for unbounded bit-vector arithmetic\n", "abstract": " We propose to describe computations using QFPAbit, a language of quantifier-free linear arithmetic on unbounded integers with bitvector operations. We describe an algorithm that, given a QFPAbit formula with input and output variables denoting integers, generates an efficient function from a sequence of inputs to a sequence of outputs, whenever such function on integers exists. The starting point for our method is a polynomial-time translation mapping a QFPAbit formula into the sequential circuit that checks the correctness of the input/output relation. From such a circuit, our synthesis algorithm produces solved circuits from inputs to outputs that are no more than singly exponential in size of the original formula. In addition to the general synthesis algorithm, we present techniques that ensure that, for example, multiplication and division with large constants do not lead to an exponential blowup, addressing\u00a0\u2026", "num_citations": "12\n", "authors": ["1700"]}
{"title": "Opis: Reliable distributed systems in OCaml\n", "abstract": " Concurrency and distribution pose algorithmic and implementation challenges in developing reliable distributed systems, making the field an excellent testbed for evaluating programming language and verification paradigms. Several specialized domain-specific languages and extensions of memory-unsafe languages were proposed to aid distributed system development. We present an alternative to these approaches, showing that modern, higher-order, strongly typed, memory safe languages provide an excellent vehicle for developing and debugging distributed systems.", "num_citations": "12\n", "authors": ["1700"]}
{"title": "System FR: formalized foundations for the stainless verifier\n", "abstract": " We present the design, implementation, and foundation of a verifier for higher-order functional programs with generics and recursive data types. Our system supports proving safety and termination using preconditions, postconditions and assertions. It supports writing proof hints using assertions and recursive calls. To formalize the soundness of the system we introduce System FR, a calculus supporting System F polymorphism, dependent refinement types, and recursive types (including recursion through contravariant positions of function types). Through the use of sized types, System FR supports reasoning about termination of lazy data structures such as streams. We formalize a reducibility argument using the Coq proof assistant and prove the soundness of a type-checker with respect to call-by-value semantics, ensuring type safety and normalization for typeable programs. Our program verifier is implemented as\u00a0\u2026", "num_citations": "11\n", "authors": ["1700"]}
{"title": "Deciding functional lists with sublist sets\n", "abstract": " Motivated by the problem of deciding verification conditions for the verification of functional programs, we present new decision procedures for automated reasoning about functional lists. We first show how to decide in NP the satisfiability problem for logical constraints containing equality, constructor, selectors, as well as the transitive sublist relation. We then extend this class of constraints with operators to compute the set of all sublists, and the set of objects stored in a list. Finally, we support constraints on sizes of sets, which gives us the ability to compute list length as well as the number of distinct list elements. We show that the extended theory is reducible to the theory of sets with linear cardinality constraints, and therefore still in NP. This reduction enables us to combine our theory with other decidable theories that impose constraints on sets of objects, which further increases the potential of our\u00a0\u2026", "num_citations": "11\n", "authors": ["1700"]}
{"title": "On modular pluggable analyses using set interfaces\n", "abstract": " We present a technique that enables the focused applicationof multiple analyses to different modules in the same program. Our researchhas two goals: 1) to address the scalability limitations of preciseanalyses by focusing the analysis on only those parts of the program thatare relevant to the properties that the analysis is designed to verify, and2) to enable the application of specialized analyses that verify propertiesof specifc classes of data structures to programs that simultaneouslymanipulate several dfferent kinds of data structures.In our approach, each module encapsulates a data structure and usesmembership in abstract sets to characterize how objects participate inits data structure. Each analysis verifies that the implementation of themodule 1) preserves important internal data structure representationinvariants and 2) conforms to a specification that uses formulas in a setalgebra to characterize the effects of operations on the data structure.The analyses use the common set abstraction to 1) characterize howobjects participate in multiple data structures and to 2) enable the interanalysiscommunication required to verify properties that depend onmultiple modules analyzed by different analyses.We characterize the key soundness property that an analysis plugin mustsatisfy to successfully participate in our system and present several analysisplugins that satisfy this property: a flag plugin that analyzes modulesin which abstract set membership is determined by a flag  field in eachobject, and a graph types plugin that analyzes modules in which abstractset membership is determined by reachability properties of objects storedin tree-like data\u00a0\u2026", "num_citations": "11\n", "authors": ["1700"]}
{"title": "In-place refinement for effect checking\n", "abstract": " \u0423\u041a \u042a\u0439\u0437\u0438 \u0432 \u0425\u041a \u0424 \u0432\u0433 \u0425 \u0436\u0433\u0437\u0433 \u0438 \u042a \u0437 \u0436 \u0427\u0432 \u0425 \u0436\u0433\u0437\u0433 \u0438 \u042f \u043d \u042a \u0431\u0433\u0432 \u0418 \u042f \u041c \u041e\u0418 \u042d\u042b \u0430 \u0432\u0433 \u0431 \u0436\u0433\u0437\u0433 \u0438\u041a \u0433\u0431 \u0437\u0438\u0436 \u0438\u041a \u042c \u0436 \u040c\u0432 \u0431 \u0432\u0438 \u0430 \u0439\u0430\u0439\u0437 \u0437 \u0434\u0433\u043b \u0436 \u0439\u0430 \u0436 \u0431 \u043b\u0433\u0436 \u0433\u0436 \u0436 \u0419 \u0437\u0433\u0432 \u0432 \u0433\u0439\u0438 \u0434\u0436\u0433 \u0436 \u0431\u0437\u0418 \u0437\u0434 \u040c \u0438 \u0433\u0432\u0437\u0418 \u0432 \u0436 \u040c\u0432 \u0431 \u0432\u0438 \u0436 \u0430 \u0438 \u0433\u0432\u0437 \u0438\u043b \u0432 \u0434\u0436\u0433 \u0436 \u0431\u0437 \u0432 \u0437\u0434 \u040c \u0438 \u0433\u0432\u0437\u041a\u0421\u0432 \u0438 \u0437 \u0434 \u0434 \u0436 \u043b \u0432\u0438\u0436\u0433 \u0439 \u0432 \u043b \u0436 \u040c\u0432 \u0431 \u0432\u0438 \u0430 \u0439\u0430\u0439\u0437 \u0433\u0432\u0437\u0438\u0436\u0439 \u0438\u0418 \u0432\u0419\u0434\u0430 \u0436 \u040c\u0432 \u0431 \u0432\u0438\u041a \u042f \u0439\u0437 \u0432\u0419\u0434\u0430 \u0436 \u040c\u0432 \u0431 \u0432\u0438 \u0438\u0433 \u0434\u0436\u0433\u043a \u0438 \u0433\u0436\u0436 \u0438\u0432 \u0437\u0437 \u0433 \u0438 \u0419 \u0432 \u0435\u0439 \u0433\u0436 \u0432 \u0438 \u0436 \u040c\u0432 \u0431 \u0432\u0438 \u0436 \u0430 \u0438 \u0433\u0432 \u0438\u043b \u0432 \u0434\u0436\u0433 \u0436 \u0431\u0437 \u0432 \u0437\u0434 \u0419 \u040c \u0438 \u0433\u0432\u0437\u041a \u042c \u0438 \u0432 \u0435\u0439 \u0437 \u0434\u0434\u0430 \u0430 \u043b \u0432 \u043a \u0436 \u0438 \u0437\u0434 \u040c \u0438 \u0433\u0432 \u0437 \u0432 \u0431\u0434\u0433\u0438 \u0432\u0438 \u0434\u0436 \u0438 \u0438\u0436 \u0432\u0437 \u0433\u0436\u0431 \u0436\u0418 \u0437 \u0437 \u0438 \u0437 \u0433\u0436 \u0431\u0433\u0437\u0438 \u0434\u0436\u0433 \u0439\u0436 \u0419 \u0438\u0437\u041a", "num_citations": "11\n", "authors": ["1700"]}
{"title": "On algorithms and complexity for sets with cardinality constraints\n", "abstract": " Typestate systems ensure many desirable properties of imperative programs, including initialization of object fields and correct use of stateful library interfaces. Abstract sets with cardinality constraints naturally generalize typestate properties: relationships between the typestates of objects can be expressed as subset and disjointness relations on sets, and elements of sets can be represented as sets of cardinality one. Motivated by these applications, this paper presents new algorithms and new complexity results for constraints on sets and their cardinalities. We study several classes of constraints and demonstrate a trade-off between their expressive power and their complexity. Our first result concerns a quantifier-free fragment of Boolean Algebra with Presburger Arithmetic. We give a nondeterministic polynomial-time algorithm for reducing the satisfiability of sets with symbolic cardinalities to constraints on constant cardinalities, and give a polynomial-space algorithm for the resulting problem. In a quest for more efficient fragments, we identify several subclasses of sets with cardinality constraints whose satisfiability is NP-hard. Finally, we identify a class of constraints that has polynomial-time satisfiability and entailment problems and can serve as a foundation for efficient program analysis.", "num_citations": "10\n", "authors": ["1700"]}
{"title": "An instantiation-based approach for solving quantified linear arithmetic\n", "abstract": " This paper presents a framework to derive instantiation-based decision procedures for satisfiability of quantified formulas in first-order theories, including its correctness, implementation, and evaluation. Using this framework we derive decision procedures for linear real arithmetic (LRA) and linear integer arithmetic (LIA) formulas with one quantifier alternation. Our procedure can be integrated into the solving architecture used by typical SMT solvers. Experimental results on standardized benchmarks from model checking, static analysis, and synthesis show that our implementation of the procedure in the SMT solver CVC4 outperforms existing tools for quantified linear arithmetic.", "num_citations": "9\n", "authors": ["1700"]}
{"title": "On repair with probabilistic attribute grammars\n", "abstract": " Program synthesis and repair have emerged as an exciting area of research, driven by the potential for revolutionary advances in programmer productivity. Among most promising ideas emerging for synthesis are syntax-driven search, probabilistic models of code, and the use of input-output examples. Our paper shows how to combine these techniques and use them for program repair, which is among the most relevant applications of synthesis to general-purpose code. Our approach combines semantic specifications, in the form of pre- and post-conditions and input-output examples with syntactic specifications in the form of term grammars and AST-level statistics extracted from code corpora. We show that synthesis in this framework can be viewed as an instance of graph search, permitting the use of well-understood families of techniques such as A*. We implement our algorithm in a framework for verification, synthesis and repair of functional programs, demonstrating that our approach can repair programs that are beyond the reach of previous tools.", "num_citations": "8\n", "authors": ["1700"]}
{"title": "SMT-based Checking of Predicate-qualified Types for Scala\n", "abstract": " We present* qualified types* for Scala, a form of refinement types adapted to the Scala language. Qualified types allow users to refine base types and classes using predicate expressions. We implemented a type checker for qualified types that is embedded in Scala's next-generation compiler Dotty and delegates constraint checking to an SMT solver. Our system supports many of Scala's functional as well as its object-oriented constructs. To propagate user-provided qualifier ascriptions we utilize both Scala's own type system and an incomplete, but effective qualifier inference algorithm. Our evaluation shows that for a series of examples exerting various of Scala's language features, the additional compile-time overhead is manageable. By combining these features we show that one can verify essential safety properties such as static bounds-checks while retaining several of Scala's advanced features.", "num_citations": "8\n", "authors": ["1700"]}
{"title": "Translating scala programs to isabelle/HOL\n", "abstract": " We present a trustworthy connection between the Leon verification system and the Isabelle proof assistant. Leon is a system for verifying functional Scala programs. It uses a variety of automated theorem provers (ATPs) to check verification conditions (VCs) stemming from the input program. Isabelle, on the other hand, is an interactive theorem prover used to verify mathematical specifications using its own input language Isabelle/Isar. Users specify (inductive) definitions and write proofs about them manually, albeit with the help of semi-automated tactics. The integration of these two systems allows us to exploit Isabelle\u2019s rich standard library and give greater confidence guarantees in the correctness of analysed programs.", "num_citations": "8\n", "authors": ["1700"]}
{"title": "Scife: Scala framework for efficient enumeration of data structures with invariants\n", "abstract": " We introduce SciFe, a tool for automated generation of complex structures, suitable for tasks such as automated testing and synthesis. SciFe is capable of exhaustive, memoized enumeration of values from finite or infinite domains. SciFe is based on the concept of an enumerator, defined as an efficiently computable bijection between natural numbers and values from a given set. SciFe introduces higher-order enumerators which define enumerators that depend on additional parameters. SciFe also includes combinators that can construct more complex enumerators from existing ones while preserving exhaustiveness and efficiency. SciFe is a Scala library that implements a domain-specific language. This tool demo presents an overview of SciFe as well as its use to generate complex structures such as search trees and models of class hierarchies. Our experiments demonstrate better performance and shorter\u00a0\u2026", "num_citations": "8\n", "authors": ["1700"]}
{"title": "Game programming by demonstration\n", "abstract": " The increasing adoption of smartphones and tablets has provided tens of millions of users with substantial resources for computation, communication and sensing. The availability of these resources has a huge potential to positively transform our society and empower individuals. Unfortunately, although the number of users has increased dramatically, the number of developers is still limited by the high barrier that existing programming environments impose.", "num_citations": "8\n", "authors": ["1700"]}
{"title": "Fractional collections with cardinality bounds, and mixed linear arithmetic with stars\n", "abstract": " We present decision procedures for logical constraints involving collections such as sets, multisets, and fuzzy sets. Element membership in our collections is given by characteristic functions from a finite universe (of unknown size) to a user-defined subset of rational numbers. Our logic supports standard operators such as union, intersection, difference, or any operation defined pointwise using mixed linear integer-rational arithmetic. Moreover, it supports the notion of cardinality of the collection, defined as the sum of occurrences of all elements. Deciding formulas in such logic has applications in software verification.               Our decision procedure reduces satisfiability of formulas with collections to satisfiability of formulas in an extension of mixed linear integer-rational arithmetic with a \u201cstar\u201d operator. The star operator computes the integer cone (closure under vector addition) of the solution set of a given\u00a0\u2026", "num_citations": "8\n", "authors": ["1700"]}
{"title": "Confluence of untyped lambda calculus via simple types\n", "abstract": " We present a new proof of confluence of the untyped lambda calculus by reducing the confluence of \u03b2-reduction in the untyped lambda calculus to the confluence of \u03b2-reduction in the simply typed lambda calculus. This is achieved by embedding typed lambda terms into simply typed lambda terms. Using this embedding, an auxiliary reduction, and \u03b2-reduction on simply typed lambda terms we define a new reduction on all lambda terms. The transitive closure of the reduction defined is \u03b2-reduction on all lambda terms. This embedding allows us to use the confluence of \u03b2-reduction on simply typed lambda terms and thus prove the confluence of the reduction defined. As a consequence we obtain the confluence of \u03b2-reduction in the untyped lambda calculus.", "num_citations": "8\n", "authors": ["1700"]}
{"title": "Developing verified software using Leon\n", "abstract": " We present Leon, a system for developing functional Scala programs annotated with contracts. Contracts in Leon can themselves refer to recursively defined functions. Leon aims to find counterexamples when functions do not meet the specifications, and proofs when they do. Moreover, it can optimize run-time checks by eliminating statically checked parts of contracts and doing memoization. For verification Leon uses an incremental function unfolding algorithm (which could be viewed as k-induction) and SMT solvers. For counterexample finding it uses these techniques and additionally specification-based test generation. Leon can also execute specifications (e.g. functions given only by postconditions), by invoking a constraint solver at run time. To make this process more efficient and predictable, Leon supports deductive synthesis of functions from specifications, both interactively and in an automated\u00a0\u2026", "num_citations": "7\n", "authors": ["1700"]}
{"title": "Code completion using quantitative type inhabitation\n", "abstract": " Developing modern software applications typically involves composing functionality from existing libraries. This task is difficult because libraries may expose many methods to the developer. To help developers in such scenarios, we present a technique that synthesizes and suggests valid expressions of a given type at a given program point. The technique generates expressions by taking into account 1) polymorphic type constraints of the values in scope, 2) the API usage patterns in a corpus of code, and 3) any available test cases. It supports polymorphic type declarations and can synthesize expressions containing methods with any number of arguments and any depth. Our synthesis approach is based on a quantitative generalization of the type inhabitation problem with weighted type assignments. Weights indicate preferences to certain type bindings; they guide the search and enable the ranking of solutions. We present a new polynomial-time algorithm for a restricted version of quantitative type inhabitation, as well as a complete semidecision procedure for the general case of generic types. We identify a simple method to handle subtyping by introducing coercion functions and then erasing them in the final expressions. We have implemented our technique and evaluated it on over 100 examples taken from the Web. The system was remarkably effective in reinventing the erased expressions from the (previously unprocessed) code and ranking these expressions among the top suggestions for the developer. Our overall experience indicates that this approach to synthesizing and suggesting code fragments goes beyond currently available\u00a0\u2026", "num_citations": "7\n", "authors": ["1700"]}
{"title": "Zippy LL (1) parsing with derivatives\n", "abstract": " In this paper, we present an efficient, functional, and formally verified parsing algorithm for LL (1) context-free expressions based on the concept of derivatives of formal languages. Parsing with derivatives is an elegant parsing technique, which, in the general case, suffers from cubic worst-case time complexity and slow performance in practice. We specialise the parsing with derivatives algorithm to LL (1) context-free expressions, where alternatives can be chosen given a single token of lookahead. We formalise the notion of LL (1) expressions and show how to efficiently check the LL (1) property. Next, we present a novel linear-time parsing with derivatives algorithm for LL (1) expressions operating on a zipper-inspired data structure. We prove the algorithm correct in Coq and present an implementation as a part of Scallion, a parser combinators framework in Scala with enumeration and pretty printing capabilities.", "num_citations": "6\n", "authors": ["1700"]}
{"title": "Verifying and synthesizing software with recursive functions\n", "abstract": " Our goal is to help people construct software that does what they wish. We develop tools and algorithms that span static and dynamic verification, constraint solving, and program synthesis. I will outline the current state our verification and synthesis system, Leon, which translates software into a functional language and uses SMT solvers to reason about paths in programs and specifications. Certain completeness results partly explain the effectiveness of verification and synthesis procedures implemented within Leon, in particular results on decidability of sufficiently surjective abstraction functions, and the framework of complete functional synthesis", "num_citations": "6\n", "authors": ["1700"]}
{"title": "Verification of imperative programs in Scala\n", "abstract": " Safety-critical software systems can only support a limited number of failures. Extensive testing is good at catching errors, however that will never certify their absence. Formal verification is an alternative to testing that can (automatically) provide a mathematical proof of correctness of programs. In this thesis, we present a verification procedure for imperative programs. Our procedure reduces imperative programming to functional programming and uses a semi-decision procedure that can reason modulo recursive functions. As a complementary method, we propose an algorithm to generate test cases that attain a high coverage of the program statements or can force the execution of some very refined control paths. We have implemented these algorithms and have integrated them in the Leon verification system. Leon can be used to verify programs written in a proper subset of Scala.", "num_citations": "6\n", "authors": ["1700"]}
{"title": "Roles Are Really Great!\n", "abstract": " We present a new role system for specifying changing referencing relationships of heap objects. The role of an object depends, in large part, on its aliasing relationships with other objects, with the role of each object changing as its aliasing relationships change. Roles therefore capture important object and data structure properties and provide useful information about how the actions of the program interact with these properties. Our role system enables the programmer to specify the legal aliasing relationships that define the set of roles that objects may play, the roles of procedure parameters and object fields, and the role changes that procedures perform while manipulating objects. We present an interprocedural, compositional, and context-sensitive role analysis algorithm that verifies that a program respects the role constraints.", "num_citations": "6\n", "authors": ["1700"]}
{"title": "Designing an algorithm for role analysis\n", "abstract": " This thesis presents a system for specifying constraints on dynamically changing referencing relationships of heap objects, and an analysis for static verification of these constraints. The constraint specification system is based on the concept of role. The role of an object depends, in large part, on its aliasing relationships with other objects, with the role of each object changing as its aliasing relationships change. In this way roles capture object and data structure properties such as unique references, membership of objects in data structures, disjointness of data structures, absence of representation exposure, bidirectional associations, treeness, and absence or presence of cycles in the heap.", "num_citations": "6\n", "authors": ["1700"]}
{"title": "Sound reasoning about integral data types with a reusable SMT solver interface\n", "abstract": " We extend the Leon verification system for Scala with support for bit-vector reasoning, thus addressing one of its fundamental soundness limitation with respect to the treatment of integers primitives. We leverage significant progresses recently achieved in SMT solving by developing a solver-independent interface to easily configure the back-end of Leon. Our interface is based on the emerging SMT-LIB standard for SMT solvers, and we release a Scala library offering full support for the latest version of the standard. We use the standard BigInt Scala library to represent mathematical integers, whereas we correctly model Int as 32-bit integers. We ensure safety of arithmetic by checking for division by zero and correctly modeling division and modulo. We conclude with a performance comparison between the sound representation of Ints and the cleaner abstract representation using mathematical integers, and discuss\u00a0\u2026", "num_citations": "5\n", "authors": ["1700"]}
{"title": "On numerical error propagation with sensitivity\n", "abstract": " An emerging area of research is to automatically compute reasonably accurate upper bounds on numerical errors, including roundoffs due to the use of a finite-precision representation for real numbers such as floating point or fixed-point arithmetic. Previous approaches for this task are limited in their accuracy and scalability, especially in the presence of nonlinear arithmetic. Our main idea is to decouple the computation of newly introduced roundoff errors from the amplification of existing errors. To characterize the amplification of existing errors, we use the derivatives of functions corresponding to program fragments. We implemented this technique in an analysis for programs containing nonlinear computation, conditionals, and a certain class of loops. We evaluate our system on a number of benchmarks from embedded systems and scientific computation, showing substantial improvements in accuracy and scalability over the state of the art.", "num_citations": "5\n", "authors": ["1700"]}
{"title": "Certifying solutions for numerical constraints\n", "abstract": " A large portion of software is used for numerical computation in mathematics, physics and engineering. Among the aspects that make verification in this domain difficult is the need to quantify numerical errors, such as roundoff errors and errors due to the use of approximate numerical methods. Much of numerical software uses self-stabilizing iterative algorithms, for example, to find solutions of nonlinear equations.             To support such algorithms, we present a runtime verification technique that checks, given a nonlinear equation and a tentative solution, whether this value is indeed a solution to within a specified precision.             Our technique combines runtime verification approaches with information about the analytical equation being solved. It is independent of the algorithm used for finding the solution and is therefore applicable to a wide range of problems. We have implemented our technique for the\u00a0\u2026", "num_citations": "5\n", "authors": ["1700"]}
{"title": "MUNCH-automated reasoner for sets and multisets\n", "abstract": " This system description provides an overview of the MUNCH reasoner for sets and multisets. MUNCH takes as the input a formula in a logic that supports expressions about sets, multisets, and integers. Constraints over collections and integers are connected using the cardinality operator. Our logic is a fragment of logics of popular interactive theorem provers, and MUNCH is the first fully automated reasoner for this logic. MUNCH reduces input formulas to equisatisfiable linear integer arithmetic formulas. MUNCH reasoner is publicly available. It is implemented in the Scala programming language and currently uses the SMT solver Z3 to solve the generated integer linear arithmetic constraints.", "num_citations": "5\n", "authors": ["1700"]}
{"title": "Simplifying distributed system development\n", "abstract": " Distributed systems are difficult to design and develop. The difficulties arise both in basic safety correctness properties, as well as in achieving high performance. As a result of this complexity, the implementation of a distributed system often contains the basic algorithm coupled with an embedded strategy for making choices, such as the choice of a node to interact with. This paper proposes a programming model for distributed systems where 1) the application explicitly exposes the choices (decisions) that it needs to make as well as the objectives that it needs to maximize; 2) the application and the runtime system cooperate to maintain a predictive model of the distributed system and its environment; and 3) the runtime uses the predictive model to resolve the choices so as to maximize the objectives. We claim that this programming model results in simpler source code and lower development effort, and that it can lead to increased performance and robustness to various deployment settings. Our initial results of applying this model to a sample application are encouraging.", "num_citations": "5\n", "authors": ["1700"]}
{"title": "Automating verification of functional programs with quantified invariants\n", "abstract": " We present the foundations of a verifier for higher-order functional programs with generics and recursive algebraic data types. Our verifier supports finding sound proofs and counterexamples even in the presence of certain quantified invariants and recursive functions. Our approach uses the same language to describe programs and invariants and uses semantic criteria for establishing termination. Our implementation makes effective use of SMT solvers by encoding first-class functions and quantifiers into a quantifier-free fragment of first-order logic with theories. We are able to specify properties of datastructure operations involving higher-order functions with minimal annotation overhead and verify them with a high degree of automation. Our system is also effective at reporting counterexamples, even in the presence of first-order quantification.", "num_citations": "4\n", "authors": ["1700"]}
{"title": "On recursion-free Horn clauses and Craig interpolation\n", "abstract": " One of the main challenges in software verification is efficient and precise analysis of programs with procedures and loops. Interpolation methods remain among the most promising techniques for such verification. To accommodate the demands of various programming language features, over the past years several extended forms of interpolation have been introduced. We give a precise ontology of such extended interpolation methods, and investigate the relationship between interpolation and fragments of constrained recursion-free Horn clauses. We also introduce a new notion of interpolation, disjunctive interpolation, which solves a more general class of problems in one step compared to previous notions of interpolants, such as tree interpolants or inductive sequences of interpolants. We present algorithms and complexity for construction of interpolants, as well as the corresponding decision problems\u00a0\u2026", "num_citations": "4\n", "authors": ["1700"]}
{"title": "Set interfaces for generalized typestate and data structure consistency verification\n", "abstract": " Typestate systems allow the type of an object to change during its lifetime in the computation. Unlike standard type systems, they can enforce safety properties that depend on changing object states. We present a new, generalized formulation of typestate that models the typestate of an object through membership in abstract sets. This abstract set formulation enables developers to reason about cardinalities of sets, and in particular to state and verify the condition that certain sets are empty. We support hierarchical typestate classifications by specifying subset and disjointness properties over the typestate sets. We present our formulation of typestate in the context of the Hob program specification and verification framework. The Hob framework allows the combination of typestate analysis with powerful independently developed analyses such as shape analyses or theorem proving techniques. We implemented our analysis and annotated several programs 75-2500 lines of code with set specifications. Our implementation includes several optimizations that improve the scalability of the analysis and a novel loop invariant inference algorithm that eliminates the need to specify loop invariants. We present experimental data demonstrating the effectiveness of our techniques.Descriptors:", "num_citations": "4\n", "authors": ["1700"]}
{"title": "Polynomial constraints for sets with cardinality bounds\n", "abstract": " Logics that can reason about sets and their cardinality bounds are useful in program analysis, program verification, databases, and knowledge bases. This paper presents a class of constraints on sets and their cardinalities for which the satisfiability and the entailment problems are computable in polynomial time. Our class of constraints, based on tree-shaped formulas, is unique in being simultaneously tractable and able to express 1) that a set is a union of other sets, 2) that sets are disjoint, and 3) that a set has cardinality within a given range. As the main result we present a polynomial-time algorithm for checking entailment of our constraints.", "num_citations": "4\n", "authors": ["1700"]}
{"title": "A language for role specifications\n", "abstract": " This paper presents a new language for identifying the changing roles that objects play over the course of the computation. Each object\u2019s points-to relationships with other objects determine the role that it currently plays. Roles therefore reflect the object\u2019s membership in specific data structures, with the object\u2019s role changing as it moves between data structures. We provide a programming model which allows the developer to specify the roles of objects at different points in the computation. The model also allows the developer to specify the effect of each operation at the granularity of role changes that occur in identified regions of the heap.", "num_citations": "4\n", "authors": ["1700"]}
{"title": "Neural-network guided expression transformation\n", "abstract": " Optimizing compilers, as well as other translator systems, often work by rewriting expressions according to equivalence preserving rules. Given an input expression and its optimized form, finding the sequence of rules that were applied is a non-trivial task. Most of the time, the tools provide no proof, of any kind, of the equivalence between the original expression and its optimized form. In this work, we propose to reconstruct proofs of equivalence of simple mathematical expressions, after the fact, by finding paths of equivalence preserving transformations between expressions. We propose to find those sequences of transformations using a search algorithm, guided by a neural network heuristic. Using a Tree-LSTM recursive neural network, we learn a distributed representation of expressions where the Manhattan distance between vectors approximately corresponds to the rewrite distance between expressions. We then show how the neural network can be efficiently used to search for transformation paths, leading to substantial gain in speed compared to an uninformed exhaustive search. In one of our experiments, our neural-network guided search algorithm is able to solve more instances with a 2 seconds timeout per instance than breadth-first search does with a 5 minutes timeout per instance.", "num_citations": "3\n", "authors": ["1700"]}
{"title": "Termination of open higher-order programs\n", "abstract": " We study the problem of proving termination of open, higher-order programs with recursive functions and datatypes. We identify a new point in the design space of solutions, with an appealing trade-off between simplicity of specification, modularity, and amenability to automation. Specifically, we consider termination of open expressions in the presence of higher-order, recursive functions, and introduce a new notion of termination that is conditioned on the termination of the callbacks made by the expressions. For closed expressions our definition coincides with the traditional definition of termination. We derive sound proof obligations for establishing termination modulo callbacks, and develop a modular approach for verifying the obligations. Our approach is novel in three aspects.(a) It allows users to express properties about the environment in the form of higher-order contracts.(b) It establishes properties on the creation sites of closures instead of application sites and does not require knowing the targets of applications.(c) It uses a modular reasoning where termination (modulo callbacks) is verified for each function independently and then composed to check termination of their callers. We present the results of evaluating our approach on benchmarks comprising more than 10K lines of functional Scala code. The results show that our approach, when combined with a safety verifier, established both termination and safety of complex algorithms and data structures that are beyond the reach of state-of-the-art techniques. For example, it verifies Okasaki\u2019s scheduling-based data structures and lazy trees in under a few seconds.", "num_citations": "3\n", "authors": ["1700"]}
{"title": "Disjunctive interpolants for horn-clause verification (extended technical report)\n", "abstract": " One of the main challenges in software verification is efficient and precise compositional analysis of programs with procedures and loops. Interpolation methods remain one of the most promising techniques for such verification, and are closely related to solving Horn clause constraints. We introduce a new notion of interpolation, disjunctive interpolation, which solve a more general class of problems in one step compared to previous notions of interpolants, such as tree interpolants or inductive sequences of interpolants. We present algorithms and complexity for construction of disjunctive interpolants, as well as their use within an abstraction-refinement loop. We have implemented Horn clause verification algorithms that use disjunctive interpolants and evaluate them on benchmarks expressed as Horn clauses over the theory of integer linear arithmetic.", "num_citations": "3\n", "authors": ["1700"]}
{"title": "On decision procedures for ordered collections\n", "abstract": " We describe a decision procedure for a logic that supports 1) finite collections of elements (sets or multisets), 2) the cardinality operator, 3) a total order relation on elements, and 4) min and max operators on entire collections. Among the applications of this logic are 1) reasoning about the externally observable behavior of data structures such as random access priority queues, 2) specifying witness functions for synthesis problem of set algebra, and 3) reasoning about constraints on orderings arising in termination proofs.", "num_citations": "3\n", "authors": ["1700"]}
{"title": "Implications of a data structure consistency checking system\n", "abstract": " We present a framework for verifying that programs correctly preserve important data structure consistency properties. Results from our implemented system indicate that our system can effectively enable the scalable verification of very precise data structure consistency properties within complete programs. Our system treats both internal properties, which deal with a single data structure implementation, and external properties, which deal with properties that involve multiple data structures. A key aspect of our system is that it enables multiple analysis and verification packages to productively interoperate to analyze a single program. In particular, it supports the targeted use of very precise, unscalable analyses in the context of a larger analysis and verification system. The integration of different analyses in our system is based on a common set-based specification language: precise analyses verify that data\u00a0\u2026", "num_citations": "3\n", "authors": ["1700"]}
{"title": "On computing the fixpoint of a set of boolean equations\n", "abstract": " This paper presents a method for computing a least fixpoint of a system of equations over booleans. The resulting computation can be significantly shorter than the result of iteratively evaluating the entire system until a fixpoint is reached.", "num_citations": "3\n", "authors": ["1700"]}
{"title": "Reducibility method for termination properties of typed lambda terms\n", "abstract": " Il report seguente simula gli indicatori relativi alla produzione scientifica in relazione alle soglie ASN 2018-2020 del proprio SC/SSD. Si ricorda che il superamento dei valori soglia (almeno 2 su 3) \u00e8 requisito necessario ma non sufficiente al conseguimento dell'abilitazione.La simulazione si basa sui dati IRIS e presenta gli indicatori calcolati alla data indicata sul report. Si ricorda che in sede di domanda ASN presso il MIUR gli indicatori saranno invece calcolati a partire dal 1 gennaio rispettivamente del quinto/decimo/quindicesimo anno precedente la scadenza del quadrimestre di presentazione della domanda (art 2 del DM 598/2018).", "num_citations": "3\n", "authors": ["1700"]}
{"title": "Minimal synthesis of string to string functions from examples\n", "abstract": " We study the problem of synthesizing string to string transformations from a set of input/output examples. The transformations we consider are expressed using a particular class of transducers: functional non-deterministic Mealy machines (f-NDMM). These are machines that read input letters one at a time, and output one letter at each step. The functionality constraint ensures that, even though the machine is locally non-deterministic, each input string is mapped to exactly one output string by the transducer.               We suggest that, given a set of input/output examples, the smallest f-NDMM consistent with the examples is a good candidate for the transformation the user was expecting. We therefore study the problem of, given a set of examples, finding a minimal f-NDMM consistent with the examples and satisfying the functionality and totality constraints mentioned above.               We prove that, in general, the\u00a0\u2026", "num_citations": "2\n", "authors": ["1700"]}
{"title": "Polynomial-time proactive synthesis of tree-to-string functions from examples\n", "abstract": " Synthesis from examples enables non-expert users to generate programs by specifying examples of their behavior. A domain-specific form of such synthesis has been recently deployed in a widely used spreadsheet software product. In this paper we contribute to foundations of such techniques and present a complete algorithm for synthesis of a class of recursive functions defined by structural recursion over a given algebraic data type definition. The functions we consider map an algebraic data type to a string; they are useful for, e.g., pretty printing and serialization of programs and data. We formalize our problem as learning deterministic sequential top-down tree-to-string transducers with a single state. The first problem we consider is learning a tree-to-string transducer from any set of input/output examples provided by the user. We show that this problem is NP-complete in general, but can be solved in polynomial time under a (practically useful) closure condition that each subtree of a tree in the input/output example set is also part of the input/output examples. Because coming up with relevant input/output examples may be difficult for the user while creating hard constraint problems for the synthesizer, we also study a more automated active learning scenario in which the algorithm chooses the inputs for which the user provides the outputs. Our algorithm asks a worst-case linear number of queries as a function of the size of the algebraic data type definition to determine a unique transducer.", "num_citations": "2\n", "authors": ["1700"]}
{"title": "Proactive synthesis of recursive tree-to-string functions from examples (artifact)\n", "abstract": " This artifact, named Prosy, is an interactive command-line tool for synthesizing recursive tree-to-string functions (eg pretty-printers) from examples. Specifically, Prosy takes as input a Scala file containing a hierarchy of abstract and case classes, and synthesizes the printing function after interacting with the user. Prosy first pro-actively generates a finite set of trees such that their string representations uniquely determine the function to synthesize. While asking the output for each example, Prosy prunes away questions when it can infer their answers from previous answers. In the companion paper, we prove that this pruning allows Prosy not to require that the user provides answers to the entire set of questions, which is of size O (n^ 3) where n is the size of the input file, but only to a reasonably small subset of size O (n). Furthermore, Prosy guides the interaction by providing suggestions whenever it can.", "num_citations": "2\n", "authors": ["1700"]}
{"title": "On deductive program repair in Leon\n", "abstract": " We present an approach to program repair and its application to programs with recursive functions over unbounded data types. Our approach formulates program repair in the framework of deductive synthesis that uses existing program structure as a hint to guide synthesis. We introduce a new specification construct for symbolic tests. We rely on such user-specified tests as well as automatically generated ones to localize the fault and speed up synthesis. Our implementation is able to eliminate errors within seconds from a variety of functional programs, including symbolic computation code and implementations of functional data structures. The resulting programs are formally verified by the Leon system.", "num_citations": "2\n", "authors": ["1700"]}
{"title": "Interpolation for synthesis on unbounded domains\n", "abstract": " Synthesis procedures compile relational specifications into functions. In addition to bounded domains, synthesis procedures are applicable to domains such as mathematical integers, where the domain and range of relations and synthesized code is unbounded. Previous work presented synthesis procedures that generate self-contained code and do not require components as inputs. The advantage of this approach is that it requires only specifications as user input. On the other hand, in some cases it can be desirable to require that the synthesized system reuses existing components. This paper describes a technique to automatically synthesize systems from components. It is also applicable to repair scenarios where the desired sub-component of the system should be replaced to satisfy the overall specification. The technique is sound, and it is complete for constraints for which an interpolation procedure exists\u00a0\u2026", "num_citations": "2\n", "authors": ["1700"]}
{"title": "On Verification by Translation to Recursive Functions\n", "abstract": " We present the Leon verification system for a subset of the Scala programming language. Along with several functional features of Scala, Leon supports imperative constructs such as mutations and loops, using a translation into recursive functional form. Both properties and programs in Leon are expressed in terms of user-defined functions. We discuss several techniques that led to an efficient semi-decision procedure for first-order constraints with recursive functions, which is the core solving engine of Leon. We describe a generational unrolling strategy for recursive templates that yields smaller satisfiable formulas and ensures completeness for counter-examples. We evaluate the benefits of these techniques on a set of examples.", "num_citations": "2\n", "authors": ["1700"]}
{"title": "Abortable linearizable modules\n", "abstract": " We define the Abortable Linearizable Module automaton (ALM for short) and prove its key composition property using the IOA theory of HOLCF. The ALM is at the heart of the Speculative Linearizability framework. This framework simplifies devising correct speculative algorithms by enabling their decomposition into independent modules that can be analyzed and proved correct in isolation. It is particularly useful when working in a distributed environment, where the need to tolerate faults and asynchrony has made current monolithic protocols so intricate that it is no longer tractable to check their correctness. Our theory contains a typical example of a refinement proof in the I/O-automata framework of Lynch and Tuttle.", "num_citations": "2\n", "authors": ["1700"]}
{"title": "On Fast Code Completion using Type Inhabitation\n", "abstract": " Developing modern software applications typically involves composing functionality from existing libraries. This task is difficult because libraries may expose many methods to the developer. To help developers in such scenarios, we present a technique that synthesizes and suggests valid expressions of a given type at a given program point. As the basis of our technique we use type reconstruction for lambda calculus with subtyping. We show that the inhabitation problem in the presence of subtyping remains PSPACE-complete. We introduce a succinct representation for type judgements that merges types into equivalence classes to reduce the search space. We introduce a proof rule on this succinct representation of types and show that it is sound and complete for inhabitation. We implemented the resulting algorithm and deployed it as a plugin for the Eclipse IDE for Scala.", "num_citations": "2\n", "authors": ["1700"]}
{"title": "On set-driven combination of logics and verifiers\n", "abstract": " We explore the problem of automated reasoning about the nondisjoint combination of logics that share set variables and operations. We prove a general combination theorem, and apply it to show the decidability for the quantifier-free combination of formulas in WS2S, two-varible logic with counting, and Boolean Algebra with Presburger Arithmetic. Furthermore, we present an over-approximating algorithm that uses such combined logics to synthesize universally quantified invariants of infinite-state systems. The algorithm simultaneously synthesizes loop invariants of interest, and infers the relationships between sets to exchange the information between logics. We have implemented this algorithm and used it to prove detailed correctness properties of operations of linked data structure implementations.", "num_citations": "2\n", "authors": ["1700"]}
{"title": "Quantifier-free Boolean algebra with Presburger arithmetic is NP-complete\n", "abstract": " Boolean Algebra with Presburger Arithmetic (BAPA) combines1) Boolean algebras of sets of uninterpreted elements (BA)and 2) Presburger arithmetic operations (PA).  BAPA canexpress the relationship between integer variables andcardinalities of unbounded finite sets and can be used toexpress verification conditions in verification of datastructure consistency properties.In this report I consider the Quantifier-Free fragment ofBoolean Algebra with Presburger Arithmetic (QFBAPA).Previous algorithms for QFBAPA had non-deterministicexponential time complexity.  In this report I show thatQFBAPA is in NP, and is therefore NP-complete.  My resultyields an algorithm for checking satisfiability of QFBAPAformulas by converting them to polynomially sized formulasof quantifier-free Presburger arithmetic.  I expect thisalgorithm to substantially extend the range of QFBAPAproblems whose satisfiability can be checked in practice.", "num_citations": "2\n", "authors": ["1700"]}
{"title": "Combining Theorem Proving with Static Analysis for Data Structure Consistency\n", "abstract": " We describe an approach for combining theorem proving techniques with static analysis to analyze data structure consistency for programs that manipulate heterogenous data structures. Our system uses interactive theorem proving and shape analysis to verify that data structure implementations conform to set interfaces. A simpler static analysis uses the verified set interfaces to verify properties that characterize how shared objects participate in multiple data structures. We have succesfully applied this technique to several programs and found that we were able to effectively use theorem proving within circumscribed regions of the program to enable the verification, with an appropriate amount of effort, of large-scale program properties.", "num_citations": "2\n", "authors": ["1700"]}
{"title": "Modular Language Specifications in Haskell\n", "abstract": " We propose a framework for specification of programming language semantics, abstract and concrete syntax, and lexical structure. The framework is based on Modular Monadic Semantics and allows independent specification of various language features. Features such as arithmetics, conditionals, exceptions, state and nondeterminism can be freely combined into working interpreters, facilitating experiments in language design. A prototype implementation of this system in Haskell is described and possibilities for more sophisticated interpreter generator are outlined.", "num_citations": "2\n", "authors": ["1700"]}
{"title": "Modular Interpreters in Haskell\n", "abstract": " Cilj ovog rada je bilo ispitivanje prednosti koje u razvoju slozenih programa pruzaju mogucnosti savremenih funkcionalnih programskih jezika. U realizaciji je kori\u0161cena implementacija lenjog cisto funkcionalnog programskog jezika Haskell koja podrzava i niz pro\u0161irenja u odnosu na standard Haskell98, medu kojima su i multiparametarske klase. Kori\u0161cenjem ovih mogucnosti jezika razvijene su komponente za modularni interpreter zasnovan na denotacionoj semantici. Komponente podrzavaju razlicite semanticke osobine jezika i mogu se medusobno kombinovati. Time je kreirano okruzenje koje omogucava jednostavnu realizaciju prototipova interpretera i eksperimentisanje sa razlicitim dizajnima jezika. Postignut je vi\u0161i stepen modularnosti u odnosu na neke ranije poku\u0161aje jer je i specifikacija leksike i sintakse jezika modularna. Leksika komponenti interpretera se zadaje regularnim izrazima, a leksicki analizator je zasnovan na lenjoj konstrukciji prelaza i stanja deterministickog konacnog automata. Modularnost specifikacije sintakse je ostvarena zadavanjem sintakse pomocu infiksnih, prefiksnih i postfiksnih operatora sa prioritetima. Uz ranije rezultate koji realizuju modularnost apstraktne sintakse i denotacione semantike, ovo omogucava modularnu specifikaciju kompletnog interpretera, a pristup se moze pro\u0161iriti i na pisanje kompajlera.", "num_citations": "2\n", "authors": ["1700"]}
{"title": "Stainless verification system tutorial\n", "abstract": " Stainless (https://stainless. epfl. ch) is an open-source tool for verifying and finding errors in programs written in the Scala programming language. This tutorial will not assume any knowledge of Scala. It aims to get first-time users started with verification tasks by introducing the language, providing modelling and verification tips, and giving a glimpse of the tool\u2019s inner workings (encoding into functional programs, function unfolding, and using theories of satisfiability modulo theory solvers Z3 and CVC4).Stainless (and its predecessor, Leon) has been developed primarily in the EPFL\u2019s Laboratory for Automated Reasoning and Analysis in the period from 2011-2021. Its core specification and implementation language are typed recursive higher-order functional programs (imperative programs are also supported by automated translation to their functional semantics). Stainless can verify that functions are correct for all inputs with respect to provided preconditions and postconditions, it can prove that functions terminate (with optionally provided termination measure functions), and it can provide counter-examples to safety properties. Stainless enables users to write code that is both executed and verified using the same source files. Users can compile programs using the Scala compiler and run them on the JVM. For programs that adhere to certain discipline, users can generate source code in a small fragment of C and then use standard C compilers.", "num_citations": "1\n", "authors": ["1700"]}
{"title": "Proving and Disproving Programs with Shared Mutable Data\n", "abstract": " We present a tool for verification of deterministic programs with shared mutable references against specifications such as assertions, preconditions, postconditions, and read/write effects. We implement our tool by encoding programs with mutable references into annotated purely functional recursive programs. We then rely on function unfolding and the SMT solver Z3 to prove or disprove safety and to establish program termination. Our tool uses a new translation of programs where frame conditions are encoded using quantifier-free formulas in first-order logic (instead of relying on quantifiers or separation logic). This quantifier-free encoding enables SMT solvers to prove safety or report counterexamples relative to the semantics of procedure specifications. Our encoding is possible thanks to the expressive power of the extended array theory of the Z3 SMT solver. In addition to the ability to report counterexamples, our tool retains efficiency of reasoning about purely functional layers of data structures, providing expressiveness for mutable data but also a significant level of automation for purely functional aspects of software. We illustrate our tool through examples manipulating mutable linked structures and arrays.", "num_citations": "1\n", "authors": ["1700"]}
{"title": "Verifying resource bounds of programs with lazy evaluation and memoization\n", "abstract": " We present a new approach for specifying and verifying resource utilization of higher-order functional programs that use lazy evaluation and memoization. In our approach, users can specify the desired resource bound as templates with numerical holes eg as steps\u2264?\u2217 size (l)+? in the contracts of functions. They can also express invariants necessary for establishing the bounds that may depend on the state of memoization. Our approach operates in two phases: first generating an instrumented first-order program that accurately models the higher-order control flow and the effects of memoization on resources using sets, algebraic datatypes and mutual recursion, and then verifying the contracts of the first-order program by producing verification conditions of the form\u2203\u2200 using an extended assume/guarantee reasoning. We use our approach to verify precise bounds on resources such as evaluation steps and number of heap-allocated objects on 17 challenging data structures and algorithms. Our benchmarks, comprising of 5K lines of functional Scala code, include lazy mergesort, Okasaki\u2019s real-time queue and deque data structures that rely on aliasing of references to first-class functions; lazy data structures based on numerical representations such as the conqueue data structure of Scala\u2019s dataparallel library, cyclic streams, as well as dynamic programming algorithms such as knapsack and Viterbi. Our evaluations show that when averaged over all benchmarks the actual runtime resource consumption is 80% of the value inferred by our tool when estimating the number of evaluation steps, and is 88% for the number of heap-allocated objects.", "num_citations": "1\n", "authors": ["1700"]}
{"title": "On synthesizing code from free-form queries\n", "abstract": " We present a new code assistance tool for integrated development environments. Our system accepts free-form queries allowing a mixture of English and Java as an input, and produces Java code fragments that take the query into account and respect syntax, types, and scoping rules of Java as well as statistical usage patterns. The returned results need not have the structure of any previously seen code fragment. As part of our system we have constructed a probabilistic context free grammar for Java constructs and library invocations, as well as an algorithm that uses a customized natural language processing tool chain to extract information from free-form text queries. We present the results on a number of examples showing that our technique can tolerate much of the flexibility present in natural language, and can also be used to repair incorrect Java expressions that contain useful information about the developer\u2019s intent.", "num_citations": "1\n", "authors": ["1700"]}
{"title": "The relationship between Craig interpolation and recursion-free Horn clauses\n", "abstract": " Despite decades of research, there are still a number of concepts commonly found in software programs that are considered challenging for verification: among others, such concepts include concurrency, and the compositional analysis of programs with procedures. As a promising direction to overcome such difficulties, recently the use of Horn constraints as intermediate representation of software programs has been proposed. Horn constraints are related to Craig interpolation, which is one of the main techniques used to construct and refine abstractions in verification, and to synthesise inductive loop invariants. We give a survey of the different forms of Craig interpolation found in literature, and show that all of them correspond to natural fragments of (recursion-free) Horn constraints. We also discuss techniques for solving systems of recursion-free Horn constraints.", "num_citations": "1\n", "authors": ["1700"]}
{"title": "Automatic verification with abstraction and theorem proving\n", "abstract": " Things like even software verification, this has been the Holy Grail of computer science for many decades but now in some very key areas, for example, driver verification we\u2019re building tools that can do actual proof about the software and how it works in order to guarantee the reliability", "num_citations": "1\n", "authors": ["1700"]}
{"title": "On the Design and Implementation of SmartFloat and AffineFloat\n", "abstract": " Modern computing has adopted the floating point type as a default way to describe computations with real numbers. Thanks to dedicated hardware support, such computations are efficient on modern architectures. However, rigorous reasoning about the resulting programs remains difficult, because of a large gap between the finite floating point representation and the infinite-precision real-number semantics that serves as the mental model for the developers. Because programming languages do not provide support for estimating errors, some computations in practice are performed more and some less precisely than needed. We present a library solution for rigorous arithmetic computation. Our numerical data type library tracks a (double) floating point value, but also a guaranteed upper bound on the error between this value and the ideal value that would be computed in the real-value semantics. Our implementation involves a set of linear approximations based on an extension of affine arithmetic. The derived approximations cover most of the standard mathematical operations including trigonometric functions, and are more comprehensive than any publicly available ones. Moreover, while interval arithmetic rapidly yields overly pessimistic estimates, our approach remains precise for a range of computational tasks of interest.", "num_citations": "1\n", "authors": ["1700"]}
{"title": "On complete reasoning about axiomatic specifications\n", "abstract": " Automated software verification tools typically accept specifications of functions in terms of pre-and postconditions. However, many properties of functional programs can be more naturally specified using a more general form of universally quantified properties. Such general specifications may relate multiple user-defined functions, and compare multiple invocations of a function on different arguments. We present new decision procedures for complete and terminating reasoning about such universally quantified properties of functional programs. Our results use local theory extension methodology. We establish new classes of universally quantified formulas whose satisfiability can be checked in a complete way by finite quantifier instantiation. These new classes include single-invocation axioms that generalize standard function contracts, but also certain many-invocation axioms, specifying that functions satisfy congruence, injectivity, or monotonicity with respect to abstraction functions, as well as conjunctions of some of these properties. These many-invocation axioms can specify correctness of abstract data type implementations as well as certain information-flow properties. We also present a construction that enables the same function to be specified using different classes of decidable specifications on different partitions of its domain. This results in complete and terminating decision procedure for proving an interesting class of universally quantified specifications of functional programs.", "num_citations": "1\n", "authors": ["1700"]}
{"title": "On Complete Functional Synthesis\n", "abstract": " Synthesis of program fragments from specifications can make programs easier to write and easier to reason about. To integrate synthesis into programming languages, synthesis algorithms should behave in a predictable way\u2014they should succeed for a well-defined class of specifications. They should also support unbounded data types such as numbers and data structures. We propose to generalize decision procedures into predictable and complete synthesis procedures. Such procedures are guaranteed to find code that satisfies the specification if such code exists. Moreover, we identify conditions under which synthesis will statically decide whether the solution is guaranteed to exist, and whether it is unique. We demonstrate our approach by extending decision procedures for integer linear arithmetic and data structures into synthesis procedures, and establishing results on the size and the efficiency of the synthesized code. We show that such procedures are useful as a language extension with implicit value definitions, and we show how to extend a compiler to support such definitions. Our constructs provide the benefits of synthesis to programmers, without requiring them to learn new concepts or give up a deterministic execution model.", "num_citations": "1\n", "authors": ["1700"]}
{"title": "An Overview of the Jahob Analysis System\n", "abstract": " We present an overview of the Jahob system for modular analysis of data structure properties. Jahob uses a subset of Java as the implementation language and annotations with formulas in a subset of Isabelle as the specification language. It uses monadic secondorder logic over trees to reason about reachability in linked data structures, the Isabelle theorem prover and Nelson-Oppen style theorem provers to reason about high-level properties and arrays, and a new technique to combine reasoning about constraints on uninterpreted function symbols with other decision procedures. It also incorporates new decision procedures for reasoning about sets with cardinality constraints. The system can infer loop invariants using new symbolic shape analysis. Initial results in the use of our system are promising; we are continuing to develop and evaluate it.", "num_citations": "1\n", "authors": ["1700"]}
{"title": "Solving sets with cardinality constraints in pspace\n", "abstract": " Logics that can express properties of sets and their cardinalities are important in software analysis and verification. Such logics can specify preconditions and postconditions of data structure operations, such as the precondition that two input data structures store the same number of elements. They can also express data structure invariants such as the consistency of a size field of a data structures with the number of elements stored in the data structure. Boolean Algebra Algebra with Presburger Arithmetic (BAPA) is a natural language for expressing properties of sets and their cardinalities. We have previously showed that BAPA with arbitrary quantifiers has elementary complexity. Motivated by the observation that most decision procedure queries in program analysis and verification are quantifier-free, this paper examines QFBAPA, the quantifier-free fragment of BAPA. Previous algorithms for QFBAPA required\u00a0\u2026", "num_citations": "1\n", "authors": ["1700"]}
{"title": "Object Models, Heaps, and Interpretations\n", "abstract": " This paper explores the use of object models for specifying verifiable heap invariants. We define a simple language based on sets and relations and illustrate its use through examples. We give formal semantics of the language by translation into predicate calculus and interpretation of predicates in terms of objects and references in the program heap.", "num_citations": "1\n", "authors": ["1700"]}
{"title": "Types and confluence in lambda calculus\n", "abstract": " Types and confluence in lambda calculus - Infoscience English Fran\u00e7ais login Home > Types and confluence in lambda calculus Infoscience Information Usage statistics Files Types and confluence in lambda calculus Ghilezan, Silvia ; Kuncak, Viktor Published in: 3rd Panhellenic Logic Symposium, 17-21 Year: 2001 Laboratories: LARA Record appears in: Scientific production and competences > I&C - School of Computer and Communication Sciences > IINFCOM > LARA - Laboratory for Automated Reasoning and Analysis Work outside EPFL Conference Papers Published Export as: BibTeX | MARC | MARCXML | DC | EndNote | NLM | RefWorks | RIS View as: MARC | MARCXML | DC Add to your basket: Back to search Record created 2007-08-21, last modified 2020-04-20 Rate this document: 1 2 3 4 5 (Not yet reviewed) Add to personal basket Export as BibTeX, MARC, MARCXML, DC, EndNote, NLM, RefWorks \u2026", "num_citations": "1\n", "authors": ["1700"]}
{"title": "REDUCTIBILITY METHOD IN SIMPLY TYPED LAMBDA CALCULUS\n", "abstract": " A general reducibility method for proving reduction properties of the simply typed lambda calculus is presented and sufficient conditions for its application are derived.", "num_citations": "1\n", "authors": ["1700"]}