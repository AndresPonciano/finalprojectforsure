{"title": "Minimum risk training for neural machine translation\n", "abstract": " We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics, which are not necessarily differentiable. Experiments show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs. Transparent to architectures, our approach can be applied to more neural networks and potentially benefit more NLP tasks.", "num_citations": "398\n", "authors": ["1181"]}
{"title": "Max-margin deepwalk: Discriminative learning of network representation.\n", "abstract": " DeepWalk is a typical representation learning method that learns low-dimensional representations for vertices in social networks. Similar to other network representation learning (NRL) models, it encodes the network structure into vertex representations and is learnt in unsupervised form. However, the learnt representations usually lack the ability of discrimination when applied to machine learning tasks, such as vertex classification. In this paper, we overcome this challenge by proposing a novel semisupervised model, max-margin DeepWalk (MMDW). MMDW is a unified NRL framework that jointly optimizes the max-margin classifier and the aimed social representation learning model. Influenced by the max-margin classifier, the learnt representations not only contain the network structure, but also have the characteristic of discrimination. The visualizations of learnt representations indicate that our model is more discriminative than unsupervised ones, and the experimental results on vertex classification demonstrate that our method achieves a significant improvement than other state-of-theart methods. The source code can be obtained from https://github. com/thunlp/MMDW.", "num_citations": "271\n", "authors": ["1181"]}
{"title": "Semi-supervised learning for neural machine translation\n", "abstract": " While end-to-end neural machine translation (NMT) has made remarkable progress recently, NMT systems only rely on parallel corpora for parameter estimation. Since parallel corpora are usually limited in quantity, quality, and coverage, especially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT. We propose a semi-supervised approach for training NMT models on the concatenation of labeled (parallel corpora) and unlabeled (monolingual corpora) data. The central idea is to reconstruct the monolingual corpora using an autoencoder, in which the sourceto-target and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the ChineseEnglish dataset show that our approach achieves significant\u00a0\u2026", "num_citations": "233\n", "authors": ["1181"]}
{"title": "Adversarial training for unsupervised bilingual lexicon induction\n", "abstract": " Word embeddings are well known to capture linguistic regularities of the language on which they are trained. Researchers also observe that these regularities can transfer across languages. However, previous endeavors to connect separate monolingual word embeddings typically require cross-lingual signals as supervision, either in the form of parallel corpus or seed lexicon. In this work, we show that such cross-lingual connection can actually be established without any form of supervision. We achieve this end by formulating the problem as a natural adversarial game, and investigating techniques that are crucial to successful training. We carry out evaluation on the unsupervised bilingual lexicon induction task. Even though this task appears intrinsically cross-lingual, we are able to demonstrate encouraging performance without any cross-lingual clues.", "num_citations": "224\n", "authors": ["1181"]}
{"title": "Representation Learning of Knowledge Graphs with Hierarchical Types.\n", "abstract": " Representation learning of knowledge graphs aims to encode both entities and relations into a continuous low-dimensional vector space. Most existing methods only concentrate on learning representations with structured information located in triples, regardless of the rich information located in hierarchical types of entities, which could be collected in most knowledge graphs. In this paper, we propose a novel method named Type-embodied Knowledge Representation Learning (TKRL) to take advantages of hierarchical entity types. We suggest that entities should have multiple representations in different types. More specifically, we consider hierarchical types as projection matrices for entities, with two type encoders designed to model hierarchical structures. Meanwhile, type information is also utilized as relation-specific type constraints. We evaluate our models on two tasks including knowledge graph completion and triple classification, and further explore the performances on long-tail dataset. Experimental results show that our models significantly outperform all baselines on both tasks, especially with long-tail distribution. It indicates that our models are capable of capturing hierarchical type information which is significant when constructing representations of knowledge graphs. The source code of this paper can be obtained from https://github. com/thunlp/TKRL.", "num_citations": "213\n", "authors": ["1181"]}
{"title": "PLDA+ parallel latent dirichlet allocation with data placement and pipeline processing\n", "abstract": " Previous methods of distributed Gibbs sampling for LDA run into either memory or communication bottlenecks. To improve scalability, we propose four strategies: data placement, pipeline processing, word bundling, and priority-based scheduling. Experiments show that our strategies significantly reduce the unparallelizable communication bottleneck and achieve good load balancing, and hence improve scalability of LDA.", "num_citations": "210\n", "authors": ["1181"]}
{"title": "Iterative entity alignment via knowledge embeddings\n", "abstract": " Entity alignment aims to link entities and their counterparts among multiple knowledge graphs (KGs). Most existing methods typically rely on external information of entities such as Wikipedia links and require costly manual feature construction to complete alignment. In this paper, we present a novel approach for entity alignment via joint knowledge embeddings. Our method jointly encodes both entities and relations of various KGs into a unified low-dimensional semantic space according to a small seed set of aligned entities. During this process, we can align entities according to their semantic distance in this joint semantic space. More specifically, we present an iterative and parameter sharing method to improve alignment performance. Experiment results on realworld datasets show that, as compared to baselines, our method achieves significant improvements on entity alignment, and can further improve knowledge graph completion performance on various KGs with the favor of joint knowledge embeddings.", "num_citations": "176\n", "authors": ["1181"]}
{"title": "Improving the transformer translation model with document-level context\n", "abstract": " Although the Transformer translation model (Vaswani et al., 2017) has achieved state-of-the-art performance in a variety of translation tasks, how to use document-level context to deal with discourse phenomena problematic for Transformer still remains a challenge. In this work, we extend the Transformer model with a new context encoder to represent document-level context, which is then incorporated into the original encoder and decoder. As large-scale document-level parallel corpora are usually not available, we introduce a two-step training method to take full advantage of abundant sentence-level parallel corpora and limited document-level parallel corpora. Experiments on the NIST Chinese-English datasets and the IWSLT French-English datasets show that our approach improves over Transformer significantly.", "num_citations": "156\n", "authors": ["1181"]}
{"title": "Chinese word segmentation without using lexicon and hand-crafted training data\n", "abstract": " Chinese word segmentation is the first step in any Chinese NLP system. This paper presents a new algorithm for segmenting Chinese texts without making use of any lexicon and hand-crafted linguistic resource. The statistical data required by the algorithm, that is, mutual information and the difference of t-score between characters, is derived automatically from raw Chinese corpora. The preliminary experiment shows that the segmentation accuracy of our algorithm is acceptable. We hope the gaining of this approach will be beneficial to improving the perfomaance (especially in ability to cope with unknown words and ability to adapt to various domains) of the existing segmenters, though the algorithm itself can also be utilized as a stand-alone segmenter in some NLP applications.", "num_citations": "148\n", "authors": ["1181"]}
{"title": "Visualizing and understanding neural machine translation\n", "abstract": " While neural machine translation (NMT) has made remarkable progress in recent years, it is hard to interpret its internal workings due to the continuous representations and non-linearity of neural networks. In this work, we propose to use layer-wise relevance propagation (LRP) to compute the contribution of each contextual word to arbitrary hidden states in the attention-based encoder-decoder framework. We show that visualization with LRP helps to interpret the internal workings of NMT and analyze translation errors.", "num_citations": "144\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e\u5c42\u6b21\u7ed3\u6784\u7684\u591a\u7b56\u7565\u4e2d\u6587\u5fae\u535a\u60c5\u611f\u5206\u6790\u548c\u7279\u5f81\u62bd\u53d6\n", "abstract": " \u968f\u7740 Web2. 0 \u65f6\u4ee3\u7684\u5174\u8d77, \u4e0e\u5fae\u535a\u76f8\u5173\u7684\u7814\u7a76\u5f97\u5230\u4e86\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u7684\u5e7f\u6cdb\u5173\u6ce8. \u8be5\u6587\u4f7f\u7528\u65b0\u6d6a API \u83b7\u53d6\u6570\u636e, \u9488\u5bf9\u4e2d\u6587\u5fae\u535a\u6d88\u606f\u5c55\u5f00\u4e86\u60c5\u611f\u5206\u6790\u65b9\u9762\u7684\u7814\u7a76. \u6211\u4eec\u5bf9\u4e8e\u4e09\u79cd\u60c5\u611f\u5206\u6790\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u6df1\u5165\u7814\u7a76, \u5305\u62ec\u8868\u60c5\u7b26\u53f7\u7684\u89c4\u5219\u65b9\u6cd5, \u60c5\u611f\u8bcd\u5178\u7684\u89c4\u5219\u65b9\u6cd5, \u57fa\u4e8e SVM \u7684\u5c42\u6b21\u7ed3\u6784\u7684\u591a\u7b56\u7565\u65b9\u6cd5, \u5b9e\u9a8c\u8868\u660e\u57fa\u4e8e SVM \u7684\u5c42\u6b21\u7ed3\u6784\u591a\u7b56\u7565\u65b9\u6cd5\u6548\u679c\u6700\u597d. \u5176\u6b21, \u9488\u5bf9\u5c42\u6b21\u7ed3\u6784\u7684\u591a\u7b56\u7565\u65b9\u6cd5\u7684\u7279\u5f81\u9009\u62e9\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790, \u5305\u62ec\u4e3b\u9898\u65e0\u5173, \u4e3b\u9898\u76f8\u5173\u7684\u7279\u5f81. \u5b9e\u9a8c\u8868\u660e\u4f7f\u7528\u4e3b\u9898\u65e0\u5173\u7684\u7279\u5f81\u65f6\u83b7\u5f97\u7684\u51c6\u786e\u7387\u4e3a 66.467%. \u5f15\u5165\u4e3b\u9898\u76f8\u5173\u7684\u7279\u5f81\u540e, \u51c6\u786e\u7387\u63d0\u5347\u81f3 67.283%.", "num_citations": "135\n", "authors": ["1181"]}
{"title": "Tag-LDA for scalable real-time tag recommendation\n", "abstract": " In this paper, we propose a scalable and real-time method for tag recommendation. We model document, words and tags using tag-LDA model, which extends Latent Dirichlet Allocation model by adding the tag variable. With tag-LDA model, we can make real-time inference about the likelihood of assigning a tag to a new document, and use the likelihood to generate recommended tags. To handle large scale data set from the web, we implement a distributed training program, which can train the tag-LDA model in parallel with multiple machines. We use a real world blog data set containing 386,012 documents to evaluate our method. The distributed training program can handle the data set efficiently. The tags recommended by our method are 32% better than search-based collaborative filtering. We also analyze some examples to show the ability and weakness of our method.", "num_citations": "134\n", "authors": ["1181"]}
{"title": "\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u8bcd\u5178\u673a\u5236\u7684\u5b9e\u9a8c\u7814\u7a76\n", "abstract": " \u6458\u8981 \u5206\u8bcd\u8bcd\u5178\u662f\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u7cfb\u7edf\u7684\u4e00\u4e2a\u57fa\u672c\u7ec4\u6210\u90e8\u5206. \u5176\u67e5\u8be2\u901f\u5ea6\u76f4\u63a5\u5f71\u54cd\u5230\u5206\u8bcd\u7cfb\u7edf\u7684\u5904\u7406\u901f\u5ea6. \u672c\u6587\u8bbe\u8ba1\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8003\u5bdf\u4e86\u4e09\u79cd\u5178\u578b\u7684\u5206\u8bcd\u8bcd\u5178\u673a\u5236: \u6574\u8bcd\u4e8c\u5206, TRIE \u7d22\u5f15\u6811\u53ca\u9010\u5b57\u4e8c\u5206, \u7740\u91cd\u6bd4\u8f83\u4e86\u5b83\u4eec\u7684\u65f6\u95f4, \u7a7a\u95f4\u6548\u7387. \u5b9e\u9a8c\u663e\u793a: \u57fa\u4e8e\u9010\u5b57\u4e8c\u5206\u7684\u5206\u8bcd\u8bcd\u5178\u673a\u5236\u7b80\u6d01, \u9ad8\u6548, \u8f83\u597d\u5730\u6ee1\u8db3\u4e86\u5b9e\u7528\u578b\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u7cfb\u7edf\u7684\u9700\u8981.", "num_citations": "129\n", "authors": ["1181"]}
{"title": "\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u7814\u7a76\u8bc4\u8ff0\n", "abstract": " \u672c\u6587\u9996\u5148\u9610\u8ff0\u4e86\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u7814\u7a76\u7684\u73b0\u5b9e\u6027\u548c\u53ef\u80fd\u6027,\u63a5\u7740\u56f4\u7ed5\u8be5\u7814\u7a76\u4e2d\u7684\u4e09\u4e2a\u57fa\u672c\u95ee\u9898(\u5207\u5206\u6b67\u4e49\u6d88\u89e3,\u672a\u767b\u5f55\u8bcd\u5904\u7406\u548c\u8bed\u8a00\u8d44\u6e90\u5efa\u8bbe)\u5c55\u5f00\u4e86\u91cd\u70b9\u8ba8\u8bba,\u5e76\u627c\u8981\u8bc4\u4ecb\u4e86\u5341\u51e0\u5e74\u6765\u4ea7\u751f\u7684\u5404\u79cd\u65b9\u6cd5.\u6700\u540e\u5c31\u8fd9\u4e2a\u9886\u57df\u672a\u6765\u7684\u7814\u7a76\u8981\u70b9\u53d1\u8868\u4e86\u4e00\u4e9b\u4e2a\u4eba\u610f\u89c1.", "num_citations": "119\n", "authors": ["1181"]}
{"title": "\u4e2d\u6587\u59d3\u540d\u7684\u81ea\u52a8\u8fa8\u8bc6\n", "abstract": " \u4e2d\u6587\u59d3\u540d \u7684\u8fab\u8bc6\u5bf9\u6c49\u8bed \u81ea\u52a8\u5206\u8bcd\u7814\u7a76\u5177\u6709\u91cd\u8981\u610f\u4e49 \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4e2d\u6587\u6587\u672c\u4e2d\u81ea\u52a8\u8fab\u8bc6\u4e2d\u6587\u59d3\u540d \u7684\u7b97\u6cd5 \u6211\u4eec\u4ece\u65b0\u534e\u901a\u8baf\u793e\u65b0\u95fb\u8bed\u6599\u5e93\u4e2d\u968f\u673a\u62bd\u53d6\u4e86 \u5306 \u4e2a\u5305\u542b\u4e2d\u6587\u7272\u540d \u7684\u53e5\u5b50\u4f5c\u4e3a\u6d4b\u8bd5\u6837\u672c \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e, \u53ec\u56de\u7387\u8fbe\u5230\u4e86 \u536f", "num_citations": "117\n", "authors": ["1181"]}
{"title": "Earth mover\u2019s distance minimization for unsupervised bilingual lexicon induction\n", "abstract": " Cross-lingual natural language processing hinges on the premise that there exists invariance across languages. At the word level, researchers have identified such invariance in the word embedding semantic spaces of different languages. However, in order to connect the separate spaces, cross-lingual supervision encoded in parallel data is typically required. In this paper, we attempt to establish the cross-lingual connection without relying on any cross-lingual supervision. By viewing word embedding spaces as distributions, we propose to minimize their earth mover\u2019s distance, a measure of divergence between distributions. We demonstrate the success on the unsupervised bilingual lexicon induction task. In addition, we reveal an interesting finding that the earth mover\u2019s distance shows potential as a measure of language difference.", "num_citations": "115\n", "authors": ["1181"]}
{"title": "Punctuation as implicit annotations for Chinese word segmentation\n", "abstract": " We present a Chinese word segmentation model learned from punctuation marks which are perfect word delimiters. The learning is aided by a manually segmented corpus. Our method is considerably more effective than previous methods in unknown word recognition. This is a step toward addressing one of the toughest problems in Chinese word segmentation.", "num_citations": "112\n", "authors": ["1181"]}
{"title": "Experimental study on sentiment classification of Chinese review using machine learning techniques\n", "abstract": " Machine learning method in text classification has expanded from topic identification to more challenging tasks such as sentiment classification, and it is valuable to explore, compare methods applied in sentiment classification and investigate relevant influence factors. The chief aim of the present work is to compare four machine learning methods to sentiment classification of Chinese review. The corpus is made up of 16000 reviews from website. We investigate the factors which affect the performance: namely feature representation via Word-Based Unigram (WBU), Bigram (WBB) and Chinese Character-Based Bigram (CBB), Trigram (CBT); feature weighting schemes and feature dimensionality. Experimental evaluations show that performance depends on different settings. As a result, we draw a conclusion that Naive Bayes (NB) classifier obtains the best averaging performance when using WBB, CBT as features\u00a0\u2026", "num_citations": "90\n", "authors": ["1181"]}
{"title": "Hierarchical structure based hybrid approach to sentiment analysis of Chinese micro blog and its feature extraction\n", "abstract": " With the development of Web 2.0, micro blog has drawn substantial attention from both academia and industry communities. This paper utilizes micro blog API from Sina and carries out sentiment analysis on Chinese micro blog. We compare performances of three method, based on the emoticon, the sentiment lexicon and the hybrid approach over hierarchical structure using SVM, respectively. Through the experiments, we find that SVM based hybrid approach achieves the best performance. Furthermore, we analyze the contribution of various features in this model, including target-independent features and target-dependent features. Experimental results show that SVM based method can gain an accuracy of 66.467% with target-independent features, and an improved accuracy of 67.283% with the addition of target-dependent features.", "num_citations": "89\n", "authors": ["1181"]}
{"title": "Generating chinese classical poems with rnn encoder-decoder\n", "abstract": " We take the generation of Chinese classical poetry as a sequence-to-sequence learning problem, and investigate the suitability of recurrent neural network (RNN) for poetry generation task by various qualitative analyses. Then we build a novel system based on the RNN Encoder-Decoder structure to generate quatrains (Jueju in Chinese), with a keyword as input. Our system can learn semantic meaning within a single sentence, semantic relevance among sentences in a poem, and the use of structural, rhythmical and tonal patterns jointly, without utilizing any constraint templates. Experimental results show that our system outperforms other competitive systems.", "num_citations": "87\n", "authors": ["1181"]}
{"title": "Recursive autoencoders for ITG-based translation\n", "abstract": " While inversion transduction grammar (ITG) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules (ie, straight and inverted) dependent on actual blocks being merged remains a challenge. Unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively. The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling\u2019s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points.", "num_citations": "86\n", "authors": ["1181"]}
{"title": "\u5229\u7528\u6c49\u5b57\u4e8c\u5143\u8bed\u6cd5\u5173\u7cfb\u89e3\u51b3\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u4e2d\u7684\u4ea4\u96c6\u578b\u6b67\u4e49\n", "abstract": " \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u53e5\u5185\u76f8\u90bb\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u53cat-\u6d4b\u8bd5\u5dee\u8fd9\u4e24\u4e2a\u7edf\u8ba1\u91cf\u89e3\u51b3\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u4e2d\u4ea4\u96c6\u578b\u6b67\u4e49\u5207\u5206\u5b57\u6bb5\u7684\u65b9\u6cd5.\u521d\u6b65\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a,\u53ef\u4ee5\u6b63\u786e\u5904\u740690.3%\u7684\u4ea4\u96c6\u5b57\u6bb5.", "num_citations": "81\n", "authors": ["1181"]}
{"title": "TransNet: Translation-Based Network Representation Learning for Social Relation Extraction.\n", "abstract": " Conventional network representation learning (NRL) models learn low-dimensional vertex representations by simply regarding each edge as a binary or continuous value. However, there exists rich semantic information on edges and the interactions between vertices usually preserve distinct meanings, which are largely neglected by most existing NRL models. In this work, we present a novel Translation-based NRL model, TransNet, by regarding the interactions between vertices as a translation operation. Moreover, we formalize the task of Social Relation Extraction (SRE) to evaluate the capability of NRL methods on modeling the relations between vertices. Experimental results on SRE demonstrate that TransNet significantly outperforms other baseline methods by 10% to 20% on hits@ 1. The source code and datasets can be obtained from https://github. com/thunlp/TransNet.", "num_citations": "72\n", "authors": ["1181"]}
{"title": "Agreement-based joint training for bidirectional attention-based neural machine translation\n", "abstract": " The attentional mechanism has proven to be effective in improving end-to-end neural machine translation. However, due to the intricate structural divergence between natural languages, unidirectional attention-based models might only capture partial aspects of attentional regularities. In this chapter, we propose agreement-based joint training for bidirectional attention-based end-to-end neural machine translation. Instead of training source-to-target and target-to-source translation models independently, our approach encourages the two complementary models to agree on word alignment matrices on the same training data. Experiments on ChineseEnglish and English-French translation tasks show that agreement-based joint training significantly improves both alignment and translation quality over independent training.", "num_citations": "70\n", "authors": ["1181"]}
{"title": "Joint training for pivot-based neural machine translation\n", "abstract": " While recent neural machine translation approaches have delivered state-of-the-art performance for resource-rich language pairs, they suffer from the data scarcity problem for resource-scarce language pairs. Although this problem can be alleviated by exploiting a pivot language to bridge the source and target languages, the source-to-pivot and pivot-to-target translation models are usually independently trained. In this work, we introduce a joint training algorithm for pivot-based neural machine translation. We propose three methods to connect the two models and enable them to interact with each other during training. Experiments on Europarl and WMT corpora show that joint training of source-to-pivot and pivot-to-target models leads to significant improvements over independent training across various languages.", "num_citations": "69\n", "authors": ["1181"]}
{"title": "\u9ad8\u9891\u6700\u5927\u4ea4\u96c6\u578b\u6b67\u4e49\u5207\u5206\u5b57\u6bb5\u5728\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u4e2d\u7684\u4f5c\u7528\n", "abstract": " \u6458\u8981 \u4ea4\u96c6\u578b\u6b67\u4e49\u5207\u5206\u5b57\u6bb5\u662f\u5f71\u54cd\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u7cfb\u7edf\u7cbe\u5ea6\u7684\u4e00\u4e2a\u91cd\u8981\u56e0\u7d20. \u672c\u6587\u5f15\u5165\u4e86\u6700\u5927\u4ea4\u96c6\u578b\u6b67\u4e49\u5207\u5206\u5b57\u6bb5\u7684\u6982\u5ff5, \u5e76\u5c06\u4e4b\u533a\u5206\u4e3a\u771f, \u4f2a\u4e24\u79cd\u4e3b\u8981\u7c7b\u578b. \u8003\u5bdf\u4e00\u4e2a\u7ea6 1 \u4ebf\u5b57\u7684\u6c49\u8bed\u8bed\u6599\u5e93, \u6211\u4eec\u53d1\u73b0, \u6700\u5927\u4ea4\u96c6\u578b\u6b67\u4e49\u5207\u5206\u5b57\u6bb5\u7684\u9ad8\u9891\u90e8\u5206\u8868\u73b0\u51fa\u76f8\u5f53\u5f3a\u7684\u8986\u76d6\u80fd\u529b\u53ca\u7a33\u5b9a\u6027: \u524d 4, 619 \u4e2a\u7684\u8986\u76d6\u7387\u4e3a 59. 20%, \u4e14\u8986\u76d6\u7387\u53d7\u9886\u57df\u53d8\u5316\u7684\u5f71\u54cd\u4e0d\u5927. \u800c\u5176\u4e2d 4, 279 \u4e2a\u4e3a\u4f2a\u6b67\u4e49\u578b, \u8986\u76d6\u7387\u9ad8\u8fbe 53. 35%. \u6839\u636e\u4ee5\u4e0a\u5206\u6790, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u7684, \u9ad8\u9891\u6700\u5927\u4ea4\u96c6\u578b\u6b67\u4e49\u5207\u5206\u5b57\u6bb5\u7684\u5904\u7406\u7b56\u7565, \u53ef\u6709\u6548\u6539\u5584\u5b9e\u7528\u578b\u975e\u53d7\u9650\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u7cfb\u7edf\u7684\u7cbe\u5ea6.", "num_citations": "65\n", "authors": ["1181"]}
{"title": "Neural headline generation with minimum risk training\n", "abstract": " Automatic headline generation is an important research area within text summarization and sentence compression. Recently, neural headline generation models have been proposed to take advantage of well-trained neural networks in learning sentence representations and mapping sequence to sequence. Nevertheless, traditional neural network encoder utilizes maximum likelihood estimation for parameter optimization, which essentially constraints the expected training objective within word level instead of sentence level. Moreover, the performance of model prediction significantly relies on training data distribution. To overcome these drawbacks, we employ minimum risk training strategy in this paper, which directly optimizes model parameters with respect to evaluation metrics and statistically leads to significant improvements for headline generation. Experiment results show that our approach outperforms state-of-the-art systems on both English and Chinese headline generation tasks.", "num_citations": "64\n", "authors": ["1181"]}
{"title": "\u4e00\u79cd\u9ad8\u6027\u80fd\u7684\u4e24\u7c7b\u4e2d\u6587\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\n", "abstract": " \u6458! \u8981! \u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6027\u80fd\u7684\u4e24\u7c7b\u4e2d\u6587\u6587\u672c\u5206\u7c7b\u65b9\u6cd5 9 \u8be5\u65b9\u6cd5\u91c7\u7528\u4e24\u6b65\u5206\u7c7b\u7b56\u7565! \u7b2c# \u6b65\u4ee5\u8bcd\u6027\u4e3a\u52a8\u8bcd\u2019\u540d\u8bcd\u2019\u5f62\u5bb9\u8bcd\u6216\u526f\u8bcd\u7684\u8bcd\u8bed\u4f5c\u4e3a\u7279\u5f81 (\u4ee5\u6539\u8fdb\u7684\u4e92\u4fe1\u606f\u516c\u5f0f\u6765\u9009\u62e9\u7279\u5f81 (\u4ee5\u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\u8fdb\u884c\u5206\u7c7b 9 \u5229\u7528\u6587\u672c\u7279\u5f81\u4f30\u7b97\u6587\u672c\u5c5e\u4e8e\u4e24\u79cd\u7c7b\u578b\u7684\u6d4b\u5ea6! \u548c\"(\u6784\u9020\u4e8c\u7ef4\u6587\u672c\u7a7a\u95f4 (\u5c06\u6587\u672c\u6620\u5c04\u4e3a\u4e8c\u7ef4\u7a7a\u95f4\u4e2d\u7684\u4e00\u4e2a\u70b9 (\u5c06\u5206\u7c7b\u5668\u770b\u4f5c\u662f\u5728\u4e8c\u7ef4\u7a7a\u95f4\u4e2d\u5bfb\u6c42\u4e00\u6761\u5206\u5272\u76f4\u7ebf 9 \u6839\u636e\u6587\u672c\u70b9\u5230\u5206\u5272\u76f4\u7ebf\u7684\u8ddd\u79bb\u5c06\u4e8c\u7ef4\u7a7a\u95f4\u5206\u4e3a\u53ef\u9760\u548c\u4e0d\u53ef\u9760\u4e24\u90e8\u5206 (\u4ee5\u6b64\u8bc4\u4f30\u7b2c# \u6b65\u5206\u7c7b\u7ed3\u679c (\u82e5\u7b2c# \u6b65\u5206\u7c7b\u53ef\u9760 (\u505a\u51fa\u5206\u7c7b\u51b3\u7b56\" \u5426\u5219\u8fdb\u884c\u7b2c! \u6b65 9 \u7b2c! \u6b65\u5c06\u6587\u672c\u770b\u4f5c\u7531\u8bcd\u6027\u4e3a\u52a8\u8bcd\u6216\u540d\u8bcd\u7684\u8bcd\u8bed\u6784\u6210\u7684\u5e8f\u5217 (\u4ee5\u8be5\u5e8f\u5217\u4e2d\u76f8\u90bb\u4e24\u4e2a\u8bcd\u8bed\u6784\u6210\u7684\u4e8c\u5143\u8bcd\u8bed\u4e32\u4f5c\u4e3a\u7279\u5f81 (\u4ee5\u6539\u8fdb\u4e92\u4fe1\u606f\u516c\u5f0f\u6765\u9009\u62e9\u7279\u5f81 (\u4ee5\u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\u8fdb\u884c\u5206\u7c7b 9 \u5728\u7531#!% $$ \u7bc7\u6587\u672c\u6784\u6210\u7684\u6570\u636e\u96c6\u4e0a\u8fd0\u884c\u7684\u5b9e\u9a8c\u8868\u660e (\u4e24\u6b65\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\u8fbe\u5230\u4e86\u8f83\u9ad8\u7684\u5206\u7c7b\u6027\u80fd (\u7cbe\u786e\u7387\u2019\u53ec\u56de\u7387\u548c## \u503c\u5206\u522b\u4e3a\" AM#\" N (\"< M\"@ N \u548c\"> M>@ N9", "num_citations": "64\n", "authors": ["1181"]}
{"title": "Thumt: An open source toolkit for neural machine translation\n", "abstract": " This paper introduces THUMT, an open-source toolkit for neural machine translation (NMT) developed by the Natural Language Processing Group at Tsinghua University. THUMT implements the standard attention-based encoder-decoder framework on top of Theano and supports three training criteria: maximum likelihood estimation, minimum risk training, and semi-supervised training. It features a visualization tool for displaying the relevance between hidden states in neural networks and contextual words, which helps to analyze the internal workings of NMT. Experiments on Chinese-English datasets show that THUMT using minimum risk training significantly outperforms GroundHog, a state-of-the-art toolkit for NMT.", "num_citations": "62\n", "authors": ["1181"]}
{"title": "\u6c49\u8bed\u642d\u914d\u5b9a\u91cf\u5206\u6790\u521d\u63a2\n", "abstract": " \u642d\u914d\u5728\u8bed\u8a00\u6559\u5b66\u548c\u8bed\u8a00\u4fe1\u606f\u5904\u7406\u4e2d\u5177\u6709\u4e00\u5b9a\u7684\u5e94\u7528\u4ef7\u503c.\u6c49\u8bed\u642d\u914d\u7684\u7814\u7a76\u4ecd\u505c\u7559\u5728\u4e3b\u8981\u4ee5\u4eba\u7684\u4e3b\u89c2\u5224\u65ad\u4e3a\u6807\u51c6\u7684\u5b9a\u6027\u5206\u6790\u9636\u6bb5,\u7f3a\u4e4f\u5b9a\u91cf\u6570\u636e\u7684\u652f\u5f85.\u672c\u6587\u501f\u9274\u4e86\u56fd\u5916\u5728\u8bed\u8a00\u5b66\u548c\u8bed\u6599\u5e93\u8bed\u8a00\u5b66\u4e24\u4e2a\u65b9\u9762\u5173\u4e8e\u642d\u914d\u7684\u7814\u7a76\u6210\u679c,\u63d0\u51fa\u4e86\u5305\u62ec\u5f3a\u5ea6,\u79bb\u6563\u5ea6\u53ca\u5c16\u5cf0\u4e09\u9879\u7edf\u8ba1\u6307\u6807\u5728\u5185\u7684\u642d\u914d\u5b9a\u91cf\u8bc4\u4f30\u4f53\u7cfb,\u6784\u9020\u4e86\u76f8\u5e94\u7684\u642d\u914d\u5224\u65ad\u7b97\u6cd5.\u4f5c\u4e3a\u5bf9\u7b97\u6cd5\u7684\u521d\u6b65\u6d4b\u8bd5,\u6211\u4eec\u4ee5\u4e00\u4e2a\u7ea6710\u4e07\u8bcd\u6b21\u7684\u65b0\u534e\u793e\u65b0\u95fb\u8bed\u6599\u5e93\u4e3a\u5de5\u4f5c\u5e73\u53f0,\u5229\u7528\u8ba1\u7b97\u673a\u5bf9\"\u80fd\u529b\"\u4e00\u8bcd\u53ef\u80fd\u6784\u6210\u7684\u642d\u914d\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790.\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a,\u5c31\u8be5\u8bcd\u800c\u8a00,\u7b97\u6cd5\u81ea\u52a8\u53d1\u73b0\u642d\u914d\u7684\u51c6\u786e\u7387\u7ea6\u4e3a33.94%.\u672c\u9879\u7814\u7a76\u53ef\u671b\u4e3a\u8bed\u8a00\u5b66\u5bb6\u5ba2\u89c2,\u7cfb\u7edf,\u4e00\u81f4\u5730\u5206\u6790\u642d\u914d\u63d0\u4f9b\u5b9a\u91cf\u8f85\u52a9\u624b\u6bb5.", "num_citations": "62\n", "authors": ["1181"]}
{"title": "Enhancing stock movement prediction with adversarial training\n", "abstract": " This paper contributes a new machine learning solution for stock movement prediction, which aims to predict whether the price of a stock will be up or down in the near future. The key novelty is that we propose to employ adversarial training to improve the generalization of a neural network prediction model. The rationality of adversarial training here is that the input features to stock prediction are typically based on stock price, which is essentially a stochastic variable and continuously changed with time by nature. As such, normal training with static price-based features (e.g. the close price) can easily overfit the data, being insufficient to obtain reliable models. To address this problem, we propose to add perturbations to simulate the stochasticity of price variable, and train the model to work well under small yet intentional perturbations. Extensive experiments on two real-world stock data show that our method outperforms the state-of-the-art solution with 3.11% relative improvements on average w.r.t. accuracy, validating the usefulness of adversarial training for stock prediction task.", "num_citations": "58\n", "authors": ["1181"]}
{"title": "Sense vocabulary compression through the semantic knowledge of wordnet for neural word sense disambiguation\n", "abstract": " In this article, we tackle the issue of the limited quantity of manually sense annotated corpora for the task of word sense disambiguation, by exploiting the semantic relationships between senses such as synonymy, hypernymy and hyponymy, in order to compress the sense vocabulary of Princeton WordNet, and thus reduce the number of different sense tags that must be observed to disambiguate all words of the lexical database. We propose two different methods that greatly reduces the size of neural WSD models, with the benefit of improving their coverage without additional training data, and without impacting their precision. In addition to our method, we present a WSD system which relies on pre-trained BERT word vectors in order to achieve results that significantly outperform the state of the art on all WSD evaluation tasks.", "num_citations": "57\n", "authors": ["1181"]}
{"title": "Network representation learning: an overview\n", "abstract": " Networks are important ways of representing objects and their relationships. A key problem in the study of networks is how to represent the network information properly. With the developments in machine learning, feature learning of network vertices has become an important area of study. Network representation learning algorithms turn network information into dense, low-dimensional real-valued vectors that can be used as inputs for existing machine learning algorithms. For example, the representation of vertices can be fed to a classifier such as a Support Vector Machine (SVM) for vertex classification. In addition, the representations can be used for visualization by taking the representations as points in a Euclidean space. The study of network representation learning has attracted the attention of many researchers. In this article, recent works on network representation learning are introduced and summarized.", "num_citations": "56\n", "authors": ["1181"]}
{"title": "Contrastive unsupervised word alignment with non-local features\n", "abstract": " Word alignment is an important natural language processing task that indicates the correspondence between natural languages. Recently, unsupervised learning of log-linear models for word alignment has received considerable attention as it combines the merits of generative and discriminative approaches. However, a major challenge still remains: it is intractable to calculate the expectations of non-local features that are critical for capturing the divergence between natural languages. We propose a contrastive approach that aims to differentiate observed training examples from noises. It not only introduces prior knowledge to guide unsupervised learning but also cancels out partition functions. Based on the observation that the probability mass of log-linear models for word alignment is usually highly concentrated, we propose to use top- alignments to approximate the expectations with respect to posterior distributions. This allows for efficient and accurate calculation of expectations of non-local features. Experiments show that our approach achieves significant improvements over state-of-the-art unsupervised word alignment methods.", "num_citations": "56\n", "authors": ["1181"]}
{"title": "Prior knowledge integration for neural machine translation using posterior regularization\n", "abstract": " Although neural machine translation has made significant progress recently, how to integrate multiple overlapping, arbitrary prior knowledge sources remains a challenge. In this work, we propose to use posterior regularization to provide a general framework for integrating prior knowledge into neural machine translation. We represent prior knowledge sources as features in a log-linear model, which guides the learning process of the neural translation model. Experiments on Chinese-English translation show that our approach leads to significant improvements.", "num_citations": "54\n", "authors": ["1181"]}
{"title": "Two-character Chinese word extraction based on hybrid of internal and contextual measures\n", "abstract": " Word extraction is one of the important tasks in text information processing. There are mainly two kinds of statisticbased measures for word extraction: the internal measure and the contextual measure. This paper discusses these two kinds of measures for Chinese word extraction. First, nine widely adopted internal measures are tested and compared on individual basis. Then various schemes of combining these measures are tried so as to improve the performance. Finally, the left/right entropy is integrated to see the effect of contextual measures. Genetic algorithm is explored to automatically adjust the weights of combination and thresholds. Experiments focusing on two-character Chinese word extraction show a promising result: the F-measure of mutual information, the most powerful internal measure, is 57.82%, whereas the best combination scheme of internal measures achieves the F-measure of 59.87%. With the integration of the contextual measure, the word extraction achieves the F-measure of 68.48% at last.", "num_citations": "52\n", "authors": ["1181"]}
{"title": "Covering ambiguity resolution in Chinese word segmentation based on contextual information\n", "abstract": " Covering ambiguity is one of the two basic types of ambiguities in Chinese word segmentation. We regard its resolution as equivalent to word sense disambiguation, and make use of the classical vector space model in information retrieval to formulate the contexts of ambiguous words. A variation form of TFIDF weighting is proposed and a Chinese thesaurus is additionally utilized to cope with data sparseness problem. We select 90 frequent cases of covering ambiguities as the target. The training set includes 77654 sentences, and the test set includes 19242 sentences. The experimental results showed that our model has achieved 96.58% accuracy, outperforming the original form of TFIDF weighting as well as another baseline model, the hidden Markov model.", "num_citations": "46\n", "authors": ["1181"]}
{"title": "Improving back-translation with uncertainty-based confidence estimation\n", "abstract": " While back-translation is simple and effective in exploiting abundant monolingual corpora to improve low-resource neural machine translation (NMT), the synthetic bilingual corpora generated by NMT models trained on limited authentic bilingual data are inevitably noisy. In this work, we propose to quantify the confidence of NMT model predictions based on model uncertainty. With word- and sentence-level confidence measures based on uncertainty, it is possible for back-translation to better cope with noise in synthetic bilingual corpora. Experiments on Chinese-English and English-German translation tasks show that uncertainty-based confidence estimation significantly improves the performance of back-translation.", "num_citations": "45\n", "authors": ["1181"]}
{"title": "A neural reordering model for phrase-based translation\n", "abstract": " While lexicalized reordering models have been widely used in phrase-based translation systems, they suffer from three drawbacks: context insensitivity, ambiguity, and sparsity. We propose a neural reordering model that conditions reordering probabilities on the words of both the current and previous phrase pairs. Including the words of previous phrase pairs significantly improves context sensitivity and reduces reordering ambiguity. To alleviate the data sparsity problem, we build one classifier for all phrase pairs, which are represented as continuous space vectors. Experiments on the NIST Chinese-English datasets show that our neural reordering model achieves significant improvements over state-of-the-art lexicalized reordering models.", "num_citations": "45\n", "authors": ["1181"]}
{"title": "A critical appraisal of the research on Chinese word segmentation\n", "abstract": " This paper firstly discusses the practical significance and feasibility of word segmentation system for unrestricted Chinese texts, then focuses on three basic issues in the field, that is, segmentation ambiguity disambiguation, unknown word processing and language resource construction. The history of the researches and the important methods developed in the past are reviewed and assessed. Suggestions for future study are proposed.", "num_citations": "45\n", "authors": ["1181"]}
{"title": "\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u7814\u7a76\u4e2d\u7684\u82e6\u5e72\u7406\u8bba\u95ee\u9898\n", "abstract": " \u8ba8\u8bba\u4e86\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u7814\u7a76\u4e2d\u7684\u82e5\u5e72\u7406\u8bba\u95ee\u9898:\u5206\u8bcd\u8bcd\u8868,\u6b67\u4e49\u5207\u5206\u5b57\u6bb5,\u751f\u8bcd\u4ee5\u53ca\u5168\u5c40\u548c\u5c40\u90e8\u7edf\u8ba1\u91cf,\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u5904\u7406\u7b56\u7565.\u5b9e\u7528\u578b\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u7cfb\u7edf(\u5bf9\u4efb\u610f\u5f00\u653e\u6587\u672c,\u5207\u4ed6\u7cbe\u786e\u7387\u57fa\u672c\u7a33\u5b9a\u572899%\u5de6\u53f3)\u7684\u5b9e\u73b0\u5df2\u4e3a\u671f\u4e0d\u8fdc.", "num_citations": "45\n", "authors": ["1181"]}
{"title": "Confucius and its intelligent disciples: integrating social with search\n", "abstract": " Q&A sites continue to flourish as a large number of users rely on them as useful substitutes for incomplete or missing search results. In this paper, we present our experience with developing Confucius, a Google Q&A service launched in 21 countries and four languages by the end of 2009. Confucius employs six data mining subroutines to harness synergy between web search and social networks. We present these subroutines' design goals, algorithms, and their effects on service quality. We also describe techniques for and experience with scaling the subroutines to mine massive data sets.", "num_citations": "44\n", "authors": ["1181"]}
{"title": "\u6c49\u8bed\u6700\u957f\u540d\u8bcd\u77ed\u8bed\u7684\u81ea\u52a8\u8bc6\u522b\n", "abstract": " \u901a\u8fc7\u5bf9\u5305\u542b5573\u4e2a\u6c49\u8bed\u53e5\u5b50\u7684\u8bed\u6599\u6587\u672c\u4e2d\u7684\u6700\u957f\u540d\u8bcd\u77ed\u8bed\u7684\u5206\u5e03\u7279\u70b9\u7684\u7edf\u8ba1\u5206\u5e03,\u63d0\u51fa\u4e86\u4e24\u79cd\u6709\u6548\u7684\u6c49\u8bed\u6700\u957f\u540d\u8bcd\u77ed\u81ea\u52a8\u522b\u7b97\u6cd5;\u57fa\u4e8e\u8fb9\u754c\u5206\u5e03\u6982\u7387\u7684\u8bc6\u522b\u7b97\u6cd5\u548c\u57fa\u4e8e\u5185\u90e8\u7ed3\u6784\u7ec4\u5408\u7684\u8bc6\u522b\u7b97\u6cd5.\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a,\u540e\u8005\u7684\u8bc6\u522b\u6b63\u786e\u7387\u548c\u53ec\u56de\u7387\u5206\u522b\u8fbe\u5230\u4e8685.4%\u548c82.3%,\u53d6\u5f97\u4e86\u8f83\u597d\u7684\u81ea\u52a8\u8bc6\u522b\u6548\u679c.", "num_citations": "43\n", "authors": ["1181"]}
{"title": "Scalable term selection for text categorization\n", "abstract": " In text categorization, term selection is an important step for the sake of both categorization accuracy and computational efficiency. Different dimensionalities are expected under different practical resource restrictions of time or space. Traditionally in text categorization, the same scoring or ranking criterion is adopted for all target dimensionalities, which considers both the discriminability and the coverage of a term, such as \u03c72 or IG. In this paper, the poor accuracy at a low dimensionality is imputed to the small average vector length of the documents. Scalable term selection is proposed to optimize the term set at a given dimensionality according to an expected average vector length. Discriminability and coverage are separately measured; by adjusting the ratio of their weights in a combined criterion, the expected average vector length can be reached, which means a good compromise between the specificity and the exhaustivity of the term subset. Experiments show that the accuracy is considerably improved at lower dimensionalities, and larger term subsets have the possibility to lower the average vector length for a lower computational cost. The interesting observations might inspire further investigations.", "num_citations": "42\n", "authors": ["1181"]}
{"title": "CSeg&Tagl. 0: A Practical Word Segmenter and POS Tagger for Chinese Texts\n", "abstract": " Chinese word segmentation and POS tagging are two key techniques in many applications in Chinese information processing. Great efforts have been paid to the research in the last decade, but unfortunately, no practical system with high performance for unrestricted texts is available up to date. CSeg&Tagl. 0, a Chinese word segmenter and POS tagger which unifies these two procedures into one model, is introduced in this paper. The preliminary open tests show that the segmentation precision of CSeg&Tagl. 0 is about 98.0%-99.3%, POS tagging precision about 91.0% 97.1%, and the recall and precision for unknown words are ranging from 95.0% to 99.0% and from 87.6% to 95.3% respectively. The processing speed is about 100 characters per second on Pentium 133 PC. The work of improving the performance of the system is still ongoing.", "num_citations": "42\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e\u5b57\u4e32\u5185\u90e8\u7ed3\u5408\u7d27\u5bc6\u5ea6\u7684\u6c49\u8bed\u81ea\u52a8\u62bd\u8bcd\u5b9e\u9a8c\u7814\u7a76\n", "abstract": " (\u667a\u80fd\u6280\u672f\u4e0e\u7cfb\u7edf\u56fd\u5bb6\u91cd\u70b9\u5b9e\u9a8c\u5ba4\u6e05\u534e\u5927\u5b66\u8ba1\u7b97\u673a\u79d1\u5b66\u4e0e\u6280\u672f\u7cfb, \u5317\u4eac 100084) \u6458\u8981: \u81ea\u52a8\u62bd\u8bcd\u662f\u6587\u672c\u4fe1\u606f\u5904\u7406\u4e2d\u7684\u91cd\u8981\u8bfe\u9898\u4e4b\u4e00. \u5f53\u524d\u6bd4\u8f83\u901a\u884c\u7684\u89e3\u51b3\u7b56\u7565\u662f\u901a\u8fc7\u8bc4\u4f30\u5019\u9009\u5b57\u4e32\u5185\u90e8\u7ed3\u5408\u7d27\u5bc6\u5ea6\u6765\u5224\u65ad\u8be5\u4e32\u6210\u8bcd\u4e0e\u5426. \u672c\u6587\u5206\u522b\u8003\u5bdf\u4e86\u4e5d\u79cd\u5e38\u7528\u7edf\u8ba1\u91cf\u5728\u6c49\u8bed\u81ea\u52a8\u62bd\u8bcd\u4e2d\u7684\u8868\u73b0, \u8fdb\u800c\u5c1d\u8bd5\u5c06\u5b83\u4eec\u7ec4\u5408\u5728\u4e00\u8d77, \u4ee5\u671f\u63d0\u9ad8\u6027\u80fd. \u4e3a\u4e86\u8fbe\u5230\u5c3d\u53ef\u80fd\u597d\u7684\u7ec4\u5408\u6548\u679c, \u91c7\u7528\u4e86\u9057\u4f20\u7b97\u6cd5\u6765\u81ea\u52a8\u8c03\u6574\u7ec4\u5408\u6743\u91cd. \u5bf9\u4e8c\u5b57\u8bcd\u7684\u81ea\u52a8\u62bd\u8bcd\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e, \u8fd9\u4e5d\u79cd\u5e38\u7528\u7edf\u8ba1\u91cf\u4e2d, \u4e92\u4fe1\u606f\u7684\u62bd\u8bcd\u80fd\u529b\u6700\u5f3a, F2measure \u53ef\u8fbe 54177%, \u800c\u7ec4\u5408\u540e\u7684 F2measure \u4e3a 55147%, \u4ec5\u6bd4\u4e92\u4fe1\u606f\u63d0\u9ad8\u4e86 0170%, \u6548\u679c\u5e76\u4e0d\u663e\u8457. \u6211\u4eec\u7684\u7ed3\u8bba\u662f:(1) \u4e0a\u8ff0\u7edf\u8ba1\u91cf\u5e76\u4e0d\u5177\u5907\u826f\u597d\u7684\u4e92\u8865\u6027;(2) \u901a\u5e38\u60c5\u51b5\u4e0b, \u5efa\u8bae\u76f4\u63a5\u9009\u7528\u4e92\u4fe1\u606f\u8fdb\u884c\u81ea\u52a8\u62bd\u8bcd, \u7b80\u5355\u6709\u6548.", "num_citations": "40\n", "authors": ["1181"]}
{"title": "\u6c49\u8bed\u53e5\u5b50\u7684\u7ec4\u5757\u5206\u6790\u4f53\u7cfb\n", "abstract": " \u4ecb\u7ecd\u4e86\u4e00\u79cd\u63cf\u8ff0\u80fd\u529b\u4ecb\u4e8e\u7ebf\u6027\u8bcd\u5e8f\u5217\u548c\u5b8c\u6574\u53e5\u6cd5\u6811\u8868\u793a\u4e4b\u95f4\u7684\u6d45\u5c42\u53e5\u6cd5\u77e5\u8bc6\u63cf\u8ff0\u4f53\u7cfb-\u7ec4\u5757\u5206\u6790\u4f53\u7cfb,\u5e76\u8be6\u7ec6\u8ba8\u8bba\u4e86\u5176\u4e2d\u4e24\u5927\u90e8\u5206;\u8bcd\u754c\u5757\u548c\u6210\u5206\u7ec4\u7684\u57fa\u672c\u5185\u5bb9\u53ca\u5176\u81ea\u52a8\u8bc6\u522b\u7b97\u6cd5,\u5728\u6b64\u57fa\u7840\u4e0a,\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u9636\u6bb5\u6784\u9020\u6c49\u8bed\u6811\u5e93\u7684\u65b0\u8bbe\u60f3,\u5373\u5148\u6784\u9020\u7ec4\u5757\u5e93,\u518d\u6784\u9020\u6811\u5e93,\u8fdb\u884c\u4e86\u4e00\u7cfb\u5217\u53e5\u6cd5\u5206\u6790\u548c\u77e5\u8bc6\u83b7\u53d6\u5b9e\u9a8c,\u5305\u62ec1)\u81ea\u7136\u8bc6\u522b\u6c49\u8bed\u6700\u957f\u540d\u8bcd\u77ed\u8bed;2)\u81ea\u52a8\u83b7\u53d6\u6c49\u8bed\u53e5\u6cd5\u77e5\u8bc6\u7b49.\u6240\u6709\u8fd9\u4e9b\u5de5\u4f5c\u90fd\u8bc1\u660e\u4e86\u8fd9\u79cd\u77e5\u8bc6\u63cf\u8ff0\u4f53\u7cfb\u7684\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027.", "num_citations": "40\n", "authors": ["1181"]}
{"title": "Fast-champollion: a fast and robust sentence alignment algorithm\n", "abstract": " Sentence-level aligned parallel texts are important resources for a number of natural language processing (NLP) tasks and applications such as statistical machine translation and cross-language information retrieval. With the rapid growth of online parallel texts, efficient and robust sentence alignment algorithms become increasingly important. In this paper, we propose a fast and robust sentence alignment algorithm, ie, Fast-Champollion, which employs a combination of both length-based and lexiconbased algorithm. By optimizing the process of splitting the input bilingual texts into small fragments for alignment, Fast-Champollion, as our extensive experiments show, is 4.0 to 5.1 times as fast as the current baseline methods such as Champollion (Ma, 2006) on short texts and achieves about 39.4 times as fast on long texts, and Fast-Champollion is as robust as Champollion.", "num_citations": "39\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e\u65e0\u6307\u5bfc\u5b66\u4e60\u7b56\u7565\u7684\u65e0\u8bcd\u8868\u6761\u4ef6\u4e0b\u7684\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\n", "abstract": " \u63a2\u8ba8\u4e86\u57fa\u4e8e\u65e0\u6307\u5bfc\u5b66\u4e60\u7b56\u7565\u548c\u65e0\u8bcd\u8868\u6761\u4ef6\u4e0b\u7684\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u65b9\u6cd5,\u4ee5\u671f\u5bf9\u7814\u5236\u5f00\u653e\u73af\u5883\u4e0b\u5065\u58ee\u7684\u5206\u8bcd\u7cfb\u7edf\u6709\u6240\u88e8\u76ca.\u5168\u90e8\u5206\u8bcd\u77e5\u8bc6\u6e90\u81ea\u4ece\u751f\u8bed\u6599\u5e93\u4e2d\u81ea\u52a8\u83b7\u5f97\u7684\u6c49\u5b57Bigram.\u5728\u5b57\u95f4\u4e92\u4fe1\u606f\u548ct-\u6d4b\u8bd5\u5dee\u7684\u57fa\u7840\u4e0a,\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u4e24\u8005\u7ebf\u6027\u53e0\u52a0\u7684\u65b0\u7684\u7edf\u8ba1\u91cfmd,\u5e76\u5f15\u5165\u4e86\u5cf0\u548c\u8c37\u7684\u6982\u5ff5,\u8fdb\u800c\u8bbe\u8ba1\u4e86\u76f8\u5e94\u7684\u5206\u8bcd\u7b97\u6cd5.\u5927\u89c4\u6a21\u5f00\u653e\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a,\u8be5\u7b97\u6cd5\u5173\u4e8e\u5b57\u95f4\u4f4d\u7f6e\u7684\u5206\u8bcd\u6b63\u786e\u7387\u4e3a85.88%,\u8f83\u5355\u72ec\u4f7f\u7528\u4e92\u4fe1\u606f\u6216t-\u6d4b\u8bd5\u5dee\u5206\u522b\u63d0\u9ad8\u4e862.47%\u548c5.66%.", "num_citations": "39\n", "authors": ["1181"]}
{"title": "Stylistic chinese poetry generation via unsupervised style disentanglement\n", "abstract": " The ability to write diverse poems in different styles under the same poetic imagery is an important characteristic of human poetry writing. Most previous works on automatic Chinese poetry generation focused on improving the coherency among lines. Some work explored style transfer but suffered from expensive expert labeling of poem styles. In this paper, we target on stylistic poetry generation in a fully unsupervised manner for the first time. We propose a novel model which requires no supervised style labeling by incorporating mutual information, a concept in information theory, into modeling. Experimental results show that our model is able to generate stylistic poems without losing fluency and coherency.", "num_citations": "38\n", "authors": ["1181"]}
{"title": "Automatic poetry generation with mutual reinforcement learning\n", "abstract": " Poetry is one of the most beautiful forms of human language art. As a crucial step towards computer creativity, automatic poetry generation has drawn researchers\u2019 attention for decades. In recent years, some neural models have made remarkable progress in this task. However, they are all based on maximum likelihood estimation, which only learns common patterns of the corpus and results in loss-evaluation mismatch. Human experts evaluate poetry in terms of some specific criteria, instead of word-level likelihood. To handle this problem, we directly model the criteria and use them as explicit rewards to guide gradient update by reinforcement learning, so as to motivate the model to pursue higher scores. Besides, inspired by writing theories, we propose a novel mutual reinforcement learning schema. We simultaneously train two learners (generators) which learn not only from the teacher (rewarder) but also from each other to further improve performance. We experiment on Chinese poetry. Based on a strong basic model, our method achieves better results and outperforms the current state-of-the-art method.", "num_citations": "36\n", "authors": ["1181"]}
{"title": "Lexical Sememe Prediction via Word Embeddings and Matrix Factorization.\n", "abstract": " Sememes are defined as the minimum semantic units of human languages. People have manually annotated lexical sememes for words and form linguistic knowledge bases. However, manual construction is time-consuming and labor-intensive, with significant annotation inconsistency and noise. In this paper, we for the first time explore to automatically predict lexical sememes based on semantic meanings of words encoded by word embeddings. Moreover, we apply matrix factorization to learn semantic relations between sememes and words. In experiments, we take a real-world sememe knowledge base HowNet for training and evaluation, and the results reveal the effectiveness of our method for lexical sememe prediction. Our method will be of great use for annotation verification of existing noisy sememe knowledge bases and annotation suggestion of new words and phrases.", "num_citations": "35\n", "authors": ["1181"]}
{"title": "Automatic image annotation based on wordnet and hierarchical ensembles\n", "abstract": " Automatic image annotation concerns a process of automatically labeling image contents with a pre-defined set of keywords, which are regarded as descriptors of image high-level semantics, so as to enable semantic image retrieval via keywords. A serious problem in this task is the unsatisfactory annotation performance due to the semantic gap between the visual content and keywords. Targeting at this problem, we present a new approach that tries to incorporate lexical semantics into the image annotation process. In the phase of training, given a training set of images labeled with keywords, a basic visual vocabulary consisting of visual terms, extracted from the image to represent its content, and the associated keywords is generated at first, using K-means clustering combined with semantic constraints obtained from WordNet, then the statistical correlation between visual terms and keywords is modeled\u00a0\u2026", "num_citations": "34\n", "authors": ["1181"]}
{"title": "\u300a \u4fe1\u606f\u5904\u7406\u7528\u8bcd\u6c47\u7814\u7a76\u300b \u4e5d\u4e94\u9879\u76ee\u7ed3\u9898\u6c47\u62a5\u2014\u2014\u4fe1\u606f\u5904\u7406\u7528\u73b0\u4ee3\u6c49\u8bed\u5206\u8bcd\u8bcd\u8868\n", "abstract": " \u5efa\u7acb\u4e00\u4e2a\u201c\u4fe1\u606f\u5904\u7406\u7528\u73b0\u4ee3\u6c49\u8bed\u5206\u8bcd\u8bcd\u8868\u201d\u662f\u56fd\u5bb6\u793e\u79d1\u7814\u7a76\u201c\u4e5d\u4e94\u201d\u89c4\u5212\u91cd\u5927\u9879\u76ee\u300a\u4fe1\u606f\u5904\u7406\u7528\u73b0\u4ee3\u6c49\u8bed\u8bcd\u6c47\u7814\u7a76\u300b\u4e4b\u5b50\u8bfe\u989897@YY001\u7684\u594b\u6597\u76ee\u6807.\u672c\u5b50\u8bfe\u9898\u7acb\u9879\u7684\u57fa\u672c\u8003\u8651\u662f:\u56fd\u5bb6\u6280\u672f\u76d1\u7763\u5c40\u4e8e1993\u5e74\u53d1\u5e03\u4e86\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u56fd\u5bb6\u6807\u51c6GB/T13715-92\u300a\u4fe1\u606f\u5904\u7406\u7528\u73b0\u4ee3\u6c49\u8bed\u5206\u8bcd\u89c4\u8303\u300b.\u8fd9\u4e2a\u89c4\u8303\u5bf9\u63a8\u52a8\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u7814\u7a76\u7684\u53d1\u5c55,\u8d77\u5230\u4e86\u79ef\u6781\u7684\u4f5c\u7528.", "num_citations": "33\n", "authors": ["1181"]}
{"title": "Multi-scale Information Diffusion Prediction with Reinforced Recurrent Networks.\n", "abstract": " Information diffusion prediction is an important task which studies how information items spread among users. With the success of deep learning techniques, recurrent neural networks (RNNs) have shown their powerful capability in modeling information diffusion as sequential data. However, previous works focused on either microscopic diffusion prediction which aims at guessing the next influenced user or macroscopic diffusion prediction which estimates the total numbers of influenced users during the diffusion process. To the best of our knowledge, no previous works have suggested a unified model for both microscopic and macroscopic scales. In this paper, we propose a novel multi-scale diffusion prediction model based on reinforcement learning (RL). RL incorporates the macroscopic diffusion size information into the RNN-based microscopic diffusion model by addressing the non-differentiable problem. We also employ an effective structural context extraction strategy to utilize the underlying social graph information. Experimental results show that our proposed model outperforms state-of-the-art baseline models on both microscopic and macroscopic diffusion predictions on three real-world datasets.", "num_citations": "32\n", "authors": ["1181"]}
{"title": "Building earth mover's distance on bilingual word embeddings for machine translation\n", "abstract": " Following their monolingual counterparts, bilingual word embeddings are also on the rise. As a major application task, word translation has been relying on the nearest neighbor to connect embeddings cross-lingually. However, the nearest neighbor strategy suffers from its inherently local nature and fails to cope with variations in realistic bilingual word embeddings. Furthermore, it lacks a mechanism to deal with many-to-many mappings that often show up across languages. We introduce Earth Mover's Distance to this task by providing a natural formulation that translates words in a holistic fashion, addressing the limitations of the nearest neighbor. We further extend the formulation to a new task of identifying parallel sentences, which is useful for statistical machine translation systems, thereby expanding the application realm of bilingual word embeddings. We show encouraging performance on both tasks.", "num_citations": "32\n", "authors": ["1181"]}
{"title": "Chunk parsing scheme for Chinese sentence\n", "abstract": " CiNii \u8ad6\u6587 - Chunk parsing scheme for Chinese sentence CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831 \u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c \u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b \u306b\u3064\u3044\u3066 Chunk parsing scheme for Chinese sentence QIANG Z \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 QIANG Z \u53ce\u9332\u520a\u884c\u7269 Chinese Journal of Computers Chinese Journal of Computers 22(11), 1158-1165, 1999 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 Improving Parsing of 'BA' Sentences for Machine Translation YIN Dapeng , SHAO Min , REN Fuji , KUROIWA Shingo IEEJ transactions on electrical and electronic engineering : official journal of the Institute of Electrical Engineering of Japan / = \u96fb\u6c17\u5b66\u4f1a 3(1), 106-112, 2008-01-01 \u53c2\u8003\u6587\u732e19\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(\u2026", "num_citations": "32\n", "authors": ["1181"]}
{"title": "Chinese poetry generation with a working memory model\n", "abstract": " As an exquisite and concise literary form, poetry is a gem of human culture. Automatic poetry generation is an essential step towards computer creativity. In recent years, several neural models have been designed for this task. However, among lines of a whole poem, the coherence in meaning and topics still remains a big challenge. In this paper, inspired by the theoretical concept in cognitive psychology, we propose a novel Working Memory model for poetry generation. Different from previous methods, our model explicitly maintains topics and informative limited history in a neural memory. During the generation process, our model reads the most relevant parts from memory slots to generate the current line. After each line is generated, it writes the most salient parts of the previous line into memory slots. By dynamic manipulation of the memory, our model keeps a coherent information flow and learns to express each topic flexibly and naturally. We experiment on three different genres of Chinese poetry: quatrain, iambic and chinoiserie lyric. Both automatic and human evaluation results show that our model outperforms current state-of-the-art methods.", "num_citations": "31\n", "authors": ["1181"]}
{"title": "A high performance two-class Chinese text categorization method\n", "abstract": " BackgroundText filtering for topic-sensitive information is one of the important applications in text categorization. To effectively filter out the topic-sensitive information from Chinese text collections is a technical challenge. This paper presents a high performance method employing a two-step strategy to classify texts. In the first step, authors regard the words with parts of speech verb, noun, adjective and adverb as candidate features, perform feature selection on them in terms of the improved mutual information formula, and classify the input texts with a na? ve Bayes classifier. A portion of texts which are currently thought of being unreliable in categorization are identified, forming a fuzzy area between categories. In the second step, authors regard the bigrams of words with parts of speech verb and noun as candidate features, use the same feature selection and classifier to deal with the texts in the fuzzy area. The\u00a0\u2026", "num_citations": "31\n", "authors": ["1181"]}
{"title": "\u5229\u7528\u4e0a\u4e0b\u6587\u4fe1\u606f\u89e3\u51b3\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u4e2d\u7684\u7ec4\u5408\u578b\u6b67\u4e49\n", "abstract": " \u7ec4\u5408\u578b\u6b67\u4e49\u5207\u5206\u5b57\u6bb5\u4e00\u76f4\u662f\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u7814\u7a76\u4e2d\u7684\u4e00\u4e2a\u96be\u70b9.\u8be5\u6587\u5c06\u4e4b\u89c6\u4e3a\u4e0eWord Sense Disambiguation(WSD)\u76f8\u7b49\u4ef7\u7684\u95ee\u9898.\u6587\u7ae0\u501f\u9274\u4e86WSD\u7814\u7a76\u4e86\u5e7f\u6cdb\u4f7f\u7528\u7684\u5411\u91cf\u7a7a\u95f4\u6cd5,\u9009\u53d6\u4e8620\u4e2a\u5178\u578b\u7684\u7ec4\u5408\u578b\u6b67\u4e49\u8fdb\u884c\u4e86\u8be6\u5c3d\u8ba8\u8bba.\u63d0\u51fa\u4e86\u6839\u636e\u5b83\u4eec\u7684\u5206\u5e03\u201c\u5206\u800c\u6cbb\u4e4b\u201d\u7684\u7b56\u7565,\u7ee7\u800c\u6839\u636e\u5b9e\u9a8c  \u5b9a\u4e86\u4e0e\u7279\u5f81\u77e9\u9635\u76f8\u5173\u8054\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u5927\u5c0f,\u7a97\u53e3\u4f4d\u7f6e\u533a\u5206,\u6743\u503c\u4f30\u8ba1\u7b49\u8981\u7d20,\u5e76\u4e14\u9488\u5bf9\u6570\u636e\u7a00\u758f\u95ee\u9898,\u5229\u7528\u8bcd\u7684\u8bed\u4e49\u4ee3\u7801\u4fe1\u606f\u5bf9\u5f81\u77e9\u9635\u8fdb\u884c\u4e86\u964d\u7ef4\u5904\u7406,\u53d6\u5f97\u4e86\u8f83\u597d\u7684\u6548\u679c.\u7b14\u8005\u76f8\u4fe1,\u8fd9\u4e2a\u6a21\u578b\u5bf9\u7ec4\u5408\u578b\u6b67\u4e49\u5207\u5206\u5b57\u6bb5\u7684\u6392\u6b67\u5177\u6709\u4e00\u822c\u6027.", "num_citations": "31\n", "authors": ["1181"]}
{"title": "Community-enhanced network representation learning for network analysis\n", "abstract": " Network representation learning (NRL) aims to build low-dimensional vectors for vertices in a network. Most existing NRL methods focus on learning representations from local context of vertices (such as their neighbors). Nevertheless, vertices in many complex networks also exhibit significant global patterns widely known as communities. It\u2019s a common sense that vertices in the same community tend to connect densely, and usually share common attributes. These patterns are expected to improve NRL and benefit relevant evaluation tasks, such as link prediction and vertex classification. In this work, we propose a novel NRL model by introducing community information of vertices to learn more discriminative network representations, named as Community-enhanced Network Representation Learning (CNRL). CNRL simultaneously detects community distribution of each vertex and learns embeddings of both vertices and communities. In this way, we can obtain more informative representation of a vertex accompanying with its community information. In experiments, we evaluate the proposed CNRL model on vertex classification, link prediction, and community detection using several real-world datasets. The results demonstrate that CNRL significantly and consistently outperforms other state-of-the-art methods. Meanwhile, the detected meaningful communities verify our assumptions on the correlations among vertices, sequences, and communities.", "num_citations": "30\n", "authors": ["1181"]}
{"title": "A comparison and semi-quantitative analysis of words and character-bigrams as features in chinese text categorization\n", "abstract": " Words and character-bigrams are both used as features in Chinese text processing tasks, but no systematic comparison or analysis of their values as features for Chinese text categorization has been reported heretofore. We carry out here a full performance comparison between them by experiments on various document collections (including a manually word-segmented corpus as a golden standard), and a semi-quantitative analysis to elucidate the characteristics of their behavior; and try to provide some preliminary clue for feature term choice (in most cases, character-bigrams are better than words) and dimensionality setting in text categorization systems.", "num_citations": "30\n", "authors": ["1181"]}
{"title": "Neural machine translation with pivot languages\n", "abstract": " While recent neural machine translation approaches have delivered state-of-the-art performance for resource-rich language pairs, they suffer from the data scarcity problem for resource-scarce language pairs. Although this problem can be alleviated by exploiting a pivot language to bridge the source and target languages, the source-to-pivot and pivot-to-target translation models are usually independently trained. In this work, we introduce a joint training algorithm for pivot-based neural machine translation. We propose three methods to connect the two models and enable them to interact with each other during training. Experiments on Europarl and WMT corpora show that joint training of source-to-pivot and pivot-to-target models leads to significant improvements over independent training across various languages.", "num_citations": "29\n", "authors": ["1181"]}
{"title": "Semi-supervised learning for image annotation based on conditional random fields\n", "abstract": " Automatic image annotation (AIA) has been proved to be an effective and promising solution to automatically deduce the high-level semantics from low-level visual features. Due to the inherent ambiguity of image-label mapping and the scarcity of training examples, it has become a challenge to systematically develop robust annotation models with better performance. In this paper, we try to attack the problem based on 2D CRFs (Conditional Random Fields) and semi-supervised learning which are seamlessly integrated into a unified framework. 2D CRFs can effectively capture the spatial dependency between the neighboring labels, while the semi-supervised learning techniques can exploit the unlabeled data to improve the joint classification performance. We conducted experiments on a medium-sized image collection including about 500 images from Corel Stock Photo CDs. The experimental results\u00a0\u2026", "num_citations": "29\n", "authors": ["1181"]}
{"title": "Next: Nus-tsinghua center for extreme search of user-generated content\n", "abstract": " The Web has revolutionized the way we create, disseminate, and consume information. Users have changed from passive recipients of information to active content consumers and creators, and the nature of information has also changed from static text to dynamic multimedia. With the widespread use of social networks, live user-generated content (UGC) has begun to dominate the Internet. Such UGC covers a range of media, from text (tweets, forums, and Facebook messages) to images (Instagram and Flickr), videos (YouTube), location check-ins (Foursquare), and community question-and-answer forums (Yahoo!Answers and WikiAnswers). The NUS-Tsinghua Center for Extreme Search (or NExT Center) is collaboration between the National University of Singapore (NUS) and Tsinghua University that focuses on the novel, challenging task of analyzing and organizing UGC to make it available for general access\u00a0\u2026", "num_citations": "28\n", "authors": ["1181"]}
{"title": "Chinese word extraction based on the internal associative strength of character strings\n", "abstract": " Word extraction is one of the important tasks in text information processing. A conventional scheme for word extraction is to estimate the soundness of a candidate character string being a word by the internal associative strength among characters involved. In this paper, the authors at first test the performance of nine widely adopted statistical measures of such kind in Chinese word extraction on the individual basis, then try the possibility of improving the performance by properly combining these measures. Genetic algorithm is explored to automatically adjust the weighting of combination. Experiments focusing on two-character Chinese word extraction show that mutual information is most powerful in these measures, achieving the F-measure 54 77%, and the effectiveness of combination is not significant, only achieving the F-measure 55 47%. This suggests that these measures could not supplement well each other, and the simplest and effective way in Chinese word extraction would be using mutual information directly.", "num_citations": "28\n", "authors": ["1181"]}
{"title": "Monte Carlo methods for maximum margin supervised topic models\n", "abstract": " An effective strategy to exploit the supervising side information for discovering predictive topic representations is to impose discriminative constraints induced by such information on the posterior distributions under a topic model. This strategy has been adopted by a number of supervised topic models, such as MedLDA, which employs max-margin posterior constraints. However, unlike the likelihoodbased supervised topic models, of which posterior inference can be carried out using the Bayes\u2019 rule, the max-margin posterior constraints have made Monte Carlo methods infeasible or at least not directly applicable, thereby limited the choice of inference algorithms to be based on variational approximation with strict mean field assumptions. In this paper, we develop two efficient Monte Carlo methods under much weaker assumptions for max-margin supervised topic models based on an importance sampler and a collapsed Gibbs sampler, respectively, in a convex dual formulation. We report thorough experimental results that compare our approach favorably against existing alternatives in both accuracy and efficiency", "num_citations": "27\n", "authors": ["1181"]}
{"title": "Semi-supervised simhash for efficient document similarity search\n", "abstract": " Searching documents that are similar to a query document is an important component in modern information retrieval. Some existing hashing methods can be used for efficient document similarity search. However, unsupervised hashing methods cannot incorporate prior knowledge for better hashing. Although some supervised hashing methods can derive effective hash functions from prior knowledge, they are either computationally expensive or poorly discriminative. This paper proposes a novel (semi-) supervised hashing method named Semi-Supervised SimHash (S3H) for high-dimensional data similarity search. The basic idea of S3H is to learn the optimal feature weights from prior knowledge to relocate the data such that similar data have similar hash codes. We evaluate our method with several state-of-the-art methods on two large datasets. All the results show that our method gets the best performance.", "num_citations": "26\n", "authors": ["1181"]}
{"title": "Reasoning algorithm in multi-value causality diagram\n", "abstract": " BackgroundThe multi-value causality diagram developed on the belief network does not satisfy probability theory rigorous, and the inference result may be error when it is used in practice. In order to overcome these difficulties, this paper presents a reasoning algorithm based on possibility allocation. The reasoning process is separates into 3 stages. Firstly, the multi-value causality diagram is supplementally defined. It is compatible with a single-value causality diagram. Secondly, it transforms a multi-value causality diagram to a single-value causality diagram that is used to compute the probability; Thirdly, it allocate the probability to every state according to its possibility value that is computed in multi-value causality diagram. An example about fault diagnosis of a steam generator in the nuclear power plant demonstrates that this algorithm could overcome efficiently the difficulties in multi-value diagram, the\u00a0\u2026", "num_citations": "26\n", "authors": ["1181"]}
{"title": "Ambiguity resolution in Chinese word segmentation\n", "abstract": " A new method for Chinese word segmentation named Conditional F&BMM (Forward and Backward Maximal Matching) which incorporates both bigram statistics (ie, mutual information and difference of t-test between Chinese characters) and linguistic rules for ambiguity resolution is proposed in this paper. The key characteristics of this model are the use of:(i) statistics which can be automatically derived from any raw corpus,(ii) a rule base for disambiguation with consistency and controlled size to be built up in a systematic way.", "num_citations": "26\n", "authors": ["1181"]}
{"title": "Bilingual lexicon induction from non-parallel data with minimal supervision\n", "abstract": " Building bilingual lexica from non-parallel data is a long-standing natural language processing research problem that could benefit thousands of resource-scarce languages which lack parallel data. Recent advances of continuous word representations have opened up new possibilities for this task, eg by establishing cross-lingual mapping between word embeddings via a seed lexicon. The method is however unreliable when there are only a limited number of seeds, which is a reasonable setting for resource-scarce languages. We tackle the limitation by introducing a novel matching mechanism into bilingual word representation learning. It captures extra translation pairs exposed by the seeds to incrementally improve the bilingual word embeddings. In our experiments, we find the matching mechanism to substantially improve the quality of the bilingual vector space, which in turn allows us to induce better bilingual lexica with seeds as few as 10.", "num_citations": "25\n", "authors": ["1181"]}
{"title": "A logic for reasoning about game strategies\n", "abstract": " This paper introduces a modal logic for reasoning about game strategies. The logic is based on a variant of the well-known game description language for describing game rules and further extends it with two modalities for reasoning about actions and strategies. We develop an axiomatic system and prove its soundness and completeness with respect to a specific semantics based on the state transition model of games. Interestingly, the completeness proof makes use of forgetting techniques that have been widely used in the KR&R literature. We demonstrate how general game-playing systems can apply the logic to develop game strategies.", "num_citations": "25\n", "authors": ["1181"]}
{"title": "Chinese word cooccurrence network: its small world effect and scale-free property\n", "abstract": " Some perspectives of human languages can be characterized by complex network analysis. In this paper, word co-occurrence networks for the Chinese language are automatically constructed based on very large manually word-segmented Chinese corpora with different size and style at first. Then systematic observations on these networks are made from the complex network's point of view. Experimental results show that these networks display two important features of complex networks:(1) The average distance between two words is 2.63-2.75, and the clustering coefficient is much greater than that given by a random network with the same parameters, which exhibits a typical small-world effect; and (2) The degree distributions of these networks generally obey the power-law, ie, the scale-free property. In addition, quantitative analysis is conducted for the kernel lexicons derived from these experiments.", "num_citations": "25\n", "authors": ["1181"]}
{"title": "\u4ece\u6280\u672f\u548c\u7814\u7a76\u89d2\u5ea6\u770b MOOC\n", "abstract": " \u4ece\u6559\u80b2\u6280\u672f\u7684\u89d2\u5ea6\u9610\u8ff0MOOC\u5728\u57fa\u672c\u6559\u5b66\u5355\u5143\u6a21\u5f0f,\u5373\u65f6\u53cd\u9988,\u4e2a\u6027\u5316\u670d\u52a1,\u793e\u4ea4\u4e92\u52a8,\u7c7b\u7ebf\u4e0b\u8bfe\u5802\u4f53\u9a8c\u7b495\u4e2a\u65b9\u9762\u7684\u663e\u8457\u7279\u70b9,\u4ecb\u7ecd\u4e86\u6b63\u5728\u5174\u8d77\u7684\u9488\u5bf9MOOC\u7684\u7814\u7a76\u5de5\u4f5c.", "num_citations": "24\n", "authors": ["1181"]}
{"title": "Chime: An efficient error-tolerant chinese pinyin input method\n", "abstract": " Chinese Pinyin input methods are very important for Chinese language processing. In many cases, users may make typing errors. For example, a user wants to type in \u201cshenme\u201d(, meaning \u201cwhat\u201d in English) but may type in \u201cshenem\u201d instead. Existing Pinyin input methods fail in converting such a Pinyin sequence with errors to the right Chinese words. To solve this problem, we developed an efficient error-tolerant Pinyin input method called \u201cCHIME\u201d that can handle typing errors. By incorporating state-of-the-art techniques and languagespecific features, the method achieves a better performance than state-of-the-art input methods. It can efficiently find relevant words in milliseconds for an input Pinyin sequence.", "num_citations": "24\n", "authors": ["1181"]}
{"title": "\u6d88\u89e3\u4e2d\u6587\u4e09\u5b57\u957f\u4ea4\u96c6\u578b\u5206\u8bcd\u6b67\u4e49\u7684\u7b97\u6cd5\n", "abstract": " \u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u5728\u4e2d\u6587\u4fe1\u606f\u5904\u7406\u73b0\u5b9e\u5e94\u7528\u4e2d\u5360\u636e\u7740\u5341\u5206\u91cd\u6781\u7684\u4f4d\u7f6e,\u4e09\u5b57\u957f\u4ea4\u96c6\u578b\u5206\u8bcd\u6b67\u4e49\u662f\u5206\u8bcd\u6b67\u4e49\u7684\u4e3b\u8981\u7c7b\u578b\u4e4b\u4e00,\u5728\u771f\u5b9e\u6587\u672c\u4e2d\u7684\u51fa\u73b0\u9891\u7387\u76f8\u5f53\u9ad8,\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8fd9\u79cd\u5206\u8bcd\u6b67\u4e49\u7684\u6d88\u89e3\u7b97\u6cd5,\u56de\u907f\u4e86\u8bad\u7ec3\u4ee3\u4ef7\u6bd4\u8f83\u9ad8\u6602\u7684\u8bcd\u6027\u4fe1\u606f\u800c\u4ec5\u4ec5\u5229\u7528\u8bcd\u7684\u6982\u7387\u4fe1\u606f\u53ca\u67d0\u4e9b\u5177\u6709\u7279\u5b9a\u6027\u8d28\u7684\u5e38\u7528\u5b57\u96c6\u5408,\u4ece\u4e00\u4e2a60\u4e07\u5b57\u7684\u6c49\u8bed\u8bed\u6599\u5e93\u4e2d\u62bd\u53d6\u51fa\u5168\u90e8\u4e0d\u540c\u7684\u4e09\u5b57\u957f\u4ea4\u96c6\u578b\u5206\u8bcd\u6b67\u4e49\u51715367\u4e2a\u4f5c\u4e3a\u6d4b\u8bd5\u6837\u672c,\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e,\u8be5\u7b97\u6cd5\u7684\u6d88\u89e3\u6b63\u786e\u7387\u8fbe92.07%,\u57fa\u672c", "num_citations": "24\n", "authors": ["1181"]}
{"title": "\u6587\u672c\u53ef\u89c6\u5316\u7814\u7a76\u7efc\u8ff0\n", "abstract": " \u968f\u7740\u6d77\u91cf\u6587\u672c\u7684\u6d8c\u73b0,\u4fe1\u606f\u8d85\u8f7d\u548c\u6570\u636e\u8fc7\u5269\u7b49\u95ee\u9898\u4fc3\u4f7f\u4e86\u6587\u672c\u53ef\u89c6\u5316\u6280\u672f\u7684\u51fa\u73b0.\u6587\u672c\u53ef\u89c6\u5316\u6280\u672f\u7efc\u5408\u4e86\u6587\u672c\u5206\u6790,\u6570\u636e\u6316\u6398,\u6570\u636e\u53ef\u89c6\u5316,\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66,\u4eba\u673a\u4ea4\u4e92,\u8ba4\u77e5\u79d1\u5b66\u7b49\u5b66\u79d1\u7684\u7406\u8bba\u548c\u65b9\u6cd5,\u4e3a\u4eba\u4eec\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u89e3\u590d\u6742\u6587\u672c\u7684\u5185\u5bb9,\u7ed3\u6784\u548c\u5185\u5728\u89c4\u5f8b\u7b49\u4fe1\u606f\u7684\u6709\u6548\u624b\u6bb5.\u6587\u4e2d\u9996\u5148\u9610\u8ff0\u4e86\u6587\u672c\u53ef\u89c6\u5316\u7684\u6982\u5ff5\u548c\u91cd\u8981\u6027,\u7136\u540e\u6309\u7167\u4e0d\u540c\u53ef\u89c6\u5316\u5bf9\u8c61\u7c7b\u578b\u7efc\u8ff0\u4e86\u6587\u672c\u53ef\u89c6\u5316\u7684\u7814\u7a76\u73b0\u72b6,\u5e76\u4ecb\u7ecd\u4e86\u5178\u578b\u7684\u6587\u672c\u53ef\u89c6\u5316\u65b9\u6cd5\u4e0e\u65b9\u6848;\u6700\u540e,\u5bf9\u6587\u672c\u53ef\u89c6\u5316\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u8fdb\u884c\u4e86\u5c55\u671b.", "num_citations": "23\n", "authors": ["1181"]}
{"title": "Automatic identification of Chinese maximal noun phrases\n", "abstract": " Based on the statistical characteristics of Chinese maximal noun phrases (MNPs) in a Chinese corpus with 5573 sentences, two efficient identifying algorithms for Chinese MNPs are proposed. Which are based on the boundary distribution probabilities and the internal structure rules, respectively. Experimental results show that they are of better performances, 82.3%, the identifying algorithm based on internal structure rules is up to precision of 85.4% and recall of 82.3%.", "num_citations": "22\n", "authors": ["1181"]}
{"title": "\u7f51\u7edc\u8868\u793a\u5b66\u4e60\u7efc\u8ff0\n", "abstract": " \u6458\u8981 \u7f51\u7edc\u662f\u8868\u8fbe\u7269\u4f53\u548c\u7269\u4f53\u95f4\u8054\u7cfb\u7684\u4e00\u79cd\u91cd\u8981\u5f62\u5f0f, \u9488\u5bf9\u7f51\u7edc\u7684\u5206\u6790\u7814\u7a76\u7684\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u5c31\u662f\u7814\u7a76\u5982\u4f55\u5408\u7406\u5730\u8868\u793a\u7f51\u7edc\u4e2d\u7684\u7279\u5f81\u4fe1\u606f. \u968f\u7740\u673a\u5668\u5b66\u4e60\u6280\u672f\u7684\u53d1\u5c55, \u9488\u5bf9\u7f51\u7edc\u4e2d\u8282\u70b9\u7684\u7279\u5f81\u5b66\u4e60\u6210\u4e3a\u4e86\u4e00\u9879\u65b0\u5174\u7684\u7814\u7a76\u4efb\u52a1. \u7f51\u7edc\u8868\u793a\u5b66\u4e60\u7b97\u6cd5\u5c06\u7f51\u7edc\u4fe1\u606f\u8f6c\u5316\u4e3a\u4f4e\u7ef4\u7a20\u5bc6\u7684\u5b9e\u6570\u5411\u91cf, \u5e76\u7528\u4e8e\u5df2\u6709\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u8f93\u5165. \u4e3e\u4f8b\u6765\u8bf4, \u8282\u70b9\u8868\u793a\u53ef\u4ee5\u4f5c\u4e3a\u7279\u5f81\u9001\u5165\u652f\u6301\u5411\u91cf\u673a\u7b49\u5206\u7c7b\u5668\u7528\u4e8e\u8282\u70b9\u5206\u7c7b\u4efb\u52a1, \u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u6b27\u6c0f\u7a7a\u95f4\u4e2d\u7684\u70b9\u5750\u6807\u7528\u4e8e\u53ef\u89c6\u5316\u4efb\u52a1. \u8fd1\u5e74\u6765, \u7f51\u7edc\u8868\u793a\u5b66\u4e60\u95ee\u9898\u5438\u5f15\u4e86\u5927\u91cf\u7684\u7814\u7a76\u8005\u7684\u76ee\u5149, \u672c\u6587\u5c06\u9488\u5bf9\u8fd1\u5e74\u6765\u7684\u7f51\u7edc\u8868\u793a\u5b66\u4e60\u5de5\u4f5c\u8fdb\u884c\u7cfb\u7edf\u6027\u7684\u4ecb\u7ecd\u548c\u603b\u7ed3.", "num_citations": "22\n", "authors": ["1181"]}
{"title": "Jiuge: A human-machine collaborative chinese classical poetry generation system\n", "abstract": " Research on the automatic generation of poetry, the treasure of human culture, has lasted for decades. Most existing systems, however, are merely model-oriented, which input some user-specified keywords and directly complete the generation process in one pass, with little user participation. We believe that the machine, being a collaborator or an assistant, should not replace human beings in poetic creation. Therefore, we proposed Jiuge, a human-machine collaborative Chinese classical poetry generation system. Unlike previous systems, Jiuge allows users to revise the unsatisfied parts of a generated poem draft repeatedly. According to the revision, the poem will be dynamically updated and regenerated. After the revision and modification procedure, the user can write a satisfying poem together with Jiuge system collaboratively. Besides, Jiuge can accept multi-modal inputs, such as keywords, plain text or images. By exposing the options of poetry genres, styles and revision modes, Jiuge, acting as a professional assistant, allows constant and active participation of users in poetic creation.", "num_citations": "21\n", "authors": ["1181"]}
{"title": "Using bert for word sense disambiguation\n", "abstract": " Word Sense Disambiguation (WSD), which aims to identify the correct sense of a given polyseme, is a long-standing problem in NLP. In this paper, we propose to use BERT to extract better polyseme representations for WSD and explore several ways of combining BERT and the classifier. We also utilize sense definitions to train a unified classifier for all words, which enables the model to disambiguate unseen polysemes. Experiments show that our model achieves the state-of-the-art results on the standard English All-word WSD evaluation.", "num_citations": "20\n", "authors": ["1181"]}
{"title": "Joint POS tagging and dependence parsing with transition-based neural networks\n", "abstract": " While part-of-speech (POS) tagging and dependency parsing are observed to be closely related, existing work on joint modeling with manually crafted feature templates suffers from the feature sparsity and incompleteness problems. In this paper, we propose an approach to joint POS tagging and dependency parsing using transition-based neural networks. Three neural network based classifiers are designed to resolve shift/reduce, tagging, and labeling conflicts. Experiments show that our approach significantly outperforms previous methods for joint POS tagging and dependency parsing across a variety of natural languages.", "num_citations": "20\n", "authors": ["1181"]}
{"title": "A statistical method for Uyghur tokenization\n", "abstract": " Tokenization is very important for Uyghur language processing. Tokenization of Uyghur, an agglutinative language, is quite different from other languages such as Chinese and English. In this paper we propose a two-steps statistical tokenization method for Uyghur. Two related factors, the feature template scheme and the manually tokenized corpora, are also discussed. The preliminary experiment results demonstrate that the proposed method is effective: the F-measure of tokenization reaches 88.9% in the open test.", "num_citations": "20\n", "authors": ["1181"]}
{"title": "\u56e0\u7279\u7f51\u641c\u7d22\u5f15\u64ce\u8bc4\u4ef7\u7cfb\u7edf\n", "abstract": " \u56e0\u7279\u7f51\u641c\u7d22\u5f15\u64ce\u7684\u8bc4\u4ef7\u5bf9\u56e0\u7279\u7f51\u7528\u6237\u5341\u5206\u91cd\u8981,\u4f46\u76ee\u524d\u8fd8\u7f3a\u4e4f\u5f88\u79d1\u5b66\u7684\u8bc4\u4ef7\u65b9\u6cd5.\u672c\u6587\u6839\u636e\u7cfb\u7edf\u5de5\u7a0b\u7684\u57fa\u672c\u601d\u60f3,\u5bf9\u6b64\u8fdb\u884c\u4e86\u521d\u6b65\u63a2\u8ba8,\u5e76\u4e14\u5229\u7528\u5c42\u6b21\u5206\u6790\u6cd5\u5bf9\u5404\u6307\u6807\u7684\u6743\u91cd\u8fdb\u884c\u4e86\u8ba1\u7b97,\u63d0\u51fa\u4e86\u56e0\u7279\u7f51\u641c\u7d22\u5f15\u64ce\u8bc4\u4ef7\u6307\u6807\u4f53\u7cfb,\u5efa\u7acb\u4e86\u4e00\u4e2a\u641c\u7d22\u5f15\u64ce\u8bc4\u4ef7\u6a21\u578b.", "num_citations": "19\n", "authors": ["1181"]}
{"title": "\u8c08\u8c08\u6c49\u8bed\u5206\u8bcd\u8bed\u6599\u5e93\u7684\u4e00\u81f4\u6027\u95ee\u9898\n", "abstract": " \u7ecf\u8fc7\u5206\u8bcd\u5904\u7406\u7684\u5927\u578b\u6c49\u8bed\u8bed\u6599\u5e93\u662f\u8fdb\u884c\u8bed\u8a00\u5b66\u548c\u8ba1\u7b97\u8bed\u8a00\u5b66\u7814\u7a76\u7684\u91cd\u8981\u8d44\u6e90.\u4e00\u81f4\u6027\u662f\u8861\u91cf\u5206\u8bcd\u8bed\u6599\u5e93\u8d28\u91cf\u7684\u91cd\u8981\u6807\u51c6\u4e4b\u4e00.\u672c\u6587\u5217\u4e3e\u4e86\u5bfc\u81f4\u5206\u8bcd\u8bed\u6599\u5e93\u51fa\u73b0\u4e0d\u4e00\u81f4\u7684\u4e3b\u8981\u7ed3\u6784\u7c7b\u578b,\u8ba8\u8bba\u4e86\u201c\u8bed\u6cd5\u8bcd\u201d\u4e0e\u201c\u5fc3\u7406\u8bcd\u201d\u7684\u533a\u522b,\u6307\u51fa\u5206\u8bcd\u8bed\u6599\u5e93\u4ee5\u5207\u6210\u201c\u5fc3\u7406\u8bcd\u201d\u4e3a\u5b9c.\u201c\u5fc3\u7406\u8bcd\u201d\u7684\u6a21\u7cca\u6027\u51b3\u5b9a\u4e86\u4e25\u683c\u610f\u4e49\u7684\u5b8c\u5168\u4e00\u81f4\u5bf9\u5206\u8bcd\u8bed\u6599\u5e93\u662f\u4e0d\u53ef\u80fd\u5b9e\u73b0\u7684,\u6211\u4eec\u6240\u8ffd\u6c42\u7684\u76ee\u6807\u5e94\u8c03\u6574\u4e3a\u53d7\u63a7\u6761\u4ef6\u4e0b\u7684\u4e00\u81f4\u6027.", "num_citations": "19\n", "authors": ["1181"]}
{"title": "Bandit learning with implicit feedback\n", "abstract": " Implicit feedback, such as user clicks, although abundant in online information service systems, does not provide substantial evidence on users\u2019 evaluation of system\u2019s output. Without proper modeling, such incomplete supervision inevitably misleads model estimation, especially in a bandit learning setting where the feedback is acquired on the fly. In this work, we perform contextual bandit learning with implicit feedback by modeling the feedback as a composition of user result examination and relevance judgment. Since users\u2019 examination behavior is unobserved, we introduce latent variables to model it. We perform Thompson sampling on top of variational Bayesian inference for arm selection and model update. Our upper regret bound analysis of the proposed algorithm proves its feasibility of learning from implicit feedback in a bandit setting; and extensive empirical evaluations on click logs collected from a major MOOC platform further demonstrate its learning effectiveness in practice.", "num_citations": "18\n", "authors": ["1181"]}
{"title": "\u591a\u503c\u56e0\u679c\u56fe\u7684\u63a8\u7406\u7b97\u6cd5\u7814\u7a76\n", "abstract": " \u9488\u5bf9\u591a\u503c\u56e0\u679c\u56fe\u5b58\u5728\u7684\u4e24\u4e2a\u56f0\u96be:(1)\u4e0d\u4e25\u683c\u6ee1\u8db3\u6982\u7387\u8bba;(2)\u5c06\u5176\u7528\u4e8e\u5b9e\u9645\u95ee\u9898\u65f6,\u63a8\u7406\u7ed3\u679c\u53ef\u80fd\u51fa\u73b0\u9519\u8bef,\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u5f71\u54cd\u53ef\u80fd\u6027\u5206\u914d\u7684\u63a8\u7406\u7b97\u6cd5.\u8be5\u7b97\u6cd5\u5c06\u591a\u503c\u56e0\u679c\u56fe\u7684\u63a8\u7406\u5206\u62103\u4e2a\u9636\u6bb5,\u9996\u5148\u5bf9\u591a\u503c\u56e0\u679c\u56fe\u8fdb\u884c\u8865\u5145\u5b9a\u4e49,\u4f7f\u591a\u503c\u56e0\u679c\u56fe\u80fd\u591f\u517c\u5bb9\u5355\u503c\u56e0\u679c\u56fe;\u63a5\u7740\u5c06\u591a\u503c\u56e0\u679c\u56fe\u8f6c\u5316\u4e3a\u5355\u503c\u56e0\u679c\u56fe\u8fdb\u884c\u6982\u7387\u8ba1\u7b97;\u6700\u540e\u5bf9\u591a\u503c\u56e0\u679c\u56fe\u8fdb\u884c\u53ef\u80fd\u6027\u8ba1\u7b97,\u5c06\u5355\u503c\u56e0\u679c\u56fe\u8ba1\u7b97\u5f97\u5230\u7684\u6982\u7387\u6309\u591a\u503c\u56e0\u679c\u56fe\u8ba1\u7b97\u5f97\u5230\u7684\u53ef\u80fd\u6027\u8fdb\u884c\u5206\u914d.\u4ee5\u6838\u7535\u7ad9\u4e8c\u56de\u8def\u7cfb\u7edf\u4e2d\u84b8\u6c7d\u53d1\u751f\u5668\u6545\u969c\u8bca\u65ad\u56e0\u679c\u56fe\u4e3a\u4f8b,\u5c55\u793a\u4e86\u8be5\u7b97\u6cd5\u63a8\u7406\u8ba1\u7b97\u7684\u5168\u8fc7\u7a0b.\u5b9e\u4f8b\u8868\u660e,\u8be5\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u514b\u670d\u591a\u503c\u56e0\u679c\u56fe\u5b58\u5728\u7684\u56f0\u96be,\u5176\u63a8\u7406\u8fc7\u7a0b\u4e25\u8c28,\u8ba1\u7b97\u7ed3\u679c\u7b26\u5408\u5b9e\u9645\u60c5\u51b5.\u5728\u524d\u9762\u63d0\u51fa\u7684\u63a8\u7406\u7b97\u6cd5\u57fa\u7840\u4e0a,\u9488\u5bf9\u5176\u4e0d\u80fd\u5904\u7406\u6a21\u7cca\u60c5\u51b5\u7684\u5c40\u9650\u6027,\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u7cca\u63a8\u7406\u7b97\u6cd5.\u8be5\u7b97\u6cd5\u5bf9\u591a\u503c\u56e0\u679c\u56fe\u8fdb\u884c\u4e86\u6a21\u7cca\u6269\u5c55\u5b9a\u4e49,\u5728\u8bfb\u6570\u53d8\u91cf\u548c\u4e8b\u4ef6\u53d8\u91cf\u4e4b\u95f4\u5efa\u7acb\u4e86\u7528\u4e8e\u8868\u8fbe\u6a21\u7cca\u77e5\u8bc6\u7684\u6a21\u7cca\u5bf9\u5e94\u5173\u7cfb,\u5728\u4e8b\u4ef6\u53d8\u91cf\u4e0a\u5b9a\u4e49\u4e86\u4e00\u4e2a\u7b49\u4ef7\u7684\u865a\u62df\u6a21\u7cca\u72b6\u6001,\u4f7f\u8bfb\u6570\u53d8\u91cf\u53d6\u503c\u5bf9\u5e94\u4e00\u4e2a\u6a21\u7cca\u72b6\u6001,\u628a\u8bfb\u6570\u7684\u6a21\u7cca\u63a8\u7406\u8f6c\u5316\u4e3a\u5bf9\u5e94\u6a21\u7cca\u72b6\u6001\u7684\u975e\u6a21\u7cca\u63a8\u7406.\u901a\u8fc7\u672c\u6587\u7684\u5de5\u4f5c,\u76ee\u524d\u56e0\u679c\u56fe\u5df2\u53d1\u5c55\u6210\u4e86\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u79bb\u6563\u53d8\u91cf\u548c\u8fde\u7eed\u53d8\u91cf\u7684\u6df7\u5408\u56e0\u679c\u56fe\u6a21\u578b.", "num_citations": "18\n", "authors": ["1181"]}
{"title": "Sentiment-Controllable Chinese Poetry Generation.\n", "abstract": " Expressing diverse sentiments is one of the main purposes of human poetry creation. Existing Chinese poetry generation models have made great progress in poetry quality, but they all neglected to endow generated poems with specific sentiments. Such defect leads to strong sentiment collapse or bias and thus hurts the diversity and semantics of generated poems. Meanwhile, there are few sentimental Chinese poetry resources for studying. To address this problem, we first collect a manually-labelled sentimental poetry corpus with fine-grained sentiment labels. Then we propose a novel semi-supervised conditional Variational Auto-Encoder model for sentimentcontrollable poetry generation. Besides, since poetry is discourse-level text where the polarity and intensity of sentiment could transfer among lines, we incorporate a temporal module to capture sentiment transition patterns among different lines. Experimental results show our model can control the sentiment of not only a whole poem but also each line, and improve the poetry diversity against the state-of-the-art models without losing quality.", "num_citations": "17\n", "authors": ["1181"]}
{"title": "Reducing word omission errors in neural machine translation: A contrastive learning approach\n", "abstract": " While neural machine translation (NMT) has achieved remarkable success, NMT systems are prone to make word omission errors. In this work, we propose a contrastive learning approach to reducing word omission errors in NMT. The basic idea is to enable the NMT model to assign a higher probability to a ground-truth translation and a lower probability to an erroneous translation, which is automatically constructed from the ground-truth translation by omitting words. We design different types of negative examples depending on the number of omitted words, word frequency, and part of speech. Experiments on Chinese-to-English, German-to-English, and Russian-to-English translation tasks show that our approach is effective in reducing word omission errors and achieves better translation performance than three baseline methods.", "num_citations": "17\n", "authors": ["1181"]}
{"title": "Improved learning of chinese word embeddings with semantic knowledge\n", "abstract": " While previous studies show that modeling the minimum meaning-bearing units (characters or morphemes) benefits learning vector representations of words, they ignore the semantic dependencies across these units when deriving word vectors. In this work, we propose to improve the learning of Chinese word embeddings by exploiting semantic knowledge. The basic idea is to take the semantic knowledge about words and their component characters into account when designing composition functions. Experiments show that our approach outperforms two strong baselines on word similarity, word analogy, and document classification tasks.", "num_citations": "17\n", "authors": ["1181"]}
{"title": "Natural language processing based on naturally annotated web resources\n", "abstract": " This article proposes an idea of\" natural language processing based on naturally annotated Web resources\". The discussion is carried out from three perspectives: the definition and types of naturally annotated resources, naturally annotated resource-based computing, as well as several key points concerned at the methodological level. A fundamental problem is presented for further exploration at last: If we could explore and integrate all the information provided by all the available naturally annotated resourcesin different respectssystematically, can themachine, as expected, ultimatelyachieve some degree of deep understanding of naturallanguage?", "num_citations": "17\n", "authors": ["1181"]}
{"title": "\u4e2d\u6587\u4fe1\u606f\u5904\u7406\u82e5\u5e72\u91cd\u8981\u95ee\u9898\n", "abstract": " \u6458 \u8981 \u673a\u5668\u7ffb\u8bd1\u7684\u7814\u7a76\u6b63\u5411\u7740\u66f4\u52a0\u52a1\u5b9e\u7684\u53d7\u9650\u9886\u57df\u65b9\u5411\u53d1\u5c55, \u5176\u7814\u7a76\u65b9\u6cd5\u4e5f\u66f4\u52a0\u4f9d\u8d56\u4e8e\u8bed\u6599\u5e93. \u4f5c\u4e3a\u8bed\u97f3\u8bc6\u522b\u5411\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5ef6\u4f38\u7684\u8bed\u97f3\u7ffb\u8bd1, \u4ece\u8bde\u751f\u8d77\u5c31\u4ee5\u9488\u5bf9\u53d7\u9650\u9886\u57df\u548c\u4ee5\u8bed\u6599\u5e93\u4e3a\u4e2d\u5fc3\u7684\u8ba1\u7b97\u4e3a\u5176\u4e3b\u8981\u7279\u5f81. \u672c\u6587\u9996\u5148\u660e\u786e\u63d0\u51fa\u4e86\u5bf9\u8bed\u97f3\u7ffb\u8bd1\u7814\u7a76\u548c\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u7684 \u201c\u53d7\u9650\u9886\u57df\u201d \u4e0b\u573a\u666f\u5f0f\u548c\u529f\u80fd\u578b\u4e24\u5927\u7279\u5f81, \u7136\u540e\u5408\u8bed\u97f3\u8bc6\u522b\u9886\u57df\u7814\u7a76\u7684\u6210\u529f\u7ecf\u9a8c, \u63d0\u51fa\u4e86\u7ffb\u8bd1\u65b9\u6cd5\u4e2d\u7edf\u8ba1\u4e0e\u89c4\u5219\u76f8\u878d\u5408\u7684\u7b56\u7565, \u5e76\u63a2\u8ba8\u4e86\u4e24\u8005 \u5408\u7684\u6846\u67b6\u548c\u53ef\u884c\u7684\u6280\u672f\u8def\u7ebf.", "num_citations": "17\n", "authors": ["1181"]}
{"title": "\u4e07\u7ef4\u7f51\u77e5\u8bc6\u6316\u6398\u65b9\u6cd5\u7684\u7814\u7a76\n", "abstract": " \u6b63 1. \u5f15\u8a00\u4e07\u7ef4\u7f51 (World Wide Web) \u7684\u51fa\u73b0\u4f7f\u8ba1\u7b97\u673a\u62e5\u6709\u6d77\u91cf\u7684\u4fe1\u606f\u8d44\u6e90, \u7136\u800c\u8fd9\u4e9b\u4fe1\u606f\u5374\u5f88\u5c11\u4ee5\u8ba1\u7b97\u673a\u53ef\u7406\u89e3\u7684\u7ed3\u6784\u5b58\u5728, \u56e0\u4e3a, \u4e07\u7ef4\u7f51\u4e0a\u7684\u9875\u9762\u672c\u6765\u5c31\u662f\u4ee5\u4eba, \u800c\u4e0d\u662f\u8ba1\u7b97\u673a\u4e3a\u5176\u9605\u8bfb\u5bf9\u8c61\u7684. \u56e0\u6b64, \u590d\u6742\u7684\u6587\u672c\u7ed3\u6784, \u56fe\u50cf, \u58f0\u97f3\u7b49\u591a\u79cd\u4fe1\u606f\u7684\u5b58\u5728, \u65e2\u628a\u4e07\u7ef4\u7f51\u53d8\u6210\u4e00\u79cd\u4e30\u5bcc\u591a\u91c7\u7684\u5a92\u4f53, \u53c8\u9020\u6210\u4e86\u8ba1\u7b97\u673a\u5bf9\u4e07\u7ef4\u7f51\u4fe1\u606f\u8fdb\u4e00\u6b65\u5904\u7406\u7684\u969c\u788d.", "num_citations": "17\n", "authors": ["1181"]}
{"title": "\u6c49\u8bed\u4e2d\u7684\u517c\u7c7b\u8bcd, \u540c\u5f62\u8bcd\u7c7b\u7ec4\u53ca\u5176\u5904\u7406\u7b56\u7565\n", "abstract": " \u672c\u6587\u4ece\u8ba1\u846c\u8bed\u8a00\u7684\u89d2\u5ea6, \u7cfb\u7edf\u5730\u603b\u7ed3 \u6c49\u8bed\u4e2d\u7684\u6b67\u4e49\u73b0\u8c61\u2014\u517c\u7c7b\u8bcd\u548c\u540c\u5f62\u8bcd\u7c7b\u7ec4, \u5bf9\u4e4b\u8fdb\u884c \u4e86\u6bd4\u8f83\u6df1 \u5165\u7684\u7814\u7a76 \u5e76\u4e14\u7ed3\u5408\u6c49\u8bed\u767d\u52a8\u53e5\u6cd5\u4eca\u6790, \u7ed9 \u51fa\u4e86\u76f8\u5e94\u7684\u5904\u7406\u7b56\u7565.\u6b67\u4e49\u662f\u8bed\u8a00\u4e2d\u5927\u91cf\u51fa\u73b0\u7684\u8bed\u8a00\u73b0\u8c61, \u6307\u51fa, \u81ea\u7136\u8bed\u8a00\u4e0e\u5176\u4ed6 \u4efb \u4f55 \u4e8c\u503c\u903b\u8f91\u901a\u8baf\u7cfb\u7edf\u7684\u6839\u672c\u533a\u522b\u5c31\u5728\u4e8e\u524d\u8005\u5177\u6709\u6b67\u4e49\u6027 \u540e\u8005\u6392\u9664\u4e86\u4e8c\u4e49, \u5e76\u4e14\u8bed\u8a00\u4e4b\u6240\u4ee5\u751f\u52a8\u591a\u91c7, \u5176\u90e8\u5206\u539f\u56e0\u4e5f\u6b63\u5728\u4e8e\u6b64. \u7136\u800c, \u6b67\u4e49\u73b0\u8c61\u540c\u65f6\u4e5f\u7ed9\u8ba1\u7b97\u673a\u5904\u7406\u81ea\u7136\u8bed \u8a00 \u8bbe \u7f6e \u4e86\u5de8\u5927\u969c\u788d, \u6210\u4e3a\u6211\u4eec\u4e0d\u80fd\u56de\u907f\u4e5f\u65e0\u6cd5\u540c\u907f\u7684\u4e00\u4e2a\u56f0\u96be\u95ee\u9898. \u8fd9\u91cc, \u8bd5\u56fe\u5c31\u73b0\u4ee3\u6c49\u8bed\u4e2d\u7684\u6b67\u4e49\u73b0\u8c61\u8fdb\u884c\u4e00\u4e9b\u8ba8\u8bba, \u7c97\u964b\u4e4b\u5904, \u656c\u7948\u8d50\u6559.", "num_citations": "17\n", "authors": ["1181"]}
{"title": "Smart jump: Automated navigation suggestion for videos in moocs\n", "abstract": " Statistics show that, on average, each user of Massive Open Online Courses (MOOCs) uses' jump-back'to navigate a course video for 2.6 times. By taking a closer look at the navigation data, we found that more than half of the jump-backs are due to the'bad'positions of the previous jump-backs. In this work, employing one of the largest Chinese MOOCs, XuetangX. com, as the source for our research, we study the extent to which we can develop a methodology to understand the user intention and help the user alleviate this problem by suggesting the best position for a jumpback. We demonstrate that it is possible to accurately predict 90% of users' jump-back intentions in the real online system. Moreover, our study reveals several interesting patterns, eg, students in nonscience courses tend to jump back from the first half of the course video, and students in science courses tend to replay for longer time.", "num_citations": "16\n", "authors": ["1181"]}
{"title": "An extended GHKM algorithm for inducing lambda-SCFG\n", "abstract": " Semantic parsing, which aims at mapping a natural language (NL) sentence into its formal meaning representation (eg, logical form), has received increasing attention in recent years. While synchronous context-free grammar (SCFG) augmented with lambda calculus (lambda-SCFG) provides an effective mechanism for semantic parsing, how to learn such lambda-SCFG rules still remains a challenge because of the difficulty in determining the correspondence between NL sentences and logical forms. To alleviate this structural divergence problem, we extend the GHKM algorithm, which is a state-of-the-art algorithm for learning synchronous grammars in statistical machine translation, to induce lambda-SCFG from pairs of NL sentences and logical forms. By treating logical forms as trees, we reformulate the theory behind GHKM that gives formal semantics to the alignment between NL words and logical form tokens. Experiments on the GEOQUERY dataset show that our semantic parser achieves an F-measure of 90.2%, the best result published to date.", "num_citations": "16\n", "authors": ["1181"]}
{"title": "\u4e2d\u56fd\u5730\u540d\u7684\u81ea\u52a8\u8fa8\u8bc6\n", "abstract": " \u4e2d\u56fd\u5730\u540d\u8fa8\u8bc6\u5bf9\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u7814\u7a76\u5177\u6709\u4e00\u5b9a\u610f\u4e49. \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4e2d\u6587\u6587\u672c\u4e2d\u81ea\u52a8\u8fa8\u8bc6\u4e2d\u56fd\u5730\u540d\u7684\u7b97\u6cd5. \u6211\u4eec\u4ece\u65b0\u534e\u901a\u8baf\u793e\u65b0\u95fb\u8bed\u6599\u5e93\u4e2d\u968f\u673a\u62bd\u53d6\u4e86 350 \u4e2a\u542b\u4e2d\u56fd\u5730\u540d\u7684\u53e5\u5b50\u4f5c\u4e3a\u6d4b\u8bd5\u6837\u672c. \u5b9e\u9a8c\u8868\u660e, \u7cbe\u786e\u7387\u8fbe\u5230 81.1%, \u53ec\u56de\u7387\u8fbe\u5230 95.0%.", "num_citations": "16\n", "authors": ["1181"]}
{"title": "Text style transfer via learning style instance supported latent space\n", "abstract": " Text style transfer pursues altering the style of a sentence while remaining its main content unchanged. Due to the lack of parallel corpora, most recent work focuses on unsupervised methods and has achieved noticeable progress. Nonetheless, the intractability of completely disentangling content from style for text leads to a contradiction of content preservation and style transfer accuracy. To address this problem, we propose a style instance supported method, StyIns. Instead of representing styles with embeddings or latent variables learned from single sentences, our model leverages the generative flow technique to extract underlying stylistic properties from multiple instances of each style, which form a more discriminative and expressive latent style space. By combining such a space with the attention-based structure, our model can better maintain the content and simultaneously achieve high transfer accuracy. Furthermore, the proposed method can be flexibly extended to semi-supervised learning so as to utilize available limited paired data. Experiments on three transfer tasks, sentiment modification, formality rephrasing, and poeticness generation, show that StyIns obtains a better balance between content and style, outperforming several recent baselines.", "num_citations": "15\n", "authors": ["1181"]}
{"title": "Query lattice for translation retrieval\n", "abstract": " Translation retrieval aims to find the most likely translation among a set of target-language strings for a given source-language string. Previous studies consider the single-best translation as a query for information retrieval, which may result in translation error propagation. To alleviate this problem, we propose to use the query lattice, which is a compact representation of exponentially many queries containing translation alternatives. We verified the effectiveness of query lattice through experiments, where our method explores a much larger search space (from 1 query to 1.24\u00d7 1062 queries), runs much faster (from 0.75 to 0.13 second per sentence), and retrieves more accurately (from 83.76% to 93.16% in precision) than the standard method based on the query single-best. In addition, we show that query lattice significantly outperforms the method of (Munteanu and Marcu, 2005) on the task of parallel sentence mining from comparable corpora.", "num_citations": "15\n", "authors": ["1181"]}
{"title": "Complex network properties of Chinese syntactic dependency network\n", "abstract": " In this paper, we build Chinese syntactic dependency network based on a large corpus and adopt complex network as the tool to analyze the language network. The network shows two important features: the small world effect and the scale-free property. The statistical properties are similar to Czech, German and Romanian which indicate that there are underlying common characteristics among different human languages in despite of their different grammar rules. The common characteristics might make sense for the study of evolution and essence of human languages.", "num_citations": "15\n", "authors": ["1181"]}
{"title": "\u6c49\u8bed\u7ec4\u5757\u7684\u5b9a\u4e49\u548c\u83b7\u53d6\n", "abstract": " \u7ec4\u5757\u662f\u4ecb\u4e8e\u8bcd\u8bed\u548c\u53e5\u5b50\u4e4b\u95f4\u7684\u4e00\u79cd\u8bed\u8a00\u7ed3\u6784, \u76ee\u524d\u8fd8\u6ca1\u6709\u660e\u786e\u7684\u5b9a\u4e49. \u672c\u6587\u603b\u7ed3\u4e86\u5f53\u524d\u5bf9\u7ec4\u5757\u7684\u5404\u79cd\u7814\u7a76, \u5bf9\u6c49\u8bed\u7ec4\u5757\u8fdb\u884c\u4e86\u5b9a\u4e49. \u540c\u65f6\u7ec4\u5757\u7684\u83b7\u53d6\u548c\u6536\u96c6\u4e5f\u662f\u4e00\u9879\u8feb\u5207\u7684\u4efb\u52a1, \u7531\u4e8e\u4e0d\u6613\u76f4\u63a5\u83b7\u53d6\u5230\u5177\u6709\u7ec4\u5757\u6807\u6ce8\u7684\u8bed\u6599, \u6211\u4eec\u4ece\u73b0\u6709\u6811\u5e93\u4e2d\u62bd\u53d6\u7ec4\u5757. \u672c\u6587\u6839\u636e\u6c49\u8bed\u7279\u70b9\u63d0\u51fa\u4e86 12 \u79cd\u6c49\u8bed\u7ec4\u5757\u7c7b\u578b, \u5e76\u6839\u636e\u8fd9\u4e9b\u7ec4\u5757\u7c7b\u578b\u548c\u5bbe\u5dde\u5927\u5b66\u4e2d\u6587\u6811\u5e93\u77ed\u8bed\u7c7b\u578b\u7684\u5bf9\u5e94\u5173\u7cfb\u8fdb\u884c\u8f6c\u5316\u83b7\u5f97\u7ec4\u5757\u5e93.", "num_citations": "15\n", "authors": ["1181"]}
{"title": "Pirhdy: Learning pitch-, rhythm-, and dynamics-aware embeddings for symbolic music\n", "abstract": " Definitive embeddings remain a fundamental challenge of computational musicology for symbolic music in deep learning today. Analogous to natural language, music can be modeled as a sequence of tokens. This motivates the majority of existing solutions to explore the utilization of word embedding models to build music embeddings. However, music differs from natural languages in two key aspects:(1) musical token is multi-faceted--it comprises of pitch, rhythm and dynamics information; and (2) musical context is two-dimensional--each musical token is dependent on both melodic and harmonic contexts. In this work, we provide a comprehensive solution by proposing a novel framework named PiRhDy that integrates pitch, rhythm, and dynamics information seamlessly. PiRhDy adopts a hierarchical strategy which can be decomposed into two steps:(1) token (ie, note event) modeling, which separately represents\u00a0\u2026", "num_citations": "14\n", "authors": ["1181"]}
{"title": "Community detection on weighted networks: A variational Bayesian method\n", "abstract": " Massive real-world data are network-structured, such as social network, relationship between proteins and power grid. Discovering the latent communities is a useful way for better understanding the property of a network. In this paper, we present a fast, effective and robust method for community detection. We extend the constrained Stochastic Block Model (conSBM) on weighted networks and use a Bayesian method for both parameter estimation and community number identification. We show how our method utilizes the weight information within the weighted networks, reduces the computation complexity to handle large-scale weighted networks, measure the estimation confidence and automatically identify the community number. We develop a variational Bayesian method for inference and parameter estimation. We demonstrate our method on a synthetic data and three real-world networks. The results\u00a0\u2026", "num_citations": "14\n", "authors": ["1181"]}
{"title": "Word segmentation standard in Chinese, Japanese and Korean\n", "abstract": " Word segmentation is a process to divide a sentence into meaningful units called \u201cword unit\u201d[ISO/DIS 24614-1]. What is a word unit is judged by principles for its internal integrity and external use constraints. A word unit\u2019s internal structure is bound by principles of lexical integrity, unpredictability and so on in order to represent one syntactically meaningful unit. Principles for external use include language economy and frequency such that word units could be registered in a lexicon or any other storage for practical reduction of processing complexity for the further syntactic processing after word segmentation. Such principles for word segmentation are applied for Chinese, Japanese and Korean, and impacts of the standard are discussed.", "num_citations": "14\n", "authors": ["1181"]}
{"title": "The role of high frequent maximal crossing ambiguities in Chinese word segmentation\n", "abstract": " CiNii \u8ad6\u6587 - The role of high frequent maximal crossing ambiguities in Chinese word segmentation CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66 \u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 The role of high frequent maximal crossing ambiguities in Chinese word segmentation SUN MS \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 SUN MS \u53ce\u9332 \u520a\u884c\u7269 Journal of Chinese Information Processing Journal of Chinese Information Processing 13(1), 27-34, 1999 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 Resolving Overlapping Ambiguities and Selecting Correct Word Sequence in Chinese Using Internet Corpus Han Dongli , Wu Haodong , Furugori Teiji \u81ea\u7136\u8a00\u8a9e\u51e6\u7406 = Journal of natural language processing 8(3), 107-121, \u2026", "num_citations": "14\n", "authors": ["1181"]}
{"title": "\u6c49\u8bed\u5206\u8bcd\u7cfb\u7edf\u4e2d\u7684\u4fe1\u606f\u96c6\u6210\u548c\u6700\u4f73\u8def\u5f84\u641c\u7d22\u65b9\u6cd5\n", "abstract": " \u590d\u6742\u7684\u6c49\u8bed\u5206\u8bcd\u7cfb\u7edf\u4e2d,\u5404\u79cd\u4fe1\u606f\u7684\u6709\u6548\u96c6\u6210\u662f\u7cfb\u7edf\u5b9e\u73b0\u7684\u5173\u952e.\u672c\u6587\u4ecb\u7ecd\u4e86\u5206\u8bcd\u7cfb\u7edfSegTag\u4e2d\u4fe1\u606f\u96c6\u6210\u65b9\u6cd5,\u5e76\u8ba8\u8bba\u4e86\u4fe1\u606f\u96c6\u6210\u7ed3\u6784\u4e2d\u7684\u4e24\u79cd\u6700\u4f73\u8def\u5f84\u641c\u7d22\u65b9\u6cd5.\u6700\u540e,\u6211\u4eec\u7ed9\u51fa\u5b9e\u9a8c\u7ed3\u679c\u548c\u7ed3\u8bba.", "num_citations": "14\n", "authors": ["1181"]}
{"title": "\u641c\u7d22\u5f15\u64ce\u4e2d\u76f8\u5173\u6027\u53cd\u9988\u6280\u672f\n", "abstract": " As an important component of search engines, the relevance feedback system is very effective for improving the performance of search engines. This paper firstly reviews the history of relevance feedback technology in the past 30 years, then introduces 2 major methods in relevance feedback, ie term reweighting and query expansion, and discusses the relevance feedback technologies based on vector space model and statistical ranking model.", "num_citations": "13\n", "authors": ["1181"]}
{"title": "Identifying Chinese Names in Unrestricted Texts [J]\n", "abstract": " he processing of Chinese names is significant to the approach of Chinese word segmentation. This paper presents an effective algorithm for automatically identifying this sort of proper nouns in Chinese texts. The testing sample, involving 300 sentences each of which contains at least one Chinese names, is extracted at random from the Xinhua News Corpus. The preliminary experiment shows that the recall rate of this algorithm reaches 99.77%.", "num_citations": "13\n", "authors": ["1181"]}
{"title": "Multi-round transfer learning for low-resource NMT using multiple high-resource languages\n", "abstract": " Neural machine translation (NMT) has made remarkable progress in recent years, but the performance of NMT suffers from a data sparsity problem since large-scale parallel corpora are only readily available for high-resource languages (HRLs). In recent days, transfer learning (TL) has been used widely in low-resource languages (LRLs) machine translation, while TL is becoming one of the vital directions for addressing the data sparsity problem in low-resource NMT. As a solution, a transfer learning method in NMT is generally obtained via initializing the low-resource model (child) with the high-resource model (parent). However, leveraging the original TL to low-resource models is neither able to make full use of highly related multiple HRLs nor to receive different parameters from the same parents. In order to exploit multiple HRLs effectively, we present a language-independent and straightforward multi-round\u00a0\u2026", "num_citations": "12\n", "authors": ["1181"]}
{"title": "A study on feature weighting in Chinese text categorization\n", "abstract": " In Text Categorization (TC) based on Vector Space Model, feature weighting and feature selection are major problems and difficulties. This paper proposes two methods of weighting features by combining the relevant influential factors together. A TC system for Chinese texts is designed in terms of character bigrams as features. Experiments on a document collection of 71,674 texts show that the F1 metric of categorization performance of the system is 85.9%, which is about 5% higher than that of the well-known TF*IDF weighting scheme. Moreover, a multi-step feature selection process is exploited to reduce the dimension of the feature space effectively in the system.", "num_citations": "12\n", "authors": ["1181"]}
{"title": "The application & implementation of local statistics in Chinese unknown word indentification\n", "abstract": " CiNii \u8ad6\u6587 - The application & implementation of local statistics in Chinese unknown word indentification CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66 \u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003 \u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 The application & implementation of local statistics in Chinese unknown word indentification SHEN Dayang \u88ab \u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 SHEN Dayang \u53ce\u9332\u520a\u884c\u7269 COLIPS COLIPS 8, 1997 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u4e2d\u56fd\u8a9e\u306e\u672a\u77e5\u8a9e\u62bd\u51fa\u306b\u304a\u3051\u308b\u5f62\u614b\u7d20\u89e3\u6790\u3068\u30c1\u30e3\u30f3\u30ad\u30f3\u30b0\u306e\u5229\u7528 \u30b4\u30fc \u30c1\u30e5\u30a4\u30ea\u30f3 , \u6d45\u539f \u6b63\u5e78 , \u677e\u672c \u88d5\u6cbb \u60c5\u5831\u51e6\u7406\u5b66\u4f1a\u7814\u7a76\u5831\u544a. NL,\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u7814\u7a76\u4f1a\u5831\u544a 155, 7-12, 2003-05-26 \u53c2\u8003\u6587\u732e 6\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10011463513 \u8cc7\u6599\u7a2e\u5225 \u96d1\u8a8c\u8ad6\u6587 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 CJP\u5f15\u7528 \u2026", "num_citations": "12\n", "authors": ["1181"]}
{"title": "Mixpoet: Diverse poetry generation via learning controllable mixed latent space\n", "abstract": " As an essential step towards computer creativity, automatic poetry generation has gained increasing attention these years. Though recent neural models make prominent progress in some criteria of poetry quality, generated poems still suffer from the problem of poor diversity. Related literature researches show that different factors, such as life experience, historical background, etc., would influence composition styles of poets, which considerably contributes to the high diversity of human-authored poetry. Inspired by this, we propose MixPoet, a novel model that absorbs multiple factors to create various styles and promote diversity. Based on a semi-supervised variational autoencoder, our model disentangles the latent space into some subspaces, with each conditioned on one influence factor by adversarial training. In this way, the model learns a controllable latent variable to capture and mix generalized factor-related properties. Different factor mixtures lead to diverse styles and hence further differentiate generated poems from each other. Experiment results on Chinese poetry demonstrate that MixPoet improves both diversity and quality against three state-of-the-art models.", "num_citations": "11\n", "authors": ["1181"]}
{"title": "Inducing bilingual lexica from non-parallel data with earth mover\u2019s distance regularization\n", "abstract": " Being able to induce word translations from non-parallel data is often a prerequisite for cross-lingual processing in resource-scarce languages and domains. Previous endeavors typically simplify this task by imposing the one-to-one translation assumption, which is too strong to hold for natural languages. We remove this constraint by introducing the Earth Mover\u2019s Distance into the training of bilingual word embeddings. In this way, we take advantage of its capability to handle multiple alternative word translations in a natural form of regularization. Our approach shows significant and consistent improvements across four language pairs. We also demonstrate that our approach is particularly preferable in resource-scarce settings as it only requires a minimal seed lexicon.", "num_citations": "11\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e\u4e92\u8054\u7f51\u81ea\u7136\u6807\u6ce8\u8d44\u6e90\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\n", "abstract": " \u8be5\u6587\u63d0\u51fa\u4e86 \u201c\u57fa\u4e8e\u4e92\u8054\u7f51\u81ea\u7136\u6807\u6ce8\u8d44\u6e90\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u201d \u7684\u5b66\u672f\u601d\u60f3, \u5e76\u4ece\u81ea\u7136\u6807\u6ce8\u8d44\u6e90\u7684\u5b9a\u4e49\u548c\u57fa\u672c\u7c7b\u578b, \u57fa\u4e8e\u81ea\u7136\u6807\u6ce8\u8d44\u6e90\u7684\u8ba1\u7b97, \u65b9\u6cd5\u8bba\u5c42\u9762\u4e0a\u7684\u521d\u6b65\u601d\u8003\u7b49\u4e09\u4e2a\u89d2\u5ea6\u5bf9\u8fd9\u4e00\u5b66\u672f\u601d\u60f3\u8fdb\u884c\u4e86\u521d\u6b65\u7684\u9610\u53d1. \u6700\u540e\u6307\u51fa\u4e86\u5176\u4e2d\u7684\u4e00\u4e2a\u57fa\u7840\u95ee\u9898: \u5982\u679c\u6211\u4eec\u628a\u5168\u90e8\u81ea\u7136\u6807\u6ce8\u8d44\u6e90\u6240\u80fd\u63d0\u4f9b\u7684\u5168\u90e8\u4fe1\u606f\u6216\u77e5\u8bc6\u90fd\u4ee5\u4e00\u79cd\u7cfb\u7edf\u7684\u65b9\u5f0f\u7528\u5230\u4e86\u6781\u81f4, \u5e76\u4e14\u628a\u5b83\u4eec\u6700\u5927\u9650\u5ea6\u5730\u6709\u673a\u96c6\u6210\u8d77\u6765, \u673a\u5668\u80fd\u5426\u5982\u613f\u4ee5\u507f\u5730\u83b7\u5f97\u5bf9\u81ea\u7136\u8bed\u8a00\u4e00\u5b9a\u6df1\u5ea6\u7684\u7406\u89e3\u5462?", "num_citations": "11\n", "authors": ["1181"]}
{"title": "A reasoning algorithm of applying causality diagram to fault diagnosis of complex hybrid systems\n", "abstract": " Fault diagnosis of a complex hybrid system, which bears causality loops, non-deterministic system dynamics, presence of not-directly-observable system states, delay of fault effects, and the mixture of discrete and continuous variables in the observed signals, is a very challenging task. This paper presents a novel method based on the causality diagram model for this problem. The method extends the causality diagram to the 2-time-slice causality diagram (2-TSCD), which represents both the fault propagation and the conditional probability distribution of system states in two consecutive time-slices. In the 2-TSCD, basic event variables represent failures; node event variables represent system states at time-slice t+l; some system states at time-slice t are introduced as basic event variables to represent the delay of fault effects. We develop a reasoning algorithm for the 2-TSCD: firstly, the candidate fault modes at time\u00a0\u2026", "num_citations": "11\n", "authors": ["1181"]}
{"title": "\u5173\u4e8e\u8bcd\u6c47\u4f7f\u7528\u5ea6\u7684\u521d\u6b65\u7814\u7a76\n", "abstract": " \u5173\u4e8e\u8bcd\u6c47\u4f7f\u7528\u5ea6\u7684\u521d\u6b65\u7814\u7a76-\u3010\u7ef4\u666e\u7f51\u3011-\u4ed3\u50a8\u5f0f\u5728\u7ebf\u4f5c\u54c1\u51fa\u7248\u5e73\u53f0-www.cqvip.com \ufeff \u6211\u7684\u7ef4\u666e \u5e10\u6237 \u4f59\u989d \u5145\u503c\u8bb0\u5f55 \u4e0b\u8f7d\u8bb0\u5f55 \u6211\u7684\u6536\u85cf \u8d2d\u7269\u8f66 \u5145\u503c \u5ba2\u670d \u9996\u9875 | \u671f\u520a\u5927\u5168 | \u8bba\u6587\u9009\u9898 | \u8bba\u6587\u68c0\u6d4b | \u5728\u7ebf \u51fa\u7248 | \u4f18\u5148\u51fa\u7248 | \u7ef4\u666e\u5b98\u65b9\u5929\u732b\u5e97 \u60a8\u7684\u4f4d\u7f6e\uff1a\u7f51\u7ad9\u9996\u9875 > \u300a\u4e2d\u6587\u79d1\u6280\u671f\u520a\u6570\u636e\u5e93\u300b > \u4eba\u6587\u793e\u79d1 > \u8bed\u8a00 > \u8bed\u8a00\u5b66 > \u6458\u8981 \u5173\u4e8e\u8bcd\u6c47\u4f7f\u7528\u5ea6\u7684\u521d\u6b65\u7814\u7a76 \u300a\u8bed\u8a00\u6587\u5b57\u5e94\u7528\u300b2000\u5e74\u7b2c1\u671f | \u5b59\u8302\u677e \u8d2d\u7269\u8f66 | \u2605 \u6536\u85cf | \u5206\u4eab \u8bba\u6587\u670d\u52a1\uff1a \u6458\u8981\uff1a \u3010\u5206\u7c7b\u3011 \u3010\u8bed\u8a00\u3001\u6587\u5b57\u3011 > \u8bed\u8a00\u5b66 > \u5e94\u7528\u8bed\u8a00\u5b66 > \u6570\u7406\u8bed\u8a00\u5b66 \u3010\u5173\u952e\u8bcd\u3011 \u8bcd\u6c47 \u4f7f\u7528\u5ea6 \u4fe1\u606f\u5904\u7406 \u6c49\u8bed \u3010\u51fa\u5904\u3011 \u300a\u8bed\u8a00\u6587\u5b57\u5e94\u7528\u300b2000\u5e74\u7b2c1\u671f 6-10\u9875\u51715\u9875 \u3010\u6536\u5f55\u3011 \u4e2d\u6587\u79d1\u6280\u671f\u520a\u6570\u636e\u5e93 \u4f18\u8d28\u671f\u520a\u63a8\u8350 \u4e2d\u56fd\u7ffb\u8bd1 \u4e2d\u56fd\u7ffb\u8bd1 \u4e2d\u56fd\u7ffb\u8bd1\u5de5\u4f5c\u8005\u534f\u4f1a\u4f1a\u520a,\u8bd1\u754c\u9ad8\u6c34\u5e73\u5b66\u672f \u520a\u7269\u3002...\u8be6\u7ec6 \u540c\u671f\u6587\u732e \u8bed\u8a00\u6587\u5b57\u5e94\u7528 \u300a\u8bed\u8a00\u6587\u5b57\u5e94\u7528\u300b Applied Linguistics 2000\u5e74\u7b2c1\u671f \u671f\u520a\u8be6\u7ec6 \u4fe1\u606f \u8de8\u8d8a\u9e3f\u6c9f\u2014\u2014\u5bfb\u627e\u8bed\u4fdd\u6700\u6709\u6548\u7684\u65b9\u5f0f \u8bed\u8a00\u8d44\u6e90\u4fdd\u62a4\u4e0e\u5f71\u89c6\u5178\u85cf \u65b0\u5a92\u4f53\u5728\u8bed\u8a00\u6587\u5316\u4f20\u627f\u4e2d\u7684 \u5e94\u7528 \u4e5f\u8bba\u201c\u65b9\u8a00\u6587\u5316\u8fdb\u8bfe\u5802\u201d \u6784\u5f0f\u7684\u8fd8\u662f\u52a8\u8bcd\u7684\uff1f\u2014\u2014\u6c49\u8bed\u53cc\u5bbe\u53e5\u52a0\u5de5\u7684\u795e\u7ecf... \u6c49\u8bed\u540e\u7eed\u53e5\u4e2d\u7701\u7565\u2026", "num_citations": "11\n", "authors": ["1181"]}
{"title": "THUMT: An Open-Source Toolkit for Neural Machine Translation\n", "abstract": " THUMT is an open-source toolkit for neural machine translation (NMT) developed by the Natural Language Processing Group at Tsinghua University. The toolkit is easy to use, modify and extend while provides the latest advances in NMT research and production. THUMT implements several standard NMT models and supports distributed training across multiple machines, fast inference, and model visualization. Experiments on English-German and Chinese-English datasets show that THUMT can obtain results that are comparable to state-of-the-art NMT systems.", "num_citations": "10\n", "authors": ["1181"]}
{"title": "Neural machine translation: A review of methods, resources, and tools\n", "abstract": " Machine translation (MT) is an important sub-field of natural language processing that aims to translate natural languages using computers. In recent years, end-to-end neural machine translation (NMT) has achieved great success and has become the new mainstream method in practical MT systems. In this article, we first provide a broad review of the methods for NMT and focus on methods relating to architectures, decoding, and data augmentation. Then we summarize the resources and tools that are useful for researchers. Finally, we conclude with a discussion of possible future research directions.", "num_citations": "10\n", "authors": ["1181"]}
{"title": "Chinese poetry generation with a salient-clue mechanism\n", "abstract": " As a precious part of the human cultural heritage, Chinese poetry has influenced people for generations. Automatic poetry composition is a challenge for AI. In recent years, significant progress has been made in this area benefiting from the development of neural networks. However, the coherence in meaning, theme or even artistic conception for a generated poem as a whole still remains a big problem. In this paper, we propose a novel Salient-Clue mechanism for Chinese poetry generation. Different from previous work which tried to exploit all the context information, our model selects the most salient characters automatically from each so-far generated line to gradually form a salient clue, which is utilized to guide successive poem generation process so as to eliminate interruptions and improve coherence. Besides, our model can be flexibly extended to control the generated poem in different aspects, for example, poetry style, which further enhances the coherence. Experimental results show that our model is very effective, outperforming three strong baselines.", "num_citations": "10\n", "authors": ["1181"]}
{"title": "A multi-answer multi-task framework for real-world machine reading comprehension\n", "abstract": " The task of machine reading comprehension (MRC) has evolved from answering simple questions from well-edited text to answering real questions from users out of web data. In the real-world setting, full-body text from multiple relevant documents in the top search results are provided as context for questions from user queries, including not only questions with a single, short, and factual answer, but also questions about reasons, procedures, and opinions. In this case, multiple answers could be equally valid for a single question and each answer may occur multiple times in the context, which should be taken into consideration when we build MRC system. We propose a multi-answer multi-task framework, in which different loss functions are used for multiple reference answers. Minimum Risk Training is applied to solve the multi-occurrence problem of a single answer. Combined with a simple heuristic passage extraction strategy for overlong documents, our model increases the ROUGE-L score on the DuReader dataset from 44.18, the previous state-of-the-art, to 51.09.", "num_citations": "10\n", "authors": ["1181"]}
{"title": "MOOC: \u592a\u9633\u7167\u5e38\u5347\u8d77, \u5883\u754c\u5df2\u7136\u4e0d\u540c\n", "abstract": " \u6b63 MOOC \u662f\u4e00\u79cd\u91cd\u8981\u7684\u73b0\u4ee3\u6559\u80b2\u6280\u672f\u624b\u6bb5\u548c\u521b\u65b0\u5e73\u53f0, \u5b83\u4f1a\u653e\u5927, \u4f18\u5316\u4f20\u7edf\u8bfe\u5802, \u4f46\u5728\u53ef\u9884\u671f\u7684\u672a\u6765\u7edd\u4e0d\u4f1a\u53d6\u4ee3\u4f20\u7edf\u8bfe\u5802. MOOC \u4f1a\u5bf9\u5e73\u5eb8\u7684\u5927\u5b66\u5f62\u6210\u5de8\u5927\u7684\u538b\u529b, \u751a\u81f3\u662f\u751f\u5b58\u538b\u529b, \u4f46\u62d2\u7edd\u5e73\u5eb8, \u987a\u53d8\u6c42\u65b0\u7684\u4f20\u7edf\u610f\u4e49\u4e0a\u7684\u5927\u5b66\u4f9d\u7136\u4f1a\u975e\u5e38\u597d\u5730\u8fd0\u4f5c. \u5728\u9ad8\u7b49\u6559\u80b2\u7684\u5929\u7a7a\u4e2d, \u592a\u9633\u4f1a\u7167\u5e38\u5347\u8d77, \u4f46\u662f\u5b83\u5347\u8d77\u7684\u65f6\u5019, \u5883\u754c\u56e0 MOOC \u53ef\u80fd\u5df2\u7ecf\u5168\u7136\u4e0d\u540c.", "num_citations": "10\n", "authors": ["1181"]}
{"title": "Word segmentation on Chinese mirco-blog data with a linear-time incremental model\n", "abstract": " This paper describes the model we designed for the word segmentation bakeoff on Chinese micro-blog data in the 2nd CIPS-SIGHAN joint conference on Chinese language processing. We presented a linear-time incremental model for word segmentation where rich features including character-based features, word-based features as well as other possible features can be easily employed. We report the performances of our model on four datasets in the SIGHAN bake-off 2005. After adding more features designed for the micro-blog data, the performance of our model is further improved. The F-score of our model for this bake-off is 0.9478 and 44.88% of the sentences are segmented correctly.", "num_citations": "10\n", "authors": ["1181"]}
{"title": "Chinese new word detection from query logs\n", "abstract": " Existing works in literature mostly resort to the web pages or other author-centric resources to detect new words, which require highly complex text processing. This paper exploits the visitor-centric resources, specifically, query logs from the commercial search engine, to detect new words. Since query logs are generated by the search engine users, and are segmented naturally, the complex text processing work can be avoided. By dynamic time warping, a new word detection algorithm based on the trajectory similarity is proposed to distinguish new words from the query logs. Experiments based on real world data sets show the effectiveness and efficiency of the proposed algorithm.", "num_citations": "10\n", "authors": ["1181"]}
{"title": "Statistical properties of overlapping ambiguities in Chinese word segmentation and a strategy for their disambiguation\n", "abstract": " Overlapping ambiguity is a major ambiguity type in Chinese word segmentation. In this paper, the statistical properties of overlapping ambiguities are intensively studied based on the observations from a very large balanced general-purpose Chinese corpus. The relevant statistics are given from different perspectives. The stability of high frequent maximal overlapping ambiguities is tested based on statistical observations from both general-purpose corpus and domain-specific corpora. A disambiguation strategy for overlapping ambiguities, with a predefined solution for each of the 5,507 pseudo overlapping ambiguities, is proposed consequently, suggesting that over 42% of overlapping ambiguities in Chinese running text could be solved without making any error. Several state-of-the-art word segmenters are used to make comparisons on solving these overlapping ambiguities. Preliminary experiments\u00a0\u2026", "num_citations": "10\n", "authors": ["1181"]}
{"title": "A new transductive support vector machine approach to text categorization\n", "abstract": " Transductive inference is well suited for text categorization tasks with an enormous amount of unlabeled data in addition to a small number of labeled data. We present a new transductive support vector machine approach for text categorization in order to make use of the large amount of unlabeled data efficiently. In our experiments the performance of transductive methods greatly exceeds that of conventional SVM. Experimental results also show that our algorithm outperforms Joachims' TSVM, especially when there is a significant deviation between the distribution of training and test data.", "num_citations": "10\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e k-\u8fd1\u4f3c\u7684\u6c49\u8bed\u8bcd\u7c7b\u81ea\u52a8\u5224\u5b9a\n", "abstract": " \u751f\u8bcd\u5904\u7406\u5728\u9762\u5411\u5927\u89c4\u6a21\u8d77\u521d\u6587\u672c\u7684\u81ea\u7136\u8bed\u8a00\u81ea\u7406\u5404\u9879\u5e94\u7528\u4e2d\u5360\u6709\u91cd\u8981\u4f4d\u7f6e.\u8bcd\u7c7b\u81ea\u52a8\u5224\u5b9a\u5c31\u662f\u5bf9\u8bf4\u60c5\u6c34\u77e5\u7684\u751f\u8bcd\u7531\u673a\u5668\u81ea\u52a8\u8d4b\u4e88\u4e00\u4e2a\u5408\u9002\u7684\u8bcd\u7c7b\u6807\u8bb0.\u6587\u4e2d\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ek=\u8fd1\u62df\u7684\u8bcd\u7c7b\u81ea\u52a8\u5224\u5b9a\u7b97\u6cd5,\u5e76\u5728\u4e00\u4e2a1\u4ebf\u5b57\u6c49\u8bed\u8bed\u6599\u5e93\u53ca\u4e00\u4e2a60\u4e07\u5b57\u7ecf\u8fc7\u4eba\u5de5\u5206\u8bcd\u548c\u8bcd\u7c7b\u6807\u6ce8\u6c49\u8bed\u719f\u8bed\u6599\u5e93\u7684\u652f\u6301\u4e0b,\u6784\u9020\u4e86\u76f8\u5e94\u5b9e\u9a8c.\u5b9e\u9a8c\u7ed3\u679c\u521d\u6b65\u663e\u793a,\u672c\u7b97\u6cd5\u5bf9\u6c49\u8bed\u5f00\u653e\u8bcd\u7c7b--\u540d\u8bcd\u52a8\u8bcd\u5f00\u7a8d\u8bcd\u7684\u8bcd\u7c7b\u81ea\u52a8\u5224\u5b9a\u5e73\u5747\u6b63\u786e\u7387\u5206\u522b\u4e3a99.21%,84.73%,76.5", "num_citations": "10\n", "authors": ["1181"]}
{"title": "Listwise ranking functions for statistical machine translation\n", "abstract": " Decision rules play an important role in the tuning and decoding steps of statistical machine translation. The traditional decision rule selects the candidate with the greatest potential from a candidate space by examining each candidate individually. However, viewing each candidate as independent imposes a serious limitation on the translation task. We instead view the problem from a ranking perspective that naturally allows the consideration of an entire list of candidates as a whole through the adoption of a listwise ranking function. Our shift from a pointwise to a listwise perspective proves to be a simple yet powerful extension to current modeling that allows arbitrary pairwise functions to be incorporated as features, whose weights can be estimated jointly with traditional ones. We further demonstrate that our formulation encompasses the minimum Bayes risk (MBR) approach, another decision rule that considers\u00a0\u2026", "num_citations": "9\n", "authors": ["1181"]}
{"title": "\u4e2d\u6587\u6587\u672c\u81ea\u52a8\u5206\u7c7b\u4e2d\u7684\u5173\u952e\u95ee\u9898\u7814\u7a76\n", "abstract": " 1. \u5efa\u8bbe\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u4e2d\u6587\u6570\u636e\u96c6, \u5305\u542b 55 \u7c7b, 71674 \u7bc7\u6587\u672c. \u57fa\u4e8e\u591a\u9879\u5f0f\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\u4e2d, \u6bd4\u8f83\u4e86\u6c49\u5b57 Unigram \u548c Bigram \u7279\u5f81\u5728\u6587\u672c\u5206\u7c7b\u4e2d\u7684\u4f5c\u7528. Bigram \u7684\u6700\u4f18 F1 \u503c\u6bd4 Unigram \u9ad8 7.7%, \u4e5f\u4f18\u4e8e\u5b83\u4eec\u7684\u7ec4\u5408\u65b9\u5f0f. 2. \u63d0\u51fa\u4e86\u7279\u5f81\u7684\u5206\u7c7b\u80fd\u529b\u548c\u63cf\u8ff0\u80fd\u529b\u7684\u6982\u5ff5. \u5206\u7c7b\u80fd\u529b\u4fdd\u8bc1\u4e86\u5206\u7c7b\u5668\u80fd\u5c06\u6587\u672c\u6309\u7c7b\u578b\u5206\u5f00, \u63cf\u8ff0\u80fd\u529b\u4fdd\u8bc1\u4e86\u5206\u7c7b\u662f\u57fa\u4e8e\u6587\u672c\u5185\u5bb9\u7684. \u5c06\u7279\u5f81\u5206\u4e3a\u5f3a\u4fe1\u606f\u7279\u5f81, \u5f31\u4fe1\u606f\u7279\u5f81\u548c\u4e0d\u76f8\u5173\u7279\u5f81\u4e09\u7c7b. \u63d0\u51fa\u4e86\u7279\u5f81\u9009\u62e9\u65b9\u6cd5, \u5b83\u5728\u7279\u5f81\u7684\u5206\u7c7b\u80fd\u529b\u548c\u63cf\u8ff0\u80fd\u529b\u4e4b\u95f4\u53d6\u5f97\u8f83\u597d\u7684\u5e73\u8861. \u540c\u6837\u9009\u62e9 70000 \u4e2a\u7279\u5f81\u65f6, \u5b83\u7684 F1 \u503c\u6bd4 \u63d0\u9ad8 3.1%, \u6bd4 \u63d0\u9ad8 5.8%. 3. \u53d1\u73b0\u7279\u5f81\u96c6\u4e2d\u5b58\u5728\u5927\u91cf\u9ad8\u5ea6\u91cd\u53e0\u7279\u5f81\u548c\u9ad8\u5ea6\u504f\u5dee\u7279\u5f81. \u63d0\u51fa\u5c06\u9ad8\u5ea6\u91cd\u53e0\u7684 Bigram \u7279\u5f81\u63d0\u5347\u5230\u5bf9\u5e94\u7684 Trigram \u7279\u5f81\u7684\u964d\u7ef4\u65b9\u6cd5. \u63d0\u51fa\u76f4\u63a5\u5220\u9664\u9ad8\u5ea6\u504f\u5dee\u7279\u5f81\u7684 \u65b9\u6cd5\u548c\u5c06\u9ad8\u5ea6\u504f\u5dee\u7279\u5f81\u7528\u91cd\u8981\u5b57\u7b26\u66ff\u4ee3\u7684 \u65b9\u6cd5. \u5728 \u7279\u5f81\u9009\u62e9\u7684\u57fa\u7840\u4e0a, \u7684\u964d\u7ef4\u5ea6\u8fbe\u5230 6.2%, \u8fbe\u5230 11% \u65f6\u4e0d\u635f\u5931\u5206\u7c7b\u6548\u679c. \u7ec4\u5408\u56db\u79cd\u964d\u7ef4\u65b9\u6cd5, \u63d0\u51fa\u4e86\u591a\u6b65\u964d\u7ef4\u7b56\u7565, \u5176\u4e2d \u548c \u5408\u5728\u4e00\u8d77\u7684\u964d\u7ef4\u5ea6\u8fbe\u5230 26.7% \u65f6, \u4fdd\u6301\u5206\u7c7b\u6548\u679c\u4e0d\u4e0b\u964d. 4. \u5728\u4e24\u4e2a\u76f8\u53cd\u65b9\u5411\u4e0a\u5bf9\u7279\u5f81\u6743\u91cd\u8ba1\u7b97\u8fdb\u884c\u4e86\u7814\u7a76. \u4e00\u662f\u5f80\u590d\u6742\u65b9\u5411\u8d70, \u5c06 \u4e0e \u7ed3\u5408, \u63d0\u51fa\u4e86\u7279\u5f81\u6743\u91cd\u8ba1\u7b97\u65b9\u6cd5; \u63d0\u51fa\u4e86\u7edf\u8ba1\u91cf, \u5e76\u4e0e, \u7ed3\u5408, \u63d0\u51fa\u4e86\u65b9\u6cd5. \u5728 \u9009\u62e9\u7684 70000 \u7279\u5f81\u96c6\u4e0a, \u548c \u7684 F1 \u503c\u6bd4\u4f20\u7edf\u7684 \u63d0\u9ad8 5.7%, \u5728 \u9009\u62e9\u7684\u7279\u5f81\u96c6\u4e0a\u63d0\u9ad8 3%. \u4e8c\u662f\u5f80\u7b80\u5355\u65b9\u5411\u8d70, \u63d0\u51fa\u4e86\u4f9d\u8d56\u5927\u7279\u5f81\u96c6\u7684\u4e8c\u5143\u6743\u91cd\u65b9\u6cd5, \u8fdb\u4e00\u6b65\u63d0\u51fa BW+ \u6570\u503c\u5e73\u6ed1\u6743\u91cd\u65b9\u6cd5 BW-NWS, \u89e3\u51b3\u4e86 \u4e2d\u7684\u5206\u7c7b\u4e0d\u786e\u5b9a\u95ee\u9898. BW-NWS \u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u5206\u7c7b\u6548\u679c, \u800c\u4e14\u4e0e\u6570\u503c\u5e73\u6ed1\u6743\u91cd\u65b9\u6cd5\u7684\u590d\u6742\u6027\u65e0\u5173. \u5728 \u9009\u62e9\u7684 70000 \u7279\u5f81\u96c6\u4e0a, BW-NWS \u7684 F1 \u8fbe\u5230 97.7%, \u6bd4 \u63d0\u9ad8 16.6%. 5. \u7814\u7a76\u4e86\u8bcd\u7279\u5f81\u5728\u6587\u672c\u5206\u7c7b\u4e2d\u7684\u4f5c\u7528, \u5e76\u4e0e Bigram \u8fdb\u884c\u4e86\u6bd4\u8f83. \u57fa\u4e8e Bigram \u7684\u7ed3\u8bba\u4e5f\u9002\u7528\u4e8e\u8bcd\u7279\u5f81. Bigram \u7279\u5f81\u7684\u5206\u7c7b\u6548\u679c\u4f18\u4e8e\u8bcd\u7279\u5f81.", "num_citations": "9\n", "authors": ["1181"]}
{"title": "Chinese text categorization based on the binary weighting model with non-binary smoothing\n", "abstract": " In Text Categorization (TC) based on the vector space model, feature weighting is vital for the categorization effectiveness. Various non-binary weighting schemes are widely used for this purpose. By emphasizing the category discrimination capability of features, the paper firstly puts forward a new weighting scheme TF*IDF*IG. Upon the fact that refined statistics may have more chance to meet sparse data problem, we re-evaluate the role of the Binary Weighting Model (BWM) in TC for further consideration. As a consequence, a novel approach named the Binary Weighting Model with Non-Binary Smoothing (BWM-NBS) is then proposed so as to overcome the drawback of BWM. A TC system for Chinese texts using words as features is implemented. Experiments on a large-scale Chinese document collection with 71,674 texts show that the F1 metric of categorization performance of BWM-NBS gets to 94.9\u00a0\u2026", "num_citations": "9\n", "authors": ["1181"]}
{"title": "\u641c\u7d22\u5f15\u64ce\u641c\u7d22\u7ed3\u679c\u7684\u8bc4\u4ef7\u6280\u672f\n", "abstract": " \u641c\u7d22\u5f15\u64ce\u6839\u636e\u7528\u6237\u67e5\u8be2\u5728\u81ea\u5df1\u7684\u7d22\u5f15\u6570\u636e\u5e93\u4e2d\u8fdb\u884c\u67e5\u627e,\u5e76\u6839\u636e\u76f8\u5173\u6027\u5206\u6790\u5c06\u67e5\u8be2\u7ed3\u679c\u8fd4\u56de\u7ed9\u7528\u6237.\u672c\u6587\u5c31\u4f20\u7edf\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u7684\u6027\u80fd\u6548\u7387\u8bc4\u4ef7\u6280\u672f,\u9488\u5bf9Internet\u7684\u7279\u70b9,\u5bf9\u641c\u7d22\u5f15\u64ce\u641c\u7d22\u7ed3\u679c\u7684\u8bc4\u4ef7\u8fdb\u884c\u4e86\u8ba8\u8bba.", "num_citations": "9\n", "authors": ["1181"]}
{"title": "Automatic Chinese text error correction approach based-on fast approximate Chinese word-matching algorithm\n", "abstract": " A fast approximate Chinese word-matching algorithm is presented. The algorithm can be used to implement the Chinese fuzzy-matching conception. Based on the algorithm, an automatic Chinese text error correction approach using confusing-word substitution and language model evaluation is designed. Compared with Zhang's (1994) confusing-character substitution method, this new approach can deal with not only character substitution errors but also insertion, deletion and string substitution errors. Besides, the algorithm can handle Chinese \"non-word\" error, making it possible and easy to establish a two-level structure in Chinese spelling correction.", "num_citations": "9\n", "authors": ["1181"]}
{"title": "Learning to predict explainable plots for neural story generation\n", "abstract": " Story generation is an important natural language processing task that aims to generate coherent stories automatically. While the use of neural networks has proven effective in improving story generation, how to learn to generate an explainable high-level plot still remains a major challenge. In this work, we propose a latent variable model for neural story generation. The model treats an outline, which is a natural language sentence explainable to humans, as a latent variable to represent a high-level plot that bridges the input and output. We adopt an external summarization model to guide the latent variable model to learn how to generate outlines from training data. Experiments show that our approach achieves significant improvements over state-of-the-art methods in both automatic and human evaluations.", "num_citations": "8\n", "authors": ["1181"]}
{"title": "\u501f\u91cd\u4e8e\u4eba\u5de5\u77e5\u8bc6\u5e93\u7684\u8bcd\u548c\u4e49\u9879\u7684\u5411\u91cf\u8868\u793a: \u4ee5 HowNet \u4e3a\u4f8b\n", "abstract": " \u6458! \u8981!\"# $%/8@* AB&'() $*+,-./0123456789:;<=>;?@ ABCDE FGHIJKLM./$ NO. PQ7RSTUVWXY Z [\\]^ _abcd eEfgA8ABChC i% jk lEDmA8mnBCSopX!\" qrstu/8@* AB< NO. PQ78mvBC,-JKLM./w% mvBC&xy z {XYmnBC|}~ ABC 8*!./=> q $ A< Am R8;= fgA< DmA8 r 345678JKLMP./! & ez P 8\u00a1\u00a2\u00a3\u00a4 e", "num_citations": "8\n", "authors": ["1181"]}
{"title": "Generalized agreement for bidirectional word alignment\n", "abstract": " While agreement-based joint training has proven to deliver state-of-the-art alignment accuracy, the produced word alignments are usually restricted to one-toone mappings because of the hard constraint on agreement. We propose a general framework to allow for arbitrary loss functions that measure the disagreement between asymmetric alignments. The loss functions can not only be defined between asymmetric alignments but also between alignments and other latent structures such as phrase segmentations. We use a Viterbi EM algorithm to train the joint model since the inference is intractable. Experiments on Chinese-English translation show that joint training with generalized agreement achieves significant improvements over two state-ofthe-art alignment methods.", "num_citations": "8\n", "authors": ["1181"]}
{"title": "Iterative learning of parallel lexicons and phrases from non-parallel corpora\n", "abstract": " While parallel corpora are an indispensable resource for data-driven multilingual natural language processing tasks such as machine translation, they are limited in quantity, quality and coverage. As a result, learning translation models from non-parallel corpora has become increasingly important nowadays, especially for low-resource languages. In this work, we propose a joint model for iteratively learning parallel lexicons and phrases from nonparallel corpora. The model is trained using a Viterbi EM algorithm that alternates between constructing parallel phrases using lexicons and updating lexicons based on the constructed parallel phrases. Experiments on Chinese-English datasets show that our approach learns better parallel lexicons and phrases and improves translation performance significantly.", "num_citations": "8\n", "authors": ["1181"]}
{"title": "\u4ee5 MOOC \u652f\u6491\u4e00\u4e2a\u5b8c\u6574\u7684\u9ad8\u6c34\u5e73\u672c\u79d1\u4e13\u4e1a\u8bfe\u7a0b\u4f53\u7cfb: \u4e00\u79cd\u73b0\u5b9e\u53ef\u80fd\u6027\n", "abstract": " \u63d0\u8981\u5f15\u8a00 MOOC \u5e73\u53f0\u8ba1\u7b97\u673a\u4e13\u4e1a\u8bfe\u7a0b\u5f00\u8bbe\u60c5\u51b5 MOOC \u8bfe\u7a0b\u5bf9\u6e05\u534e\u5927\u5b66\u8ba1\u7b97\u673a\u79d1\u5b66\u4e0e\u6280\u672f\u672c\u79d1\u4e13\u4e1a\u4f53\u7cfb\u7684\u8986\u76d6\u60c5\u51b5 MOOC \u73af\u5883\u4e0b\u7684\u5b9e\u9a8c\u6559\u5b66\u7ed3\u675f\u8bed", "num_citations": "8\n", "authors": ["1181"]}
{"title": "A Chinese couplet generation model based on statistics and rules\n", "abstract": " This paper presents an approach to computer generation of Chinese couplets. After dividing the composition of Chinese couplets into hard rules and soft rules, this paper further points out the soft rules consists of character correspondence and context correspondence. A probabilistic graphical model is proposed for couplet generation based on the soft rules, with parameters estimated by EM (Expectation-Maximization) algorithm. The decoding of the model integrates hard rules as heuristics. The experiment result demonstrates that the candidate characters produced by this model are better than those produced simply by frequency. The model can even learn parameters from the data set containing some couplets with poor quality. The couplet generation program implemented by this approach bears an acceptable performance.", "num_citations": "8\n", "authors": ["1181"]}
{"title": "Part-of-speech identification for unknown chinese words based on k-nearest-neighbors strategy\n", "abstract": " BackgroundUnknown word processing plays an important role in many natural language application systems aiming at large-scale unrestricted texts. The task of part-of-speech identification is to automatically assign a part-of-speech tag to an unknown word with empty part-of-speech information. A part-of-speech identification algorithm based on k-nearest-neighbors strategy is presented in this paper. The preliminary experiment, supported by a Chinese corpus of 100M characters and a part-of-speech annotated corpus of 0.6 M characters, shows that the average accuracy rates of the algorithm can reach 99.21%, 84.73%, 70.67% for Chinese words of nouns, verbs and adjectives respectively.", "num_citations": "8\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e\u9690 Markov \u6a21\u578b\u7684\u6c49\u8bed\u8bcd\u7c7b\u81ea\u52a8\u6807\u6ce8\u7684\u5b9e\u9a8c\u7814\u7a76\n", "abstract": " \u6c49\u8bed\u8bcd\u7c7b\u81ea\u52a8\u6807\u6ce8\u6280\u672f\u5728\u4e2d\u6587\u4fe1\u606f\u5904\u7406\u73b0\u5b9e\u5e94\u7528\u4e2d\u5360\u636e\u7740\u5341\u5206\u91cd\u8981\u7684\u4f4d\u7f6e.\u8bba\u6587\u5728\u7ecf\u8fc7\u4eba\u5de5\u5206\u8bcd\u548c\u8bcd\u7c7b\u6807\u6ce8\u7684\u5927\u89c4\u6a21\u6c49\u8bed\u8bed\u6599\u5e93\u7684\u652f\u6301\u4e0b,\u901a\u8fc7\u4e00\u7cfb\u5217\u5bf9\u6b64\u5b9e\u9a8c,\u5bf9\u57fa\u4e8e\u9690Markov\u6a21\u578b\u7684\u6c49\u8bed\u8bcd\u7c7b\u81ea\u52a8\u6807\u6ce8\u7b97\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u8003\u5bdf,\u5e76\u5f97\u51fa\u7ed3\u8bba:\u2460Bigram\u6a21\u578b\u7684\u201c\u6027\u80fd\u4ef7\u683c\u6bd4\u201d\u8f83Trigram\u6a21\u578b\u66f4\u4ee4\u4eba\u6ee1\u610f;\u2461\u4ee57\u4e07\u8bcd\u6b21\u5de6\u53f3\u7684\u6807\u6ce8\u8bed\u6599\u5e93\u8bad\u7ec3Bigram\u6a21\u578b\u5373\u5df2\u57fa\u672c\u591f\u7528(\u6b64\u65f6,\u517c\u7c7b\u8bcd\u8bcd\u7c7b\u6807\u6ce8\u6b63\u786e\u7387\u53ca\u6587\u672c\u8bcd\u7c7b\u6807\u6ce8\u6b63\u786e\u7387\u5206\u522b\u53ef\u8fbe", "num_citations": "8\n", "authors": ["1181"]}
{"title": "A beam search algorithm for itg word alignment\n", "abstract": " Inversion transduction grammar (ITG) provides a syntactically motivated solution to modeling the distortion of words between two languages. Although the Viterbi ITG alignments can be found in polynomial time using a bilingual parsing algorithm, the computational complexity is still too high to handle real-world data, especially for long sentences. Alternatively, we propose a simple and effective beam search algorithm. The algorithm starts with an empty alignment and keeps adding single promising links as early as possible until the model probability does not increase. Experiments on Chinese-English data show that our algorithm is one order of magnitude faster than the bilingual parsing algorithm with bitext cell pruning without loss in alignment and translation quality.", "num_citations": "7\n", "authors": ["1181"]}
{"title": "Fast query recommendation by search\n", "abstract": " Query recommendation can not only effectively facilitate users to obtain their desired information but alsoincrease ads\u2019 click-through rates. This paper presentsa general and highly efficient method for query recommendation. Given query sessions, we automatically generate many similar and dissimilar query-pairs as the prior knowledge. Then we learn a transformation from the prior knowledge to move similar queries closer such that similar queries tend to have similar hash values. This is formulated as minimizing the empirical error on the prior knowledge while maximizing the gap between the data and some partition hyperplanes randomly generated in advance. In the recommendation stage, we search queries that have similar hash values to the given query, rank the found queries and return the top K queries as the recommendation result. All the experimental results demonstrate that our method achieves encouraging results in terms of efficiency and recommendation performance.", "num_citations": "7\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e SOM \u7684\u8bed\u4e49\u8bcd\u5178\u81ea\u52a8\u6784\u5efa\u5b9e\u9a8c\u7814\u7a76\n", "abstract": " \u8bed\u4e49\u8bcd\u5178\u5728\u8bed\u8a00\u5b66\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u4e2d\u5360\u6709\u76f8\u5f53\u5173\u952e\u7684\u4f4d\u7f6e.\u8bed\u4e49\u8bcd\u5178\u7684\u6784\u9020,\u901a\u5e38\u6709\u4e24\u7c7b\u505a\u6cd5.\u4e00\u7c7b\u662f\u57fa\u4e8e\u8bed\u8a00\u5b66\u5bb6\u7684\u4e3b\u89c2\u5224\u65ad,\u53e6\u4e00\u7c7b\u5219\u662f\u57fa\u4e8e\u673a\u5668\u7684\u81ea\u52a8\u805a\u7c7b.\u540e\u8005\u662f\u672c\u6587\u6240\u8981\u7814\u7a76\u7684\u4e3b\u9898.\u672c\u6587\u57fa\u4e8e\u5927\u89c4\u6a21\u7684\u8bed\u6599\u5e93.\u5229\u7528\u81ea\u7ec4\u7ec7\u6620\u5c04\u795e\u7ecf\u7f51\u7edc(SOM)\u5bf9\u8bcd\u5178\u8fdb\u884c\u65e0\u76d1\u7763\u7684\u81ea\u52a8\u6784\u9020.\u9996\u5148\u4ece\u8bed\u6599\u5e93\u4e2d\u62bd\u53d6\u5f85\u805a\u7c7b\u8bcd\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e2d\u7684\u8bcd,\u5e76\u5229\u7528\u4fe1\u606f\u589e\u76ca(Information Gain)\u5bf9\u7279\u5f81\u8bcd\u8fdb\u884c\u9009\u62e9,\u7136\u540e\u501f\u9274\u4fe1\u606f\u68c0\u7d22\u6a21\u578b\u4e2d\u7684TFIDF\u8ba1\u7b97\u7279\u5f81\u5411\u91cf\u4e2d\u6bcf\u4e00\u4e2a\u7279\u5f81\u7684\u7279\u5f81\u6743\u91cd.\u6700\u540e\u5c06\u6784\u9020\u597d\u7684\u5f85\u805a\u7c7b\u8bcd\u7684\u7279\u5f81\u5411\u91cf\u4f5c\u4e3aSOM\u7684\u8f93\u5165,\u7ecf\u8fc7\u7f51\u7edc\u7684\u8fed\u4ee3\u8ba1\u7b97\u5c06\u4e0d\u540c\u7c7b\u522b\u7684\u8bcd\u6620\u5c04\u5728SOM\u8f93\u51fa\u7f51\u683c\u7684\u4e0d\u540c\u7ed3\u70b9.", "num_citations": "7\n", "authors": ["1181"]}
{"title": "An integrated approach to Chinese word segmentation and part-of-speech tagging\n", "abstract": " This paper discusses and compares various integration schemes of Chinese word segmentation and part-of-speech tagging in the framework of true-integration and pseudo-integration. A true-integration approach, named \u2018the divide-and-conquer integration\u2019, is presented. The experiments based on a manually word-segmented and part-of-speech tagged corpus with about 5.8 million words show that this true integration achieves 98.61% F-measure in word segmentation, 95.18% F-measure in part-of-speech tagging, and 93.86% F-measure in word segmentation and part-of-speech tagging, outperforming all other kinds of combinations to some extent. The experimental results demonstrate the potential for further improving the performance of Chinese word segmentation and part-of-speech tagging.", "num_citations": "7\n", "authors": ["1181"]}
{"title": "A method of recognizing entity and relation\n", "abstract": " The entity and relation recognition, i.e. (1) assigning semantic classes to entities in a sentence, and (2) determining the relations held between entities, is an important task in areas such as information extraction. Subtasks (1) and (2) are typically carried out sequentially, but this approach is problematic: the errors made in subtask (1) are propagated to subtask (2) with an accumulative effect; and, the information available only in subtask (2) cannot be used in subtask (1). To address this problem, we propose a method that allows subtasks (1) and (2) to be associated more closely with each other. The process is performed in three stages: firstly, employing two classifiers to do subtasks (1) and (2) independently; secondly, recognizing an entity by taking all the entities and relations into account, using a model called the Entity Relation Propagation Diagram; thirdly, recognizing a relation based on the results of\u00a0\u2026", "num_citations": "7\n", "authors": ["1181"]}
{"title": "\u519c\u4e1a\u75c5\u866b\u5bb3\u8bcd\u6c47\u83b7\u53d6\u65b9\u6cd5\u521d\u63a2\n", "abstract": " \u672c\u6587\u91c7\u53d6\u7edf\u8ba1\u7684\u65b9\u6cd5\u83b7\u53d6\u519c\u4e1a\u75c5\u866b\u5bb3\u8bcd\u6c47\u7684\u8bcd\u6027\u642d\u914d\u89c4\u5219, \u8bed\u4e49\u7c7b\u5206\u5e03\u89c4\u5219, \u5e76\u8fdb\u4e00\u6b65\u5229\u7528\u8fd9\u4e9b\u89c4\u5219\u5728\u5927\u89c4\u6a21\u8bed\u6599\u4e2d\u91c7\u7528\u5e76\u5217\u540c\u73b0, \u6a21\u5f0f\u5339\u914d, \u7279\u5f81\u8bcd\u5339\u914d\u7b49\u7b56\u7565\u83b7\u53d6\u75c5\u866b\u5bb3\u8bcd\u6c47, \u5efa\u7acb\u7279\u5b9a\u4e13\u4e1a\u9886\u57df (\u4e3b\u8981\u4e3a\u519c\u4e1a\u75c5\u866b\u5bb3\u9886\u57df) \u8bcd\u6c47\u8bcd\u5178.", "num_citations": "7\n", "authors": ["1181"]}
{"title": "Internet \u4e2d\u6587\u4e2a\u4eba\u4fe1\u606f\u641c\u7d22\n", "abstract": " \u6458\u8981 \u672c\u6587\u6784\u9020\u4e86\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u751f\u6210 Internet \u4e2a\u4eba\u4fe1\u606f\u7d22\u5f15\u7684\u5b9e\u9a8c\u7cfb\u7edf PersonIndexer. \u5728 CERNET \u4e24\u4e2a\u7f51\u5740\u4e0a\u8fdb\u884c\u7684\u521d\u6b65\u5b9e\u9a8c\u8868\u660e, PersonIndexer \u5bf9\u4e2d\u6587\u59d3\u540d, \u62fc\u97f3\u4eba\u540d, \u4e2d\u6587\u673a\u6784\u540d\u7684\u53ec\u56de\u7387\u548c\u7cbe\u786e\u7387\u5e73\u5747\u5206\u522b\u4e3a 97. 8% \u548c 61. 9%, 100% \u548c 64. 5%, 94. 5% \u548c 92. 1%, \u5bf9\u7535\u5b50\u90ae\u4ef6\u5730\u5740\u548c\u7535\u8bdd\u4f20\u771f\u53f7\u7801\u7684\u53ec\u56de\u7387\u548c\u7cbe\u786e\u7387\u5747\u4e3a 100%. \u9274\u4e8e Internet \u4e0a\u7684\u4fe1\u606f\u68c0\u7d22\u4ee5\u53ca\u81ea\u7136\u8bed\u8a00\u5904\u7406\u8fd9\u4e24\u4e2a\u9886\u57df\u90fd\u4e92\u5411\u5bf9\u65b9\u63d0\u51fa\u4e86\u8981\u6c42, \u6211\u4eec\u76f8\u4fe1, \u9762\u5411\u5927\u89c4\u6a21\u771f\u5b9e\u6587\u672c\u7684\u6c49\u8bed\u5206\u6790\u6280\u672f\u4e0e Internet \u7684\u7ed3\u5408, \u5c06\u662f\u4eca\u540e\u51e0\u5e74\u4e2d\u6587\u4fe1\u606f\u5904\u7406\u4e00\u4e2a\u65b0\u7684\u7814\u7a76\u70ed\u70b9.", "num_citations": "7\n", "authors": ["1181"]}
{"title": "\u4eba\u673a\u5e76\u5b58,\u201c\u8d28\u201d\u201c\u91cf\u201d \u5408\u4e00\ue012\u2014\u8c08\u8c08\u5236\u5b9a\u4fe1\u606f\u5904\u7406\u7528\u6c49\u8bed\u8bcd\u8868\u7684\u7b56\u7565\n", "abstract": " \u4e2d\u6587\u4fe1\u606f\u5904\u7406\u4e8b\u4e1a\u8feb\u5207\u9700\u8981\u4e3a\u4eba\u4eec\u666e\u904d\u63a5\u53d7\u7684\u6c49\u8bed\u8bcd\u8868.\u4ece\u8bed\u8a00\u5de5\u7a0b\u7684\u89d2\u5ea6\u8ba8\u8bba\u5236\u5b9a\u6b64\u7c7b\u8bcd\u8868\u7684\u7b56\u7565,\u5f3a\u8c03\u8981\u8c03\u52a8\u4e24\u65b9\u9762\u7684\u56e0\u7d20,\u5373\u4eba\u5728\u201c\u8d28\u201d\u4e0a\u7684\u7406\u6027\u5224\u65ad,\u4ee5\u53ca\u5229\u7528\u8ba1\u7b97\u673a\u548c\u5927\u578b\u8bed\u6599\u5e93\u5728\u201c\u91cf\u201d\u4e0a\u7684\u7ecf\u9a8c\u7ea6\u5b9a.\u8bcd\u8868\u5e94\u662f\u4e0a\u8ff0\u624b\u6bb5\u5171\u540c\u4f5c\u7528\u7684\u7ed3\u679c", "num_citations": "7\n", "authors": ["1181"]}
{"title": "THUTR: A translation retrieval system\n", "abstract": " We introduce a translation retrieval system THUTR, which casts translation as a retrieval problem. Translation retrieval aims at retrieving a list of target-language translation candidates that may be helpful to human translators in translating a given source-language input. While conventional translation retrieval methods mainly rely on parallel corpus that is difficult and expensive to collect, we propose to retrieve translation candidates directly from target-language documents. Given a source-language query, we first translate it into target-language queries and then retrieve translation candidates from target language documents. Experiments on Chinese-English data show that the proposed translation retrieval system achieves 95.32% and 92.00% in terms of P@ 10 at sentence level and phrase level tasks, respectively. Our system also outperforms a retrieval system that uses parallel corpus significantly. TITLE AND ABSTRACT IN CHINESE", "num_citations": "6\n", "authors": ["1181"]}
{"title": "Non-Independent term selection for Chinese text categorization\n", "abstract": " Chinese text categorization differs from English text categorization due to its much larger term set (of words or character n-grams), which results in very slow training and working of modern high-performance classifiers. This study assumes that this high-dimensionality problem is related to the redundancy in the term set, which cannot be solved by traditional term selection methods. A greedy algorithm framework named \u201cnon-independent term selection\u201d is presented, which reduces the redundancy according to string-level correlations. Several preliminary implementations of this idea are demonstrated. Experiment results show that a good tradeoff can be reached between the performance and the size of the term set.", "num_citations": "6\n", "authors": ["1181"]}
{"title": "Knowledge representation and reasoning based on entity and relation propagation diagram/tree\n", "abstract": " The entity and relation recognition, ie (1) assigning semantic classes (eg, person, organization and location) to entities in a sentence, and (2) determining the relations (eg, born-in and employee-of) held between the corresponding entities, is an important task in areas such as information extraction and question answering. Subtasks (1) and (2) are typically carried out sequentially, and this procedure is problematic: errors made during subtask (1) are propagated to subtask (2) with an accumulative effect; and in many cases information that becomes available only during subtask (2)(eg, the class of an entity corresponds to the first argument of relation born-in (X, China)) would be helpful for subtask (1)(eg, the class of the entity cannot be a location but a person). To address problems of this kind, this paper develops a novel method, which allows subtasks (1) and (2) to be linked more closely together. The procedure is\u00a0\u2026", "num_citations": "6\n", "authors": ["1181"]}
{"title": "Classifying Chinese texts in two steps\n", "abstract": " This paper proposes a two-step method for Chinese text categorization (TC). In the first step, a Na\u00efve Bayesian classifier is used to fix the fuzzy area between two categories, and, in the second step, the classifier with more subtle and powerful features is used to deal with documents in the fuzzy area, which are thought of being unreliable in the first step. The preliminary experiment validated the soundness of this method. Then, the method is extended from two-class TC to multi-class TC. In this two-step framework, we try to further improve the classifier by taking the dependences among features into consideration in the second step, resulting in a Causality Na\u00efve Bayesian Classifier.", "num_citations": "6\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e DCC \u7684\u6d41\u884c\u8bed\u52a8\u6001\u8ddf\u8e2a\u4e0e\u8f85\u52a9\u53d1\u73b0\u7814\u7a76\n", "abstract": " \u672c\u6587\u4ecb\u7ecd\u4e86\u57fa\u4e8e DCC (Dynamic Circulating Corpus \u52a8\u6001\u6d41\u901a\u8bed\u6599\u5e93) \u7684\u6d41\u884c\u8bed\u52a8\u6001\u8ddf\u8e2a\u53d1\u5e03\u7814\u7a76\u7684\u57fa\u672c\u60c5\u51b5. \u7740\u91cd\u4ecb\u7ecd\u4e86\u6d41\u884c\u8bed\u7684\u754c\u5b9a\u4e0e\u7279\u70b9, \u6d41\u884c\u8bed\u7684\u52a8\u6001\u66f2\u7ebf\u7279\u70b9\u548c\u610f\u4e49, \u8ba1\u7b97\u673a\u8f85\u52a9\u53d1\u73b0\u7684\u53ef\u80fd\u7b49. \u6700\u540e\u8fd8\u6307\u51fa\u4eca\u540e\u7684\u7814\u7a76\u76ee\u6807\u4e0e\u65b9\u5411.", "num_citations": "6\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e\u795e\u7ecf\u5143\u7f51\u7edc\u7684\u6c49\u8bed\u77ed\u8bed\u8fb9\u754c\u8bc6\u522b\n", "abstract": " Prediction of Chinese phrase boundary location is the base of shallow parsing or chunk parsing. It is also very im2 portant for processing real texts. With the support of our Chinese treebank including 64426 words, this paper designs and implements a method for automatic prediction of Chinese phrase boundary location based on neural network. The prelimi2 nary results show that the precision is 93. 24%(close testing) and 92. 56%(open testing) respectively.", "num_citations": "6\n", "authors": ["1181"]}
{"title": "Chinese word segmentation and part-of-speech tagging in one step\n", "abstract": " In Chinese natural language processing, word segmentation and part-of-speech tagging is generally carried out as two separate steps. Earlier, the authors introduced a tag-based Markov-model approach to word segmentation. As the tags are of a syntactic nature, this is effectively doing word segmentation and part-of-speech tagging simultaneously. We have used a best-first algorithm with empirical results showing the search for the best solution to be efficient for inputs of reasonable length. In this paper, we will see that the job can be done using an O (n?) algorithm. In our experiments, we actually had the algorithm reduced to O (n) by setting a maximum number of character for words in Chinese to a constant. We also show that performing word segmentation and part-of-speech tagging in one step will bring about improvement in accurracy.", "num_citations": "6\n", "authors": ["1181"]}
{"title": "\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u548c\u8bcd\u6027\u6807\u6ce8\u4e00\u4f53\u5316\u7cfb\u7edf\n", "abstract": " \u5065\u626b\u9972\u8bcd\u568e\u646d\u5ba2\u4e2d\u6587\u4fe1\u606f\u00b7 C| F\u00b7 1996 \u5e74\u7b2c 5 \u671f {j. \u8f6f\u4ef6\u4ea7\u4e1a 0/7-t~ \u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u548c\u8bcd\u6027\u6807\u6ce8\u4e00\u4f53\u5316\u7cfb\u7edf\u6c55\u5934\u5927\u5b66\u8ba1\u7b97\u673a\u79d1\u7814\u6240\u5a46 i \u6e05\u534e\u5927\u5b66\u8ba1\u7b97\u673a\u7cfb\u5b59\u8302\u69ad\u9ec4\u660c\u5b81\u4f5c\u4e3a\u9ad8\u5c42\u6b21\u4fe1\u606f\u5904\u7406\u6280\u672f\u7684\u4e00\u4e2a\u91cd\u8981\u65b9\u5411, \u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4e00\u76f4\u662f\u4eba\u5de5\u667a\u80fd\u754c\u6240\u5173\u6ce8\u7684\u6838\u5fc3\u8bfe\u9898\u4e4b\u4e00.\u201c\u6c49\u8bed\u5206\u8bcd\u201d \u5c31\u6210\u4e3a\u4e2d\u6587\u4fe1\u606f\u5904\u7406\u7684\u4e00\u4e2a\u7814\u7a76 i \u679c\u9898. \u56fd\u5185\u5916\u4ece\u4e8b\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u7cfb\u7edf\u7814\u7a76\u5df2\u6709\u5341\u51e0\u5e74, \u8fc4\u4eca\u5c1a\u65e0\u771f\u6b63\u6210\u719f\u7684\u5b9e\u7528\u7cfb\u7edf\u9762\u4e16\u81ea\u52a8\u5206\u8bcd\u7814\u7a76\u4e2d\u7684\u4e24 1, \u4e3b\u8981\u7684\u56f0\u96be: \u2460 \u672a\u767b\u5f55\u8bcd\u7684\u8fa8\u8bc6, \u2461 \u6b67\u4e49\u5207\u5206\u5b57\u6bb5\u7684\u5904\u7406. \u6240\u8c13\u672a\u767b\u5f55\u8bcd\u5c31\u662f\u5206\u8bcd\u7cfb\u7edf\u7684\u8bcd\u5178\u4e2d\u6ca1\u6709\u6536\u5f55\u7684\u8bcd. \u6c49\u8bed\u8bcd\u6c47\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u96c6\u5408, \u65e0\u8bba\u5efa\u7acb\u591a\u4e48\u5e9e\u5927\u7684\u8bcd\u5178, \u90fd\u4e0d\u53ef\u80fd\u7a77\u4e3e\u6240\u6709\u7684\u8bcd\u6240\u8c13\u6b67\u4e49\u5207\u5206\u5b57\u6bb5. \u5c31\u662f\u6307\u6c49\u8bed\u53e5\u5b50\u4e2d\u7684\u67d0\u4e9b\u5b57\u6bb5, \u5982\u679c\u7eaf\u7cb9\u6839\u636e\u8bcd\u8868\u4f5c\u673a\u68b0\u7684\u5b57\u7b26\u4e32\u5339\u914d. \u5219\u5b83\u53ef\u80fd\u5b58\u5728\u591a\u79cd\u5207\u5206\u7684\u5f62\u5f0f. \u5173\u4e8e\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u7cfb\u7edf\u7684\u7814\u7a76\u65e9\u671f\u4e3b\u8981\u91c7\u7528\u57fa\u4e8e\u673a\u68b0\u5339\u914d\u7684\u65b9\u6cd5. \u5355\u7eaf\u91c7\u7528\u6b64\u7c7b\u65b9\u6cd5\u7684\u7cfb\u7edf\u5927\u591a\u7f3a\u4e4f\u6b67\u4e49\u89e3\u51b3\u80fd\u529b, \u5207\u5206\u7cbe\u5ea6\u6bd4\u8f83\u4f4e. \u968f\u7740\u7814\u7a76\u7684\u53d1\u5c55, \u8bb8\u591a\u5206\u8bcd\u7cfb\u7edf\u5148\u540e\u589e\u52a0\u4e86\u89c4\u5219\u6392\u6b67\u7684\u80fd\u529b, \u4f46\u662f, \u7531\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u5206\u8bcd\u7cfb\u7edf\u9700\u8981\u4eba\u5de5\u63d0\u4f9b\u5927\u91cf\u7684\u5207\u5206\u77e5\u8bc6, \u56e0\u6b64. \u4eba\u4eec\u5bf9\u7cfb\u7edf\u7684\u5f00\u653e\u6027\u8981\u6c42\u8d8a\u6765\u8d8a\u9ad8, \u89c4\u5219\u7cfb\u7edf\u4e0d\u591f\u7075\u6d3b\u7684\u5c40\u9650\u6027\u5c31\u9010\u6e10\u4f53\u73b0\u51fa\u6765. \u4e0e\u6b64\u540c\u65f6, \u6d77\u5916\u6709\u5c11\u91cf\u7cfb\u7edf\u91c7\u7528\u7eaf\u7edf\u8ba1\u7684\u65b9\u6cd5 (\u5176\u4e2d\u4e3b\u8981\u662f\u8bcd\u9891\u4fe1\u606f) \u6765\u5efa\u6784\u5206\u8bcd\u7cfb\u7edf. \u4f46\u5207\u5206\u7cbe\u5ea6\u4e0d\u9ad8; \u8fd1\u5e74\u6765, \u56fd\u5185\u91c7\u7528\u7edf\u8ba1\u65b9\u6cd5\u7684\u5206\u8bcd\u7cfb\u7edf\u9010\u6e10\u589e\u591a. \u800c\u4e14\u7edf\u8ba1\u4fe1\u606f\u4e5f\u4e0d\u9650\u4e8e\u7b80\u5355\u7684\u8bcd\u9891\u4fe1\u606f. \u800c\u91c7\u7528\u8bcd\u6027\u7b49\u5c42\u6b21\u6bd4\u8f83\u9ad8\u7684\u4fe1\u606f\u6765\u6821\u6b63\u5207\u5206\u6b67\u4e49, \u8fd9\u662f\u6c49\u8bed\u5206\u8bcd\u7684\u4e00\u6761\u65b0\u601d\u8def. \u4f46\u5176\u4e2d\u89c4\u6a21\u6bd4\u8f83\u5927\u7684\u7cfb\u7edf\u8fd8\u8f83\u5c11. \u7531\u4e8e\u5207\u5206\u6b67\u4e49\u95ee\u9898\u662f\u5c01} jj \u7684\u548c\u5f00\u653e\u7684\u5206\u8bcd\u7cfb\u7edf\u6240\u5171\u6709\u7684, \u800c\u672a\u767b\u5f55\u8bcd\u95ee\u9898\u662f\u5f00\u653e\u965b\u7684\u7cfb\u7edf\u6240\u7279\u6709\u7684. \u5927\u591a\u6570\u5229\u7528\u5404\u79cd\u624b\u6bb5\u6765\u83b7\u5f97\u67d0\u79cd\u7c7b\u578b\u672a\u767b\u5f55\u8bcd\u7684\u542f\u53d1\u77e5\u8bc6, \u4ece\u800c\u63d0\u4f9b\u9884\u6d4b\u67d0\u4e00\u7279\u5b9a\u7c7b\u578b\u672b\u767b\u5f55\u8bcd (\u5982\u4e2d\u56fd\u4eba\u540d) \u7684\u80fd\u529b. \u4ece\u5df2\u6709\u7684\u82e5\u5e72\u4e2a\u672a\u767b\u5f55\u8bcd\u8fa9\u8bc6\u7cfb\u7edf\u770b, \u53ec\u56de\u7387\u8fd8\u662f\u80fd\u4ee4\u4eba\u6ee1\u610f\u7684. \u4f46\u7cbe\u786e\u7387\u4e00\u822c\u90fd\u8fd8\u4e0d\u591f\u9ad8. \u53e6\u5916, \u7531\u4e8e\u672a\u767b\u5f55\u8bcd\u8fa8\u8bc6\u7684\u7814\u7a76\u57fa\u7840\u8fd8\u6bd4\u8f83\u00a0\u2026", "num_citations": "6\n", "authors": ["1181"]}
{"title": "Neural machine translation with explicit phrase alignment\n", "abstract": " While neural machine translation has achieved state-of-the-art translation performance, it is unable to capture the alignment between the input and output during the translation process. The lack of alignment in neural machine translation models leads to three problems: it is hard to (1) interpret the translation process, (2) impose lexical constraints, and (3) impose structural constraints. These problems not only increase the difficulty of designing new architectures for neural machine translation, but also limit its applications in practice. To alleviate these problems, we propose to introduce explicit phrase alignment into the translation process of arbitrary neural machine translation models. The key idea is to build a search space similar to that of phrase-based statistical machine translation for neural machine translation where phrase alignment is readily available. We design a new decoding algorithm that can easily\u00a0\u2026", "num_citations": "5\n", "authors": ["1181"]}
{"title": "Mask-Align: Self-Supervised Neural Word Alignment\n", "abstract": " Neural word alignment methods have received increasing attention recently. These methods usually extract word alignment from a machine translation model. However, there is a gap between translation and alignment tasks, since the target future context is available in the latter. In this paper, we propose Mask-Align, a self-supervised model specifically designed for the word alignment task. Our model parallelly masks and predicts each target token, and extracts high-quality alignments without any supervised loss. In addition, we introduce leaky attention to alleviate the problem of unexpected high attention weights on special tokens. Experiments on four language pairs show that our model significantly outperforms all existing unsupervised neural baselines and obtains new state-of-the-art results.", "num_citations": "5\n", "authors": ["1181"]}
{"title": "Learning to copy for automatic post-editing\n", "abstract": " Automatic post-editing (APE), which aims to correct errors in the output of machine translation systems in a post-processing step, is an important task in natural language processing. While recent work has achieved considerable performance gains by using neural networks, how to model the copying mechanism for APE remains a challenge. In this work, we propose a new method for modeling copying for APE. To better identify translation errors, our method learns the representations of source sentences and system outputs in an interactive way. These representations are used to explicitly indicate which words in the system outputs should be copied, which is useful to help CopyNet (Gu et al., 2016) better generate post-edited translations. Experiments on the datasets of the WMT 2016-2017 APE shared tasks show that our approach outperforms all best published results.", "num_citations": "5\n", "authors": ["1181"]}
{"title": "Uyghur morphological segmentation with bidirectional GRU neural networks\n", "abstract": " Abstract Information processing of low-resource, morphologically-rich languages such as Uyghur is critical for addressing the language barrier problem faced by the One Belt and One Road (B&R) program in China. In such languages, individual words encode rich grammatical and semantic information by concatenating morphemes to a root form, which leads to severe data sparsity for language processing. This paper introduces an approach for Uyghur morphological segmentation which divides Uyghur words into sequences of morphemes based on bidirectional gated recurrent unit (GRU) neural networks. The bidirectional GRU exploits the bidirectional context to resolve ambiguities and model long-distance dependencies using the gating mechanism. Tests show that this approach significantly outperforms conditional random fields and unidirectional GRUs. This approach is language-independent and can be\u00a0\u2026", "num_citations": "5\n", "authors": ["1181"]}
{"title": "\u4ee5\u7ffb\u8f6c\u601d\u7ef4\u5bf9\u63a5 MOOC \u6559\u80b2\u65b0\u8303\u5f0f\n", "abstract": " 2012\u5e74,MOOC\u5174\u8d77\u4e8e\u7f8e\u56fd,\u4e0d\u5c11\u4eba\u7531\u8877\u5730\u611f\u5230\u9ad8\u7b49\u6559\u80b2\u7684\u201c\u4e00\u573a\u6d77\u5578\u6b63\u5728\u88ad\u6765\u201d(\u65af\u5766\u798f\u5927\u5b66\u6821\u957fJohn Hennessy\u8bed).\u77ed\u77ed\u4e24\u5e74\u7684\u5de5\u592b,\u867d\u5176\u6c14\u52bf\u5e76\u672a\u5982\u9884\u671f\u822c\u5728\u9ad8\u7b49\u6559\u80b2\u754c\u201c\u6a2a\u626b\u5343\u519b\u5982\u5377\u5e2d\u201d,\u4f46\u5176\u53d1\u5c55\u4f9d\u7136\u5341\u5206\u8fc5\u901f,\u6f6e\u5934\u9707\u64bc\u6fc0\u5c04,\u4e0d\u65ad\u51b2\u51fb\u7740\u4f20\u7edf\u6559\u80b2\u7684\u5824\u575d,2013\u5e74,MOOC\u5728\u56fd\u5185\u5f00\u59cb\u53d1\u529b,\u6211\u56fd\u7b2c\u4e00\u4e2a\u5927\u89c4\u6a21\u5728\u7ebf\u6559\u80b2\u5e73\u53f0\u201c\u5b66\u5802\u5728\u7ebf\u201d\u6b63\u5f0f\u53d1\u5e03\u5e76\u6210\u529f\u8fd0\u884c,\u6e05\u534e,", "num_citations": "5\n", "authors": ["1181"]}
{"title": "A survey of text visualization [J]\n", "abstract": " With the emergence of massive texts, information overload and data redundancy raise great challenges for information processing. To address these issues, text visualization has been proposed for understanding the content, structure and patterns hidden behind complicated textual information. Text visualization integrates several techniques including text analysis, data mining, data visualization, computer graphics, human computer interaction, cognitive science and so on. In this paper, we first introduce the concepts of text visualization. Afterwards, we present the research achievements according to different visualization objects, and introduce typical visualization methods and schemes. As a conclusion, we give an outlook to future research directions of text visualization.", "num_citations": "5\n", "authors": ["1181"]}
{"title": "\u56fd\u5185\u4e00\u6d41\u5927\u5b66\u8ba1\u7b97\u673a\u6559\u5b66\u6539\u9769\u4e09\u4e2a\u503c\u5f97\u6ce8\u610f\u7684\u95ee\u9898\n", "abstract": " \u9762\u5bf9\u8ba1\u7b97\u673a\u79d1\u5b66\u4e0e\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55, \u7ed3\u5408\u7b14\u8005\u5728\u6559\u5b66\u5b9e\u8df5\u4e2d\u7684\u4f53\u4f1a, \u6307\u51fa\u76ee\u524d\u56fd\u5185\u4e00\u6d41\u5927\u5b66\u8ba1\u7b97\u673a\u6559\u5b66\u6539\u9769\u4e2d\u5b58\u5728\u7684\u4e09\u4e2a\u503c\u5f97\u6ce8\u610f\u7684\u95ee\u9898, \u5bf9\u5982\u4f55\u5904\u7406\u8fd9\u4e09\u4e2a\u95ee\u9898\u4f5c\u51fa\u521d\u6b65\u89e3\u7b54.", "num_citations": "5\n", "authors": ["1181"]}
{"title": "Unified framework of performing Chinese word segmentation and part-of-speech tagging\n", "abstract": " The paper proposes a unified framework to combine the advantages of the fast one-at-a-time approach and the high-performance all-at-once approach to perform Chinese Word Segmentation (CWS) and Part-of-Speech (PoS) tagging. In this framework, the input of the PoS tagger is a candidate set of several CWS results provided by the CWS model. The widely used one-at-a-time approach and all-at-once approach are two extreme cases of the proposed candidate-based approaches. Experiments on Penn Chinese Treebank 5 and Tsinghua Chinese Treebank show that the generalized candidate-based approach outperforms one-at-a-time approach and even the all-at-once approach. The candidate-based approach is also faster than the time-consuming all-at-once approach. The authors compare three different methods based on sentence, words and character-intervals to generate the candidate set. It turns out\u00a0\u2026", "num_citations": "5\n", "authors": ["1181"]}
{"title": "Extract chinese unknown words from a large-scale corpus using morphological and distributional evidences\n", "abstract": " The representative method of using morphological evidence for Chinese unknown word (UW) extraction is Chinese word segmentation (CWS) model, and the method of using distributional evidence for UW extraction is accessor variety (AV) criterion. However, neither of these methods has been verified on large-scale corpus. In this paper, we propose extensions to remedy the drawbacks of these two methods to handle large-scale corpus:(1) for CWS, we propose a generalized definition of word to improve the recall; and (2) for AV, we propose a restricted version to decrease noise. We carry out experiments on a Chinese Web corpus with approximate 200 billion Chinese characters. Experimental results show that our methods outperform the baselines, and the combination of the two evidences can further improve the performance. Moreover, our methods can also efficiently segment the corpus on the fly, which is especially valuable for processing large-scale corpus.", "num_citations": "5\n", "authors": ["1181"]}
{"title": "Chinese word frequency approximation based on multitype corpora\n", "abstract": " Due to the nature of Chinese, a perfect word-segmented Chinese corpus that is ideal for the task of word frequency estimation may never exist. Therefore, a reliable estimation for Chinese word frequencies remains a challenge. Currently, three types of corpora can be considered for this purpose: raw corpora, automatically word-segmented corpora, and manually word-segmented corpora. As each type has its own advantages and drawbacks, none of them is sufficient alone. In this article, we propose a hybrid scheme which utilizes existing corpora of different types for word frequency approximation. Experiments have been performed from statistical and application-oriented perspectives. We demonstrate that, compared with other schemes, the proposed scheme is the most effective one and leads to better word frequency approximation results.", "num_citations": "5\n", "authors": ["1181"]}
{"title": "Incorporating prior knowledge into multi-label boosting for cross-modal image annotation and retrieval\n", "abstract": " Automatic image annotation (AIA) has proved to be an effective and promising solution to automatically deduce the high-level semantics from low-level visual features. In this paper, we formulate the task of image annotation as a multi-label, multi class semantic image classification problem and propose a simple yet effective joint classification framework in which probabilistic multi-label boosting and contextual semantic constraints are integrated seamlessly. We conducted experiments on a medium-sized image collection including about 5000 images from Corel Stock Photo CDs. The experimental results demonstrated that the annotation performance of our proposed method is comparable to state-of-the-art approaches, showing the effectiveness and feasibility of the proposed unified framework.", "num_citations": "5\n", "authors": ["1181"]}
{"title": "\u4e00\u4e2a\u73b0\u4ee3\u6c49\u8bed\u8bed\u4e49\u77e5\u8bc6\u5e93\u7684\u7814\u7a76\u548c\u5b9e\u73b0\n", "abstract": " \u672c\u6587\u9996\u5148\u63d0\u51fa\u6c49\u8bed\u8bcd\u6c47\u8bed\u4e49\u77e5\u8bc6\u8868\u793a\u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6cd5: \u5bf9\u52a8\u8bcd, \u5f62\u5bb9\u8bcd\u4ece\u683c\u6846\u67b6\u89d2\u5ea6\u7528\u8bba\u65e8\u7f51\u683c\u63cf\u8ff0\u5176\u53e5\u6cd5, \u8bed\u4e49, \u8bed\u7528\u77e5\u8bc6, \u5bf9\u540d\u8bcd\u4ece\u69fd\u5173\u7cfb\u89d2\u5ea6\u7528\u69fd\u5173\u7cfb\u8054\u60f3\u8868\u8fbe\u5f0f (\u69fd\u7c7b\u578b, \u69fd\u5e8f) \u63cf\u8ff0\u590d\u6742\u5b9a\u8bed\u4e0e\u88ab\u4fee\u9970\u7684\u4e2d\u5fc3\u8bcd\u7684\u8bed\u4e49\u5173\u7cfb, \u5bf9\u540d\u8bcd\u8fd8\u4ece\u8bed\u4e49\u573a\u89d2\u5ea6\u63cf\u8ff0\u4e0a\u4e0b\u4f4d\u8bed\u4e49\u5173\u7cfb. \u7136\u540e. \u672c\u6587\u7740\u91cd\u4ecb\u7ecd\u6e05\u534e\u5927\u5b66\u8ba1\u7b97\u673a\u7cfb\u4e0e\u4e2d\u56fd\u4eba\u6c11\u5927\u5b66, \u5317\u4eac\u8bed\u8a00\u5927\u5b66, \u5317\u4eac\u9752\u5e74\u653f\u6cbb\u5b66\u9662\u5171\u540c\u627f\u62c5\u7684\u56db\u4e2a\u8bed\u8a00\u5de5\u7a0b \u201c\u73b0\u4ee3\u6c49\u8bed\u8ff0\u8bed\u52a8\u8bcd\u673a\u5668\u8bcd\u5178\u201d,\u201c\u73b0\u4ee3\u6c49\u8bed\u8ff0\u8bed\u5f62\u5bb9\u8bcd\u673a\u5668\u8bcd\u5178\u201d,\u201c\u73b0\u4ee3\u6c49\u8bed\u540d\u8bcd\u69fd\u5173\u7cfb\u7cfb\u7edf\u201d,\u201c\u4fe1\u606f\u5904\u7406\u7528\u73b0\u4ee3\u6c49\u8bed\u8bed\u4e49\u5206\u7c7b\u8bcd\u5178\u201d \u7684\u7814\u7a76\u548c\u5b9e\u73b0\u60c5\u51b5. \u6700\u540e, \u8bba\u6587\u8fd8\u5efa\u7acb\u4e86\u73b0\u4ee3\u6c49\u8bed\u8bed\u4e49\u77e5\u8bc6\u5e93\u7532\u53f0.", "num_citations": "5\n", "authors": ["1181"]}
{"title": "Automatic image annotation using maximum entropy model\n", "abstract": " Automatic image annotation is a newly developed and promising technique to provide semantic image retrieval via text descriptions. It concerns a process of automatically labeling the image contents with a pre-defined set of keywords which are exploited to represent the image semantics. A Maximum Entropy Model-based approach to the task of automatic image annotation is proposed in this paper. In the phase of training, a basic visual vocabulary consisting of blob-tokens to describe the image content is generated at first; then the statistical relationship is modeled between the blob-tokens and keywords by a Maximum Entropy Model constructed from the training set of labeled images. In the phase of annotation, for an unlabeled image, the most likely associated keywords are predicted in terms of the blob-token set extracted from the given image. We carried out experiments on a medium-sized image\u00a0\u2026", "num_citations": "5\n", "authors": ["1181"]}
{"title": "\u673a\u5668\u7ffb\u8bd1\u7684\u73b0\u72b6\u548c\u95ee\u9898\n", "abstract": " \u25a0 \u673a\u5668\u7ffb\u8bd1\u601d\u60f3\u7684\u840c\u82bd\u5173\u4e8e\u7528\u673a\u5668\u6765\u8fdb\u884c\u8bed\u8a00\u7ffb\u8bd1\u7684\u60f3\u6cd5, \u8fdc\u5728\u53e4\u5e0c\u814a\u65f6\u4ee3\u5c31\u6709\u4eba\u63d0\u51fa\u8fc7\u4e86. \u5728 17 \u4e16\u7eaa, \u4e00\u4e9b\u6709\u8bc6\u4e4b\u58eb\u63d0\u51fa\u4e86\u91c7\u7528\u673a\u5668\u8bcd\u5178\u6765\u514b\u670d\u8bed\u8a00\u969c\u788d\u7684\u60f3\u6cd5. \u7b1b\u5361\u513f (Descartes) \u548c\u83b1\u5e03\u5c3c\u5179 (Leibniz) \u90fd\u8bd5\u56fe\u5728\u7edf\u4e00\u7684\u6570\u5b57\u4ee3\u7801\u7684\u57fa\u7840\u4e0a\u6765\u7f16\u5199\u8bcd\u5178. \u5728 17 \u4e16\u7eaa\u4e2d\u53f6, \u8d1d\u514b (Cave Beck), \u57fa\u5c14\u65bd (Athanasius Kircher) \u548c\u8d1d\u5e0c\u5c14 (Johann Joachim Becher) \u7b49\u4eba\u90fd\u51fa\u7248\u8fc7\u8fd9\u7c7b\u7684\u8bcd\u5178. \u7531\u6b64\u5f00\u5c55\u4e86\u5173\u4e8e \u201c\u666e\u904d\u8bed\u8a00\u201d \u7684\u8fd0\u52a8. \u7ef4\u5c14\u91d1\u65af (John Wilkins) \u5728\u300a \u5173\u4e8e\u771f\u5b9e\u7b26\u53f7\u548c\u54f2\u5b66\u8bed\u8a00\u7684\u8bba\u6587\u300b(An Essay towards a Real Character and Philosophical Language, 1668) \u4e2d\u63d0\u51fa\u7684\u4e2d\u4ecb\u8bed (Interlingua) \u662f\u8fd9\u65b9\u9762\u6700\u8457\u540d\u7684\u6210\u679c, \u8fd9\u79cd\u4e2d\u4ecb\u8bed\u7684\u8bbe\u8ba1\u8bd5\u56fe\u5c06\u4e16\u754c\u4e0a\u6240\u6709\u7684\u6982\u5ff5\u548c\u5b9e\u4f53\u90fd\u52a0\u4ee5\u5206\u7c7b\u548c\u7f16\u7801, \u6709\u89c4\u5219\u5730\u5217\u51fa\u5e76\u63cf\u8ff0\u6240\u6709\u7684\u6982\u5ff5\u548c\u5b9e\u4f53, \u5e76\u6839\u636e\u5b83\u4eec\u5404\u81ea\u7684\u7279\u70b9\u548c\u6027\u8d28, \u7ed9\u4e88\u4e0d\u540c\u7684\u8bb0\u53f7\u548c\u540d\u79f0. \u672c\u4e16\u7eaa\u4e09\u5341\u5e74\u4ee3\u4e4b\u521d, \u4e9a\u7f8e\u5c3c\u4e9a\u88d4\u7684\u6cd5\u56fd\u5de5\u7a0b\u5e08\u963f\u5c14\u695a\u5c3c (GB Artsouni) \u63d0\u51fa\u4e86\u7528\u673a\u5668\u6765\u8fdb\u884c\u8bed\u8a00\u7ffb\u8bd1\u7684\u60f3\u6cd5, \u5e76\u5728 1933 \u5e74 7 \u6708 22 \u65e5\u83b7\u5f97\u4e86\u4e00\u9879 \u201c\u7ffb\u8bd1\u673a\u201d \u7684\u4e13\u5229, \u53eb\u505a \u201c\u673a\u68b0\u8111\u00a0\u2026", "num_citations": "5\n", "authors": ["1181"]}
{"title": "On the consistency of word-segmented Chinese corpus\n", "abstract": " The large scale word segmented corpus is an important resource for the study of both linguistics and computational linguistics. One of the criteria on the quality of corpus is its consistency. This paper discusses the major structural types which are likely to generate word segmentation inconsistencies, discriminates between the concepts oflinguistic wordandpsychological word, and points out that the basic unit of segmented corpus would better bepsychological word. We conclude that it is impossible to conduct a fully consistent word segmented corpus due to the fuzziness ofpsychological word', and that our goal should be adjusted to seeking the consistency under controlled condition instead.", "num_citations": "5\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e\u7edf\u8ba1\u7684\u6c49\u8bed\u5206\u8bcd\u6a21\u578b\u53ca\u5b9e\u73b0\u65b9\u6cd5\n", "abstract": " \u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u7edf\u8ba1\u7684\u6c49\u8bed\u5206\u8bcd\u6a21\u578b\u4ee5\u53ca\u8be5\u6a21\u578b\u5728\u4e0d\u540c\u8d44\u6e90\u8981\u6c42\u4e0b\u7684\u5b9e\u73b0\u65b9\u6cd5, \u5e76\u8fdb\u4e00\u6b65\u8ba8\u8bba\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9, \u6700\u540e, \u4f5c\u8005\u5b9e\u73b0\u4e86\u8fd9\u4e9b\u65b9\u6cd5, \u5e76\u7ed9\u51fa\u76f8\u5e94\u7684\u5b9e\u9a8c\u7ed3\u679c\u548c\u7ed3\u8bba.", "num_citations": "5\n", "authors": ["1181"]}
{"title": "Enriching the transfer learning with pre-trained lexicon embedding for low-resource neural machine translation\n", "abstract": " Most State-Of-The-Art (SOTA) Neural Machine Translation (NMT) systems today achieve outstanding results based only on large parallel corpora. The large-scale parallel corpora for high-resource languages is easily obtainable. However, the translation quality of NMT for morphologically rich languages is still unsatisfactory, mainly because of the data sparsity problem encountered in Low-Resource Languages (LRLs). In the low-resource NMT paradigm, Transfer Learning (TL) has been developed into one of the most efficient methods. It is difficult to train the model on high-resource languages to include the information in both parent and child models, as well as the initially trained model that only contains the lexicon features and word embeddings of the parent model instead of the child languages feature. In this work, we aim to address this issue by proposing the language-independent Hybrid Transfer Learning\u00a0\u2026", "num_citations": "4\n", "authors": ["1181"]}
{"title": "Towards interpretable natural language understanding with explanations as latent variables\n", "abstract": " Recently generating natural language explanations has shown very promising results in not only offering interpretable explanations but also providing additional information and supervision for prediction. However, existing approaches usually require a large set of human annotated explanations for training while collecting a large set of explanations is not only time consuming but also expensive. In this paper, we develop a general framework for interpretable natural language understanding that requires only a small set of human annotated explanations for training. Our framework treats natural language explanations as latent variables that model the underlying reasoning process of a neural model. We develop a variational EM framework for optimization where an explanation generation module and an explanation-augmented prediction module are alternatively optimized and mutually enhance each other. Moreover, we further propose an explanation-based self-training method under this framework for semi-supervised learning. It alternates between assigning pseudo-labels to unlabeled data and generating new explanations to iteratively improve each other. Experiments on two natural language understanding tasks demonstrate that our framework can not only make effective predictions in both supervised and semi-supervised settings, but also generate good natural language explanation.", "num_citations": "4\n", "authors": ["1181"]}
{"title": "Modeling voting for system combination in machine translation\n", "abstract": " System combination is an important technique for combining the hypotheses of different machine translation systems to improve translation performance. Although early statistical approaches to system combination have been proven effective in analyzing the consensus between hypotheses, they suffer from the error propagation problem due to the use of pipelines. While this problem has been alleviated by end-to-end training of multi-source sequence-to-sequence models recently, these neural models do not explicitly analyze the relations between hypotheses and fail to capture their agreement because the attention to a word in a hypothesis is calculated independently, ignoring the fact that the word might occur in multiple hypotheses. In this work, we propose an approach to modeling voting for system combination in machine translation. The basic idea is to enable words in hypotheses from different systems to vote on words that are representative and should get involved in the generation process. This can be done by quantifying the influence of each voter and its preference for each candidate. Our approach combines the advantages of statistical and neural methods since it can not only analyze the relations between hypotheses but also allow for end-to-end training. Experiments show that our approach is capable of better taking advantage of the consensus between hypotheses and achieves significant improvements over state-of-the-art baselines on Chinese-English and English-German machine translation tasks.", "num_citations": "4\n", "authors": ["1181"]}
{"title": "Domain-specific new words detection in chinese\n", "abstract": " With the explosive growth of Internet, more and more domain-specific environments appear, such as forums, blogs, MOOCs and etc. Domain-specific words appear in these areas and always play a critical role in the domain-specific NLP tasks. This paper aims at extracting Chinese domain-specific new words automatically. The extraction of domain-specific new words has two parts including both new words in this domain and the especially important words. In this work, we propose a joint statistical model to perform these two works simultaneously. Compared to traditional new words detection models, our model doesn\u2019t need handcraft features which are labor intensive. Experimental results demonstrate that our joint model achieves a better performance compared with the state-of-the-art methods.", "num_citations": "4\n", "authors": ["1181"]}
{"title": "Learning distributed representations of uyghur words and morphemes\n", "abstract": " While distributed representations have proven to be very successful in a variety of NLP tasks, learning distributed representations for agglutinative languages such as Uyghur still faces a major challenge: most words are composed of many morphemes and occur only once on the training data. To address the data sparsity problem, we propose an approach to learn distributed representations of Uyghur words and morphemes from unlabeled data. The central idea is to treat morphemes rather than words as the basic unit of representation learning. We annotate a Uyghur word similarity dataset and show that our approach achieves significant improvements over CBOW, a state-of-the-art model for computing vector representations of words.", "num_citations": "4\n", "authors": ["1181"]}
{"title": "The design of a live social observatory system\n", "abstract": " With the emergence of social networks and their potential impact on society, many research groups and originations are collecting huge amount of social media data from various sites to serve different applications. These systems offer insights on different facets of society at different moments of time. Collectively they are known as social observatory systems. This paper describes the architecture and implementation of a live social observatory system named'NExT-Live'. It aims to analyze the live online social media data streams to mine social senses, phenomena, influences and geographical trends dynamically. It incorporates an efficient and robust set of crawlers to continually crawl online social interactions on various social network sites. The data crawled are stored and processed in a distributed Hadoop architecture. It then performs the analysis on these social media streams jointly to generate analytics at\u00a0\u2026", "num_citations": "4\n", "authors": ["1181"]}
{"title": "\u5f53\u4ee3\u673a\u5668\u8bed\u8a00\u80fd\u529b\u7684\u7814\u7a76\u73b0\u72b6\u4e0e\u5c55\u671b\n", "abstract": " \u63d0\u8981 \u673a\u5668\u8bed\u8a00\u80fd\u529b\u662f\u5f53\u4ee3\u79d1\u5b66\u7814\u7a76\u9886\u57df\u4e2d\u7684\u524d\u6cbf\u548c\u91cd\u5927\u8bfe\u9898. \u6587\u7ae0\u9996\u5148\u5bf9\u673a\u5668\u8bed\u8a00\u80fd\u529b\u7814\u7a76\u7684\u73b0\u72b6\u8fdb\u884c\u4e86\u56de\u987e\u548c\u5206\u6790, \u7136\u540e\u5bf9\u5176\u53d1\u5c55\u8d8b\u52bf\u8fdb\u884c\u4e86\u5c55\u671b, \u6307\u51fa\u7814\u7a76\u8bed\u8a00\u7684\u5927\u8111\u795e\u7ecf\u673a\u5236\u662f\u5b9e\u73b0\u673a\u5668\u8bed\u8a00\u80fd\u529b\u7814\u7a76\u7a81\u7834\u7684\u65b0\u8def\u5f84; \u6709\u5fc5\u8981\u6784\u5efa\u673a\u5668\u8bed\u8a00\u80fd\u529b\u8bc4\u4f30\u7b49\u7ea7\u91cf\u8868, \u4ee5\u4fbf\u6709\u9488\u5bf9\u6027\u5730\u63d0\u5347\u673a\u5668\u5904\u7406\u4eba\u7c7b\u81ea\u7136\u8bed\u8a00\u7684\u80fd\u529b; \u63d0\u9ad8\u673a\u5668\u7684\u8bed\u8a00\u80fd\u529b\u8fd8\u5fc5\u987b\u52a0\u5f3a\u5b66\u79d1\u95f4\u7684\u5408\u4f5c\u548c\u7814\u7a76\u961f\u4f0d\u7684\u57f9\u517b\u4e0e\u5efa\u8bbe.", "num_citations": "4\n", "authors": ["1181"]}
{"title": "\u65b0\u5a92\u4f53\u4e2d\u7684\u8bcd\u4e91: \u5185\u5bb9\u7b80\u660e\u8868\u8fbe\u7684\u4e00\u79cd\u53ef\u89c6\u5316\u5f62\u5f0f\n", "abstract": " \u6b63\u5728\u65b0\u5a92\u4f53\u65f6\u4ee3, \u4eba\u4eec\u9762\u5bf9\u5c42\u51fa\u4e0d\u7a77\u7684\u5927\u6570\u636e, \u5f88\u5bb9\u6613\u9677\u5165\u4fe1\u606f\u76f2\u533a. \u5982\u4f55\u6709\u6548\u5904\u7406\u548c\u7406\u89e3\u8fd9\u4e9b\u6570\u636e, \u6210\u4e3a\u4eba\u4eec\u65e0\u6cd5\u56de\u907f\u7684\u6311\u6218\u4e4b\u4e00. \u4e00\u822c\u5730, \u81ea\u7136\u8bed\u8a00\u5206\u6790\u6280\u672f\u53ef\u4ee5\u8f83\u597d\u5730\u4ece\u6587\u672c\u5927\u6570\u636e\u4e2d\u6316\u6398\u51fa\u91cd\u8981\u4fe1\u606f, \u800c\u8fd9\u4e9b\u6316\u6398\u51fa\u7684\u4fe1\u606f\u7a76\u7adf\u4ee5\u600e\u6837\u4e00\u79cd\u5f62\u5f0f\u7ec4\u7ec7\u8d77\u6765, \u624d\u66f4\u9002\u5408\u4e8e\u4eba\u4eec\u5bf9\u5176\u8fdb\u884c\u7406\u89e3, \u6d4f\u89c8, \u4f20\u64ad\u53ca\u5e94\u7528, \u662f\u4e00\u4e2a\u503c\u5f97\u5173\u6ce8\u7684\u95ee\u9898. \u57fa\u4e8e\u4fe1\u606f\u53ef\u89c6\u5316\u6280\u672f\u7684\" \u8bcd\u4e91\"(\u4e5f\u88ab\u79f0\u4f5c\" \u8bcd\u4e91\u56fe\"),", "num_citations": "4\n", "authors": ["1181"]}
{"title": "URL \u6a21\u5f0f\u4e0e HTML \u7ed3\u6784\u76f8\u7ed3\u5408\u7684\u5e73\u884c\u7f51\u9875\u83b7\u53d6\u65b9\u6cd5\n", "abstract": " \u6458! \u8981\u00ae \u884c\u8bed\u6599\u5e93\u662f\u5bf9 A \u5668 \u00db\u00dc \u00f3 \u8bed\u8a00\u4fe1\u606f \u00ed\u00ee \u7b49\u5e94\u7528\u6280\u6728 \u00f4 \u6709\u91cd\u8981\u652f \u00f5 \u4f5c\u7528\u7684\u57fa\u7840\u6570\u636e \u00dd\u00de \u00f6 \u7136\u00f7 \u00f8\u00f9 \u4e0a\u7684\u00ae \u884c \u00f9\u00fa \u6570 \u00fb. \u00fc \u6301 \u00de\u00fd\u00ef \u00fe\u00a5 \u4e8e\u00ae \u884c \u00f9\u00ff \u7684! \u6784^ \u548c \u00b9\"^# $% \u901f\u81ea\u52a8\u00bf{i\u00b1\u7684\u00ae \u884c \u00f9\u00fa \u8fdb K \u6784'\u00ae \u884c\u8bed\u6599\u5e93 & \u7136\u662f \u00fb. \u7684 \u00cd\u00ce \u8be5\u6587\u63d0 i \u4e86 R#)*-\u6a21\u5f0f\u4e0e 054-\u7ed3\u6784\u76f8\u7ed3@ \u7684\u00ae \u884c \u00f9\u00fa\u00bf{\u65b9\u6cd5 \u00be \u5148 \u00e8 \u7528 054-\u7ed3\u6784 e \u73b0\u00ae \u884c \u00f9\u00fa \u7684'\u5f52 (\u95ee d) m \u7528)*-\u6a21\u5f0f! 4* \u5386\u00ae \u884c \u00f9\u00ff \u7684+,-\u4ece Ke \u73b0 i 56 \u7684\u00ae \u884c \u00f9\u00fa\u00bf{&\u00f8@\u00b6 \u4e0e./01$ \u4e2a\u00ae \u884c \u00f9\u00ff \u4e0a\u7684 e \u8868 c \u8be5\u65b9\u6cd5\u76f8\u5bf9 h \u7edf\u00bf{\u65b9\u6cd5 &\u00bf{2 \u95f4\u4e0a XY& $\\0 \u4e0a 56q \u63d0 i% &\\\u5e76 \u00c42 \u63d0 i \u4e86 A \u5668 \u00db\u00dc \u7684\u00b1P-2) \u5206\u522b\u63d0 i%; Z \u548c $;\" \u4e2a J \u5206#", "num_citations": "4\n", "authors": ["1181"]}
{"title": "Tag Allocation Model: Model Noisy Social Annotations by Reason Finding\n", "abstract": " We propose the Tag Allocation Model (TAM) to model social annotation data. TAM is a probabilistic generative model, its key feature is finding the latent reason for each tag. A latent reason can be any discrete features of the document (such as words) or a global noise variable. Inferring the reason for each tag helps TAM reduce the ambiguity of a document with multiple tags. By introducing noise as a reason, TAM can handle noise tags naturally. We perform experiments on three real world data sets. The results show that TAM outperforms state-of-the-art approaches in both held-out perplexity and tag recommendation accuracy.", "num_citations": "4\n", "authors": ["1181"]}
{"title": "Joint Chinese word segmenta\u2043 tion and named entity recognition based on max-margin Markov networks\n", "abstract": " Chinese word segmentation (CWS) and named entity recognition (NER) are often separately processed. Max-margin Markov networks (M3N) were used to construct a joint CWS and NER scheme in which joint training and testing are performed. Experiments on the SIGHAN_2005 dataset show that M3N-based word segmenters outperforms CRFs-based word segmenters by 0.3%-2.0%. Experiments on the SIGHAN_2005 and SIGHAN_2006 datasets show that the joint CWS and NER scheme benefits both the two tasks with a CWS improvement of 1.5%-5.5% and an NER improvement of 5.7%-7.9%. The influence of the feature template and decoding on the scheme is also discussed in this paper.", "num_citations": "4\n", "authors": ["1181"]}
{"title": "Word frequency approximation for chinese without using manually-annotated corpus\n", "abstract": " Word frequencies play important roles in a variety of NLP-related applications. Word frequency estimation for Chinese is a big challenge due to characteristics of Chinese, in particular word-formation and word segmentation. This paper concerns the issue of word frequency estimation in the condition that we only have a Chinese wordlist and a raw Chinese corpus with arbitrarily large size, and do not perform any manual annotation to the corpus. Several realistic schemes for approximating word frequencies under the framework of STR (frequency of string of characters as an approximation of word frequency) and MM (Maximal matching) are presented. Large-scale experiments indicate that the proposed scheme, MinMaxMM, can significantly benefit the estimation of word frequencies, though its performance is still not very satisfactory in some cases.", "num_citations": "4\n", "authors": ["1181"]}
{"title": "Word extraction based on semantic constraints in Chinese word-formation\n", "abstract": " This paper presents a novel approach to Chinese word extraction based on semantic information of characters. A thesaurus of Chinese characters is conducted. A Chinese lexicon with 63,738 two-character words, together with the thesaurus of characters, are explored to learn semantic constraints between characters in Chinese word-formation, forming a semantic-tag-based HMM. The Baum-Welch re-estimation scheme is then chosen to train parameters of the HMM in the way of unsupervised learning. Various statistical measures for estimating the likelihood of a character string being a word are further tested. Large-scale experiments show that the results are promising: the F-score of this word extraction method can reach 68.5% whereas its counterpart, the character-based mutual information method, can only reach 47.5%.", "num_citations": "4\n", "authors": ["1181"]}
{"title": "\u591a\u8bed\u79cd\u8bcd\u6c47\u8bed\u4e49\u7f51\u5efa\u8bbe\u4e2d\u7684\u51e0\u4e2a\u95ee\u9898\n", "abstract": " \u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df, \u4ece\u8bed\u4e49\u5904\u7406\u7684\u89d2\u5ea6\u6765\u770b, \u50cf\u8bcd\u6c47\u8bed\u4e49\u7f51\u8fd9\u6837\u9ad8\u5c42\u6b21\u7684\u77e5\u8bc6\u5e93\u662f\u4fdd\u969c\u6b63\u786e\u7684\u8bed\u4e49\u89e3\u91ca\u548c\u8bed\u4e49\u5173\u7cfb\u7684\u8bbe\u7f6e\u6240\u5fc5\u987b\u7684\u8d44\u6e90. \u672c\u6587\u5c31\u591a\u8bed\u79cd\u8bcd\u6c47\u8bed\u4e49\u7f51\u6784\u5efa\u4e2d\u7684\u6982\u5ff5\u53ca\u5176\u5173\u7cfb\u8868\u793a, \u6982\u5ff5\u4f53\u7cfb, \u8bed\u79cd\u8f6c\u6362, UNICODE \u7f16\u7a0b\u7b49\u95ee\u9898\u8fdb\u884c\u4e86\u521d\u6b65\u5206\u6790, \u4ee5\u671f\u5bf9\u591a\u8bed\u79cd\u8bcd\u6c47\u8bed\u4e49\u7f51\u7684\u6784\u5efa\u8d77\u5230\u79ef\u6781\u7684\u610f\u4e49.", "num_citations": "4\n", "authors": ["1181"]}
{"title": "Raising high-degree overlapped character bigrams into trigrams for dimensionality reduction in Chinese text categorization\n", "abstract": " High dimensionality of feature space is a crucial obstacle for Automated Text Categorization. According to the characteristics of Chinese character N-grams, this paper reveals that there exists a kind of redundancy arising from feature overlapping. Focusing on Chinese character bigrams, the paper puts forward a concept of \u03b4-overlapping between two bigrams, and proposes a new method of dimensionality reduction, called \u03b4-Overlapped Raising (\u03b4 \u2013 OR), by raising the \u03b4-overlapped bigrams into their corresponding trigrams. Moreover, the paper designs a two-stage dimensionality reduction strategy for Chinese bigrams by integrating a filtering method based on Chi-CIG score function and the \u03b4 \u2013 OR method. Experimental results on a large-scale Chinese document collection indicate that, on the basis of the first stage of reduction processing, \u03b4 \u2013 OR at the second stage can significantly reduce the\u00a0\u2026", "num_citations": "4\n", "authors": ["1181"]}
{"title": "\u5206\u9636\u6bb5\u6784\u5efa\u6c49\u8bed\u6811\u5e93\n", "abstract": " \u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5206\u9636\u6bb5\u6784\u5efa\u6c49\u8bed\u6811\u5e93\u7684\u7814\u7a76\u601d\u8def. \u901a\u8fc7\u5f15\u8fdb\u9002\u5f53\u7684\u4e2d\u95f4\u6807\u6ce8\u5c42\u6b21\u2014\u8bed\u5757\u6807\u6ce8, \u5927\u5927\u964d\u4f4e\u4e86\u6811\u5e93\u6784\u5efa\u7684\u4eba\u529b\u7269\u529b\u6d88\u8017. \u6211\u4eec\u4f7f\u7528\u6b64\u65b9\u6cd5\u6807\u6ce8\u5b8c\u6210\u4e86 200 \u4e07\u6c49\u5b57\u7684\u6c49\u8bed\u529f\u80fd\u8bed\u5757\u5e93\u548c 20 \u4e07\u8bcd\u7684\u6c49\u8bed\u6811\u5e93, \u53d6\u5f97\u4e86\u5f88\u597d\u7684\u6574\u4f53\u5904\u7406\u6548\u679c.", "num_citations": "4\n", "authors": ["1181"]}
{"title": "A prototype of Chinese search engine based on word segmentation techniques\n", "abstract": " The aim of this paper is to reveal and stress the role of word segmentation in implementing Chinese search engines on the WWW. CSeg and Tag1.1, an integrated system of word segmentation and part-of-speech tagging for unrestricted Chinese texts, along with some basic ideas of its design, are then presented. A prototype of the Chinese search engine WalkerSun, which makes use of CSeg and Tag1.1 as its underlying NLP tool, is introduced briefly.", "num_citations": "4\n", "authors": ["1181"]}
{"title": "Identification of chinese personal names in unrestricted texts\n", "abstract": " Automatic identification of Chinese personal names in unrestricted texts is a key task in Chinese word segmentation, and can affect other NLP tasks such as word segmentation and information retrieval, if it is not properly addressed. This paper (1) demonstrates the problems of Chinese personal name identification in some IT applications,(2) analyzes the structure of Chinese personal names, and (3) further presents the relevant processing strategies. The geographical differences of Chinese personal names between Beijing and Hong Kong are highlighted at the end. It shows that variation in names across different Chinese communities constitutes a critical factor in designing Chinese personal name identification algorithm.", "num_citations": "4\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e\u6f5c\u8bed\u4e49\u6807\u5f15\u7684\u81ea\u7136\u8bed\u8a00\u68c0\u7d22\n", "abstract": " \u5728\u4fe1\u606f\u68c0\u7d22\u4e2d,\u5411\u91cf\u7a7a\u95f4\u6a21\u578b\u662f\u6700\u6709\u6548\u7684\u6570\u5b66\u5de5\u5177\u4e4b\u4e00.\u7531\u4e8e\u81ea\u7136\u8bed\u8a00\u68c0\u7d22\u7684\u7279\u6b8a\u6027,\u4ee5\u53ca\u4f20\u7edf\u4fe1\u606f\u68c0\u7d22\u6a21\u578b\u53d7\u5230\u540c\u4e49\u8bcd,\u591a\u4e49\u8bcd\u7684\u5f71\u54cd,\u68c0\u7d22\u7684\u67e5\u51c6\u7387\u4e0d\u9ad8.\u4e3a\u4e86\u63d0\u9ad8\u81ea\u7136\u8bed\u8a00\u68c0\u7d22\u7684\u67e5\u51c6\u7387,\u6211\u4eec\u5bf9\u57fa\u4e8e\u6982\u5ff5\u7684\u4fe1\u606f\u68c0\u7d22\u6a21\u578b--\u6f5c\u8bed\u4e49\u6807\u5f15(LSI)\u6a21\u578b\u8fdb\u884c\u4e86\u63a2\u8ba8,\u5e76\u5206\u6790\u4e86\u57fa\u4e8eLSI\u7684\u4e24\u4e2a\u5b9e\u4f8b.", "num_citations": "4\n", "authors": ["1181"]}
{"title": "HuaYu: A Word-Segmented and Part-of-Speech Tagged Chinese Corpus\n", "abstract": " As the outcome of a 3-year joint effort of Department of Computer Science, Tsinghua University and Language Information Processing Institute, Beijing Language and Culture University, Beijing, China, a word-segmented and part-of-speech tagged Chinese corpus with size of 2 million Chinese characters, named HuaYu, has been established. This paper firstly introduces some basics about HuaYu in brief, as its genre distribution, fundamental considerations in designing it, word segmentation and part-of-speech tagging standards. Then the complete list of tag set used in HuaYu is given, along with typical examples for each tag accordingly. Several pieces of annotated texts in each genre are also included at last for reader\u2019s reference.", "num_citations": "4\n", "authors": ["1181"]}
{"title": "Resources for\n", "abstract": " Tao Lin of the mathematics department presented a paper on\" An adaptive finite element scheme for a Fredholm type integro-differential equation\" in a special session on\" Non-local foundary value problems and applications at the 14th International Association for Mathematics and Computers in Simulation World Congress on Computational and Applied Mathematics in Atlanta. The paper was a joint work with Robert Rogers, also of mathematics. The main results were published in the conference proceedings.", "num_citations": "4\n", "authors": ["1181"]}
{"title": "Generating Major Types of Chinese Classical Poetry in a Uniformed Framework\n", "abstract": " Poetry generation is an interesting research topic in the field of text generation. As one of the most valuable literary and cultural heritages of China, Chinese classical poetry is very familiar and loved by Chinese people from generation to generation. It has many particular characteristics in its language structure, ranging from form, sound to meaning, thus is regarded as an ideal testing task for text generation. In this paper, we propose a GPT-2 based uniformed framework for generating major types of Chinese classical poems. We define a unified format for formulating all types of training samples by integrating detailed form information, then present a simple form-stressed weighting method in GPT-2 to strengthen the control to the form of the generated poems, with special emphasis on those forms with longer body length. Preliminary experimental results show this enhanced model can generate Chinese classical poems of major types with high quality in both form and content, validating the effectiveness of the proposed strategy. The model has been incorporated into Jiuge, the most influential Chinese classical poetry generation system developed by Tsinghua University (Guo et al., 2019).", "num_citations": "3\n", "authors": ["1181"]}
{"title": "Performance comparison of neural machinetranslation systems in Uyghur-Chinese translation\n", "abstract": " The neural machine translation based on deep learning significantly surpasses the traditional statistical machine translation in many languages, and becomes the current mainstream machine translation technology. This paper compares six influential neural machine translation methods from the level of word granularity in the task of Uyghur-Chinese machine translation. These methods are attention mechanism (GroundHog), vocabulary expansion (LV-groundhog), source language and target language with subword units (subword-nmt), characters and words mixed (nmt. hybrid), subword units and characters (dl4mt-cdec), and complete characters (dl4mt-c2c). The experimental results show that Uyghur-Chinese neural machine translation performs best when the source language is segmented into subword units and the target language is represented by characters (dl4mt-cdec). This paper is the first to use neural\u00a0\u2026", "num_citations": "3\n", "authors": ["1181"]}
{"title": "Exploring the Concept Levels of Social Tags in Chinese Blogs\n", "abstract": " Tags are widely used in web 2.0 applications such as blogs and online bookmarks. Many researchers have studied the characteristics of tags, but the concept level of tags is still an untouched region. Concept level describes how focus a tag is, the lower the level, the more concrete concept the tag covers. The paper has two main contributions: 1) We propose a method to discover the concept level of tags automatically. 2) We perform tag recommendation concerning their concept levels. Experiments are conducted on a large-scale real world Chinese blog data set. We find that most tags are in more concrete levels, and the users tend to choose tags within the same concept level for a document. When predicting tags for a new document, tags in higher concept levels provide little help, but hurt the performance.", "num_citations": "3\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e M~ 3N \u7684\u4e2d\u6587\u5206\u8bcd\u4e0e\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4e00\u4f53\u5316\n", "abstract": " \u4e2d\u6587\u5206\u8bcd\u548c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7ecf\u5e38\u88ab\u89c6\u4e3a2\u4e2a\u72ec\u7acb\u7684\u4efb\u52a1.\u8be5\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6700\u5927\u95f4\u9694Markov\u7f51\u7edc\u6a21\u578b(M3N)\u7684\u4e2d\u6587\u5206\u8bcd\u548c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4e00\u4f53\u5316\u65b9\u6cd5,\u5c06\u4e8c\u8005\u7edf\u4e00\u5728\u4e00\u4e2a\u5b57\u5e8f\u5217\u6807\u6ce8\u6846\u67b6\u4e0b,\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\u548c\u6d4b\u8bd5.\u5728SIGHAN_2005\u5206\u8bcd\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a,\u4e0e\u57fa\u4e8e\u6761\u4ef6\u968f\u673a\u573a\u6a21\u578b\u7684\u5206\u8bcd\u5668\u76f8\u6bd4,\u57fa\u4e8eM3N\u7684\u5206\u8bcd\u5668\u52a0\u6743\u7efc\u5408\u503c\u63d0\u9ad80.3%~2.0%.\u5728SIGHAN_2005\u5206\u8bcd\u6570\u636e\u96c6\u548cSIGHAN_2006\u547d\u540d\u5b9e\u4f53\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u7684\u7ed3\u679c\u663e\u793a,\u4e0e\u5206\u6b65\u65b9\u6cd5\u76f8\u6bd4,\u4e00\u4f53\u5316\u65b9\u6cd5\u80fd\u591f\u540c\u65f6\u63d0\u9ad8\u4e2d\u6587\u5206\u8bcd\u548c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u6027\u80fd,\u52a0\u6743\u7efc\u5408\u503c\u7684\u63d0\u9ad8\u5e45\u5ea6\u5206\u522b\u4e3a1.5%~5.5%\u548c5.7%~7.9%.\u540c\u65f6,\u8fd8\u57fa\u4e8e\u5206\u8bcd\u4efb\u52a1\u8003\u5bdf\u4e86\u7279\u5f81\u6a21\u7248\u548c\u4e0d\u5408\u6cd5\u5e8f\u5217\u5bf9M3N\u6027\u80fd\u7684\u5f71\u54cd.", "num_citations": "3\n", "authors": ["1181"]}
{"title": "Disambiguating Tags in Blogs\n", "abstract": " Blog users enjoy tagging for better document organization, while ambiguity in tags leads to inaccuracy in tag-based applications, such as retrieval, visualization or trend discovery. The dynamic nature of tag meanings makes current word sense disambiguation(WSD) methods not applicable. In this paper, we propose an unsupervised method for disambiguating tags in blogs. We first cluster the tags by their context words using Spectral Clustering. Then we compare a tag with these clusters to find the most suitable meaning. We use Normalized Google Distance to measure word similarity, which can be computed by querying search engines, thus reflects the up-to-date meaning of words. No human labeling efforts or dictionary needed in our method. Evaluation using crawled blog data showed a promising micro average precision of 0.842.", "num_citations": "3\n", "authors": ["1181"]}
{"title": "\u7edf\u8ba1\u4e0e\u89c4\u5219\u76f8\u7ed3\u5408\u7684\u53e4\u6587\u5bf9\u8054\u5e94\u5bf9\u6a21\u578b\n", "abstract": " \u8be5\u6587\u5c06\u53e4\u6587\u5bf9\u8054\u89c4\u5219\u533a\u5206\u4e3a\u786c\u89c4\u5219\u4e0e\u8f6f\u89c4\u5219, \u5c06\u8f6f\u89c4\u5219\u533a\u5206\u4e3a\u5b57\u76f8\u5bf9\u4e0e\u4e0a\u4e0b\u6587\u76f8\u5bf9. \u5e76\u5728\u8f6f\u89c4\u5219\u6307\u5bfc\u4e0b\u5efa\u7acb\u5bf9\u8054\u5e94\u5bf9\u7684\u6709\u5411\u6982\u7387\u56fe\u6a21\u578b, \u4f7f\u7528 EM (Expectation-Maximization) \u7b97\u6cd5\u4f30\u8ba1\u6a21\u578b\u53c2\u6570, \u5728\u6c42\u89e3\u7684\u641c\u7d22\u8fc7\u7a0b\u4e2d\u52a0\u5165\u786c\u89c4\u5219, \u4ece\u800c\u7ed9\u51fa\u4e86\u4e00\u79cd\u5b8c\u6574\u7684\u5bf9\u8054\u81ea\u52a8\u5e94\u5bf9\u65b9\u6cd5. \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u53c2\u6570\u5b66\u4e60\u540e\u7684\u5019\u9009\u5b57\u5217\u8868\u7531\u4e8e\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u4e0d\u8003\u8651\u4e0a\u4e0b\u6587\u76f8\u5bf9\u7684\u5f71\u54cd, \u6bd4\u4ec5\u7528\u9891\u6b21\u7edf\u8ba1\u7684\u5019\u9009\u5b57\u5217\u8868\u66f4\u4e3a\u5408\u7406. \u8be5\u65b9\u6cd5\u8fd8\u80fd\u591f\u5bf9\u8bad\u7ec3\u8bed\u6599\u5e93\u4e2d\u5de5\u6574\u4e0e\u4e0d\u5de5\u6574\u7684\u5bf9\u8054\u533a\u5206\u5b66\u4e60. \u57fa\u4e8e\u8be5\u65b9\u6cd5\u6240\u5b9e\u73b0\u7684\u53e4\u6587\u5bf9\u8054\u5e94\u5bf9\u7a0b\u5e8f\u8fbe\u5230\u4e86\u4e00\u5b9a\u6c34\u5e73.", "num_citations": "3\n", "authors": ["1181"]}
{"title": "Multi-modal Multi-label Semantic Indexing of Images Using Unlabeled Data\n", "abstract": " Automatic image annotation (AIA) refers to the association of words to whole images which is considered as a promising and effective approach to bridge the semantic gap between low-level visual features and high-level semantic concepts. In this paper, we formulate the task of image annotation as a multi-label multi class semantic image classification problem and propose a simple yet effective algorithm: hybrid self-learning with alternating space between uni-modality and bi-modality, which integrate multi-label boosting with asymmetric binary SVM-based active learning into a joint hierarchical classification framework to perform cross-modal image annotation by incorporating unlabeled images. We conducted experiments on a medium-sized image collection including about 15000 images from Corel Stock Photo CDs. The experimental results demonstrated that our proposed method can enhance a given\u00a0\u2026", "num_citations": "3\n", "authors": ["1181"]}
{"title": "\u6c49\u8bed\u4ea4\u96c6\u578b\u6b67\u4e49\u5207\u5206\u5b57\u6bb5\u5173\u4e8e\u4e13\u4e1a\u9886\u57df\u7684\u7edf\u8ba1\u7279\u6027\n", "abstract": " \u4ea4\u96c6\u578b\u5206\u8bcd\u6b67\u4e49\u662f\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u4e2d\u7684\u4e3b\u8981\u6b67\u4e49\u7c7b\u578b\u4e4b\u4e00. \u73b0\u6709\u7684\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u7cfb\u7edf\u5bf9\u5b83\u7684\u5904\u7406\u80fd\u529b\u5c1a\u4e0d\u80fd\u5b8c\u5168\u4ee4\u4eba\u6ee1\u610f. \u9488\u5bf9\u4ea4\u96c6\u578b\u5206\u8bcd\u6b67\u4e49, \u57fa\u4e8e\u901a\u7528\u8bed\u6599\u5e93\u7684\u8003\u5bdf\u76ee\u524d\u5df2\u6709\u4e0d\u5c11, \u4f46\u8fd8\u6ca1\u6709\u57fa\u4e8e\u4e13\u4e1a\u9886\u57df\u8bed\u6599\u5e93\u7684\u76f8\u5173\u8003\u5bdf. \u6839\u636e\u4e00\u4e2a\u4e2d\u7b49\u89c4\u6a21\u7684\u6c49\u8bed\u901a\u7528\u8bcd\u8868, \u4e00\u4e2a\u89c4\u6a21\u7ea6\u4e3a 9 \u4ebf\u5b57\u7684\u901a\u7528\u8bed\u6599\u5e93\u548c\u4e24\u4e2a\u6db5\u76d6 55 \u4e2a\u4e13\u4e1a\u9886\u57df, \u603b\u89c4\u6a21\u7ea6\u4e3a 1. 4 \u4ebf\u5b57\u7684\u4e13\u4e1a\u9886\u57df\u8bed\u6599\u5e93, \u5bf9\u4ece\u901a\u7528\u8bed\u6599\u5e93\u4e2d\u62bd\u53d6\u7684\u9ad8\u9891\u4ea4\u96c6\u578b\u6b67\u4e49\u5207\u5206\u5b57\u6bb5\u5728\u4e13\u4e1a\u9886\u57df\u8bed\u6599\u5e93\u4e2d\u7684\u7edf\u8ba1\u7279\u6027, \u4ee5\u53ca\u4ece\u4e13\u4e1a\u9886\u57df\u8bed\u6599\u5e93\u4e2d\u62bd\u53d6\u7684\u4ea4\u96c6\u578b\u6b67\u4e49\u5207\u5206\u5b57\u6bb5\u5173\u4e8e\u4e13\u4e1a\u9886\u57df\u7684\u7edf\u8ba1\u7279\u6027\u8fdb\u884c\u4e86\u7a77\u5c3d\u5f0f, \u591a\u89d2\u5ea6\u7684\u8003\u5bdf. \u7ed9\u51fa\u7684\u89c2\u5bdf\u7ed3\u679c\u5bf9\u8bbe\u8ba1\u9762\u5411\u4e13\u4e1a\u9886\u57df\u7684\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u7b97\u6cd5\u5177\u6709\u4e00\u5b9a\u7684\u53c2\u8003\u4ef7\u503c.", "num_citations": "3\n", "authors": ["1181"]}
{"title": "Automated Construction of Chinese Thesaurus Based on Self-Organizing Map\n", "abstract": " Thesauri are vital for many NLP applications, for example, query expansion in information retrieval, estimation of semantic similarities between example sentences in machine translation, and smoothing for handling data sparseness problems in language computing. They also play important roles in content-oriented knowledge management systems, such as the design and construction of ontologies, either linguistic or engineering, in sementic web. There are two ways for building thesaurus. The first one is manual manipulation by human experts, such as The Merriam Webster", "num_citations": "3\n", "authors": ["1181"]}
{"title": "Transductive support vector machines using simulated annealing\n", "abstract": " Transductive inference estimates classification function at samples within the test data using information from both the training and the test data set. In this paper, a new algorithm of transductive support vector machine is proposed to improve Joachims\u2019 transductive SVM to handle various data distributions. Simulated annealing heuristic is used to solve the combinatorial optimization problem of TSVM, in order to avoid the problems of having to estimate the ratio of positive/negative samples and local optimum. The experimental result shows that TSVM-SA algorithm outperforms Joachims\u2019 TSVM, especially when there is a significant deviation between the distribution of training and test data.", "num_citations": "3\n", "authors": ["1181"]}
{"title": "Automatic content based title extraction for chinese documents using support vector machine\n", "abstract": " In this paper, a content-based and domain-independent method for automatically extracting titles from Chinese research papers is proposed. The information contained in the title itself and the similarity between the title and the body of the paper is exploited, under the condition that the experiment is carried out on plain texts in which no any format information such as font is used. A list of words only used in Chinese titles and a list of words never used in Chinese titles are further collected to facilitate the title extraction. We use the support vector machine classifier to perform a robust and more adaptable automatic title extraction. The method achieves good performance on a test set consisting of 2438 research papers which cover almost all of the academic disciplines.", "num_citations": "3\n", "authors": ["1181"]}
{"title": "\u5e38\u7528\u53cc\u97f3\u91ca\u8bcd\u8bcd\u91cf\u53ca\u63d0\u53d6\u65b9\u6cd5\u2014\u2014\u5bf9\u300a \u73b0\u4ee3\u6c49\u8bed\u8bcd\u5178\u300b \u53cc\u97f3\u540c\u4e49\u91ca\u8bcd\u7684\u91cf\u5316\u5206\u6790\n", "abstract": " \u4e00\u79cd\u8bed\u8a00\u4e2d\u6700\u91cd\u8981\u7684\u8bcd\u8bed\u6709\u591a\u5c11,\u662f\u8ba4\u77e5\u79d1\u5b66,\u8bed\u4e49\u5b66,\u8bcd\u5178\u5b66\u4e2d\u7684\u91cd\u8981\u547d\u9898.\u672c\u6587\u63d0\u53d6\u51fa\u300a\u73b0\u4ee3\u6c49\u8bed\u8bcd\u5178\u300b\u4e2d\u5355\u4e49\u7684\u53cc\u97f3\u91ca\u8bcd6010\u4f8b,\u6839\u636e\u9891\u6b21\u4e0e\u91ca\u8bcd\u4f4d\u7f6e\u8fdb\u884c\u52a0\u6743,\u518d\u7528\u540c\u91ca\u8bcd,\u8f6c\u91ca\u8bcd,\u540c\u7d20\u8bcd\u4e09\u79cd\u65b9\u6cd5\u6765\u8fdb\u884c\u7cfb\u8054,\u63d0\u53d6\u51fa\u4e86502\u6761\u5e38\u7528\u91ca\u8bcd.\u53d1\u73b0\u91ca\u8bcd\u9891\u7387\u9ad8,\u91ca\u8bcd\u4f4d\u7f6e\u9760\u524d\u7684\u4e00\u822c\u90fd\u5177\u6709\u901a\u7528\u6027\u5f3a,\u8bcd\u4e49\u8986\u76d6\u9762\u5e7f,\u8bed\u4e49\u4f4d\u7f6e\u91cd\u8981\u7684\u7279\u70b9.", "num_citations": "3\n", "authors": ["1181"]}
{"title": "\u73b0\u4ee3\u85cf\u8bed\u7684\u53e5\u6cd5\u7ec4\u5757\u4e0e\u5f62\u5f0f\u6807\u8bb0\n", "abstract": " \u672c\u6587\u5b9a\u4e49\u548c\u63cf\u8ff0\u4e86\u73b0\u4ee3\u85cf\u8bed\u53e5\u6cd5\u7ec4\u5757\u7684\u57fa\u672c\u7c7b\u578b\u4ee5\u53ca\u76f8\u5173\u7684\u5f62\u5f0f\u6807\u8bb0, \u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u85cf\u8bed\u81ea\u52a8\u5206\u8bcd\u7684\u7ec4\u5757\u65b9\u6cd5. \u800c\u5b9e\u65bd\u7ec4\u5757\u5206\u8bcd\u65b9\u6cd5\u7684\u63aa\u65bd\u5305\u62ec\u6309\u7167\u4e00\u5b9a\u987a\u5e8f\u539f\u5219\u8bc6\u522b\u7ec4\u5757\u7684\u5f62\u5f0f\u6807\u8bb0, \u901a\u8fc7\u5404\u7c7b\u6807\u8bb0\u51fd\u6570\u96c6, \u8f85\u52a9\u8bcd\u8868, \u4ee5\u53ca\u4ece\u7ec4\u5757\u4e2d\u62bd\u53d6\u7684\u53e5\u6cd5\u4fe1\u606f\u786e\u5b9a\u7ec4\u5757\u7684\u8fb9\u754c, \u7136\u540e\u5bf9\u7ec4\u5757\u8fdb\u884c\u5206\u8bcd\u548c\u8bcd\u6027\u6807\u6ce8. \u8fdb\u4e00\u6b65\u7684\u8bbe\u60f3\u662f\u5bf9\u7ec4\u5757\u8fdb\u884c\u5f52\u5e76, \u4f7f\u5176\u4e0e\u85cf\u8bed\u53e5\u6cd5\u6210\u5206\u5f62\u6210\u4e00\u81f4\u5173\u7cfb, \u8fbe\u5230\u6d88\u9664\u63cf\u7b51\u5957\u7ec4\u5757\u548c\u5229\u4e8e\u53e5\u6cd5\u7406\u89e3\u7684\u76ee\u7684.", "num_citations": "3\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e\u77e5\u7f51\u7684\u76f8\u5173\u6982\u5ff5\u573a\u7684\u6784\u5efa\n", "abstract": " \u8bcd\u8bed\u7684\u76f8\u5173\u6027\u53ca\u5176\u77e5\u8bc6\u7684\u83b7\u53d6\u662f\u4eba\u7c7b\u8bed\u8a00\u6280\u672f\u7814\u7a76\u4e2d\u7684\u70ed\u70b9\u4e4b\u4e00. \u56fd\u5185\u5916\u5173\u4e8e\u8fd9\u4e2a\u65b9\u9762\u4ee5\u53ca\u4e0e\u4e4b\u76f8", "num_citations": "3\n", "authors": ["1181"]}
{"title": "\u4e2d\u6587\u53e5\u5b50\u4e2d\u53cc\u97f3\u8282\u517c\u7c7b\u8bcd\u53e5\u6cd5\u5206\u6790\u5386\u7a0b\u521d\u63a2\n", "abstract": " \u4ee5\u65e2\u53ef\u7528\u4f5c\u540d\u8bcd,\u53c8\u53ef\u7528\u4f5c\u52a8\u8bcd\u7684\u6c49\u8bed\u53cc\u97f3\u8282\u517c\u7c7b\u8bcd\u4e3a\u6750\u6599,\u91c7\u7528\u79fb\u52a8\u7a97\u53e3\u8303\u5f0f,\u8003\u5bdf\u4e86\u6c49\u8bed\u53e5\u5b50\u4e2d\u540d\u8bcd\u503e\u5411\u6027\u9ad8\u548c\u4f4e\u4e24\u7c7b\u517c\u7c7b\u8bcd\u53e5\u6cd5\u5206\u6790\u7684\u8be6\u7ec6\u5386\u7a0b.\u7ed3\u679c\u53d1\u73b0,\u4e24\u7c7b\u517c\u7c7b\u8bcd\u4e0e\u5404\u81ea\u7684\u63a7\u5236\u8bcd\u4e4b\u95f4,\u5747\u672a\u51fa\u73b0\u9605\u8bfb\u65f6\u95f4\u4e0a\u7684\u663e\u8457\u5dee\u5f02,\u56e0\u6b64\u5e76\u4e0d\u652f\u6301\u5ef6\u8fdf\u6a21\u578b\u6240\u63d0\u51fa\u7684\u517c\u7c7b\u8bcd\u53e5\u6cd5\u89d2\u8272\u5ef6\u8fdf\u6307\u6d3e\u7684\u89c2\u70b9.\u5b9e\u9a8c\u8fd8\u53d1\u73b0,\u540d\u8bcd\u503e\u5411\u6027\u9ad8\u7684\u517c\u7c7b\u8bcd\u5f15\u8d77\u4e86\u66f4\u5927\u7684\u52a0\u5de5\u56f0\u96be.\u6b64\u5916,\u5373\u4fbf\u662f\u540d\u8bcd\u503e\u5411\u6027\u4f4e\u7684\u517c\u7c7b\u8bcd,\u88ab\u8bd5\u4e5f\u4ecd\u4e3a\u5176\u6307\u6d3e\u540d\u8bcd\u7684\u53e5\u6cd5\u89d2\u8272.\u82b1\u56ed\u8def\u5f84\u6a21\u578b\u548c\u5236\u7ea6\u6ee1\u610f\u7406\u8bba\u5747\u4e0d\u80fd\u5355\u72ec\u548c\u5b8c\u6574\u89e3\u91ca\u4e0a\u8ff0\u53d1\u73b0.\u6587\u7ae0\u6307\u51fa,\u5b9e\u9a8c\u7ed3\u679c\u652f\u6301\u4e00\u79cd\u65e2\u4e0d\u540c\u4e8e\u82b1\u56ed\u8def\u5f84\u6a21\u578b,\u53c8\u4e0d\u540c\u4e8e\u5236\u7ea6\u6ee1\u610f\u7406\u8bba\u7684\u6743\u53d8\u7684\u7406\u8bba.\u9605\u8bfb\u4e2d\u9047\u5230\u517c\u7c7b\u8bcd\u65f6,\u8bfb\u8005\u80fd\u7acb\u5373\u4e3a\u5176\u6307\u6d3e\u53e5\u6cd5\u89d2\u8272.\u8fd9\u79cd\u6307\u6d3e\u8fc7\u7a0b\u65e2\u53ef\u4ee5\u5229\u7528\u6982\u7387\u5236\u7ea6\u4fe1\u606f,\u4e5f\u53ef\u4ee5\u5728\u53e5\u6cd5\u5206\u6790\u539f\u5219\u6307\u5bfc\u4e0b\u8fdb\u884c.\u53e5\u6cd5\u5206\u6790\u539f\u5219\u662f\u5426\u8d77\u4f5c\u7528,\u53d6\u51b3\u4e8e\u6982\u7387\u5236\u7ea6\u4fe1\u606f\u7684\u5236\u7ea6\u5f3a\u5ea6.", "num_citations": "3\n", "authors": ["1181"]}
{"title": "\u73b0\u4ee3\u85cf\u8bed\u5224\u5b9a\u52a8\u8bcd\u53e5\u4e3b\u5bbe\u8bed\u7684\u81ea\u52a8\u8bc6\u522b\u65b9\u6cd5\n", "abstract": " \u672c\u6587\u901a\u8fc7\u5256\u6790\u73b0\u4ee3\u85cf\u8bed\u5224\u5b9a\u53e5\u4e3b\u8bed, \u5bbe\u8bed\u4ee5\u53ca\u52a8\u8bcd\u7684\u7ed3\u6784\u53ca\u5f62\u5f0f\u6807\u8bb0, \u63d0\u51fa\u8bc6\u522b\u4e3b\u8bed\u548c\u5bbe\u8bed\u7684\u65b9\u6cd5. \u5176\u4e2d\u4f9d\u636e\u52a8\u8bcd\u7684\u5f62\u5f0f\u548c\u524d\u9644\u4fee\u9970\u6210\u5206\u5bf9\u5bbe\u8bed\u4e0e\u52a8\u8bcd\u7684\u5b9a\u754c\u6709\u6548\u6027\u53ef\u8fbe 99% \u4ee5\u4e0a, \u800c\u91c7\u7528\u7efc\u5408\u6027\u5f62\u5f0f\u6807\u8bb0\u5bf9\u4e3b, \u5bbe\u8bed\u7684\u5b9a\u754c\u53ef\u8fbe\u5230 75% \u4ee5\u4e0a. \u6587\u7ae0\u6700\u540e\u6307\u51fa, \u8981\u5927\u5e45\u63d0\u9ad8\u5224\u5b9a\u53e5\u4e3b\u8bed\u7684\u8bc6\u522b\u7387\u5e94\u8003\u8651\u5229\u7528\u8bc6\u522b\u5bbe\u8bed\u548c\u52a8\u8bcd\u65f6\u6240\u83b7\u53d6\u7684\u53e5\u6cd5\u8bed\u4e49\u7b49\u5176\u4ed6\u4fe1\u606f.", "num_citations": "3\n", "authors": ["1181"]}
{"title": "\u53e5\u5904\u7406\u4e2d\u6392\u6b67\u95ee\u9898\u8865\u8bae\n", "abstract": " \u6392\u6b67\u95ee\u9898\u662f\u53e5\u5904\u7406\u4e2d\u5fc5\u987b\u9762\u5bf9\u7684\u4e00\u4e2a\u91cd\u8981\u95ee\u9898, \u5df1\u6709\u4e00\u4e9b\u5b66\u8005\u8fdb\u884c\u8fc7\u7814\u7a76\u4e0e\u63a2\u7d22, \u63d0\u51fa\u4e86\u591a\u79cd", "num_citations": "3\n", "authors": ["1181"]}
{"title": "Towards a universal continuous knowledge base\n", "abstract": " In artificial intelligence (AI), knowledge is the information required by an intelligent system to accomplish tasks. While traditional knowledge bases use discrete, symbolic representations, detecting knowledge encoded in the continuous representations learned from data has received increasing attention recently. In this work, we propose a method for building a continuous knowledge base (CKB) that can store knowledge imported from multiple, diverse neural networks. The key idea of our approach is to define an interface for each neural network and cast knowledge transferring as a function simulation problem. Experiments on text classification show promising results: the CKB imports knowledge from a single model and then exports the knowledge to a new model, achieving comparable performance with the original model. More interesting, we import the knowledge from multiple models to the knowledge base\u00a0\u2026", "num_citations": "2\n", "authors": ["1181"]}
{"title": "Language models are good translators\n", "abstract": " Recent years have witnessed the rapid advance in neural machine translation (NMT), the core of which lies in the encoder-decoder architecture. Inspired by the recent progress of large-scale pre-trained language models on machine translation in a limited scenario, we firstly demonstrate that a single language model (LM4MT) can achieve comparable performance with strong encoder-decoder NMT models on standard machine translation benchmarks, using the same training data and similar amount of model parameters. LM4MT can also easily utilize source-side texts as additional supervision. Though modeling the source- and target-language texts with the same mechanism, LM4MT can provide unified representations for both source and target sentences, which can better transfer knowledge across languages. Extensive experiments on pivot-based and zero-shot translation tasks show that LM4MT can outperform the encoder-decoder NMT model by a large margin.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "On the Language Coverage Bias for Neural Machine Translation\n", "abstract": " Language coverage bias, which indicates the content-dependent differences between sentence pairs originating from the source and target languages, is important for neural machine translation (NMT) because the target-original training data is not well exploited in current practice. By carefully designing experiments, we provide comprehensive analyses of the language coverage bias in the training data, and find that using only the source-original data achieves comparable performance with using full training data. Based on these observations, we further propose two simple and effective approaches to alleviate the language coverage bias problem through explicitly distinguishing between the source- and target-original training data, which consistently improve the performance over strong baselines on six WMT20 translation tasks. Complementary to the translationese effect, language coverage bias provides another explanation for the performance drop caused by back-translation. We also apply our approach to both back- and forward-translation and find that mitigating the language coverage bias can improve the performance of both the two representative data augmentation methods and their tagged variants.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "Learning to generate explainable plots for neural story generation\n", "abstract": " Story generation is an importantnatural language processing task that aims to generate coherent stories automatically. While the use of neural networks has proven effective in improving story generation, how to learn to generate an explainable high-level plot still remains a major challenge. In this article, we propose a latent variable model for neural story generation. The model treats an outline, which is a natural language sentence explainable to humans, as a latent variable to represent a high-level plot that bridges the input and output. We adopt an external summarization model to guide the latent variable model to learn how to generate outlines from training data. Experiments show that our approach achieves significant improvements over state-of-the-art methods in both automatic and human evaluations.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e\u975e\u5e73\u884c\u8bed\u6599\u7684\u53cc\u8bed\u8bcd\u5178\u6784\u5efa\n", "abstract": " \u57fa\u4e8e\u975e\u5e73\u884c\u8bed\u6599\u7684\u53cc\u8bed\u8bcd\u5178\u6784\u5efa Page 1 \u57fa\u4e8e\u975e\u5e73\u884c\u8bed\u6599\u7684\u53cc\u8bed\u8bcd\u5178\u6784\u5efa \u5f20\u6aac, \u5218\u6d0band \u5b59 \u8302\u677e Citation: \u4e2d\u56fd\u79d1\u5b66: \u4fe1\u606f\u79d1\u5b66; doi: 10.1360/N112017-00256 View online: http://engine.scichina.com/doi/10.1360/N112017-00256 Published by the \u300a\u4e2d\u56fd\u79d1\u5b66\u300b\u6742\u5fd7\u793e Articles you may be interested in \u60c5\u611f\u9690\u55bb\u8bed\u6599\u5e93\u6784\u5efa\u4e0e \u5e94\u7528 \u4e2d\u56fd\u79d1\u5b66: \u4fe1\u606f\u79d1\u5b6645, 1574 (2015); \u57fa\u4e8e\u968f\u673a\u884c\u8d70$\\boldsymbol N$\u6b65\u7684\u6c49\u8bed\u590d\u8ff0\u77ed\u8bed \u83b7\u53d6\u65b9\u6cd5 \u4e2d\u56fd\u79d1\u5b66: \u4fe1\u606f\u79d1\u5b6647, 1066 (2017); \u516c\u4f17\u6587\u672c\u4e4b\u60c5\u611f\u8bcd\u5178\u7814\u7a76\u8fdb\u5c55 \u4e2d\u56fd\u79d1\u5b66: \u4fe1\u606f \u79d1\u5b6644, 825 (2014); \u300a\u7535\u5b50\u5b66\u8bcd\u5178\u300b \u79d1\u5b66\u901a\u62a548, 2337 (2003); \u57fa\u4e8e\u7edf\u8ba1\u5efa\u6a21\u7684\u53ef\u8bad\u7ec3\u5355\u5143\u6311\u9009 \u8bed\u97f3\u5408\u6210\u65b9\u6cd5 \u79d1\u5b66\u901a\u62a554, 1133 (2009); Page 2 SCIENTIA SINICA Informationis \u4e2d)$\u5b66: \u4fe1\u606f $\u5b66 !! !!\"\" \" 1,2,3, 1,2,3*, 1,2,3 1. %\"'\u5b66%%/$\u5b66\u4e0e$=\u7cfb, \u5317.100084 2. ,&$=\u4e0e\u7cfb+)(\u91cd\u70b9,7(%\"'\u5b66) , \u5317.100084 3. %\"\u4fe1\u606f$\u5b66\u4e0e$=)(,7(/) , \u5317.100084 * )\u4fe1-1. E-mail: liuyang2011@tsinghua.edu.cn :)*': ; \"<*': )('%$\u5b66.)\u4f18\u79c0$(\u9879\"( \"/: 61522204) %\u52a9 !\"!$$!!$$!, $ $\" \u2026", "num_citations": "2\n", "authors": ["1181"]}
{"title": "Neural parse combination\n", "abstract": " Analyzing the syntactic structure of natural languages by parsing is an important task in artificial intelligence. Due to the complexity of natural languages, individual parsers tend to make different yet complementary errors. We propose a neural network based approach to combine parses from different parsers to yield a more accurate parse than individual ones. Unlike conventional approaches, our method directly transforms linearized candidate parses into the ground-truth parse. Experiments on the Penn English Treebank show that the proposed method improves over a state-of-the-art parser combination approach significantly.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "\u4ece\u673a\u5668\u7ffb\u8bd1\u5386\u7a0b\u770b\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u7684\u53d1\u5c55\u7b56\u7565\n", "abstract": " \u672c\u6587\u8bd5\u56fe\u4ece\u8d85\u8131\u7ec6\u8282\u7684\u5b8f\u89c2\u89d2\u5ea6, \u5bf9\u673a\u5668\u7ffb\u8bd1\u7684\u53d1\u5c55\u5386\u7a0b\u8fdb\u884c\u627c\u8981\u7684\u603b\u7ed3\u548c\u6df1\u523b\u7684\u8bc4\u4ecb, \u7740\u91cd\u4e8e\u523b\u753b\u5404\u4e2a\u65f6\u671f\u5728\u57fa\u672c\u65b9\u6cd5\u548c\u6838\u5fc3\u6280\u672f\u4e0a\u7684\u4e3b\u8981\u7279\u5f81, \u4ece\u800c\u52fe\u52d2\u51fa\u673a\u5668\u7ffb\u8bd1\u7684\u5168\u8fc7\u7a0b\u6f14\u8fdb\u8109\u7edc. \u5728\u4e0a\u8ff0\u8003\u5bdf\u548c\u5206\u6790\u7684\u57fa\u7840\u4e0a, \u6587\u7ae0\u5bf9\u56fd\u5185\u673a\u5668\u7ffb\u8bd1\u4e43\u81f3\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u7684\u8fd1\u671f\u53d1\u5c55\u7b56\u7565\u63d0\u51fa\u4e86\u82e5\u5e72\u5efa\u8bae.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "Consistency-aware search for word alignment\n", "abstract": " As conventional word alignment search algorithms usually ignore the consistency constraint in translation rule extraction, improving alignment accuracy does not necessarily increase translation quality. We propose to use coverage, which reflects how well extracted phrases can recover the training data, to enable word alignment to model consistency and correlate better with machine translation. This can be done by introducing an objective that maximizes both alignment model score and coverage. We introduce an efficient algorithm to calculate coverage on the fly during search. Experiments show that our consistency-aware search algorithm significantly outperforms both generative and discriminative alignment approaches across various languages and translation models.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "\u96cf\u51e4\u6e05\u58f0\u6850\u82b1\u8def\u2014\u2014\u6e05\u534e\u5927\u5b66\u7684 MOOC \u5b9e\u8df5\n", "abstract": " \u4e00,\u5b66\u5802\u5728\u7ebf\u4e00\u5e74\u6765\u7684\u53d1\u5c55 \u6e05\u534e\u5927\u5b66\u53c2\u4e0eMOOC\u7684\u4e3b\u8981\u7ecf\u5386\u662f:2013\u5e745\u670817\u65e5,\u6e05\u534e\u5927\u5b66\u52a0\u76dfedX;2013\u5e745\u670824\u65e5,\u6e05\u534e\u5927\u5b66\u6210\u7acb\u5927\u89c4\u6a21\u5728\u7ebf\u6559\u80b2\u7814\u7a76\u4e2d\u5fc3:2013\u5e746\u67081\u65e5,edX\u5e73\u53f0\u53d1\u5e03OpenEdX\u5f00\u6e90\u9879\u76ee:2013\u5e746\u67086\u65e5,\u6e05\u534e\u5927\u5b66\u7ec4\u5efa\u201c\u5b66\u5802\u5728\u7ebf\u201d\u7814\u53d1\u56e2\u961f:2013\u5e7410\u670810\u65e5,\u6e05\u534e\u5927\u5b66\u53d1\u5e03\u201c\u5b66\u5802\u5728\u7ebf\u201d\u5e73\u53f0.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "Reduce meaningless words for joint chinese word segmentation and part-of-speech tagging\n", "abstract": " Conventional statistics-based methods for joint Chinese word segmentation and part-of-speech tagging (S&T) have generalization ability to recognize new words that do not appear in the training data. An undesirable side effect is that a number of meaningless words will be incorrectly created. We propose an effective and efficient framework for S&T that introduces features to significantly reduce meaningless words generation. A general lexicon, Wikepedia and a large-scale raw corpus of 200 billion characters are used to generate word-based features for the wordhood. The word-lattice based framework consists of a character-based model and a word-based model in order to employ our word-based features. Experiments on Penn Chinese treebank 5 show that this method has a 62.9% reduction of meaningless word generation in comparison with the baseline. As a result, the F1 measure for segmentation is increased to 0.984.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "Binary tree based Chinese word segmentation\n", "abstract": " Chinese word segmentation is a fundamental task for Chinese language processing. The granularity mismatch problem is the main cause of the errors. This paper showed that the binary tree representation can store outputs with different granularity. A binary tree based framework is also designed to overcome the granularity mismatch problem. There are two steps in this framework, namely tree building and tree pruning. The tree pruning step is specially designed to focus on the granularity problem. Previous work for Chinese word segmentation such as the sequence tagging can be easily employed in this framework. This framework can also provide quantitative error analysis methods. The experiments showed that after using a more sophisticated tree pruning function for a state-of-the-art conditional random field based baseline, the error reduction can be up to 20%.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "\u5728\u7ebf\u6559\u80b2\u9762\u9762\u8c08\n", "abstract": " \u6b63\u5728\u7f8e\u56fd\u9ad8\u7b49\u6559\u80b2\u754c, \u5728\u7ebf\u6559\u5b66\u66fe\u88ab\u89c6\u4e3a\u4e00\u79cd\u4f4e\u7b49\u7684\u6559\u80b2\u6a21\u5f0f. \u4f46\u5982\u4eca, \u4e00\u6d41\u5b66\u5e9c\u90fd\u4e50\u4e8e\u628a\u81ea\u5df1\u7684\u7cbe\u534e\u8bfe\u7a0b\u5236\u4f5c\u6210\u7f51\u7edc\u89c6\u9891, \u4f9b\u5b66\u5b50\u5728\u7ebf\u5206\u4eab. \u5728\u7ebf\u6559\u5b66\u5df2\u6210\u4e3a\u9ad8\u7b49\u6559\u80b2\u53d1\u5c55\u65b0\u8d8b\u52bf, \u6700\u8fd1\u5317\u4eac\u5927\u5b66\u548c", "num_citations": "2\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e\u5bf9\u5076\u5206\u89e3\u7684\u8bcd\u8bed\u5bf9\u9f50\u641c\u7d22\u7b97\u6cd5\n", "abstract": " \u6458! \u8981\"> T $\u00a1\u00a2 m [1> H\u00a3 4!>+,-.'\u00a4\u00a5\u00a6 \u00a7 \u00a9 \u00aa \u00abN\u00ac R\u00ae Q\u00b1h1\u00b2\u00b3&N\u00b5\u00b6:> T\u00b7 A\u00b9\u00ba &% l\u00bb \u00bc \u00bd\u00be1\u00bf! \u00c0\u00c1\u00c2 &\u00c3\u00c3\u00c4I\u00c5\u00c6 1\u00c7@ \u00c8\u00c9! l {\" \u00c1\u00c2\u00ca\u00cb\u00ccM1\u00cd &n X\u00ce \u00cf\u00d0\u00d1\u00d2> \u00d37\u00d41> \u00c1\u00c2 &! \u00d5\u00d6\u00d7 \u00cd 7\u00d4* \u00d8 \u00d9> \u00c5\u00c61\u00da\u00cd! \u00db\u00dcK\u00d4\u00dd\u00de\u00df\u00e0\u00d2\u00e1\u00e2\u00d4&\u00e3\u00d2> \u00d37\u00d4V\u00e4\u00e5\u00e6K\u00d41\u00df\u00e0UA\u00e1\u00e2U! n X \u00ce1\u00c1\u00c2 &T! $$]\u00b6 \u00e7>^ & $ \u00e8> 56 \u00e9\u00eaW\u00bd\u00be\u00eb\u00ec 71E, ddA\u00ed\u00ee\u00ef> 4#!> \u00ca\u00cb \u00f07\u00ee\u00f1\u00f2#;! _A%;% _&", "num_citations": "2\n", "authors": ["1181"]}
{"title": "A comparison study of candidate generation for Chinese word segmentation\n", "abstract": " Chinese word segmentation can be implemented in a coarse-to-fine schema. In such schema, a candidate set containing multiple segmentations of a sentence (rather than only one segmentation) is used as the output of a coarse-grained CWS model. Then a more sophisticated CWS model or other models of downstream tasks will reconsider all the segmentations in the candidate set to determine the best segmentation. This paper discussed and compared three candidate generation methods, namely boundary level method, word level method and sentence level method, in a unified form. The oracle F1-measures of the candidate sets of these methods were compared. The performances were also compared in a joint CWS and POS-tagging task. The results showed that the word level method has the best performance among these three candidate generation methods. Results also showed that the coarse-to-fine\u00a0\u2026", "num_citations": "2\n", "authors": ["1181"]}
{"title": "\u4e2d\u6587\u8bed\u4e49\u4f9d\u5b58\u6811\u5e93\u6784\u5efa\u53ca\u81ea\u52a8\u5206\u6790\u6280\u672f\n", "abstract": " \u5317\u4eac\u57ce\u5e02\u5b66\u9662\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u6240, \u5317\u4eac 100083; \u5317\u4eac\u5927\u5b66\u8ba1\u7b97\u8bed\u8a00\u5b66\u6559\u80b2\u90e8\u91cd\u70b9\u5b9e\u9a8c\u5ba4, \u5317\u4eac 100871 E-mai:{yqshao. nanyangcx, naOning}@ bcueducngqk@ pkueducn \u6458\u8981: \u8bed\u4e49\u4f9d\u5b58\u5206\u6790\u662f\u4e00\u79cd\u5bf9\u53e5\u5b50\u8fdb\u884c\u6df1\u5c42\u8bed\u4e49\u5206\u6790\u7684\u6280\u672f. \u8bed\u4e49\u4f9d\u5b58\u6811\u5e93\u662f\u4f9d\u5b58\u5206\u6790\u7684\u57fa\u7840. \u672c\u6587\u7efc\u5408\u4e86\u4e0d\u540c\u5b66\u8005\u5b9a\u4e49\u7684\u6c49\u8bed\u8bed\u4e49\u5173\u7cfb\u4f53\u7cfb, \u9762\u5411\u8bed\u4e49\u5206\u6790\u7684\u5b9e\u9645\u5e94\u7528, \u8bbe\u8ba1\u4e86\u4e00\u5957\u8bed\u4e49\u5173\u7cfb\u4f53\u7cfb, \u8be5\u4f53\u7cfb\u4e2d\u9664\u4e86\u5e38\u89c4\u7684\u8bed\u4e49\u5173\u7cfb\u5b9a\u4e49, \u5bf9\u5b9a\u8bed\u52a0\u4e2d\u5fc3\u8bed\u7684\u77ed\u8bed\u5185\u90e8\u6d89\u53ca\u5230\u7684\u8bed\u4e49\u5173\u7cfb\u8fdb\u884c\u4e86\u66f4\u8be6\u7ec6\u7684\u5b9a\u4e49. \u540c\u65f6, \u4f9d\u636e\u6b64\u5173\u7cfb\u4f53\u7cfb, \u91c7\u7528\u81ea\u52a8\u548c\u624b\u5de5\u76f8\u7ed3\u5408\u7684\u65b9\u5f0f\u5efa\u7acb\u4e86\u5927\u89c4\u6a21\u7684\u6c49\u8bed\u8bed\u4e49\u4f9d\u5b58\u5173\u7cfb\u6811\u5e93, \u5e76\u5728\u6b64\u6811\u5e93\u7684\u57fa\u7840\u4e0a\u6784\u9020\u4e86\u8bed\u4e49\u4f9d\u5b58\u6807\u6ce8\u6a21\u578b.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "A local generative model for chinese word segmentation\n", "abstract": " This paper presents a local generative model for Chinese word segmentation, which has faster learning process than discriminative models and can do unsupervised learning. It has the ability to make use of larger resources. In this model, four successive characters are used to determine whether a character interval should be a word boundary or not. The Gibbs sampling algorithm, as well as three additional rules, is applied for the unsupervised learning. Besides words, the word candidates that are generated by our model can improve the performance of Chinese information retrieval. The experiments show that in supervised learning our method outperforms a language model based method. And the performance on one corpus is better than the best one reported in SIGHAN bakeoff 05. In unsupervised learning, our method achieves the comparable performance compared to the state-of-the-art method.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "Incorporate web search technology to solve out-of-vocabulary words in Chinese word segmentation\n", "abstract": " Chinese word segmentation (CWS) is the fundamental technology for many NLP-related applications. It is reported that more than 60% of segmentation errors is caused by the out-of-vocabulary (OOV) words. Recent studies in CWS show that, statistical machine learning method is, to some extent, effective on solving OOV words. But labeled data is limited in size and unbalanced in content which makes it impossible to obtain all the required knowledge to recognize OOV words. In this paper, large scaled web data is incorporated as knowledge supplement. A framework which combines using web search technology and machine learning method is proposed. For each sentence, basic segmentation is performed using linear-chain Conditional Random Fields (CRF) model. Substrings which CRF model gives low confidence decisions are extracted and sent to search engine to perform web search based word segmentation. Final decision is made by considering both CRF model based segmentation result and that of web search based result. Evaluations are conducted on SIGHAN Bakeoff 2005 and 2006 datasets, showing the effectiveness of the proposed framework on dealing with OOV words.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "\u9996\u90fd\u5e73\u9762\u5a92\u4f53\u7528\u5b57\u7528\u8bed\u72b6\u51b5\u8c03\u67e5\n", "abstract": " \u8bed\u8a00\u751f\u6d3b\u662f\u793e\u4f1a\u751f\u6d3b\u7684\u4e00\u4e2a\u4e0d\u53ef\u6216\u7f3a\u7684\u7ec4\u6210\u90e8\u5206, \u5b83\u4e00\u76f4\u500d\u53d7\u8bed\u8a00\u5b66\u4e13\u5bb6, \u5b66\u8005\u7684\u5173\u6ce8.\u201c\u4ece\u8bed\u8a00\u751f\u6d3b\u7684\u5386\u53f2\u8fdb\u7a0b\u770b, \u4eba\u7c7b\u8bed\u8a00\u751f\u6d3b\u7684\u53d1\u5c55\u8282\u594f\u8d8a\u6765\u8d8a\u5feb\u201d, \u8fd9\u5df2\u7ecf\u6210\u4e3a\u8bb8\u591a\u5b66\u8005\u7684\u5171\u8bc6 (\u738b\u5747, 2000 \u674e\u5b87\u660e, 2004 \u738b\u94c1\u7405, 2007 \u5468\u6709\u5149, 2008). \u4e3a\u4e86\u52a8\u6001\u5730\u5bf9\u8bed\u8a00\u751f\u6d3b\u7684 \u201c\u5b9e\u6001\u201d \u8fdb\u884c\u8c03\u7814\u4e0e\u603b\u7ed3, \u56fd\u5bb6\u8bed\u8a00\u76d1\u6d4b\u4e0e\u7814\u7a76\u4e2d\u5fc3\u8bfe\u9898\u7ec4\u5df2\u7ecf\u8fde\u7eed\u53d1\u5e03\u4e86\u4e09\u4e2a\u5e74\u5ea6\u7684\u300a \u4e2d\u56fd\u8bed\u8a00\u751f\u6d3b\u72b6\u51b5\u62a5\u544a\u300b, \u672c\u6587\u5c1d\u8bd5\u5728\u6b64\u601d\u60f3\u7684\u6307\u5bfc\u4e0b, \u4ee5\u9996\u90fd\u5730\u533a\u4e3a\u5207\u5165\u70b9, \u4f9d\u636e\u5927\u89c4\u6a21\u771f\u5b9e\u8bed\u6599 (\u5e73\u9762\u5a92\u4f53) \u5bf9\u5176\u7528\u5b57\u7528\u8bed\u60c5\u51b5\u8fdb\u884c\u8c03\u67e5\u5206\u6790, \u4ece\u4e2d\u63a2\u7a76\u9996\u90fd\u5730\u533a\u7684\u8bed\u8a00\u751f\u6d3b\u7279\u8272.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e\u4e13\u4e1a\u9886\u57df\u5e73\u884c\u8bed\u6599\u7684\u53cc\u8bed\u6838\u5fc3\u672f\u8bed\u62bd\u53d6\u7814\u7a76\n", "abstract": " \u53cc\u8bed\u672f\u8bed\u62bd\u53d6\u5728\u53cc\u8bed\u672f\u8bed\u8bcd\u5178\u7f16\u64b0, \u53cc\u8bed\u672c\u4f53\u6784\u5efa, \u673a\u5668\u7ffb\u8bd1\u4ee5\u53ca\u8de8\u8bed\u8a00\u4fe1\u606f\u68c0\u7d22\u4e2d\u5177\u6709\u91cd\u8981\u7684\u4f5c\u7528. \u5176\u4e2d, \u53cc\u8bed\u6838\u5fc3\u672f\u8bed\u662f\u53cc\u8bed\u672f\u8bed\u8bc6\u522b\u548c\u62bd\u53d6\u7684\u5173\u952e\u8d44\u6e90\u4e4b\u4e00. \u672c\u6587\u5c06\u4e13\u4e1a\u9886\u57df\u6587\u6863\u7684\u5173\u952e\u8bcd\u4f5c\u4e3a\u5019\u9009\u6838\u5fc3\u672f\u8bed, \u5229\u7528\u4e2d\u6587\u548c\u82f1\u6587\u7684\u4e13\u4e1a\u9886\u57df\u5206\u7c7b\u8bed\u6599, \u901a\u8fc7\u5173\u952e\u8bcd\u62bd\u53d6, \u672f\u8bed\u5ea6\u8ba1\u7b97\u7b49\u5173\u952e\u6280\u672f, \u5206\u522b\u8fdb\u884c\u4e2d\u6587\u548c\u82f1\u6587\u7684\u6838\u5fc3\u672f\u8bed\u7684\u8bc6\u522b. \u63a5\u7740, \u4ee5\u4e2d\u82f1\u6587\u4e13\u4e1a\u9886\u57df\u5e73\u884c\u8bed\u6599\u4e3a\u57fa\u7840, \u901a\u8fc7\u53cc\u8bed\u5bf9\u9f50\u6280\u672f, \u81ea\u52a8\u751f\u6210\u4e2d\u82f1\u6587\u5bf9\u7167\u7684\u53cc\u8bed\u6838\u5fc3\u672f\u8bed\u5217\u8868. \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e, \u6bcf\u4e2a\u4e13\u4e1a\u9886\u57df\u4e2d, \u524d 200 \u5bf9\u4e2d\u82f1\u6587\u5bf9\u7167\u6838\u5fc3\u672f\u8bed\u7684\u5e73\u5747\u6b63\u786e\u7387\u5728 50% \u4ee5\u4e0a, \u4e2a\u522b\u9886\u57df\u6b63\u786e\u7387\u8fbe 80% \u5de6\u53f3.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e\u7528\u6237\u67e5\u8be2\u65e5\u5fd7\u548c\u951a\u6587\u5b57\u7684\u6c49\u8bed\u7f29\u7565\u8bed\u8bc6\u522b\n", "abstract": " \u7f29\u7565\u8bed\u662f\u81ea\u7136\u8bed\u8a00\u7684\u5e38\u89c1\u73b0\u8c61\u4e4b\u4e00, \u5176\u76f8\u5173\u7814\u7a76\u662f\u4e2d\u6587\u4fe1\u606f\u5904\u7406\u9886\u57df\u7684\u91cd\u8981\u7814\u7a76\u8bfe\u9898. \u672c\u6587\u9488\u5bf9\u7f29\u7565\u8bed\u7684\u81ea\u52a8\u8bc6\u522b\u95ee\u9898, \u91c7\u7528\u7528\u6237\u67e5\u8be2\u65e5\u5fd7\u548c\u951a\u6587\u5b57\u6587\u4ef6, \u8fd0\u7528\" \u540c\u7f51\u7ad9\u4e3b\u9898\u76f8\u5173\u6027\"(\u5373\u5bf9\u5e94\u7684 url \u6307\u5411\u540c\u4e00\u7f51\u7ad9\u7684\u67e5\u8be2\u8bcd\u8f83\u4e3a\u76f8\u5173) \u7684\u601d\u60f3\u8fdb\u884c\u521d\u6b65\u7684\u7f29\u7565\u8bed, \u6e90\u77ed\u8bed\u5bf9\u7684\u62bd\u53d6, \u7136\u540e\u91c7\u7528\u4e00\u7cfb\u5217\u8fc7\u6ee4\u89c4\u5219, \u7ed3\u5408\u5206\u8bcd\u6309\u7167\u7f29\u7565\u8bed\u7684\u5f62\u6210\u65b9\u5f0f\u8fdb\u884c\u5206\u7c7b, \u6700\u540e\u8c03\u7528\u641c\u7d22\u5f15\u64ce\u91c7\u7528\u591a\u7b56\u7565\u6765\u8bc6\u522b\u7f29\u7565\u8bed, \u6e90\u77ed\u8bed\u5bf9. \u76f8\u6bd4\u524d\u4eba\u7814\u7a76, \u6211\u4eec\u7684\u5b9e\u9a8c\u5728\u89c4\u6a21\u548c\u51c6\u786e\u7387\u4e0a\u90fd\u6709\u63d0\u5347, \u5176\u4e2d\u7528\u6237\u67e5\u8be2\u65e5\u5fd7\u7684\u51c6\u786e\u7387\u4e3a 68.33%, \u951a\u6587\u5b57\u7684\u51c6\u786e\u7387\u4e3a 92.66%.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "Full-reference quality diagnosis for video summary\n", "abstract": " As video summarization techniques have attracted more and more attention for efficient multimedia data management, objective quality assessment of video summary is desired. To address the lack of automatic evaluation techniques, this paper proposes a 3C-diagnosis algorithm to diagnose the video summary from the perspective of coverage, conciseness, and coherence. The candidate summary is first aligned against the reference summary. Then the coverage of the candidate summary is calculated according to the information bearing of the matcThing frames and the information loss of the missing frames. The conciseness is calculated based on the unwanted information contained in the candidate summary, and the coherence is calculated based on the ratio of the appearances of the frame loss for the aligned candidate summary. The proposed techniques are experimented on a standard dataset of\u00a0\u2026", "num_citations": "2\n", "authors": ["1181"]}
{"title": "\u8bb2\u5e2d\u6559\u6388\u7ec4\u673a\u5236\u662f\u57f9\u517b\u8ba1\u7b97\u673a\u79d1\u5b66\u4e0e\u6280\u672f\u62d4\u5c16\u521b\u65b0\u4eba\u624d\u7684\u73b0\u5b9e\u624b\u6bb5\n", "abstract": " \u6b63\u6e05\u534e\u5927\u5b66\u6b63\u5728\u4e3a\u5b9e\u73b0\u5efa\u8bbe\u4e16\u754c\u4e00\u6d41\u5927\u5b66\u7684\u5b8f\u4f1f\u76ee\u6807\u800c\u594b\u6597. \u6839\u636e\u5b66\u6821\u5236\u5b9a\u7684\u53d1\u5c55\u6218\u7565, \u5230 2011 \u5e74, \u5b66\u6821\u5e94\u529b\u4e89\u8dfb\u8eab\u4e16\u754c\u4e00\u6d41\u5927\u5b66\u884c\u5217, \u5230 2020 \u5e74, \u5e94\u52aa\u529b\u5728\u603b\u4f53\u4e0a\u8fbe\u5230\u4e16\u754c\u4e00\u6d41\u5927\u5b66\u6c34\u5e73. \u663e\u7136, \u76ee\u524d\u6b63\u5904\u5728\u8fd9\u4e2a\u53d1\u5c55\u6218\u7565\u5b9e\u65bd\u7684\u4e00\u4e2a\u5173\u952e\u65f6\u671f.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "Multi-modal multi-label semantic indexing of images based on hybrid ensemble learning\n", "abstract": " Automatic image annotation (AIA) refers to the association of words to whole images which is considered as a promising and effective approach to bridge the semantic gap between low-level visual features and high-level semantic concepts. In this paper, we formulate the task of image annotation as a multi-label multi class semantic image classification problem and propose a simple yet effective method: hybrid ensemble learning framework in which multi-label classifier based on uni-modal features and ensemble classifier based on bi-modal features are integrated into a joint classification model to perform multi-modal multi-label semantic image annotation. We conducted experiments on two commonly-used keyframe and image collections: MediaMill and Scene dataset including about 40,000 examples. The empirical studies demonstrated that the proposed hybrid ensemble learning method can enhance\u00a0\u2026", "num_citations": "2\n", "authors": ["1181"]}
{"title": "\u751f\u7269\u533b\u5b66\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u7279\u5f81\u9009\u53d6\u4e0e\u8bc4\u4ef7\n", "abstract": " E-maikhcwang@ mdabhitedu. cn \u6458\u8981: \u672c\u6587\u7814\u7a76\u7684\u4e3b\u8981\u76ee\u7684\u662f\u9488\u5bf9\u751f\u7269\u533b\u5b66\u547d\u540d\u5b9e\u4f53\u7684\u7279\u70b9\u5f00\u53d1\u4f7f\u7528\u6709\u6548\u7684\u7279\u5f81\u96c6\u5408, \u5e76\u7ed3\u5408\u4f7f\u7528\u4e0d\u540c\u7684\u4fe1\u606f\u8d44\u6e90, \u4ee5\u63d0\u9ad8\u57fa\u4e8e\u7279\u5f81\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8bc6\u522b\u751f\u7269\u533b\u5b66\u547d\u540d\u5b9e\u4f53\u7684\u6027\u80fd. \u672c\u6587\u8be6\u7ec6\u5206\u6790\u4e86\u5404\u79cd\u7279\u5f81\u5305\u62ec\u5c40\u90e8\u7279\u5f81, \u5168\u6587\u7279\u5f81\u53ca\u5916\u90e8\u8d44\u6e90\u7279\u5f81\u5bf9\u57fa\u4e8e\u6761\u4ef6\u968f\u673a\u57df\u6a21\u578b\u7684\u751f\u7269\u533b\u5b66\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7cfb\u7edf\u7684\u8d21\u732e. \u7cfb\u7edf\u4f7f\u7528 JNLPBA \u8bed\u6599\u4f5c\u4e3a\u8bc4\u6d4b\u8bed\u6599, \u83b7\u5f97\u4e86 720% \u7684 F \u6d4b\u5ea6\u503c, \u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u672c\u6587\u9488\u5bf9\u751f\u7269\u533b\u5b66\u9886\u57df\u547d\u540d\u5b9e\u4f53\u7279\u70b9\u800c\u9009\u62e9\u7684\u7279\u5f81\u5bf9\u4e8e\u8bc6\u522b\u4efb\u52a1\u662f\u975e\u5e38\u6709\u6548\u7684.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "\u9762\u5411\u8bed\u8a00\u5904\u7406\u7684\u5355\u53e5\u53e5\u578b\u53e5\u6a21\u5bf9\u5e94\u5173\u7cfb\u7814\u7a76\u2014\u2014\u57fa\u4e8e\u6807\u6ce8\u8bed\u6599\u5e93\u7684\u5b9a\u91cf\u8003\u5bdf\n", "abstract": " \u6587\u7ae0\u9996\u5148\u4ecb\u7ecd\u4e86\u53e5\u5b50\u53e5\u6cd5,\u8bed\u4e49\u7ed3\u6784\u6807\u6ce8\u8bed\u6599\u5e93\u7684\u57fa\u672c\u60c5\u51b5.\u5728\u6807\u6ce8\u8bed\u6599\u5e93\u7684\u57fa\u7840\u4e0a.\u4ece\u53e5\u578b\u7684\u89d2\u5ea6\u7740\u773c\u5b9a\u91cf\u7edf\u8ba1,\u6784\u5efa\u73b0\u4ee3\u6c49\u8bed\u5355\u53e5\u7684\u53e5\u578b\u53e5\u6a21\u7684\u5bf9\u5e94\u4f53\u7cfb,\u5e76\u8fdb\u4e00\u6b65\u8bf4\u660e\u4e86\u53e5\u578b\u53e5\u6a21\u5bf9\u5e94\u5173\u7cfb\u7814\u7a76\u7684\u610f\u4e49.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "Merging case relations into VSM to improve information retrieval precision\n", "abstract": " This paper presents an approach that merges case relations into the well-known Vector Space Model (VSM), leading to a new model named C-VSM (Case relation-based VSM). A Chinese case system with 23 case relations is established, and a Chinese Olympic news corpus of 7,662 sentences, denoted COCS, is constructed by manual annotation with these 23 case relations. We use 50 queries on COCS as a test set. Experimental results on the test set show that C-VSM outperforms W-VSM (Word-based VSM) by 3.4% on the average 11-point precision. It is worth pointing out that almost all the previous studies on semantic IR obtained no better, even worse, results than W-VSM, our work thus validates the usefulness of case relations in IR through the validation is still preliminary. The proposed model is believed to be language-independent.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "\u73b0\u4ee3\u85cf\u8bed\u540d\u8bcd\u7ec4\u5757\u7684\u7c7b\u578b\u53ca\u5f62\u5f0f\u6807\u8bb0\u7279\u5f81\n", "abstract": " \u672c\u6587\u9488\u5bf9\u73b0\u4ee3\u85cf\u8bed\u540d\u8bcd\u7ec4\u5757\u7684\u6784\u6210\u4e0e\u7ed3\u6784, \u5bf9\u540d\u8bcd\u7ec4\u5757\u505a\u4e86\u521d\u6b65\u5b9a\u4e49, \u5e76\u6839\u636e\u540d\u8bcd\u7ec4\u5757\u7684\u53e5\u6cd5\u529f\u80fd\u5f00\u5c55\u4e86\u540d\u8bcd\u7ec4\u5757\u7684\u5206\u7c7b\u7814\u7a76, \u63d0\u51fa\u540d\u8bcd\u7ec4\u5757\u7684\u5f62\u5f0f\u6807\u8bb0\u53ef\u5206\u4e3a\u4e09\u79cd\u7c7b\u578b, \u4e00\u662f\u8bcd\u683c, \u6d3e\u751f\u8bcd\u7f00, \u540d\u7269\u5316\u6807\u8bb0\u7b49\u663e\u6027\u6807\u8bb0, \u4e8c\u662f\u4ee3\u8bcd, \u6570\u8bcd, \u6307\u793a\u8bcd\u7b49\u53ef\u7a77\u5c3d\u6027\u8bcd\u7c7b\u4f5c\u4e3a\u81ea\u6307\u6807\u8bb0, \u4e09\u662f\u540d\u8bcd, \u5f62\u5bb9\u8bcd\u7b49\u6784\u8bcd\u8bcd\u7f00\u4f5c\u4e3a\u9690\u6027", "num_citations": "2\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e\u6807\u6ce8\u8bed\u6599\u5e93\u7684\u73b0\u4ee3\u6c49\u8bed\u5355\u53e5\u53e5\u578b\u53e5\u6a21\u7684\u5bf9\u5e94\u5173\u7cfb\u7814\u7a76\n", "abstract": " \u672c\u6587\u4ecb\u7ecd\u4e86\u6807\u6ce8\u53e5\u5b50\u53e5\u6cd5, \u8bed\u4e49\u7ed3\u6784\u8bed\u6599\u5e93\u7684\u57fa\u672c\u60c5\u51b5, \u5728\u6807\u6ce8\u8bed\u6599\u5e93\u7684\u57fa\u7840\u4e0a, \u4ece\u53e5\u578b\u7684\u89d2\u5ea6\u7740", "num_citations": "2\n", "authors": ["1181"]}
{"title": "\u73b0\u4ee3\u6c49\u8bed\u591a\u97f3\u8bcd\u81ea\u52a8\u6807\u97f3\u7814\u7a76\n", "abstract": " \u6c49\u8bed\u591a\u97f3\u8bcd\u81ea\u52a8\u6807\u97f3\u7684\u7814\u7a76\u53ef\u4ee5\u4e3a\u8bed\u97f3\u5408\u6210\u53ca\u6c49\u8bed\u6559\u5b66\u7b49\u9886\u57df\u670d\u52a1. \u672c\u6587\u91c7\u7528 N \u5143\u6a21\u578b\u7684\u7edf\u8ba1\u65b9\u6cd5\u5bf9\u73b0\u4ee3\u6c49\u8bed\u4e2d\u7684\u591a\u97f3\u8bcd\u8fdb\u884c\u81ea\u52a8\u6807\u97f3, \u901a\u8fc7\u5f15\u5165\u76f8\u4f3c\u8bcd, \u91c7\u53d6 \u201c\u8bcd\u5f62-\u76f8\u4f3c\u8bcd-\u8bcd\u6027\u201d \u4e09\u6b65\u56de\u9000\u7684\u7b56\u7565, \u7f13\u89e3\u4e86\u6570\u636e\u7a00\u758f\u95ee\u9898, \u5b9e\u9a8c\u7ed3\u679c\u8bf4\u660e\u8fd9\u4e2a\u65b9\u6cd5\u662f\u6709\u610f\u4e49\u7684.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "Select strong information features to improve text categorization effectiveness\n", "abstract": " ObjectiveThe Journal of Intelligent Systems provides readers with a compilation of stimulating and up-to-date articles within the field of intelligent systems. The focus of the journal is on high quality research that addresses paradigms, development, applications and implications in the field of intelligent systems. The Journal of Intelligent Systems is a peer reviewed journal. Special issues can be arranged by contacting the Editor.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "Integrated Chinese word segmentation and part-of-speech tagging based on the divide-and-conquer strategy\n", "abstract": " In this paper, various ways of integration of Chinese word segmentation and part-of-speech tagging, including the so-called true-integration and pseudo-integration, are tested and compared based on a test corpus consisting of 367,114 Chinese characters. A novel true-integration approach, named 'the divide-and-conquer integration', is originally proposed. Preliminary experiments show that this true integration achieves 98.72% accuracy of word segmentation, 95.65% accuracy of part-of-speech tagging, and 94.43% accuracy of word segmentation and part-of-speech tagging, outperforming all other kinds of combinations to some extent (though not very significant). The results demonstrate the potential for further improving the performance of Chinese word segmentation and part-of-speech tagging.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e\u683c\u5173\u7cfb\u548c\u914d\u4ef7\u7684\u85cf\u8bed\u52a8\u8bcd\u518d\u5206\u7c7b\u7814\u7a76\n", "abstract": " \u672c\u6587\u7740\u91cd\u4ecb\u7ecd\u4e86\u6211\u4eec\u6839\u636e\u7814\u5236\u85cf\u6c49\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u7684\u5b9e\u9645\u9700\u8981, \u5728\u91c7\u7528\u4f20\u7edf\u8bed\u6cd5\u5bf9\u85cf\u8bed\u52a8\u8bcd\u7684\u5df1\u6709\u5206\u7c7b\u57fa\u7840\u4e0a, \u91c7\u7eb3\u683c\u8bed\u6cd5\u548c\u914d\u4ef7\u7406\u8bba\u7684\u5408\u7406\u601d\u60f3, \u7ed3\u5408\u85cf\u8bed\u7684\u683c\u63a5\u7eed\u7279\u5f81, \u901a\u8fc7\u6bcf\u4e2a\u52a8\u8bcd\u7684\u914d\u4ef7\u4fe1\u606f\u4ee5\u53ca\u6240\u80fd\u643a\u5e26\u7684\u76f8\u5e94\u7684\u683c\u52a9\u8bcd\u7c7b\u578b (\u683c\u6807) \u53ca\u5176\u6570\u91cf\u6765\u5bf9\u85cf\u8bed\u52a8\u8bcd\u8fdb\u884c\u518d\u5206\u7c7b, \u4ece\u800c\u5f62\u6210\u4e00\u4e2a\u96c6\u8bed\u6cd5\u8bed\u4e49\u4e3a\u4e00\u4f53\u7684\u85cf\u8bed\u52a8\u8bcd\u518d\u5206\u7c7b\u6846\u67b6.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "\u5b9a\u8bed\u7c7b\u578b\u548c\u69fd\u5173\u7cfb\u7c7b\u578b\u7684\u5bf9\u5e94\u53ca\u5176\u5bf9\u540d\u8bcd\u8bed\u4e49\u5206\u6790\u7684\u4f5c\u7528\n", "abstract": " \u6458\u8981\u5728\u540d\u8bcd\u77ed\u8bed\u4e2d\u5b9a\u8bed\u7684\u7c7b\u578b\u548c\u540d\u8bcd\u6816\u5173\u7cfb\u7814\u7a76\u6210\u679c\u7684\u57fa\u7840\u4e0a, \u9996\u5148\u7814\u7a76\u4e86\u5404\u79cd\u7c7b\u578b\u7684\u5b9a\u8bed\u4e0e\u535a\u5173\u7cfb\u7c7b\u578b\u7684\u5bf9", "num_citations": "2\n", "authors": ["1181"]}
{"title": "\u4e2d\u6587\u8bed\u8a00\u8d44\u6e90\u8054\u76df\u7684\u5efa\u8bbe\u548c\u53d1\u5c55\n", "abstract": " \u6458\u8981 \u8bed\u8a00\u8d44\u6e90\u7684\u5efa\u8bbe\u662f\u8bed\u8a00\u4fe1\u606f\u5904\u7406\u9886\u57df\u7684\u91cd\u8981\u57fa\u7840. \u672c\u6587\u4ecb\u7ecd\u4e86\u5728 973 \u8ba1\u5212\u7279\u522b\u4e13\u9879 \u201c\u4e2d\u6587\u8bed\u6599\u5e93\u5efa\u8bbe\u201d \u7684\u652f\u6301\u4e0b\u5efa\u7acb\u7684\u4e2d\u6587\u8bed\u8a00\u8d44\u6e90\u8054\u76df (ChineseLDC) \u7684\u60c5\u51b5, \u5305\u62ec\u8bed\u8a00\u8d44\u6e90\u5efa\u8bbe, \u89c4\u8303\u548c\u6807\u51c6\u7684\u5236\u8ba2\u4ee5\u53ca\u7ba1\u7406\u673a\u5236\u7684\u5efa\u7acb\u7b49\u65b9\u9762. \u8fdb\u4e00\u6b65\u5bf9 ChineseLDC \u5728\u8bed\u8a00\u8d44\u6e90\u5efa\u8bbe\u548c\u8bc4\u6d4b\u673a\u5236\u5efa\u7acb\u65b9\u9762\u7684\u4e0b\u4e00\u6b65\u53d1\u5c55\u89c4\u5212\u8fdb\u884c\u4e86\u9610\u8ff0.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e\u6807\u6ce8\u8bed\u6599\u5e93\u7684\u73b0\u4ee3\u6c49\u8bed\u53e5\u5b50\u8bed\u4e49\u7ed3\u6784\u7814\u7a76\n", "abstract": " \u6211\u4eec\u5bf9\u5305\u542b 228960 \u4e2a\u53e5\u5b50, \u8ba1 420 \u4e07\u5b57\u7684\u300a \u4eba\u6c11\u65e5\u62a5\u300b \u8bed\u6599\u5e93\u8fdb\u884c\u4e86\u4eba\u5de5\u8bed\u4e49\u6807\u6ce8, \u4ece\u4e2d\u5f52\u7eb3\u51fa\u4e0d\u540c\u7684\u73b0\u4ee3\u6c49\u8bed\u53e5\u5b50\u8bed\u4e49\u7ed3\u6784\u5171 3542 \u79cd. \u8fdb\u4e00\u6b65\u7ed9\u51fa\u4e86\u5176\u5206\u5e03, \u5e76\u9610\u8ff0\u4e86\u6807\u6ce8\u53e5\u5b50\u8bed\u4e49\u7ed3\u6784\u7684\u610f\u4e49.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "\u6c49\u8bed\u90e8\u5206\u5206\u6790\u7814\u7a76\n", "abstract": " \u672c\u6587\u6982\u8981\u4ecb\u7ecd\u4e86\u8fd1\u5e74\u6765\u6211\u4eec\u5728\u6c49\u8bed\u90e8\u5206\u5206\u6790\u65b9\u9762\u7684\u7814\u7a76\u5de5\u4f5c, \u5305\u62ec\u8bbe\u8ba1\u90e8\u5206\u5206\u6790\u548c\u6807\u6ce8\u4f53\u7cfb, \u6784\u5efa\u5927\u89c4\u6a21\u7684\u90e8\u5206\u4fe1\u606f\u6807\u6ce8\u8bed\u6599\u5e93, \u63a2\u7d22\u4e0d\u540c\u5c42\u6b21\u7684\u90e8\u5206\u5206\u6790\u65b9\u6cd5\u7b49, \u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b\u5e94\u7528\u8bbe\u60f3.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "\u201c\u53d6\u51b3\u201d \u4e0e \u201c\u6765\u6e90\n", "abstract": " <\u6b63> \u611f\u89c9\u4e0a,\"\u53d6\u51b3\"\u662f\u52a8\u8bcd,\u4f46\u4f3c\u4e4e\u5f88\u96be\u5355\u72ec\u51fa\u73b0\u4e8e\u53e5\u5b50\u4e2d,\u5fc5\u987b\u540e\u52a0\"\u4e8e\"\u5b57\u624d\u80fd\u5b9e\u73b0\u5176\u52a8\u8bcd\u7684\u7528\u6cd5;\"\u6765\u6e90\"\u6709\u4e24\u4e2a\u4e49\u9879:\u4e00\u4e2a\u4e49\u9879\u662f\u540d\u8bcd,\u53e6\u5916\u4e00\u4e2a\u4e49\u9879\u50cf\u662f\u52a8\u8bcd,\u540c\"\u53d6\u51b3\"\u4e00\u6837,\u4e5f\u8981\u6c42\u540e\u52a0\"\u4e8e\"\u5b57.\u67e5\u9605\u4e86\u300a\u73b0\u4ee3\u6c49\u8bed\u8bcd\u5178\u300b(\u4fee\u8ba2\u672c)(\u7b80\u79f0\u300a\u73b0\u6c49\u300b)\u4e0a\u7684\u76f8\u5173\u91ca\u6587:[\u53d6\u51b3]\u7531\u67d0\u65b9\u9762\u6216\u67d0\u79cd\u60c5\u51b5\u51b3\u5b9a(\u540e\u9762\u591a\u8ddf\u6709\u2018\u4e8e\u2019\u5b57):\u6210\u7ee9\u7684\u5927\u5c0f~\u4e8e\u52aa\u529b\u7684\u7a0b\u5ea6.[\u6765\u6e90]\u2460\u4e8b\u7269\u6240\u4ece\u6765\u7684\u5730\u65b9;\u4e8b\u7269\u7684\u6839\u6e90:\u7ecf\u6d4e~.\u2461(\u4e8b\u7269)\u8d77\u6e90;\u53d1\u751f(\u540e\u9762\u8ddf\u2018\u4e8e\u2019):\u795e", "num_citations": "2\n", "authors": ["1181"]}
{"title": "\u5c40\u90e8\u7edf\u8ba1\u5728\u6c49\u8bed\u672a\u767b\u5f55\u8bcd\u8fa8\u8bc6\u4e2d\u5e94\u7528\u548c\u5b9e\u73b0\u65b9\u6cd5\u2019\n", "abstract": " \u771f\u5b9e\u6587\u672c\u4e2d\u7684\u672a\u767b\u5f55\u8bcd\u8fa8\u8bc6\u662f\u4e2d\u6587\u4fe1\u606f\u5904\u7406\u4e2d\u7684\u65b0\u95ee\u9898. \u672c\u6587\u8ba8\u8bba\u4e86\u6587\u672c\u5c40\u90e8\u7edf\u8ba1\u5728\u6c49\u8bed\u672a\u767b\u5f55\u8bcd\u8fa8\u8bc6\u4e2d\u7684\u5e94\u7528, \u63a2\u8ba8\u4e86\u5c40\u90e8\u7f13\u51b2\u5927\u5c0f\u548c\u672a\u767b\u5f55\u8bcd\u8fa8\u8bc6\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb, \u5e76\u7ed9\u51fa\u4e86\u4e00\u79cd\u5c40\u90e8\u7edf\u8ba1\u7684\u5b9e\u73b0\u65b9\u6cd5. \u5b9e\u9a8c\u8868\u660e, \u5bf9\u4e8e\u672a\u767b\u5f55\u8bcd\u6bd4\u8f83\u5bc6\u96c6\u7684\u771f\u5b9e\u6587\u672c, \u5c40\u90e8\u7edf\u8ba1\u53ef\u4ee5\u8fa8\u8bc6\u51fa\u4e00\u534a\u4ee5\u4e0a\u7684\u672a\u767b\u5f55\u8bcd. \u7cfb\u7edf\u4f5c\u4e3a\u6e05\u534e\u5927\u5b66\u5206\u8bcd\u548c\u8bcd\u6027\u6807\u6ce8\u7cfb\u7edf SegTag \u7684\u4e00\u4e2a\u5b50\u7cfb\u7edf, \u5904\u7406\u4e86\u4e00\u5343\u4e94\u767e\u4e07\u5b57\u7684\u65b0\u534e\u793e\u901a\u8baf\u7a3f, \u663e\u8457\u63d0", "num_citations": "2\n", "authors": ["1181"]}
{"title": "The Approaches of Information Integration and Bestpath Searching in CWASS\n", "abstract": " In a complex Chinese word automatic segmentaion system (CWASS), how to integrate all sorts of information efficiently is the key problem. In the paper, we introduce the information integrated approach in a CWASS called SegTag, and discuss two best-path searching approaches for relative data structure. In the end, we present the experimental result and some conclusions.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "Automatic judgment prediction via legal reading comprehension\n", "abstract": " Automatic judgment prediction aims to predict the judicial results based on case materials. It has been studied for several decades mainly by lawyers and judges, considered as a novel and prospective application of artificial intelligence techniques in the legal field. Most existing methods follow the text classification framework, which fails to model the complex interactions among complementary case materials. To address this issue, we formalize the task as Legal Reading Comprehension according to the legal scenario. Following the working protocol of human judges, LRC predicts the final judgment results based on three types of information, including fact description, plaintiffs\u2019 pleas, and law articles. Moreover, we propose a novel LRC model, AutoJudge, which captures the complex semantic interactions among facts, pleas, and laws. In experiments, we construct a real-world civil case dataset for LRC. Experimental results on this dataset demonstrate that our model achieves significant improvement over state-of-the-art models. We have published all source codes of this work on https://github. com/thunlp/AutoJudge.", "num_citations": "2\n", "authors": ["1181"]}
{"title": "Quantitative analysis on the communication of COVID-19 related social media rumors\n", "abstract": " The outbreak of the COVID-19 pandemic is accompanied by numerous rumors spreading on the social media platform, which seriously affects the stability of society and the safety of public. Existing quantitative analyses of COVID-19 related social media rumors only focus on single element of communication, such as content, while ignoring other basic elements of communication, including communicator, audience, and effect. Besides, compared with the real social media rumor data, the rumor data of these studies have distribution bias and lack of information. Therefore, we conduct a more comprehensive quantitative analysis on the communication of COVID-19 related social media rumors based on the Sina Weibo platform. Specifically, we first analyze the communication content of rumors, including the analysis of the topic, involved regions, event tendency and sentiment. Further, we investigate the users\u00a0\u2026", "num_citations": "1\n", "authors": ["1181"]}
{"title": "CCPM: A Chinese Classical Poetry Matching Dataset\n", "abstract": " Poetry is one of the most important art forms of human languages. Recently many studies have focused on incorporating some linguistic features of poetry, such as style and sentiment, into its understanding or generation system. However, there is no focus on understanding or evaluating the semantics of poetry. Therefore, we propose a novel task to assess a model's semantic understanding of poetry by poem matching. Specifically, this task requires the model to select one line of Chinese classical poetry among four candidates according to the modern Chinese translation of a line of poetry. To construct this dataset, we first obtain a set of parallel data of Chinese classical poetry and modern Chinese translation. Then we retrieve similar lines of poetry with the lines in a poetry corpus as negative choices. We name the dataset Chinese Classical Poetry Matching Dataset (CCPM) and release it at https://github.com/THUNLP-AIPoet/CCPM. We hope this dataset can further enhance the study on incorporating deep semantics into the understanding and generation system of Chinese classical poetry. We also preliminarily run two variants of BERT on this dataset as the baselines for this dataset.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference\n", "abstract": " Existing pre-trained language models (PLMs) are often computationally expensive in inference, making them impractical in various resource-limited real-world applications. To address this issue, we propose a dynamic token reduction approach to accelerate PLMs' inference, named TR-BERT, which could flexibly adapt the layer number of each token in inference to avoid redundant calculation. Specially, TR-BERT formulates the token reduction process as a multi-step token selection problem and automatically learns the selection strategy via reinforcement learning. The experimental results on several downstream NLP tasks show that TR-BERT is able to speed up BERT by 2-5 times to satisfy various performance demands. Moreover, TR-BERT can also achieve better performance with less computation in a suite of long-text tasks since its token-level layer number adaption greatly accelerates the self-attention operation in PLMs. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/TR-BERT.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "Neural Quality Estimation with Multiple Hypotheses for Grammatical Error Correction\n", "abstract": " Grammatical Error Correction (GEC) aims to correct writing errors and help language learners improve their writing skills. However, existing GEC models tend to produce spurious corrections or fail to detect lots of errors. The quality estimation model is necessary to ensure learners get accurate GEC results and avoid misleading from poorly corrected sentences. Well-trained GEC models can generate several high-quality hypotheses through decoding, such as beam search, which provide valuable GEC evidence and can be used to evaluate GEC quality. However, existing models neglect the possible GEC evidence from different hypotheses. This paper presents the Neural Verification Network (VERNet) for GEC quality estimation with multiple hypotheses. VERNet establishes interactions among hypotheses with a reasoning graph and conducts two kinds of attention mechanisms to propagate GEC evidence to verify the quality of generated hypotheses. Our experiments on four GEC datasets show that VERNet achieves state-of-the-art grammatical error detection performance, achieves the best quality estimation results, and significantly improves GEC performance by reranking hypotheses. All data and source codes are available at https://github.com/thunlp/VERNet.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "Improving Diversity of Neural Text Generation via Inverse Probability Weighting\n", "abstract": " The neural text generation suffers from the text degeneration issue such as repetition. Traditional methods only focus on truncating the unreliable \"tail\" of the distribution, and do not address the \"head\" part, which we show might contain tedious or even repetitive candidates with high probability that lead to repetition loops. They also do not address the issue that human text does not always favor high probability words. Inspired by these, in this work we propose a heuristic sampling method. We propose to use interquartile range of the predicted distribution to determine the \"head\" part, then permutate and rescale the \"head\" with inverse probability. This aims at decreasing the probability for the tedious and possibly repetitive candidates with higher probability, and increasing the probability for the rational but more surprising candidates with lower probability. The proposed algorithm provides a controllable variation on the predicted distribution which enhances diversity without compromising rationality of the distribution. We use pre-trained language model to compare our algorithm with nucleus sampling. Results show that our algorithm can effectively increase the diversity of generated samples while achieving close resemblance to human text.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "Embedding Calibration for Music Semantic Similarity using Auto-regressive Transformer\n", "abstract": " One of the advantages of using natural language processing (NLP) technology for music is to fully exploit the embedding based representation learning paradigm that can easily handle classical tasks such as semantic similarity. However, recent researches have revealed the poor performance issue of common baseline methods for semantic similarity in NLP. They show that some simple embedding calibration methods can easily promote the performance of semantic similarity without extra training hence is ready-to-use. Nevertheless, it is still unclear which is the best combination of calibration methods and by how much can we further improve the performance with such methods. Most importantly, previous works are based on auto-encoder Transformer, hence the performance under auto-regressive model for music is unclear. These render the following open questions: does embedding based semantic similarity also apply for auto-regressive music model, does poor baseline issue for semantic similarity also exists, and if so, are there unexplored embedding calibration methods to better promote the performance of music semantic similarity? In this paper, we answer these questions by exploring different combination of embedding calibration under auto-regressive language model for symbolic music. Our results show that music semantic similarity works under auto-regressive model, and also suffers from poor baseline issues like in NLP. Furthermore, we provide optimal combination of embedding calibration that has not been explored in previous researches. Results show that such combination of embedding calibration can greatly improve\u00a0\u2026", "num_citations": "1\n", "authors": ["1181"]}
{"title": "Construction of an English-Uyghur wordnet dataset\n", "abstract": " Automatically building semantic resources is essential to low resource-languages like Uyghur. However, Uyghur suffers from a lack of publicly available evaluation dataset for automatically building semantic resources like WordNet. To cope with this problem, first, we build the largest Uyghur-English and English-Uyghur dictionaries by exploiting many possible online and offline resources. Then by using Princeton WordNet (PWN) 3.0 and Contemporary Uyghur Detailed Dictionary (CUDD), we construct an English-Uyghur WordNet evaluation dataset which is publicly available ( https://github.com/kaharjan/uywordnet  ). In this dataset, more than 73,000 English synsets are mapped Uyghur automatically, in which over 20,000 are annotated manually. And the corresponding Uyghur words include definition and examples in Uyghur language context. We also propose a Synset Mapping based on Word Embeddings\u00a0\u2026", "num_citations": "1\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u96c6\u53e5\u8bd7\u81ea\u52a8\u751f\u6210\n", "abstract": " \u6458! \u8981 \u00d8 (6K \u56fd\u53e4 2 (1S e\u00a1 \u00de\u00a2 6g \u524d \u00d41 (# K \u9009\u53d6\u5df2 \u00f4 (\u518d>\u00a3\u00a4 \u7ec4 \u5f62 S \u9996\u65b0 (6S\u00a5 \u6728 1 \u518d R \u9020\u5f62\u5f0f \u00d8 (1\u00e4\u00a6 \u00d8 \u00a7 \u800c 1 (\u00ac \u4ec5\u00a9 \u00aa i\u00f4 \u5b8c\u6574 1/\u00ec\u00ed1r \u4e0b Ga \u65b0 \u00ab1 \u4e3b'\u610f\u00ac RC \u8005 1 \u77e5 MEa (Z\u00ae mn\u00f4\u00ffk1\u00a6 FGDY \u00c61\\= a\u00b1\u00b2: m n \u00e1\u00e2\u00ae\u00b1UV G\u00aa23 \u8f83 \u00bc1]^ a; \u00camn QRS \u65b0 \u00ab1\u00d8 (7 \u52a8 \u00e4 UV FUV\u00e1\u00d6\u00b3 \u9996\u53e4 (C DE WX \u5faa\u73af\u00ae\u00b1)** 7 \u52a8\u5b66\u4e60\u53e4 (123]^ \u5e76\u8bbe J _7 \u52a8 h (1r \u4e0b GL4\u00eb \u00ea\u00d7 X5 \u8f93\u00b6 1 \u9996 UVm_7 \u52a8 \u9009\u53d6 r \u4e0b G23 \u6700) L\u00ec\u00ed1 (\u00f9\u00fa\u00d8 \u00a7 g \u800c\u5f62 S \u9996\u5b8c\u6574 1\u00d8 (7 \u52a8 \u00fe \u6d4b a\u00d4B\u00fe \u6d4b 1stc\u00c3\u00b5] u FGUVm_\u00e4 \u00d2\\\u8f83\u597d 1\u00d8 (\u00b6\u00b6 \u00e0 D \u7ebf UV1 \u6548 \u00c3", "num_citations": "1\n", "authors": ["1181"]}
{"title": "Optimizing non-decomposable evaluation metrics for neural machine translation\n", "abstract": " While optimizing model parameters with respect to evaluation metrics has recently proven to benefit end to-end neural machine translation (NMT), the evaluation metrics used in the training are restricted to be defined at the sentence level to facilitate online learning algorithms. This is undesirable because the final evaluation metrics used in the testing phase are usually non-decomposable (i.e., they are defined at the corpus level and cannot be expressed as the sum of sentence-level metrics). To minimize the discrepancy between the training and the testing, we propose to extend the minimum risk training (MRT) algorithm to take non-decomposable corpus-level evaluation metrics into consideration while still keeping the advantages of online training. This can be done by calculating corpus-level evaluation metrics on a subset of training data at each step in online training. Experiments on Chinese-English\u00a0\u2026", "num_citations": "1\n", "authors": ["1181"]}
{"title": "Agreement-based learning of parallel lexicons and phrases from non-parallel corpora\n", "abstract": " We introduce an agreement-based approach to learning parallel lexicons and phrases from non-parallel corpora. The basic idea is to encourage two asymmetric latent-variable translation models (i.e., source-to-target and target-to-source) to agree on identifying latent phrase and word alignments. The agreement is defined at both word and phrase levels. We develop a Viterbi EM algorithm for jointly training the two unidirectional models efficiently. Experiments on the Chinese-English dataset show that agreement-based learning significantly improves both alignment and translation performance.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "\u5c42\u6b21\u77ed\u8bed\u7ffb\u8bd1\u7684\u795e\u7ecf\u7f51\u7edc\u8c03\u5e8f\u6a21\u578b\n", "abstract": " \u8c03\u5e8f\u6b67\u4e49\u662f\u5c42\u6b21\u77ed\u8bed\u7ffb\u8bd1\u6a21\u578b\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u4e4b\u4e00, \u4f46\u5728\u8be5\u7c7b\u6a21\u578b\u4e2d\u4f7f\u7528\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u975e\u5e38\u6709\u9650, \u5236\u7ea6\u4e86\u8be5\u7c7b\u6a21\u578b\u5904\u7406\u8c03\u5e8f\u6b67\u4e49\u7684\u80fd\u529b. \u4e3a\u4e86\u66f4\u5145\u5206\u5730\u5229\u7528\u4e0a\u4e0b\u6587\u4fe1\u606f, \u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u5c42\u6b21\u77ed\u8bed\u7ffb\u8bd1\u6a21\u578b\u7684\u795e\u7ecf\u7f51\u7edc\u8c03\u5e8f\u6a21\u578b. \u8be5\u6a21\u578b\u5c06\u8c03\u5e8f\u770b\u4f5c\u5206\u7c7b\u95ee\u9898, \u9996\u5148\u4f7f\u7528\u9012\u5f52\u81ea\u52a8\u7f16\u7801\u5668\u4e3a\u4efb\u610f\u957f\u5ea6\u7684\u5b57\u7b26\u4e32\u8ba1\u7b97\u5411\u91cf\u8868\u793a, \u7136\u540e\u4f7f\u7528\u8fd9\u4e9b\u5411\u91cf\u8868\u793a\u4f5c\u4e3a\u5206\u7c7b\u7279\u5f81, \u7528\u4e8e\u9884\u6d4b\u4e0d\u540c\u8c03\u5e8f\u65b9\u5f0f\u7684\u6982\u7387, \u6700\u540e\u5c06\u8fd9\u4e9b\u6982\u7387\u4f5c\u4e3a\u65b0\u7684\u7279\u5f81\u52a0\u5165\u7ffb\u8bd1\u6a21\u578b\u4e2d\u8fdb\u884c\u7ffb\u8bd1. \u5b9e\u9a8c\u7ed3\u679c\u663e\u793a: \u5728\u4e2d\u2014\u82f1\u7ffb\u8bd1\u4efb\u52a1\u4e0a, \u8be5\u6a21\u578b\u76f8\u6bd4\u57fa\u7ebf\u7cfb\u7edf\u83b7\u5f97\u4e86 0.3~", "num_citations": "1\n", "authors": ["1181"]}
{"title": "Analysis and Recommendation of Social Media User Tags\n", "abstract": " Microblog is an important online service in Web 2. 0. As a platform for web users to post messages, communicate and share information, microblog contains rich information of users. Microblog services can use tags to represent interests and attributes of users. Meanwhile, the interests and attributes of a microblog user also hide behind his/her text and network information. In this paper, we quantitatively analyze user tags and propose a network-regularized tag dispatch model for user tag recommendation. NTDM models the semantic relations between words in user descriptions and tags, with social network structure as its regularization factor. Experiment results in a real world dataset show its effectiveness compared to other baseline methods.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "Exploiting Lexicalized Statistical Patterns in Chinese Linguistic Analysis\n", "abstract": " The web corpus has been used for linguistic analysis with the help of search engines. In this paper, we describe the concept of lexicalized patterns, which we exploit to obtain statistical information using the simple string matching strategy via search engines. We discuss the usage of lexicalized statistical patterns at three linguistic levels of Chinese analysis: lexical, syntactic and semantic. We develop a specialized search engine to get frequency counts for these patterns on SogouT corpus. Experimental results show that lexicalized statistical patterns are effective on analyzing the cohesion of phrases, determining the phrasal category and discovering patient objects.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data: 12th China National Conference, CCL 2013 and First International\u00a0\u2026\n", "abstract": " This book constitutes the refereed proceedings of the 12th China National Conference on Computational Linguistics, CCL 2013, and of the First International Symposium on Natural Language Processing Based on Naturally Annotated Big Data, NLP-NABD 2013, held in Suzhou, China, in October 2013. The 32 papers presented were carefully reviewed and selected from 252 submissions. The papers are organized in topical sections on word segmentation; open-domain question answering; discourse, coreference and pragmatics; statistical and machine learning methods in NLP; semantics; text mining, open-domain information extraction and machine reading of the Web; sentiment analysis, opinion mining and text classification; lexical semantics and ontologies; language resources and annotation; machine translation; speech recognition and synthesis; tagging and chunking; and large-scale knowledge acquisition and reasoning.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "\u6267\u884c\u4e2d\u6587\u5206\u8bcd\u548c\u8bcd\u6027\u6807\u6ce8\u7684\u7edf\u4e00\u6846\u67b6\n", "abstract": " The paper proposes a unified framework to combine the advantages of the fast one-at-a-time approach and the high-performance all-at-once approach to perform Chinese Word Segmentation (CWS) and Part-of-Speech (PoS) tagging. In this framework, the input of the PoS tagger is a candidate set of several CWS results provided by the CWS model. The widely used one-at-a-time approach and all-at-once approach are two extreme cases of the proposed candidate-based approaches. Experiments on Penn Chinese Treebank 5 and Tsinghua Chinese Treebank show that the generalized candidate-based approach outperforms one-at-a-time approach and even the all-at-once approach. The candidate-based approach is also faster than the time-consuming all-at-once approach. The authors compare three different methods based on sentence, words and character-intervals to generate the candidate set. It turns out that the word-based method has the best performance.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "Exploring the Granularity Level of Social Tags\n", "abstract": " Tags are widely used in Web 2.0 applications, such as blogs and online bookmarks. Many characteristics of tags have been studied in previous works, but the granularity level of tags still remains untouched. Granularity level describes how focused a tag is, the lower the level, the more concrete concept the tag covers. The paper has three main contributions: 1) We propose a method to discover the concept level of tags automatically. 2) We apply the method to a real-world tag data set, and study the distribution of levels. 3) We perform tag recommendation concerning their concept levels. Experiments are conducted on a large-scale real world Chinese blog data set. We find that most tags are in more concrete levels, and the users tend to choose tags within the same concept level for a document. When predicting tags for a new document, tags in higher concept levels provide little help, yet may hurt the performance.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "\u4e2d\u6587\u673a\u6784\u540d\u7b80\u79f0\u7684\u81ea\u52a8\u751f\u6210\u7814\u7a76\n", "abstract": " SchoolofComputerScienceandTechnology, FudanUniversity, Shanghai200433 E-mail:{fengi, 0572237, xpqiu, xjhuang}@ fudanedu. cn Abstract: Inthispaper, anautomaticabbreviationgenerationmethodofChinese oganizationhasbeenproposed, In our method, we transformedthe probleminto an equivalentsequence taggingproblem, and buitup the automaticgeneration modelthroughthe frstorderconditionalrandomfeld. Diferentfrom theworksofothers, ourmethod didn\u201d tneed the Segmentation information, In ourexperimentsin the high-schooland corporation abbreviationdatasets, F1achieved86. 18% and75. 89% respectivey", "num_citations": "1\n", "authors": ["1181"]}
{"title": "\u4e00\u79cd\u57fa\u4e8e\u5171\u4eab\u540e\u7f00\u672f\u8bed\u96c6\u6539\u8fdb\u4e2d\u6587\u6838\u5fc3\u9886\u57df\u672c\u4f53\u6784\u5efa\u7684\u65b9\u6cd5\n", "abstract": " \u6838\u5fc3\u672c\u4f53\u5bf9\u6700\u57fa\u672c\u7684\u9886\u57df\u77e5\u8bc6\u5efa\u6a21\u5e76\u5728\u4e0a\u4f4d\u672c\u4f53\u548c\u9886\u57df\u672c\u4f53\u4e4b\u95f4\u5efa\u7acb\u8054\u7cfb. \u4e0a\u4f4d\u672c\u4f53\u662f\u9886\u57df\u65e0\u5173\u7684\u800c\u6838\u5fc3\u672c\u4f53\u662f\u9886\u57df\u76f8\u5173\u7684, \u56e0\u6b64\u5728\u81ea\u52a8\u521b\u5efa\u4e2d\u6587\u6838\u5fc3\u672c\u4f53\u8fc7\u7a0b\u4e2d, \u6620\u5c04\u4e2d\u6587\u6838\u5fc3\u672f\u8bed\u5230\u4e0a\u4f4d\u672c\u4f53\u6982\u5ff5\u6709\u5f88\u591a\u7684\u9519\u8bef. \u672c\u6587\u4ee5\u4e00\u4e2a\u57fa\u4e8e\u672f\u8bed\u8bcd\u96c6\u62bd\u53d6\u5171\u4eab\u540e\u7f00\u7684\u65b9\u6cd5, \u627e\u5230\u88ab\u5171\u4eab\u7684\u672f\u8bed\u6761\u6570\u66f4\u591a, \u4e0e\u5404\u672f\u8bed\u7684\u610f\u4e49\u66f4\u63a5\u8fd1\u7684\u4e0a\u4f4d\u6982\u5ff5; \u7528\u5176\u6765\u6539\u8fdb\u8bcd\u96c6\u4e2d\u7684\u6838\u5fc3\u672f\u8bed\u548c\u6982\u5ff5\u4e4b\u95f4\u7684\u6620\u5c04. \u5b9e\u9a8c\u8bc1\u660e, \u8be5\u65b9\u6cd5\u6709\u6548\u7684\u63d0\u9ad8\u4e86\u81ea\u52a8\u6838\u5fc3\u672c\u4f53\u521b\u5efa\u7684\u7cbe\u786e\u5ea6.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "Disyllabic Chinese Word Extraction Based on Character Thesaurus and Semantic Constraints in Word-Formation\n", "abstract": " This paper presents a novel approach to Chinese disyllabic word extraction based on semantic information of characters. Two thesauri of Chinese characters, manually-crafted and machine-generated, are conducted. A Chinese wordlist with 63,738 two-character words, together with the character thesauri, are explored to learn semantic constraints between characters in Chinese word-formation, resulting in two types of semantic-tag-based HMM. Experiments show that: (1) both schemes outperform their character-based counterpart; (2) the machine-generated thesaurus outperforms the hand-crafted one to some extent in word extraction, and (3) the proper combination of semantic-tag-based and character-based methods could benefit word extraction.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "Leveraging World Knowledge in Chinese Text Classification\n", "abstract": " In state-of-the-art Text Classification (TC) approaches, only features explicitly mentioned in training set are taken into consideration, but after several decades' endeavor, it seems that these approaches have all reached a plateau. In this paper, we propose an automatic taxonomy mapping algorithm to map from original flat taxonomy to a hierarchical, human-edit on-line taxonomy (ODP), from which we could then synthesize new training samples with common-sense world knowledge by performing a constrained web focus crawling. We show that by leveraging the domain-knowledge which otherwise can't be deduced from training set directly, the text classifier will have better generalization ability. Preliminary Experimental Results on several Chinese data sets confirm the effectiveness of this approach.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "\u6c49\u8bed\u4f9d\u5b58\u56fe\u5e93\u5efa\u8bbe\u7814\u7a76\n", "abstract": " \u6811\u5e93\u7684\u6784\u5efa\u662f\u8bed\u6599\u5e93\u8bed\u8a00\u5b66\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206. \u672c\u6587\u63d0\u51fa\u6784\u5efa\u4e00\u79cd\u5168\u65b0\u7684\u6c49\u8bed\u6811\u5e93\u2014\u4f9d\u5b58\u56fe\u5e93. \u4f9d\u5b58\u56fe\u8868\u793a\u53ef\u4ee5\u7a81\u7834\u4ee5\u5f80\u6811\u7ed3\u6784\u8868\u793a\u7684\u5c40\u9650, \u66f4\u597d\u5730\u8868\u793a\u8bcd\u8bed\u4e4b\u95f4\u7684\u5173\u7cfb, \u5bf9\u4e8e\u6c49\u8bed\u7279\u6b8a\u7ed3\u6784\u7684\u8868\u793a\u5177\u6709\u66f4\u5927\u7684\u4f18\u52bf.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "\u85cf\u8bed\u52a8\u8bcd\u7684\u53ca\u7269\u6027, \u81ea\u4e3b\u6027\u4e0e\u65bd\u683c\u8bed\u8a00\u7c7b\u578b\u201d\n", "abstract": " Emaijangdi@ cassorgcn \u6458\u8981: \u85cf\u8bed\u8bed\u6cd5\u5c5e\u6027\u8bcd\u5178\u5bf9\u52a8\u8bcd\u7684\u63cf\u8ff0\u6d89\u53ca\u85cf\u8bed\u7684\u53e5\u6cd5\u7279\u5f81\u548c\u53e5\u6cd5\u8bed\u4e49\u7c7b\u578b. \u8bba\u6587\u7b80\u8981\u4ecb\u7ecd\u4e86\u85cf\u8bed\u52a8\u8bcd\u7684\u53ca\u7269\u6027\u4e0e\u81ea\u4e3b\u6027\u73b0\u8c61, \u901a\u8fc7\u82f1, \u85cf\u8bed\u8a00\u7684\u53e5\u6cd5\u8bed\u4e49\u5173\u7cfb\u6bd4\u8f83, \u660e\u786e\u4e86\u85cf\u8bed\u5c5e\u4e8e\u901a\u683c\u4e00\u65bd\u683c\u578b\u8bed\u8a00, \u662f\u4e0e\u82f1\u8bed\u8fd9\u7c7b\u4e3b\u683c\u4e00\u53d7\u683c\u578b\u8bed\u8a00\u76f8\u5bf9\u5e94\u7684\u53e6\u4e00\u79cd\u8bcd\u683c\u5173\u7cfb\u8bed\u6cd5\u7cfb\u7edf, \u5e76\u8fdb\u4e00\u6b65\u63a2\u8ba8\u4e86\u85cf\u8bed\u65bd\u683c\u578b\u8bed\u8a00\u7684\u539f\u578b. \u85cf\u8bed\u52a8\u8bcd\u7684\u53ca\u7269\u6027, \u81ea\u4e3b\u6027\u7b49\u53e5\u6cd5\u5c5e\u6027\u662f\u52a8\u8bcd\u7684\u5185\u5728\u5c5e\u6027, \u6ca1\u6709\u5f62\u5f0f\u4e0a\u7684\u6807\u5fd7. \u4e3a\u6b64, \u5728\u85cf\u8bed\u8bed\u6cd5\u5c5e\u6027\u8bcd\u5178\u4e2d\u5bf9\u52a8\u8bcd\u4f5c\u53ca\u7269\u6027\u548c\u81ea\u4e3b\u6027\u6807\u6ce8\u662f\u4e00\u9879\u5fc5\u8981\u7684\u5de5\u4f5c.,-", "num_citations": "1\n", "authors": ["1181"]}
{"title": "\u5f71\u54cd\u4f9d\u5b58\u53e5\u6cd5\u5206\u6790\u7684\u56e0\u7d20\u63a2\u8ba8\n", "abstract": " E-mailhtcuc@ gmalcom \u6458\u8981: \u672c\u6587\u91c7\u7528 Ma\u017fParser \u548c\u54c8\u5de5\u5927\u6c49\u8bed\u4f9d\u5b58\u6811\u5e93\u8fdb\u884c\u4e86\u57fa\u4e8e\u6811\u5e93\u7684\u6c49\u8bed\u4f9d\u5b58\u53e5\u6cd5\u5206\u6790\u5b9e\u9a8c, \u76ee\u7684\u5728\u4e8e\u53d1\u73b0\u5f71\u54cd\u4f9d\u5b58\u53e5\u6cd5\u5206\u6790\u7cbe\u5ea6, \u6548\u7387\u548c\u8fde\u901a\u6027\u7684\u56e0\u7d20. \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e, POS \u4f7f\u5f97\u7528\u5c0f\u8bad\u7ec3\u96c6\u4e5f\u80fd\u5f97\u5230\u53ef\u63a5\u53d7\u7684\u7ed3\u679c, \u5e76\u4e14\u5bf9\u4e8e\u53e5\u6cd5\u5206\u6790\u5668\u6548\u7387\u7684\u63d0\u9ad8\u548c\u7a33\u5b9a\u8d77\u7740\u5173\u952e\u7684\u4f5c\u7528; LEX \u6709\u52a9\u4e8e\u89e3\u51b3\u53e5\u5b50\u7684\u8054\u901a\u6027, \u4e5f\u4f1a\u964d\u4f4e\u53e5\u6cd5\u5206\u6790\u7684\u6548\u7387: DEP \u65e0\u6cd5\u5355\u72ec\u5de5\u4f5c, \u4f46\u548c\u5176\u5b83\u4e24\u7c7b\u7279\u5f81\u7ed3\u5408\u53ef\u4ee5\u6539\u5584\u53e5\u6cd5\u5206\u6790\u5668\u7684\u7cbe\u5ea6\u548c\u6548\u7387.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e\u7ec4\u5408\u65b9\u6cd5\u7684\u7ec4\u5757\u8bc6\u522b\n", "abstract": " \u7ed9\u51fa\u4e86\u4e00\u79cd\u8bcd\u6027\u6269\u5c55\u4e0e voting \u6cd5\u7ed3\u5408\u7684\u6c49\u8bed\u7ec4\u5757\u8bc6\u522b\u65b9\u6cd5 (\u7b80\u79f0\u7ec4\u5408\u65b9\u6cd5). \u9996\u9009\u6bd4\u8f83\u4e86\u7279\u6b8a\u9690\u9a6c\u5c14\u53ef\u592b, SVM, CRF \u4e09\u79cd\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\u5728\u7ec4\u5757\u8bc6\u522b\u4e0a\u7684\u6548\u679c. \u4e3a\u4e86\u6539\u5584\u8bc6\u522b\u6548\u679c, \u5bf9\u8bed\u6599\u4e2d\u7684\u7279\u6b8a\u7b26\u53f7, \u5e76\u5217\u5173\u7cfb\u4ee5\u53ca\u8f83\u7c97\u7684\u8bcd\u6027\u8fdb\u884c\u4e86\u8bcd\u6027\u6269\u5c55, \u5e76\u91c7\u7528\u4e86\u4e00\u79cd\u57fa\u4e8e\u6807\u70b9\u7b26\u53f7\u5206\u5272\u6bb5\u7684 voting \u65b9\u6cd5. \u5b9e\u9a8c\u8868\u660e, \u4e09\u4e2a\u57fa\u672c\u6a21\u578b\u4e2d CRF \u8bc6\u522b\u6548\u679c\u6700\u597d, \u800c\u7ec4\u5408\u65b9\u6cd5\u80fd\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7ec4\u5757\u8bc6\u522b\u7684\u7cbe\u786e\u7387, \u53ec\u56de\u7387, F \u503c.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e\u6587\u6863\u4e2d\u5fc3\u5185\u5bb9\u5feb\u901f\u63d0\u53d6\u7684 Web \u76d1\u63a7\u8f85\u52a9\u7cfb\u7edf\n", "abstract": " Web \u5df2\u6210\u4e3a\u4eba\u4eec\u83b7\u53d6\u4fe1\u606f\u7684\u91cd\u8981\u6765\u6e90. Web \u6570\u636e\u7684\u4ea7\u751f\u4e0e\u4f20\u64ad\u53d8\u5f97\u66f4\u52a0\u81ea\u7531\u548c\u4fbf\u6377, \u6570\u636e\u91cf\u6301\u7eed\u7206\u70b8\u5f0f\u589e\u957f, \u56e0\u800c\u5bf9 Web \u8fdb\u884c\u76d1\u63a7\u548c\u9884\u8b66\u7684\u9700\u6c42\u4e5f\u66f4\u52a0\u8feb\u5207. \u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5feb\u901f\u63d0\u53d6\u6587\u6863\u4e2d\u5fc3\u5185\u5bb9\u7684\u7b97\u6cd5, \u7528\u4e8e\u7f29\u51cf Web \u6d77\u91cf\u6570\u636e\u7684\u89c4\u6a21. \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u63d0\u53d6\u51fa\u7684\u4e2d\u5fc3\u5185\u5bb9\u53ea\u5360\u539f\u6587\u6863\u5927\u5c0f\u7684 2.2%, \u5728\u666e\u901a PC \u673a\u4e0a\u7684\u5904\u7406\u901f\u5ea6\u8fbe\u5230\u6bcf\u79d2 380 \u7bc7, \u5e76\u5728\u5c0f\u89c4\u6a21\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u4e86 75% \u4ee5\u4e0a\u7684\u7cbe\u5ea6. \u4f7f\u7528\u6587\u6863\u4e2d\u5fc3\u5185\u5bb9\u8fdb\u884c\u6587\u6863\u5206\u7c7b, \u805a\u7c7b\u548c\u70ed\u70b9\u62bd\u53d6\u4e0e\u8ffd\u8e2a, \u6784\u6210\u4e86 Web \u76d1\u63a7\u8f85\u52a9\u7cfb\u7edf\u7684\u6846\u67b6.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "\u4e2d\u6587\u6587\u672c\u5168\u6587\u67e5\u91cd\u7684\u5b9e\u9a8c\u7814\u7a76\u201d\n", "abstract": " \u4e92\u8054\u7f51\u4e2d\u5927\u91cf\u7684\u91cd\u590d\u6587\u672c\u4e0d\u4ec5\u7ed9\u4fe1\u606f\u68c0\u7d22\u5e26\u6765\u4e86\u8bf8\u591a\u4e0d\u4fbf, \u800c\u4e14\u4e5f\u662f\u5bf9\u77e5\u8bc6\u4ea7\u6743\u7684\u4fb5\u72af. \u672c\u6587\u4e3b\u8981\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\u4e2d\u6587\u6587\u672c\u67e5\u91cd\u7684\u4e24\u4e2a\u56e0\u7d20:(1) \u7279\u5f81\u5b57\u7684\u56e0\u7d20, \u5373\u5206\u522b\u9009\u53d6\u9ad8\u9891\u5b57 (\u5305\u62ec\u9017\u53f7\u548c\u53e5\u53f7), \u4e2d\u9891\u5b57", "num_citations": "1\n", "authors": ["1181"]}
{"title": "\u8bed\u8a00\u8ba1\u7b97: \u4fe1\u606f\u79d1\u5b66\u6280\u672f\u4e2d\u957f\u671f\u53d1\u5c55\u7684\u6218\u7565\u5236\u9ad8\u70b9\n", "abstract": " \u968f\u7740\u4e92\u8054\u7f51\u4ee5\u53ca\u5927\u89c4\u6a21\u6570\u636e\u5b58\u50a8\u4f53\u7cfb\u7684\u8fc5\u731b\u53d1\u5c55, \u4eba\u7c7b\u5df2\u7ecf\u8fdb\u5165\u540d\u526f\u5176\u5b9e\u7684\u6d77\u91cf\u4fe1\u606f\u65f6\u4ee3. \u4f8b\u5982, \u8457\u540d\u7684\u641c\u7d22\u5f15\u64ce Google \u7684\u68c0\u7d22\u8303\u56f4\u5df2\u8fbe 80 \u591a\u4ebf\u5f20\u7f51\u9875, \u5141\u8bb8\u5bf9\u8fd1\u4e09\u5341\u79cd\u8bed\u8a00\u8fdb\u884c\u641c\u7d22 (\u5305\u62ec\u82f1\u8bed, \u4e3b\u8981\u6b27\u6d32\u56fd\u5bb6\u8bed\u8a00, \u65e5\u8bed, \u4e2d\u6587\u7b80\u7e41\u4f53, \u671d\u9c9c\u8bed\u7b49). \u4eba\u7c7b\u77e5\u8bc6\u66f4\u65b0\u7684\u6b65\u4f10\u65e5\u65b0\u6708\u5f02. \u636e\u6fc0\u5149\u6253\u5370\u673a\u53d1\u660e\u4eba Gary Starkweather \u535a\u58eb\u79f0: \u5728 1750~ 1950 \u5e74\u4e2d, \u77e5\u8bc6\u589e\u957f\u7684\u901f\u5ea6\u662f 150 \u5e74\u7ffb\u4e00\u756a, \u800c 1950~ 1960 \u5e74\u95f4, 10 \u5e74\u5c31\u7ffb\u4e86\u4e00\u756a, 1960~ 1992 \u5e74\u95f4, \u7ffb\u756a\u65f6\u95f4\u5df2\u7f29\u77ed\u5230 5 \u5e74. \u671f\u671b\u5230 2020 \u5e74, \u4fe1\u606f\u91cf\u6bcf 73 \u5929\u5c31\u5c06\u7ffb\u4e00\u756a. \u7edd\u5927\u591a\u6570\u65b0\u4ea7\u751f\u51fa\u6765\u7684\u4fe1\u606f\u90fd\u662f\u6570\u5b57\u5316\u7684, \u540c\u65f6\u65e7\u7684\u4fe1\u606f\u4e5f\u6b63\u5728\u901a\u8fc7\u5927\u578b\u7684\u6570\u5b57\u56fe\u4e66\u9986\u8ba1\u5212\u4e0d\u65ad\u5730\u5728\u88ab\u6570\u5b57\u5316\u4e2d. \u53ef\u4ee5\u8bbe\u60f3, \u5728\u4e0d\u8fdc\u7684\u5c06\u6765, \u4e92\u8054\u7f51\u4e0a\u5c06\u96c6\u805a\u4eba\u7c7b\u6709\u53f2\u4ee5\u6765\u521b\u9020\u7684\u51e0\u4e4e\u5168\u90e8\u77e5\u8bc6. \u7136\u800c, \u62e5\u6709\u6d77\u91cf\u6570\u636e\u4ec5\u4ec5\u610f\u5473\u7740\u4eba\u7c7b\u62e5\u6709\u5168\u9762, \u6df1\u5165, \u65b9\u4fbf\u5730\u9a7e\u9a6d\u8fd9\u4e9b\u6d77\u91cf\u6570\u636e\u4e2d\u6240\u8574\u6db5\u77e5\u8bc6\u7684\u6f5c\u5728\u53ef\u80fd\u6027, \u4f46\u53ef\u80fd\u6027\u4e0e\u73b0\u5b9e\u6027\u6709\u5929\u58e4\u4e4b\u522b. \u73b0\u5b9e\u72b6\u51b5\u662f: \u76ee\u524d\u5bf9\u6d77\u91cf\u6570\u636e\u7684\u64cd\u4f5c\u4e3b\u8981\u8fd8\u5728\u4fe1\u606f\u68c0\u7d22\u9636\u6bb5, \u6839\u672c\u8c08\u4e0d\u4e0a\u6784\u5efa\u4e8e\u5176\u4e0a\u7684\u77e5\u8bc6\u7ec4\u7ec7, \u603b\u7ed3\u53ca\u5206\u6790. \u5373\u4f7f\u662f\u4fe1\u606f\u68c0\u7d22\u8fd9\u4e2a\u6bd4\u8f83\u521d\u7ea7\u7684\u4efb\u52a1, \u6548\u679c\u4e5f\u5f88\u4e0d\u7406\u60f3: TREC 2004 Terabyte Track \u7684\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a, \u6587\u672c\u4fe1\u606f\u68c0\u7d22\u7684\u6700\u9ad8\u7cbe\u5ea6\u4e0d\u8d85\u8fc7 30%. \u800c\u5bf9\u58f0\u97f3, \u56fe\u8c61, \u89c6\u50cf\u7b49\u7684\u641c\u7d22\u80fd\u529b\u5c31\u66f4\u5dee\u4e86. \u5c31\u76ee\u524d\u72b6\u51b5\u800c\u8a00, \u4e92\u8054\u7f51\u8fd9\u4e2a\u77e5\u8bc6\u6d77\u6d0b\u9887\u50cf\u865a\u62df\u4e16\u754c\u4e2d\u5de8\u5927\u65e0\u6bd4\u7684 \u201c\u9ed1\u6d1e\u201d, \u5927\u591a\u6570\u5b9d\u7269\u90fd\u88ab\u9ed8\u9ed8\u5730\u57cb\u85cf\u4e8e\u5e7d\u6df1\u7684\u6d77\u5e95\u96be\u89c1\u5929\u65e5, \u800c\u6211\u4eec\u5374\u7f3a\u4e4f\u6709\u6548\u624b\u6bb5\u5b9e\u73b0\u968f\u5fc3\u6240\u6b32\u7684 \u201c\u5927\u6d77\u635e\u9488\u201d, \u53ea\u597d\u65e0\u5948\u5730 \u201c\u671b\u6d0b\u5174\u53f9\u201d. \u4eba\u7c7b\u6b63\u9762\u4e34\u7740\u4e00\u79cd\u524d\u6240\u672a\u6709\u7684\u5c34\u5c2c\u4e0e\u56f0\u60d1\u7684\u5c40\u9762: \u5bf9\u6570\u5b57\u4fe1\u606f\u5229\u7528\u7684\u6709\u6548\u7387\u6781\u5176\u4f4e\u4e0b. \u6362\u4e2a\u5f62\u8c61\u7684\u8bf4\u6cd5, \u4e92\u8054\u7f51\u8c61\u4e2a\u5927\u8336\u58f6, \u5b83\u7684\u58f6\u4f53\u6b63\u5728\u6025\u5267\u81a8\u80c0, \u9887\u6709 \u201c\u9189\u91cc\u4e7e\u5764\u5927, \u58f6\u4e2d\u65e5\u6708\u957f\u201d \u7684\u5473\u9053, \u4f46\u8336\u58f6\u5634\u51e0\u4e4e\u6ca1\u6709\u6269\u5f20, \u867d\u7136\u5927\u809a\u80fd\u5bb9, \u6709\u8d27\u5374\u5012\u4e0d\u51fa\u6765.\u5fc5\u987b\u6307\u51fa, \u8ba1\u7b97\u673a\u7684\u8fd0\u7b97\u901f\u5ea6, \u78c1\u76d8\u5bb9\u91cf, \u5b58\u53d6\u6548\u7387, \u7f51\u7edc\u5e26\u5bbd\u7b49\u56e0\u7d20\u4e0e\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u5e76\u65e0\u5b9e\u8d28\u6027\u5173\u7cfb (\u8457\u540d\u7684\u6469\u5c14\u5b9a\u5f8b\u6307\u51fa, \u8ba1\u7b97\u673a\u7684\u6027\u80fd\u6bcf 18 \u4e2a\u6708\u7ffb\u4e00\u00a0\u2026", "num_citations": "1\n", "authors": ["1181"]}
{"title": "\u57fa\u4e8e NN-LSVM \u7684\u65e5\u8bed\u4f9d\u5b58\u5173\u7cfb\u89e3\u6790\n", "abstract": " \u65e5\u8bed\u4f9d\u5b58\u5173\u7cfb\u89e3\u6790\u662f\u57fa\u4e8e\u65e5\u8bed\u4f9d\u5b58\u6587\u6cd5, \u786e\u5b9a\u53e5\u5b50\u4e2d\u5404\u4e2a\u6587\u8282\u95f4\u7684\u4f9d\u5b58\u5173\u7cfb. \u4e3a\u63d0\u9ad8\u89e3\u6790\u7cbe\u5ea6, \u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e NN-LSVM \u5bf9\u5927\u89c4\u6a21\u8bad\u7ec3\u8bed\u6599\u8fdb\u884c\u4fee\u526a\u7684\u65e5\u8bed\u4f9d\u5b58\u5173\u7cfb\u89e3\u6790\u65b9\u6cd5: \u4f7f\u7528 LSVM \u548c NN \u5220\u9664\u5bf9\u5206\u7c7b\u6ca1\u6709\u4f5c\u7528\u7684, \u751a\u81f3\u8d77\u53cd\u4f5c\u7528\u7684\u8bad\u7ec3\u6837\u672c, \u518d\u7ecf\u8bad\u7ec3\u5f97\u5230\u89e3\u6790\u5668. \u7528\u4eac\u90fd\u5927\u5b66\u8bed\u6599\u5e93\u8fdb\u884c\u6d4b\u8bd5, \u7ed3\u679c\u8868\u660e\u5728\u89e3\u6790\u7cbe\u5ea6\u548c\u89e3\u6790\u901f\u5ea6\u4e0a\u5747\u5f97\u5230\u4e00\u5b9a\u7684\u6539\u5584.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "\u641c\u7d22\u7684\u672a\u6765\n", "abstract": " \u4e92\u8054\u7f51\u7684\u8fc5\u731b\u53d1\u5c55\u4f7f\u641c\u7d22\u5f15\u64ce\u6210\u4e3a\u5fc5\u4e0d\u53ef\u5c11\u7684\u5de5\u5177, \u5b83\u80fd\u5e2e\u52a9\u4eba\u4eec\u5728\u6d69\u701a\u7684\u4fe1\u606f\u6d77\u6d0b\u4e2d\u65b9\u4fbf\u5feb\u6377\u5730\u83b7\u53d6\u4f17\u591a\u7684\u4fe1\u606f\u670d\u52a1. \u540c\u65f6\u4e92\u8054\u7f51\u6b63\u5728\u63d0\u4f9b\u5e9e\u5927\u548c\u4e30\u5bcc\u7684\u8bed\u6599\u8d44\u6e90, \u4e3a\u4e2d\u6587\u6280\u672f\u7814\u7a76\u63d0\u4f9b\u5f3a\u5927\u52a9\u529b. \u672c\u6587\u9996\u5148\u901a\u8fc7\u5bf9\u5404\u4ee3\u641c\u7d22\u5f15\u64ce, \u7279\u522b\u662f\u4e2d\u641c\u7f51\u7edc\u732a\u7684\u4ecb\u7ecd, \u63ed\u793a\u4e86\u641c\u7d22\u5f15\u64ce\u6b63\u5728\u4ece\u5185\u5bb9, \u5f62\u5f0f\u548c\u641c\u7d22\u8303\u56f4\u7b49\u65b9\u9762\u53d1\u751f\u5de8\u5927\u7684\u6f14\u53d8. \u63a5\u7740\u4ee5\u5b9e\u4f8b\u663e\u793a\u4e86\u4ee3\u8868\u641c\u7d22\u672a\u6765\u7684\u7f51\u7edc\u732a\u56e0\u5176\u641c\u7d22\u65b9\u5f0f\u7684\u6539\u53d8, \u6781\u5927\u5730\u6539\u5584\u4e86\u7528\u6237\u7684\u641c\u7d22\u611f\u53d7. \u6700\u540e\u57fa\u4e8e\u4e2d\u56fd\u641c\u7d22\u7684\u641c\u7d22\u5f15\u64ce\u7ed3\u6784\u6846\u67b6\u7684\u5206\u6790, \u8bf4\u660e\u641c\u7d22\u5f15\u64ce\u4f5c\u4e3a\u4e92\u8054\u7f51\u4e0a\u5e9e\u5927\u7684\u5b9e\u65f6\u670d\u52a1\u5e73\u53f0, \u6b63\u5728\u4ece\u8f83\u4e13\u4e00\u7684\u68c0\u7d22\u670d\u52a1\u5e73\u53f0\u5411\u4e92\u8054\u7f51\u7efc\u5408\u4fe1\u606f\u670d\u52a1\u5e73\u53f0\u6f14\u53d8, \u540c\u65f6\u4e2d\u6587\u641c\u7d22\u5f15\u64ce\u7cfb\u7edf\u6b63\u5728\u6210\u4e3a\u5404\u79cd\u4e2d\u6587\u6280\u672f\u7684\u7814\u7a76\u4e0e\u5e94\u7528\u5e73\u53f0.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "Sentence difficulty evaluation for a learner's dictionary\n", "abstract": " In order to choose the best example sentences for Chinese language learners, we have compared how well different methods of estimating difficulty (reading time, translation time, direct rating) could be approximated from a sentence's superficial features. We have found that direct rating of difficulty by a user is the most promising, and that character-based features allow for better evaluation than word-based ones, all of which bodes well for user-tailored learner's dictionaries.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "Computations on Chinese morphology\n", "abstract": " Natural language processing (N1. P) typically requires the following sequence of major steps: morphological analysis. syntactic analysis, semantic analysis and pragmatic analysis. For NLP by means of computer,'analysis' entails quantitative and qualitative'computation'in a broad sense. This thesis focuses on morphological computations of the Chinese language", "num_citations": "1\n", "authors": ["1181"]}
{"title": "Eliminating high-degree biased character bigrams for dimensionality reduction in Chinese text categorization\n", "abstract": " High dimensionality of feature space is a main obstacle for Text Categorization (TC). In a candidate feature set consisting of Chinese character bigrams, there exist a number of bigrams which are high-degree biased according to character frequencies. Usually, these bigrams are likely to survive for their strength of discriminating documents after the process of feature selection. However, most of them are useless for document categorization because of the weakness in representing document contents. The paper firstly defines a criterion to identify the high-degree biased Chinese bigrams. Then, two schemes called s-BR1 and s-BR2 are proposed to deal with these bigrams: the former directly eliminates them from the feature set whereas the latter replaces them with the corresponding significant characters involved. Experimental results show that the high-degree biased bigrams should be eliminated from the\u00a0\u2026", "num_citations": "1\n", "authors": ["1181"]}
{"title": "LFG for Chinese: issues of Representation and Computation\n", "abstract": " LFG has been widely used to analyze English language as well as other languages from linguistic point of view [Joan Bresnan 2001; Louisa Sadler 1996], including Chinese [Lian-Cheng Chief 1996; One-Soon Her. 1997]. A new direction in LFG research field is applying it to language computation, ranging from parsing to machine translation [Louisa Sadler, Josef van Genabith, and Andy Way 2000; Mark Johnson 2000; Miriam Butt, Stefanie Dipper, Anette Frank, and Tracy Holloway King 1999]. However, the LFG-based work in Chinese computing is rather rare [Lian-Cheng Chief, Chu-Ren Huang, Keh-Jiann Chen et al 1998]. The current framework of LFG shows two folds when being employed in Chinese computing tasks: it is quite powerful for linguistic representation, but seems not to be strong enough for Chinese computation\u2013there exists some room for improving the formalism of LFG. This paper will focus on these two issues, suggesting some possible augmentations on LFG paradigm, though the idea is still preliminary. The author believes linguistic resources, such as annotated corpora, mainly semantics-oriented, are also required to make manipulations on the augmented paradigm possible. The total solution is based on not only academic research but also engineering realization\u2013it will not work without either. http://csli-publications. stanford. edu/", "num_citations": "1\n", "authors": ["1181"]}
{"title": "\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u7814\u7a76\u7684\u82e5\u5e72\u6700\u65b0\u8fdb\u5c55\u2014\u2014\u6e05\u534e\u5927\u5b66\u76f8\u5173\u5de5\u4f5c\u7b80\u4ecb\n", "abstract": " \u6b63\u6e05\u534e\u5927\u5b66\u667a\u80fd\u6280\u672f\u4e0e\u7cfb\u7edf\u56fd\u5bb6\u91cd\u70b9\u5b9e\u9a8c\u5ba4\u4e00\u76f4\u81f4\u529b\u4e8e\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u7684\u7814\u7a76. \u73b0\u5c06\u6700\u8fd1\u51e0\u5e74\u5728\u8fd9\u65b9\u9762\u7684\u4e3b\u8981\u7814\u7a76\u6210\u679c\u5411\u5927\u5bb6\u627c\u8981\u6c47\u62a5\u4e00\u4e0b, \u5e0c\u671b\u5f97\u5230\u5b66\u8005\u4eec\u7684\u6307\u6b63. 1. \u4fe1\u606f\u5904\u7406\u7528\u73b0\u4ee3\u6c49\u8bed\u5206\u8bcd\u8bcd\u8868\u56fd\u5bb6\u6280\u672f\u76d1\u7763\u5c40\u4e8e 1993 \u5e74\u53d1\u5e03\u4e86\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u56fd\u5bb6\u6807\u51c6 GB/T 13715-92\u300a \u4fe1\u606f\u5904\u7406\u7528\u73b0\u4ee3\u6c49\u8bed\u5206\u8bcd\u89c4\u8303\u300b. \u8fd9\u4e2a\u89c4\u8303\u5bf9\u63a8\u52a8\u6c49\u8bed\u81ea\u52a8\u5206\u8bcd\u7814\u7a76\u7684\u53d1\u5c55, \u8d77\u5230\u4e86\u79ef\u6781\u4f5c\u7528. \u7136\u800c, \u4f7f\u7528\u8005\u4e5f\u666e\u904d\u53cd\u6620, \u8be5\u89c4\u8303\u4e2d\u591a\u6b21\u51fa\u73b0\u4e86 \u201c\u7ed3\u5408\u7d27\u5bc6, \u4f7f\u7528\u7a33\u5b9a (\u9891\u7e41)\u201d \u7684\u8868\u8ff0, \u8bd5\u56fe\u5bf9\u67d0\u4e9b\u4ecb\u4e4e \u201c\u8bcd\u201d \u4e0e \u201c\u8bcd\u7ec4\u201d \u4e4b\u95f4\u7684 \u201c\u5206\u8bcd\u5355\u4f4d\u201d \u8fdb\u884c\u754c\u5b9a, \u4f46\u8fd9\u4e2a\u8868\u8ff0\u5f88\u6a21\u7cca, \u96be\u4ee5\u64cd\u4f5c. \u6765\u81ea\u8bed\u8a00\u5b66\u548c\u8ba1\u7b97\u8bed\u8a00\u5b66\u4e24\u4e2a\u9886\u57df\u7684\u4e13\u5bb6\u4eec\u5efa\u8bae, \u5e94\u8be5\u641e\u51fa\u4e00\u4e2a\u8bcd\u8868\u4f5c\u4e3a\u5206\u8bcd\u89c4\u8303\u7684\u8865\u5145, \u51e1\u6536\u8fdb\u8bcd\u8868\u7684 \u201c\u5206\u8bcd\u5355\u4f4d\u201d, \u8ba4\u4e3a\u5c31\u662f \u201c\u7ed3\u5408\u7d27\u5bc6, \u4f7f\u7528\u7a33\u5b9a (\u9891\u7e41)\u201d, \u5426\u5219\u4e0d\u662f. \u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f, \u5c06\u8be5\u89c4\u8303\u4e2d\u5b58\u5728\u7684 \u201c\u7070\u8272\u5730\u5e26\u201d \u660e\u786e\u5730\u533a\u5206\u5f00\u6765. \u6b64\u5373\u4e3a\u672c\u9879\u7814\u7a76\u7684\u57fa\u672c\u8003\u91cf.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "\u5168\u7403\u8ba1\u7b97\u8bed\u8a00\u5b66\u7684\u76db\u4f1a COLING\u2014ACL8 \u7b80\u4ecb\n", "abstract": " [\u5173\u952e\u8bcd] \u8ba1\u7b97\u8bed\u8a00\u5b66 \u81ea\u7136\u8bed\u8a00\u5904\u7406 \u57fa\u672c\u540d\u8bcd\u77ed\u8bed \u8bcd\u6c47\u8bed\u4e49\u5b66 \u4fe1\u606f\u62bd\u53d6 \u8bed\u6599\u5e93 \u7814\u8ba8\u4f1a \u53e5\u6cd5\u5206\u6790 \u4fe1\u606f\u68c0\u7d22 \u8bcd\u4e49\u6392\u6b67", "num_citations": "1\n", "authors": ["1181"]}
{"title": "\u5e94\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u95ee\u9898\n", "abstract": " \u7b2c 5 \u5c4a\u5e94\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u56fd\u9645\u4f1a\u8bae\u4e8e 1997 \u5e74 3 \u6708 31 \u65e5\u81f3 4 \u6708 3 \u65e5\u5728\u7f8e\u56fd\u9996\u90fd\u534e\u76db\u987f\u4e3e\u884c. \u4f1a\u8bae\u4e3b\u5e2d\u662f\u7ebd\u7ea6\u5927\u5b66\u8ba1\u7b97\u673a\u79d1\u5b66\u7cfb\u7684 Ralph Grishman \u6559\u6388. \u5171\u6709 342 \u540d\u4e13\u5bb6, \u5b66\u8005\u548c\u8f6f\u4ef6\u4ece\u4e1a\u4eba\u5458\u53c2\u52a0\u4e86\u6b64\u6b21\u76db\u4f1a. \u6211\u662f\u552f\u4e00\u6765\u81ea\u4e2d\u56fd\u7684\u4ee3\u8868. \u672c\u5c4a\u4f1a\u8bae\u7684\u4e3b\u9898\u662f \u201c\u5e94\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u95ee\u9898\u201d. \u5927\u4f1a\u5b89\u6392\u4e86 4 \u4e2a\u8bb2\u5ea7. \u6d89\u53ca \u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u7684 4 \u4e2a\u91cd\u8981\u5e94\u7528\u9886\u57df. \u8bb2\u5ea7 l \u201c\u521b\u5efa\u548c\u5229\u7528 \u81ea\u52a8\u8bed\u8a00\u6807\u6ce8\u8f6f\u4ef6\u201d(\u7f8e\u56fd Johns Hopkins \u5927\u5b66\u8ba1\u7b97\u673a\u79d1\u5b66\u7cfb Eric Brill \u535a\u58eb). \u5176\u5185\u5bb9\u6982\u8981\u5982\u4e0b: \u4e3a\u5b8c\u6210\u590d\u6742\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1, \u9996\u5148\u5fc5\u987b\u53d1\u73b0\u8f93\u5165\u53e5\u5b50\u7684\u6df1\u5c42\u8bed\u8a00\u5b66\u7ed3\u6784. \u76f8\u5173\u7684\u4fe1\u606f\u6709\u8bcd\u7c7b, \u8bcd\u4e49, \u77ed\u8bed\u7ed3\u6784, \u4e0d\u540c\u7c7b\u578b\u7684\u4e13\u540d\u7b49. \u73b0\u5df2\u5f00\u53d1\u51fa\u4e0d\u5c11\u9488\u5bf9\u4e0a\u8ff0\u5404\u7c7b\u4fe1\u606f\u7684\u81ea\u52a8\u6807\u6ce8\u5de5\u5177. \u4e5f\u4ea7\u751f\u4e86\u5404\u79cd\u81ea\u52a8\u8bad\u7ec3\u6807\u6ce8\u5de5\u5177\u7684\u65b9\u6cd5. \u8bb2\u8005\u7efc\u8ff0\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9, \u6307\u51fa\u8fc4\u4eca\u4e3a\u6b62\u6700\u7cbe\u786e\u7684\u81ea\u52a8\u8bad\u7ec3\u7cfb\u7edf\u90fd\u9700\u8981\u5927\u89c4\u6a21\u7684, \u7ecf\u8fc7\u4eba\u5de5\u6807\u6ce8\u7684\u8bed\u6599\u5e93\u624d\u80fd\u5b8c\u6210\u8bad\u7ec3. \u56e0\u6b64, \u5f53\u6b32\u628a\u6807\u6ce8\u5de5\u5177\u79fb\u690d\u5230\u4e00\u4e2a\u65b0\u7684\u9886\u57df\u6216\u4e00\u79cd\u65b0\u7684\u8bed\u8a00\u65f6. \u4ee3\u4ef7\u5c06\u6781\u5176\u9ad8\u6602. \u8bb2\u8005\u4ecb\u7ecd\u4e86\u5b9e\u73b0\u5feb\u901f\u79fb\u690d\u7684\u4e00\u4e9b\u65b0\u6280\u672f, \u5305\u62ec\u4e0d\u9700\u8981\u4eba\u5de5\u6807\u6ce8\u8bed\u6599\u5e93\u5373\u53ef\u5b8c\u6210\u8bad\u7ec3, \u5229\u7528\u6700\u5c11\u8d44\u6e90\u5c06\u6807\u6ce8\u5de5\u5177\u8fc5\u901f\u8c03\u6559\u81f3\u53ef\u9002\u5e94\u4e00\u4e2a\u65b0\u7684\u9886\u57df, \u4ee5\u53ca\u5982\u4f55\u628a\u4eba\u7c7b\u76f4\u89c9\u4e0e\u81ea\u52a8\u83b7\u53d6\u7ed3\u5408\u8d77\u6765\u7b49.\u8bb2\u5ea7 2 \u201c\u5efa\u9020\u5e94\u7528\u578b\u81ea\u7136\u8bed\u8a00\u751f\u6210\u7cfb\u7edf\u201d(\u82cf\u683c\u5170 Aberdeen \u5927\u5b66\u8ba1\u7b97\u673a\u79d1\u5b66\u7cfb EhudRe\u2014iter \u6559\u6388\u548c\u6fb3\u5927\u5229\u4e9a Macquarie \u5927\u5b66\u5fae\u8f6f\u5b66\u9662 Robert Dale \u535a\u58eb). \u5176\u5185\u5bb9\u6982\u8981\u4e3a: \u81ea\u7136\u8bed\u8a00\u751f\u6210 (NLG) \u7cfb\u7edf\u7684\u4efb\u52a1\u662f\u4ece\u4fe1\u606f\u7684\u975e\u8bed\u8a00\u8868\u793a\u4e2d\u4ea7\u751f\u53ef\u7406\u89e3\u7684\u81ea\u7136\u8bed\u8a00\u6587\u672c. NLG \u7cfb\u7edf\u5c06 \u81ea\u7136\u8bed\u8a00\u4e0e\u5176\u81ea\u8eab\u7684\u5e94\u7528\u9886\u57df (\u5982\u81ea\u52a8\u4ea7\u751f\u6587\u4e66, \u62a5\u544a, \u89e3\u91ca, \u5e2e\u52a9\u4fe1\u606f\u4ee5\u53ca\u5176\u4ed6\u7c7b\u578b\u7684\u6587\u672c) \u7ed3\u5408\u8d77\u6765. 9O \u5e74\u4ee3\u540e\u671f\u662f\u5e94\u7528\u578b NLG \u6280\u672f\u6fc0\u52a8\u4eba\u5fc3\u7684\u65f6\u671f. 10 \u5e74\u524d NLG \u4ec5\u4ec5\u662f\u7eaf\u7814\u7a76\u6027\u8d28, \u4f46\u5230 1997 \u5e74, \u5c31\u6709\u51e0\u4e2a NLG \u7cfb\u7edf\u5728\u4e0d\u540c\u9886\u57df\u5f97\u5230\u4e86\u5b9e\u9645\u5e94\u7528, \u5e76\u4e14\u66f4\u591a\u7684\u7cfb\u7edf\u6b63\u5728\u6784\u9020\u4e2d. \u8bb2\u8005\u4ecb\u7ecd\u4e86\u51e0\u79cd\u6d41\u884c\u7684 NLG \u5b9e\u7528\u6280\u672f, \u4ee5\u53ca\u4e16\u754c\u9886\u5148\u7684\u82e5\u5e72\u7814\u7a76\u5de5\u4f5c. \u8bb2\u5ea7 3 \u201c\u8bed\u97f3\u8bc6\u00a0\u2026", "num_citations": "1\n", "authors": ["1181"]}
{"title": "CSeg&Tag1. 0: A Practical Word Segmenter and POS Tagger\n", "abstract": " Chinese word segmentation and POS tagging are two key techniques in many applications in Chinese information processing. Great efforts have been paid to the research in the last decade, but unfortunately, no practical system with high performance for unrestricted texts is available up to date. CSeg&Tag1. 0, a Chinese word segmenter and POS tagger which unifies these two procedures into one model, is introduced in this paper. The preliminary open tests show that the segmentation precision of CSeg&Tag1. 0 is about 98.0%-99.3%, POS tagging precision about 91.0%-97.1%, and the recall and precision for unknown words are ranging from95.0% to 99.0% and from 87. 6% to 95.3% respectively. The processing speed is about 100 characters per second on Pentium 133 PC. The work of improving the performance of the system is still ongoing. ambiguity and the sentence (2) and (3) examples of unknown word.(la) \u8fd9\u4e2a\u7814\u7a76\u6240\u5f88\u6709\u540d,(16) \u8fd9\u9879\u7814\u7a76\u6240\u6d89\u53ca\u7684\u95ee\u9898\u5f88\u590d\u6742.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "\u6c49\u5b57\u6587\u672c\u8bc6\u522b\u7684\u81ea\u52a8\u540e\u5904\u7406\n", "abstract": " \u5728\u6c49\u5b57\u8bc6\u522b\u7387\u4e0d\u662f\u5f88\u9ad8\u65f6,\u53ef\u5229\u7528\u4e0a\u4e0b\u6587\u76f8\u5173\u4fe1\u606f(\u8bcd\u6c47\u53ca\u5b57\u5b57\u76f8\u90bb\u51fa\u73b0\u6982\u7387),\u628a\u4e00\u4e2a\u6c49\u8bed\u53e5\u5b50\u6216\u77ed\u8bed\u4f5c\u4e3a\u4e00\u4e2a\u5904\u7406\u5355\u5143,\u5229\u7528\u8bcd\u5339\u914d\u4fe1\u606f\u4fee\u6539\u6c49\u5b57\u8bc6\u522b\u7684\u53ef\u4fe1\u5ea6,\u518d\u7528\u52a8\u6001\u89c4\u5212\u65b9\u6cd5\u627e\u5230\u6700\u4f73\u7ed3\u679c.\u8fd9\u79cd\u57fa\u4e8e\u7edf\u8ba1\u6982\u7387\u548c\u8bcd\u6c47\u5339\u914d\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5,\u5bf9\u8131\u673a\u624b\u5199\u6c49\u5b57\u8bc6\u522b\u6587\u672c\u8fdb\u884c\u81ea\u52a8\u540e\u5904\u7406,\u83b7\u5f97\u4e86\u4ee4\u4eba\u6ee1\u610f\u7684\u6548\u679c", "num_citations": "1\n", "authors": ["1181"]}
{"title": "\u8ba1\u7b97\u673a\u8bed\u8a00\u5b66\u65b9\u6cd5\u5728\u4e2d\u6587\u6587\u5b57\u8bc6\u522b\u540e\u5904\u7406\u4e2d\u7684\u5e94\u7528\n", "abstract": " \u8fd1\u5e74\u6765, \u8131\u673a\u624b\u5199\u6c49\u5b57\u8bc6\u522b [OCR] \u7814\u7a76\u53d6\u5f97\u4e86\u5f88\u5927\u8fdb\u6b65, \u8bc6\u522b\u7387\u5728\u63d0\u9ad8, \u524d\u5341\u9009\u8bc6\u522b\u53ef\u8fbe 95% \u4ee5\u4e0a, \u4f46\u7b2c\u9009\u8bc6\u522b\u7387\u4ecd\u4e0d\u591f\u9ad8. OCR \u7684\u7814\u7a76\u8005\u4eec\u5df2\u770b\u5230\u5355\u7eaf\u7684\u5355\u5b57 (Isolatedcharacter) \u8bc6\u522b\u7684\u65b9\u6cd5\u5bf9\u6574\u4e2a\u6587\u672c\u8bc6\u522b\u662f\u4e0d\u591f\u7684.", "num_citations": "1\n", "authors": ["1181"]}
{"title": "\u4e2d\u6587\u4fe1\u606f\u5904\u7406\u6700\u65b0\u6210\u679c\u7684\u68c0\u9605\u2014\u2014\u8bb0\u65b0\u52a0\u5761\u4e2d\u6587\u7535\u8111\u56fd\u9645\u4f1a\u8bae ICCC\u201996\n", "abstract": " <\u6b63> 1.\u4f1a\u8bae\u6982\u51b5 \u7531\u65b0\u52a0\u5761\u4e3e\u529e\u7684\u4e2d\u6587\u7535\u8111\u56fd\u9645\u4f1a\u8bae(International Conference on Chinese Computing,\u7b80\u79f0ICCC)\u8d77\u6e90\u4e8e1986\u5e748\u6708.\u56fd\u7acb\u65b0\u52a0\u5761\u5927\u5b66\u4e3e\u529e\u7684ICCC\u201986\u6807\u5fd7\u7740\u8be5\u56fd\u4e2d\u6587\u4fe1\u606f\u5904\u7406\u7814\u7a76\u7684\u5f00\u59cb.\u7d27\u63a5\u7740,1988\u5e748\u6708\u65b0\u52a0\u5761\u7684\u4e2d\u6587\u4e0e\u4e1c\u65b9\u8bed\u8a00\u4fe1\u606f\u5904\u7406\u5b66\u4f1a(Chinese and Oriental Languages Information Processing Society,\u7b80\u79f0COLIPS)\u5ba3\u544a\u6210\u7acb,\u5e76\u51fa\u7248\u4e86\u8be5\u5b66\u4f1a\u7684\u5b66\u672f\u520a\u7269\u300aCOLIPS\u901a\u8baf\u300b.\u6b64\u540e,COLIPS\u540c\u4e2d\u56fd\u4e2d\u6587\u4fe1\u606f\u5b66\u4f1a\u5efa\u7acb\u4e86\u4eb2\u5bc6\u7684\u5408\u4f5c\u5173\u7cfb,\u5b83\u66fe\u5148\u540e8", "num_citations": "1\n", "authors": ["1181"]}
{"title": "\u85cf\u6587\u5b57\u540c\u73b0\u7f51\u7edc\u7684\u5c0f\u4e16\u754c\u6548\u5e94\u548c\u65e0\u6807\u5ea6\u7279\u6027\n", "abstract": " \u6458! \u8981\" \u590d\u6742\u7f51\u7edc\u5177\u6709\u81ea\u00ae & \u81ea\u76f8& \u5438\u00b1\u00b2&\u00b3\u754c &\u00b5 \u6807\u5ea6\u4e2d@ \u5206\u00b6\u00b7@ \u6027\u8d28! \u800c\u8bed\u8a00\u6587\u4f5c\u4e3a & \u7c7b\u667a \u00b9 \u548c\u6587) \u7684\u7ed3 \u00ba! \u662f q \u8fc7\u00bb \u00bc\u00bd \u5316/\u6210\u7684\u590d\u6742\u7f51\u7edc% \u8be5\u6587\u5bf9 \u00be \u8bed\u00bf \u00c0&\u00c1 \u6587 &\u00c2\u00c3&\u00c4\u00c5&\u00c5\u00c6 \u548c 8 \u8bed\u7b49 \u00c7 \u7c7b\u5177\u6709\u4ee3\u8868\u6027\u7684\u4f53 \u00c8 \u8bed! \u00c9 \u7c7b\u5404\u53d6# W\u00ca\u00cb? $ \u00ca \u6587 \u00cc% \u4e86?> &\u00be \u6587\u540c\u73b0\u7f51\u7edc! \u5206\u6790\u4e86 \u00be \u6587\u540c\u73b0\u7f51\u7edc\u7684\u6700 \u00cd \u8def \u00ce\u00bc \u5ea6 &\u00cf \u7c7b% \u6570\u548c\u5ea6\u5206 \u00d0! \u5b9e\u00a5 \u6570\u636e \u793a?> &\u00be \u6587\u540c\u73b0\u7f51\u7edc \u00d1 \u5177\u6709 \u00b3\u754c\u6548\u5e94\u548c \u00b5 \u6807\u5ea6\u7279\u6027! \u8868) \u00be \u6587\u540c\u73b0\u7f51\u7edc \u00d1 \u5177\u6709 \u00b3\u754c\u6548\u5e94\u548c \u00b5 \u6807\u5ea6\u7279\u6027%", "num_citations": "1\n", "authors": ["1181"]}
{"title": "Build a Chinese Treebank as the test suite for Chinese parser\n", "abstract": " This paper will introduce our current work to build a Chinese treebank that can be used as a test suite for Chinese parser. The treebank will consist of 10,000 Chinese sentences extracted from a Chinese balanced corpus with about 2,000,000 Chinese characters. The corpus has already been annotated with correct segmentation and Part-Of-Speech (POS) information. The following issues will be discussed in the paper: the survey of the balanced corpus, the strategies and methods for sampling the treebank sentences, the processing schemes and tools for treebank construction.", "num_citations": "1\n", "authors": ["1181"]}