{"title": "Beauty and the beast: Diverting modern web browsers to build unique browser fingerprints\n", "abstract": " Worldwide, the number of people and the time spent browsing the web keeps increasing. Accordingly, the technologies to enrich the user experience are evolving at an amazing pace. Many of these evolutions provide for a more interactive web (e.g., boom of JavaScript libraries, weekly innovations in HTML5), a more available web (e.g., explosion of mobile devices), a more secure web (e.g., Flash is disappearing, NPAPI plugins are being deprecated), and a more private web (e.g., increased legislation against cookies, huge success of extensions such as Ghostery and AdBlock). Nevertheless, modern browser technologies, which provide the beauty and power of the web, also provide a darker side, a rich ecosystem of exploitable data that can be used to build unique browser fingerprints. Our work explores the validity of browser fingerprinting in today's environment. Over the past year, we have collected 118,934\u00a0\u2026", "num_citations": "228\n", "authors": ["320"]}
{"title": "Validation in model-driven engineering: testing model transformations\n", "abstract": " The OMG's model-driven architecture is quickly attracting attention as a method of constructing systems that offers advantages over traditional approaches in terms of reliability, consistency, and maintainability. The key concepts in the MDA are models that are related by model transformations. However, for the MDA to provide an adequate alternative to existing approaches, it must offer comparable support for software engineering processes such as requirements analysis, design and testing. This paper attempts to explore the application of the last of these processes, testing, to the most novel part of the MDA, that of model transformation. We present a general view of the roles of testing in the different stages of model-driven development, and a more detailed exploration of approaches to testing model transformations. Based on this, we highlight the particular issues for the different testing tasks, including adequacy\u00a0\u2026", "num_citations": "167\n", "authors": ["320"]}
{"title": "Testing security policies: going beyond functional testing\n", "abstract": " While important efforts are dedicated to system functional testing, very few works study how to test specifically security mechanisms, implementing a security policy. This paper introduces security policy testing as a specific target for testing. We propose two strategies for producing security policy test cases, depending if they are built in complement of existing functional test cases or independently from them. Indeed, any security policy is strongly connected to system functionality: testing functions includes exercising many security mechanisms. However, testing functionality does not intend at putting to the test security aspects. We thus propose test selection criteria to produce tests from a security policy. To quantify the effectiveness of a set of test cases to detect security policy flaws, we adapt mutation analysis and define security policy mutation operators. A library case study, a 3-tiers architecture, is used to obtain\u00a0\u2026", "num_citations": "107\n", "authors": ["320"]}
{"title": "Pacogen: Automatic generation of pairwise test configurations from feature models\n", "abstract": " Feature models are commonly used to specify variability in software product lines. Several tools support feature models for variability management at different steps in the development process. However, tool support for test configuration generation is currently limited. This test generation task consists in systematically selecting a set of configurations that represent a relevant sample of the variability space and that can be used to test the product line. In this paper we propose \\pw tool to analyze feature models and automatically generate a set of configurations that cover all pair wise interactions between features. \\pw tool relies on constraint programming to generate configurations that satisfy all constraints imposed by the feature model and to minimize the set of the tests configurations. This work also proposes an extensive experiment, based on the state-of-the art SPLOT feature models repository, showing that \\pw\u00a0\u2026", "num_citations": "103\n", "authors": ["320"]}
{"title": "The multiple facets of software diversity: Recent developments in year 2000 and beyond\n", "abstract": " Early experiments with software diversity in the mid 1970s investigated N-version programming and recovery blocks to increase the reliability of embedded systems. Four decades later, the literature about software diversity has expanded in multiple directions: goals (fault tolerance, security, software engineering), means (managed or automated diversity), and analytical studies (quantification of diversity and its impact). Our article contributes to the field of software diversity as the first work that adopts an inclusive vision of the area, with an emphasis on the most recent advances in the field. This survey includes classical work about design and data diversity for fault tolerance, as well as the cybersecurity literature that investigates randomization at different system levels. It broadens this standard scope of diversity to include the study and exploitation of natural diversity and the management of diverse software products\u00a0\u2026", "num_citations": "98\n", "authors": ["320"]}
{"title": "Modeling modeling modeling\n", "abstract": " Model-driven engineering and model-based approaches have permeated all branches of software engineering to the point that it seems that we are using models, as Moli\u00e8re\u2019s Monsieur Jourdain was using prose, without knowing it. At the heart of modeling, there is a relation that we establish to represent something by something else. In this paper we review various definitions of models and relations between them. Then, we define a canonical set of relations that can be used to express various kinds of representation relations and we propose a graphical concrete syntax to represent these relations. We also define a structural definition for this language in the form of a metamodel and a formal interpretation using Prolog. Hence, this paper is a contribution towards a theory of modeling.", "num_citations": "81\n", "authors": ["320"]}
{"title": "Hiding in the crowd: an analysis of the effectiveness of browser fingerprinting at large scale\n", "abstract": " Browser fingerprinting is a stateless technique, which consists in collecting a wide range of data about a device through browser APIs. Past studies have demonstrated that modern devices present so much diversity that fingerprints can be exploited to identify and track users online. With this work, we want to evaluate if browser fingerprinting is still effective at uniquely identifying a large group of users when analyzing millions of fingerprints over a few months. We collected 2,067,942 browser fingerprints from one of the top 15 French websites. The analysis of this novel dataset sheds a new light on the ever-growing browser fingerprinting domain. The key insight is that the percentage of unique fingerprints in our dataset is much lower than what was reported in the past: only 33.6% of fingerprints are unique by opposition to over 80% in previous studies. We show that non-unique fingerprints tend to be fragile. If some\u00a0\u2026", "num_citations": "80\n", "authors": ["320"]}
{"title": "Modeling model slicers\n", "abstract": " Among model comprehension tools, model slicers are tools that extract a subset from a model, for a specific purpose. Model slicers are tools that let modelers rapidly gather relevant knowledge from large models. However, existing slicers are dedicated to one modeling language. This is an issue when we observe that new domain specific modeling languages (DSMLs), for which we want slicing abilities, are created almost on a daily basis. This paper proposes the Kompren language to model and generate model slicers for any DSL (e.g. software development and building architecture) and for different purposes (e.g. monitoring and model comprehension). Kompren\u2019s abilities for model slicers construction is based on case studies from various domains.", "num_citations": "60\n", "authors": ["320"]}
{"title": "Mitigating browser fingerprint tracking: multi-level reconfiguration and diversification\n", "abstract": " The diversity of software components (e.g., Browsers, plugins, fonts) is a wonderful opportunity for users to customize their platforms. Yet, massive customization creates a privacy issue: browsers are slightly different from one another, allowing third parties to collect unique and stable fingerprints to track users. Although software diversity appears to be the source of this privacy issue, we claim that this same diversity, combined with automatic reconfiguration, provides the essential ingredients to constantly change browsing platforms. Constant change acts as a moving target defense strategy against fingerprint tracking by breaking one essential property: stability over time. We leverage virtualization and modular architectures to automatically assemble and reconfigure software components at multiple levels. We operate on operating systems, browsers, fonts and plugins. This work is the first application of software\u00a0\u2026", "num_citations": "57\n", "authors": ["320"]}
{"title": "Browser fingerprinting: A survey\n", "abstract": " With this article, we survey the research performed in the domain of browser fingerprinting, while providing an accessible entry point to newcomers in the field. We explain how this technique works and where it stems from. We analyze the related work in detail to understand the composition of modern fingerprints and see how this technique is currently used online. We systematize existing defense solutions into different categories and detail the current challenges yet to overcome.", "num_citations": "54\n", "authors": ["320"]}
{"title": "FPRandom: Randomizing core browser objects to break advanced device fingerprinting techniques\n", "abstract": " The rich programming interfaces (APIs) provided by web browsers can be diverted to collect a browser fingerprint. A small number of queries on these interfaces are sufficient to build a fingerprint that is statistically unique and very stable over time. Consequently, the fingerprint can be used to track users. Our work aims at mitigating the risk of browser fingerprinting for users privacy by \u2018breaking\u2019 the stability of a fingerprint over time. We add randomness in the computation of selected browser functions, in order to have them deliver slightly different answers for each browsing session. Randomization is possible thanks to the following properties of browsers implementations: (i) some functions have a nondeterministic specification, but a deterministic implementation; (ii) multimedia functions can be slightly altered without deteriorating user\u2019s perception. We present FPRandom, a modified version of Firefox that\u00a0\u2026", "num_citations": "53\n", "authors": ["320"]}
{"title": "Classifying and qualifying GUI defects\n", "abstract": " Graphical user interfaces (GUIs) are integral parts of software systems that require interactions from their users. Software testers have paid special attention to GUI testing in the last decade, and have devised techniques that are effective in finding several kinds of GUI errors. However, the introduction of new types of interactions in GUIs (e.g., direct manipulation) presents new kinds of errors that are not targeted by current testing techniques. We believe that to advance GUI testing, the community needs a comprehensive and high level GUI fault model, which incorporates all types of interactions. The work detailed in this paper establishes 4 contributions: 1) A GUI fault model designed to identify and classify GUI faults. 2) An empirical analysis for assessing the relevance of the proposed fault model against failures found in real GUIs. 3) An empirical assessment of two GUI testing tools (i.e. GUITAR and Jubula) against\u00a0\u2026", "num_citations": "50\n", "authors": ["320"]}
{"title": "Kompren: modeling and generating model slicers\n", "abstract": " Among model comprehension tools, model slicers are tools that extract a subset of model elements, for a specific purpose. Model slicers provide a mechanism to isolate and focus on parts of the model, thereby improving the overall analysis process. However, existing slicers are dedicated to a specific modeling language. This is an issue when we observe that new domain specific modeling languages, for which we want slicing abilities, are created almost on a daily basis. This paper proposes the Kompren language to model and generate model slicers for any DSL (e.g. modeling for software development or for civil engineering) and for different purposes (e.g. monitoring and model comprehension). We detail the semantics of the Kompren language and of the model slicer generator. This provides a set of expected properties about the slices that are extracted by the different forms of the slicer. Then we\u00a0\u2026", "num_citations": "50\n", "authors": ["320"]}
{"title": "Supporting efficient and advanced omniscient debugging for xDSMLs\n", "abstract": " Omniscient debugging is a promising technique that relies on execution traces to enable free traversal of the states reached by a system during an execution. While some General-Purpose Languages (GPLs) already have support for omniscient debugging, developing such a complex tool for any executable Domain-Specific Modeling Language (xDSML) remains a challenging and error prone task. A solution to this problem is to define a generic omniscient debugger for all xDSMLs. However, generically supporting any xDSML both compromises the efficiency and the usability of such an approach. Our contribution relies on a partly generic omniscient debugger supported by generated domain-specific trace management facilities. Being domain-specific, these facilities are tuned to the considered xDSML for better efficiency. Usability is strengthened by providing multidimensional omniscient debugging. Results show\u00a0\u2026", "num_citations": "49\n", "authors": ["320"]}
{"title": "A generic metamodel for security policies mutation\n", "abstract": " We present a new approach for mutation analysis of security policies test cases. We propose a metamodel that provides a generic representation of security policies access control models and define a set of mutation operators at this generic level. We use Kermeta to build the metamodel and implement the mutation operators. We also illustrate our approach with two successful instantiation of this metamodel: we defined policies with RBAC and OrBAC and mutated these policies.", "num_citations": "47\n", "authors": ["320"]}
{"title": "Leveraging software product lines engineering in the development of external dsls: A systematic literature review\n", "abstract": " The use of domain-specific languages (DSLs) has become a successful technique in the development of complex systems. Consequently, nowadays we can find a large variety of DSLs for diverse purposes. However, not all these DSLs are completely different; many of them share certain commonalities coming from similar modeling patterns \u2013 such as state machines or petri nets \u2013 used for several purposes. In this scenario, the challenge for language designers is to take advantage of the commonalities existing among similar DSLs by reusing, as much as possible, formerly defined language constructs. The objective is to leverage previous engineering efforts to minimize implementation from scratch. To this end, recent research in software language engineering proposes the use of product line engineering, thus introducing the notion of language product lines. Nowadays, there are several approaches that result\u00a0\u2026", "num_citations": "45\n", "authors": ["320"]}
{"title": "Multitier diversification in web-based software applications\n", "abstract": " Web application development benefits massively from modular architectures and reuse. This excellent software engineering practice is also the source of a new form of monoculture in application-level co de, which creates a potential risk for dependability. Researchers propose using software diversification in multiple components of Web applications to reconcile the tension between reuse and dependability. This article identifies key enablers for the effective diversification of software, especially at the application-code level. It's possible to combine different software diversification strategies, from deploying different vendor solutions to fine-grained code transformations, to provide different forms of protection.", "num_citations": "36\n", "authors": ["320"]}
{"title": "\" May the fork be with you\": novel metrics to analyze collaboration on GitHub\n", "abstract": " Multi-repository software projects are becoming more and more popular, thanks to web-based facilities such as Github. Code and process metrics generally assume a single repository must be analyzed, in order to measure the characteristics of a codebase. Thus they are not apt to measure how much relevant information is hosted in multiple repositories contributing to the same codebase. Nor can they feature the characteristics of such a distributed development process. We present a set of novel metrics, based on an original classification of commits, conceived to capture some interesting aspects of a multi-repository development process. We also describe an efficient way to build a data structure that allows to compute these metrics on a set of Git repositories. Interesting outcomes, obtained by applying our metrics on a large sample of projects hosted on Github, show the usefulness of our contribution.", "num_citations": "35\n", "authors": ["320"]}
{"title": "Omniscient debugging for executable DSLs\n", "abstract": " Omniscient debugging is a promising technique that relies on execution traces to enable free traversal of the states reached by a model (or program) during an execution. While a few General-Purpose Languages (GPLs) already have support for omniscient debugging, developing such a complex tool for any executable Domain Specific Language (DSL) remains a challenging and error prone task. A generic solution must: support a wide range of executable DSLs independently of the metaprogramming approaches used for implementing their semantics; be efficient for good responsiveness. Our contribution relies on a generic omniscient debugger supported by efficient generic trace management facilities. To support a wide range of executable DSLs, the debugger provides a common set of debugging facilities, and is based on a pattern to define runtime services independently of metaprogramming approaches\u00a0\u2026", "num_citations": "34\n", "authors": ["320"]}
{"title": "Modeling modeling\n", "abstract": " Model-driven engineering and model-based approaches have permeated all branches of software engineering; to the point that it seems that we are using models, as Moli\u00e8re\u2019s Monsieur Jourdain was using prose, without knowing it. At the heart of modeling, there is a relation that we establish to represent something by something else. In this paper we review various definitions of models and relations between them. Then, we define a canonical set of relations that can be used to express various kinds of representation relations and we propose a graphical concrete syntax to represent these relations. Hence, this paper is a contribution towards a theory of modeling.", "num_citations": "34\n", "authors": ["320"]}
{"title": "Practical minimization of pairwise-covering test configurations using constraint programming\n", "abstract": " Context: Testing highly-configurable software systems is challenging due to a large number of test configurations that have to be carefully selected in order to reduce the testing effort as much as possible, while maintaining high software quality. Finding the smallest set of valid test configurations that ensure sufficient coverage of the system\u2019s feature interactions is thus the objective of validation engineers, especially when the execution of test configurations is costly or time-consuming. However, this problem is NP-hard in general and approximation algorithms have often been used to address it in practice.Objective: In this paper, we explore an alternative exact approach based on constraint programming that will allow engineers to increase the effectiveness of configuration testing while keeping the number of configurations as low as possible.Method: Our approach consists in using a (time-aware) minimization\u00a0\u2026", "num_citations": "33\n", "authors": ["320"]}
{"title": "Tailored source code transformations to synthesize computationally diverse program variants\n", "abstract": " The predictability of program execution provides attackers a rich source of knowledge who can exploit it to spy or remotely control the program. Moving target defense ad-dresses this issue by constantly switching between many di-verse variants of a program, which reduces the certainty that an attacker can have about the program execution. The ef-fectiveness of this approach relies on the availability of a large number of software variants that exhibit dierent ex-ecutions. However, current approaches rely on the natural diversity provided by o-the-shelf components, which is very limited. In this paper, we explore the automatic synthe-sis of large sets of program variants, called sosies. Sosies provide the same expected functionality as the original pro-gram, while exhibiting dierent executions. They are said to be computationally diverse. This work addresses two objectives: comparing dierent transformations for\u00a0\u2026", "num_citations": "33\n", "authors": ["320"]}
{"title": "Empirical evidence of large-scale diversity in api usage of object-oriented software\n", "abstract": " In this paper, we study how object-oriented classes are used across thousands of software packages. We concentrate on \u201cusage diversity\u201d, defined as the different statically observable combinations of methods called on the same object. We present empirical evidence that there is a significant usage diversity for many classes. For instance, we observe in our dataset that Java's String is used in 2460 manners. We discuss the reasons of this observed diversity and the consequences on software engineering knowledge and research.", "num_citations": "32\n", "authors": ["320"]}
{"title": "Advanced and efficient execution trace management for executable domain-specific modeling languages\n", "abstract": " Executable Domain-Specific Modeling Languages (xDSMLs) enable the application of early dynamic verification and validation (V&V) techniques for behavioral models. At the core of such techniques, execution traces are used to represent the evolution of models during their execution. In order to construct execution traces for any xDSML, generic trace metamodels can be used. Yet, regarding trace manipulations, generic trace metamodels lack efficiency in time because of their sequential structure, efficiency in memory because they capture superfluous data, and usability because of their conceptual gap with the considered xDSML. Our contribution is a novel generative approach that defines a multidimensional and domain-specific trace metamodel enabling the construction and manipulation of execution traces for models conforming to a given xDSML. Efficiency in time is improved by providing a variety\u00a0\u2026", "num_citations": "31\n", "authors": ["320"]}
{"title": "Diversify: ecology-inspired software evolution for diversity emergence\n", "abstract": " DIVERSIFY is an EU funded project, which aims at favoring spontaneous diversification in software systems in order to increase their adaptive capacities. This objective is founded on three observations: software has to constantly evolve to face unpredictable changes in its requirements, execution environment or to respond to failure (bugs, attacks, etc.): the emergence and maintenance of high levels of diversity are essential to provide adaptive capacities to many forms of complex systems, ranging from ecological and biological systems to social and economical systems; diversity levels tend to be very low in software systems. DIVERSIFY explores how the biological evolutionary mechanisms, which sustain high levels of biodiversity in ecosystems (speciation, phenotypic plasticity and natural selection) can be translated in software evolution principles. In this work, we consider evolution as a driver for diversity as a\u00a0\u2026", "num_citations": "29\n", "authors": ["320"]}
{"title": "Aligning SysML with the B method to provide V&V for systems engineering\n", "abstract": " Systems engineering, and especially the modeling of safety critical systems, needs proper means for early Validation and Verification (V&V) to detect critical issues as soon as possible. The objective of our work is to identify a verifiable subset of SysML that is usable by system engineers, while still amenable to automatic transformation towards formal verification tools. As we are interested in proving safety properties expressed using invariants on states, we consider the B method for this purpose. Our approach consists in an alignment of SysML concepts with an identified subset of the B method, using semantic similarities between both languages. We define a restricted SysML extended by a lightweight profile and a transformation towards the B method for V&V purposes. The obtained process is applied to a simplified concrete case study from the railway industry: a SysML model is designed with safety properties\u00a0\u2026", "num_citations": "29\n", "authors": ["320"]}
{"title": "A generative approach to define rich domain-specific trace metamodels\n", "abstract": " Executable Domain-Specific Modeling Languages (xDSMLs) open many possibilities for performing early verification and validation (V&V) of systems. Dynamic V&V approaches rely on execution traces, which represent the evolution of models during their execution. In order to construct traces, generic trace metamodels can be used. Yet, regarding trace manipulations, they lack both efficiency because of their sequential structure, and usability because of their gap to the xDSML. Our contribution is a generative approach that defines a rich and domain-specific trace metamodel enabling the construction of execution traces for models conforming to a given xDSML. Efficiency is increased by providing a variety of navigation paths within traces, while usability is improved by narrowing the concepts of the trace metamodel to fit the considered xDSML. We evaluated our approach by generating a trace metamodel\u00a0\u2026", "num_citations": "25\n", "authors": ["320"]}
{"title": "A comprehensive study of pseudo-tested methods\n", "abstract": " Pseudo-tested methods are defined as follows: they are covered by the test suite, yet no test case fails when the method body is removed, i.e., when all the effects of this method are suppressed. This intriguing concept was coined in 2016, by Niedermayr and colleagues, who showed that such methods are systematically present, even in well-tested projects with high statement coverage. This work presents a novel analysis of pseudo-tested methods. First, we run a replication of Niedermayr\u2019s study with 28K+ methods, enhancing its external validity thanks to the use of new tools and new study subjects. Second, we perform a systematic characterization of these methods, both quantitatively and qualitatively with an extensive manual analysis of 101 pseudo-tested methods. The first part of the study confirms Niedermayr\u2019s results: pseudo-tested methods exist in all our subjects. Our in-depth characterization of\u00a0\u2026", "num_citations": "23\n", "authors": ["320"]}
{"title": "Bridging the chasm between executable metamodeling and models of computation\n", "abstract": " The complete and executable definition of a Domain Specific Language (DSL) includes the specification of two essential facets: a model of the domain-specific concepts with actions and their semantics; and a scheduling model that orchestrates the actions of a domain-specific model. Metamodels can capture the former facet, while Models of Computation (MoCs) capture the latter facet. Unfortunately, theories and tools for metamodeling and MoCs have evolved independently, creating a cultural and technical chasm between the two communities. Consequently, there is currently no framework to explicitly model and compose both facets of a DSL. This paper introduces a new framework to combine a metamodel and a MoC in a modular fashion. This allows (i) the complete and executable definition of a DSL, (ii) the reuse of a given MoC for different domain-specific metamodels, and (iii) the use of different\u00a0\u2026", "num_citations": "23\n", "authors": ["320"]}
{"title": "A UML-based concept for high concurrency: the real-time object\n", "abstract": " Real-time (RT) applications are designed to control systems that are inherently parallel. For easing development, abstractions are mandatory to model this concurrency. To achieve this goal, the real-time object paradigm is proposed. It is used to demonstrate how to separate functional and concurrency concerns ensuring also high-level abstraction for parallelism modeling", "num_citations": "22\n", "authors": ["320"]}
{"title": "A chaos engineering system for live analysis and falsification of exception-handling in the JVM\n", "abstract": " Software systems contain resilience code to handle those failures and unexpected events happening in production. It is essential for developers to understand and assess the resilience of their systems. Chaos engineering is a technology that aims at assessing resilience and uncovering weaknesses by actively injecting perturbations in production. In this paper, we propose a novel design and implementation of a chaos engineering system in Java called ChaosMachine. It provides a unique and actionable analysis on exception-handling capabilities in production, at the level of try-catch blocks. To evaluate our approach, we have deployed ChaosMachine on top of 3 large-scale and well-known Java applications totaling 630k lines of code. Our results show that ChaosMachine reveals both strengths and weaknesses of the resilience code of a software system at the level of exception handling.", "num_citations": "21\n", "authors": ["320"]}
{"title": "Automatic test improvement with DSpot: a study with ten mature open-source projects\n", "abstract": " In the literature, there is a rather clear segregation between manually written tests by developers and automatically generated ones. In this paper, we explore a third solution: to automatically improve existing test cases written by developers. We present the concept, design and implementation of a system called DSpot, that takes developer-written test cases as input (JUnit tests in Java) and synthesizes improved versions of them as output. Those test improvements are given back to developers as patches or pull requests, that can be directly integrated in the main branch of the test code base. We have evaluated DSpot in a deep, systematic manner over 40 real-world unit test classes from 10 notable and open-source software projects. We have amplified all test methods from those 40 unit test classes. In 26/40 cases, DSpot is able to automatically improve the test under study, by triggering new behaviors and\u00a0\u2026", "num_citations": "21\n", "authors": ["320"]}
{"title": "On model-based testing advanced GUIs\n", "abstract": " Graphical User Interface (GUI) design is currently shifting from designing GUIs composed of standard widgets to designing GUIs relying on more natural interactions and ad hoc widgets. This shift is meant to support the advent of GUIs providing users with more adapted and natural interactions, and the support of new input devices such as multi-touch screens. Standard widgets (e.g. buttons) are more and more replaced by ad hoc ones (e.g. the drawing area of graphical editors), and interactions are shifting from mono-event (e.g. button pressures) to multi-event interactions (e.g. multi-touch and gesture-based interactions). As a consequence, the current GUI model-based testing approaches, which target event-based systems, show their limits when applied to test such new advanced GUIs. The work introduced in this paper establishes three contributions: a precise analysis of the reasons of these current limits; a\u00a0\u2026", "num_citations": "21\n", "authors": ["320"]}
{"title": "Improving maintenance in aop through an interaction specification framework\n", "abstract": " The invasiveness of aspects is beneficial to modularize crosscutting concerns that require the modification of the data or control flow. However, it introduces subtle errors that are hard to locate and fix in case of evolution. In this paper we illustrate this issue by evolving a program implemented using aspects. Interaction issues, between aspects and the program, emerge from this evolution. We locate them through manual inspection and test execution. This tedious process motivates the need for an abstract specification of intended interactions. To tackle this issue, we propose a framework for specifying the types of invasiveness pattern that are allowed of forbidden in the program. We have also implemented a tool that automatically checks whether the specification is satisfied by the aspects.", "num_citations": "21\n", "authors": ["320"]}
{"title": "An analysis of metamodeling practices for MOF and OCL\n", "abstract": " The definition of a metamodel that precisely captures domain knowledge for effective know-how capitalization is a challenging task. A major obstacle for domain experts who want to build a metamodel is that they must master two radically different languages: an object-oriented, MOF-compliant, modeling language to capture the domain structure and first order logic (the Object Constraint Language) for the definition of well-formedness rules. However, there are no guidelines to assist the conjunct usage of both paradigms, and few tools support it. Consequently, we observe that most metamodels have only an object-oriented domain structure, leading to inaccurate metamodels. In this paper, we perform the first empirical study, which analyzes the current state of practice in metamodels that actually use logical expressions to constrain the structure. We analyze 33 metamodels including 995 rules coming from industry\u00a0\u2026", "num_citations": "20\n", "authors": ["320"]}
{"title": "On analyzing the topology of commit histories in decentralized version control systems\n", "abstract": " Empirical analysis of software repositories usually deals with linear histories derived from centralized versioning systems. Decentralized version control systems allow a much richer structure of commit histories, which presents features that are typical of complex graph models. In this paper we bring some evidences of how the very structure of these commit histories carries relevant information about the distributed development process. By means of a novel data structure that we formally define, we analyze the topological characteristics of commit graphs of a sample of GIT projects. Our findings point out the existence of common recurrent structural patterns which identically occur in different projects and can be consider building blocks of distributed collaborative development.", "num_citations": "19\n", "authors": ["320"]}
{"title": "Artificial table testing dynamically adaptive systems\n", "abstract": " Dynamically Adaptive Systems (DAS) are systems that modify their behavior and structure in response to changes in their surrounding environment. Critical mission systems increasingly incorporate adaptation and response to the environment; examples include disaster relief and space exploration systems. These systems can be decomposed in two parts: the adaptation policy that specifies how the system must react according to the environmental changes and the set of possible variants to reconfigure the system. A major challenge for testing these systems is the combinatorial explosions of variants and envi-ronment conditions to which the system must react. In this paper we focus on testing the adaption policy and propose a strategy for the selection of envi-ronmental variations that can reveal faults in the policy. Artificial Shaking Table Testing (ASTT) is a strategy inspired by shaking table testing (STT), a technique widely used in civil engineering to evaluate building's structural re-sistance to seismic events. ASTT makes use of artificial earthquakes that simu-late violent changes in the environmental conditions and stresses the system adaptation capability. We model the generation of artificial earthquakes as a search problem in which the goal is to optimize different types of envi-ronmental variations.", "num_citations": "18\n", "authors": ["320"]}
{"title": "The maven dependency graph: a temporal graph-based representation of maven central\n", "abstract": " The Maven Central Repository provides an extraordinary source of data to understand complex architecture and evolution phenomena among Java applications. As of September 6, 2018, this repository includes 2.8M artifacts (compiled piece of code implemented in a JVM-based language), each of which is characterized with metadata such as exact version, date of upload and list of dependencies towards other artifacts. Today, one who wants to analyze the complete ecosystem of Maven artifacts and their dependencies faces two key challenges: (i) this is a huge data set; and (ii) dependency relationships among artifacts are not modeled explicitly and cannot be queried. In this paper, we present the Maven Dependency Graph. This open source data set provides two contributions: a snapshot of the whole Maven Central taken on September 6, 2018, stored in a graph database in which we explicitly model all\u00a0\u2026", "num_citations": "17\n", "authors": ["320"]}
{"title": "INCREMENT: A mixed MDE-IR approach for regulatory requirements modeling and analysis\n", "abstract": " [Context and motivation] Regulatory requirements for Nuclear instrumentation and control (I&C) systems are first class requirements. They are written by national safety entities and are completed through a large documentation set of national recommendation guides and national/international standards. [Question/Problem] I&C systems important to safety must comply to all of these requirements. The global knowledge of this domain is scattered through these different documents and not formalized. Its organization and traceability relationships within this domain is mainly implicit. As a consequence, such long lasting nuclear I&C projects set important challenges in terms of tacit expertise capitalization and domain analysis. [Principal ideas/results] To tackle this domain formalization issue, we propose a dual Model-driven Engineering (MDE) and Information Retrieval (IR) approach to address the\u00a0\u2026", "num_citations": "17\n", "authors": ["320"]}
{"title": "Formally defining and iterating infinite models\n", "abstract": " The wide adoption of MDE raises new situations where we need to manipulate very large models or even infinite model streams gathered at runtime (e.g., monitoring). These new uses cases for MDE raise challenges that had been unforeseen by the time standard modeling framework were designed. This paper proposes a formal definition of an infinite model, as well as a formal framework to reason on queries over infinite models. This formal query definition aims at supporting the design and verification of operations that manipulate infinite models. First, we precisely identify the MOF parts which must be refined to support infinite structure. Then, we provide a formal coinductive definition dealing with unbounded and potentially infinite graph-based structure.", "num_citations": "17\n", "authors": ["320"]}
{"title": "Formalizing standards and regulations variability in longlife projects. A challenge for Model-driven engineering\n", "abstract": " Safety regulations and standards imposed by national regulators on nuclear power plant systems provide high-level requirements, recommendations and/or guidance expressed in natural language. In many cases, this leaves a large margin for interpretation, not all of which are acceptable to a given regulator. Currently the elements that lead to the establishment of acceptable/accepted practices are not always documented, nor are these practices formally modeled. When a new standard appears or when Electricit\u00e9 de France (EDF) has to discuss a standard with another regulator, there is no systematic process to build a practice. Domain-specific modeling, traceability and variability modeling are Model-Driven Engineering (MDE) techniques that could address various aspects of practice formalization. This paper precisely defines the modeling issues that are currently faced by EDF when managing regulatory safety\u00a0\u2026", "num_citations": "17\n", "authors": ["320"]}
{"title": "The emergence of software diversity in maven central\n", "abstract": " Maven artifacts are immutable: an artifact that is uploaded on Maven Central cannot be removed nor modified. The only way for developers to upgrade their library is to release a new version. Consequently, Maven Central accumulates all the versions of all the libraries that are published there, and applications that declare a dependency towards a library can pick any version. In this work, we hypothesize that the immutability of Maven artifacts and the ability to choose any version naturally support the emergence of software diversity within Maven Central. We analyze 1,487,956 artifacts that represent all the versions of 73,653 libraries. We observe that more than 30% of libraries have multiple versions that are actively used by latest artifacts. In the case of popular libraries, more than 50% of their versions are used. We also observe that more than 17% of libraries have several versions that are significantly more used\u00a0\u2026", "num_citations": "16\n", "authors": ["320"]}
{"title": "Correctness attraction: a study of stability of software behavior under runtime perturbation\n", "abstract": " Can the execution of software be perturbed without breaking the correctness of the output? In this paper, we devise a protocol to answer this question from a novel perspective. In an experimental study, we observe that many perturbations do not break the correctness in ten subject programs. We call this phenomenon \u201ccorrectness attraction\u201d. The uniqueness of this protocol is that it considers a systematic exploration of the perturbation space as well as perfect oracles to determine the correctness of the output. To this extent, our findings on the stability of software under execution perturbations have a level of validity that has never been reported before in the scarce related work. A qualitative manual analysis enables us to set up the first taxonomy ever of the reasons behind correctness attraction.", "num_citations": "16\n", "authors": ["320"]}
{"title": "Automatic microbenchmark generation to prevent dead code elimination and constant folding\n", "abstract": " Microbenchmarking evaluates, in isolation, the execution time of small code segments that play a critical role in large applications. The accuracy of a microbenchmark depends on two critical tasks: wrap the code segment into a pay-load that faithfully recreates the execution conditions of the large application; build a scaffold that runs the payload a large number of times to get a statistical estimate of the execution time. While recent frameworks such as the Java Microbenchmark Harness (JMH) address the scaffold challenge, developers have very limited support to build a correct payload. This work focuses on the automatic generation of pay-loads, starting from a code segment selected in a large application. Our generative technique prevents two of the most common mistakes made in microbenchmarks: dead code elimination and constant folding. A microbenchmark is such a small program that can be \u201cover\u00a0\u2026", "num_citations": "16\n", "authors": ["320"]}
{"title": "Ten years of Meta-Object Facility: an analysis of metamodeling practices\n", "abstract": " The definition of a metamodel that precisely captures domain knowledge for effective know-how capitalization is a challenging task. A major obstacle for domain experts who want to build a metamodel is that they must master two radically different languages: an object-oriented, MOF-compliant, modeling language to capture the domain structure and first order logic (the Object Constraint Language) for the definition of well-formedness rules. However, there are no guidelines or tools to assist the conjunct usage of both paradigms. Consequently, we observe that most metamodels have only an object-oriented domain structure, leading to inaccurate metamodels. In this paper, we perform the first empirical study, which analyzes the current state of practice in metamodels that actually use logical expressions to constrain the structure. We analyze 33 metamodels including 1262 rules coming from industry, academia and the Object Management Group, to understand how metamodelers articulate both languages. We implement a set of metrics in the OCLMetrics tool to evaluate the complexity of both parts, as well as the coupling between both. We observe that all metamodels tend to have a small, core subset of concepts, which are constrained by most of the rules, in general the rules are loosely coupled to the structure and we identify the set of OCL constructs actually used in rules.", "num_citations": "16\n", "authors": ["320"]}
{"title": "A comprehensive study of bloated dependencies in the maven ecosystem\n", "abstract": " Build automation tools and package managers have a profound influence on software development. They facilitate the reuse of third-party libraries, support a clear separation between the application\u2019s code and its external dependencies, and automate several software development tasks. However, the wide adoption of these tools introduces new challenges related to dependency management. In this paper, we propose an original study of one such challenge: the emergence of bloated dependencies. Bloated dependencies are libraries that are packaged with the application\u2019s compiled code but that are actually not necessary to build and run the application. They artificially grow the size of the built binary and increase maintenance effort. We propose DepClean, a tool to determine the presence of bloated dependencies in Maven artifacts. We analyze 9,639 Java artifacts hosted on Maven Central, which\u00a0\u2026", "num_citations": "15\n", "authors": ["320"]}
{"title": "A novelty search approach for automatic test data generation\n", "abstract": " In search-based structural testing, metaheuristic search techniques have been frequently used to automate the test data generation. In Genetic Algorithms (GAs) for example, test data are rewarded on the basis of an objective function that represents generally the number of statements or branches covered. However, owing to the wide diversity of possible test data values, it is hard to find the set of test data that can satisfy a specific coverage criterion. In this paper, we introduce the use of Novelty Search (NS) algorithm to the test data generation problem based on statement-covered criteria. We believe that such approach to test data generation is attractive because it allows the exploration of the huge space of test data within the input domain. In this approach, we seek to explore the search space without regard to any objectives. In fact, instead of having a fitness-based selection, we select test cases based on a\u00a0\u2026", "num_citations": "15\n", "authors": ["320"]}
{"title": "Morellian analysis for browsers: Making web authentication stronger with canvas fingerprinting\n", "abstract": " In this paper, we present the first fingerprinting-based authentication scheme that is not vulnerable to trivial replay attacks. Our proposed canvas-based fingerprinting technique utilizes one key characteristic: it is parameterized by a challenge, generated on the server side. We perform an in-depth analysis of all parameters that can be used to generate canvas challenges, and we show that it is possible to generate unique, unpredictable, and highly diverse canvas-generated images each time a user logs onto a service. With the analysis of images collected from more than 1.1 million devices in a real-world large-scale experiment, we evaluate our proposed scheme against a large set of attack scenarios and conclude that canvas fingerprinting is a suitable mechanism for stronger authentication on the web.", "num_citations": "14\n", "authors": ["320"]}
{"title": "Reverse-engineering reusable language modules from legacy domain-specific languages\n", "abstract": " The use of domain-specific languages (DSLs) has become a successful technique in the development of complex systems. Nevertheless, the construction of this type of languages is time-consuming and requires highly-specialized knowledge and skills. An emerging practice to facilitate this task is to enable reuse through the definition of language modules which can be later put together to build up new DSLs. Still, the identification and definition of language modules are complex and error-prone activities, thus hindering the reuse exploitation when developing DSLs. In this paper, we propose a computer-aided approach to (i) identify potential reuse in a set of legacy DSLs; and (ii) capitalize such potential reuse by extracting a set of reusable language modules with well defined interfaces that facilitate their assembly. We validate our approach by using realistic DSLs coming out from industrial case studies\u00a0\u2026", "num_citations": "14\n", "authors": ["320"]}
{"title": "Embedding adaptivity in software systems using the ECSELR framework\n", "abstract": " ECSELR is an ecologically-inspired approach to software evolution that enables environmentally driven evolution at runtime in extant software systems without relying on any offline components or management. ECSELR embeds adaptation and evolution inside the target software system enabling the system to transform itself via darwinian evolutionary mechanisms and adapt in a self contained manner. This allows the software system to benefit autonomously from the useful emergent byproducts of evolution like adaptivity and bio-diversity, avoiding the problems involved in engineering and maintaining such properties. ECSELR enables software systems to address changing environments at runtime, ensuring benefits like mitigation of attacks and memory-optimization among others while avoiding time consuming and costly maintenance and downtime. ECSELR differs from existing work in that, 1) adaptation is\u00a0\u2026", "num_citations": "14\n", "authors": ["320"]}
{"title": "Toward multilevel textual requirements traceability using model-driven engineering and information retrieval\n", "abstract": " In complex industrial projects, textual information remains the main vector of information at the project level. Consequently, requirements are scattered throughout multiple documents expressing different levels of requirements and different kinds of requirements. Formalizing this information and tracing different relationships among documents and organizing this environment present a challenging question. Domain-specific modeling and traceability modeling are Model-Driven Engineering (MDE) techniques that could address various aspects of requirements formalization. Text-based high level requirements can be formalized as document concepts can be gathered and represented. Still, relationships cannot always be determined using sole MDE approaches and, as a consequence, relationships and traceability issue remains. Information retrieval (IR) approaches have already proved to work in an efficient way\u00a0\u2026", "num_citations": "14\n", "authors": ["320"]}
{"title": "A journey among Java neutral program variants\n", "abstract": " Neutral program variants are alternative implementations of a program, yet equivalent with respect to the test suite. Techniques such as approximate computing or genetic improvement share the intuition that potential for enhancements lies in these acceptable behavioral differences (e.g., enhanced performance or reliability). Yet, the automatic synthesis of neutral program variants, through program transformations remains a key challenge. This work aims at characterizing plastic code regions in Java programs, i.e., the code regions that are modifiable while maintaining functional correctness, according to a test suite. Our empirical study relies on automatic variations of 6 real-world Java programs. First, we transform these programs with three state-of-the-art program transformations: add, replace and delete statements. We get a pool of 23,445 neutral variants, from which we gather the following novel insights\u00a0\u2026", "num_citations": "13\n", "authors": ["320"]}
{"title": "Descartes: a PITest engine to detect pseudo-tested methods: tool demonstration\n", "abstract": " Descartes is a tool that implements extreme mutation operators and aims at finding pseudo-tested methods in Java projects. It leverages the efficient transformation and runtime features of PITest. The demonstration compares Descartes with Gregor, the default mutation engine provided by PITest, in a set of real open source projects. It considers the execution time, number of mutants created and the relationship between the mutation scores produced by both engines. It provides some insights on the main features exposed by Descartes.", "num_citations": "13\n", "authors": ["320"]}
{"title": "Automatic detection of GUI design smells: The case of blob listener\n", "abstract": " Graphical User Interfaces (GUIs) intensively rely on event-driven programming: widgets send GUI events, which capture users' interactions, to dedicated objects called controllers. Controllers implement several GUI listeners that handle these events to produce GUI commands. In this work, we conducted an empirical study on 13 large Java Swing open-source software systems. We study to what extent the number of GUI commands that a GUI listener can produce has an impact on the change-and fault-proneness of the GUI listener code. We identify a new type of design smell, called Blob listener that characterizes GUI listeners that can produce more than two GUI commands. We show that 21% of the analyzed GUI controllers are Blob listeners. We propose a systematic static code analysis procedure that searches for Blob listener that we implement in InspectorGuidget. We conducted experiments on six software\u00a0\u2026", "num_citations": "13\n", "authors": ["320"]}
{"title": "Modeling variability in the video domain: Language and experience report\n", "abstract": " This paper reports about a new domain-specific variability modeling language, called VM, resulting from the close collaboration with industrial partners in the video domain. We expose the requirements and advanced variability constructs required to characterize and realize variations of physical properties of a video (such as objects' speed or scene illumination). The results of our experiments and industrial experience show that VM is effective to model complex variability information and can be exploited to synthesize video variants. We concluded that basic variability mechanisms are useful but not enough, attributes and multi-features are of prior importance, and meta-information is relevant for efficient variability analysis. In addition, we questioned the existence of one-size-fits-all variability modeling solution applicable in any industry. Yet, some common needs for modeling variability are becoming apparent such as support for attributes and multi-features.", "num_citations": "13\n", "authors": ["320"]}
{"title": "An approach and benchmark to detect behavioral changes of commits in continuous integration\n", "abstract": " When a developer pushes a change to an application\u2019s codebase, a good practice is to have a test case specifying this behavioral change. Thanks to continuous integration (CI), the test is run on subsequent commits to check that they do no introduce a regression for that behavior. In this paper, we propose an approach that detects behavioral changes in commits. As input, it takes a program, its test suite, and a commit. Its output is a set of test methods that capture the behavioral difference between the pre-commit and post-commit versions of the program. We call our approach DCI (Detecting behavioral changes in CI). It works by generating variations of the existing test cases through (i) assertion amplification and (ii) a search-based exploration of the input space. We evaluate our approach on a curated set of 60 commits from 6 open source Java projects. To our knowledge, this is the first ever curated dataset of real\u00a0\u2026", "num_citations": "12\n", "authors": ["320"]}
{"title": "Reverse engineering language product lines from existing DSL variants\n", "abstract": " The use of domain-specific languages (DSLs) has become a successful technique to develop complex systems. In this context, an emerging phenomenon is the existence of DSL variants, which are different versions of a DSL adapted to specific purposes but that still share commonalities. In such a case, the challenge for language designers is to reuse, as much as possible, previously defined language constructs to narrow implementation from scratch. To overcome this challenge, recent research in software languages engineering introduced the notion of language product lines. Similarly to software product lines, language product lines are often built from a set of existing DSL variants.In this article, we propose a reverse-engineering technique to ease-off such a development scenario. Our approach receives a set of DSL variants which are used to automatically recover a language modular design and to\u00a0\u2026", "num_citations": "12\n", "authors": ["320"]}
{"title": "A categorical model of model merging and weaving\n", "abstract": " Model driven engineering advocates the separation of concerns during the design time of a system, which leads to the creation of several different models, using several different syntaxes. However, to reason on the overall system, we need to compose these models. Unfortunately, composition of models is done in an ad hoc way, preventing comparison, capitalisation and reuse of the composition operators. In order to improve comprehension and allow comparison of merging and weaving operators, we use category theory to propose a unified framework to formally define merging and weaving of models. We successfully use this framework to compare them, both through the way they are transformed in the formalism, and through several properties, such as completeness or non-redundancy. Finally, we validate this framework by checking that it correctly identifies three tools as performing merging or weaving of\u00a0\u2026", "num_citations": "12\n", "authors": ["320"]}
{"title": "Towards ecology inspired software engineering\n", "abstract": " Ecosystems are complex and dynamic systems. Over billions of years, they have developed advanced capabilities to provide stable functions, despite changes in their environment. In this paper, we argue that the laws of organization and development of ecosystems provide a solid and rich source of inspiration to lay the foundations for novel software construction paradigms that provide stability as much as openness.", "num_citations": "12\n", "authors": ["320"]}
{"title": "Empirical evaluation of the conjunct use of MOF and OCL\n", "abstract": " MOF and OCL are commonly used for metamodeling: MOF to model the domain structure, and OCL for the well-formedness rules. Thus, metamodelers have to master both formalisms and understand how to articulate them in order to build metamodels that accurately capture domain knowledge. A systematic empirical analysis of the conjunct use of MOF and OCL in existing metamodels could help metamodelers un- derstand how to use these formalisms. However, existing metamodels usually present anomalies that prevent automatic analysis without prior fixing. In particular, it often happens that both parts of the metamodel (MOF and OCL) are inconsistent. In this paper, we propose a process for analyzing metamodels and we report on the pre-processing phase we went through on 52 metamodels in order to get them ready for automatic empirical analysis.", "num_citations": "12\n", "authors": ["320"]}
{"title": "Testing model transformations: A case for test generation from input domain models\n", "abstract": " Model transformations can automate critical tasks in model-driven development. Thorough validation techniques are required to ensure their correctness. In this lecture we focus on testing model transformations. In particular, we present an approach for systematic selection of input test data. This approach is based on a key characteristic of model transformations: their input domain is formally captured in a metamodel. A major challenge for test generation is that metamodels usually model an infinite set of possible input models for the transformation. We start with a general motivation of the need for specific test selection techniques in the presence of very large and possibly infinite input domains. We also present two existing black-box strategies to systematically select test data: category-partition and combinatorial interaction testing. Then, we detail specific criteria based on metamodel coverage to select data for model transformation testing. We introduce object and model fragments to capture specific structural constraints that should be satisfied by input test data. These fragments are the basis for the definition of coverage criteria and for automatic generation of test data. They also serve to drive the automatic generation of models for testing.", "num_citations": "12\n", "authors": ["320"]}
{"title": "Breathing ontological knowledge into feature model management\n", "abstract": " Feature Models (FMs) are a popular formalism for modeling and reasoning about the configurations of a software product line. As the manual construction or management of an FM is time-consuming and error-prone for large software projects, recent works have focused on automated operations for reverse engineering or refactoring FMs from a set of configurations/dependencies. Without prior knowledge, meaningless ontological relations (as defined by the feature hierarchy and groups) are likely to be synthesized and cause severe difficulties when reading, maintaining or exploiting the resulting FM. In this paper we define a generic, ontological-aware synthesis procedure that guides users when identifying the likely siblings or parent candidates for a given feature. We develop and evaluate a series of heuristics for clustering/weighting the logical, syntactic and semantic relationships between features. Empirical experiments on hundreds of FMs, coming from the SPLOT repository and Wikipedia, show that an hybrid approach mixing logical and ontological techniques outperforms state-of-the-art solutions and offers the best support for reducing the number of features a user has to consider during the interactive selection of a hierarchy.", "num_citations": "11\n", "authors": ["320"]}
{"title": "A classification of invasive patterns in AOP\n", "abstract": " Aspect-Oriented Programming (AOP) improves modularity by encapsulating crosscutting concerns into aspects. Some mechanisms to compose aspects allow invasiveness as a mean to integrate concerns. Invasiveness means that AOP languages have unrestricted access to program properties. Such kind of languages are interesting because they allow performing complex operations and better introduce functionalities. In this report we present a classification of invasive patterns in AOP. This classification characterizes the aspects invasive behavior and allows developers to abstract about the aspect incidence over the program they crosscut.", "num_citations": "11\n", "authors": ["320"]}
{"title": "Vigilant usage of aspects\n", "abstract": " In the last 10 years the Aspect-Oriented Software Development (AOSD) has gradually become a concern stone in Software Engineering as an engine to reduce complexity and increase reuse by providing modularization of concerns that tend to crosscut. Nevertheless, its use in certain situations can presents some problems that can not only discourage its mainstream adoption, but also hinder the realization of software quality goals. The first problem, the AOSD-Evolution paradox, encompasses the difficulties with evolving software developed using AOSD. The second arises as a result of the invasive nature of aspects. The use of aspects without any control can result in a harmful practice. This work describes these problems and exposes the strength and limitations of the current approaches to solve them. Thus allowing us to reason in a clear fashion about the problems and their solutions, then justifying a contract base approach, which aims to control the usage of aspect without constraining the power of AOSD.", "num_citations": "11\n", "authors": ["320"]}
{"title": "The emerging field of test amplification: A survey\n", "abstract": " Context: The increasing adoption of test-driven development results in software projects with strong test suites. These suites include a large number of test cases, in which developers embed knowledge about meaningful input data and expected properties in the form of oracles.Objective: This article surveys various works that aim at exploiting this knowledge in order to enhance these manually written tests with respect to an engineering goal (eg, improve coverage of changes or increase the accuracy of fault localization). While these works rely on various techniques and address various goals, we believe they form an emerging and coherent field of research, and which we call \u201ctest amplification\u201d.Method: We devised a first set of papers based on our knowledge of the literature (we have been working in software testing for years). Then, we systematically followed the citation graph. Results: This survey is the first that draws a comprehensive picture of the different engineering goals proposed in the literature for test amplification. In particular, we note that the goal of test amplification goes far beyond maximizing coverage only.Conclusion: We believe that this survey will help researchers and practitioners entering this new field to understand more quickly and more deeply the intuitions, concepts and techniques used for test amplification.", "num_citations": "10\n", "authors": ["320"]}
{"title": "Notice: A framework for non-functional testing of compilers\n", "abstract": " Generally, compiler users apply different optimizations to generate efficient code with respect to non-functional properties such as energy consumption, execution time, etc. However, due to the huge number of optimizations provided by modern compilers, finding the best optimization sequence for a specific objective and a given program is more and more challenging. This paper proposes NOTICE, a component-based framework for non-functional testing of compilers through the monitoring of generated code in a controlled sand-boxing environment. We evaluate the effectiveness of our approach by verifying the optimizations performed by the GCC compiler. Our experimental results show that our approach is able to auto-tune compilers according to user requirements and construct optimizations that yield to better performance results than standard optimization levels. We also demonstrate that NOTICE can be used\u00a0\u2026", "num_citations": "10\n", "authors": ["320"]}
{"title": "Automatic software diversity in the light of test suites\n", "abstract": " A few works address the challenge of automating software diversification, and they all share one core idea: using automated test suites to drive diversification. However, there is is lack of solid understanding of how test suites, programs and transformations interact one with another in this process. We explore this intricate interplay in the context of a specific diversification technique called \"sosiefication\". Sosiefication generates sosie programs, i.e., variants of a program in which some statements are deleted, added or replaced but still pass the test suite of the original program. Our investigation of the influence of test suites on sosiefication exploits the following observation: test suites cover the different regions of programs in very unequal ways. Hence, we hypothesize that sosie synthesis has different performances on a statement that is covered by one hundred test case and on a statement that is covered by a single test case. We synthesize 24583 sosies on 6 popular open-source Java programs. Our results show that there are two dimensions for diversification. The first one lies in the specification: the more test cases cover a statement, the more difficult it is to synthesize sosies. Yet, to our surprise, we are also able to synthesize sosies on highly tested statements (up to 600 test cases), which indicates an intrinsic property of the programs we study. The second dimension is in the code: we manually explore dozens of sosies and characterize new types of forgiving code regions that are prone to diversification.", "num_citations": "10\n", "authors": ["320"]}
{"title": "Scalable armies of model clones through data sharing\n", "abstract": " Cloning a model is usually done by duplicating all its runtime objects into a new model. This approach leads to memory consumption problems for operations that create and manipulate large quantities of clones (e.g., design space exploration). We propose an original approach that exploits the fact that operations rarely modify a whole model. Given a set of immutable properties, our cloning approach determines the objects and fields that can be shared between the runtime representations of a model and its clones. Our generic cloning algorithm is parameterized with three strategies that establish a trade-off between memory savings and the ease of clone manipulation. We implemented the strategies within the Eclipse Modeling Framework (EMF) and evaluated memory footprints and computation overheads with 100 randomly generated metamodels and models. Results show a positive correlation\u00a0\u2026", "num_citations": "10\n", "authors": ["320"]}
{"title": "Towards trust-aware and self-adaptive systems\n", "abstract": " The Future Internet (FI) comprises scenarios where many heterogeneous and dynamic entities must interact to provide services (e.g., sensors, mobile devices and information systems in smart city scenarios). The dynamic conditions under which FI applications must execute call for self-adaptive software to cope with unforeseeable changes in the application environment. Software engineering currently provides frameworks to develop reasoning engines that automatically take reconfiguration decisions and that support the runtime adaptation of distributed, heterogeneous applications. However, these frameworks have very limited support to address security concerns of these application, hindering their usage for FI scenarios. We address this challenge by enhancing self-adaptive systems with the concepts of trust and reputation. Trust will improve decision-making processes under risk and uncertainty, in\u00a0\u2026", "num_citations": "9\n", "authors": ["320"]}
{"title": "Observability and chaos engineering on system calls for containerized applications in docker\n", "abstract": " In this paper, we present a novel fault injection system called ChaosOrca for system calls in containerized applications. ChaosOrca aims at evaluating a given application\u2019s self-protection capability with respect to system call errors. The unique feature of ChaosOrca is that it conducts experiments under production-like workload without instrumenting the application. We exhaustively analyze all kinds of system calls and utilize different levels of monitoring techniques to reason about the behavior under perturbation. We evaluate ChaosOrca on three real-world applications: a file transfer client, a reverse proxy server and a micro-service oriented web application. Our results show that it is promising to detect weaknesses of resilience mechanisms related to system calls issues.", "num_citations": "8\n", "authors": ["320"]}
{"title": "Online genetic improvement on the java virtual machine with ECSELR\n", "abstract": " Online Genetic Improvement embeds the ability to evolve and adapt inside a target software system enabling it to improve at runtime without any external dependencies or human intervention. We recently developed a general purpose tool enabling Online Genetic Improvement in software systems running on the java virtual machine. This tool, dubbed ECSELR, is embedded inside extant software systems at runtime, enabling such systems to self-improve and adapt autonomously online. We present this tool, describing its architecture and focusing on its design choices and possible uses.", "num_citations": "8\n", "authors": ["320"]}
{"title": "DSpot: Test amplification for automatic assessment of computational diversity\n", "abstract": " Context: Computational diversity, i.e., the presence of a set of programs that all perform compatible services but that exhibit behavioral differences under certain conditions, is essential for fault tolerance and security. Objective: We aim at proposing an approach for automatically assessing the presence of computational diversity. In this work, computationally diverse variants are defined as (i) sharing the same API, (ii) behaving the same according to an input-output based specification (a test-suite) and (iii) exhibiting observable differences when they run outside the specified input space. Method: Our technique relies on test amplification. We propose source code transformations on test cases to explore the input domain and systematically sense the observation domain. We quantify computational diversity as the dissimilarity between observations on inputs that are outside the specified domain. Results: We run our experiments on 472 variants of 7 classes from open-source, large and thoroughly tested Java classes. Our test amplification multiplies by ten the number of input points in the test suite and is effective at detecting software diversity. Conclusion: The key insights of this study are: the systematic exploration of the observable output space of a class provides new insights about its degree of encapsulation; the behavioral diversity that we observe originates from areas of the code that are characterized by their flexibility (caching, checking, formatting, etc.).", "num_citations": "8\n", "authors": ["320"]}
{"title": "Managing execution environment variability during software testing: An industrial experience\n", "abstract": " Nowadays, telecom software applications are expected to run on a tremendous variety of execution environments. For example, network connection software must deliver the same functionalities on distinct physical platforms, which themselves run several distinct operating systems, with various applications and physical devices. Testing those applications is challenging as it is simply impossible to consider every possible environment configuration. This paper reports on an industrial case study called BIEW (Business and Internet EveryWhere) where the combinatorial explosion of environment configurations has been tackled with a dedicated and original methodology devised by KEREVAL, a french SME focusing on software testing services. The proposed solution samples a subset of configurations to be tested, based on environment modelling, requirement analysis and systematic traceability. From the\u00a0\u2026", "num_citations": "8\n", "authors": ["320"]}
{"title": "Defining and retrieving themes in nuclear regulations\n", "abstract": " Safety systems in nuclear industry must conform to an increasing set of regulatory requirements. These requirements are scattered throughout multiple documents expressing different levels of requirements or different kinds of requirements. Consequently, when licensees want to extract the set of regulations related to a specific concern, they lack explicit traces between all regulation documents and mostly get lost while attempting to compare two different regulatory corpora. This paper presents the regulatory landscape in the context of digital Instrumentation and Command systems in nuclear power plants. To cope with this complexity, we define and discuss challenges toward an approach based on information retrieval techniques to first narrow the regulatory research space into themes and then assist the recovery of these traceability links.", "num_citations": "8\n", "authors": ["320"]}
{"title": "Minimum pairwise coverage using constraint programming techniques\n", "abstract": " This paper presented the global constraint pairwise that can be used to enforce the presence of a given pair within a set of test cases or configurations. It also introduced several optimizations for implementing a method that computes the minimum set of test cases that covers pairwise. In addition, the method, seen as a constraint optimization problem, provides a way to compromise between time and efficiency by allowing anytime interruption or time-contract execution. Our approach has been implemented and evaluated on several instances of a test configurations generation problems [5] where input variables are boolean only. We envision to address other instances of these problem where the variables take their values in larger finite domains.", "num_citations": "8\n", "authors": ["320"]}
{"title": "A collaborative strategy for mitigating tracking through browser fingerprinting\n", "abstract": " Browser fingerprinting is a technique that collects information about the browser configuration and the environment in which it is running. This information is so diverse that it can partially or totally identify users online. Over time, several countermeasures have emerged to mitigate tracking through browser fingerprinting. However, these measures do not offer full coverage in terms of privacy protection, as some of them may introduce inconsistencies or unusual behaviors, making these users stand out from the rest.", "num_citations": "7\n", "authors": ["320"]}
{"title": "User interface design smell: Automatic detection and refactoring of Blob listeners\n", "abstract": " Context. User Interfaces (UIs) intensively rely on event-driven programming: interactive objects send UI events, which capture users\u2019 interactions, to dedicated objects called controllers. Controllers use several UI listeners that handle these events to produce UI commands.Objective. First, we reveal the presence of design smells in the code that describes and controls UIs. Second, we demonstrate that specific code analyses are necessary to analyze and refactor UI code, because of its coupling with the rest of the code.Method. We conducted an empirical study on four large Java software systems. We studied to what extent the number of UI commands that a UI listener can produce has an impact on the change- and fault-proneness of the UI listener code. We developed a static code analysis for detecting UI commands in the code.Results. We identified a new type of design smell, called Blob listener, that\u00a0\u2026", "num_citations": "7\n", "authors": ["320"]}
{"title": "Towards scalable multidimensional execution traces for xDSMLs\n", "abstract": " Executable Domain Specific Modeling Languages (xDSML) opens many possibilities in terms of early verification and validation (V&V) of systems, including the use of dynamic V&V approaches. Such approaches rely on the notion of execution trace, i.e. the evolution of a system during a run. To benefit from dynamic V&V approaches, it is therefore necessary to characterize what is the structure of the executions traces of a given xDSML. Our goal is to provide an approach to design trace metamodels for xDSMLs. We identify seven problems that must be considered when modeling execution traces, including concurrency, modularity, and scalability. Then we present our envisioned approach to design scalable multidimensional trace metamodels for xDSMLs. Our work in progress relies on the dimensions of a trace (i.e. subsets of mu- table elements of the traced model) to provide an original structure that faces the identified problems, along with a trace API to manipulate them.", "num_citations": "7\n", "authors": ["320"]}
{"title": "Leveraging variability modeling for multi-dimensional model-driven software product lines\n", "abstract": " In order to be adopted in industrial cases, the Software Product Line paradigm must be adapted to the specific organizational context and culture. In this paper, we consider a scenario of a multinational company that would benefit from SPL. This company uses a model-based software and system development process, which allows them to build reliable and consistent systems for the defence, security, aerospace and transportation domain. Initial efforts to adopt SPL in their software production proved successful. However, they still need to leverage variability modeling to the software and system level, integrating it to their existing model-based development. Therefore, this work aims at (i) presenting an industrial scenario and identifying the main challenges to leverage variability modeling for it, (ii) outlining our point of view and perspectives on how these challenges can be addressed, and (iii) discussing the\u00a0\u2026", "num_citations": "7\n", "authors": ["320"]}
{"title": "Interpretation of swedish sign language using convolutional neural networks and transfer learning\n", "abstract": " The automatic interpretation of sign languages is a challenging task, as it requires the usage of high-level vision and high-level motion processing systems for providing accurate image perception. In this paper, we use Convolutional Neural Networks (CNNs) and transfer learning to make computers able to interpret signs of the Swedish Sign Language (SSL) hand alphabet. Our model consists of the implementation of a pre-trained InceptionV3 network, and the usage of the mini-batch gradient descent optimization algorithm. We rely on transfer learning during the pre-training of the model and its data. The final accuracy of the model, based on 8 study subjects and 9400 images, is 85%. Our results indicate that the usage of CNNs is a promising approach to interpret sign languages, and transfer learning can be used to achieve high testing accuracy despite using a small training dataset. Furthermore, we\u00a0\u2026", "num_citations": "6\n", "authors": ["320"]}
{"title": "Java decompiler diversity and its application to meta-decompilation\n", "abstract": " During compilation from Java source code to bytecode, some information is irreversibly lost. In other words, compilation and decompilation of Java code is not symmetric. Consequently, decompilation, which aims at producing source code from bytecode, relies on strategies to reconstruct the information that has been lost. Different Java decompilers use distinct strategies to achieve proper decompilation. In this work, we hypothesize that the diverse ways in which bytecode can be decompiled has a direct impact on the quality of the source code produced by decompilers.In this paper, we assess the strategies of eight Java decompilers with respect to three quality indicators: syntactic correctness, syntactic distortion and semantic equivalence modulo inputs. Our results show that no single modern decompiler is able to correctly handle the variety of bytecode structures coming from real-world programs. The highest\u00a0\u2026", "num_citations": "6\n", "authors": ["320"]}
{"title": "Automatic non-functional testing of code generators families\n", "abstract": " The intensive use of generative programming techniques provides an elegant engineering solution to deal with the heterogeneity of platforms and technological stacks. The use of domain-specific languages for example, leads to the creation of numerous code generators that automatically translate highlevel system specifications into multi-target executable code. Producing correct and efficient code generator is complex and error-prone. Although software designers provide generally high-level test suites to verify the functional outcome of generated code, it remains challenging and tedious to verify the behavior of produced code in terms of non-functional properties. This paper describes a practical approach based on a runtime monitoring infrastructure to automatically check the potential inefficient code generators. This infrastructure, based on system containers as execution platforms, allows code-generator\u00a0\u2026", "num_citations": "6\n", "authors": ["320"]}
{"title": "A novelty search-based test data generator for object-oriented programs\n", "abstract": " In search-based structural testing, meta-heuristic search techniques have been frequently used to automate test data generation. In this paper, we introduce the use of novelty search algorithm to the test data generation problem based on statement-covered criterion. In this approach, we seek to explore the search space by considering diversity as the unique objective function to be optimized. In fact, instead of having a fitness-based selection, we select test cases based on a novelty score showing how different they are compared to all other solutions evaluated so far.", "num_citations": "6\n", "authors": ["320"]}
{"title": "Toward a model-driven access-control enforcement mechanism for pervasive systems\n", "abstract": " Pervasive systems typically involve heterogeneous users, devices and networks to provide services seamlessly interacting with the physical world. In order to be flexible, these systems must be both dynamically adaptive to handle and still open to the ability of receiving new elements. Characteristics of these systems can have a major impact on the enforcement of role-based access control policies. Enforcement mechanism for RBAC policies need to be tailored to distributed and adaptive software architectures. It must be capable of handling architectural changes (eg, a resource hosted by a node is moved to another node) in order to maintain the enforced policy. In this paper we describe an approach of policy enforcement that leverages on a mapping between RBAC and a component-based architecture to reason on architectural changes and maintain the enforced policy.[email protected] paradigm provides\u00a0\u2026", "num_citations": "6\n", "authors": ["320"]}
{"title": "Leveraging metamorphic testing to automatically detect inconsistencies in code generator families\n", "abstract": " Generative software development has paved the way for the creation of multiple code generators that serve as a basis for automatically generating code to different software and hardware platforms. In this context, the software quality becomes highly correlated to the quality of code generators used during software development. Eventual failures may result in a loss of confidence for the developers, who will unlikely continue to use these generators. It is then crucial to verify the correct behaviour of code generators in order to preserve software quality and reliability. In this paper, we leverage the metamorphic testing approach to automatically detect inconsistencies in code generators via so\u2010called \u201cmetamorphic relations\u201d. We define the metamorphic relation (i.e., test oracle) as a comparison between the variations of performance and resource usage of test suites running on different versions of generated code. We\u00a0\u2026", "num_citations": "5\n", "authors": ["320"]}
{"title": "The strengths and behavioral quirks of Java bytecode decompilers\n", "abstract": " During compilation from Java source code to bytecode, some information is irreversibly lost. In other words, compilation and decompilation of Java code is not symmetric. Consequently, the decompilation process, which aims at producing source code from bytecode, must establish some strategies to reconstruct the information that has been lost. Modern Java decompilers tend to use distinct strategies to achieve proper decompilation. In this work, we hypothesize that the diverse ways in which bytecode can be decompiled has a direct impact on the quality of the source code produced by decompilers. We study the effectiveness of eight Java decompilers with respect to three quality indicators: syntactic correctness, syntactic distortion and semantic equivalence modulo inputs. This study relies on a benchmark set of 14 real-world open-source software projects to be decompiled (2041 classes in total). Our results show\u00a0\u2026", "num_citations": "5\n", "authors": ["320"]}
{"title": "Approximate loop unrolling\n", "abstract": " We introduce Approximate Unrolling, a compiler loop optimization that reduces execution time and energy consumption, exploiting code regions that can endure some approximation and still produce acceptable results. Specifically, this work focuses on counted loops that map a function over the elements of an array. Approximate Unrolling transforms loops similarly to Loop Unrolling. However, unlike its exact counterpart, our optimization does not unroll loops by adding exact copies of the loop's body. Instead, it adds code that interpolates the results of previous iterations.", "num_citations": "5\n", "authors": ["320"]}
{"title": "Detection and analysis of behavioral T-patterns in debugging activities\n", "abstract": " A growing body of research in empirical software engineering applies recurrent patterns analysis in order to make sense of the developers' behavior during their interactions with IDEs. However, the exploration of hidden real-time structures of programming behavior remains a challenging task. In this paper, we investigate the presence of temporal behavioral patterns (T-patterns) in debugging activities using the THEME software. Our preliminary exploratory results show that debugging activities are strongly correlated with code editing, file handling, window interactions and other general types of programming activities. The validation of our T-patterns detection approach demonstrates that debugging activities are performed on the basis of repetitive and well-organized behavioral events. Furthermore, we identify a large set of T-patterns that associate debugging activities with build success, which corroborates the\u00a0\u2026", "num_citations": "5\n", "authors": ["320"]}
{"title": "Exhaustive exploration of the failure-oblivious computing search space\n", "abstract": " High-availability of software systems requires automated handling of crashes in presence of errors. Failure-oblivious computing is one technique that aims to achieve high availability. We note that failure-obliviousness has not been studied in depth yet, and there is very few study that helps understand why failure-oblivious techniques work. In order to make failure-oblivious computing to have an impact in practice, we need to deeply understand failure-oblivious behaviors in software. In this paper, we study, design and perform an experiment that analyzes the size and the diversity of the failure-oblivious behaviors. Our experiment consists of exhaustively computing the search space of 16 field failures of large-scale open-source Java software. The outcome of this experiment is a much better understanding of what really happens when failure-oblivious computing is used, and this opens new promising research\u00a0\u2026", "num_citations": "5\n", "authors": ["320"]}
{"title": "Automating the maintenance of nonfunctional system properties using demonstration\u2010based model transformation\n", "abstract": " Domain\u2010Specific Modeling Languages (DSMLs) are playing an increasingly significant role in software development. By raising the level of abstraction using notations that are representative of a specific domain, DSMLs allow the core essence of a problem to be separated from irrelevant accidental complexities, which are typically found at the implementation level in source code. In addition to modeling the functional aspects of a system, a number of nonfunctional properties (e.g., quality of service constraints and timing requirements) also need to be integrated into models in order to reach a complete specification of a system. This is particularly true for domains that have distributed real time and embedded needs. Given a base model with functional components, maintaining the nonfunctional properties that crosscut the base model has become an essential modeling task when using DSMLs. The task of\u00a0\u2026", "num_citations": "5\n", "authors": ["320"]}
{"title": "Generating counterexamples of model-based software product lines: an exploratory study\n", "abstract": " Model-based Software Product Line (MSPL) engineering aims at deriving customized models corresponding to individual products of a family. MSPL approaches usually promote the joint use of a variability model, a base model expressed in a specific formalism, and a realization layer that maps variation points to model elements. The design space of an MSPL is extremely complex to manage for the engineer, since the number of variants may be exponential and the derived product models have to be conformant to numerous well-formedness and business rules. In this paper, the objective is to provide a way to generate MSPLs, called counterexamples, that can produce invalid product models despite a valid configuration in the variability model. We provide a systematic and automated process, based on the Common Variability Language (CVL), to randomly search the space of MSPLs for a specific formalism. We\u00a0\u2026", "num_citations": "5\n", "authors": ["320"]}
{"title": "Two flavors in automated software repair: Rigid repair and plastic repair\n", "abstract": " In this paper, we discuss two families of automated software repair approaches that we call \u201crigid repair\u201d and \u201cplastic repair\u201d. We shape the notions of rigid repair and plastic repair around the perception of software correctness. Rigid repair relies on a binary notion of \u201cbug\u201d and \u201crepair\u201d. Plastic repair refers to the plasticity of software, both in terms of correctness and in terms of intrinsic characteristics.", "num_citations": "5\n", "authors": ["320"]}
{"title": "Requirements-driven runtime reconfiguration for security\n", "abstract": " The boundary between development time and run time of eternal software intensive applications is fading. In particular, these systems are characterized by the necessity to evolve requirements continuously, even after the deployment. In this paper we focus on the evolution of security requirements and introduce an integrated process to drive runtime reconfiguration from requirements changes. This process relies on two key proposals: systematic strategies to evolve architecture models according to security requirements evolution and the combination of reflective middleware and models@runtime for runtime reconfiguration.", "num_citations": "5\n", "authors": ["320"]}
{"title": "Validation challenges in model composition: The case of adaptive systems\n", "abstract": " Model Driven Engineering helps dealing with complexity by promoting models as abstraction units. Aspect Oriented Modeling helps separating concerns that crosscut across different models. MDE and AOM have well identified challenges that need to be addressed. However, there are new challenges that appear when combining both techniques. In this paper we present the challenges that appear when validating the model composition in the context of MDE and AOM applied to adaptive systems.", "num_citations": "5\n", "authors": ["320"]}
{"title": "Assemblage testable et validation de composants\n", "abstract": " 5.7 Design Patterns pour la testabilit\u00e9 du mod\u00e8le ____________________129 5.7. 1 Concevoir par cristallisation de patterns ______________________129 5.7. 2 Application du design pattern State pour am\u00e9liorer la testabilit\u00e9 ___131", "num_citations": "5\n", "authors": ["320"]}
{"title": "Puzzle: A tool for analyzing and extracting specification clones in DSLs\n", "abstract": " The use of domain-specific languages (DSLs) is a successful technique in the development of complex systems. Indeed, the construction of new DSLs addressing the particular needs of software projects has become a recurrent activity. In this context, the phenomenon of specification cloning has started to appear. Language designers often copy&paste some parts of the specification from legacy DSLs to \u201creuse\u201d formerly defined language constructs. As well known, this type of practices introduce problems such as bugs propagation, thus increasing of maintenance costs. In this paper, we present Puzzle, a tool that uses static analysis to facilitate the detection of specification clones in DSLs implemented under the executable metamodeling paradigm. Puzzle also enables the extraction specification clones as reusable language modules that can be later used to build up new DSLs.", "num_citations": "4\n", "authors": ["320"]}
{"title": "The multiple facets of software diversity: A survey\n", "abstract": " Early experiments with software diversity in the mid 1970\u2019s investigated N-version programming and recovery blocks to increase the reliability of embedded systems. Four decades later, the literature about software diversity has expanded in multiple directions: goals (fault-tolerance, security, software engineering); means (managed or automated diversity) and analytical studies (quantification of diversity and its impact). This survey adopts an inclusive understanding of software diversity, which allows us to relate the work about N-version and randomization to the recent work about unsound program transformations and automatic test generation. Assembling the multiple facets of this fascinating topic sheds a new light on the field. By reviewing and gathering these different dimensions of software diversity across multiple scientific communities, we aim at contributing to the future of software diversity understanding and\u00a0\u2026", "num_citations": "4\n", "authors": ["320"]}
{"title": "Enhanced set of solutions supporting secure service architecture and design\n", "abstract": " The main objective of work package 7 is to provide improved support for architecting and designing secure services in Future Internet applications. This report presents advances about the prototypes for secure architecture and design, which all target specific modeling challenges for trust, access control and reputation in FI applications. This spectrum reflects the different concerns that impact the security of software architecture. It also reflects the challenges that arise in a context where applications have to run in constantly evolving environments.", "num_citations": "4\n", "authors": ["320"]}
{"title": "Multi-language support for model-driven requirement analysis and test generation\n", "abstract": " Expressing a valid requirements model is a crucial activity in a MDE context as it provides the starting point for further refinement steps that will lead to the implementation. In previous work we have proposed a requirements model from which it is possible to do simulation and high-level test generation. A requirements metamodel captures the key concepts to model the requirements and allows us to define several automatic model transformations to manipulate the requirements. In this paper, we focus on surface languages that can assist the edition of such a requirements model for simulation or testing purposes. We propose to use two different languages: a constrained natural language and UML activity diagrams. For each language, we discuss the main interesting features to express requirements and we define a model transformation to generate the corresponding requirements model.", "num_citations": "4\n", "authors": ["320"]}
{"title": "Engineering software diversity: A model-based approach to systematically diversify communications\n", "abstract": " Automated diversity is a promising mean of increasing the security of software systems. However, current automated diversity techniques operate at the bottom of the software stack (operating system and compiler), yielding a limited amount of diversity. We present a novel Model-Driven Engineering approach to the diversification of communicating systems, building on abstraction, model transformations and code generation. This approach generates significant amounts of diversity with a low overhead, and addresses a large number of communicating systems, including small communicating devices.", "num_citations": "3\n", "authors": ["320"]}
{"title": "Emergent robustness in software systems through decentralized adaptation: an ecologically-inspired alife approach\n", "abstract": " The ecosystem of web applications faces a critical paradox: on one hand, the Internet is a constantly evolving and unpredictable computing platform, on the other hand, the software services that run on top of it hardly have the ability to adapt to the evolution of this platform. Among the software services, we distinguish between service providers that provide micro services and service consumers that aggregate several micro services to deliver macro services to customers. Providers and consumers must handle uncertainty: providers cannot know in advance what consumers need; consumers rely on third-parties that can disappear at any time. Our proposal analogizes the software consumer/provider network to a bipartite ecological graph. This analogy provides the foundations for the design of EVOSERV, an individual-based ALife simulator used to experiment with decentralized adaptation strategies for providers and\u00a0\u2026", "num_citations": "3\n", "authors": ["320"]}
{"title": "Energy Demand Shifting in Residential Households: The Interdependence between Social Practices and Technology Design\n", "abstract": " Emerging energy technologies, such as smart meters and solar photovoltaic systems (solar PV), are changing our relationship to energy. There is increasing evidence that households with solar PV on their roof tend naturally to shift their energy consumption in time to match their local generation, but what do people actually do to achieve this and how ICT can support them to optimize their consumption? In this paper we present a year-long user study to understand social practices around laundry routines and local energy generation. We highlight four challenges for the next generation of home energy management systems.", "num_citations": "3\n", "authors": ["320"]}
{"title": "Engineering trust-awareness and self-adaptability in services and systems\n", "abstract": " The Future Internet (FI) comprises scenarios where many heterogeneous and dynamic entities must interact to provide services (e.g., sensors, mobile devices and information systems in smart city scenarios). The dynamic conditions under which FI applications must execute call for self-adaptive software to cope with unforeseeable changes in the application environment. Models@run.time is a promising model-driven approach that supports the runtime adaptation of distributed, heterogeneous systems. Yet frameworks that accommodate this paradigm have limited support to address security concerns, hindering their usage in real scenarios. We address this challenge by enhancing models@run.time with the concepts of trust and reputation. Trust improves decision-making processes under risk and uncertainty and constitutes a distributed and flexible mechanism that does not entail heavyweight\u00a0\u2026", "num_citations": "3\n", "authors": ["320"]}
{"title": "D\u00e9fis pour la variabilit\u00e9 et la tra\u00e7abilit\u00e9 des exigences en ing\u00e9nierie syst\u00e8me\n", "abstract": " Les grands projets industriels font face \u00e0 une volum\u00e9trie importante des exigences, souvent contraintes par un cadre r\u00e9glementaire ou l\u00e9gislatif important mais implicite. Ces projets sont men\u00e9s via des approches centr\u00e9es documents et connaissent une variabilit\u00e9 importante de leurs exigences tant \u00e0 la conception que pendant l'exploitation. Apr\u00e8s avoir identifi\u00e9 un certain nombre de facteurs de variabilit\u00e9, nous nous positionnons pour une approche dirig\u00e9e par les mod\u00e8les pour expliciter ce contexte r\u00e9glementaire et adresser la variabilit\u00e9 et la tra\u00e7abilit\u00e9 des exigences dans un contexte industriel et s\u00fbret\u00e9 de fonctionnement.", "num_citations": "3\n", "authors": ["320"]}
{"title": "Trust in MDE components: the DOMINO experiment\n", "abstract": " A large number of modeling activities can be automatic or computer assisted. This automation ensures a more rapid and robust software development. However, engineers must ensure that the models have the properties required for the application. In order to tend towards this requirement, the Domino project (DOMaINs and methodological prOcess) proposes to use the so-called trustworthy Model-Driven Engineering (MDE) components and aims to provide a methodology for the validation and qualification of such components.", "num_citations": "3\n", "authors": ["320"]}
{"title": "CROW: Code Diversification for WebAssembly\n", "abstract": " The adoption of WebAssembly increases rapidly, as it provides a fast and safe model for program execution in the browser. However, WebAssembly is not exempt from vulnerabilities that can be exploited by malicious observers. Code diversification can mitigate some of these attacks. In this paper, we present the first fully automated workflow for the diversification of WebAssembly binaries. We present CROW, an open-source tool implementing this workflow through enumerative synthesis of diverse code snippets expressed in the LLVM intermediate representation. We evaluate CROW\u2019s capabilities on 303 C programs and study its use on a real-life securitysensitive program: libsodium, a modern cryptographic library. Overall, CROW is able to generate diverse variants for 239 out of 303 (79%) small programs. Furthermore, our experiments show that our approach and tool is able to successfully diversify offthe-shelf cryptographic software (libsodium).", "num_citations": "2\n", "authors": ["320"]}
{"title": "Trace-based Debloat for Java Bytecode\n", "abstract": " Software bloat is code that is packaged in an application but is actually not used and not necessary to run the application. The presence of bloat is an issue for software security, for performance, and for maintenance. In recent years, several works have proposed techniques to detect and remove software bloat. In this paper, we introduce a novel technique to debloat Java bytecode through dynamic analysis, which we call trace-based debloat. We have developed JDBL, a tool that automates the collection of accurate execution traces and the debloating process. Given a Java project and a workload, JDBL generates a debloated version of the project that is syntactically correct and preserves the original behavior, modulo the workload. We evaluate the feasibility and the effectiveness of trace-based debloat with 395 open-source Java libraries for a total 10M+ lines of code. We demonstrate that our approach significantly reduces the size of these libraries while preserving the functionalities needed by their clients.", "num_citations": "2\n", "authors": ["320"]}
{"title": "CROW: Code Diversification for WebAssembly\n", "abstract": " The adoption of WebAssembly has rapidly increased in the last few years as it provides a fast and safe model for program execution. However, WebAssembly is not exempt from vulnerabilities that could be exploited by side channels attacks. This class of vulnerabilities that can be addressed by code diversification. In this paper, we present the first fully automated workflow for the diversification of WebAssembly binaries. We present CROW, an open-source tool implementing this workflow. We evaluate CROW's capabilities on 303 C programs and study its use on a real-life security-sensitive program: libsodium, a cryptographic library. Overall, CROWis able to generate diverse variants for 239 out of 303,(79%) small programs. Furthermore, our experiments show that our approach and tool is able to successfully diversify off-the-shelf cryptographic software (libsodium).", "num_citations": "2\n", "authors": ["320"]}
{"title": "Scalable comparison of javascript V8 bytecode traces\n", "abstract": " The comparison and alignment of runtime traces are essential, eg, for semantic analysis or debugging. However, naive sequence alignment algorithms cannot address the needs of the modern web:(i) the bytecode generation process of V8 is not deterministic;(ii) bytecode traces are large.", "num_citations": "2\n", "authors": ["320"]}
{"title": "Suggestions on test suite improvements with automatic infection and propagation analysis\n", "abstract": " An extreme transformation removes the body of a method that is reached by one test case at least. If the test suite passes on the original program and still passes after the extreme transformation, the transformation is said to be undetected, and the test suite needs to be improved. In this work we propose a technique to automatically determine which of the following three reasons prevent the detection of the extreme transformation is : the test inputs are not sufficient to infect the state of the program; the infection does not propagate to the test cases; the test cases have a weak oracle that does not observe the infection. We have developed Reneri, a tool that observes the program under test and the test suite in order to determine runtime differences between test runs on the original and the transformed method. The observations gathered during the analysis are processed by Reneri to suggest possible improvements to the developers. We evaluate Reneri on 15 projects and a total of 312 undetected extreme transformations. The tool is able to generate a suggestion for each each undetected transformation. For 63% of the cases, the existing test cases can infect the program state, meaning that undetected transformations are mostly due to observability and weak oracle issues. Interviews with developers confirm the relevance of the suggested improvements and experiments with state of the art automatic test generation tools indicate that no tool can improve the existing test suites to fix all undetected transformations.", "num_citations": "2\n", "authors": ["320"]}
{"title": "Images of code: Lossy compression for native instructions\n", "abstract": " Developers can use lossy compression on images and many other artifacts to reduce size and improve network transfer times. Native program instructions, however, are typically not considered candi-dates for lossy compression since arbitrary losses in instructions may dramatically affect program output. In this paper we show that lossy compression of compiled native instructions is possible in certain circumstances. We demonstrate that the instructions sequence of a program can be lossily translated into a separate but equivalent program with instruction-wise differences, which still produces the same output. We contribute the novel insight that it is possible to exploit such instruction differences to design lossy compression schemes for native code. We support this idea with sound and unsound program transformations that improve performance of compression techniques such as Run-Length (RLE), Huffman and\u00a0\u2026", "num_citations": "2\n", "authors": ["320"]}
{"title": "The Emerging Field of Test Amplification: A Survey\n", "abstract": " Context: The increasing adoption of test-driven development results in software projects with strong test suites. These suites include a large number of test cases, in which developers embed knowledge about meaningful input data and expected properties in the form of oracles. Objective: This article surveys various works that aim at exploiting this knowledge in order to enhance these manually written tests with respect to an engineering goal (e.g., improve coverage of changes or increase the accuracy of fault localization). While these works rely on various techniques and address various goals, we believe they form an emerging and coherent field of research, and which we call \"test amplification\". Method: We devised a first set of papers based on our knowledge of the literature (we have been working in software testing for years). Then, we systematically followed the citation graph. Results: This survey is the first that draws a comprehensive picture of the different engineering goals proposed in the literature for test amplification. In particular, we note that the goal of test amplification goes far beyond maximizing coverage only. Conclusion: We believe that this survey will help researchers and practitioners entering this new field to understand more quickly and more deeply the intuitions, concepts and techniques used for test amplification.", "num_citations": "2\n", "authors": ["320"]}
{"title": "libmask: Protecting Browser JIT Engines from the Devil in the Constants\n", "abstract": " JavaScript (JS) engines are virtual machines that execute JavaScript code. These engines find frequent application in web browsers like Google Chrome, Mozilla Firefox, Microsoft Internet Explorer and Apple Safari. Since, the purpose of a JS engine is to produce executable code, it cannot be run in a non-executable environment, and is susceptible to attacks like Just-in-Time (JIT) Spraying, which embed return-oriented programming (ROP) gadgets in arithmetic or logical instructions as immediate offsets. This paper introduces libmask, a JIT compiler extension to prevent the JIT-spraying attacks as an effective alternative to XOR based constant blinding. libmask transforms constants into global variables and marks the memory area for these global variables as read only. Hence, any constant is referred to by a memory address making exploitation of arithmetic and logical instructions more difficult. Further, these\u00a0\u2026", "num_citations": "2\n", "authors": ["320"]}
{"title": "A Variability-Based Testing Approach for Synthesizing Video Sequences\n", "abstract": " A key problem when developing video processing software is the difficulty to test different input combinations. In this paper, we present VANE, a variability-based testing approach to derive video sequence variants. The ideas of VANE are i) to encode in a variability model what can vary within a video sequence; ii) to exploit the variability model to generate testable configurations; iii) to synthesize variants of video sequences corresponding to configurations. VANE computes T-wise covering sets while optimizing a function over attributes. Also, we present a preliminary validation of the scalability and practicality of VANE in the context of an industrial project involving the test of video processing algorithms.VANE computes T-wise covering sets while optimizing a function over attributes. Also, we present a preliminary validation of the scalability and practicality of VANE in the context of an industrial project involving the test of video processing algorithms.", "num_citations": "2\n", "authors": ["320"]}
{"title": "A variability-based testing approach for synthesizing video sequences\n", "abstract": " A key problem when developing video processing software is the di culty to test di erent input combinations. In this paper, we present VANE, a variability-based testing approach to derive video sequence variants. The ideas of VANE are i) to encode in a variability model what can vary within a video sequence; ii) to exploit the variability model to generate testable con gurations; iii) to synthesize variants of video sequences corresponding to con gurations. VANE computes T-wise covering sets while optimizing a function over attributes. Also, we present a preliminary validation of the scalability and practicality of VANE in the context of an industrial project involving the test of video processing algorithms.", "num_citations": "2\n", "authors": ["320"]}
{"title": "API beauty is in the eye of the clients: 2.2 million Maven dependencies reveal the spectrum of client-API usages\n", "abstract": " Hyrum\u2019s law states a common observation in the software industry: \u201cWith a sufficient number of users of an API, it does not matter what you promise in the contract: all observable behaviors of your system will be depended on by somebody\u201d. Meanwhile, recent research results seem to contradict this observation when they state that \u201cfor most APIs, there is a small number of features that are actually used\u201d. In this work, we perform a large scale empirical study of client API relationships in the Maven ecosystem, in order to investigate this seeming paradox between the observations in industry and the research literature.We study the 94 most popular libraries in Maven Central, as well as the 829,410 client artifacts that declare a dependency to these libraries and that are available in Maven Central, summing up to 2.2M dependencies. Our analysis indicates the existence of a wide spectrum of API usages, with enough\u00a0\u2026", "num_citations": "1\n", "authors": ["320"]}
{"title": "Production Monitoring to Improve Test Suites\n", "abstract": " In this article, we propose to use production executions to improve the quality of testing for certain methods of interest for developers. These methods can be methods that are not covered by the existing test suite or methods that are poorly tested. We devise an approach called pankti which monitors applications as they execute in production and then automatically generates differential unit tests, as well as derived oracles, from the collected data. pankti\u2019s monitoring and generation focuses on one single programming language, Java. We evaluate it on three real-world, open-source projects: a videoconferencing system, a PDF manipulation library, and an e-commerce application. We show that pankti is able to generate differential unit tests by monitoring target methods in production and that the generated tests improve the quality of the test suite of the application under consideration.", "num_citations": "1\n", "authors": ["320"]}
{"title": "The Behavioral Diversity of Java JSON Libraries\n", "abstract": " JSON is a popular file and data format that is precisely specified by the IETF in RFC 8259. Yet, this specification implicitly and explicitly leaves room for many design choices when it comes to parsing and generating JSON. This yields the opportunity of diverse behavior among independent implementations of JSON libraries. A thorough analysis of this diversity can be used by developers to choose one implementation or to design a resilient multi-version architecture. We present the first systematic analysis and comparison of the input / output behavior of 20 JSON libraries, in Java. We analyze the diversity of architectural choices among libraries, and we execute each library with well-formed and ill-formed JSON files to assess their behavior. We first find that the data structure selected to represent JSON objects and the encoding of numbers are the main design differences, which influence the behavior of the libraries. Second, we observe that the libraries behave in a similar way with regular, well-formed JSON files. However, there is a remarkable behavioral diversity with ill-formed files, or corner cases such as large numbers or duplicate data.", "num_citations": "1\n", "authors": ["320"]}
{"title": "Maximizing Error Injection Realism for Chaos Engineering with System Calls\n", "abstract": " In this paper, we present a novel fault injection framework for system call invocation errors, called Phoebe. Phoebe is unique as follows; First, Phoebe enables developers to have full observability of system call invocations. Second, Phoebe generates error models that are realistic in the sense that they mimic errors that naturally happen in production. Third, Phoebe is able to automatically conduct experiments to systematically assess the reliability of applications with respect to system call invocation errors in production. We evaluate the effectiveness and runtime overhead of Phoebe on two real-world applications in a production environment. The results show that Phoebe successfully generates realistic error models and is able to detect important reliability weaknesses with respect to system call invocation errors. To our knowledge, this novel concept of \"realistic error injection\", which consists of grounding fault\u00a0\u2026", "num_citations": "1\n", "authors": ["320"]}
{"title": "DUETS: A Dataset of Reproducible Pairs ofJava Library-Clients\n", "abstract": " Software engineering researchers look for software artifacts to study their characteristics or to evaluate new techniques. In this paper, we introduce DUETS, a new dataset of software libraries and their clients. This dataset can be exploited to gain many different insights, such as API usage, usage inputs, or novel observations about the test suites of clients and libraries. DUETS is meant to support both static and dynamic analysis. This means that the libraries and the clients compile correctly, they are executable and their test suites pass. The dataset is composed of open-source projects that have more than five stars on GitHub. The final dataset contains 395 libraries and 2,874 clients. Additionally, we provide the raw data that we use to create this dataset, such as 34,560 pom.xml files or the complete file list from 34,560 projects. This dataset can be used to study how libraries are used by their clients or as a list of software projects that successfully build. The client's test suite can be used as an additional verification step for code transformation techniques that modify the libraries.", "num_citations": "1\n", "authors": ["320"]}
{"title": "Constraint-based software diversification for efficient mitigation of code-reuse attacks\n", "abstract": " Modern software deployment process produces software that is uniform, and hence vulnerable to large-scale code-reuse attacks. Compiler-based diversification improves the resilience and security of software systems by automatically generating different assembly code versions of a given program. Existing techniques are efficient but do not have a precise control over the quality of the generated code variants. This paper introduces Diversity by Construction (DivCon), a constraint-based compiler approach to software diversification. Unlike previous approaches, DivCon allows users to control and adjust the conflicting goals of diversity and code quality. A key enabler is the use of Large Neighborhood Search (LNS) to generate highly diverse assembly code efficiently.\u00a0Experiments using two popular compiler benchmark suites confirm that there is a trade-off between quality of each assembly code version and diversity of\u00a0\u2026", "num_citations": "1\n", "authors": ["320"]}
{"title": "Superoptimization of WebAssembly bytecode\n", "abstract": " Motivated by the fast adoption of WebAssembly, we propose the first functional pipeline to support the superoptimization of WebAssembly bytecode. Our pipeline works over LLVM and Souper. We evaluate our superoptimization pipeline with 12 programs from the Rosetta code project. Our pipeline improves the code section size of 8 out of 12 programs. We discuss the challenges faced in superoptimization of WebAssembly with two case studies.", "num_citations": "1\n", "authors": ["320"]}
{"title": "Automatic Observability for Dockerized Java Applications\n", "abstract": " Docker is a virtualization technique heavily used in the industry to build cloud-based systems. In the context of Docker, a system is said to be observable if engineers can get accurate information about its running state in production. In this paper, we present a novel approach, called POBS, to automatically improve the observability of Dockerized Java applications. POBS is based on automated transformations of Docker configuration files. Our approach injects additional modules in the production application, in order to provide better observability. We evaluate POBS by applying it on open-source Java applications which are containerized with Docker. Our key result is that 223/248 (90%) of Docker Java containers can be automatically augmented with better observability.", "num_citations": "1\n", "authors": ["320"]}
{"title": "API Beauty is in the eye of the Clients: 2.2 Million Maven Dependencies reveal the Spectrum of Client-API Usages\n", "abstract": " Hyrum's law states a common observation in the software industry: \"With a sufficient number of users of an API, it does not matter what you promise in the contract: all observable behaviors of your system will be depended on by somebody\". Meanwhile, recent research results seem to contradict this observation when they state that \"for most APIs, there is a small number of features that are actually used\". We investigate this seeming paradox between the observations in industry and the research literature, with a large scale empirical study of client API relationships in one single ecosystem: Maven Central. We study the 94 most popular libraries in Maven Central, as well as the 829,410 client artifacts that declare a dependency to these libraries and that are available in Maven Central, summing up to 2.2M dependencies. Our analysis indicates the existence of a wide spectrum of API usages, with enough clients, most API types end up being used at least once. Our second key observation is that, for all libraries, there is a small set of API types that are used by the vast majority of its clients. The practical consequences of this study are two-fold: (i) it is possible for API maintainers to find an essential part of their API on which they can focus their efforts; (ii) API developers should limit the public API elements to the set of features for which they are ready to have users.", "num_citations": "1\n", "authors": ["320"]}
{"title": "Hiding in the Crowd: an Analysis of the E ectiveness of Browser Fingerprinting at Large Scale\n", "abstract": " Browser ngerprinting is a stateless technique, which consists in collecting a wide range of data about a device through browser APIs. Past studies have demonstrated that modern devices present so much diversity that ngerprints can be exploited to identify and track users online. With this work, we want to evaluate if browser ngerprinting is still e ective at uniquely identifying a large group of users when analyzing millions of ngerprints over a few months. We collected 2,067,942 browser ngerprints from one of the top 15 French websites. The analysis of this novel dataset sheds a new light on the ever-growing browser ngerprinting domain. The key insight is that the percentage of unique ngerprints in our dataset is much lower than what was reported in the past: only 33.6% of ngerprints are unique by opposition to over 80% in previous studies. We show that non-unique ngerprints tend to be fragile. If some features of\u00a0\u2026", "num_citations": "1\n", "authors": ["320"]}
{"title": "Variability management in domain-specific languages\n", "abstract": " Domain-specific languages (DSLs) have demonstrated their capability to reduce the gap between the problem domain and the techni-cal decisions during the software development process. However, building a DSL is not an easy task because it requires specialized knowledge and skills. Moreover, the challenge becomes even more complex in the con-text of multi-domain companies where several domains coexist across the business units and, consequently, there is a need of dealing not only with isolated DSLs but also with families of DSLs. To deal with this complexity, the research community has been working on the definition of approaches that use the ideas of Software Product Lines Engineering (SPLE) for building and maintaining families of DSLs. In this paper, we present a PhD thesis that is aimed to contribute to this effort. In particular, we explain the challenges that need to be addressed during the process of going from a family of DSLs to a software language line. Then, we briefly discuss the state of the art, and finally we introduce a research plan.", "num_citations": "1\n", "authors": ["320"]}
{"title": "Analysis and Exploitation of Natural Software Diversity: The Case of API Usages\n", "abstract": " In this paper, we study how object-oriented classes are used across thousands of software packages. We concentrate on \"usage diversity\", defined as the different statically observable combinations of methods called on the same object. We present empirical evidence that there is a significant usage diversity for many classes. For instance, we observe in our dataset that Java's String is used in 2460 manners. Beyond those empirical observations, we show that we can use this API usage diversity to reason on the core design of object-oriented classes. We think that our pieces of evidence on API usage diversity shake up some established ideas on the nature of software and how to engineer it. Hence, we discuss those empirical results in the general context of software engineering: what are the reasons behind this diversity? what are the implications of this diversity?", "num_citations": "1\n", "authors": ["320"]}
{"title": "On Product Comparison Matrices and Variability Models from a Product Comparison/Configuration Perspective\n", "abstract": " Comparators and configurators have now become common in our daily activities and are usually based on Product Comparison Matrices (PCMs) to present and compare features. Based on a previous analysis of 300+ PCMs from Wikipedia, we identify the limits of existing comparators, configurators and PCMs. Variability Models (VMs) have been extensively used through the last 20 years to provide a synthetic and formal way to represent a product line. As a consequence, using VMs instead of PCMs could tackle these limits and improve comparison and configuration activities. In this paper, we present 5 research questions that focus on using VMs to represent PCMs and their applications for comparators and configurators.", "num_citations": "1\n", "authors": ["320"]}
{"title": "Toward a Model-driven Access-control Enforcement Mechanism for Pervasive Systems\n", "abstract": " Pervasive systems typically involve heterogeneous users, devices and networks to provide services seamlessly interacting with the physical world. In order to be flexible, these systems must be both dynamically adaptive to handle and still open to the ability of receiving new elements. Characteristics of these systems can have a major impact on the enforcement of role-based access control policies. Enforcement mechanism for RBAC policies need to be tailored to distributed and adaptive software architectures. It must be capable of handling architectural changes (e.g., a resource hosted by a node is moved to another node) in order to maintain the enforced policy. In this paper we describe an approach of policy enforcement that leverages on a mapping between RBAC and a component-based architecture to reason on architectural changes and maintain the enforced policy. Models@runtime paradigm provides elementary bricks to reason on adaptive architecture. Relying on it and on runtime adaptation and monitoring mechanisms we propose a design for a model-driven RBAC enforcement mechanism.", "num_citations": "1\n", "authors": ["320"]}
{"title": "Action Sp\u00e9cifique 2011 GeMoC du GDR GPL \u00abIng\u00e9nierie du logiciel pour les syst\u00e8mes h\u00e9t\u00e9rog\u00e8nes\u00bb\n", "abstract": " L\u2019Action Sp\u00e9cifique 2011 GeMoC est une action \u00e0 court terme (mai 2011-d\u00e9cembre 2011) soutenue par le GDR CNRS G\u00e9nie de la Programmation et du Logiciel (GPL) pour r\u00e9aliser une \u00e9tude sur l\u2019activit\u00e9 men\u00e9e en France et \u00e0 l\u2019international dans le domaine de \u00abl\u2019ing\u00e9nierie du logiciel pour les syst\u00e8mes h\u00e9t\u00e9rog\u00e8nes\u00bb.Apr\u00e8s un rappel du contexte et des objectifs de l\u2019AS GeMoC (Section 1), nous pr\u00e9sentons dans ce rapport d\u2019activit\u00e9 le r\u00e9sultat des diff\u00e9rents travaux men\u00e9s. Apr\u00e8s le d\u00e9tail des diff\u00e9rents \u00e9v\u00e8nements r\u00e9alis\u00e9s (Section 2), nous proposons une terminologie pr\u00e9cise caract\u00e9risant les diff\u00e9rentes h\u00e9t\u00e9rog\u00e9n\u00e9it\u00e9s identifi\u00e9es dans l\u2019ing\u00e9nierie du logiciel (Section 3). Sur la base de cette terminologie, nous proposons dans la section 4 la synth\u00e8se d\u2019une revue syst\u00e9matique et comparative sur une des h\u00e9t\u00e9rog\u00e9n\u00e9it\u00e9s identifi\u00e9es: la mod\u00e9lisation h\u00e9t\u00e9rog\u00e8ne. Nous pr\u00e9sentons ensuite l\u2019organisation et les\u00a0\u2026", "num_citations": "1\n", "authors": ["320"]}
{"title": "Extension du modelbased testing pour la prise en compte de la variabilit\u00e9 dans les syst\u00e8mes complexes\n", "abstract": " Le test est la phase la plus couteuse dans un cycle de d\u00e9veloppement. Plusieurs m\u00e9thodes de test sont utilis\u00e9es pour optimiser cette phase, parmi ces solutions le Modelbased Testing (MBT) est de plus en plus utilis\u00e9 pour le test de syst\u00e8mes complexes dans l'industrie.Le MBT est une technique en \u00e9volution pour g\u00e9n\u00e9rer automatiquement des tests \u00e0 partir d\u2019un mod\u00e8le d\u00e9crivant certains aspects du syst\u00e8me sous test (SUT). Son but principal est la g\u00e9n\u00e9ration des tests de validation en se basant sur la sp\u00e9cification. La g\u00e9n\u00e9ration de test est faite g\u00e9n\u00e9ralement \u00e0 partir d\u2019un mod\u00e8le de test (Mark Utting, 2007).", "num_citations": "1\n", "authors": ["320"]}
{"title": "A framework for testing model composition engines\n", "abstract": " Model composition helps designers managing complexities by modeling different system views separately, and later compose them into an integrated model. In the past years, researchers have focused on the definition of model composition approaches (operators) and the tools supporting them (model composition engines). Testing model composition engines is hard. It requires the synthesis and analysis of complex data structures (models). In this context, synthesis means to assembly complex structures in a coherent way with respect to semantic constraints. In this paper we propose to automatically synthesize input data for model composition engines using a model decomposition operator. Through this operator we synthesize models in a coherent way, satisfying semantic constraints and taking into account the complex mechanics involved in the model composition. Furthermore, such operator enables a\u00a0\u2026", "num_citations": "1\n", "authors": ["320"]}
{"title": "Reusable MDA Components: A Design-for-Trust Approach\n", "abstract": " In essence, MDA proposes a move away from human interpretation of high-level models, such as design diagrams, into implementations, towards a more automated process where the models are used as first-class artifacts of the development process. The core mechanism for this automation is model transformation. Among the many aspects of the model-driven development process, making model transformations trustable is an obvious target since they impact on the design process reliability. This is even more critical when transformations are to be reused. Ideally, a model transformation program should be designed and tested so that it may be used and reused safely as a MDA component. This paper presents a method for building trustable MDA components. We first define the notion of MDA components, eg model transformation programs, as composed of its specification, one implementation and the test cases needed for testing it. The testing-for-trust approach, using the mutation analysis, checks the consistency between specification, implementation and tests. It points out the tests lack of efficiency but also the lack of precision of the executable part of the component\u2019s specification which is captured by the notion of guard in the QVT 2 OMG standard. Thus, we can associate each self-testable component with a value\u2014its level of trustability\u2014that quantifies the test sequence\u2019s effectiveness at testing a given implementation of the component. Our method for achieving this quantification is a version of selective mutation analysis that we adapted to transformation languages, wrt the QVT 2 upcoming standard. Relying on this objective estimation\u00a0\u2026", "num_citations": "1\n", "authors": ["320"]}
{"title": "Master internship: Browser fingerprint obfuscation through random reconfiguration\n", "abstract": " Mayer in 2009 [4] and Eckersley in 2010 [1] have shown that a web browser\u2019s features (version, operating system, IP address, plugins, fonts, Flash and Javascript) can provide enough information to constitute a unique ID, or fingerprint, and can be used to track users without the need for cookies. Unlike cookies, browser fingerprinting is completely transparent to the user. Today, there are a small number of commercial companies that use such methods to provide browser identification through web-based fingerprinting instead of, or in complement to, using cookies, web bugs and other known techniques. Browser fingerprints can be used by owners of web sites for legitimate purposes such as combating fraud by tracking session hijacking. However, fingerprints can also serve more suspicious purposes such as tracking users between websites for targeted advertisement or for delivering exploits specific to specific\u00a0\u2026", "num_citations": "1\n", "authors": ["320"]}