{"title": "A review of structural health monitoring literature: 1996\u20132001\n", "abstract": " Staff members at Los Alamos National Laboratory (LANL) produced a summary of the structural health monitoring literature in 1995. This presentation will summarize the outcome of an updated review covering the years 1996-2001. The updated review follows the LANL statistical pattern recognition paradigm for SHM, which addresses four topics: 1. Operational Evaluation; 2. Data Acquisition and Cleansing; 3. Feature Extraction; and 4. Statistical Modeling for Feature Discrimination. The literature has been reviewed based on how a particular study addresses these four topics. A significant observation from this review is that although there are many more SHM studies being reported, the investigators, in general, have not yet fully embraced the well-developed tools from statistical pattern recognition. As such, the discrimination procedures employed are often lacking the appropriate rigor necessary for this\u00a0\u2026", "num_citations": "2365\n", "authors": ["1394"]}
{"title": "Updating finite element dynamic models using an element-by-element sensitivity methodology\n", "abstract": " A sensitivity-based methodology for improving the finite element model of a given structure using test modal data and a few sensors is presented. The proposed method searches for both the location and sources of the mass and stiffness errors and does not interfere with the theory behind the finite element model while correcting these errors. The updating algorithm is derived from the unconstrained minimization of the squared\u00a3 2 norms of the modal dynamic residuals via an iterative two-step staggered procedure. At each iteration, the measured mode shapes are first expanded assuming that the model is error free, then the model parameters are corrected assuming that the expanded mode shapes are exact. The numerical algorithm is implemented in an element-by-element fashion and is capable of\" zooming\" on the detected error locations. Several simulation examples which demonstrate the potential of the\u00a0\u2026", "num_citations": "347\n", "authors": ["1394"]}
{"title": "Concepts of model verification and validation\n", "abstract": " Model verification and validation (V&V) is an enabling methodology for the development of computational models that can be used to make engineering predictions with quantified confidence. Model V&V procedures are needed by government and industry to reduce the time, cost, and risk associated with full-scale testing of products, materials, and weapon systems. Quantifying the confidence and predictive accuracy of model calculations provides the decision-maker with the information necessary for making high-consequence decisions. The development of guidelines and procedures for conducting a model V&V program are currently being defined by a broad spectrum of researchers. This report reviews the concepts involved in such a program.Model V&V is a current topic of great interest to both government and industry. In response to a ban on the production of new strategic weapons and nuclear testing, the Department of Energy (DOE) initiated the Science-Based Stockpile Stewardship Program (SSP). An objective of the SSP is to maintain a high level of confidence in the safety, reliability, and performance of the existing nuclear weapons stockpile in the absence of nuclear testing. This objective has challenged the national laboratories to develop high-confidence tools and methods that can be used to provide credible models needed for stockpile certification via numerical simulation.", "num_citations": "309\n", "authors": ["1394"]}
{"title": "Review and assessment of model updating for non-linear, transient dynamics\n", "abstract": " The purpose of this publication is to motivate the need of validating numerical models based on time-domain data for non-linear, transient, structural dynamics and to discuss some of the challenges faced by this technology. Our approach is two-fold. First, several numerical and experimental testbeds are presented that span a wide variety of applications (from non-linear vibrations to shock response) and difficulty [from a single-degree-of-freedom (sdof) system with localised non-linearity to a three-dimensional (3-D), multiple-component assembly featuring non-linear material response and contact mechanics]. These testbeds have been developed at Los Alamos National Laboratory in support of the Advanced Strategic Computing Initiative and our code validation and verification program. Conventional, modal-based updating techniques are shown to produce erroneous models although the discrepancy between\u00a0\u2026", "num_citations": "158\n", "authors": ["1394"]}
{"title": "Uncertainty and sensitivity analysis of damage identification results obtained using finite element model updating\n", "abstract": " A full\u2010scale seven\u2010story reinforced concrete shear wall building structure was tested on the UCSD\u2010NEES shake table in the period October 2005\u2013January 2006. The shake table tests were designed so as to damage the building progressively through several historical seismic motions reproduced on the shake table. A sensitivity\u2010based finite element (FE) model updating method was used to identify damage in the building. The estimation uncertainty in the damage identification results was observed to be significant, which motivated the authors to perform, through numerical simulation, an uncertainty analysis on a set of damage identification results. This study investigates systematically the performance of FE model updating for damage identification. The damaged structure is simulated numerically through a change in stiffness in selected regions of a FE model of the shear wall test structure. The uncertainty of the\u00a0\u2026", "num_citations": "146\n", "authors": ["1394"]}
{"title": "Improved damage location accuracy using strain energy-based mode selection criteria\n", "abstract": " IN the maintenance of aerospace and civil structures, the ability to evaluate the integrity of the structure is becoming an important technology. Inspection techniques that require physically dismantling the structure are not appropriate because of interference with its operation. Assessing the structural condition without removing the individual structural components is known as nondestructive evaluation (NDE) or nondestructive inspection. Many methods have been developedfor NDE, and an overview of the various techniques is presented by Witherell. 1 Some techniques are based on visual observationsof cracks, such as visual inspection and dye-penetrant inspection. Some are based on the electromagnetic propertiesof the material, such as magnetic particle inspection", "num_citations": "130\n", "authors": ["1394"]}
{"title": "An energy based optimum sensor placement criterion and its application to structural damage detection\n", "abstract": " 2. INTRODUCTION? be growi\u201d g size and cmnplexity of space smx-Nes makes it \u201cecessary t\u201d design se\u201d si\u201d g coniigurations that provide the ide\u201d tificatio\u201d,\u201cpdati\u201d g, a\u201d d control algorithms with the best p\u201d ssible i\u201d f\u201d r\u201d xxion about the snuctwe\u2019s carrent dynanics. However, only a snxill nu\u201d ker of points can be insmmlenti for practical and ec\u201d\u201c0\u201d 6 caI reasons while the quality of the measured medal parameters depe\u201d ds o\u201d both the ability to excite the target modal set a\u201d d the observability of the s! mctue~ s resIxmse.\u2018Iherefore, the optbnal placement of actuamrs a\u201d d se\u201d s\u201d rs", "num_citations": "96\n", "authors": ["1394"]}
{"title": "Structural damage detection via a finite-element model updating methodology\n", "abstract": " This paper describes the application of a finite element model updating methodology to structural damage identification problems. After giving a brief summary of the Sensitivity-based Element-by-element (SB-EBE) updating algorithm issues related to the damage detection problem are addressed. The relevance of specific identified modal data is determined with a strain energy based criterion. The degree to which the finite element discretization and the sensing configuration both influence the localization of structural damage is investigated. Finally the difficulty of assessing the quality of the updated model is demonstrated using a damage case for which the solution is not known to the authors. The experimental data used for demonstrating the potential of the updating procedure for damage detection in real structures were obtained from the NASA Langley Research Center.", "num_citations": "87\n", "authors": ["1394"]}
{"title": "Proton-induced cross sections relevant to production of 225Ac and 223Ra in natural thorium targets below 200 MeV\n", "abstract": " Cross sections for 223,225Ra, 225Ac and 227Th production by the proton bombardment of natural thorium targets were measured at proton energies below 200\u00a0MeV. Our measurements are in good agreement with previously published data and offer a complete excitation function for 223,225Ra in the energy range above 90\u00a0MeV. Comparison of theoretical predictions with the experimental data shows reasonable-to-good agreement. Results indicate that accelerator-based production of 225Ac and 223Ra below 200\u00a0MeV is a viable production method.", "num_citations": "80\n", "authors": ["1394"]}
{"title": "A Review of Structural Health Review of Structural Health Monitoring Literature 1996-2001.\n", "abstract": " Staff members at Los Alamos National Laboratory (LANL) produced a summary of the structural health monitoring literature in 1995. This presentation will summarize the outcome of an updated review covering the years 1996-2001. The updated review follows the LANL statistical pattern recognition paradigm for SHM, which addresses four topics:(1) Operational Evaluation;(2) Data Acquisition and Cleansing;(3) Feature Extraction; and (4) Statistical Modeling for Feature Discrimination. The literature has been reviewed based on how a particular study addresses these four topics. A significant observation from this review is that although there are many more SHM studies being reported, the investigators, in general, have not yet fully embraced the well-developed tools from statistical pattern recognition. As such, the discrimination procedures employed are often lacking the appropriate rigor necessary for this technology to evolve beyond demonstration problems carried out in laboratory setting.", "num_citations": "80\n", "authors": ["1394"]}
{"title": "Theoretical and experimental correlation between finite element models and modal tests in the context of large flexible space structures\n", "abstract": " Although the field of finite element analysis of structural dynamic has witnessed tremendous progress in the last two decades, more confidence is still placed in experimental data. Therefore, the finite element model of a given structure is often updated to reflect the results of a particular experiment.", "num_citations": "76\n", "authors": ["1394"]}
{"title": "225Ac and 223Ra production via 800 MeV proton irradiation of natural thorium targets\n", "abstract": " Cross sections for the formation of 225,227Ac, 223,225Ra, and 227Th via the proton bombardment of natural thorium targets were measured at a nominal proton energy of 800\u00a0MeV. No earlier experimental cross section data for the production of 223,225Ra, 227Ac and 227Th by this method were found in the literature. A comparison of theoretical predictions with the experimental data shows agreement within a factor of two. Results indicate that accelerator-based production of 225Ac and 223Ra is a viable production method.", "num_citations": "74\n", "authors": ["1394"]}
{"title": "Uncertainty quantification in model verification and validation as applied to large scale historic masonry monuments\n", "abstract": " This publication focuses on the Verification and Validation (V&V) of numerical models for establishing confidence in model predictions, and demonstrates the complete process through a case study application completed on the Washington National Cathedral masonry vaults. The goal herein is to understand where modeling errors and uncertainty originate from, and obtain model predictions that are statistically consistent with their respective measurements. The approach presented in this manuscript is comprehensive, as it considers all major sources of errors and uncertainty that originate from numerical solutions of differential equations (numerical uncertainty), imprecise model input parameter values (parameter uncertainty), incomplete definitions of underlying physics due to assumptions and idealizations (bias error) and variability in measurements (experimental uncertainty). The experimental evidence\u00a0\u2026", "num_citations": "69\n", "authors": ["1394"]}
{"title": "Bayesian model screening for the identification of nonlinear mechanical structures\n", "abstract": " The development of techniques for identification and updating of nonlinear mechanical structures has received increasing attention in recent years. In practical situations, there is not necessarily a priori knowledge about the nonlinearity. This suggests the need for strategies that allow inference of useful information from the data. The present study proposes an algorithm based on a Bayesian inference approach for giving insight into the form of the nonlinearity. A family of parametric models is defined to represent the nonlinear response of a system and the selection algorithm estimates the likelihood that each member of the family is appropriate. The (unknown) probability density function of the family of models is explored using a simple variant of the Markov Chain Monte Carlo sampling technique. This technique offers the advantage that the nature of the underlying statistical distribution need not be assumed a\u00a0\u2026", "num_citations": "64\n", "authors": ["1394"]}
{"title": "Improved best estimate plus uncertainty methodology, including advanced validation concepts, to license evolving nuclear reactors\n", "abstract": " Many evolving nuclear energy technologies use advanced predictive multiscale, multiphysics modeling and simulation (M&S) capabilities to reduce the cost and schedule of design and licensing. Historically, the role of experiments has been as a primary tool for the design and understanding of nuclear system behavior, while M&S played the subordinate role of supporting experiments. In the new era of multiscale, multiphysics computational-based technology development, this role has been reversed. The experiments will still be needed, but they will be performed at different scales to calibrate and validate the models leading to predictive simulations for design and licensing. Minimizing the required number of validation experiments produces cost and time savings. The use of multiscale, multiphysics models introduces challenges in validating these predictive tools \u2013 traditional methodologies will have to be\u00a0\u2026", "num_citations": "58\n", "authors": ["1394"]}
{"title": "Defining predictive maturity for validated numerical simulations\n", "abstract": " The increasing reliance on computer simulations in decision-making motivates the need to formulate a commonly accepted definition for \u201cpredictive maturity.\u201d The concept of predictive maturity involves quantitative metrics that could prove useful while allocating resources for physical testing and code development. Such metrics should be able to track progress (or lack thereof) as additional knowledge becomes available and is integrated into the simulations for example, through the addition of new experimental datasets during model calibration, and/or through the implementation of better physics models in the codes. This publication contributes to a discussion of attributes that a metric of predictive maturity should exhibit. It is contended that the assessment of predictive maturity must go beyond the goodness-of-fit of the model to the available test data. We firmly believe that predictive maturity must also consider the\u00a0\u2026", "num_citations": "57\n", "authors": ["1394"]}
{"title": "Vibration characteristics of vaulted masonry monuments undergoing differential support settlement\n", "abstract": " This paper assesses the feasibility of vibration testing to detect structural damage caused by the settlement of buttresses in the Beverley Minster, a Gothic church located in the UK. Over the past eight centuries, the accumulated support settlements of the buttresses of Beverley Minster have pulled the main nave walls outward, causing severe separation along the edges of the masonry vaults. Bays closer to the main crossing tower have remained intact; however, at the west end of the Minster, the crack width between the walls and vaults has reached about 150\u00a0mm, leading to approximately 200\u00a0mm of sag at the crown of the vaults. Due to uneven settlement of buttresses along the nave of the church, the Minster now has ten nominally identical vaults at different damage states. In this work, two of these vaults representing the two extremes, the most damaged and undamaged structural states, are subjected to vibration\u00a0\u2026", "num_citations": "56\n", "authors": ["1394"]}
{"title": "MIMO LMS-ARMAX identification of vibrating structures\u2014part II: a critical assessment\n", "abstract": " In this part of the paper, a critical assessment of the MIMO (multiple-input multiple-output) LMS-ARMAX method is presented, along with comparisons with a pure ARX version and the Eigensystem Realisation Algorithm (ERA) based upon two-input three-output vibration data obtained from a scale aircraft skeleton structure. This structure is characterised by light (\u03b6<1%) damping and seven modes within the considered frequency range, two of which are closely spaced and another is a \u2018local\u2019 tail mode. The study focuses on the: (i) ability of the methods to handle higher-dimensional problems, (ii) ability to estimate closely spaced and \u2018local\u2019 modes. (iii) ability to accurately estimate light modal damping, (iv) required model overdetermination, (v) distinction of structural from \u2018extraneous\u2019 modes, (vi) effects of various (white/colour) noise environments, and (vii) suitability of various discrete-time representations for effective\u00a0\u2026", "num_citations": "55\n", "authors": ["1394"]}
{"title": "Uncertainty, validation of computer models and the myth of numerical predictability\n", "abstract": " This publication addresses the issues of modeling, uncertainty quantification, model validation and numerical predictability. With the increasing role of numerical simulation in science, technology as well as every day decision-making, assessing the predictive accuracy of computer models becomes essential. Conventional approaches such as finite element model updating or Bayesian inference are undeniably useful tools but they do not fully answer the question: How accurately does the model represent reality? First, the evolution of scientific computing and consequences in terms of modeling and analysis practices are discussed. The intimate relationship between modeling and uncertainty is explored by defining uncertainty as an integrate part of the model, not just parametric variability or the lack of knowledge about the physical system being investigated. Examples from nuclear physics, climate prediction and\u00a0\u2026", "num_citations": "55\n", "authors": ["1394"]}
{"title": "A review of structural health monitoring literature 1996-2001\n", "abstract": " DSpace at KOASAS: A review of structural health monitoring literature 1996-2001 KOASAS menu About KOASAS KAIST Library \uac80\uc0c9 Advanced Search Browse Communities & Collections Researchers at KAIST Titles Subject By Date rss_1.rss_2.atom_1.sherpa SEARCH DSpace at KOASAS College of Engineering(\uacf5\uacfc\ub300\ud559)Dept. of Civil and Environmental Engineering(\uac74\uc124\ubc0f \ud658\uacbd\uacf5\ud559\uacfc)CE-Conference Papers(\ud559\uc220\ud68c\uc758\ub17c\ubb38) A review of structural health monitoring literature 1996-2001 Cited 0 time in webofscience Cited 0 time in scopus Hit : 570 Download : 0 Export DC(XML) Excel Farrar, Charles R. / Czarnecki, Jerry J. / Sohn, Hoonresearcher / Hemez, Francois M. Publisher Structural Control Issue Date 2002-04-07 Language English Citation The 3rd World Conference on Structural Control URI http://hdl.handle.net/10203/138322 Appears in Collection CE-Conference Papers(\ud559\uc220\ud68c\uc758\ub17c\ubb38) Files in This Item \u2026", "num_citations": "53\n", "authors": ["1394"]}
{"title": "Identification of response surface models using genetic programming\n", "abstract": " There is a move in modern research in Structural Dynamics towards analysing the inherent uncertainty in a given problem. This may be quantifying or fusing uncertainty models, or can be propagation of uncertainty through a system or calculation. If the system of interest is represented by, e.g. a large Finite Element (FE) model the large number of computations involved can rule out many approaches due to the expense of carrying out many runs. One way of circumnavigating this problem is to replace the true system by an approximate surrogate/replacement model, which is fast-running compared to the original. In traditional approaches using response surfaces a simple least-squares multinomial model is often adopted. The objective of this paper is to extend the class of possible models considerably by carrying out a general symbolic regression using a Genetic Programming approach. The approach is\u00a0\u2026", "num_citations": "52\n", "authors": ["1394"]}
{"title": "Uncertainty analysis of system identification results obtained for a seven\u2010story building slice tested on the UCSD\u2010NEES shake table\n", "abstract": " A full\u2010scale seven\u2010story reinforced concrete building section/slice was tested on the Network for Earthquake Engineering Simulation (NEES) shake table at the University of California San Diego during the period of October 2005 to January 2006. Three output\u2010only system identification methods were used to extract the modal parameters (natural frequencies, damping ratios, and mode shapes) of the test structure at different damage states. In this study, the performance of these system identification methods is investigated in two cases: (Case I) when these methods are applied to the measured dynamic response of the structure and (Case II) when these methods are applied to the dynamic response of the structure simulated using a three\u2010dimensional nonlinear finite element model thereof. In both cases, the uncertainty/variability of the identified modal parameters due to the variability of several input factors is\u00a0\u2026", "num_citations": "47\n", "authors": ["1394"]}
{"title": "Info-gap robustness for the correlation of tests and simulations of a non-linear transient\n", "abstract": " An alternative to the theory of probability is applied to the problem of assessing the robustness, to uncertainty in model parameters, of the correlation between measurements and computer simulations. The analysis is based on the theory of information-gap uncertainty, which models the clustering of uncertain events in families of nested sets instead of assuming a probability structure. The system investigated is the propagation of a transient impact through a layer of hyper-elastic material. The two sources of non-linearity are (1) the softening of the constitutive law representing the hyper-elastic material and (2) the contact dynamics at the interface between metallic and crushable materials. The robustness of the correlation between test and simulation, to sources of parameter variability, is first studied to identify the parameters of the model that significantly influence the agreement between measurements and\u00a0\u2026", "num_citations": "44\n", "authors": ["1394"]}
{"title": "Uncertainty quantification and the verification and validation of computational models\n", "abstract": " The material presented in this chapter is largely based on a tutorial from the Los Alamos Dynamics Summer School [1].In computational physics and engineering, numerical models are developed to predict the behavior of a system whose response cannot be measured experimentally. A key aspect of science-based predictive modeling is to assess the credibility of predictions. Credibility, which is usually demonstrated through the activities of model verification and validation (V&V) refers to the extent to which numerical simulations can be analyzed with confidence to represent the phenomenon of interest [2]. One can argue, as it has been proposed in recent work [3], that the credibility of a mathematical or numerical model must combine three components:(i) an assessment of fidelity to test data;(ii) an assessment of the robustness of prediction-based decisions to variability, uncertainty, and lack-of-knowledge, and (iii) an assessment of the consistency of predictions provided by a family of models in situations where test measurements are not available. Unfortunately, the three goals are antagonistic, as illustrated in Reference [3] for a wide class of uncertainty models.", "num_citations": "44\n", "authors": ["1394"]}
{"title": "Neural identification of non-linear dynamic structures\n", "abstract": " Neural networks are applied to the identification of non-linear structural dynamic systems. Two complementary problems inspired from customer surveys are successively considered. Each of them calls for a different neural approach. First, the mass of the system is identified based on acceleration recordings. Statistical experiments are carried out to simultaneously characterize optimal pre-processing of the accelerations and optimal neural network models. It is found that key features for mass identification are the fourth statistical moment and the normalized power spectral density of the acceleration. Second, two architectures of recurrent neural networks, an autoregressive and a state-space model, are derived and tested for dynamic simulations, showing higher robustness of the autoregressive form. Discussion is first based on a non-linear two-degree-of-freedom problem. Neural identification is then used to\u00a0\u2026", "num_citations": "44\n", "authors": ["1394"]}
{"title": "Simulating the dynamics of wind turbine blades: part II, model validation and uncertainty quantification\n", "abstract": " Verification and validation (V&V) offers the potential to play an indispensable role in the development of credible models for the simulation of wind turbines. This paper highlights the development of a three\u2010dimensional finite element model of the CX\u2010100 wind turbine blade. The scientific hypothesis that we wish to confirm by applying V&V activities is that it is possible to develop a fast\u2010running model capable of predicting the low\u2010order vibration dynamics with sufficient accuracy. A computationally efficient model is achieved by segmenting the geometry of the blade into six sections only. It is further assumed that each cross section can be homogenized with isotropic material properties. The main objectives of V&V activities deployed are to, first, assess the extent to which these assumptions are justified and, second, to quantify the resulting prediction uncertainty. Designs of computer experiments are analyzed to\u00a0\u2026", "num_citations": "43\n", "authors": ["1394"]}
{"title": "Robustness, fidelity and prediction-looseness of models\n", "abstract": " Assessment of the credibility of a mathematical or numerical model of a complex system must combine three components: (i) the fidelity of the model to test data, e.g. as quantified by a mean-squared error; (ii) the robustness, of model fidelity, to lack of understanding of the underlying processes; and (iii) the prediction-looseness of the model. \u2018Prediction-looseness\u2019 is the range of predictions of models that are equivalent in terms of fidelity. The main result of this paper asserts that fidelity, robustness and prediction-looseness are mutually antagonistic. A change in the model that enhances one of these attributes will cause deterioration of another. In particular, increasing the fidelity to test data will decrease the robustness to imperfect understanding of the process. Likewise, increasing the robustness will increase the predictive looseness. The conclusion is that focusing only on fidelity-to-data is not a sound decision\u00a0\u2026", "num_citations": "42\n", "authors": ["1394"]}
{"title": "Simulating the dynamics of wind turbine blades: part I, model development and verification\n", "abstract": " In the state of the art of modeling and simulation of wind turbines, verification and validation\u2009(V&V) is a somewhat underdeveloped field. The purpose of this paper is to spotlight the process of a completely integrated V&V procedure, as it is applied to a wind turbine blade. The novelty, besides illustrating the application of V&V to blade modeling, is to challenge the conventional separation between verification and validation activities. First, simple closed\u2010form solutions for bending stress, torsional stress and mode shapes of a hollow cylinder are derived analytically to verify the ANSYS finite element software. Shell\u2010281 elements are used to approximate these closed\u2010form solutions and demonstrate that the software runs properly. The grid convergence index is used to quantify the degree of numerical uncertainty that results. Next, model development and verification activities are applied to the CX\u2010100 blade\u00a0\u2026", "num_citations": "41\n", "authors": ["1394"]}
{"title": "A coupled approach to developing damage prognosis solutions\n", "abstract": " 1MS T-006, Los Alamos National Laboratory, Los Alamos, NM 87545, USA farrar@ lanl. gov 2MS T-006, Los Alamos National Laboratory, Los Alamos, NM 87545, USA hemez@ lanl. gov 3MS T-006, Los Alamos National Laboratory, Los Alamos, NM 87545, USA gpark@ lanl. gov 4 HYTEC, Inc., 4735 Walnut, Suite W-100 Boulder, CO 80301, USA, arobertson@ hytecinc. com 5MS T-006, Los Alamos National Laboratory, Los Alamos, NM 87545, USA sohn@ lanl. gov 6MS B-216, Los Alamos National Laboratory, Los Alamos, NM 87545, USA oakhill@ lanl. gov", "num_citations": "40\n", "authors": ["1394"]}
{"title": "A forecasting metric for predictive modeling\n", "abstract": " In science and engineering, simulation models calibrated against a limited number of experiments are commonly used to forecast at settings where experiments are unavailable, raising concerns about the unknown forecasting errors. Forecasting errors can be quantified and controlled by deploying statistical inference procedures, combined with an experimental campaign to improve the fidelity of a simulation model that is developed based on sound physics or engineering principles. This manuscript illustrates that the number of experiments required to reduce the forecasting errors to desired levels can be determined by focusing on the proposed forecasting metric.", "num_citations": "34\n", "authors": ["1394"]}
{"title": "A framework for assessing confidence in computational predictions\n", "abstract": " T his article is the third in a series of papers concern-ing the importance of simulation code validation to the US Department of Energy Accelerated Strategic Computing Initiative (ASCI) program. 1 The series started with a review by John Garcia of the critical need for advanced validation techniques in the ASCI program, which was created to make up for the absence of nuclear testing through the use of simulation codes. Without testing, the simulation codes must be able to answer critical questions about the reliability of our aging stockpile of weapons. In the second paper, Bill Oberkampf gave an overview of validation concepts and described the requirements for a well-executed validation experiment. In this article we discuss the analysis of data obtained from validation experiments and motivate the use of uncertainties to quantify the accuracy of predictions made by simulation codes. This work represents merely a\u00a0\u2026", "num_citations": "33\n", "authors": ["1394"]}
{"title": "Application of non-linear system model updating using feature extraction and parameter effects analysis\n", "abstract": " This research presents a new method to improve analytical model fidelity for non-linear systems. The approach investigates several mechanisms to assist the analyst in updating an analytical model based on experimental data and statistical analysis of parameter effects. The first is a new approach at data reduction called feature extraction. This approach is an expansion of theclassic'update metrics to include specific phenomena or character of the response that is critical to model application. This is an extension of the familiar linear updating paradigm of utilizing the eigen-parameters or frequency response functions (FRFs) to include such devices as peak acceleration, time of arrival or standard deviation of model error. The next expansion of the updating process is the inclusion of statistical based parameter analysis to quantify the effects of uncertain or significant effect parameters in the construction of a meta\u00a0\u2026", "num_citations": "33\n", "authors": ["1394"]}
{"title": "A metamodel-based approach to model validation for nonlinear finite element simulations\n", "abstract": " Metamodeling, also known as response surface analysis, is the de facto standard for mathematical representation of complex phenomena in many fields, especially when first principles physical relationships are not well-defined, eg economics, climatology, and government policy. Metamodels provide a computationally efficient, low-dimension relationship for studying the behavior of a physical system. They can be used for understanding the physical system, predicting its response, optimizing its design or the parameters in a physical model, and performing verification and validation. Metamodels can be derived from simulation results or fit directly to observed test data. In structural dynamics, typical practice is to develop a first-principles-based model such as a finite element model to study the behavior of the system. However, it is common that the features of interest in a structural dynamics simulation are relatively low order (eg first few modal frequencies, peak acceleration at certain locations) and sensitive to relatively few model and simulation parameters. In these cases, metamodeling provides a convenient format to facilitate activities of model validation, including parameter screening, sensitivity analysis [3], uncertainty analysis, and test/analysis correlation. This paper describes the creation of metamodels, and presents some examples of how metamodels can be employed to facilitate model validation for more\u00bb", "num_citations": "32\n", "authors": ["1394"]}
{"title": "MODEL VALIDATION AND UNCERTAINTY QUANTIFICATION.\n", "abstract": " This session offers an open forum to discuss issues and directions of research in the areas of model updating, predictive quality of computer simulations, model validation and uncertainty quantification. Technical presentations review the state-of-the-art in nonlinear dynamics and model validation for structural dynamics. A panel discussion introduces the discussion on technology needs, future trends and challenges ahead with an emphasis placed on soliciting participation of the audience, One of the goals is to show, through invited contributions, how other scientific communities are approaching and solving difficulties similar to those encountered in structural dynamics. The session also serves the purpose of presenting the on-going organization of technical meetings sponsored by the US Department of Energy and dedicated to health monitoring, damage prognosis, model validation and uncertainty quantification in engineering applications. The session is part of the SD-2000 Forum, a forum to identify research trends, funding opportunities and to discuss the future of structural dynamics.", "num_citations": "31\n", "authors": ["1394"]}
{"title": "Overview of uncertainty assessment for structural health monitoring\n", "abstract": " Uncertainty quantification is an emergent field in engineering mechanics that makes use of statistical sampling, hypothesis testing and input-output effect analysis to characterize the effect that parametric and non-parametric uncertainty has on physical experiment or numerical simulation output. This publication overviews a project at Los Alamos National Laboratory that aims at developing a methodology for quantifying uncertainty and assessing the total predictability of structural dynamics simulations. The propagation of parametric variability through numerical simulations is discussed. Uncertainty assessment is also a critical component of model validation, where the total error between physical observation and model prediction must be characterized. The purpose of model validation is to assess the extent to which a model is an appropriate representation of reality, given the purpose intended for the numerical\u00a0\u2026", "num_citations": "30\n", "authors": ["1394"]}
{"title": "Design of computer experiments for improving an impact test simulation\n", "abstract": " This paper gives an overall presentation of a research project pursued at Los Alamos National Laboratory for the validation of numerical simulations for engineering structural dynamics. An impact experiment used to develop and test the model validation methodology is presented. Design of experiments techniques are implemented to perform parametric studies using the numerical model and improve itspredictive quality. The analysis relies on correlation study where input parameters responsible for explaining the total variability of the numerical experiment are identified, then, updated. The quality of the model is assessed via its ability to reproduce the same statistics as those inferred from the experiment data sets. Throughout the paper, a particular emphasis is placed on presenting the contribution to this project of Amanda Wilson, undergraduate student at Texas Tech University, and research assistant at Los Alamos in the summer of 2000 in conjunction with the Los Alamos Dynamics Summer School. The model validation project is described in greater details inthe companion paper [1].", "num_citations": "29\n", "authors": ["1394"]}
{"title": "Uncertainty quantification of simulation codes based on experimental data\n", "abstract": " Our approach to understanding the uncertainties in simulation code predictions combines the principles of physics and Bayesian analysis. The focus is on understanding and quantifying the uncertainties in the simulation-code submodels and the numerical errors introduced in solving the dynamical equations. Bayesian analysis provides the underpinning for quantifying the uncertainties in models inferred from experimental results, which possess their own degree of uncertainty. The aim is to construct an uncertainty model that is based on inferences drawn from comparing the code\u2019s predictions to relevant experimental results. In the context of the proposed framework, it is possible to design new experiments that can best provide data for reducing prediction uncertainty [1, 2].The sources of uncertainty in a simulation-code prediction of the outcome to a hypothesized physical situation include a) uncertainties in the\u00a0\u2026", "num_citations": "27\n", "authors": ["1394"]}
{"title": "Use of response surface metamodels for identification of stiffness and damping coefficients in a simple dynamic system\n", "abstract": " Metamodels have been used with success in many areas of engineering for decades but only recently in the field of structural dynamics. A metamodel is a fast running surrogate that is typically used to aid an analyst or test engineer in the fast and efficient exploration of the design space. Response surface metamodels are used in this work to perform parameter identification of a simple five degree of freedom system, motivated by their low training requirements and ease of use. In structural dynamics applications, response surface metamodels have been utilized in a forward sense, for activities such as sensitivity analysis or uncertainty quantification. In this study a polynomial response surface model is developed, relating system parameters to measurable output features. Once this relationship is established, the response surface is used in an inverse sense to identify system parameters from measured output features.", "num_citations": "26\n", "authors": ["1394"]}
{"title": "The myth of science-based predictive modeling.\n", "abstract": " A key aspect of science-based predictive modeling is the assessment of prediction credibility. This publication argues that the credibility of a family of models and their predictions must combine three components:(1) the fidelity of predictions to test data;(2) the robustness of predictions to variability, uncertainty, and lack-of-knowledge; and (3) the prediction accuracy of models in cases where measurements are not available. Unfortunately, these three objectives are antagonistic. A recently published Theorem that demonstrates the irrevocable trade-offs between fidelity-to-data, robustness-to-uncertainty, and confidence in prediction is summarized. High-fidelity models cannot be made increasingly robust to uncertainty and lack-of-knowledge. Similarly, robustness-to-uncertainty can only be improved at the cost of reducing the confidence in prediction. The concept of confidence in prediction relies on a metric for total uncertainty, capable of aggregating different representations of uncertainty (probabilistic or not). The discussion is illustrated with an engineering application where a family of models is developed to predict the acceleration levels obtained when impacts of varying levels propagate through layers of crushable hyper-foam material of varying thicknesses. Convex modeling is invoked to represent a severe lack-of-knowledge about the constitutive material behavior. The analysis produces intervals of performance metrics from which the total uncertainty and confidence more\u00bb", "num_citations": "25\n", "authors": ["1394"]}
{"title": "The good, the bad, and the ugly of predictive science\n", "abstract": " In computational physics and engineering, numerical models are developed to predict the behavior of a system whose response cannot be measured experimentally. A key aspect of science-based predictive modeling is the assessment of prediction credibility. Credibility, which is demonstrated through the activities of Verification and Validation, quantifies the extent to which simulation results can be analyzed with confidence to represent the phenomenon of interest with accuracy consistent with the intended use of the model. This paper argues that assessing the credibility of a mathematical or numerical model must combine three components: 1) Improving the fidelity to test data; 2) Studying the robustness of prediction-based decisions to variability, uncertainty, and lack-of-knowledge; and 3) Establishing the expected prediction accuracy of the models in situations where test measurements are not available. A recently published Theorem that demonstrates the irrevocable trade-offs between \u201cThe Good, The Bad, and The Ugly,\u201d or robustness-to-uncertainty, fidelity-to-data, and confidence-in-prediction, is summarized. The main implication is that high-fidelity models cannot, at the same time, be made robust to uncertainty and lack-of-knowledge. Similarly, equally robust models do not provide consistent predictions, hence reducing confidence-in-prediction. The conclusion of the theoretical investigation is that, in assessing the predictive accuracy of numerical models, one should never focus on a single aspect. Instead, the trade-offs between fidelity-to-data, robustness-to-uncertainty, and confidence-in-prediction should be explored. The discussion\u00a0\u2026", "num_citations": "24\n", "authors": ["1394"]}
{"title": "Validation of structural dynamics models at Los Alamos National Laboratory\n", "abstract": " This publication proposes a discussion of the general problem of validating numerical models for nonlinear, transient dynamics. The predictive quality of a numerical model is generally assessed by comparing the computed response to test data. If the correlation is not satisfactory, an inverse problem must be formulated and solved to identify the sources of discrepancy between test and_ analysis data. Some of the most recent work summarized in this publication has focused on developing test-analysis correlation and inverse problem solving capabilities for nonlinear vibrations. Among the difficulties encountered, we cite the necessity to satisfy continuity of the response when several finite element optimizations are successively carried out and the need to propagate variability throughout the optimization of the model's parameters. After a brief discussion of the formulation of inverse problems for nonlinear dynamics\u00a0\u2026", "num_citations": "24\n", "authors": ["1394"]}
{"title": "Updating nonlinear finite element models in the time domain\n", "abstract": " This paper describes the implementation, verification, and comparison of two techniques for updating nonlinear finite-element structural dynamics models using transient time-domain data. The methods are motivated in terms of the intended applications, and the derivations are shown as they relate to the model updating methods for linear finite element models. The application of the two methods to simulated results for an impact problem (with a structure containing a hyperelastic polymer) is presented.", "num_citations": "24\n", "authors": ["1394"]}
{"title": "Test-analysis correlation and finite element model updating for nonlinear transient dynamics\n", "abstract": " This research aims at formulating criteria for measuring the correlation between test data and finite element results for nonlinear, transient dynamics. After reviewing the linear case and illustrating the limitations of modal-based updating when it is applied to nonlinear experimental data, simple time-domain, test-analysis correlation metrics are proposed. Two implementations are compared: the conventional least-squares technique and the Principal Component Decomposition that correlates subspaces rather than individual time-domain responses. Illustrations and discussions are provided using the LANL 8-DOF system, an experimental testbed for validating nonlinear data correlation and model updating techniques.", "num_citations": "24\n", "authors": ["1394"]}
{"title": "The dangers of sparse sampling for the quantification of margin and uncertainty\n", "abstract": " Activities such as global sensitivity analysis, statistical effect screening, uncertainty propagation, or model calibration have become integral to the Verification and Validation (V&V) of numerical models and computer simulations. One of the goals of V&V is to assess prediction accuracy and uncertainty, which feeds directly into reliability analysis or the Quantification of Margin and Uncertainty (QMU) of engineered systems. Because these analyses involve multiple runs of a computer code, they can rapidly become computationally expensive. An alternative to Monte Carlo-like sampling is to combine a design of computer experiments to meta-modeling, and replace the potentially expensive computer simulation by a fast-running emulator. The surrogate can then be used to estimate sensitivities, propagate uncertainty, and calibrate model parameters at a fraction of the cost it would take to wrap a sampling algorithm or\u00a0\u2026", "num_citations": "23\n", "authors": ["1394"]}
{"title": "Uncertainty analysis of modal parameters obtained from three system identification methods\n", "abstract": " Uncertainty analysis of modal parameters obtained from three system identification methods \u2014 University of Bristol Skip to main navigation Skip to search Skip to main content University of Bristol Logo Help & Terms of Use Home Profiles Research Units Research Outputs Projects Student theses Datasets Activities Prizes Facilities/Equipment Search by expertise, name or affiliation Uncertainty analysis of modal parameters obtained from three system identification methods B. Moaveni, AR Barbosa, JP Conte, FM Hemez Earthquake and Geotechnical Engineering Department of Civil Engineering Research output: Chapter in Book/Report/Conference proceeding \u203a Conference Contribution (Conference Proceeding) 12 Citations (Scopus) Overview Original language English Title of host publication Conference Proceedings of the Society for Experimental Mechanics Series Publication status Published - 2007 Access to \u2026", "num_citations": "22\n", "authors": ["1394"]}
{"title": "From shock response spectrum to temporal moments and vice-versa\n", "abstract": " Temporal momerils have been used in engineering mechanics to condense the information contained in the shock response spectrum into a few scalar quantities. This paper presents an application of temporal moments to the propagation of an explosive-driven shock wave through an assembly of metallic parts. For this particular application, it is shown that temporal moments characterize the response of the system better than other features traditionally used in the analysis of nonlinear, transient events, such as the peak response or 10% duration of event. The inverse problem is also illustrated: the original, time-domain signals and their shock response spectra can be reconstructed from the temporal moments. This property makes temporal moments features of choice for the analysis of experimental data or the development of numerical models because they are low-dimensional quantities; they capture transient dynamics well; and they can be used to re-generate the original time signals.", "num_citations": "21\n", "authors": ["1394"]}
{"title": "On assessing the robustness of structural health monitoring technologies\n", "abstract": " As structural health monitoring continues to gain popularity, both as an area of research and as a tool for use in industrial applications, the number of technologies associated with structural health monitoring will also continue to grow. As a result, the engineer tasked with developing a structural health monitoring system is faced with myriad hardware and software technologies from which to choose, often adopting an ad hoc qualitative approach based on physical intuition or past experience to making such decisions, and offering little in the way of justification for a particular decision. This article offers a framework that aims to provide the engineer with a quantitative approach for choosing from among a suite of candidate structural health monitoring technologies. The framework is outlined for the general case, where a supervised learning approach to structural health monitoring is adopted and is then demonstrated\u00a0\u2026", "num_citations": "20\n", "authors": ["1394"]}
{"title": "Overview of structural dynamics model validation activities at Los Alamos National Laboratory\n", "abstract": " This presentation will provide a summary of the research and applications of structural dynamics model validation at Los Alamos National Laboratory. In this context model validation refers to the assessment of confidence in the usefulness of computational structural dynamics predictions for a particular application. The presentation will cover the problem definition, objectives, and motivation for studying model validation. Current paradigms for the model validation problem will also be presented. Supporting technologies such as uncertainty quantification, global sensitivity analysis, metamodeling, parameter updating, and design of experiments will be discussed, along with their role in the model validation process. The usefulness of model validation results for the computational modeling of system-level structural dynamics will be demonstrated. Examples of model validation techniques applied to transient structural\u00a0\u2026", "num_citations": "20\n", "authors": ["1394"]}
{"title": "Statistical based non-linear model updating using feature extraction\n", "abstract": " This research presents a new method to improve analytical model fidelity for non-linear systems. The approach investigates several mechanisms to assist the analyst in updating an analytical model based on experimental data and statistical analysis of parameter effects. The first is a new approach at data reduction called feature extraction. This is an expansion of the update metrics to include specific phenomena or character of the response that is critical to model application. This is an extension of the classical linear updating paradigm of utilizing the eigen-parameters or FRFs to include such devices as peak acceleration, time of arrival or standard deviation of model error. The next expansion of the updating process is the inclusion of statistical based parameter analysis to quantify the effects of uncertain or significant effect parameters in the construction of a meta-model. This provides indicators of the statistical variation associated with parameters as well as confidence intervals on the coefficients of the resulting meta-model, Also included in this method is the investigation of linear parameter effect screening using a partial factorial variable array for simulation. This is intended to aid the analyst in eliminating from the investigation the parameters that do not have a significant variation more\u00bb", "num_citations": "20\n", "authors": ["1394"]}
{"title": "Damage detection in a suspended scale model truss via model update\n", "abstract": " An experimental investigation of the effect of test structure modal complexity on the success of a damage detection algorithm is presented. The type of structure used, the boundary conditions of the structure and the choice of experimental and identification parameters impact the success of any damage detection algorithm. This study presents a damage detection algorithm based on a particular \ufb01nite element updating scheme. It is demonstrated that the algorithm works very well for a cantilevered truss structure, but has trouble locating damage in a suspended truss with several concentrated masses. The reasons surrounding the difficulty of damage location in such a structure and possible sources of error in the experiment and analysis are examined.", "num_citations": "20\n", "authors": ["1394"]}
{"title": "Model validation for a complex jointed structure\n", "abstract": " An overview of the modeling and validation of a complex engineering simulation performed at Los Alamos National Laboratory is presented. The application discussed represents the highly transient response of an assembly with complex joints subjected to an impulse. The primary source of nonlinearity are the contact mechanics. Several tests are conducted to assess the degree of environmental uncertainty, the variability of the assembly and to provide reference data for model validation. After presenting the experiment and the corresponding numerical simulation, several issues of model validation are addressed. They include data reduction, feature extraction, design of computer experiments, statistical effect analysis and model updating. It is shown how these tools can help the analyst gain confidence regarding the predictive quality of the simulation.", "num_citations": "19\n", "authors": ["1394"]}
{"title": "A validation of Bayesian finite element model updating for linear dynamics\n", "abstract": " This work addresses the issue of statistical model updating and correlation. The updating procedure is formulated to improve the predictive quality of a structural model by minimizing out-of-balance modal forces. It is shown how measurement and modeling uncertainties can be taken into account to provide not only the correlated model but also the associated confidence levels. Hence, a Bayesian parameter estimation technique is derived and its numerical implementation is discussed. Two demonstration examples that involve test-analysis correlation with real test data are presented. First, the validation of an engine cradle model used in the automotive industry shows how the design's uncertainties can be reduced via model updating. The second example consists of employing test-analysis correlation for identifying the degree of nonlinearity of the LANL 8-DOF testbed.", "num_citations": "19\n", "authors": ["1394"]}
{"title": "Improving structural dynamics models by correlating simulated to measured frequency response functions\n", "abstract": " This paper examines difficulties in the updating of industrial structures using Frequency Response Functions (FRF). Such structures typically involve large numbers of elements, with few measurement points. Frequently, these structures are composed of discrete components or substructures at the interface of which energy may be dissipated and high frequency dynamics may be generated. These characteristics can create difficulty for system identification techniques that attempt to extract modal parameters from the measured response. The advantages and limitations of FRF-based model updating are investigated. After being formulated, the method is compared to conventional updating schemes that correlate modal parameters. Problems of interest include the computational efficiency of the method, its sensitivity to measurement noise, and the selection of frequency points used for updating the model. Results are\u00a0\u2026", "num_citations": "18\n", "authors": ["1394"]}
{"title": "Guaranteeing robustness of structural condition monitoring to environmental variability\n", "abstract": " Advances in sensor deployment and computational modeling have allowed significant strides to be recently made in the field of Structural Health Monitoring (SHM). One widely used SHM strategy is to perform a vibration analysis where a model of the structure's pristine (undamaged) condition is compared with vibration response data collected from the physical structure. Discrepancies between model predictions and monitoring data can be interpreted as structural damage. Unfortunately, multiple sources of uncertainty must also be considered in the analysis, including environmental variability, unknown model functional forms, and unknown values of model parameters. Not accounting for these sources of uncertainty can lead to false-positives or false-negatives in the structural condition assessment. To manage the uncertainty, we propose a robust SHM methodology that combines three technologies. A time series\u00a0\u2026", "num_citations": "17\n", "authors": ["1394"]}
{"title": "INVERSION OF STRUCTURAL DYNAMICS SIMULATIONS: STATE-OF-THE-ART AND ORIENTATIONS OF RESEARCH\n", "abstract": " This publication offers an overview of the technology available for formulating inverse problems and correlating measured responses with simulations from finite element analysis. The application targeted is clearly structural dynamics although most of the techniques discussed here originate or find their counterparts in physics and other engineering fields. After reviewing the state-of-the-art practices in model updating where a mostly linear model is optimized to satisfy a series of modal criteria of correlation, an assessment of the advantages and limitations of this technology is offered. Current orientations of inverse problem solving are discussed, including the need to propagate variability through the simulation; the generation of fast running models; the adequate choice of data metrics for nonlinear dynamics; sampling strategies for the optimization; and hypothesis testing in the context of multivariate data analysis.1\u00a0\u2026", "num_citations": "17\n", "authors": ["1394"]}
{"title": "Bypassing numerical difficulties associated with updating simultaneously mass and stiffness matrices\n", "abstract": " HIGH-ACCURACYspace structures require correlated finite el-ement models for predicting their on-orbit dynamics whenever testing is not practical and for adjusting their control laws. During the test-analysis reconciliation step, numerical instability and ill conditioning occur because of the inverse nature of the updating problem where an adjusted finite element model (FEM) that matches a set of identified modal parameters is sought. A number of authors have already exposed the difficulties associated with solving this illconditioned updating problem without introducing unrealistic nonphysical corrections. 1\" 3 For example, it has been often observed that numerical instabilities tend to produce mass and stiffness corrections that are greater than 100% of the original values. To cope with this issue, Imregun et al. 2 have proposed a scaling procedure where all rows of the correction system are made to have the same\u00a0\u2026", "num_citations": "17\n", "authors": ["1394"]}
{"title": "Locating and identifying structural damage using a sensitivity-based model updating methodology\n", "abstract": " A sensitivity based element-by-element methodology for updating finite element dynamic models and its application to structural damage detection are presented. A criterion based on the stain energy distribution is used to discriminate between identified modal data. The orthonormalization of an identified modal set is shown to play a pivotal role in locating structural damage, but the proposed methodology is proven to be insensitive to the adopted type of normalization. The potential of the updating procedure for locating and identifying structural damage is demonstrated with a ten bay truss problem using real experimental data obtained from the NASA Langley Research Center.", "num_citations": "17\n", "authors": ["1394"]}
{"title": "Model selection through robustness and fidelity criteria: Modeling the dynamics of the CX-100 wind turbine blade\n", "abstract": " Several plausible modeling strategies are available to develop numerical models for simulating the dynamics of wind turbine blades. While the modeling strategy is typically selected according to expert judgment, the \u201cbest\u201d modeling approach is unknown to the model developer. Thus, comparing plausible modeling strategies through a systematic and rigorous approach becomes necessary. This manuscript departs from the conventional approach that selects the model with the highest fidelity-to-data; and instead explores the trade-off between fidelity of model predictions to experiments and robustness of model predictions to model imprecision and inexactness. Exploring robustness in addition to fidelity lends credibility to the model, ensuring model predictions can be trusted even when lack-of-knowledge in the modeling assumptions and/or input parameters result in unforeseen errors and uncertainties. This\u00a0\u2026", "num_citations": "16\n", "authors": ["1394"]}
{"title": "Robustness-to-uncertainty, fidelity-to-data, prediction-looseness of models\n", "abstract": " A key aspect of science-based predictive modeling is to assess the credibility of predictions. To gain confidence in predictions, one should demonstrate consistency between physical observations, expert judgments, and the predictions of equally credible models. This suggests a relationship between fidelity-to-data, robustness-to-uncertainty, and confidence in prediction. The purpose of this work is to explore the interaction between these three aspects of predictive modeling. The concepts of fidelity, robustness, and confidence are first defined in a broad sense. A Theorem is then proven that establishes that these three objectives are antagonistic. This means that high-fidelity models cannot, at the same time, be made robust to uncertainty and lack-of-knowledge. Similarly, equally robust models cannot provide consistent predictions, hence reducing confidence. The conclusion of this theoretical investigation is that, in assessing the predictive accuracy of numerical models, one should never focus on a single aspect only. Instead, the trade-offs between fidelity-to-data, robustness-to-uncertainty, and confidence in prediction should be explored.", "num_citations": "16\n", "authors": ["1394"]}
{"title": "SELECTION OF EXPERIMENTAL MODAL DATA SETS FOR DEMAGE DETECTION VIA MODEL UPDATE\n", "abstract": " An issue often addressed in the study of Large Space Structures is the ability to remotely monitor the condition of the structure and to assess the extent and location of any damage due to the orbital environment. One method to detect damage within a st~~ cture is to monitor and compare data from dynamic Sensor measurements both before and after the damage has occurred. A fundamental issue of this approach is: Given two sets of dynamic sensor measurements, is it possible to uniquely determine the change in the ever, for a particular damage case, many of these structure which produced-the change in the s contribute little information, if they are largely measurements? Does the test data Contain sufficient information to discriminate between many possible damage cases? unaffected by the damage, so they increase the computational burden without contributing any insight about the damage location. In this\u00a0\u2026", "num_citations": "16\n", "authors": ["1394"]}
{"title": "Answering the question of sufficiency: How much uncertainty is enough\n", "abstract": " Quantifying uncertainty, where it comes from, and what its effect is on measurements and predictions, are cornerstone activities of any Verification and Validation (V&V) program that establishes the credibility of experiments and simulations. The themes developed in this work are how to handle uncertainty in computational sciences and make decisions based on partially-validated simulations. Two main points are made. The first point is that uncertainty should not be restricted to variability. It must encompass numerical uncertainty, conflict and ambiguity contained in datasets, and lack-of-knowledge regarding assumptions that unavoidably present themselves during modelbuilding. A consequence to the potentially great diversity of these sources of uncertainty is that alternatives to probability theory have an important role to play. But this brings a question: How much uncertainty is enough? The second point made is\u00a0\u2026", "num_citations": "15\n", "authors": ["1394"]}
{"title": "Non-linear error ansatz models in space and time for solution verification\n", "abstract": " In computational physics and engineering, partial differential equations that govern the evolution of state variables are discretized for implementation and solution using finite-digit computer arithmetic. Solution verification is the activity that verifies that computed solutions of the discretized equations converge to the exact solution of the continuous equations. The state-of-the-practice is to estimate convergence rates and numerical errors from several computed solutions obtained with a sequence of coarse-to-refined grids. In addition to being valid only in the asymptotic regime of convergence, this approach to verification relies on three unverified assumptions. First, convergence must be monotonic. Second, explicit coupling between grid resolution or element size and time increment is neglected. Third, higher-order terms of the error have no significant influence and can be neglected. The main contribution of this\u00a0\u2026", "num_citations": "14\n", "authors": ["1394"]}
{"title": "Challenges in computational social modeling and simulation for national security decision making\n", "abstract": " On October 26th and 27th, 2010, Sandia National Laboratories SNL organized an interdisciplinary workshop in which participants from a range of institutions and research backgrounds presented and discussed papers on a range of topics related to the development and use of computational social science CSS in national security decision-making. Computational social science refers to the use of computational modeling and simulation approaches, including agent-based, social network, discrete event, and systems dynamics methodologies, to study behavioral, cultural, and social dynamics. CSS has long roots in computer science, artificial intelligence, and quantitative social science. Over the past decade, CSS methods have captured the attention of the national security community as a source of analytic and decision-support technologies for a range of challenges, from counterinsurgency to terrorism. The first phase of this project was a comparative, interdisciplinary review of literature related to applied computational modeling and simulation in both the social and physical sciences this study is described in a summary paper that McNamara and Trucano authored Appendix A. The workshop participants reviewed the work by McNamara and Trucano in light of their own research and work experiences. Our participants included social, computational, and physical scientists from a range of government, industry, and academic institutions. Most, but not all, had also participated in projects to develop and deploy computational models and simulations of social phenomena for decision-making and over half the participants had worked on\u00a0\u2026", "num_citations": "13\n", "authors": ["1394"]}
{"title": "Uncertainty analysis of test data shock responses\n", "abstract": " Statistical techniques are applied to the analysis of uncertainty for a series of transient dynamics tests. The physical experiments consist of studying the transmission of a mechanical shock, induced by detonating explosive strips on one side of a structure, through a threaded assembly of components. Events last only a few milliseconds with significant dynamics resonating in the 15,000 to 50,000 Hertz range. Acceleration and strain gauge measurements are collected. Statistics of time series are estimated by looking at individual channels, then, pooling all channels together. It is shown that the repeatability of measurements is excellent since the ratio of variability to structural response is about 10%. The peak values, energy and centroid temporal moments are extracted and their statistics estimated. Methods employed to quantify uncertainty and its effect on response features include the principal component analysis, analysis-of-variance, Bayesian effect screening, and statistical modeling. Not all results of the analysis are presented. The publication emphasizes the identification of overall trends of the response variability and understanding of their origin based on acceleration time signals only. This publication was approved for unlimited, public release on October 26, 2005, LA-UR-05-8229, Unclassified.", "num_citations": "13\n", "authors": ["1394"]}
{"title": "Verification and validation of a composite model\n", "abstract": " The paper presents preliminary results of applying methods developed for verifying and validating the numerical simulations of multi-layered composite plates. A hierarchy of three validation experiments is defined to validate various aspects of the modeling. The experiments are: modal testing; quasi-static loading; and impact testing. The paper focuses on the validation of the modal response of eight-ply laminated composite plates. After verifying some implementation aspects of the code, mesh convergence studies are conducted. Effect screening is performed to restrict the varying input parameters to the most significant ones. Polynomial meta-models are developed to replace the potentially expensive finite element simulations. Uncertainty is propagated to estimate the variability of predictions given input uncertainty. Test measurements are compared to predictions of modal frequencies. A final statement is made\u00a0\u2026", "num_citations": "13\n", "authors": ["1394"]}
{"title": "Uncertainty quantification and model validation for damage prognosis\n", "abstract": " The publication overviews the quantification of uncertainty and model validation of the \u201cDamage Prognosis\u201d project at Los Alamos National Laboratory. Damage prognosis is the process of predicting the remaining useful life of an engineered system given the measurement and assessment of its current structural condition and accompanying predicted performance in anticipated future loading environments. Because the activity of prognosis relies on numerical simulations to a great extent, the sources of uncertainty must be assessed and the predictive accuracy of the mathematical models must be established. The description of uncertainty relies, for the most part, on a probabilistic representation. Uncertainty propagation therefore relies on sampling techniques. Design of experiments and methods of analysis of variance provide efficient ways of sampling the operational space and assessing the effect of uncertainty\u00a0\u2026", "num_citations": "13\n", "authors": ["1394"]}
{"title": "Statistical model updating and validation applied to nonlinear transient structural dynamics\n", "abstract": " This paper presents an example of a statistics-based model updating and validation philosophy applied to a nonlinear transient structural dynamics problem. The problem being analyzed is the response of a steel/polymer foam assembly during a drop test. The objective of the simulation is to accurately predict the acceleration response of the cylinder with the appropriate statistical distribution. Model validation is performed to ensure that the predictions agree with the experimental measurements to within an acceptable limit. A systematic approach to the model validation and updating problem is followed. The approach begins with the definition of the validation criteria, including the features of interest and the metrics to be used in comparing the features. Next, the mean values of the simulation parameters are updated, followed by an updating of the parameter covariance values to more accurately predict the distribution of the features of interest. An assessment of the predictive accuracy of the simulation using the final parameter estimates concludes the demonstration of statistical model validation and updating.", "num_citations": "13\n", "authors": ["1394"]}
{"title": "Extending sensitivity-based updating to lightly damped structures\n", "abstract": " The sensitivity-based element-by-element (SB-EBE) method is extended for updating the\u00ae nite element models of lightly damped structures using experimentally measured complex modes. The new updating method proceeds in two steps. First, the measured mode shapes are expanded and the mass and stiffness matrices are corrected to minimize a set of dynamic residuals. Next, a damping matrix is constructed to absorb the remainder of these dynamic residuals. As the mass and stiffness matrices are corrected before the computation of the damping matrix, the extension is used for the updating of lightly damped structures. The new SB-EBE method is illustrated with two examples that highlight its potential for re\u00ae ning the\u00ae nite element models of lightly damped structures.", "num_citations": "13\n", "authors": ["1394"]}
{"title": "A case study to quantify prediction bounds caused by model-form uncertainty of a portal frame\n", "abstract": " Numerical simulations, irrespective of the discipline or application, are often plagued by arbitrary numerical and modeling choices. Arbitrary choices can originate from kinematic assumptions, for example the use of 1D beam, 2D shell, or 3D continuum elements, mesh discretization choices, boundary condition models, and the representation of contact and friction in the simulation. This work takes a step toward understanding the effect of arbitrary choices and model-form assumptions on the accuracy of numerical predictions. The application is the simulation of the first four resonant frequencies of a one-story aluminum portal frame structure under free\u2013free boundary conditions. The main challenge of the portal frame structure resides in modeling the joint connections, for which different modeling assumptions are available. To study this model-form uncertainty, and compare it to other types of uncertainty, two finite\u00a0\u2026", "num_citations": "12\n", "authors": ["1394"]}
{"title": "A brief overview of the state-of-the-practice and current challenges of solution verification\n", "abstract": " The main goal of solution verification is to assess the convergence of numerical predictions as a function of discretization variables such as element size \u0394x or time step \u0394t. The challenge is to verify that the approximate solutions of the discretized conservation laws or equations-of-motion converge to the solution of the continuous equations. In the case of code verification where the continuous solution of a test problem is known, common practice is to obtain several discrete solutions from successively refined meshes or grids; calculate norms of the solution error; and verify the rate with which discrete solutions converge to the continuous solution. With solution verification, where the continuous solution is unknown, common practice is to obtain several discrete solutions from successively refined meshes or grids; extrapolate an estimate of the continuous solution; verify the rate-of-convergence; and estimate\u00a0\u2026", "num_citations": "12\n", "authors": ["1394"]}
{"title": "Non-linear models of composite laminates\n", "abstract": " This work discusses efforts to develop computational models of composite laminates for damage prognosis. Damage prognosis refers to the Los Alamos National Laboratory project that studies projectile impact induced damage on composite plates. Damage prognosis combines advanced modeling capabilities, large-scale computing, active sensing technologies, and newly developed data interrogation algorithms to identify the occurrence of structural damage, classify its type, and predict its evolution. Because any prognosis must rely on numerical predictions of damage evolution, advanced composite models have been developed and validated. This work describes an approach for physics-based modeling impact damage in a laminate composite plate. The plate structural model uses a finite element formulation with a cohesive zone model (CZM) to simulate the possible fracture surfaces characteristic of impact damage.", "num_citations": "12\n", "authors": ["1394"]}
{"title": "Estimating the error in simulation prediction over the design space\n", "abstract": " This study addresses the assessment of accuracy of simulation predictions. A procedure is developed to validate a simple non-linear model defined to capture the hardening behavior of a foam material subjected to a short-duration transient impact. Validation means that the predictive accuracy of the model must be established, not just in the vicinity of a single testing condition, but for all settings or configurations of the system. The notion of validation domain is introduced to designate the design region where the model\u2019s predictive accuracy is appropriate for the application of interest. Techniques brought to bear to assess the model\u2019s predictive accuracy include test-analysis correlation, calibration, bootstrapping and sampling for uncertainty propagation and metamodeling. The model\u2019s predictive accuracy is established by training a metamodel of prediction error. The prediction error is not assumed to be systematic\u00a0\u2026", "num_citations": "12\n", "authors": ["1394"]}
{"title": "Variable screening in metamodel design for a large structural dynamics simulation\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "12\n", "authors": ["1394"]}
{"title": "Defining coverage of an operational domain using a modified nearest-neighbor metric\n", "abstract": " Validation experiments are conducted at discrete settings within an operational domain to assess the predictive maturity of a model that is ultimately used to predict over the entire operational domain. Unless this domain is sufficiently explored with validation experiments, satisfactory model performance at these discrete, tested settings would be insufficient to ensure satisfactory model performance throughout the entire operational domain. The goal of coverage metrics is then to reveal how well a set of validation experiments represents an operational domain. The authors identify the criteria of an exemplary coverage metric, evaluate the ability of existing coverage metrics to fulfill these criteria, and propose a new, improved coverage metric. The proposed metric favors interpolation over extrapolation through a penalty function, causing the metric to prefer a design of validation experiments near the boundaries of the\u00a0\u2026", "num_citations": "11\n", "authors": ["1394"]}
{"title": "Calibration under uncertainty for finite element models of masonry monuments\n", "abstract": " Historical unreinforced masonry buildings often include features such as load bearing unreinforced masonry vaults and their supporting framework of piers, fill, buttresses, and walls. The masonry vaults of such buildings are among the most vulnerable structural components and certainly among the most challenging to analyze. The versatility of finite element (FE) analyses in incorporating various constitutive laws, as well as practically all geometric configurations, has resulted in the widespread use of the FE method for the analysis of complex unreinforced masonry structures over the last three decades. However, an FE model is only as accurate as its input parameters, and there are two fundamental challenges while defining FE model input parameters:(1) material properties and (2) support conditions. The difficulties in defining these two aspects of the FE model arise from the lack of knowledge in the common engineering understanding of masonry behavior. As a result, engineers are unable to define these FE model input parameters with certainty, and, inevitably, uncertainties are introduced to the FE model.", "num_citations": "11\n", "authors": ["1394"]}
{"title": "A combined model reduction/SVD approach to nonlinear model updating\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "11\n", "authors": ["1394"]}
{"title": "Damage detection and prediction for composite plates\n", "abstract": " DSpace at KOASAS: Damage detection and prediction for composite plates KOASAS menu About KOASAS KAIST Library \uac80\uc0c9 Advanced Search Browse Communities & Collections Researchers at KAIST Titles Subject By Date rss_1.rss_2.atom_1.sherpa SEARCH DSpace at KOASAS College of Engineering(\uacf5\uacfc\ub300\ud559)Dept. of Civil and Environmental Engineering(\uac74\uc124 \ubc0f\ud658\uacbd\uacf5\ud559\uacfc)CE-Conference Papers(\ud559\uc220\ud68c\uc758\ub17c\ubb38) Damage detection and prediction for composite plates Cited 0 time in webofscience Cited 0 time in scopus Hit : 306 Download : 0 Export DC(XML) Excel Farrar, Charles R. / Wait, Jeannette R. / Tippetts, Trevor B. / Hemez, Francois, M. / Park, Gyuhae / Sohn, Hoonresearcher Publisher Materials Damage Prognosis, Materials Science &amp; Technology Issue Date 2004-09-26 Language English Citation Proceedings of Symposium on Materials Damage Prognosis, Materials Science &amp; \u2026", "num_citations": "10\n", "authors": ["1394"]}
{"title": "A brief history of 30 years of model updating in structural dynamics\n", "abstract": " Since the development of the Finite Element (FE) method at the University of California Berkeley and the Boeing Company in the 1960s, the question of appropriateness of a model has always preoccupied developers and practicing engineers. Because of the early focus on predicting the linear vibrations of coupled systems for aerospace and civil engineering applications, test-analysis reconciliation initially consisted in updating the FE matrices such that their eigen-properties reproduce the identified resonant frequencies and mode shape vectors. As the FE method increased in sophistication in the following decades, and computational resources became widespread, test-analysis reconciliation evolved beyond optimal matrix updating to include sensitivity and residual-based methods that attempted to calibrate individual element matrices or design parameters. Fueled by an ever-increasing diversity of\u00a0\u2026", "num_citations": "9\n", "authors": ["1394"]}
{"title": "Defining a practical QMU metric\n", "abstract": " Engineers and physicists working in the Nuclear Weapons Complex have struggled with the definition of a tool to aid decision makers in their task of weapon certification. Recently, there has been much discussion of \u201cQMU,\u201d or Quantification of Margins under Uncertainty. While there is general agreement between physics and engineering analysts across the complex that QMU is an appropriate number to indicate margin and uncertainty in a calculation or experiment, there have been no guidelines on how it should be derived as noted by several prominent review panels. 1, 2, 3 The approach to defining QMU in this paper has been to combine the ideas put forth by others along with our own to come up with a QMU definition that is hopefully useful to both physicists and engineers and easily understood by decision-makers. One major contribution of this work is to address \u201cconfidence\u201d as a part of the QMU calculation\u00a0\u2026", "num_citations": "9\n", "authors": ["1394"]}
{"title": "Predictive maturity of computer models using functional and multivariate output\n", "abstract": " Computer simulations are valued in science and engineering because they enable us to gain knowledge about phenomena that would otherwise be difficult to understand. Dependency on simulations primarily stems from our inability to conduct a sufficient number of experiments within the desired settings or with sufficient detail. However, if one were able to conduct a large-enough number of experiments, it is reasonable to envision that a simulation model could be calibrated to the point that its predictive uncertainty is reduced down to uncontrolled, natural variability. We inductively conclude that, as new experimental information is used for calibration, the calibrated parameters should stabilize, and thus, the disagreement between simulations and experiments should be reduced down to\" true\" bias. We propose to use the stabilization of the incremental improvement to assess the predictive maturity of a model. Accordingly, we develop a Prediction Convergence Index (PCI) that approximates the convergence of predictions to their\" true\" or stabilized values or, conversely, can be used to estimate the number of experimental tests that would be required to reach stabilization of predictions. Once the predictive maturity of a model has been assessed, we argue that it is acceptable to extrapolate its predictions away from settings or regimes where validation tests have been conducted as long as the physics involved and modeled by the code remains unchanged. The application of the PCI is illustrated using a Preston-Tonks-Wallace material model for Tantalum and six experimental data sets in the form of strain and stress cures. For the given model, the\u00a0\u2026", "num_citations": "9\n", "authors": ["1394"]}
{"title": "Developing impact and fatigue damage prognosis solutions for composites\n", "abstract": " An approach to developing a damage prognosis solution that integrates advanced sensing technology, data interrogation procedures for damage detection, novel model validation and uncertainty quantification techniques, and reliability-based decision-making algorithms is summarized in this article. In parallel, experimental efforts are underway to deliver a proof-of-principle technology demonstration by assessing impact damage and predicting the subsequent fatigue damage accumulation in a composite plate. This article provides an overview of the various technologies that are being integrated to address this damage prognosis problem.", "num_citations": "9\n", "authors": ["1394"]}
{"title": "Feature extraction for structural dynamics model validation\n", "abstract": " This study focuses on defining and comparing response features that can be used for structural dynamics model validation studies. Features extracted from dynamic responses obtained analytically or experimentally, such as basic signal statistics, frequency spectra, and estimated timeseries models, can be used to compare characteristics of structural system dynamics. By comparing those response features extracted from experimental data and numerical outputs, validation and uncertainty quantification of numerical model containing uncertain parameters can be realized. In this study, the applicability of some response features to model validation is first discussed using measured data from a simple test-bed structure and the associated numerical simulations of these experiments. Issues that must be considered were sensitivity, dimensionality, type of response, and presence or absence of measurement\u00a0\u2026", "num_citations": "8\n", "authors": ["1394"]}
{"title": "Information-Gap Robustness for the Test-Analysis Correlation of a Nonlinear Transient Simulation\n", "abstract": " An alternative to the theory of probability is applied to the problem of assessing the robustness of test-analysis correlation to parametric sources of uncertainty. The analysis technique is based on the theory of information-gap, which models the clustering of uncertain events in families of nested sets instead of assuming a probability structure. The system investigated is the propagation of a transient impact through a layer of hyper-elastic material. The two sources of non-linearity are the softening of the constitutive law implemented to model the hyper-elastic material and contact dynamics at the interface between metallic and crushable materials. The robustness of test-analysis correlation to sources of parametric variability is first studied to identify the parameters of the model that significantly influence the agreement between measurements and predictions. Calibration under non-probabilistic uncertainty is then\u00a0\u2026", "num_citations": "8\n", "authors": ["1394"]}
{"title": "Validation of nonlinear modeling from impact test data using probability integration\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "8\n", "authors": ["1394"]}
{"title": "Can model updating tell the truth?\n", "abstract": " This paper discusses to which extent updating methods may be able to correct finite element models in such a way that the test structure is better simulated. After having unified some of the most popular modal residues used as the basis for optimization algorithms, the relationship between modal residues and model correlation is investigated. This theoretical approach leads to an error estimator that may be implemented to provide an a priori upper bound of a models predictive quality relative to test data. These estimates however assume that a full measurement set is available. Finally, an application example is presented that illustrates the effectiveness of the estimator proposed when less measurement points than degrees of freedom are available.", "num_citations": "8\n", "authors": ["1394"]}
{"title": "Comparing mode shape expansion methods for test-analysis correlation\n", "abstract": " Comparing Mode Shape Expansion Methods for Test-analysis Correlation - NASA/ADS Now on home page ads icon ads Enable full ADS view NASA/ADS Comparing Mode Shape Expansion Methods for Test-analysis Correlation Hemez, FM ; Farhat, C. Abstract Publication: Proceedings of the 12th International Modal Analysis Pub Date: March 1994 Bibcode: 1994SPIE.H No Sources Found \u00a9 The SAO/NASA Astrophysics Data System adshelp[at]cfa.harvard.edu The ADS is operated by the Smithsonian Astrophysical Observatory under NASA Cooperative Agreement NNX16AC86A NASA logo Smithsonian logo Resources About ADS ADS Help What's New Careers@ADS Social @adsabs ADS Blog Project Switch to full ADS Is ADS down? (or is it just me...) Smithsonian Institution Smithsonian Privacy Notice Smithsonian Terms of Use Smithsonian Astrophysical Observatory NASA \u2026", "num_citations": "8\n", "authors": ["1394"]}
{"title": "Robustness, fidelity and prediction-looseness of models\n", "abstract": " Assessment of the credibility of a mathematical or numerical model of an engineering system must combine three components: (1) The fidelity of the model to test data. (2) The robustness, of model fidelity, to lack of understanding of the underlying processes. (3) The prediction looseness of the model. \u2018Prediction looseness\u2019 is the range of predictions of models which are equivalent in terms of fidelity. The main result of this paper is that high fidelity, high robustness, and small prediction looseness are mutually incompatible. A model with high fidelity to data and high robustness to imperfect understanding of the process, will have low predictive focus. Our analysis is based on info-gap models of uncertainty.", "num_citations": "7\n", "authors": ["1394"]}
{"title": "Validation of transient structural dynamics simulations: an example\n", "abstract": " The field of computational structural dynamics is on the threshold of revolutionary change. The ever-increasing costs of physical experiments coupled with advances in massively parallel computer architecture are steering the engineering analyst to be more and more reliant on numerical calculations with little to no data available for experimental confirmation.New areas of research in engineering analysis have come about as a result of the changing roles of computations and experiments. Whereas in the past the primary function of physical experiments has been to confirm or \u201cprove\u201d the accuracy of a computational simulation, the new environment of engineering is forcing engineers to allocate precious experimental resources differently. Rather than trying to \u201cprove\u201d whether a calculation is correct, the focus is on learning how to use experimental data to \u201cimprove\u201d the accuracy of computational simulations. This process of improving the accuracy of calculations through the use of experimental data is termed \u201cmodel validation.\u201d", "num_citations": "7\n", "authors": ["1394"]}
{"title": "Critical assessment of MIMO structural identification methods: ARX and ARMAX schemes\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "7\n", "authors": ["1394"]}
{"title": "On The efficiency of model updating via genetic algorithm for structural damage detection\n", "abstract": " Finite element updating techniques are used to refine mathematical representations of structures by comparing their predicted dynamics to data issued from modal tests. During the correlation procedure, inverse correction systems that are often ill-conditioned must be solved using, usually, iterative improvement algorithms. This paper discusses the efficiency of treating the updating problem with a genetic solver rather than a gradientbased method. The basic principles governing the genetic model updating algorithm are enounced, its numerical implementation is discussed and the solver is applied to the problem", "num_citations": "7\n", "authors": ["1394"]}
{"title": "Practical Guide to High Accuracy Identification of Structural Damage in Complex Structures\n", "abstract": " The efficiency of system idcntilication and model updating procedures is often limited by practical constraints and difticulties. Rather than developing new methodologies, this paper attempts to identify key practical issues and to demonstrate how they may influence the identilication of stmctural damage in a large flexible truss. Modally complex structures necessitate overdetermined rcalizations for capturing the important dynamics of the system which results into identitication prohlcms such as modal splitting and pole and zero placement difficulties. A time and frequency domain procedure is summarized that addresses these issues. Flexihility indicators are proposed for health monitoring purposes and, finally. key issues of the model updating problem are discussed. Experimental data arc used to illustrate selected prohlcms introduced by large order state space models and illconditioning of the correction system. The dctection of a damaged longeron in a large truss stmcturc is shown to k sensitive to these practical problems and solutions are briefly prcsentcd for enhancing the model updating. t Formerly, Post-doctoral Research Associate at the Center for Aerospace Structures, University of Colorado at Boulder. Boulder, CO 803094429. USA Currently, Assistant Professor, MSSMAT, Ecole Centrale Paris, France. Member AIAA, Member SEM. hemez~ houlder. Colr) rado. EDU (Internet USA) hemez@ mss. ecp. fr (Intemct France)", "num_citations": "7\n", "authors": ["1394"]}
{"title": "Model choice considerations and information integration using analytical hierarchy process\n", "abstract": " Using the theory of information-gap for decision-making under severe uncertainty, it has been shown that there exist irrevocable trade-offs between fidelity-to-data, robustness-to-uncertainty and confidence-in-prediction. The paper describes the assessment and trade-offs of these three components in a data-sparse application. To augment the data and corresponding modeling, a similar application with data and models is considered. A method of information integration is illustrated. Saaty's Analytic Hierarchy Process (AHP) is used to determine weights for two models and two experimental data sets, by forming all possible pair-wise comparisons between model output and experimental data.", "num_citations": "6\n", "authors": ["1394"]}
{"title": "An Uncertainty Inventory Demonstration: A Primary Step in Uncertainty Quantification\n", "abstract": " Tools, methods, and theories for assessing and quantifying uncertainties vary by application. Uncertainty quantification tasks have unique desiderata and circumstances. To realistically assess uncertainty requires the engineer/scientist to specify mathematical models, the physical phenomena of interest, and the theory or framework for assessments. For example, Probabilistic Risk Assessment (PRA) specifically identifies uncertainties using probability theory, and therefore, PRA\u2019s lack formal procedures for quantifying uncertainties that are not probabilistic. The Phenomena Identification and Ranking Technique (PIRT) proceeds by ranking phenomena using scoring criteria that results in linguistic descriptors, such as importance ranked with words,\u201cHigh/Medium/Low.\u201d The use of words allows PIRT to be flexible, but the analysis may then be difficult to combine with other uncertainty theories. We propose that a\u00a0\u2026", "num_citations": "6\n", "authors": ["1394"]}
{"title": "Latin hypercube sampling vs. metamodel Monte Carlo for propagating uncertainty through transient dynamics simulations\n", "abstract": " Two approaches are compared to propagate uncertainty through the finite element simulation of transient dynamic tests. The first approach is to generate a Latin hypercube sample of 55 computer runs, perform the numerical analyses, and estimate the probability density functions (PDFs) directly from the finite element results. The second approach is to perform a design of computer experiments based on the same number of runs, develop polynomial meta-models to fit the 55 predictions for each response feature as a function of the 11 inputs, and propagate Monte Carlo samples through the meta-models, from which the PDFs can be estimated.", "num_citations": "6\n", "authors": ["1394"]}
{"title": "Use of response surface metamodels for damage identification of a simple nonlinear system\n", "abstract": " Metamodels have been used with success in many areas of engineering for decades but only recently has their use in the field of structural dynamics started to be explored. This research applies the approach of metamodeling to the damage identification problem in structural dynamics. In this case, a metamodel is a reduced order model, constructed by empirically fitting a model to a set of points in the design space. Points in the design space may be chosen using a design of experiments approach. Then a polynomial model is fit to the points using a multiple regression fitting scheme. This empirical fit allows models to be constructed that relate damage parameters of interest (such as stiffness reduction and its location) to measurable output features (eg natural frequencies). The method has been demonstrated and its success quantified for simple linear and nonlinear simulations.", "num_citations": "6\n", "authors": ["1394"]}
{"title": "Validation of the transient structural response of a threaded assembly\n", "abstract": " This paper will demonstrate the application of model validation techniques to a transient structural dynamics problem. The problem of interest is the propagation of an explosive shock through a complex threaded joint that is a surrogate model of a system assembly. The objective is to validate the computational modeling of the key mechanical phenomena in the assembly, so that the component can be represented with adequate fidelity in the system-level model. A set of experiments was conducted on the threaded assembly where the acceleration and strain responses to an explosive load were measured on mass-simulators representing payloads. A significantly detailed computational model of the threaded assembly was also created. Numerical features that represent the important characteristics of the response were defined and calculated for both the experimental and computational data. Each step of the model\u00a0\u2026", "num_citations": "6\n", "authors": ["1394"]}
{"title": "Advanced tools for updating damped finite element models using static, modal and flexibility data\n", "abstract": " This publication addresses the problem of test-analysis correlation of damped structural systems. A general sensitivity-based framework is presented where identified modal parameters, receptance, and static data are used for updating finite element models. In the case where model updating is based on measured frequency response functions, a two-step procedure is proposed that provides closed form expressions of the inverse of the objective function's gradient and Hessian matrices. An application example illustrates performances of the methodology during the updating of large-dimensional models for damped and modally complex structures. (Author)", "num_citations": "6\n", "authors": ["1394"]}
{"title": "Robust decision making applied to the NASA multidisciplinary uncertainty quantification challenge problem\n", "abstract": " This paper addresses the NASA Langley Research Center\u2019s Multidisciplinary Uncertainty Quantification Challenge problem, which is intended to pose challenges to the uncertainty quantification and robust design communities. The goals of the Multidisciplinary Uncertainty Quantification Challenge problem can be formulated into four main topics that are commonly encountered in the model development process: calibration, sensitivity analysis, extreme-case analysis, and robust design. The analysis presented herein places a particular emphasis on the use of info-gap decision theory to address the goals of the Multidisciplinary Uncertainty Quantification Challenge problem. Info-gap decision theory provides a convenient framework to quantify the effect of uncertainty, herein referred to as robustness, when using simulation models for decision making. Robustness, as defined in the context of info-gap decision theory\u00a0\u2026", "num_citations": "5\n", "authors": ["1394"]}
{"title": "Prediction with quantified uncertainty of temperature and rate dependent material behavior\n", "abstract": " The increasing reliance on computer simulations in decision-making motivates the need to formulate a commonly accepted definition for \u201cpredictive maturity.\u201d The concept of predictive maturity involves quantitative metrics that should be capable of tracking progress (or lack thereof) as additional knowledge becomes available and is integrated into the simulations. Two examples are the addition of new experimental datasets for model calibration and implementation of better physics models in the codes. This publication discusses the attributes that a metric of predictive maturity should exhibit by emphasizing that they must go beyond the goodness-of-fit of the model to the available test data. Predictive maturity must also consider the degree to which physical experiments cover the domain of applicability. We propose a Predictive Maturity Index (PMI). Physical datasets are used to illustrate how the PMI quantifies the\u00a0\u2026", "num_citations": "5\n", "authors": ["1394"]}
{"title": "Validation of the transient structural response of a threaded assembly: Phase I\n", "abstract": " This report explores the application of model validation techniques in structural dynamics. The problem of interest is the propagation of an explosive-driven mechanical shock through a complex threaded joint. The study serves the purpose of assessing whether validating a large-size computational model is feasible, which unit experiments are required, and where the main sources of uncertainty reside. The results documented here are preliminary, and the analyses are exploratory in nature. The results obtained to date reveal several deficiencies of the analysis, to be rectified in future work.", "num_citations": "5\n", "authors": ["1394"]}
{"title": "Bayesian calibration of simulation models using experimental data\n", "abstract": " Uncertainties in tomographic reconstructions based on Bayesian geometric models Page 1 August 7, 2002 LANL Radiographic S&A Workshop 1 Bayesian calibration of simulation models using experimental data Ken Hanson* and Fran\u00e7ois Hemez\u2020 *CCS-2, Methods for Advanced Scientific Simulations \u2020ESA-WR, Engineering - Weapons Response Los Alamos National Laboratory This presentation available at http://www.lanl.gov/home/kmh/ LA-UR-02-5292 Page 2 August 7, 2002 LANL Radiographic S&A Workshop 2 Overview \u2022 Bayesian calibration \u25ba more than parameter estimation \u25ba uncertainty quantification (UQ) is central issue \u25ba each new experiment used to improve knowledge of models \u2022 Physics simulations codes \u25ba need to be understood on basis of experimental data \u2022 Analysis process \u25ba employ hierarchy of experiments, from basic to fully integrated \u25ba goal is to learn as much possible from all \u2026", "num_citations": "5\n", "authors": ["1394"]}
{"title": "Developing damage prognosis solution\n", "abstract": " DSpace at KOASAS: Developing damage prognosis solution KOASAS menu About KOASAS KAIST Library \uac80\uc0c9 Advanced Search Browse Communities & Collections Researchers at KAIST Titles Subject By Date rss_1.rss_2.atom_1.sherpa SEARCH DSpace at KOASAS College of Engineering(\uacf5\uacfc\ub300\ud559)Dept. of Civil and Environmental Engineering(\uac74\uc124\ubc0f\ud658\uacbd\uacf5\ud559\uacfc )CE-Conference Papers(\ud559\uc220\ud68c\uc758\ub17c\ubb38) Developing damage prognosis solution Cited 0 time in webofscience Cited 0 time in scopus Hit : 213 Download : 0 Export DC(XML) Excel Farrar, Charles R. / Hemez, Francois M. / Sohn, Hoonresearcher Publisher Structural Health Monitoring Issue Date 2002-07-10 Language English Citation The 1st European Workshop on Structural Health Monitoring URI http://hdl.handle.net/10203/138381 Appears in Collection CE-Conference Papers(\ud559\uc220\ud68c\uc758\ub17c\ubb38) Files in This Item There are no files associated with this item. \u2026", "num_citations": "5\n", "authors": ["1394"]}
{"title": "Statics and inverse dynamics solvers based on strain-mode disassembly\n", "abstract": " The finite element method is widely used in design engineering for modeling and analyzing structural systems. Two approaches have been developed: the force-based method that exploits the equilibrium of forces and moments at nodal joints of the mesh to formulate the assembly of element-level matrices into master mass and stiffness matrices and its dual counterpart, the flexibility-based method. An alternative formulation of stiffness-based finite element assembly is proposed that decomposes element-level matrices even further into strain mode contributions. This decomposition (referred to as finite element disassembly here) allows the derivation of an efficient numerical solver. It is shown that a single matrix factorization is required for analyzing all models characterized by the same topology. This makes finite element disassembly and the associated inverse solver ideal in cases where multiple design analyzes\u00a0\u2026", "num_citations": "5\n", "authors": ["1394"]}
{"title": "Dynamic mode shape expansion using mass orthogonality constraints\n", "abstract": " Pascal 001 Exact sciences and technology/001B Physics/001B40 Fundamental areas of phenomenology (including applications)/001B40F Solid mechanics/001B40F30 Structural and continuum mechanics/001B40F30M Vibration, mechanical wave, dynamic stability (aeroelasticity, vibration control...)", "num_citations": "5\n", "authors": ["1394"]}
{"title": "Defining coverage of a domain using a modified nearest-neighbor metric\n", "abstract": " Validation experiments are conducted at discrete settings within the domain of interest to assess the predictive maturity of a model over the entire domain. Satisfactory model performance merely at these discrete tested settings is insufficient to ensure that the model will perform well throughout the domain, particularly at settings far from validation experiments. The goal of coverage metrics is to reveal how well a set of validation experiments represents the entire operational domain. The authors identify the criteria of an exemplary coverage metric, evaluate the ability of existing coverage metrics to fulfill each criterion, and propose a new, improved coverage metric. The proposed metric favors interpolation over extrapolation through a penalty function, causing the metric to prefer a design of validation experiments near the boundaries of the domain, while simultaneously exploring inside the domain. Furthermore\u00a0\u2026", "num_citations": "4\n", "authors": ["1394"]}
{"title": "Numerical uncertainty in computational engineering and physics\n", "abstract": " Obtaining a solution that approximates ordinary or partial differential equations on a computational mesh or grid does not necessarily mean that the solution is accurate or even'correct'. Unfortunately assessing the quality of discrete solutions by questioning the role played by spatial and temporal discretizations generally comes as a distant third to test-analysis comparison and model calibration. This publication is contributed to raise awareness of the fact that discrete solutions introduce numerical uncertainty. This uncertainty may, in some cases, overwhelm in complexity and magnitude other sources of uncertainty that include experimental variability, parametric uncertainty and modeling assumptions. The concepts of consistency, convergence and truncation error are overviewed to explain the articulation between the exact solution of continuous equations, the solution of modified equations and discrete solutions computed by a code. The current state-of-the-practice of code and solution verification activities is discussed. An example in the discipline of hydro-dynamics illustrates the significant effect that meshing can have on the quality of code predictions. A simple method is proposed to derive bounds of solution uncertainty in cases where the exact solution of the continuous equations, or its modified equations, is unknown. It is argued that numerical uncertainty originating from mesh discretization should more\u00bb", "num_citations": "4\n", "authors": ["1394"]}
{"title": "Variance-based Metric DN for Baselining, Verification, and Validation\n", "abstract": " The Dn metric is a statistic (like a chi-squared), in the true probabilistic definition. Its squared-difference form permits the comparison of two variables, or one variable to fixed values\u2014ideal for determining baselining, verification, and validation. Its realization, to date, has two different forms, each following a gamma distribution, providing standardization within probability theory. The metrics incorporate different kinds of variances (representing uncertainties) involved with the different observables. The first parameter of the gamma distribution for the Dn metrics reflects the effect of sample size (the number of comparisons), and the second parameter contains the variance terms. Included are variance from measurement precision (or\" designer expert judgment\" or\" hallway value\"), variance from calculations, variance from data, and variance of prior knowledge (before new data and/or calculation). Other advantages of the\u00a0\u2026", "num_citations": "4\n", "authors": ["1394"]}
{"title": "An introduction to the Los Alamos damage prognosis project\n", "abstract": " DSpace at KOASAS: An introduction to the los alamos damage prognosis project KOASAS menu About KOASAS KAIST Library \uac80\uc0c9 Advanced Search Browse Communities & Collections Researchers at KAIST Titles Subject By Date rss_1.rss_2.atom_1.sherpa SEARCH DSpace at KOASAS College of Engineering(\uacf5\uacfc\ub300\ud559)Dept. of Civil and Environmental Engineering(\uac74\uc124 \ubc0f\ud658\uacbd\uacf5\ud559\uacfc)CE-Conference Papers(\ud559\uc220\ud68c\uc758\ub17c\ubb38) An introduction to the los alamos damage prognosis project Cited 0 time in webofscience Cited 0 time in scopus Hit : 347 Download : 0 Export DC(XML) Excel Charles R. Farrar / Francois Hemez / Park Gyuhae / Amy N. Robertson / Sohn, Hoonresearcher / Todd. O. Williams Publisher Advanced Smart Materials and Smart Structures Technology Issue Date 2004-01-12 Language English Citation An International Workshop on Advanced Smart Materials and Smart Structures Technology URI http://\u2026", "num_citations": "4\n", "authors": ["1394"]}
{"title": "An engineering perspective on uq for validation, reliability and certification\n", "abstract": " The analysts in the Engineering Sciences & Applications (ESA) Division at Los Alamos have been developing and applying uncertainty quantification (UQ) tools and methods to model verification and validation (V&V) and engineering component/system certification for several years. We begin with dissecting the term UQ (uncertainty quantification), and demonstrating the roles of model V&V and engineering qualification in overall weapon system certification and reliability through various methodologies that have been developed.Three examples of UQ applications are presented, showing various technical challenges and solutions in handling uncertainties. The first is a fundamental model-test comparison for a single component and a single output. However, several different models calculations are possible, including a crude \u201cback of the envelope\u201d estimate and three test points from a previous experiment. These\u00a0\u2026", "num_citations": "4\n", "authors": ["1394"]}
{"title": "Developing information-Gap Models of Uncertainty for Test-Analysis Correlation\n", "abstract": " Relying on numerical simulations, as opposed to field measurements, to analyze the structural response of complex systems requires that the predictive accuracy of the models be assessed. This activity is generally known as \u201cmodel validation\u201d. Model validation requires the comparison of model predictions with test measurements at several points of the design/operational space. For example, numerical models of flutter must be validated for various combinations of fluid velocity and wing angle-of-attack. Because validation experiments become expensive when the system investigated is complex, only a few data sets are generally available. This lack of adequate representation of the design/operational space makes it questionable whether statistical models of predictive accuracy can be developed.In this work, we focus on one aspect of model validation that consists in assessing the robustness of a decision to uncertainty. In this context,\u201cdecision\u201d refers to assessing the accuracy of predictions and verifying that the accuracy is adequate for the purpose intended. Likewise,\u201cuncertainty\u201d can represent experimental variability, variability of the model\u2019s parameters but also inappropriate modeling rules in regions of the design/operational space where experiments are not available.", "num_citations": "4\n", "authors": ["1394"]}
{"title": "Closing the Gap Between Modal Parameter Based and Frequency Response Function Based Updating Methods\n", "abstract": " Closing the Gap Between Modal Parameter Based and Frequency Response Function Based Updating Methods - NASA/ADS Now on home page ads icon ads Enable full ADS view NASA/ADS Closing the Gap Between Modal Parameter Based and Frequency Response Function Based Updating Methods Hemez, FM Abstract Publication: Proceedings of the 13th International Modal Analysis Conference Pub Date: 1995 Bibcode: 1995SPIE...171H No Sources Found \u00a9 The SAO/NASA Astrophysics Data System adshelp[at]cfa.harvard.edu The ADS is operated by the Smithsonian Astrophysical Observatory under NASA Cooperative Agreement NNX16AC86A NASA logo Smithsonian logo Resources About ADS ADS Help What's New Careers@ADS Social @adsabs ADS Blog Project Switch to full ADS Is ADS down? (or is it just me...) Smithsonian Institution Smithsonian Privacy Notice Smithsonian Terms of Use \u2026", "num_citations": "4\n", "authors": ["1394"]}
{"title": "Improving the convergence rate of a transient substructuring iterative method using the rigid body modes of its static equivalent\n", "abstract": " We present a new efficient and scalable domain decomposition method for solving implicitly linear and nonlinear time-dependent problems in computational mechanics. The method is derived by adding a rigid body modes based coarse problem to the recently proposed transient FETI substructuring algorithm in order to propagate the error globally and accelerate convergence. It is proved that in the limit for large time steps, the new method converges toward the FETI algorithm for time-independent problems. Computational results confirm that the optimal convergence properties of the timeindependent FETI method are preserved in the timedependent case. We employ an iterative scheme for solving efficiently the coarse problem on massively parallel processors, and demonstrate the effective scalability of the new transient FETI method with the large-scale fi-nite element dynamic analysis on the Paragon XP/S and\u00a0\u2026", "num_citations": "4\n", "authors": ["1394"]}
{"title": "Info-Gap (IG): Robust Design of a Mechanical Latch\n", "abstract": " \u2022 Info-Gap (IG) Decision Theory, introduced in Chap. 5, is used to formulate, and solve, the analysis of robustness in the early design of a mechanical latch.\u2022 The three components necessary to assess robustness of the latch design (a system model, performance requirement, and representation of uncertainty) are discussed.\u2022 The robustness analysis indicates that the nominal design can accommodate significant uncertainty before failing to deliver the required performance.\u2022 The discussion concludes with the assessment of a variant design to show how a decision (\u201cwhich design should be chosen?\u201d) can be confidently reached despite the presence of significant gaps in knowledge.", "num_citations": "3\n", "authors": ["1394"]}
{"title": "Quantification of prediction bounds caused by model form uncertainty\n", "abstract": " Numerical simulations, irrespective of the discipline or application, are often plagued by arbitrary numerical and modeling choices. Arbitrary choices can originate from kinematic assumptions, for example the use of 1D beam, 2D shell, or 3D continuum elements, mesh discretization choices, boundary condition models, and the representation of contact and friction in the simulation. This works takes a step toward understanding the effect of arbitrary choices and model-form assumptions on the accuracy of numerical predictions. The application is the simulation of the first four resonant frequencies of a one-story aluminum portal frame structure under free-free boundary conditions. The main challenge of the portal frame structure resides in modeling joint connections, for which different modeling assumptions are available. To study this model-form uncertainty, and compare it to other types of uncertainty, two finite\u00a0\u2026", "num_citations": "3\n", "authors": ["1394"]}
{"title": "Simulating the dynamics of the CX-100 wind turbine blade: model selection using a robustness criterion\n", "abstract": " Several plausible modeling strategies are available to develop finite element (FE) models of ever-increasingly complex phenomena. Expert judgment is typically used to choose which strategy to employ, while the \u201cbest\u201d modeling approach remains unknown. This paper proposes a decision analysis methodology that offers a systematic and rigorous methodology for comparing plausible modeling strategies. The proposed methodology departs from the conventional approach that considers only test-analysis correlation to select the model that provides the highest degree of fidelity-to-data. The novelty of the methodology lies in an exploration of the trade-offs between robustness to uncertainty and fidelity-to-data. Exploring robustness to model imprecision and inexactness, in addition to fidelity-to-data, lends credibility to the simulation by guaranteeing that its predictions can be trusted even if some of the\u00a0\u2026", "num_citations": "3\n", "authors": ["1394"]}
{"title": "Evolving Desiderata for Validating Engineered-Physics Systems without Full-Scale Testing\n", "abstract": " Theory and principles of engineered-physics designs do not change over time, but the actual engineered product does evolve. Engineered components are prescient to the physics and change with time. Parts are never produced exactly as designed, assembled as designed, or remain unperturbed over time. For this reason, validation of performance may be regarded as evolving over time. Desired use of products evolves with time. These pragmatic realities require flexibility, understanding, and robustness-to-ignorance. Validation without full-scale testing involves engineering, small-scale experiments, physics theory and full-scale computersimulation validation. We have previously published an approach to validation without full-scale testing using information integration, small-scale tests, theory and full-scale simulations [Langenbrunner et al. 2008]. This approach adds value, but also adds complexity and\u00a0\u2026", "num_citations": "3\n", "authors": ["1394"]}
{"title": "Finite element model correlation of a composite UAV wing using modal frequencies\n", "abstract": " The current work details the implementation of a meta-model based correlation technique on a composite UAV wing test piece and associated finite element (FE) model. This method involves training polynomial models to emulate the FE input-output behavior and then using numerical optimization to produce a set of correlated parameters which can be returned to the FE model. After discussions about the practical implementation, the technique is validated on a composite plate structure and then applied to the UAV wing structure, where it is furthermore compared to a more traditional Newton-Raphson technique which iteratively uses first-order Taylor-series sensitivity. The experimental testpiece wing comprises two graphite/epoxy prepreg and Nomex honeycomb co-cured skins and two prepreg spars bonded together in a secondary process. MSC.Nastran FE models of the four structural components are\u00a0\u2026", "num_citations": "3\n", "authors": ["1394"]}
{"title": "Successes and failures of verifying the convergence of discrete solutions\n", "abstract": " The main goal of solution verification is to assess the convergence of numerical predictions as a function of discretization variables such as element size \u0394x or time step \u0394t. The challenge is to verify that the approximate solutions of the discretized lawsof-conservation or equations-of-motion converge to the solution of the continuous equations. In the case of code verification where the continuous solution of a test problem is known, common practice is to obtain several discrete solutions from successively refined meshes or grids; calculate norms of the solution error; and verify the rate with which discrete solutions converge to the continuous solution. In the case of solution verification where the continuous solution is unknown, common practice is to obtain several discrete solutions from successively refined meshes or grids; estimate an extrapolation of the continuous solution; verify the rate-of-convergence; and estimate numerical uncertainty bounds. The formalism proposed to verify the convergence of discrete solutions derives from postulating how truncation error behaves in the asymptotic regime of convergence. The paper offers an overview of these techniques with illustrations such as the modal analysis of plates or propagation of waves in ideal gases. It is contributed to the proceedings of IMAC-XXV in an attempt to raise awareness in the Structural Dynamics community about convergencerelated issues that ubiquitously present themselves in numerical simulations.(Publication approved for unlimited, public release, LA-UR-06-6002, Unclassified.)", "num_citations": "3\n", "authors": ["1394"]}
{"title": "Reliability methods\n", "abstract": " As stated in the introduction to this book, damage prognosis is the process of estimating a system\u2019s remaining useful life. The goal is to forecast system performance by measuring the current state of the system, estimating the future loading environments for that system, and predicting through simulation and past experience the remaining useful life of the system. Damage prognosis faces numerous sources of variability, uncertainty, and lack of knowledge. Examples are experimental variability, parametric uncertainty, unknown functional forms of the mathematical models, and extrapolated future loading and environments. The discussion of damage prognosis would therefore be incomplete without addressing the issue of decision making under uncertainty. In the presence of damage, decisions must be made about whether a system can continue to function as required for a given amount of time. For instance, if a wing of an airplane has been damaged during flight, one might need to decide whether that plane can continue its flight mission, or needs to abort. These decisions are based on an understanding of how probable it is that a system might fail during the period of time that it takes to complete the mission. Reliability analysis is the procedure used to provide an estimate of the failure probability. Reliability can be defined as the probability that a system, at a given point in time, will be able to perform a required function without failure. Through an assessment of the current reliability, as well as information about how that reliability will be changing over time, a decision of what to do in the presence of damage can be made.", "num_citations": "3\n", "authors": ["1394"]}
{"title": "Predictive Modeling Assessment of a Crushable Foam Material\n", "abstract": " In computational physics and engineering, numerical models are developed to predict the behavior of a system whose response cannot be measured experimentally. A key aspect of science-based predictive modeling is to assess the credibility of predictions. Credibility, which is demonstrated through the activities of model Verification and Validation (V&V), quantifies the extent to which simulations can be analyzed with confidence to represent the phenomenon of interest with a degree of accuracy consistent with the intended use of the model. 1Recently published work argues that assessing the credibility of a mathematical or numerical model must combine three components: 1) Demonstrating the fidelity of predictions to test data; 2) Studying the robustness of predictions to variability, uncertainty, and lack-of-knowledge; and 3) Establishing the degree of confidence in model predictions, especially in situations\u00a0\u2026", "num_citations": "3\n", "authors": ["1394"]}
{"title": "Modal Testing Repeatability of a Population of Spherical Shells\n", "abstract": " In this study, we investigated the variability in modal frequencies obtained from testing a set of hollow, almost spherical marine floats. We investigated four sources of variability: unit-to-unit variability, operator-to-operator variability, test repetition, and accelerometer placement. Because we measured the excitation and response of the marine floats, we were able to estimate impulse response and frequency response functions. Variability is assessed by measuring the deviation of each frequency response function from the mean curve for its test group.", "num_citations": "3\n", "authors": ["1394"]}
{"title": "Applying information-gap reasoning to the predictive accuracy assessment of transient dynamics simulations\n", "abstract": " An alternative to the theory of probability is applied to the problem of assessing the robustness, to parametric sources of uncertainty, of the correlation between measurements and computer simulations. The uncertainty analysis relies on the theory of information-gap, which models the clustering of uncertain events in families of nested sets instead of assuming a probability structure. The system investigated is the propagation of a transient impact through a layer of hyper-elastic material. The two sources of non-linearity are (1) the softening of the constitutive law representing the hyper-elastic material and (2) the contact dynamics at the interface between metallic and crushable materials. Information-gap models of uncertainty are developed to represent uncertainty in the knowledge of the model\u2019s parameters and in the form of the model itself. Although computationally expensive, it is demonstrated that information-gap\u00a0\u2026", "num_citations": "3\n", "authors": ["1394"]}
{"title": "Damage Identification Using a Response Surface Metamodel Approach\n", "abstract": " Metamodels have been used with success in many areas of engineering for decades but never in the field of structural dynamics. In this case, a metamodel is a reduced order model constructed by empirically fitting a model to a set of points in the design space. Points in the design space may be chosen using a design of experiments approach. Then a polynomial-type model is fit to the points using a multiple regression fitting scheme. This emperiical fit allows models to be constructed that relate damage parameters of interest (such as stiffness reduction and its location) to mearurable output features (eg natural frequencies). The method has been demonstrated and its success quantified for two structural dynamics problems, a simple five degree-of-freedom system and a cantilever beam structure. This work has been approved for unlimited public release on March 4, 2003, LA-UR 03-1502.Introduction: The concept\u00a0\u2026", "num_citations": "3\n", "authors": ["1394"]}
{"title": "Model validation and uncertainty quantification: A special session of the SD-2000 (structural dynamics 2000) forum\n", "abstract": " This session offers an open forum to discuss issues and directions of research in the areas of model updating, predictive quality of computer simulations, model validation and uncertainty quantification. Technical presentations review the state-of-the-art in nonlinear dynamics and model validation for structural dynamics. A panel discussion introduces the discussion on technology needs, future trends and challenges ahead with an emphasis placed on soliciting participation of the audience. One of the goals is to show, through invited contributions, how other scientific communities are approaching and solving difficulties similar to those encountered in structural dynamics. The session also serves the purpose of presenting the on-going organization of technical meetings sponsored by the US Department of Energy and dedicated to health monitoring, damage prognosis, model validation and uncertainty quantification in engineering applications. The session is part of the SD-2000 Forum, a forum to identify research trends, funding opportunities and to discuss the future of structural dynamics.", "num_citations": "3\n", "authors": ["1394"]}
{"title": "Achieving robust design through statistical effect screening\n", "abstract": " This work proposes a method for statistical effect screening to identify design parameters of a numerical simulation that are influential to performance while simultaneously being robust to epistemic uncertainty introduced by calibration variables. Design parameters are controlled by the analyst, but the optimal design is often uncertain, while calibration variables are introduced by modeling choices. We argue that uncertainty introduced by design parameters and calibration variables should be treated differently, despite potential interactions between the two sets. Herein, a robustness criterion is embedded in our effect screening to guarantee the influence of design parameters, irrespective of values used for calibration variables. The Morris screening method is utilized to explore the design space, while robustness to uncertainty is quantified in the context of info\u2010gap decision theory. The proposed method is applied\u00a0\u2026", "num_citations": "2\n", "authors": ["1394"]}
{"title": "Designing a mechanical latch for robust performance\n", "abstract": " Advances in computational sciences in the past three decades, such as those embodied by the finite element method, have made it possible to perform design and analysis using numerical simulations. While they offer undeniable benefits for rapid prototyping and can shorten the design-test-optimize cycle, numerical simulations also introduce assumptions and various sources of uncertainty. Examples are modeling assumptions proposed to represent a nonlinear material behavior, energy dissipation mechanisms and environmental conditions, in addition to numerical effects such as truncation error, mesh adaptation and artificial dissipation. Given these sources of uncertainty, what is the best way to support a design decision using simulations? We propose that an effective simulation-based design hinges on the ability to establish the robustness of its performance to assumptions and sources of uncertainty\u00a0\u2026", "num_citations": "2\n", "authors": ["1394"]}
{"title": "Making Structural Condition Diagnostics Robust to Environmental Variability\n", "abstract": " Advances in sensor deployment and computational modeling have allowed significant strides to be made recently in the field of Structural Health Monitoring (SHM). One widely used SHM technique is to perform a vibration analysis where a model of the structure\u2019s pristine (undamaged) condition is compared with vibration response data collected from the physical structure. Discrepancies between model predictions and monitoring data can be interpreted as structural damage. Unfortunately, multiple sources of uncertainty must also be considered in the analysis, including environmental variability and unknown values for model parameters. Not accounting for uncertainty in the analysis can lead to false-positives or false-negatives in the assessment of the structural condition. To manage the aforementioned uncertainty, we propose a robust-SHM methodology that combines three technologies. A time series\u00a0\u2026", "num_citations": "2\n", "authors": ["1394"]}
{"title": "Developing Simplified Models of Wind Turbine Blades\n", "abstract": " Simplified beam models provide a computationally efficient method for modeling the vibration of wind turbine blades. The purpose of this paper is to demonstrate the process of developing a simplified beam model of the CX-100 wind turbine blade, and quantifying its predictive capability. The motivation for this study is rooted in the development of NLBeam, a non-linear beam code developed at Los Alamos National Laboratory to simulate the structural dynamics response of wind turbines using the geometrically exact beam theory in a coupled atmospheric hydrodynamics solver. Verification activities used to assess the credibility of NLBeam are investigated. Two models of the CX-100 blade are compared:(1) a three dimensional shell model and (2) a simplified onedimensional beam model. Two sets of experimental modal data are utilized, one with the CX-100 blade in a fixed-free condition, and one with the CX-100 blade in a fixed-free condition, with large masses applied. By exploring these different configurations of the wind turbine blade, credibility can be established regarding the ability of the FE model to predict the response to different loading conditions. Through the use of test-analysis correlation, the experimental data can be compared to model output and an assessment is given of the predictive capability of the model.(Approved for unlimited, public release on October-xx-2012, LA-UR-12-xxxx, Unclassified.)", "num_citations": "2\n", "authors": ["1394"]}
{"title": "On the legitimacy of model calibration in structural dynamics\n", "abstract": " In structural dynamics, a finite element model is often calibrated to better reproduce experimental measurements collected from the structure. When the agreement between measurements and predictions is unsatisfactory, the model is parameterized and calibrated to improve its overall accuracy. We argue that model calibration may not always be legitimate to improve test-analysis correlation, because a calibration study attempts to compensate for parametric errors when, in fact, the disagreement between measurements and predictions may originate from other sources (e.g. discretization error). In this work, a scaled model of a three-story frame structure that responds mostly in bending is tested experimentally and modeled with finite elements. The agreement between measurements and predictions is assessed relative to the overall level of experimental variability. Truncation error is quantified by performing\u00a0\u2026", "num_citations": "2\n", "authors": ["1394"]}
{"title": "Inference uncertainty quantification instead of full-scale testing\n", "abstract": " We give simple examples illustrating the concept and importance of inference uncertainty, which can be defined as the difference between what is measured (the observable quantity) and what is desired (the unobserved quantity). Quantification of uncertainty arising from inference has an important role to play in lieu of full-scale testing, because system-level uncertainties may not be observable by observing separate effects tests. Yet, little attention has been paid to this type of uncertainty, which is prevalent in numerous scientific and engineering applications. We propose that inference uncertainty can be mathematically characterized using different theories of uncertainty, including probability theory. A metric for the quantification of margins and uncertainties relating to factor of safety is discussed, and an example of information integration is illustrated.(Manuscript approved for unlimited, public release, LA-UR-08\u00a0\u2026", "num_citations": "2\n", "authors": ["1394"]}
{"title": "A Decision-centric Analysis of the Sandia Structural Mechanics Validation Problem\n", "abstract": " The Sandia Validation Challenge structural mechanics problem involves predicting the displacements of a static frame structure subjected to externally applied forces. Although its academic nature renders it overly simplistic, the formulation of the problem nevertheless relies on strong, yet, somewhat unverified assumptions such as linear elasticity to describe the material behavior, idealized boundary conditions, and a simplified description of frame structure kinematics. The main contribution of this work is to argue in favor of the importance of questioning assumptions that inevitably present themselves during model-building, and to illustrate the extent to which predictions may, or not, be robust to some of these choices. The validation assessment is presented as a trade-off analysis between fidelity-to-data (or prediction accuracy) and robustness to lack-ofknowledge. These two metrics are calculated for the\u00a0\u2026", "num_citations": "2\n", "authors": ["1394"]}
{"title": "15 YEARS OF VERYIFYING AND VALIDATING STRUCTURAL DYNAMICS SIMULATIONS AT LOS ALAMOS (U)\n", "abstract": " Verification and Validation (V& V) refers to a broad range of activities that are carried out to provide evidence that measurements and predictions are credible and scientifically defendable. The publication offers an overview of lessons learned after fifteen years of research, development, and application of V&V technology at the Los Alamos National Laboratory (LANL). The discussion is restricted to Structural Dynamics even though V&V at LANL reaches across practices that include software quality assurance, verification, data analysis and archiving, engineering simulation, computational physics and astrophysics simulation, and the quantification of uncertainty. Seven principles are enounced that organize the lessons learned: 1) Model validation must be defined over a domain of settings; 2) There can be no validation without experimental data; 3) Test measurements without experimental error bounds are meaningless; 4) Simulation predictions without numerical error bounds are meaningless; 5) There is more to uncertainty than parametric variability; 6) The predictive value of a calibrated model is exactly zero; 7) The robustness to lack-of-knowledge of a calibrated model is exactly zero. The bottom line is that the cornerstone of V&V is threefold with, first, showing whenever possible that predictions of numerical simulations are accurate relative to test data over a range of settings or operating conditions; second, quantifying the sources and levels of prediction uncertainty; and, third, demonstrating that predictions are robust, that is, insensitive, to the modeling assumptions and lack-of-knowledge.(This manuscript is approved for unlimited, public\u00a0\u2026", "num_citations": "2\n", "authors": ["1394"]}
{"title": "Challenges to the State-of-the-practice of Solution Convergence Verification\n", "abstract": " The main goal of solution verification is to assess the convergence of numerical predictions as a function of discretization variables such as element size \u0394x or time step \u0394t. The challenge is to verify that the approximate solutions of the discretized laws-of-conservation or equations-of-motion converge to the solution of the continuous equations. In the case of code verification where the continuous solution of a test problem is known, common practice is to obtain several discrete solutions from successively refined meshes or grids; calculate norms of the solution error; and verify the rate with which discrete solutions converge to the continuous solution. With solution verification, where the continuous solution is unknown, common practice is to obtain several discrete solutions from successively refined meshes or grids; estimate an extrapolation of the continuous solution; verify the rate-of-convergence; and estimate\u00a0\u2026", "num_citations": "2\n", "authors": ["1394"]}
{"title": "Uncertainty Quantification of Composite Laminate Damage with the Generalized Information Theory\n", "abstract": " This work presents a survey of five theories to assess the uncertainty of projectile impact induced damage on multi-layered carbon-epoxy composite plates. Because the types of uncertainty dealt with in this application are multiple (variability, ambiguity, and conflict) and because the data sets collected are sparse, characterizing the amount of delamination damage with probability theory alone is possible but incomplete. This motivates the exploration of methods contained within a broad Generalized Information Theory (GIT) that rely on less restrictive assumptions than probability theory. Probability, fuzzy sets, possibility, and imprecise probability (probability boxes (p-boxes) and Dempster-Shafer) are used to assess the uncertainty in composite plate damage. Furthermore, this work highlights the usefulness of each theory. The purpose of the study is not to compare directly the different GIT methods but to show that they can be deployed on a practical application and to compare the assumptions upon which these theories are based. The data sets consist of experimental measurements and finite element predictions of the amount of delamination and fiber splitting damage as multilayered composite plates are impacted by a projectile at various velocities. The physical experiments consist of using a gas gun to impact suspended plates with a projectile accelerated more\u00bb", "num_citations": "2\n", "authors": ["1394"]}
{"title": "Validating finite element models of composite aerospace structures for damage detection applications\n", "abstract": " Carbon-fiber-reinforced-polymer (CFRP) composites represent the future for advanced lightweight aerospace structures. However, reliable and cost-effective techniques for structural health monitoring (SHM) are needed. Modal and vibration-based analysis, when combined with validated finite element (FE) models, can provide a key tool for SHM. Finite element models, however, can easily give spurious and misleading results if not finely tuned and validated. These problems are amplified in complex structures with numerous joints and interfaces. A small series of all-composite test pieces emulating wings from a lightweight all-composite Unmanned Aerial Vehicle (UAV) have been developed to support damage detection and SHM research. Each wing comprises two CFRP prepreg and Nomex honeycomb co-cured skins and two CFRP prepreg spars bonded together in a secondary process using a structural\u00a0\u2026", "num_citations": "2\n", "authors": ["1394"]}
{"title": "A Novel Statistical Based Approach to non-linear Model updating using Response Features\n", "abstract": " This research presents a new method to improve analytical model fidelity for non-linear systems. The approach investigates several mechanisms to assist the analyst in updating an analytical model based on experimental data and statistical analysis of parameter effects. The first is a new approach at data reduction called feature extraction. This is an expansion of the''classic''update metrics to include specific phenomena or characters of the response that are critical to model application. This is an extension of the familiar linear updating paradigm of utilizing the eigen-parameters or frequency response functions (FRFs) to include such devices as peak acceleration, time of arrival or standard deviation of model error. The next expansion of the updating process is the inclusion of statistical based parameter analysis to quantify the effects of uncertain or significant effect parameters in the construction of a meta-model. This provides indicators of the statistical variation associated with parameters as well as confidence intervals on the coefficients of the resulting meta-model. Also included in this method is the investigation of linear parameter effect screening using a partial factorial variable array for simulation. This is intended to aid the analyst in eliminating from the investigation the parameters that do not more\u00bb", "num_citations": "2\n", "authors": ["1394"]}
{"title": "Discussion of model calibration and validation for transient dynamics simulation.\n", "abstract": " Model calibration refers to a family of inverse problem-solving numerical techniques used to infer the value of parameters from test data sets. The purpose of model calibration is to optimize parametric or non-parametric models in such a way that their predictions match reality. In structural dynamics an example of calibration is the finite element model updating technology. Our purpose is essentially to discuss calibration in the broader context of model validation. Formal definitions are proposed and the notions of calibration and validation are illustrated using an example of transient structural dynamics that deals with the propagation of a shock wave through a hyper-foam pad. An important distinction that has not been made in finite element model updating and that is introduced here is that parameters of the numerical models or physical tests are categorized into input parameters, calibration variables, controllable and uncontrollable variables. Such classification helps to define model validation goals. Finally a path forward for validating numerical model is discussed and the relationship with uncertainty assessment is stressed.", "num_citations": "2\n", "authors": ["1394"]}
{"title": "Formulation and resolution of inverse problems for nonlinear, transient structural dynamics\n", "abstract": " A general framework is proposed for validating numerical models for nonlinear, transient dynamics. Previous work has focused on nonlinear vibration and several difficulties of formulating and solving inverse problems for nonlinear dynamics have been identified. Among them, we cite the necessity to satisfy continuity of the response when several finite element optimizations are successively carried out and the need to propagate variability throughout the optimization of the model's parameters. Our approach is illustrated using data from a nonlinear vibration testbed and an impact test experiment both conducted at Los Alamos National Laboratory in support of the advanced strategic computing initiative and our code validation and verification program.", "num_citations": "2\n", "authors": ["1394"]}
{"title": "Overcoming difficulties in the updating of FE models for industrial applications\n", "abstract": " This paper examines difficulties in the updating of industrial structures. Such structures typically involve large numbers of elements, with few measurement points. Frequently, these structures are composed of discrete components or substructures. These characteristics can create difficulty for conventional updating schemes. The difficulties include non-physical solutions, difficulty with error localization, inconsistent normalization of experimental modes, and indefinite correlation between analytical and experimental modal data. These issues are discussed and possible remedies presented. Results are based on experience with real industrial structures, rather than purely simulated data.", "num_citations": "2\n", "authors": ["1394"]}
{"title": "Finite element updating of substructured models\n", "abstract": " This paper addresses the problem of parametric correction of condensed finite element models using test data. A technique is proposed to correct individual elements or components within each condensed subdomain. It is shown how the original updating algorithm can be applied to models formed of condensed substructures with the additional cost of solving linear systems for obtaining contributions of the condensed subdomains to the derivatives of the objective function but the added capability of identifying modeling errors within the subdomains. The applicability of this technique to the updating of test-analysis correlation models is discussed when finite element matrices are condensed to a size intermediate between the experimental model and the fullorder model. The procedure is illustrated using simulated data and it enables the identification of two disconnected modeling errors within the same subdomain without having to perform multiple condensations at each updating iteration.", "num_citations": "2\n", "authors": ["1394"]}
{"title": "Darht multi-intelligence seismic and acoustic data analysis\n", "abstract": " The purpose of this report is to document the analysis of seismic and acoustic data collected at the Dual-Axis Radiographic Hydrodynamic Test (DARHT) facility at Los Alamos National Laboratory for robust, multi-intelligence decision making. The data utilized herein is obtained from two tri-axial seismic sensors and three acoustic sensors, resulting in a total of nine data channels. The goal of this analysis is to develop a generalized, automated framework to determine internal operations at DARHT using informative features extracted from measurements collected external of the facility. Our framework involves four components:(1) feature extraction,(2) data fusion,(3) classification, and finally (4) robustness analysis. Two approaches are taken for extracting features from the data. The first of these, generic feature extraction, involves extraction of statistical features from the nine data channels. The second approach, event detection, identifies specific events relevant to traffic entering and leaving the facility as well as explosive activities at DARHT and nearby explosive testing sites. Event detection is completed using a two stage method, first utilizing signatures in the frequency domain to identify outliers and second extracting short duration events of interest among these outliers by evaluating residuals of an autoregressive exogenous time series model. Features more\u00bb", "num_citations": "1\n", "authors": ["1394"]}
{"title": "A robust approach to quantifying forecasting uncertainty using proxy simulations\n", "abstract": " LA-UR-16-23755 Page 1 LA-UR-16-23755 Approved for public release; distribution is unlimited. Title: A Robust Approach to Quantifying Forecasting Uncertainty Using Proxy Simulations Author(s): Van Buren, Kendra Lu Cogan, Scott Hemez, Francois M. Intended for: V&V Symposium, 2016-05-18/2016-05-20 (Las Vegas, Nevada, United States) Issued: 2016-05-27 Page 2 Disclaimer: Los Alamos National Laboratory, an affirmative action/equal opportunity employer, is operated by the Los Alamos National Security, LLC for the National Nuclear Security Administration of the US Department of Energy under contract DE-AC52-06NA25396. By approving this article, the publisher recognizes that the US Government retains nonexclusive, royalty-free license to publish or reproduce the published form of this contribution, or to allow others to do so, for US Government purposes. Los Alamos National Laboratory requests that \u2026", "num_citations": "1\n", "authors": ["1394"]}
{"title": "Feature Extraction for Structural Dynamics Model Validation\n", "abstract": " As structural dynamics becomes increasingly non-modal, stochastic and nonlinear, finite element model-updating technology must adopt the broader notions of model validation and uncertainty quantification. For example, particular re-sampling procedures must be implemented to propagate uncertainty through a forward calculation, and non-modal features must be defined to analyze nonlinear data sets. The latter topic is the focus of this report, but first, some more general comments regarding the concept of model validation will be discussed.", "num_citations": "1\n", "authors": ["1394"]}
{"title": "Information gap analysis for decision support systems in evidence-based medicine\n", "abstract": " The objective of evidence-based medicine is to come to well reasoned and justified clinical decisions regarding an individual patient\u2019s case based on the integration of case-specific knowledge, medical expertise, and the best available clinical evidence. One significant challenge implicated in this pursuit stems from the volume of relevant information that can easily exceed what can reasonably be assessed. Thus intelligent systems that can mine and synthesize vast amounts of information would be invaluable. The reconciliation of such systems with the complexity and subtlety of decision support in medicine requires specialized capabilities. One untapped capability is furnished through the gap in information between what is known and what needs to be known to justify a decision. In this paper, we explore the value of an information gap analysis for robust decision-making in the context of evidence-based\u00a0\u2026", "num_citations": "1\n", "authors": ["1394"]}
{"title": "Finite Element Method Fundamentals\n", "abstract": " Objectives\u2022 The first objective is to point to assumptions introduced by computational approaches, such as the FEM, because understanding when these assumptions may break down is ultimately the focus of V&V activities.", "num_citations": "1\n", "authors": ["1394"]}
{"title": "Optimal Inequalities to Bound a Performance Probability\n", "abstract": " A challenging problem encountered in engineering applications is the estimation of a probability-of-failure based on incomplete knowledge of the sources of uncertainty and/or limited sampling. Theories formulated to derive upper probability bounds offer an attractive alternative because first, they avoid postulating the probability laws that are often unknown and second, they substitute numerical optimization for statistical sampling. A critical assessment of one such technique is presented. It derives upper probability bounds from the McDiarmid concentration-of-measure theory, which postulates that fluctuations of a function are more-or-less concentrated about its mean value. Two applications of this theory are presented. The first application analyzes a \u201ctoy\u201d polynomial function defined in two dimensions. The upper bounds of probability are calculated and compared to sampling-based estimates of the true-but\u00a0\u2026", "num_citations": "1\n", "authors": ["1394"]}
{"title": "Estimating uncertainty of inference for validation\n", "abstract": " We present a validation process based upon the concept that validation is an inference-making activity. This has always been true, but the association has not been as important before as it is now. Previously, theory had been confirmed by more data, and predictions were possible based on data. The process today is to infer from theory to code and from code to prediction, making the role of prediction somewhat automatic, and a machine function. Validation is defined as determining the degree to which a model and code is an accurate representation of experimental test data. Imbedded in validation is the intention to use the computer code to predict. To predict is to accept the conclusion that an observable final state will manifest; therefore, prediction is an inference whose goodness relies on the validity of the code. Quantifying the uncertainty of a prediction amounts to quantifying the uncertainty of validation, and this involves the characterization of uncertainties inherent in theory/models/codes and the corresponding data. An introduction to inference making and its associated uncertainty is provided as a foundation for the validation problem. A mathematical construction for estimating the uncertainty in the validation inference is then presented, including a possibility distribution constructed more\u00bb", "num_citations": "1\n", "authors": ["1394"]}
{"title": "A Similarity Metric to Assess Upper and Lower Probabilities.\n", "abstract": " Uncertainty abounds in physical system analysis and simulation. A variety of measures of uncertainty are built around measuring the amount of nonspecificity, strife and conflict in the information. The results of these measures are useful for decision making as uncertainty-based information. We are progressing towards a novel approach to uncertainty analysis in a related sense by exploiting an existing technique in linear algebra as a means towards a measure of ignorance. A test metric is proposed to measure the similarity extent between two probability distributions, upper and lower. We show analytically and numerically how this metric performs on two normal distributions (a standard and sample) for illustration.", "num_citations": "1\n", "authors": ["1394"]}
{"title": "Estimating the prediction accuracy of highrate, high temperature constitutive models for Taylor impact applications\n", "abstract": " * Los Alamos National Laboratory Engineering Sciences and Applications (ESA-WR) Mail Stop T006, Los Alamos, New Mexico 87545, USA E-mail: hemez@ lanl. gov\u2014Web: http://www. esa. lanl. gov", "num_citations": "1\n", "authors": ["1394"]}
{"title": "Structural health monitoring design using finite element analysis\n", "abstract": " DSpace at KOASAS: Structural health monitoring design using finite element analysis KOASAS menu About KOASAS KAIST Library \uac80\uc0c9 Advanced Search Browse Communities & Collections Researchers at KAIST Titles Subject By Date rss_1.rss_2.atom_1.sherpa SEARCH DSpace at KOASAS College of Engineering(\uacf5\uacfc\ub300\ud559)Dept. of Civil and Environmental Engineering(\uac74\uc124 \ubc0f\ud658\uacbd\uacf5\ud559\uacfc)CE-Conference Papers(\ud559\uc220\ud68c\uc758\ub17c\ubb38) Structural health monitoring design using finite element analysis Cited 0 time in webofscience Cited 0 time in scopus Hit : 214 Download : 0 Export DC(XML) Excel Farrar, Charles R. / Stinemates, Daniel W. / Hemez, Francois M. / Sohn, Hoonresearcher Publisher SPIE Issue Date 2002-03-17 Language English Citation SPIE\u2019s 7th Annual International Symposium on NDE for Health Monitoring and Diagnostics URI http://hdl.handle.net/10203/138241 Appears in Collection CE-Conference \u2026", "num_citations": "1\n", "authors": ["1394"]}
{"title": "A tribute to the contribution of G\u00e9rard Lallement to structural dynamics\n", "abstract": " The Society for Experimental Mechanics and the International Modal Analysis Conference recognize the remarkable contribution to experimental mechanics, mechanical engineering and structural dynamics of Professor G\u00e9rard Lallement, from the University of Franche-Comt\u00e9, France. A special session is organized during the IMAC-XX to outline the many achievements of G\u00e9rard Lallement in the fields of modal analysis, structural system identification, the theory and practice of structural modification, component mode synthesis and finite element model updating. The purpose of this publication is not to provide an exhaustive account of G\u00e9rard Lallement\u2019s contribution to structural dynamics. Numerous references are provided that should help the interested reader learn more about the many aspects of his work. Instead, the significance of this work is illustrated by discussing the role of structural dynamics in industrial\u00a0\u2026", "num_citations": "1\n", "authors": ["1394"]}
{"title": "Validation of Nonlinear Modeling from Impact Test Data Using Probability Integration,# 241\n", "abstract": " Validation of Nonlinear Modeling from Impact Test Data Using Probability Integration, #241 - NASA/ADS Now on home page ads icon ads Enable full ADS view NASA/ADS Validation of Nonlinear Modeling from Impact Test Data Using Probability Integration, #241 Hemez, FM ; Doebling, SW ; Rhee, W. Abstract Publication: Proceedings of IMAC-XVIII: A Conference on Structural Dynamics Pub Date: January 2000 Bibcode: 2000SPIE...124H No Sources Found \u00a9 The SAO/NASA Astrophysics Data System adshelp[at]cfa.harvard.edu The ADS is operated by the Smithsonian Astrophysical Observatory under NASA Cooperative Agreement NNX16AC86A NASA logo Smithsonian logo Resources About ADS ADS Help What's New Careers@ADS Social @adsabs ADS Blog Project Switch to full ADS Is ADS down? (or is it just me...) Smithsonian Institution Smithsonian Privacy Notice Smithsonian Terms of Use Smithsonian \u2026", "num_citations": "1\n", "authors": ["1394"]}
{"title": "Validation of Structural Dynamics Models at Los Alamos National Laboratory\n", "abstract": " This publication proposes a discussion of the general problem of validating numerical models for nonlinear, transient dynamics. The predictive quality of a numerical model is generally assessed by comparing the computed response to test data. If the correlation is not satisfactory, an inverse problem must be formulated and solved to identify the sources of discrepancy between test and analysis data. Some of the most recent work summarized in this publication has focused on developing test-analysis correlation and inverse problem solving capabilities for nonlinear vibrations. Among the difficulties encountered, we cite the necessity to satisfy continuity of the response when several finite element optimization are successively carried out and the need to propagate variability throughout the optimization of the model's parameters. After a brief discussion of the formulation of inverse problems for nonlinear dynamics\u00a0\u2026", "num_citations": "1\n", "authors": ["1394"]}
{"title": "Vacuum seals design and testing for a linear accelerator of the National Spallation Neutron Source\n", "abstract": " Vacuum seals are very important to ensure that the Spallation Neutron Source (SNS) Linac has an optimum vacuum system. The vacuum joints between flanges must have reliable seals to minimize the leak rate and meet vacuum and electrical requirements. In addition, it is desirable to simplify the installation and thereby also simplify the maintenance required. This report summarizes an investigation of the metal vacuum seals that include the metal C-seal, Energized Spring seal, Helcoflex Copper Delta seal, Aluminum Delta seal, delta seal with limiting ring, and the prototype of the copper diamond seals. The report also contains the material certifications, design, finite element analysis, and testing for all of these seals. It is a valuable reference for any vacuum system design. To evaluate the suitability of several types of metal seals for use in the SNS Linac and to determine the torque applied on the bolts, a series of vacuum leak rate tests on the metal seals have been completed at Los Alamos Laboratory. A copper plated flange, using the same type of delta seal that was used for testing with the stainless steel flange, has also been studied and tested. A vacuum seal is desired that requires significantly less loading than a standard ConFlat flange with a copper gasket for the coupling cavity assembly. To save the intersegment space we use thinner flanges in the design. The leak rate of the thin ConFlat flange with a copper gasket is a baseline for the vacuum test on all seals and thin flanges. A finite element analysis of a long coupling cavity flange with a copper delta seal has been performed in order to confirm the design of the long coupling cavity\u00a0\u2026", "num_citations": "1\n", "authors": ["1394"]}
{"title": "Affordable structural optimization for large-size dynamic finite element models\n", "abstract": " This paper emphasizes the derivation of computationally efficient algorithms for optimizing large dimensional structural models. Structures of interest are characterized by linear, undamped models and the problem consists essentially in minimizing the total mass while satisfying frequency and/or mode shape constraints. The dual optimization method and a resizing rule are implemented for solving the optimization. Since this approach modifies the model's design parameters, it may be implemented for solving structural optimization as well as topology optimization. An alternate model assembly technique and sensitivitybased modal parameter updating schemes are implemented for enhancing computational requirements. Finally, strain energy distribution and modal residue criteria are employed for localizing spatially the optimization. The procedure is illustrated successfully with large-size structural models where\u00a0\u2026", "num_citations": "1\n", "authors": ["1394"]}
{"title": "Structural damage detection with very large dimensional finite element models\n", "abstract": " Structural Damage Detection with Very Large Dimensional Finite Element Models - NASA/ADS Now on home page ads icon ads Enable full ADS view NASA/ADS Structural Damage Detection with Very Large Dimensional Finite Element Models Hemez, EM Abstract Publication: Proceedings of the 15th International Modal Analysis Conference Pub Date: 1997 Bibcode: 1997SPIE.H No Sources Found \u00a9 The SAO/NASA Astrophysics Data System adshelp[at]cfa.harvard.edu The ADS is operated by the Smithsonian Astrophysical Observatory under NASA Cooperative Agreement NNX16AC86A NASA logo Smithsonian logo Resources About ADS ADS Help What's New Careers@ADS Social @adsabs ADS Blog Project Switch to full ADS Is ADS down? (or is it just me...) Smithsonian Institution Smithsonian Privacy Notice Smithsonian Terms of Use Smithsonian Astrophysical Observatory NASA \u2026", "num_citations": "1\n", "authors": ["1394"]}
{"title": "Toward the updating of large-scale dynamic finite element models using massive instrumentation\n", "abstract": " High accuracy and large size structural applications require highly correlated finite element models to predict the system's dynamic behavior whenever testing is not practical nor possible. Hence, static and dynamic models are refined using updating techniques that seek to correct modeling errors by minimizing a distance between measurements and predictions. This paper focuses on identifying difficulties that often prevent from applying the current updating technology to very largescale finite element models. These issues include modal expansion, modeling error localization, parameter correction and control of numerical illconditioning. Computationally efficient algorithms are implemented for resolving these difficulties and performances are compared using large structural models of the automotive industry. The implementation presented here for a particular updating", "num_citations": "1\n", "authors": ["1394"]}
{"title": "A robust methodology for the simultaneous updating of finite element mass and stiffness matrices\n", "abstract": " Numerical difficulties are known to arise during model updating becanse of the inverse nature of the problem. Here, we show that ill-conditioning results from the disproportion between the mass and stiffness derived equations of the correction system. We discuss the effect of these numerical difficulties on a class of sensitivitybased updating methods, and propose a two-step strategy to bypass them. In the first step, the correction systeu~ 1s non-dimensionalized before it is solved in order to prevent large stiffness perturbations from masking mass errois. In the second step, the implementation of the Singular Value Decomposition factorization is revisited to filter out all non-physical contributions to the adjustment solution. The potential of this two-step strategy is demonstrated with the model refinement via the Sensitivity-Based Element-By-Element (SB-EBE) updating method of a recently published planar frame\u00a0\u2026", "num_citations": "1\n", "authors": ["1394"]}
{"title": "A Fresh Look at Mesh Refinement in Computational Physics and Engineering\n", "abstract": " This manuscript takes a fresh look at the concept of asymptotic convergence of spatial or time-varying curves simulated by a computational physics or engineering code. Asymptotic convergence is the regime where truncation error dominates the overall numerical error. Truncation error is the difference between discrete solutions obtained by a code with a given level of mesh or grid refinement, \u0394x, and the (unknown) solution of the continuous partial differential equations; it should approach zero with increased levels of refinement, that is, \u0394x\u2192 0. The current state-of-the-practice is to use mesh refinement to study the rate at which truncation error converges as \u0394x\u2192 0. The observed rate-of-convergence is then compared to the theoretical order of the numerical method to verify its implementation and assess solution accuracy. The essential idea from which our contribution originates is that the discrete solution curves, y\u00a0\u2026", "num_citations": "1\n", "authors": ["1394"]}