{"title": "Introduction to Grothendieck duality theory\n", "abstract": " Page 1 Allen Altman Steven Kleiman Introduction to Grothendieck Duality Theory \u00ba Springer Page 2 Lecture Notes in Mathematics A collection of informal reports and seminars Edited by A. Dold, Heidelberg and B. Eckmann, Zurich 146 Allen Altman University of California, La Jolla, CA/USA Steven Kleiman MIT, Dept. of Mathematics, Cambridge, MA/USA Introduction to Grothendieck Duality Theory Springer-Verlag Berlin - Heidelberg - New York 1970 Page 3 This work is subject to copyright. All rights are reserved, whether the whole or part of the material is concerned, specifically those of translation, reprinting, re-use of illustrations, broadcasting, reproduction by photocopying machine or similar means, and storage in data banks. Under $54 of the German Copyright Law where copies are made for other than private use, a fee is payable to the publisher, the amount of the fee to be determined by agreement with the \u2026", "num_citations": "727\n", "authors": ["709"]}
{"title": "DAISY: Dynamic compilation for 100% architectural compatibility\n", "abstract": " Although VLIW architectures offer the advantages of simplicity of design and high issue rates, a major impediment to their use is that they are not compatible with the existing software base. We describe new simple hardware features for a VLIW machine we call DAISY (Dynamically Architected Instruction Set from Yorktown). DAISY is specifically intended to emulate existing architectures, so that all existing software for an old architecture (including operating system kernel code) runs without changes on the VLIW. Each time a new fragment of code is executed for the first time, the code is translated to VLIW primitives, parallelized and saved in a portion of main memory not visible to the old architecture, by a Virtual Machine Monitor (software) residing in read only memory. Subsequent executions of the same fragment do not require a translation (unless cast out). We discuss the architectural requirements for such a\u00a0\u2026", "num_citations": "613\n", "authors": ["709"]}
{"title": "Dynamic binary translation and optimization\n", "abstract": " We describe a VLIW architecture designed specifically as a target for dynamic compilation of an existing instruction set architecture. This design approach offers the simplicity and high performance of statically scheduled architectures, achieves compatibility with an established architecture, and makes use of dynamic adaptation. Thus, the original architecture is implemented using dynamic compilation, a process we refer to as DAISY (Dynamically Architected Instruction Set from Yorktown). The dynamic compiler exploits runtime profile information to optimize translations so as to extract instruction level parallelism. This paper reports different design trade-offs in the DAISY system and their impact on final system performance. The results show high degrees of instruction parallelism with reasonable translation overhead and memory usage.", "num_citations": "225\n", "authors": ["709"]}
{"title": "Dynamic and transparent binary translation\n", "abstract": " High-frequency design and instruction-level parallelism (ILP) are important for high-performance microprocessor implementations. The Binary-translation Optimized Architecture (BOA), an implementation of the IBM PowerPC family, combines binary translation with dynamic optimization. The authors use these techniques to simplify the hardware by bridging a semantic gap between the PowerPC's reduced instruction set and even simpler hardware primitives. Processors like the Pentium Pro and Power4 have tried to achieve high frequency and ILP by implementing a cracking scheme in hardware: an instruction decoder in the pipeline generates multiple micro-operations that can then be scheduled out of order. BOA relies on an alternative software approach to decompose complex operations and to generate schedules, and thus offers significant advantages over purely static compilation approaches. This article\u00a0\u2026", "num_citations": "190\n", "authors": ["709"]}
{"title": "Methods and apparatus for reordering and renaming memory references in a multiprocessor computer system\n", "abstract": " There is provided a method for reordering and renaming memory references in a multiprocessor computer system having at least a first and a second processor. The first processor has a first private cache and a first buffer, and the second processor has a second private cache and a second buffer. The method includes the steps of, for each of a plurality of gated store requests received by the first processor to store a datum, exclusively acquiring a cache line that contains the datum by the first private cache, and storing the datum in the first buffer. Upon the first buffer receiving a load request from the first processor to load a particular datum, the particular datum is provided to the first processor from among the data stored in the first buffer based on an in-order sequence of load and store operations. Upon the first cache receiving a load request from the second cache for a given datum, an error condition is indicated and\u00a0\u2026", "num_citations": "188\n", "authors": ["709"]}
{"title": "Welcome to the opportunities of binary translation\n", "abstract": " A new processor architecture poses significant financial risk to hardware and software developers alike, so both have a vested interest in easily porting code from one processor to another. Binary translation offers solutions for automatically converting executable code to run on new architectures without recompiling the source code.", "num_citations": "188\n", "authors": ["709"]}
{"title": "LaTTe: A Java VM just-in-time compiler with fast and efficient register allocation\n", "abstract": " For network computing on desktop machines, fast execution of Java bytecode programs is essential because these machines are expected to run substantial application programs written in Java. Higher Java performance can be achieved by just-in-time (JIT) compilers which translate the stack-based bytecode into register-based machine code on demand. One crucial problem in Java JIT compilation is how to map and allocate stack entries and local variables into registers efficiently and quickly, so as to improve the Java performance. This paper introduces LaTTe, a Java JIT compiler that performs fast and efficient register mapping and allocation for RISC machines. LaTTe first translates the bytecode into pseudo RISC code with symbolic registers, which is then register allocated while coalescing those copies corresponding to pushes and pops between local variables and the stack. The LaTTe JVM also includes\u00a0\u2026", "num_citations": "171\n", "authors": ["709"]}
{"title": "Minimizing register requirements under resource-constrained rate-optimal software pipelining\n", "abstract": " In this paper we address the following software pipelining problem: given a loop and a machine architecture with a fixed number of processor resources (eg function units), how can one construct a software-pipelined schedule which runs on the given architecture at the maximum possible iteration rate (ala rate-optimal) while minimizing the number of registers?", "num_citations": "144\n", "authors": ["709"]}
{"title": "A Java ILP machine based on fast dynamic compilation\n", "abstract": " CiteSeerX \u2014 A JAVA ILP Machine Based on Fast Dynamic Compilation Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA A JAVA ILP Machine Based on Fast Dynamic Compilation (1997) Cached Download as a PDF Download Links [www.research.ibm.com] [cardit.et.tudelft.nl] [www.research.ibm.com] Save to List Add to Collection Correct Errors Monitor Changes by Kemal Ebcioglu , Erik Altman , Erdem Hokenek Venue: In IEEE MASCOTS International Workshop on Security and E ciency Aspects of Java Citations: 24 - 8 self Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract The JAVA programming language has recently been attracting attention... Keyphrases java ilp \u2026", "num_citations": "135\n", "authors": ["709"]}
{"title": "Symmetric multi-processing system with attached processing units being able to access a shared memory without being structurally configured with an address translation mechanism\n", "abstract": " A method and system for attached processing units accessing a shared memory in an SMP system. In one embodiment, a system comprises a shared memory. The system further comprises a plurality of processing elements coupled to the shared memory. Each of the plurality of processing elements comprises a processing unit, a direct memory access controller and a plurality of attached processing units. Each direct memory access controller comprises an address translation mechanism thereby enabling each associated attached processing unit to access the shared memory in a restricted manner without an address translation mechanism. Each attached processing unit is configured to issue a request to an associated direct memory access controller to access the shared memory specifying a range of addresses to be accessed as virtual addresses. The associated direct memory access controller is configured to\u00a0\u2026", "num_citations": "107\n", "authors": ["709"]}
{"title": "Advances and future challenges in binary translation and optimization\n", "abstract": " Binary translation and optimization have achieved a high profile in recent years. Binary translation has several potential attractions. While still in its early stages, could binary translation offer a new way to design processors, i.e. is it a disruptive technology? This paper discusses this question, examines some future possibilities for binary translation, and then gives an overview of selected projects (DAISY, Crusoe, Dynamo and LaTTe). One future possibility for binary translation is the Virtual IT Shop. Binary translation offers a possible solution for better utilization of computational resources as services over the World Wide Web. The Internet is radically changing the software landscape, and is fostering platform independence and interoperability. Along the lines of software convergence, recent advances in binary JIT (just-in-time) optimizations also present the future possibility of a convergence virtual machine (CVM\u00a0\u2026", "num_citations": "103\n", "authors": ["709"]}
{"title": "A register allocation framework based on hierarchical cyclic interval graphs\n", "abstract": " In this paper, we present a new register allocation framework based on hierarchical cyclic interval graphs. We motivate our approach by demonstrating that cyclic interval graphs provide a feasible and effective representation to characterize sequences of live ranges of variables in successive iterations of a loop. Based on this representation we provide a new heuristic algorithm for minimum register allocation, the fat cover algorithm. In addition, we present a spilling algorithm that makes use of the extra information available in the interval graph representation. Whenever possible, it favors register floats (moving values from one register to another) over the traditional register spills (storing a spilled variable into memory).             We demonstrate the effectiveness of our approach on a collection of loops by comparing the results of our algorithm to the results produced by three state-of-the-art optimizing compilers.", "num_citations": "95\n", "authors": ["709"]}
{"title": "BOA: The architecture of a binary translation processor\n", "abstract": " High frequency design and instruction-level parallelism (ILP) are two keys to high performance microprocessor implementation. To achieve these sometimes competing goals, the Binary-translationOptimized Architecture (BOA) aims to bring code translation techniques based on continuous profiling into the mainstream. Initially, code is interpreted to detect code hot spots and gather profile information to guide dynamic optimizations. To achieve compatibility with the established PowerPC architecture, a binary translation layer translates PowerPC instructions into simple VLIW operation primitives. These primitives are then scheduled using VLIW scheduling techniques to a variable length, six issue VLIW/EPIC processor. Binary translation eliminates the binary compatibility problem faced by other processors, while dynamic recompilation enables adaptive re-optimization of critical program code sections and eliminates the need for dynamic scheduling hardware. As a result, the BOA execution platform can be designed for multiple Gigahertz operation. The hardware execution platform includes novel microarchitectural features to eliminate complex stall and exception logic. Special support is also provided for binary translation in the form of several primitives designed for system-level binary translation functions. The data types of the binary translation processor are similar to that of the emulated PowerPC architecture to eliminate data representation issues which could necessitate potentially expensive data format conversion operations. In this work we examine the implications of binary translation on processor architecture and software translation and\u00a0\u2026", "num_citations": "77\n", "authors": ["709"]}
{"title": "A framework for resource-constrained rate-optimal software pipelining\n", "abstract": " The rapid advances in high-performance computer architecture and compilation techniques provide both challenges and opportunities to exploit the rich solution space of software pipelined loop schedules. In this paper, we develop a framework to construct a software pipelined loop schedule which runs on the given architecture (with a fixed number of processor resources) at the maximum possible iteration rate (a la rate-optimal) while minimizing the number of buffers-a close approximation to minimizing the number of registers. The main contributions of this paper are: First, we demonstrate that such problem can be described by a simple mathematical formulation with precise optimization objectives under a periodic linear scheduling framework. The mathematical formulation provides a clear picture which permits one to visualize the overall solution space (for rate-optimal schedules) under different sets of\u00a0\u2026", "num_citations": "74\n", "authors": ["709"]}
{"title": "Scheduling and mapping: Software pipelining in the presence of structural hazards\n", "abstract": " Recently, software pipelining methods based on an ILP (Integer Linear Programming) framework have been successfully applied to derive rate-optimal schedules for architectures involving clean pipelines-pipelines without structural hazards. The problem for architectures beyond such clean pipelines remains open. One challenge is how, under a unified ILP framework, to simultaneously represent resource constraints for unclean pipelines, and the assignment or mapping of operations from a loop to those pipelines. In this paper we provide a framework which does exactly this, and in addition constructs rate-optimal software pipelined schedules. The proposed formulation and a solution method have been implemented and tested on a set of 1006 loops taken from various scientific and integer benchmark suites. The formulation found a rate-optimal schedule for 75% of the loops, and required a median time of only 2\u00a0\u2026", "num_citations": "70\n", "authors": ["709"]}
{"title": "Method and apparatus for transferring control in a computer system with dynamic compilation capability\n", "abstract": " In a dynamically compiling computer system, a system and method for efficiently transferring control from execution of an instruction in a first representation to a second representation of the instruction is disclosed. The system and method include the setting of a tag for entry points of each instruction in a first representation that has been translated to a second representation. The tag is stored in memory in association with each such instruction. When a given instruction in a first representation is to be executed, the tag is examined, and if it indicates that a translated version of the instruction has previously been generated, control is passed to execution of the instruction in the second representation. The second representation can be a different instruction set representation, or an optimized representation in the same instruction set as the original instruction.", "num_citations": "68\n", "authors": ["709"]}
{"title": "Efficient Java exception handling in just\u2010in\u2010time compilation\n", "abstract": " Java uses exceptions to provide elegant error handling capabilities during program execution. However, the presence of exception handlers complicates the job of the just\u2010in\u2010time (JIT) compiler, while exceptions are rarely used in most programs. This paper describes two techniques for reducing such complications. First, we delay the translation of an exception handler until the exception really occurs. This on\u2010demand translation of exception handlers allows more optimizations when translating the main flow, without being hindered by constraints caused by the exception flows. Secondly, for those exceptions that are actually thrown during program execution we insert exception\u2010type check code and a direct branch to the translated exception handlers. This exception handler prediction is motivated by an observation that frequently thrown exceptions are likely to be handled by the same exception handlers, so this\u00a0\u2026", "num_citations": "67\n", "authors": ["709"]}
{"title": "An eight-issue tree-VLIW processor for dynamic binary translation\n", "abstract": " Presented is an 8-issue tree-VLIW processor designed for efficient support of dynamic binary translation. This processor confronts two primary problems faced by VLIW architectures: binary compatibility and branch performance. Binary compatibility with existing architectures is achieved through dynamic binary translation which translates and schedules PowerPC instructions to take advantage of the available instruction level parallelism. Efficient branch performance is achieved through tree instructions that support multi-way path and branch selection within a single VLIW instruction. The processor architecture is described, along with design details of the branch unit, pipeline, register file and memory hierarchy for a 0.25 micron standard-cell design. Performance simulations show that the simplicity of a VLIW architecture allows a wide-issue processor to operate at high frequencies.", "num_citations": "64\n", "authors": ["709"]}
{"title": "Method and system for multiprocessor emulation on a multiprocessor host system\n", "abstract": " A method (and system) for executing a multiprocessor program written for a target instruction set architecture on a host computing system having a plurality of processors designed to process instructions of a second instruction set architecture, includes representing each portion of the program designed to run on a processor of the target computing system as one or more program threads to be executed on the host computing system.", "num_citations": "63\n", "authors": ["709"]}
{"title": "BOA: Targeting multi-gigahertz with binary translation\n", "abstract": " This paper presents BOA (Binary-translation Optimized Architecture), a processor designed to achieve high frequency by using software dynamic binary translation. Processors for software binary translation are very conducive to high frequency because they can assume a simple hardware design. Binary translation eliminates the binary compatibility problem faced by other processors, while dynamic recompilation enables re-optimization of critical program code sections and eliminates the need for dynamic scheduling hardware. In this work we examine the implications of binary translation on processor architecture and software translation and how we support a very high frequency PowerPC implementation via dynamic binary translation.", "num_citations": "62\n", "authors": ["709"]}
{"title": "Optimizations and oracle parallelism with dynamic translation\n", "abstract": " We describe several optimizations which can be employed in a dynamic binary translation (DBT) system, where low compilation/translation overhead is essential. These optimizations achieve a high degree of ILP, sometimes even surpassing a static compiler employing more sophisticated, and more time-consuming algorithms. We present results in which we employ these optimizations in a dynamic binary translation system capable of computing oracle parallelism.", "num_citations": "61\n", "authors": ["709"]}
{"title": "Binary translation and architecture convergence issues for IBM System/390\n", "abstract": " We describe the design issues in an implementation of the ESA/390 architecture based on binary translation to a very long instruction word (VLIW) processor. During binary translation, complex ESA/390 instructions are decomposed into instruction \u201cprimitives\u201d which are then scheduled onto a wide-issue machine. The aim is to achieve high instruction level parallelism due to the increased scheduling and optimization opportunities which can be exploited by binary translation software, combined with the efficiency of long instruction word architectures. A further aim is to study the feasibility of a common execution platform for different instruction set architectures, such as ESA/390, RS? 6000, AS/400 and the Java Virtual Machine, so that multiple systems can be built around a common execution platform.", "num_citations": "54\n", "authors": ["709"]}
{"title": "Symmetric multi-processing system utilizing a DMAC to allow address translation for attached processors\n", "abstract": " A method and system for attached processing units accessing a shared memory in an SMT system. In one embodiment, a system comprises a shared memory. The system further comprises a plurality of processing elements coupled to the shared memory. Each of the plurality of processing elements comprises a processing unit, a direct memory access controller and a plurality of attached processing units. Each direct memory access controller comprises an address translation mechanism thereby enabling each associated attached processing unit to access the shared memory in a restricted manner without an address translation mechanism. Each attached processing unit is configured to issue a request to an associated direct memory access controller to access the shared memory specifying a range of addresses to be accessed as virtual addresses. The associated direct memory access controller is configured to\u00a0\u2026", "num_citations": "53\n", "authors": ["709"]}
{"title": "Precise exception semantics in dynamic compilation\n", "abstract": " Maintaining precise exceptions is an important aspect of achieving full compatibility with a legacy architecture. While asynchronous exceptions can be deferred to an appropriate boundary in the code, synchronous exceptions must be taken when they occur. This introduces uncertainty into liveness analysis since processor state that is otherwise dead may be exposed when an exception handler is invoked. Previous systems either had to sacrifice full compatibility to achieve more freedom to perform optimization, use less aggressive optimization or rely on hardware support.               In this work, we demonstrate how aggressive optimization can be used in conjunction with dynamic compilation without the need for specialized hardware. The approach is based on maintaining enough state to recompute the processor state when an unpredicted event such as a synchronous exception may make otherwise dead\u00a0\u2026", "num_citations": "47\n", "authors": ["709"]}
{"title": "Execution-based scheduling for VLIW architectures\n", "abstract": " We describe a new dynamic software scheduling technique for VLIW architectures, which compiles into VLIW code the program paths that are actually executed. Unlike trace processors, or DIF, the technique executes operations speculatively on multiple paths through the code, is resilient to branch mispredictions, and can achieve very large dynamic window sizes necessary for high ILP. Aggressive optimizations are applied to frequently executed portions of the code. Encouraging performance results were obtained on SPECint95 and TPC-C. The technique can be used for binary translation for achieving architectural compatibility with an existing processor, or as a VLIW scheduling technique in its own right.", "num_citations": "45\n", "authors": ["709"]}
{"title": "Method and system for efficient emulation of multiprocessor address translation on a multiprocessor host\n", "abstract": " A method (and system) for emulating a target system's memory addressing using a virtual-to-real memory mapping mechanism of a host multiprocessor system's operating system, includes inputting a target virtual memory address into a simulated page table to obtain a host virtual memory address. The target system is oblivious to the software it is running on.", "num_citations": "44\n", "authors": ["709"]}
{"title": "A comparative study of multiprocessor list scheduling heuristics\n", "abstract": " Many multiprocessor list scheduling heuristics that account for interprocessor communication delay have been proposed in recent years. However, no uniform comparative study of published heuristics has been performed in almost 20 years. This paper presents the results of a large quantitative study using random, but program-like input graphs. We found differences in the performance of the various heuristics related to the amount of parallelism in the input graph. As well, we found that no single heuristic consistently produces the best schedules under call program structures and multiprocessor configurations. Finally we propose enhancements to existing heuristics as well as our own heuristic, DS, which strikes a good balance between performance and scheduling (i.e. compile) time.< >", "num_citations": "38\n", "authors": ["709"]}
{"title": "Optimal Software Pipelining with Function Unit and Register Constraints\n", "abstract": " 1.2 ModemArchitectures.............................. 4 1.3 HistoricalPerspectives.............................. 5 1.4 PeriodicSchedules................................. i 1.5 Contributions.................................... 10 1.6 Synopsis...................................... 11", "num_citations": "34\n", "authors": ["709"]}
{"title": "System and method of execution of register pointer instructions ahead of instruction issues\n", "abstract": " A pipeline system and method includes a plurality of operational stages. The stages include a pointer register stage which stores pointer information and updates, and a rename and dependence checking stage located downstream of the pointer register stage, which renames registers and determines if dependencies exist. A functional unit provides pointer information updates to the pointer register stage such that pointer information is processed and updated to the pointer register stage before or in parallel with the register dependency checking.", "num_citations": "33\n", "authors": ["709"]}
{"title": "Method and apparatus for reordering memory operations along multiple execution paths in a processor\n", "abstract": " A method is provided for scheduling instructions for execution along multiple paths in a Computer processing system implementing out-of-order execution. The method includes the step of selecting and moving a next instruction from its current position in a sequence of instructions to an earlier position. It is determined whether the selected instruction may reference a memory location for read-access. It is determined whether the selected instruction was previously moved over a non-selected instruction which may ambiguously reference the memory location, when the selected instruction may reference the memory location for read-access. It is determined whether the selected instruction was previously moved over a branch instruction, when the selected instruction was previously moved over the non-selected instruction. A record of the selected instruction is stored for future reference, when the selected instruction\u00a0\u2026", "num_citations": "32\n", "authors": ["709"]}
{"title": "Method and apparatus for profiling computer program execution\n", "abstract": " According to a first aspect of the invention there is provided a method for profiling computer program executions in a computer processing system having a processor and a memory hierarchy. The method includes the step of executing a computer program. Profile counts are stored in a memory array for events associated with the execution of the computer program. The memory array is separate and distinct from the memory hierarchy so as to not perturb normal operations of the memory hierarchy.", "num_citations": "31\n", "authors": ["709"]}
{"title": "Method and apparatus for embedding wide instruction words in a fixed-length instruction set architecture\n", "abstract": " A method, system, and computer program product for mixing of conventional and augmented instructions within an instruction stream, wherein control may be directly transferred, without operating system intervention, between one type of instruction to another. Extra instruction word bits are added in a manner that is designed to minimally interfere with the encoding, decoding, and instruction processing environment in a manner compatible with existing conventional fixed instruction width code. A plurality of instruction words are inserted into an instruction word oriented architecture to form an encoding group of instruction words. The instruction words in the encoding group are dispatched and executed either independently or in parallel based on a specific microprocessor implementation. The encoding group does not indicate any form of required parallelism or sequentiality. One or more indicators for the encoding\u00a0\u2026", "num_citations": "31\n", "authors": ["709"]}
{"title": "Simulation/evaluation environment for a VLIW processor architecture\n", "abstract": " We describe the environment used for the simulation and evaluation of a processor architecture based on very long instruction word (VLIW) principles. In this architecture, a program consists of a set of tree instructions, each one containing multiple branches and operations which can be performed simultaneously. The simulation/evaluation environment comprises \u2022 An optimizing compiler, which generates tree instructions in a VLIW assembly language. \u2022 A translator from VLIW assembly code into PowerPC\u00ae assembly code which emulates the functionality of the VLIW processor for the specific VLIW program. The emulating code also includes instrumentation for collecting execution counts of VLIWs, profiling information, and generation of predecoded execution traces. \u2022 A cycle timer, invoked by the emulating code on a VLIW-by-VLIW basis, which processes VLIW execution traces as they are generated. The\u00a0\u2026", "num_citations": "30\n", "authors": ["709"]}
{"title": "Control signal memoization in a multiple instruction issue microprocessor\n", "abstract": " A dynamic predictive and/or exact caching mechanism is provided in various stages of a microprocessor pipeline so that various control signals can be stored and memorized in the course of program execution. Exact control signal vector caching may be done. Whenever an issue group is formed following instruction decode, register renaming, and dependency checking, an encoded copy of the issue group information can be cached under the tag of the leading instruction. The resulting dependency cache or control vector cache can be accessed right at the beginning of the instruction issue logic stage of the microprocessor pipeline the next time the corresponding group of instructions come up for re-execution. Since the encoded issue group bit pattern may be accessed in a single cycle out of the cache, the resulting microprocessor pipeline with this embodiment can be seen as two parallel pipes, where the shorter\u00a0\u2026", "num_citations": "29\n", "authors": ["709"]}
{"title": "High frequency pipeline architecture using the recirculation buffer\n", "abstract": " Traditional processor synchronization methods such as stall and exception handling are quickly becoming bottlenecks for new designs. In particular, both the stall mechanism and the traditional exception handling mechanism are based on the assumption that computation is expensive and communication cheap. As processors move beyond the 1GHz operating frequency, and wire delay dominates transistor switching speeds in new designs, this is no longer the case. As a result, lockstep synchronization mechanisms become increasingly problematic in achieving global synchronization in a processor pipeline.In this paper, we present a high-frequency design called BOA (Binarytranslation and Optimization Architecture). The design is built around two principles. Architecture complexity is handled with aggressive layering based on dynamic compilation techniques. High-frequency is achieved by using a new mechanism for resolving structural and data hazards, and for handling processor exceptions. This approach is based on a streaming model of pipeline execution, where instructions continue to proceed through the pipeline even when a hazard has been detected. Reissue logic then detects such conditions, invalidates instructions which have violated integrity constraints and directs the issue logic to re-issue those instructions.", "num_citations": "29\n", "authors": ["709"]}
{"title": "Optimal modulo scheduling through enumeration\n", "abstract": " Resource-constrained software-pipelining has played an increasingly significant role in exploiting instruction-level parallelism and has drawn intensive academic and industrial interest. The challenge is to find a schedule which is optimal : i.e., given the data dependence graph (DDG) for a loop, find the fastest possible schedule under given resource constraints while keeping register usage minimal. This paper proposes a novel enumeration based modulo scheduling approach to solve this problem. The proposed approach does not require any awkward reworking of constraints into linear form and employs a realistic register model. The set of schedules enumerated also allows us to characterize the schedule space and address questions such as whether schedules using a small number of registers tend to require a large number of function units. The proposed approach has been implemented under the\u00a0\u2026", "num_citations": "29\n", "authors": ["709"]}
{"title": "Transient cache storage with discard function for disposable data\n", "abstract": " A method and apparatus for storing non-critical processor information without imposing significant costs on a processor design is disclosed. Transient data are stored in the processor-local cache hierarchy. An additional control bit forms part of cache addresses, where addresses having the control bit set are designated as \u201ctransient storage addresses.\u201d Transient storage addresses are not written back to external main memory and, when evicted from the last level of cache, are discarded. Preferably, transient storage addresses are \u201cprivileged\u201d in that they are either not accessible to software or only accessible to supervisory or administrator-level software having appropriate permissions. A number of management functions/instructions are provided to allow administrator/supervisor software to manage and/or modify the behavior of transient cache storage. This transient storage scheme allows the cache hierarchy to\u00a0\u2026", "num_citations": "26\n", "authors": ["709"]}
{"title": "Inherently lower complexity architectures using dynamic optimization\n", "abstract": " Based on the conviction that modern superscalar outof-order designs squander useful resources for little incremental gain, the BOA team embarked on a design effort to develop an architecture where computational elements dominated the design. At the same time, we wanted to preserve the ability to adapt to changing workload behavior dynamically, but without the overhead inherent in traditional out-of-order designs. We turned to maturing dynamic compilation technology to achieve dynamic adaptability, while keeping core complexity low.", "num_citations": "26\n", "authors": ["709"]}
{"title": "Simulation and debugging of full system binary translation\n", "abstract": " PowerPC) with bugs possible in each level of translation. As well, the VLIW code is heavily reordered from the original PowerPC code, thus making it difficult to associate a particular VLIW operation with its original PowerPC counterpart. Finally, the simulation runs on the bare hardware of a workstation where there is an element of non-determinism and where incorrect accesses to I/O can crash the system and possibly corrupt the disk. We propose novel solutions for handling these problems.", "num_citations": "26\n", "authors": ["709"]}
{"title": "Full system binary translation: Risc to vliw\n", "abstract": " VLIW processors have traditionally suffered from compatibility problems\u2014both with existing processors and between generations of VLIW processors. Dynamic binary translation as exemplified by our DAISY [8, 9, 10, 12] and Transmeta\u2019s Crusoe [15] provide a way around these problems by using code for an existing processor (PowerPC or x86) as a distribution format, and dynamically translating the code into the native VLIW form used by the underlying machine. In contrast to binary translation approaches such as IBM\u2019s Mimic [14], HP\u2019s HP3000 Emulator [4], Compaq\u2019s FX! 32 [5] and HP\u2019s Dynamo [2], both DAISY and Crusoe make binary translation invisible to the user by emulating the entire processor including \u201chard\u201d system and privileged operations, exceptions, address translation, etc.. Tandem [1] and Apple [17] also perform full system translation like DAISY and Crusoe albeit with two significant differences:(1) Tandem/Apple binary", "num_citations": "26\n", "authors": ["709"]}
{"title": "A register allocation framework based on hierarchical cyclic interval graphs\n", "abstract": " In this paper, we propose the use of cyclic interval graphs as an alternative representation for register allocation. The\" thickness\" of the cyclic interval graph captures the notion of overlap between live ranges of variables relative to each particular point of time in the program execution. We demonstrate that cyclic interval graphs provide a feasible and effective representation that accurately captures the periodic nature of live ranges found in loops. A new heuristic algorithm for minimum register allocation, the fat cover algorithm, has been developed and implemented to exploit such program structure. In addition, a new spilling algorithm is proposed that makes use of the extra information available in the interval graph representation. These two algorithms work together to provide a two-phase register allocation process that does not require iteration of the spilling or coloring phases. We extend the notion of cyclic interval graphs to hierarchical cyclic interval graphs and we...", "num_citations": "25\n", "authors": ["709"]}
{"title": "Efficient instruction scheduling with precise exceptions\n", "abstract": " We describe the SPACE algorithm for translating from one architecture such as PowerPC into operations for another architecture such as VLIW, while also supporting scheduling, register allocation, and other optimizations. Our SPACE algorithm supports precise exceptions, but in an improvement over our previous work, eliminates the need for most hardware register commit operations, which are used to place values in their original program location in the original program sequence. The elimination of commit operations frees issue slots for other computation, a feature that is especially important for narrower machines. The SPACE algorithm is efficient, running in O (N2) time in the number N of operations in the worst case, but in practice is closer to a two-pass O (N) algorithm.", "num_citations": "24\n", "authors": ["709"]}
{"title": "A Novel Methodology Using Genetic Algorithms for the Design of Caches and Cache Replacement Policy.\n", "abstract": " This paper proposes a novel methodology for obtaining processor cache replacement policies based on techniques of genetic algorithms. When the cache is full and new data must be added, some old data must be replaced. Two standard techniques are to replace to Least Recently Used (LRU) data, or to replace the oldest data (FIFO for First-In-First-Out). The key innovation here is the use of adjustable replacement policies based on information from multiple metrics| LRU, FIFO, and others. This adjustability allows the cache to be tuned to particular programs with only a small increase in hardware complexity. Combining information from di erent replacement metrics results in a very large number of new replacement policies, so an automated means of searching for the best combinations is essential, with genetic algorithms tting the bill nicely. By use of simulation, the optimum replacement policy (OPT) can be determined for any given benchmark.(OPT is not physically realizable because it requires knowledge of the future.) Previous researchers have found a gap of about 30% between the performance of LRU and that of OPT. Using our techniques we are able to close approximately 20%-30% of this gap.", "num_citations": "24\n", "authors": ["709"]}
{"title": "Handling permanent and transient errors using a SIMD unit\n", "abstract": " A method for handling permanent and transient errors in a microprocessor is disclosed. The method includes reading a scalar value and a scalar operation from an execution unit of the microprocessor. The method further includes writing a copy of the scalar value into each of a plurality of elements of a vector register of a Single Instruction Multiple Data (SIMD) unit of the microprocessor and executing the scalar operation on each scalar value in each of the plurality of elements of the vector register of the SIMED unit using a vector operation. The method further includes comparing each result of the scalar operation on each scalar value in each of the plurality of elements of the vector register and detecting a permanent or transient error if all of the results are not identical.", "num_citations": "23\n", "authors": ["709"]}
{"title": "Power control of a processor using hardware structures controlled by a compiler with an accumulated instruction profile\n", "abstract": " A microprocessor includes a logic circuit. A selection device is coupled to the logic circuit, and the selection device provides switching of on/off states of the logic circuit based on a stored logical value. A program instruction is included which sets the stored logical value to control the on/off states of the logic circuit based on anticipated usage of the logical circuit in accordance with an instruction sequence of the microprocessor.", "num_citations": "23\n", "authors": ["709"]}
{"title": "On Achieving Precise Exceptions Semantics in Dynamic Optimization\n", "abstract": " Maintaining precise exceptions is an important aspect of achieving full compatibility with a legacy architecture. While asynchronous exceptions can be deferred to an appropriate boundary in the code, synchronous exceptions must be taken when they occur. This introduces uncertainty into liveness analysis since processor state that is otherwise dead may be exposed when an exception handler is invoked. Previous systems either had to sacrifice full compatibility to achieve more freedom to perform optimization, use less aggressive optimization or rely on hardware support. In this work, we demonstrate how aggressive optimization can be used in conjunction with dynamic compilation without the need for specialized hardware. The approach is based on maintaining enough state to recompute the processor state when an unpredicted event such as a synchronous exception may make otherwise dead processor state visible. The transformations necessary to preserve precise exception capability can be performed in linear time.", "num_citations": "23\n", "authors": ["709"]}
{"title": "DAISY/390: Full System Binary Translation of IBM System/390\n", "abstract": " We describe the design issues in an implementation of the ESA/390 architecture based on binary translation to a very long instruction word (VLIW) processor. During binary translation, complex ESA/390 instructions are decomposed into instruction\\primitives\" which are then scheduled onto a wide-issue machine. The aim is to achieve high instruction level parallelism due to the increased scheduling and optimization opportunities which can be exploited by binary translation software, combined with the e ciency of long instruction word architectures. A further aim is to study the feasibility of a common execution platform for di erent instruction set architectures, such as ESA/390, RS/6000, AS/400 and the Java Virtual Machine, so that multiple systems can be built around a common execution platform.", "num_citations": "22\n", "authors": ["709"]}
{"title": "Compiler/architecture interaction in a tree-based VLIW processor\n", "abstract": " This paper describes a compilation and simulation environment designed to explore the interaction among compiler and architecture for the case of a tree-based very-long instruction word (VLIW) processor. The environment is characterized by its flexibility and fast turn-around time, allowing the exploration of architecture/compiler trade-offs in several dimensions over complete execution runs of standard benchmarks. CHAMELEON, our research compiler, uses state-of-the-art optimizing techniques to extract and exploit instructionlevel parallelism. FORESTA, the VLIW architecture, has an instruction set which is based on the PowerPC architecture. Results reported in the paper demonstrate the suitability of the environment for the purposes of evaluating trade-offs; in particular, the interactions arising from the availability of three-input instructions in the architecture are discussed. The exploration of such interactions has led to the development of some novel ideas in the architecture as well as in the compiler.", "num_citations": "19\n", "authors": ["709"]}
{"title": "Non-homogeneous multi-processor system with shared memory\n", "abstract": " A computer architecture and programming model for high speed processing over broadband networks are provided. The architecture employs a consistent modular structure, a common computing module and uniform software cells. The common computing module includes a control processor, a plurality of processing units, a plurality of local memories from which the processing units process programs, a direct memory access controller and a shared main memory. A synchronized system and method for the coordinated reading and writing of data to and from the shared main memory by the processing units also are provided. A hardware sandbox structure is provided for security against the corruption of data among the programs being processed by the processing units. The uniform software cells contain both data and applications and are structured for processing by any of the processors of the network. Each\u00a0\u2026", "num_citations": "18\n", "authors": ["709"]}
{"title": "Extending the number of instruction bits in processors with fixed length instructions, in a manner compatible with existing code\n", "abstract": " This invention pertains to apparatus, method and a computer program stored on a computer readable medium. The computer program includes instructions for use with an instruction unit having a code page, and has computer program code for partitioning the code page into at least two sections for storing in a first section thereof a plurality of instruction words and, in association with at least one instruction word, for storing in a second section thereof an extension to each instruction word in the first section. The computer program further includes computer program code for setting a state of at least one page table entry bit for indicating, on a code page by code page basis, whether the code page is partitioned into the first and second sections for storing instruction words and their extensions, or whether the code page is comprised instead of a single section storing only instruction words.", "num_citations": "18\n", "authors": ["709"]}
{"title": "Limiting entries searched in load reorder queue to between two pointers for match with executing load instruction\n", "abstract": " A method for reducing the number of load instructions in the load reorder queue (LRQ) that are searched when a load instruction is executed by a processor, including dispatching the load instructions; inserting the load instructions in the LRQ in program order; clearing a load received data field; executing the load instructions; checking load reorder queue (LRQ) entries; re-executing the load instruction of the matching LRQ entry; continuing execution; getting the load data; setting the load received data field; comparing a load sequence number (LSQN) of each load instruction to a snoop_safe register contents; ANDing all the load received data bits if the LSQN is greater in magnitude to the snoop_safe; setting the snoop_safe register to the LSQN of the load instruction; searching the LRQ entry; and setting a load_peril_snoop register to the LRQ index value where the first load instruction younger to the snoop_safe\u00a0\u2026", "num_citations": "17\n", "authors": ["709"]}
{"title": "Method and material for treating immune diseases\n", "abstract": " An immune response in an organism is controlled by administering to said organism a therapeutically effective amount of a compound which binds to a galectin. In specific instances the compound is selected to bind to galectin-1 or galectin-3. Some therapeutic materials comprise natural or synthetic polymers having galactose or arabinose terminated side chains pendent therefrom. A group of preferred therapeutic compounds comprise modified pectins or other materials having a substantially demethoxylated rhamnogalacturan backbone having rhamnose residues interrupting the backbone. One therapeutic material includes a first functional portion which binds to the carbohydrate binding portion of a galectin, and a second functional portion which is operable to denature the galectin protein.", "num_citations": "16\n", "authors": ["709"]}
{"title": "Lightweight monitor for Java VM\n", "abstract": " This paper introduces the lightweight monitor in Java VM that is fast on single-threaded programs as well as on multi-threaded programs with little lock contention. A 32-bit lock is embedded into each object for efficient access while the lock queue and the wait set is managed through a hash table. The lock manipulation code is highly optimized and inlined by our Java VM JIT compiler called LaTTe wherever the lock is accessed. In most cases, only 9 SPARC instructions are spent for lock acquisition and 5 instructions for lock release. Our experimental results indicate that the lightweight monitor is faster than the monitor in the latest SUN JDK 1.2 Release Candidate 1 by up to 21 times in the absence of lock contention and by up to 7 times in the presence of lock contention.", "num_citations": "16\n", "authors": ["709"]}
{"title": "Co-scheduling hardware and software pipelines\n", "abstract": " In this paper we propose co-scheduling, a framework for simultaneous design of hardware pipelines structures and software-pipelined schedules. Two important components of the co-scheduling framework are: (1) An extension to the analysis of hardware pipeline design that meets the needs of periodic (or software pipelined) schedules. Reservation tables, forbidden latencies, collision vectors, and state diagrams from classical pipeline theory are revisited and extended to solve the new problems. (2) An efficient method, based on the above extension of pipeline analysis, to perform (a) software pipeline scheduling and (b) hardware pipeline reconfiguration which are mutually \"compatible\". The proposed method has been implemented and preliminary experimental results for 1008 kernel loops are reported. Co-scheduling successfully obtains a schedule for 95% of these loops. The median time to obtain these\u00a0\u2026", "num_citations": "16\n", "authors": ["709"]}
{"title": "A framework for resource-constrained rate-optimal software pipelining\n", "abstract": " In this paper, we propose a novel framework for resource-constrained rate-optimal software pipelining. The distinct feature of our work is that the constructed schedules achieve the maximum computation rate for the given resource constraints. Based on a periodic scheduling framework, we have developed a simple integer linear program formulation for the resource-constrained software pipelining problem. A solution method employs simple and inexpensive heuristics to drastically reduce the search space of the integer programming problem. We have implemented our solution method and applied it on a number of benchmark loops. A desired optimal solution is found within 1 CPU second of execution time in 57%, and within 30 seconds in more than 91% of the test cases. A salient feature of our framework is its generality: it can be applied to either homogeneous or heterogeneous function units, and both\u00a0\u2026", "num_citations": "16\n", "authors": ["709"]}
{"title": "Method and system for maintaining coherency in a multiprocessor system by broadcasting TLB invalidated entry instructions\n", "abstract": " A method and system for attached processing units accessing a shared memory in an SMP system. In one embodiment, a system comprises a shared memory. The system further comprises a plurality of processing elements coupled to the shared memory. Each of the plurality of processing elements comprises a processing unit, a direct memory access controller and a plurality of attached processing units. Each direct memory access controller comprises an address translation mechanism thereby enabling each associated attached processing unit to access the shared memory in a restricted manner without an address translation mechanism. Each attached processing unit is configured to issue a request to an associated direct memory access controller to access the shared memory specifying a range of addresses to be accessed as virtual addresses. The associated direct memory access controller is configured to\u00a0\u2026", "num_citations": "15\n", "authors": ["709"]}
{"title": "Optimization and precise exceptions in dynamic compilation\n", "abstract": " Maintaining precise exceptions is an important aspect of achieving full compatibility with a legacy architecture. While asynchronous exceptions can be deferred to an appropriate boundary in the code, synchronous exceptions must be taken when they occur. This introduces uncertainty into liveness analysis since processor state that is otherwise dead may be exposed when an exception handler is invoked. Previous systems either had to sacrifice full compatibility to achieve more freedom to perform optimization, use less aggressive optimization or rely on hardware support.In this work, we demonstrate how aggressive optimization can be used in conjunction with dynamic compilation without the need for specialized hardware. The approach is based on maintaining enough state to recompute the processor state when an unpredicted event such as a synchronous exception may make otherwise dead processor state\u00a0\u2026", "num_citations": "15\n", "authors": ["709"]}
{"title": "Reducing virtual call overheads in a Java VM just-in-time compiler\n", "abstract": " Java, an object-oriented language, uses virtual methods to support the extension and reuse of classes. Unfortunately, virtual method calls affect performance and thus require an efficient implementation, especially when just-in-time (JIT) compilation is done. Inline caches and type feedback are solutions used by compilers for dynamically-typed object-oriented languages such as SELF [1, 2, 3], where virtual call overheads are much more critical to performance than in Java. With an inline cache, a virtual call that would otherwise have been translated into an indirect jump with two loads is translated into a simpler direct jump with a single compare. With type feedback combined with adaptive compilation, virtual methods can be inlined using checking code which verifies if the target method is equal to the inlined one.This paper evaluates the performance impact of these techniques in an actual Java virtual machine\u00a0\u2026", "num_citations": "15\n", "authors": ["709"]}
{"title": "System and method including distributed instruction buffers for storing frequently executed instructions in predecoded form\n", "abstract": " A system and method is provided for processing a first instruction set and a second instruction set in a single processor. The method includes storing a plurality of instructions of the second instruction set in a plurality of buffers proximate to a plurality of execution units, executing an instruction of the first instruction set in response to a first counter, and executing at least one instruction of the second instruction set in response to at least a second counter, wherein the second counter is invoked by a branch instruction of the first instruction set.", "num_citations": "14\n", "authors": ["709"]}
{"title": "Mechanism and method for two level adaptive trace prediction\n", "abstract": " A trace cache system is provided comprising a trace start address cache for storing trace start addresses with successor trace start addresses, a trace cache for storing traces of instructions executed, a trace history table (THT) for storing trace numbers in rows, a branch history shift register (BHSR) or a trace history shift register (THSR) that stores histories of branches or traces executed, respectively, a THT row selector for selecting a trace number row from the THT, the selection derived from a combination of a trace start address and history information from the BHSR or the THSR, and a trace number selector for selecting a trace number from the selected trace number row and for outputting the selected trace number as a predicted trace number.", "num_citations": "13\n", "authors": ["709"]}
{"title": "Method to reduce the number of times in-flight loads are searched by store instructions in a multi-threaded processor\n", "abstract": " A method for reducing the number of times in-flight loads must be searched by store instructions in a multi-threaded processor. A load issue for a thread t_old is frozen for a number of cycles. A t 13 new load instruction is rejected. A notification is sent to the rest of the processor that the t_new load instruction has been rejected. A load reorder queue (LRQ) of a t_old is snooped for any load which comes from a cache line L accessed by the load instruction and then forces such loads to be re-executed. Ownership of line L is changed to thread t_new.", "num_citations": "11\n", "authors": ["709"]}
{"title": "Means for supporting and tracking a large number of in-flight stores in an out-of-order processor\n", "abstract": " A method for supporting and tracking a plurality of stores in an out-of-order processor run by a predetermined program includes executing a plurality of instructions on the processor, each instruction including an address from which data is to be loaded and a plurality of memory locations from which load data is received, determining inputs of the instructions, determining a function unit on which to execute the instructions; storing the plurality of instructions in both a Retirement Store Queue (RSTQ) and a Forwarding Store Queue (FSTQ), the RSTQ comprising a list of the plurality of stores and the FSTQ comprising a list of respective addresses of the plurality of stores, allowing the plurality of stores to be stored in the plurality of memory locations, and allowing the plurality of stores to forward the load data only after the instructions have determined that the predetermined number of the stores has completed the series of\u00a0\u2026", "num_citations": "11\n", "authors": ["709"]}
{"title": "DAISY dynamic binary translation software\n", "abstract": " DAISY (Dynamically Architected Instruction Set from Yorktown) translates a program at runtime from one instruction set to another in a manner transparent to the user. In other words, the user sees only the input instruction set. This implementation of DAISY uses PowerPC as the input instruction set and a new VLIW architecture as the target instruction set. This version also works on a page-by-page basis, translating all code it can find on the first page of an application program, then simulating this translated code until control passed to a new page, then translating the second page, and so on. Thus, if some code pages are never executed, they are never translated.", "num_citations": "11\n", "authors": ["709"]}
{"title": "Architecture, compiler and simulation of a tree-based VLIW processor\n", "abstract": " This Research Report is available. This report has been submitted for publication outside of IBM and will probably be copyrighted if accepted for publication. It has been issued as a Research Report for early dissemination of its contents. In view of the transfer of copyright to the outside publisher, its distribution outside of IBM prior to publication should be limited to peer communications and specific requests. After outside publication, requests should be filled only by reprints or legally obtained copies of the article (eg, payment of royalties). I have read and understand this notice and am a member of the scientific community outside or inside of IBM seeking a single copy only.", "num_citations": "11\n", "authors": ["709"]}
{"title": "Accepting or rolling back execution of instructions based on comparing predicted and actual dependency control signals\n", "abstract": " A dynamic predictive and/or exact caching mechanism is provided in various stages of a microprocessor pipeline so that various control signals can be stored and memorized in the course of program execution. Exact control signal vector caching may be done. Whenever an issue group is formed following instruction decode, register renaming, and dependency checking, an encoded copy of the issue group information can be cached under the tag of the leading instruction. The resulting dependency cache or control vector cache can be accessed right at the beginning of the instruction issue logic stage of the microprocessor pipeline the next time the corresponding group of instructions come up for re-execution. Since the encoded issue group bit pattern may be accessed in a single cycle out of the cache, the resulting microprocessor pipeline with this embodiment can be seen as two parallel pipes, where the shorter\u00a0\u2026", "num_citations": "10\n", "authors": ["709"]}
{"title": "Observations on tuning a Java enterprise application for performance and scalability\n", "abstract": " Enterprise software shows increasing levels of concurrency and complexity and decreasing think times between user interactions. Such trends are evident in both emerging workloads, such as social networking, and traditional applications, such as banking, for which both query counts and complexity are increasing. Similarly, in today's multicore-processor era, processor core counts double every processor generation. However, not all hardware capacity (e.g., cache, disk, and network capacity) is growing at this core rate. As a result, processor dies will have more cores sharing resources. Personnel associated with our project Multicore Applications Restructured for Scaling started with a well-tuned baseline version of a large multitier commercial workload and worked to efficiently identify a small set of software changes that, together, would lead to improved scaling and performance. This paper reports the required\u00a0\u2026", "num_citations": "10\n", "authors": ["709"]}
{"title": "A Comparative Study of DSP Multiprocessor List Scheduling Heuristics\n", "abstract": " This paper presents a quantitative comparison of a collection of DSP multiprocessor list scheduling heuristics which consider inter-processor communication delays. The following aspects have been addressed:(1) performance in terms of the total execution time (makespan),(2) sensitivity of heuristics in terms of the characteristics of acyclic precedence graphs, including graph size and graph parallelism,(3) sensitivity of heuristics to the number of processors, and (4) compile time eciency. In addition, the e ectiveness of list scheduling performance enhancement techniques is examined.", "num_citations": "10\n", "authors": ["709"]}
{"title": "Dynamically adapting a test workload to accelerate the identification of performance issues\n", "abstract": " An improvement to the process for identifying software problems in performance testing is achieved by dynamically adjusting workloads in real-time to stress the functionality of an application suspected of causing a software problem.", "num_citations": "9\n", "authors": ["709"]}
{"title": "Rule-based adaptive monitoring of application performance\n", "abstract": " A method for dynamically and adaptively monitoring a system based on its running behavior adjusts monitoring levels of the monitored application in real-time. A rules-based mechanism dynamically adjusts monitoring levels in real-time, based on the system's performance observed during a workload run, whether in a production or test environment.", "num_citations": "9\n", "authors": ["709"]}
{"title": "System and method of execution of register pointer instructions ahead of instruction issue\n", "abstract": " A pipeline system and method includes a plurality of operational stages. The stages include a pointer register stage which stores pointer information and updates, and a rename and dependence checking stage located downstream of the pointer register stage, which renames registers and determines if dependencies exist. A functional unit provides pointer information updates to the pointer register stage such that pointer information is processed and updated to the pointer register stage before or in parallel with the register dependency checking.", "num_citations": "8\n", "authors": ["709"]}
{"title": "On-demand translation of Java exception handlers in the LaTTe JVM just-in-time compiler\n", "abstract": " Exceptions are provided by the Java programming language in order to handle errors more gracefully. However, exceptions are rare in most programs, so that most catch blocks are left unused. This paper describes an on-demand catch block translation scheme which translates catch blocks only when they are actually used. By ignoring exception handling while translating normal flow, we can reduce translation overhead and make use of more optimization opportunities. In addition, we directly connect normal flow and catch blocks after an exception is thrown, which results in faster exception handling. This is useful since many exceptions thrown at certain points are usually of the same type and handled by the same catch blocks. From experimental results, we show that the existence of catch blocks indeed do not interfere with the translation of normal flow when on-demand catch block translation is done. Also, the results show that direct connection along with method inlining results...", "num_citations": "8\n", "authors": ["709"]}
{"title": "Genetic algorithms and cache replacement policy\n", "abstract": " ProfesP< Jr Gua. ng R. G8. O'S inspired teaching has stimulated my interest in ma. ny areas of computer architecture, particularly caches. Professor Vinod K. Agarwa.! hu offered numerou& insightf\u2022 suggestions along the way, and has been extremely patient in waiting for the final result. Professor PCP Bhatt'\u2022. review of an early, and none too polished, draCt hali improved this one considerably. AI weIl, the many conversations with my fellow. tudentl helped crystalize severa.! ideas {or me.", "num_citations": "8\n", "authors": ["709"]}
{"title": "An enhanced co-scheduling method using reduced ms-state diagrams\n", "abstract": " Instruction scheduling methods based on the construction of state diagrams (or automata) have been used for architectures involving deeply pipelined function units. However, the size of the state diagram is prohibitively large, resulting in high execution time and space requirement. We present a simple method for reducing the size of the state diagram by recognizing unique paths of a state diagram. Our experiments show that the number of paths in the reduced state diagram is significantly lower-by 1 to 3 orders of magnitude-compared to the number of paths in the original state diagram. Using the reduced MS-state diagrams, we develop an efficient software pipelining method. The proposed software pipelining algorithm produced efficient schedules and performed better than R.A. Huff's (1993) Slack Scheduling method, and the original Co-scheduling method, in terms of both the initiation interval (II) and the time\u00a0\u2026", "num_citations": "7\n", "authors": ["709"]}
{"title": "Method and system for preventing livelock due to competing updates of prediction information\n", "abstract": " A system to prevent livelock. An outcome of an event is predicted to form an event outcome prediction. The event outcome prediction is compared with a correct value for a datum to be accessed. An instruction is appended with a real event outcome when the outcome of the event is mispredicted to form an appended instruction. A prediction override bit is set on the appended instruction. Then, the appended instruction is executed with the real event outcome.", "num_citations": "6\n", "authors": ["709"]}
{"title": "Method and apparatus for eliminating the need for register assignment, allocation, spilling and re-filling\n", "abstract": " A method and apparatus is provided to manage data in computer registers in a program, making more computer registers available to one or more programmers utilizing a name level instruction. The method and apparatus disclosed herein presents a way of reducing the overhead of register management, by introducing a concept of a name level for each of the named architected registers in a processor. The method provides a programmer with a larger register name-space while not increasing the size of the instruction word in the processor instruction-set architecture. It also provides for the facilitation of architectural features which overload the architected register namespace and ease the overhead of register management. This provides for the addition of more computer registers without changing the instruction format of the computer.", "num_citations": "6\n", "authors": ["709"]}
{"title": "Efficient register mapping and allocation in LaTTe, an open-source Java just-in-time compiler\n", "abstract": " Java just-in-time (JIT) compilers improve the performance of a Java virtual machine (JVM) by translating Java bytecode into native machine code on demand. One important problem in Java JIT compilation is how to map stack entries and local variables to registers efficiently and quickly, since register-based computations are much faster than memory-based ones, while JIT compilation overhead is part of the whole running time. This paper introduces LaTTe, an open-source Java JIT compiler that performs fast generation of efficiently register-mapped RISC code. LaTTe first maps \"all\" local variables and stack entries into pseudoregisters, followed by real register allocation which also coalesces copies corresponding to pushes and pops between local variables and stack entries aggressively. Our experimental results indicate that LaTTe's sophisticated register mapping and allocation really pay off, achieving twice the\u00a0\u2026", "num_citations": "6\n", "authors": ["709"]}
{"title": "A unified framework for instruction scheduling and mapping for function units with structural hazards\n", "abstract": " Software pipelining methods based on an ILP (integer linear programming) framework have been successfully applied to derive rate-optimal schedules under resource constraints. However, like many other previous works on software pipelining, ILP-based work has focused on resource constraints of simple function units, e.g., \u201cclean pipelines\u201d\u2014pipelines without structural hazards. The problem for architectures beyond such clean pipelines remains open. One challenge is how to represent such resource constraints for unclean pipelines, i.e., pipelined function units, but having structural hazards.In this paper, we propose a method to constructrate-optimalsoftware pipelined schedules for pipelined architectures with structural hazards. A distinct feature of this work is that it provides a unified ILP framework for two challenging and interrelated aspects of software pipelining\u2014the scheduling of instructions at particular\u00a0\u2026", "num_citations": "6\n", "authors": ["709"]}
{"title": "Optimal software pipelining through enumeration of schedules\n", "abstract": " Resource-constrained software-pipelining has played an increasingly significant role in exploiting instruction-level parallelism and has been drawing intensive academic and industrial interest. The challenge is to find schedule which is optimal: i.e. the fastest possible schedule under given resource constraints while keeping register usage minimal. One interesting problem is open: the design of an algorithm which ensures that such an optimal schedule will always be found and with a cost which is manageable in practice. In this paper, we present a novel modulo scheduling algorithm which provides a solution to this open problem.             The proposed algorithm has been successfully implemented and tested under MOST \u2014 the Modulo Scheduling Testbed developed at McGill University. Unlike the existing optimal modulo scheduling approach based on integer linear programming (ILP) [6], our approach\u00a0\u2026", "num_citations": "6\n", "authors": ["709"]}
{"title": "Dynamically identifying performance anti-patterns\n", "abstract": " Dynamically identifying performance anti-patterns in a software system is based on a set of documented symptoms that are evaluated in real-time. The evaluation is based on the observed system behavior and its comparison against the documented symptoms of different types of performance issues.", "num_citations": "5\n", "authors": ["709"]}
{"title": "VLaTTe: A Java Just-in-Time Compiler for VLIW with Fast Scheduling and Register Allocation\n", "abstract": " For network computing on desktop machines, fast execution of Java bytecode programs is essential because these machines are expected to run substantial application programs written in Java. We believe higher Java performance can be achieved by exploiting instruction-level parallelism (ILP) in the context of Java JIT compilation. This paper introduces VLaTTe, a Java JIT compiler for VLIW machines that performs efficient scheduling while doing fast register allocation. It is an extended version of our previous JIT compiler for RISC machines called LaTTe whose translation overhead is low (i.e., consistently taking one or two seconds for SPECJVM98 benchmarks) due to its fast register allocation. VLaTTe adds the scheduling capability onto the same framework of register allocation, with a constraint for precise in-order exception handling which guarantees the same Java exception behavior with the original\u00a0\u2026", "num_citations": "5\n", "authors": ["709"]}
{"title": "Symmetric multi-processing system\n", "abstract": " A method and system for attached processing units accessing a shared memory in an SMP system. In one embodiment, a system comprises a shared memory. The system further comprises a plurality of processing elements coupled to the shared memory. Each of the plurality of processing elements comprises a processing unit, a direct memory access controller and a plurality of attached processing units. Each direct memory access controller comprises an address translation mechanism thereby enabling each associated attached processing unit to access the shared memory in a restricted manner without an address translation mechanism. Each attached processing unit is configured to issue a request to an associated direct memory access controller to access the shared memory specifying a range of addresses to be accessed as virtual addresses. The associated direct memory access controller is configured to\u00a0\u2026", "num_citations": "5\n", "authors": ["709"]}
{"title": "Towards an automated approach to use expert systems in the performance testing of distributed systems\n", "abstract": " Performance testing in distributed environments is challenging. Specifically, the identification of performance issues and their root causes are time-consuming and complex tasks which heavily rely on expertise. To simplify these tasks, many researchers have been developing tools with built-in expertise. However limitations exist in these tools, such as managing huge volumes of distributed data, that prevent their efficient usage for performance testing of highly distributed environments. To address these limitations, this paper presents an adaptive framework to automate the usage of expert systems in performance testing. Our validation assessed the accuracy of the framework and the time savings that it brings to testers. The results proved the benefits of the framework by achieving a significant decrease in the time invested in performance analysis and testing.", "num_citations": "4\n", "authors": ["709"]}
{"title": "The Postprandial Effects of a Moderately High-Fat Meal on Lipid Profiles and Vascular Inflammation in Alzheimer\u2019s Disease Patients: A Pilot Study\n", "abstract": " ObjectiveAlzheimer\u2019s disease (AD) is a neurodegenerative disease of aging with unknown causative factors. Accumulating evidence suggests that inflammation and neurovascular dysfunction play important roles in AD. The postprandial period following a moderately high-fat meal is associated with vascular inflammation in young, healthy individuals; however, this relationship has not been investigated in Alzheimer\u2019s patients despite their exaggerated inflammatory state.MethodsPatients with AD and age-matched control subjects were recruited through the UC Davis Alzheimer\u2019s Disease Center. All subjects consumed a moderately high-fat breakfast meal. Fasting and postprandial blood samples were collected for lipid, lipoprotein, and oxylipin analyses, as well as assays for cytokine levels and monocyte activation.ResultsThe plasma lipid analyses revealed similar levels of triglycerides and esterified oxylipins\u00a0\u2026", "num_citations": "4\n", "authors": ["709"]}
{"title": "A theory for software-hardware co-scheduling for ASIPs and embedded processors\n", "abstract": " Exploiting instruction-level parallelism (ILP) is extremely important for achieving high performance in application specific instruction set processors (ASIPs) and embedded processors. Existing techniques deal with either scheduling hardware pipelines to obtain higher throughput or software pipeline-an instruction scheduling technique for iterative computation-loops for exploiting greater ILP. We integrate these techniques to co-schedule hardware and software pipelines to achieve greater instruction throughput. In this paper, we develop the underlying theory of co-scheduling, called the Modulo-Scheduled Pipeline (or MS-Pipeline) theory. More specifically, we establish the necessary and sufficient condition for achieving the maximum throughput in a given pipeline operating under module scheduling. Further, we establish a sufficient condition to achieve a specified throughput, based on which we also develop a\u00a0\u2026", "num_citations": "4\n", "authors": ["709"]}
{"title": "Dynamic and Transparent Binary Translation\n", "abstract": " BOA's dynamic optimization offers significant advantages over purely static compilation approaches like those Intel and Hewlett-Packard currently propose for the IA-64 architecture.", "num_citations": "4\n", "authors": ["709"]}
{"title": "Enhanced co-scheduling: A software pipelining method using modulo-scheduled pipeline theory\n", "abstract": " Instruction scheduling methods which use the concepts developed by the classical pipeline theory have been proposed for architectures involving deeply pipelined function units. These methods rely on the construction of state diagrams (or automatons) to (i) efficiently represent the complex resource usage pattern; and (ii) analyze legal initiation sequences, i.e., those which do not cause a structural hazard. In this paper, we propose a state-diagram based approach for modulo scheduling or software pipelining, an instruction scheduling method for loops. Our approach adapts the classical pipeline theory for modulo scheduling, and, hence, the resulting theory is called Modulo-Scheduled pipeline (MS-pipeline) theory. The state diagram, called the Modulo-Scheduled (MS) state diagram is helpful in identifying legal initiation or latency sequences, that improve the number of instructions initiated in a pipeline\u00a0\u2026", "num_citations": "4\n", "authors": ["709"]}
{"title": "Tabular transformers for modeling multivariate time series\n", "abstract": " Tabular datasets are ubiquitous in data science applications. Given their importance, it seems natural to apply state-of-the-art deep learning algorithms in order to fully unlock their potential. Here we propose neural network models that represent tabular time series that can optionally leverage their hierarchical structure. This results in two architectures for tabular time series: one for learning representations that is analogous to BERT and can be pre-trained end-to-end and used in downstream tasks, and one that is akin to GPT and can be used for generation of realistic synthetic tabular sequences. We demonstrate our models on two datasets: a synthetic credit card transaction dataset, where the learned representations are used for fraud detection and synthetic data generation, and on a real pollution dataset, where the learned encodings are used to predict atmospheric pollutant concentrations. Code and data are\u00a0\u2026", "num_citations": "3\n", "authors": ["709"]}
{"title": "Synthesizing credit card transactions\n", "abstract": " Two elements have been essential to AI's recent boom: (1) deep neural nets and the theory and practice behind them; and (2) cloud computing with its abundant labeled data and large computing resources. Abundant labeled data is available for key domains such as images, speech, natural language processing, and recommendation engines. However, there are many other domains where such data is not available, or access to it is highly restricted for privacy reasons, as with health and financial data. Even when abundant data is available, it is often not labeled. Doing such labeling is labor-intensive and non-scalable. As a result, to the best of our knowledge, key domains still lack labeled data or have at most toy data; or the synthetic data must have access to real data from which it can mimic new data. This paper outlines work to generate realistic synthetic data for an important domain: credit card transactions. Some challenges: there are many patterns and correlations in real purchases. There are millions of merchants and innumerable locations. Those merchants offer a wide variety of goods. Who shops where and when? How much do people pay? What is a realistic fraudulent transaction? We use a mixture of technical approaches and domain knowledge including mechanics of credit card processing, a broad set of consumer domains: electronics, clothing, hair styling, etc. Connecting everything is a virtual world. This paper outlines some of our key techniques and provides evidence that the data generated is indeed realistic. Beyond the scope of this paper: (1) use of our data to develop and train models to predict fraud; (2) coupling models\u00a0\u2026", "num_citations": "3\n", "authors": ["709"]}
{"title": "Big Data and democratization\n", "abstract": " ...... Welcome to IEEE Micro\u2019s special issue on big data. As the Google Analytics graph shows (see Figure 1), interest in big data has exploded in the last three years. I thank Guest Editors Babak Falsafi and Boris Grot for initiating this topic and keeping reviewing and other matters on track with IEEE Micro\u2019s schedule. Indeed, the submission deadline for this issue was 15 January 2014, meaning approximately six months have elapsed from submission to publication. This fast turnaround time is typical for IEEE Micro, underscoring that the magazine provides the same quick dissemination of results as most conferences. However, most importantly I thank the guest editors for recruiting an excellent set of articles that should help continue the big-data momentum. This issue\u2019s articles cover a wide range of topics, and illustrate several issues of increasing importance. For example, application knowledge seems increasingly important for computer architecture\u2014the articles in this issue study, among other things, graph databases, text analytics, and map-reduce. As a result, these articles are also noticeably spare on the use of traditional benchmarks like SPEC, an interesting development and one that underscores the exciting and broad set of ideas currently being pursued in computer architecture. Indeed, the articles cover a broad range of architectures from commodity clusters to near-data processing (NDP) to field-programmable gate arrays (FPGAs), the latter reinforcing the reconfigurable computing theme of our January/February 2014 issue. Actually, like many computational uses of FPGAs, the article \u201cGiving Text Analytics a Boost\u201d by Raphael Polig et\u00a0\u2026", "num_citations": "3\n", "authors": ["709"]}
{"title": "Method and system for efficient emulation of multiprocessor address translation on a multiprocessor\n", "abstract": " A method (and structure) of mapping a memory addressing of a multiprocessing system when it is emulated using a virtual memory addressing of another multiprocessing system includes accessing a local lookaside table (LLT) on a target processor with a target virtual memory address. Whether there is a \u201cmiss\u201d in the LLT is determined and, with the miss determined in the LLT, a lock for a global page table is obtained.", "num_citations": "2\n", "authors": ["709"]}
{"title": "Hot chips and other themes\n", "abstract": " ...... Welcome to IEEE Micro\u2019s annual Hot Chips issue. We have five articles based on presentations made last August at the Hot Chips conference. Guest Editors Donald Newell and Samuel Naffziger have pulled together an interesting and eclectic collection (with some early assistance from Michael J. Flynn). One theme in these Hot Chips articles is systems on a chip (SoCs). The AMD Kabini article (p. 22) and Xbox One article (p. 44) feature SoC in the title. The Qualcomm Hexagon article (p. 34) discusses a digital-signal processor (DSP) in a system setting. Even the Haswell article (p. 6) has an SoC theme of sorts with the first fully integrated on-chip voltage regulator. Finally, David Kidd\u2019s article (p. 54) on process and circuit optimization picks up this voltage angle with a discussion of how SuVolta\u2019s Deeply Depleted Channel technology allows lower operating voltages. I hope you will find these articles interesting and informative. IEEE Micro is tackling several other themes over the coming year, with issues and articles on Top Picks, big data, harsh chips, novel architectures for high-speed datacenter interconnects, and mobile systems. Submissions have closed for most of these themes. However, the mobile systems issue is open", "num_citations": "2\n", "authors": ["709"]}
{"title": "Hot chips and the incomplete job of exploiting them\n", "abstract": " This column discusses the problem of integrating the software-hardware stack and considers some steps that microarchitecture community can take to optimize workloads across the stack.", "num_citations": "2\n", "authors": ["709"]}
{"title": "The Odd couple: hardware and software\n", "abstract": " This column discusses issues in software performance and communication between hardware and software. It also discusses two departures from IEEE Micro's editorial board and the introduction of a new associate editor in chief.", "num_citations": "2\n", "authors": ["709"]}
{"title": "CPUs and GPUs: Who owns the future?\n", "abstract": " This column addresses issues facing CPUs, GPUs, and how to program them and other computing devices.", "num_citations": "2\n", "authors": ["709"]}
{"title": "Hot Chips and remembering a pioneer\n", "abstract": " This column briefly introduces the issue's selections from the 22nd annual Hot Chips conference. It also discusses the IEEE's new B. Ramakrishna Rau Award and solicits appropriate nominations.", "num_citations": "2\n", "authors": ["709"]}
{"title": "Means for supporting and tracking a large number of in-flight loads in an out-of-order processor\n", "abstract": " A method for supporting and tracking a plurality of loads in an out-of-order processor being run by a program includes executing instructions on the processor, the instructions including an address from which data is to be loaded and memory locations from which load data is received, determining inputs of the instructions, determining a function unit on which to execute the instructions, storing the plurality of instructions in both a LRQ and a LIP queue, the LRQ comprising a list of the plurality of stores and the LIP comprising a list of respective addresses of the plurality of loads, dividing the LIP into a set of congruence classes, each holding a predetermined number of the loads, allowing the loads to be stored in the memory locations, snooping the load data, and allowing a plurality of snoops to selectively invalidate the load data from snooped addresses so as to maintain sequential load consistency.", "num_citations": "2\n", "authors": ["709"]}
{"title": "Method and apparatus for reducing encoding needs and ports to shared resources in a processor\n", "abstract": " The present invention relates to a method for accessing elements from a shared resource to be used by consumers that perform actions according to corresponding operations. The method creates a packet of operations to be processed simultaneously, wherein the elements from the shared resource used by the operations are specified by source and destination identifier fields that are shared among the operations in such a way that the sum of all the elements from the shared resource used by the operations does not exceed a total number of identifiers available in the packet. The method also reads the elements from the shared resource according to the shared identifier fields specified in the packet. The method decodes a number of elements from the shared resource needed by each operation, by passing the operations to an operation decoder having a defined routing scheme based on the needs of the\u00a0\u2026", "num_citations": "2\n", "authors": ["709"]}
{"title": "Precise exceptions in dynamic compilation\n", "abstract": " Precise Exceptions in Dynamic Compilation (CC 2002) Page 1 Precise Exceptions in Dynamic Compilation Michael Gschwind Erik Altman IBM TJ Watson Research Center Yorktown Heights, NY Page 2 Motivation Dynamic compilation techniques offer runtime feedback for optimization increased code density binary translation to new host architecture Dynamic compilation should not change program semantics at every point in program execution, observable state should be the same change architectural guarantees precise exception behavior should be maintained Page 3 Example Code Sequence (1) add r4,r3,r4 #DEAD! (1) add r4,r3,r4 #DEAD! (2) lwz r3,0(r9) (3) add r4,r3,r3 But a page fault at (2) lwz makes the dead value of r4 visible to the exception handler. If the handler bases any actions on the value of r4, the program may fail. Motivating Example Page 4 Prior Art Severely restrict dead code elimination \u2026", "num_citations": "2\n", "authors": ["709"]}
{"title": "Harsh Chips, but a Grateful Good-Bye\n", "abstract": " This column discusses themes in the current issue, including articles on cool chips and harsh chips; reflects back on the editor in chief's tenure; and looks to the future.", "num_citations": "1\n", "authors": ["709"]}
{"title": "Patents and High-Speed Datacenter Interconnects\n", "abstract": " ...... This issue of IEEE Micro deals with the increasingly important issue of novel architectures for high-speed datacenter interconnects. As Figure 1 shows, the bandwidth of such interconnects has been increasing at Moore\u2019s law rates\u2014doubling every 18 months, which is faster than the 24-month doubling for server I/Os. Server I/Os in turn are growing differently than core counts, and these changes in relative system performance impact microarchitects in terms of things like cache hierarchy, and even how cheap it is to use remotely computed results, which in turn impacts how important it is for a processor to change contexts quickly. Interconnect design also impacts what computation is done on mobile CPUs versus CPUs in a cloud datacenter, with resulting microarchitectural impacts on each CPU type. These indirect impacts are reminiscent of the impacts of pervasive GPU presence on CPU design, outlined in the article \u201cRedefining the Role of the CPU in the Era of CPU-GPU Integration,\u201d by Arora, Nath, Mazumdar, Baden, and Tullsen. 1I thank Guest Editors George Porter, Alex C. Snoeren, and George Papen from the University of California, San Diego, for their excellent efforts in recruiting and leading reviews for the large set of papers from which we selected the five articles in this issue. These five articles cover several key topics for high-speed datacenter interconnects. Two articles present results", "num_citations": "1\n", "authors": ["709"]}
{"title": "Reconfigurable computing, 3D integration, and recognizing leaders in our field\n", "abstract": " This column discusses the special issue on reconfigurable computing, Micro's current calls for papers, topics from the recent business meeting, and recent computer architecture award winners.", "num_citations": "1\n", "authors": ["709"]}
{"title": "Cool chips, mobile devices, memory, and IEEE Micro going digital\n", "abstract": " ...... As has been the case in recent years, this November/December issue of IEEE Micro has an assortment of articles on different topics without a single overarching theme, as is the case for most other issues. However, we do have three articles in a special Cool Chips section from guest editors Makoto Ikeda and Fumio Arakawa, who have done a fine job in recruiting, culling, and editing these selections. With strict power constraints on nearly all systems, these Cool Chips articles span an increasingly wide range of topics, from low-power inductive coupling for 3D stacking to Fujitsu\u2019s latest Sparc X server processor to a more traditional article on LTE-capable mobile processors.Although not technically in the Cool Chips section, some of the other articles in this issue also fit the theme. Rumi Zahir et al. provide an interesting look at the first Intel smartphone platform in \u201cThe Medfield Smartphone: Intel Architecture in a Handheld Form Factor.\u201d On a communication theme, Todor Cooklev and Akinori Nishihara offer \u201cAn Open RF-Digital Interface for Software-Defined Radios.\u201d The final two articles in this issue are of a memory theme, with", "num_citations": "1\n", "authors": ["709"]}
{"title": "Dark silicon and dangerous predictions\n", "abstract": " ....... This issue features several articles exploring dark or dim silicon:\u2018\u2018dark silicon\u2019\u2019referring to the observation that power limitations may prevent all silicon on a chip from being used simultaneously, and hence require that some fraction of future chip area be unpowered or underpowered at all times; and \u2018\u2018dim silicon\u2019\u2019referring to the need to always keep some or all of the chip below peak power levels. We already see abundant precursors of dark silicon in existing systems, including smartphones and tablets that mostly sleep, sleeping processors in appliances and parked cars, and traditional computing systems from desktops to clouds when not at peak load. The articles in this issue explore many key issues in future dark silicon systems, from a discussion of dark silicon trade-offs to near and subthreshold computing to computational sprinting and more. I thank Guest Editors Michael B. Taylor and Steve Swanson, who have done a fine job recruiting and editing this set of articles. I also note that the article authored by Taylor was handled under a completely separate review process, with editors and reviewers anonymous to him.", "num_citations": "1\n", "authors": ["709"]}
{"title": "Reliability, theme issues, and plagiarism\n", "abstract": " ....... This issue addresses reliability, a topic of increasing importance. As a testament to that fact, we received a very large number of submissions and had to reject many strong papers. Guest Editor Vijay Janapa Reddi has done an excellent job in recruiting these many papers, and in organizing a fair and careful reviewing process. Reliability has always been a design issue, but its import is increasing as lithographies continue to shrink, we add complexity such as new memory types and 3D stacking, and computing enters more and more environments, and more and more challenging environments in terms of voltage, temperature, power, and other factors. To complicate matters further, errors can be hard or soft and can exist from initial chip and system creation or occur later. The articles in this issue cover these issues and more. I hope that you will both learn and find useful elements for your own endeavors.As with all articles published in IEEE Micro, the articles in this issue contain significant new research material. IEEE Micro requires at least 30 percent new material over any previous publication, such as a conference publication on related material. This requirement is in keeping both with overall IEEE policy and IEEE Micro\u2019s mission to showcase important new research for the community. I should also note that the amount of new material needed remains a", "num_citations": "1\n", "authors": ["709"]}
{"title": "Ten Years of Top Picks\n", "abstract": " ....... This issue marks our tenth anniversary edition of Top Picks articles from the major microarchitecture-related conferences, a tradition started in 2003 by Pradip Bose at the start of his tenure as Editor in Chief, and an issue that I think has only grown in importance in the intervening years. Over the years, Top Picks published many great articles. This year\u2019s edition contains 11 more, and I thank Babak Falsafi and Gabriel H. Loh for their excellent work in putting this issue together\u2014from recruiting an outstanding program committee to maintaining a clear and fair process to keeping the issue on time. I also thank the authors of the 78 submissions, even though the high quality made the program committee\u2019s work ever the more challenging. Since this issue marks a tenth anniversary, I think it is interesting to compare the set of articles selected for this year\u2019s issue versus those published in 2003. In their Guest Editors\u2019 Introduction, Babak and Gabe group this year\u2019s articles in three categories: energy and efficiency, safety and security, and parallelism and memory. However, to compare to 2003, Table 1 shows a slightly finer set of categories with article count breakdowns in 2003 and 2013. Note that there were 15 articles in 2003 and 11 this year. Some articles fall in multiple categories, so the counts in Table 1 do not sum to the total number of articles.I think a few broad trends are discernible: energy stays important, accelerators and security increase, single-core performance decreases, and parallelism extraction decreases. This last trend seems surprising given the continuing importance and work on this topic, and I think likely represents a statistical\u00a0\u2026", "num_citations": "1\n", "authors": ["709"]}
{"title": "Optical interconnects and their implications\n", "abstract": " Micro-45, Charles Webb said that I/O is a key issue for performance and that optical interconnect is attractive. Alas, he also cautioned that optical interconnect is always two generations away. This issue of IEEE Micro digs deep into the underlying technologies for optical interconnect to give a better understanding of what is possible and what still holds us back. Guest Editors Jeffrey Kash and Raymond Beausoleil have done a fine job of recruiting papers that span the broad range of issues with optical interconnect.I am optimistic, so I think it is also useful to consider what we would do if optical interconnect is indeed available and practical in two generations? One result is that I/O and memory bus power would likely be reduced significantly, allowing us to either use that budget for other things or to reduce overall system power requirements. Since frequency has been power limited for many years, could we see a one-time bump in processor frequencies? Database performance is highly dependent on fast I/O. Historically, much of that has been to and from disk. However, a combination of optical interconnects and flash memory could make databases compute bound instead of I/O bound. Would transactional memory take on vastly improved importance as a direct implementation mechanism for transactions in OLTP (online transaction processing), thus exploiting the new transactional capabilities appearing in a number of recent processors? Would new in-memory databases take on new importance?", "num_citations": "1\n", "authors": ["709"]}
{"title": "Which Way Microarchitecture?\n", "abstract": " This column discusses topics of interest to the microarchitecture community, judging by Micro conference paper session topics and IEEE Micro issue themes. It also previews the two Cool Chips articles in the issue and the cover article on CPU-GPU integration.", "num_citations": "1\n", "authors": ["709"]}
{"title": "Top picks, columnists, and artists\n", "abstract": " This column discusses the process of choosing articles from the computer architecture conferences of 2011 for the Top Picks issue. It also acknowledges the 100th column of Micro Economics and the new cover artist for IEEE Micro.", "num_citations": "1\n", "authors": ["709"]}
{"title": "Micro evolution\n", "abstract": " This column discusses changes to IEEE Micro, including possible future developments such as an all-digital format and broadening the magazine's scope.", "num_citations": "1\n", "authors": ["709"]}
{"title": "Hot Interconnects and hot topics\n", "abstract": " This column discusses hot interconnects and the three keynote speeches from the Micro 2011 conference. It also introduces a new Editorial Board member.", "num_citations": "1\n", "authors": ["709"]}
{"title": "New blood, cool chips, and heterogeneous designs\n", "abstract": " This column discusses cool chips, and their place in the history of computing and current heterogeneous systems. This column also introduces the new Editorial Board members.", "num_citations": "1\n", "authors": ["709"]}
{"title": "Big Chips and Beyond\n", "abstract": " This column addresses the issues facing microarchitecture and processor design, including and beyond big chips.", "num_citations": "1\n", "authors": ["709"]}
{"title": "Very Large-Scale Systems and Some History\n", "abstract": " ....... This issue marks the first special issue developed under my editorship. Guest editors Natalie Enright Jerger and Mikko Lipasti have done a fine job recruiting five excellent articles on the theme of systems for very large-scale computing. We are seeing a proliferation of styles for very large-scale computing\u2014from data centers for commercial computing to highly distributed efforts like SETI@ home or ProteinFolding@ home, and from systems composed of highperformance processors like Blue Waters to systems relying on GPUs for most of their performance, like the Tianhe-1A system (ranked# 1 in the Top500 as I write this), to specialized systems like DE Shaw\u2019s Anton system (see page 8). With so much variation and flux, it is important to study such systems so we can understand how best to proceed with future systems.", "num_citations": "1\n", "authors": ["709"]}
{"title": "A solid past, a vital future\n", "abstract": " The new Editor in Chief of IEEE Micro introduces himself and the first issue of 2011. He thanks the outgoing Editor in Chief, David Albonesi, for his outstanding work during his tenure. He discusses developments to the magazine and future issues, and asks readers for their suggestions on topics the magazine should cover in coming issues.", "num_citations": "1\n", "authors": ["709"]}
{"title": "Method and apparatus to extend the number of instruction bits in processors with fixed length instructions, in a manner compatible with existing code\n", "abstract": " This invention pertains to apparatus, method and a computer program stored on a computer readable medium. The computer program includes instructions for use with an instruction unit having a code page, and has computer program code for partitioning the code page into at least two sections for storing in a first section thereof a plurality of instruction words and, in association with at least one instruction word, for storing in a second section thereof an extension to each instruction word in the first section. The computer program further includes computer program code for setting a state of at least one page table entry bit for indicating, on a code page by code page basis, whether the code page is partitioned into the first and second sections for storing instruction words and their extensions, or whether the code page is comprised instead of a single section storing only instruction words.", "num_citations": "1\n", "authors": ["709"]}
{"title": "Dynamic binary translation and optimization\n", "abstract": " Dynamic Binary Translation and Optimization Erik R. Altman Kemal Ebcioglu IBM TJ Watson Research Center Page 1 Dynamic Binary Translation and Optimization Erik R. Altman Kemal Ebcioglu IBM TJ Watson Research Center Micro-33 December 13, 2000 Page 2 Timetable for Micro-33 Tutorial on Dynamic Binary Translation and Optimization Wednesday, December 13, 2000 2:30 - 2:50 Kemal Ebcioglu: Future Challenges 2:50 - 2:55 Erik Altman: DAISY Demo 2:55 - 3:20 Erik Altman: Binary Translation Issues 3:20 - 3:35 Break 3:35 - 5:00 Erik Altman DAISY, Crusoe, Dynamo 5:00 - 5:15 Break 5:15 - 5:45 Kemal Ebcioglu: LaTTe Page 3 IBM DAISY Page 4 DAISY Schematic PowerPC Software AIX Applications VLIW Machine AIX DAISY Page 5 DAISY Memory Map o Side Tables o System Software o Translated Code o Translator PowerPC Memory DAISY Memory Page 6 DAISY System PowerPC Controller \u2026", "num_citations": "1\n", "authors": ["709"]}
{"title": "An experimental study of an ILP-based exact solution method for software pipelining\n", "abstract": " Software pipelining has been widely accepted as an efficient technique for scheduling instructions in a loop body for VLIW and Superscalarprocessors. Several software pipelining methods based on heuristic approachs have been proposed in the literature. Mathematical formulations based on integer linear programming (ILP) to obtain rateoptimal schedules are also becoming popular. We term formulations such as ILP exact to indicate that they solve a precisely stated optimality problem. By contrast, we term what are generally called \u201cheuristic\u201d methods inexact since they do not guarantee optimality. We do not use the term heuristic, because various heuristics can also be used to guide approaches such as ILP\u2014 without losing any optimality.             In this paper we compare our software pipelining method based on the ILP with three inexact methods. These software pipelining methods are applied to 1008\u00a0\u2026", "num_citations": "1\n", "authors": ["709"]}