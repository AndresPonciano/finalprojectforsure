{"title": "The limitations of deep learning in adversarial settings\n", "abstract": " Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample. We then\u00a0\u2026", "num_citations": "2778\n", "authors": ["1575"]}
{"title": "Practical black-box attacks against machine learning\n", "abstract": " Machine learning (ML) models, eg, deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they\u00a0\u2026", "num_citations": "2251\n", "authors": ["1575"]}
{"title": "Distillation as a defense to adversarial perturbations against deep neural networks\n", "abstract": " Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when\u00a0\u2026", "num_citations": "2217\n", "authors": ["1575"]}
{"title": "Model inversion attacks that exploit confidence information and basic countermeasures\n", "abstract": " Machine-learning (ML) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classifiers in personalized medicine by Fredrikson et al., adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown. We develop a new class of model inversion attack that exploits confidence values revealed along with predictions. Our new attacks are applicable in a variety of settings, and we explore two in depth: decision trees for lifestyle surveys as used on machine-learning-as-a-service systems and neural networks for facial recognition. In both cases confidence values are revealed to those with the ability to make prediction queries to\u00a0\u2026", "num_citations": "1327\n", "authors": ["1575"]}
{"title": "Static analysis of executables to detect malicious patterns\n", "abstract": " Malicious code detection is a crucial component of any defense mechanism. In this paper, we present a unique viewpoint on malicious code detection. We regard malicious code detection as an obfuscation-deobfuscation game between malicious code writers and researchers working on malicious code detection. Malicious code writers attempt to obfuscate the malicious code to subvert the malicious code detectors, such as anti-virus software. We tested the resilience of three commercial virus scanners against code-obfuscation attacks. The results were surprising: the three commercial virus scanners could be subverted by very simple obfuscation transformations! We present an architecture for detecting malicious patterns in executables that is resilient to common obfuscation transformations. Experimental results demonstrate the efficacy of our prototype tool, SAFE (a static analyzer for executables).", "num_citations": "956\n", "authors": ["1575"]}
{"title": "Mining specifications of malicious behavior\n", "abstract": " Malware detectors require a specification of malicious behavior. Typically, these specifications are manually constructed by investigating known malware. We present an automatic technique to overcome this laborious manual process. Our technique derives such a specification by comparing the execution behavior of a known malware against the execution behaviors of a set of benign programs. In other words, we mine the malicious behavior present in a known malware that is not present in a set of benign programs. The output of our algorithm can be used by malware detectors to detect malware variants. Since our algorithm provides a succinct description of malicious behavior present in a malware, it can also be used by security analysts for understanding the malware. We have implemented a prototype based on our algorithm and tested it on several malware programs. Experimental results obtained from our\u00a0\u2026", "num_citations": "593\n", "authors": ["1575"]}
{"title": "Privacy in pharmacogenetics: An end-to-end case study of personalized warfarin dosing\n", "abstract": " We initiate the study of privacy in pharmacogenetics, wherein machine learning models are used to guide medical treatments based on a patient\u2019s genotype and background. Performing an in-depth case study on privacy in personalized warfarin dosing, we show that suggested models carry privacy risks, in particular because attackers can perform what we call model inversion: an attacker, given the model and some demographic information about a patient, can predict the patient\u2019s genetic markers.", "num_citations": "547\n", "authors": ["1575"]}
{"title": "Testing malware detectors\n", "abstract": " In today's interconnected world, malware, such as worms and viruses, can cause havoc. A malware detector (commonly known as virus scanner) attempts to identify malware. In spite of the importance of malware detectors, there is a dearth of testing techniques for evaluating them. We present a technique based on program obfuscation for generating tests for malware detectors. Our technique is geared towards evaluating the resilience of malware detectors to various obfuscation transformations commonly used by hackers to disguise malware. We also demonstrate that a hacker can leverage a malware detector's weakness in handling obfuscation transformations and can extract the signature used by a detector for a specific malware. We evaluate three widely-used commercial virus scanners using our techniques and discover that the resilience of these scanners to various obfuscations is very poor.", "num_citations": "446\n", "authors": ["1575"]}
{"title": "Global intrusion detection in the domino overlay system\n", "abstract": " Sharing data between widely distributed intrusion detection systems offers the possibility of significant improvements in speed and accuracy over systems operating in isolation. In this paper, we describe and evaluate DOMINO (Distributed Overlay for Monitoring InterNet Outbreaks); an architecture for a distributed intrusion detection system that fosters collaboration among heterogeneous nodes organized as an overlay network. The overlay design enables DOMINO to be heterogeneous, scalable, and robust to attacks and failures. An important component of DOMINO's design is the use of tarpit nodes which respond to and measure connections on unused IP addresses. This enables efficient detection of attacks from spoofed IP sources, reduces false positives, enables attack classification and production of timely blacklists. We evaluate the capabilities and performance of DOMINO using a large set of intrusion fogs collected from over 1600 providers across the Internet. Our analysis demonstrates the significant marginal benefit obtained from distributed intrusion data sources coordinated through a system like DOMINO. We also evaluate how to configure DOMINO in order to maximize performance gains from the perspectives of blacklist length, blacklist freshness and IP proximity. We perform a retrospective analysis on the 2002 SQL-Snake and 2003 SQL-Slammer epidemics that highlights how information exchange through DOMINO would reduce reaction time and false alarm rates during outbreaks. Finally, we provide preliminary results from our prototype tarpit deployment that illustrates the limited variability in the tarpit traffic and the feasibility of\u00a0\u2026", "num_citations": "415\n", "authors": ["1575"]}
{"title": "Towards automatic generation of vulnerability-based signatures\n", "abstract": " In this paper we explore the problem of creating vulnerability signatures. A vulnerability signature matches all exploits of a given vulnerability, even polymorphic or metamorphic variants. Our work departs from previous approaches by focusing on the semantics of the program and vulnerability exercised by a sample exploit instead of the semantics or syntax of the exploit itself. We show the semantics of a vulnerability define a language which contains all and only those inputs that exploit the vulnerability. A vulnerability signature is a representation (e.g., a regular expression) of the vulnerability language. Unlike exploit-based signatures whose error rate can only be empirically measured for known test cases, the quality of a vulnerability signature can be formally quantified for all possible inputs. We provide a formal definition of a vulnerability signature and investigate the computational complexity of creating and\u00a0\u2026", "num_citations": "394\n", "authors": ["1575"]}
{"title": "Deflating the big bang: fast and scalable deep packet inspection with extended finite automata\n", "abstract": " Deep packet inspection is playing an increasingly important role in the design of novel network services. Regular expressions are the language of choice for writing signatures, but standard DFA or NFA representations are unsuitable for high-speed environments, requiring too much memory, too much time, or too much per-flow state. DFAs are fast and can be readily combined, but doing so often leads to state-space explosion. NFAs, while small, require large per-flow state and are slow.", "num_citations": "341\n", "authors": ["1575"]}
{"title": "Privacy risk in machine learning: Analyzing the connection to overfitting\n", "abstract": " Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that\u00a0\u2026", "num_citations": "335\n", "authors": ["1575"]}
{"title": "Omniunpack: Fast, generic, and safe unpacking of malware\n", "abstract": " Malicious software (or malware) has become a growing threat as malware writers have learned that signature- based detectors can be easily evaded by \"packing\" the malicious payload in layers of compression or encryption. State-of-the-art malware detectors have adopted both static and dynamic techniques to recover the pay- load of packed malware, but unfortunately such techniques are highly ineffective. In this paper we propose a new technique, called OmniUnpack, to monitor the execution of a program in real-time and to detect when the program has removed the various layers of packing. OmniUnpack aids malware detection by directly providing to the detector the unpacked malicious payload. Experimental results demonstrate the effectiveness of our approach. OmniUnpack is able to deal with both known and unknown packing algorithms and introduces a low overhead (at most 11% for packed benign\u00a0\u2026", "num_citations": "320\n", "authors": ["1575"]}
{"title": "A goal-based approach to policy refinement\n", "abstract": " As the interest in using policy-based approaches for systems management grows, it is becoming increasingly important to develop methods for performing analysis and refinement of policy specifications. Although this is an area that researchers have devoted some attention to, none of the proposed solutions address the issue of deriving implementable policies from high-level goals. A key part of the solution to this problem is having the ability to identify the operations, available on the underlying system, which can achieve a given goal. This work presents an approach by which a formal representation of a system, based on the event calculus, can be used in conjunction with abductive reasoning techniques to derive the sequence of operations that will allow a given system to achieve a desired goal. Additionally it outlines how this technique might be used for providing tool support and partial automation for policy\u00a0\u2026", "num_citations": "285\n", "authors": ["1575"]}
{"title": "An Architecture for Generating Semantic Aware Signatures.\n", "abstract": " Identifying new intrusions and developing effective signatures that detect them is essential for protecting computer networks. We present Nemean, a system for automatic generation of intrusion signatures from honeynet packet traces. Our architecture is distinguished by its emphasis on a modular design framework that encourages independent development and modification of system components and protocol semantics awareness which allows for construction of signatures that greatly reduce false alarms. The building blocks of our architecture include transport and service normalization, intrusion profile clustering and automata learning that generates connection and session aware signatures. We demonstrate the potential of Nemean's semantics-aware, resilient signatures through a prototype implementation. We use two datasets to evaluate the system:(i) a production dataset for false-alarm evaluation and (ii) a honeynet dataset for measuring detection rates. Signatures generated by Nemean for NetBIOS exploits had a 0% false-positive rate and a 0.04% false-negative rate.", "num_citations": "265\n", "authors": ["1575"]}
{"title": "XFA: Faster signature matching with extended automata\n", "abstract": " Automata-based representations and related algorithms have been applied to address several problems in information security, and often the automata had to be augmented with additional information. For example, extended finite-state automata (EFSA) augment finite- state automata (FSA) with variables to track dependencies between arguments of system calls. In this paper, we introduce extended finite automata (XFAs) which augment FSAs with finite scratch memory and instructions to manipulate this memory. Our primary motivation for introducing XFAs is signature matching in Network Intrusion Detection Systems (NIDS). Representing NIDS signatures as deterministic finite-state automata (DFAs) results in very fast signature matching but for several classes of signatures DFAs can blowup in space. Using nondeterministic finite-state automata (NFA) to represent NIDS signatures results in a succinct\u00a0\u2026", "num_citations": "252\n", "authors": ["1575"]}
{"title": "Locally differentially private protocols for frequency estimation\n", "abstract": " Protocols satisfying Local Differential Privacy (LDP) enable parties to collect aggregate information about a population while protecting each user\u2019s privacy, without relying on a trusted third party. LDP protocols (such as Google\u2019s RAPPOR) have been deployed in real-world scenarios. In these protocols, a user encodes his private information and perturbs the encoded value locally before sending it to an aggregator, who combines values that users contribute to infer statistics about the population. In this paper, we introduce a framework that generalizes several LDP protocols proposed in the literature. Our framework yields a simple and fast aggregation algorithm, whose accuracy can be precisely analyzed. Our in-depth analysis enables us to choose optimal parameters, resulting in two new protocols (ie, Optimized Unary Encoding and Optimized Local Hashing) that provide better utility than protocols previously proposed. We present precise conditions for when each proposed protocol should be used, and perform experiments that demonstrate the advantage of our proposed protocols.", "num_citations": "251\n", "authors": ["1575"]}
{"title": "Synthesizing near-optimal malware specifications from suspicious behaviors\n", "abstract": " Fueled by an emerging underground economy, malware authors are exploiting vulnerabilities at an alarming rate. To make matters worse, obfuscation tools are commonly available, and much of the malware is open source, leading to a huge number of variants. Behavior-based detection techniques are a promising solution to this growing problem. However, these detectors require precise specifications of malicious behavior that do not result in an excessive number of false alarms. In this paper, we present an automatic technique for extracting optimally discriminative specifications, which uniquely identify a class of programs. Such a discriminative specification can be used by a behavior-based malware detector. Our technique, based on graph mining and concept analysis, scales to large classes of programs due to probabilistic sampling of the specification space. Our implementation, called Holmes, can\u00a0\u2026", "num_citations": "250\n", "authors": ["1575"]}
{"title": "Privacy preserving clustering\n", "abstract": " The freedom and transparency of information flow on the Internet has heightened concerns of privacy. Given a set of data items, clustering algorithms group similar items together. Clustering has many applications, such as customerbehavior analysis, targeted marketing, forensics, and bioinformatics. In this paper, we present the design and analysis of a privacy-preserving k-means clustering algorithm, where only the cluster means at the various steps of the algorithm are revealed to the participating parties. The crucial step in our privacy-preserving k-means is privacy-preserving computation of cluster means.We present two protocols (one based on oblivious polynomial evaluation and the second based on homomorphic encryption) for privacy-preserving computation of cluster means. We have a JAVA implementation of our algorithm. Using our implementation, we have performed a thorough evaluation of\u00a0\u2026", "num_citations": "250\n", "authors": ["1575"]}
{"title": "Towards practical privacy for genomic computation\n", "abstract": " Many basic tasks in computational biology involve operations on individual DNA and protein sequences. These sequences, even when anonymized, are vulnerable to re-identification attacks and may reveal highly sensitive information about individuals. We present a relatively efficient, privacy-preserving implementation of fundamental genomic computations such as calculating the edit distance and Smith- Waterman similarity scores between two sequences. Our techniques are crypto graphically secure and significantly more practical than previous solutions. We evaluate our prototype implementation on sequences from the Pfam database of protein families, and demonstrate that its performance is adequate for solving real-world sequence-alignment and related problems in a privacy- preserving manner. Furthermore, our techniques have applications beyond computational biology. They can be used to obtain\u00a0\u2026", "num_citations": "234\n", "authors": ["1575"]}
{"title": "Software design as an investment activity: a real options perspective\n", "abstract": " * This work was supported by the National Science Foundation (USA) under grants to Kevin Sullivan numbered CCR-9502029, CCR-9506779 and CCR-9804078 decision theory and finance for mathematical foundations for key design guidelines. We take utility maximisation, generally, and wealth creation, in particular, as the basic objective of the software design decision-making process in a business context, and we try to explain how we might develop an account of the value-enhancing aspects of a number of software design guidelines in terms of mathematical utility and finance theories. In particular, we outline a theory of software design based on the idea that part of the value of a typical software product, process or project is in the form of embedded options. These real options provide design decision-makers with valuable flexibility to change products and plans as uncertainties are resolved over time. We apply some basic ideas emerging from the study of real options to try to establish the plausibility of using such concepts to develop a better account of important software design ideas in three areas: u timing of important design decisions; u designing the modular structure of a system (information hiding); and u the use of phased or iterative project structures (the spiral software development model).We are less concerned here with the use of arbitrage techniques to develop market-based valuations for real options than in developing a case for an options-based rethinking of key ideas in software design and engineering. In some cases, arbitrage pricing is possible, but it depends on knowledge of current asset values and complete markets\u00a0\u2026", "num_citations": "223\n", "authors": ["1575"]}
{"title": "Secure data aggregation technique for wireless sensor networks in the presence of collusion attacks\n", "abstract": " Due to limited computational power and energy resources, aggregation of data from multiple sensor nodes done at the aggregating node is usually accomplished by simple methods such as averaging. However such aggregation is known to be highly vulnerable to node compromising attacks. Since WSN are usually unattended and without tamper resistant hardware, they are highly susceptible to such attacks. Thus, ascertaining trustworthiness of data and reputation of sensor nodes is crucial for WSN. As the performance of very low power processors dramatically improves, future aggregator nodes will be capable of performing more sophisticated data aggregation algorithms, thus making WSN less vulnerable. Iterative filtering algorithms hold great promise for such a purpose. Such algorithms simultaneously aggregate data from multiple sources and provide trust assessment of these sources, usually in a form of\u00a0\u2026", "num_citations": "210\n", "authors": ["1575"]}
{"title": "Markov Chains, Classifiers, and Intrusion Detection.\n", "abstract": " This paper presents a statistical anomaly detection algorithm based on Markov chains. Our algorithm can be directly applied for intrusion detection by discovering anomalous activities. Our framework for constructing anomaly detectors is very general and can be used by other researchers for constructing Markov-chain-based anomaly detectors. We also present performance metrics for evaluating the effectiveness of anomaly detectors. Extensive experimental results clearly demonstrate the effectiveness of our algorithm. We discuss several future directions for research based on the framework presented in this paper.", "num_citations": "208\n", "authors": ["1575"]}
{"title": "{FIE} on firmware: Finding vulnerabilities in embedded systems using symbolic execution\n", "abstract": " Embedded systems increasingly use software-driven low-power microprocessors for security-critical settings, surfacing a need for tools that can audit the security of the software (often called firmware) running on such devices. Despite the fact that firmware programs are often written in C, existing source-code analysis tools do not work well for this setting because of the specific architectural features of low-power platforms. We therefore design and implement a new tool, called FIE, that builds off the KLEE symbolic execution engine in order to provide an extensible platform for detecting bugs in firmware programs for the popular MSP430 family of microcontrollers. FIE incorporates new techniques for symbolic execution that enable it to verify security properties of the simple firmwares often found in practice. We demonstrate FIE\u2019s utility by applying it to a corpus of 99 open-source firmware programs that altogether use 13 different models of the MSP430. We are able to verify memory safety for the majority of programs in this corpus and elsewhere discover 21 bugs.", "num_citations": "186\n", "authors": ["1575"]}
{"title": "A layered architecture for detecting malicious behaviors\n", "abstract": " We address the semantic gap problem in behavioral monitoring by using hierarchical behavior graphs to infer high-level behaviors from myriad low-level events. Our experimental system traces the execution of a process, performing data-flow analysis to identify meaningful actions such as \u201cproxying\u201d, \u201ckeystroke logging\u201d, \u201cdata leaking\u201d, and \u201cdownloading and executing a program\u201d from complex combinations of rudimentary system calls. To preemptively address evasive malware behavior, our specifications are carefully crafted to detect alternative sequences of events that achieve the same high-level goal. We tested eleven benign programs, variants from seven malicious bot families, four trojans, and three mass-mailing worms and found that we were able to thoroughly identify high-level behaviors across this diverse code base. Moreover, we effectively distinguished malicious execution of high-level\u00a0\u2026", "num_citations": "172\n", "authors": ["1575"]}
{"title": "Bolt-on differential privacy for scalable stochastic gradient descent-based analytics\n", "abstract": " While significant progress has been made separately on analytics systems for scalable stochastic gradient descent (SGD) and private SGD, none of the major scalable analytics frameworks have incorporated differentially private SGD. There are two inter-related issues for this disconnect between research and practice:(1) low model accuracy due to added noise to guarantee privacy, and (2) high development and runtime overhead of the private algorithms. This paper takes a first step to remedy this disconnect and proposes a private SGD algorithm to address both issues in an integrated manner. In contrast to the white-box approach adopted by previous work, we revisit and use the classical technique of output perturbation to devise a novel``bolt-on''approach to private SGD. While our approach trivially addresses (2), it makes (1) even more challenging. We address this challenge by providing a novel analysis of the\u00a0\u2026", "num_citations": "167\n", "authors": ["1575"]}
{"title": "Buffer overrun detection using linear programming and static analysis\n", "abstract": " This paper addresses the issue of identifying buffer overrun vulnerabilities by statically analyzing C source code. We demonstrate a light-weight analysis based on modeling C string manipulations as a linear program. We also present fast, scalable solvers based on linear programming, and demonstrate techniques to make the program analysis context sensitive. Based on these techniques, we built a prototype and used it to identify several vulnerabilities in popular security critical applications.", "num_citations": "158\n", "authors": ["1575"]}
{"title": "Towards formal verification of role-based access control policies\n", "abstract": " Specifying and managing access control policies is a challenging problem. We propose to develop formal verification techniques for access control policies to improve the current state of the art of policy specification and management. In this paper, we formalize classes of security analysis problems in the context of role-based access control. We show that in general these problems are PSPACE-complete. We also study the factors that contribute to the computational complexity by considering a lattice of various subcases of the problem with different restrictions. We show that several subcases remain PSPACE-complete, several further restricted subcases are NP-complete, and identify two subcases that are solvable in polynomial time. We also discuss our experiences and findings from experimentations that use existing formal method tools, such as model checking and logic programming, for addressing these\u00a0\u2026", "num_citations": "157\n", "authors": ["1575"]}
{"title": "Cyber SA: Situational awareness for cyber defense\n", "abstract": " Situation Awareness (SA) for cyber defense consists of at least seven aspects", "num_citations": "154\n", "authors": ["1575"]}
{"title": "Agent cloning: an approach to agent mobility and resource allocation\n", "abstract": " Multi-agent systems are subject to performance bottlenecks in cases where agents cannot perform tasks by themselves due to insufficient resources. Solutions to such problems include passing tasks to others or agent migration to remote hosts. We propose agent cloning as a more comprehensive approach to the problem of local agent overloads. Agent cloning subsumes task transfer and agent mobility. According to our paradigm, agents may clone, pass tasks to others, die, or merge. We discuss the requirements of implementing a cloning mechanism and its benefits in a multi-agent system, and support our claims with simulation results.", "num_citations": "153\n", "authors": ["1575"]}
{"title": "Using clouds to provide grids with higher levels of abstraction and explicit support for usage modes\n", "abstract": " Grids in their current form of deployment and implementation have not been as successful as hoped in engendering distributed applications. Among other reasons, the level of detail that needs to be controlled for the successful development and deployment of applications remains too high. We argue that there is a need for higher levels of abstractions for current Grids. By introducing the relevant terminology, we try to understand Grids and Clouds as systems; we find this leads to a natural role for the concept of Affinity, and argue that this is a missing element in current Grids. Providing these affinities and higher\u2010level abstractions is consistent with the common concepts of Clouds. Thus this paper establishes how Clouds can be viewed as a logical and next higher\u2010level abstraction from Grids. Copyright \u00a9 2009 John Wiley & Sons, Ltd.", "num_citations": "149\n", "authors": ["1575"]}
{"title": "SAGA: A Simple API for Grid Applications. High-level application programming on the Grid\n", "abstract": " Grid technology has matured considerably over the past few years. Progress in both implementation and standardization is reaching a level of robustness that enables production quality deployments of grid services in the academic research community with heightened interest and early adoption in the industrial community. Despite this progress, grid applications are far from ubiquitous, and new applications require an enormous amount of programming effort just to see first light. A key impediment to accelerated deployment of grid applications is the scarcity of high-level application programming abstractions that bridge the gap between existing grid middleware and application-level needs. The Simple API for Grid Applications (SAGA [1]) is a GGF standardization effort that addresses this particular gap by providing a simple, stable, and uniform programming interface that integrates the most common grid programming abstractions. These most common abstractions were identified through the analysis of several existing and emerging Grid applications. In this article, we present the SAGA effort, describe its relationship to other Grid API efforts within the GGF community, and introduce the first draft of the API using some application programming examples.", "num_citations": "146\n", "authors": ["1575"]}
{"title": "Multi-agent coordination through coalition formation\n", "abstract": " Incorporating coalition formation algorithms into agent systems shall be advantageous due to the consequent increase in the overall quality of task performance. Coalition formation was addressed in game theory, however the game theoretic approach is centralized and computationally intractable. Recent work in DAI has resulted in distributed algorithms with computational tractability. This paper addresses the implementation of distributed coalition formation algorithms within a real-world multi-agent system. We present the problems that arise when attempting to utilize the theoretical coalition formation algorithms for a real-world system, demonstrate how some of their restrictive assumptions can be relaxed, and discuss the resulting benefits. In addition, we analyze the modifications, the complexity and the quality of the cooperation mechanisms. The task domain of our multi-agent system is information\u00a0\u2026", "num_citations": "146\n", "authors": ["1575"]}
{"title": "Randomization based probabilistic approach to detect trojan circuits\n", "abstract": " In this paper, we propose a randomization based technique to verify whether a manufactured chip conforms to its design or is infected by any trojan circuit. A trojan circuit can be inserted into the design or fabrication mask by a malicious manufacturer such that it monitors for a specific rare trigger condition, and then it produces a payload error in the circuit which alters the functionality of the circuit often causing a catastrophic crash of the system where the chip was being used. Since trojans are activated by rare input patterns, they are stealthy by nature and are difficult to detect through conventional techniques of functional testing. In this paper, we propose a novel randomized approach to probabilistically compare the functionality of the implemented circuit with the design of the circuit. Using hypothesis testing, we provide quantitative guarantees when our algorithm reports that there is no trojan in the implemented\u00a0\u2026", "num_citations": "145\n", "authors": ["1575"]}
{"title": "The design and implementation of microdrivers\n", "abstract": " Device drivers commonly execute in the kernel to achieve high performance and easy access to kernel services. However, this comes at the price of decreased reliability and increased programming difficulty. Driver programmers are unable to use user-mode development tools and must instead use cumbersome kernel tools. Faults in kernel drivers can cause the entire operating system to crash. User-mode drivers have long been seen as a solution to this problem, but suffer from either poor performance or new interfaces that require a rewrite of existing drivers. This paper introduces the Microdrivers architecture that achieves high performance and compatibility by leaving critical path code in the kernel and moving the rest of the driver code to a user-mode process. This allows data-handling operations critical to I/O performance to run at full speed, while management operations such as initialization and\u00a0\u2026", "num_citations": "140\n", "authors": ["1575"]}
{"title": "Csi-mimo: Indoor wi-fi fingerprinting system\n", "abstract": " Wi-Fi based fingerprinting systems, mostly utilize the Received Signal Strength Indicator (RSSI), which is known to be unreliable due to environmental and hardware effects. In this paper, we present a novel Wi-Fi fingerprinting system, exploiting the fine-grained information known as Channel State Information (CSI). The frequency diversity of CSI can be effectively utilized to represent a location in both frequency and spatial domain resulting in more accurate indoor localization. We propose a novel location signature CSI-MIMO that incorporates Multiple Input Multiple Output (MIMO) information and use both the magnitude and the phase of CSI of each sub-carrier. We experimentally evaluate the performance of CSI-MIMO fingerprinting using the k-nearest neighbor and the Bayes algorithm. The accuracy of the proposed CSI-MIMO is compared with Finegrained Indoor Fingerprinting System (FIFS) and a simple CSI\u00a0\u2026", "num_citations": "127\n", "authors": ["1575"]}
{"title": "Received signal strength indicator and its analysis in a typical WLAN system (short paper)\n", "abstract": " Received signal strength based fingerprinting approaches have been widely exploited for localization. The received signal strength (RSS) plays a very crucial role in determining the nature and characteristics of location fingerprints stored in a radio-map. The received signal strength is a function of distance between the transmitter and receiving device, which varies due to various in-path interferences. A detailed analysis of factors affecting the received signal for indoor localization is presented in this paper. The paper discusses the effect of factors such as spatial, temporal, environmental, hardware and human presence on the received signal strength through extensive measurements in a typical IEEE 802.11b/g/n network. It also presents the statistical analysis of the measured data that defines the reliability of RSS-based location fingerprints for indoor localization.", "num_citations": "124\n", "authors": ["1575"]}
{"title": "Backtracking algorithmic complexity attacks against a NIDS\n", "abstract": " Network Intrusion Detection Systems (NIDS) have become crucial to securing modern networks. To be effective, a NIDS must be able to counter evasion attempts and operate at or near wire-speed. Failure to do so allows malicious packets to slip through a NIDS undetected. In this paper, we explore NIDS evasion through algorithmic complexity attacks. We present a highly effective attack against the Snort NIDS, and we provide a practical algorithmic solution that successfully thwarts the attack. This attack exploits the behavior of rule matching, yielding inspection times that are up to 1.5 million times slower than that of benign packets. Our analysis shows that this attack is applicable to many rules in Snorts ruleset, rendering vulnerable the thousands of networks protected by it. Our countermeasure confines the inspection time to within one order of magnitude of benign packets. Experimental results using a live system\u00a0\u2026", "num_citations": "123\n", "authors": ["1575"]}
{"title": "Combining partial order and symmetry reductions\n", "abstract": " Partial order based reduction techniques to reduce time and memory in model-checking procedures are becoming quite popular. Partial order reduction techniques exploit the independence of actions. Symmetry based reduction techniques exploit the inherent structure of the system to reduce the state space explored during model checking. We provide an abstract framework for combining partial-order and symmetry reductions. We also present algorithms which exploit both reduction techniques simultaneously.", "num_citations": "112\n", "authors": ["1575"]}
{"title": "Analyzing the robustness of nearest neighbors to adversarial examples\n", "abstract": " Motivated by safety-critical applications, test-time attacks on classifiers via adversarial examples has recently received a great deal of attention. However, there is a general lack of understanding on why adversarial examples arise; whether they originate due to inherent properties of data or due to lack of training samples remains ill-understood. In this work, we introduce a theoretical framework analogous to bias-variance theory for understanding these effects. We use our framework to analyze the robustness of a canonical non-parametric classifier {\u2013} the k-nearest neighbors. Our analysis shows that its robustness properties depend critically on the value of k {\u2013} the classifier may be inherently non-robust for small k, but its robustness approaches that of the Bayes Optimal classifier for fast-growing k. We propose a novel modified 1-nearest neighbor classifier, and guarantee its robustness in the large sample limit. Our experiments suggest that this classifier may have good robustness properties even for reasonable data set sizes.", "num_citations": "111\n", "authors": ["1575"]}
{"title": "Creating vulnerability signatures using weakest preconditions\n", "abstract": " Signature-based tools such as network intrusion detection systems are widely used to protect critical systems. Automatic signature generation techniques are needed to enable these tools due to the speed at which new vulnerabilities are discovered. In particular, we need automatic techniques which generate sound signatures - signatures which will not mistakenly block legitimate traffic or raise false alarms. In addition, we need signatures to have few false negatives and will catch many different exploit variants. We investigate new techniques for automatically generating sound vulnerability signatures with fewer false negatives than previous research using program binary analysis. The key problem to reducing false negatives is to consider as many as possible different program paths an exploit may take. Previous work considered each possible program path an exploit may take separately, thus generating\u00a0\u2026", "num_citations": "111\n", "authors": ["1575"]}
{"title": "A tale of two data-intensive paradigms: Applications, abstractions, and architectures\n", "abstract": " Scientific problems that depend on processing largeamounts of data require overcoming challenges in multiple areas:managing large-scale data distribution, co-placement andscheduling of data with compute resources, and storing and transferringlarge volumes of data. We analyze the ecosystems of thetwo prominent paradigms for data-intensive applications, hereafterreferred to as the high-performance computing and theApache-Hadoop paradigm. We propose a basis, common terminologyand functional factors upon which to analyze the two approachesof both paradigms. We discuss the concept of \"Big DataOgres\" and their facets as means of understanding and characterizingthe most common application workloads found acrossthe two paradigms. We then discuss the salient features of thetwo paradigms, and compare and contrast the two approaches.Specifically, we examine common implementation\u00a0\u2026", "num_citations": "102\n", "authors": ["1575"]}
{"title": "Semantically-aware network intrusion signature generator\n", "abstract": " An automatic technique for generating signatures for malicious network traffic performs a cluster analysis of known malicious traffic to create a signature in the form of a state machine. The cluster analysis may operate on semantically tagged data collected by connection or session and normalized to eliminate protocol specific features. The signature extractor may generalize the finite-state machine signatures to match network traffic not previously observed.", "num_citations": "100\n", "authors": ["1575"]}
{"title": "Saga bigjob: An extensible and interoperable pilot-job abstraction for distributed applications and systems\n", "abstract": " The uptake of distributed infrastructures by scientific applications has been limited by the availability of extensible, pervasive and simple-to-use abstractions which are required at multiple levels development, deployment and execution stages of scientific applications. The Pilot-Job abstraction has been shown to be an effective abstraction to address many requirements of scientific applications. Specifically, Pilot-Jobs support the decoupling of workload submission from resource assignment; this results in a flexible execution model, which in turn enables the distributed scale-out of applications on multiple and possibly heterogeneous resources. Most Pilot-Job implementations however, are tied to a specific infrastructure. In this paper, we describe the design and implementation of a SAGA-based Pilot-Job, which supports a wide range of application types, and is usable over a broad range of infrastructures, i.e., it is\u00a0\u2026", "num_citations": "99\n", "authors": ["1575"]}
{"title": "Computational steering in RealityGrid\n", "abstract": " The RealityGrid project (http://www. realitygrid. org) aims both to enable the discovery of new materials through integrated experiments and to understand the behaviour of physical systems based on the properties of their microscopic components using diverse simulation methods spanning many time and length scales. A central theme of RealityGrid is the facilitation of distributed and collaborative steering of parallel simulation codes and simultaneous on-line, high-end visualisation. In this paper, we review the motivations for computational steering and introduce the RealityGrid steering library and associated software. We then outline the capabilities of the library and describe the service-oriented architecture of the latest implementation, in which the steering controls of the application are exposed through an OGSI-compliant Grid service.", "num_citations": "99\n", "authors": ["1575"]}
{"title": "Privacy at scale: Local differential privacy in practice\n", "abstract": " Local differential privacy (LDP), where users randomly perturb their inputs to provide plausible deniability of their data without the need for a trusted party, has been adopted recently by several major technology organizations, including Google, Apple and Microsoft. This tutorial aims to introduce the key technical underpinnings of these deployed systems, to survey current research that addresses related problems within the LDP model, and to identify relevant open problems and research directions for the community.", "num_citations": "97\n", "authors": ["1575"]}
{"title": "Locally differentially private frequent itemset mining\n", "abstract": " The notion of Local Differential Privacy (LDP) enables users to respond to sensitive questions while preserving their privacy. The basic LDP frequent oracle (FO) protocol enables an aggregator to estimate the frequency of any value. But when each user has a set of values, one needs an additional padding and sampling step to find the frequent values and estimate their frequencies. In this paper, we formally define such padding and sample based frequency oracles (PSFO). We further identify the privacy amplification property in PSFO. As a result, we propose SVIM, a protocol for finding frequent items in the set-valued LDP setting. Experiments show that under the same privacy guarantee and computational cost, SVIM significantly improves over existing methods. With SVIM to find frequent items, we propose SVSM to effectively find frequent itemsets, which to our knowledge has not been done before in the LDP setting.", "num_citations": "95\n", "authors": ["1575"]}
{"title": "Improving main memory hash joins on intel xeon phi processors: An experimental approach\n", "abstract": " Modern processor technologies have driven new designs and implementations in main-memory hash joins. Recently, Intel Many Integrated Core (MIC) co-processors (commonly known as Xeon Phi) embrace emerging x86 single-chip many-core techniques. Compared with contemporary multi-core CPUs, Xeon Phi has quite different architectural features: wider SIMD instructions, many cores and hardware contexts, as well as lower-frequency in-order cores. In this paper, we experimentally revisit the state-of-the-art hash join algorithms on Xeon Phi co-processors. In particular, we study two camps of hash join algorithms: hardware-conscious ones that advocate careful tailoring of the join algorithms to underlying hardware architectures and hardware-oblivious ones that omit such careful tailoring. For each camp, we study the impact of architectural features and software optimizations on Xeon Phi in comparison with\u00a0\u2026", "num_citations": "92\n", "authors": ["1575"]}
{"title": "A methodology for formalizing model-inversion attacks\n", "abstract": " Confidentiality of training data induced by releasing machine-learning models, and has recently received increasing attention. Motivated by existing MI attacks and other previous attacks that turn out to be MI \"in disguise,\" this paper initiates a formal study of MI attacks by presenting a game-based methodology. Our methodology uncovers a number of subtle issues, and devising a rigorous game-based definition, analogous to those in cryptography, is an interesting avenue for future work. We describe methodologies for two types of attacks. The first is for black-box attacks, which consider an adversary who infers sensitive values with only oracle access to a model. The second methodology targets the white-box scenario where an adversary has some additional knowledge about the structure of a model. For the restricted class of Boolean models and black-box attacks, we characterize model invertibility using the\u00a0\u2026", "num_citations": "89\n", "authors": ["1575"]}
{"title": "Static analysis and compiler design for idempotent processing\n", "abstract": " Recovery functionality has many applications in computing systems, from speculation recovery in modern microprocessors to fault recovery in high-reliability systems. Modern systems commonly recover using checkpoints. However, checkpoints introduce overheads, add complexity, and often save more state than necessary.", "num_citations": "87\n", "authors": ["1575"]}
{"title": "Hands off the wheel in autonomous vehicles?: A systems perspective on over a million miles of field data\n", "abstract": " Autonomous vehicle (AV) technology is rapidly becoming a reality on U.S. roads, offering the promise of improvements in traffic management, safety, and the comfort and efficiency of vehicular travel. The California Department of Motor Vehicles (DMV) reports that between 2014 and 2017, manufacturers tested 144 AVs, driving a cumulative 1,116,605 autonomous miles, and reported 5,328 disengagements and 42 accidents involving AVs on public roads. This paper investigates the causes, dynamics, and impacts of such AV failures by analyzing disengagement and accident reports obtained from public DMV databases. We draw several conclusions. For example, we find that autonomous vehicles are 15 - 4000\u00c3- worse than human drivers for accidents per cumulative mile driven; that drivers of AVs need to be as alert as drivers of non-AVs; and that the AVs' machine-learning-based systems for perception and\u00a0\u2026", "num_citations": "80\n", "authors": ["1575"]}
{"title": "Exploring the performance fluctuations of hpc workloads on clouds\n", "abstract": " Clouds enable novel execution modes often supported by advanced capabilities such as autonomic schedulers. These capabilities are predicated upon an accurate estimation and calculation of runtimes on a given infrastructure. Using a well understood high-performance computing workload, we find strong fluctuations from the mean performance on EC2 and Eucalyptus-based cloud systems. Our analysis eliminates variations in IO and computational times as possible causes, we find that variations in communication times account for the bulk of the experiment-to-experiment fluctuations of the performance.", "num_citations": "80\n", "authors": ["1575"]}
{"title": "Automatic placement of authorization hooks in the Linux security modules framework\n", "abstract": " We present a technique for automatic placement of authorization hooks, and apply it to the Linux security modules (LSM) framework. LSM is a generic framework which allows diverse authorization policies to be enforced by the Linux kernel. It consists of a kernel module which encapsulates an authorization policy, and hooks into the kernel module placed at appropriate locations in the Linux kernel. The kernel enforces the authorization policy using hook calls. In current practice, hooks are placed manually in the kernel. This approach is tedious, and as prior work has shown, is prone to security holes. Our technique uses static analysis of the Linux kernel and the kernel module to automate hook placement. Given a non-hook-placed version of the Linux kernel, and a kernel module that implements an authorization policy, our technique infers the set of operations authorized by each hook, and the set of operations\u00a0\u2026", "num_citations": "80\n", "authors": ["1575"]}
{"title": "Steering in computational science: mesoscale modelling and simulation\n", "abstract": " This paper outlines the benefits of computational steering for high performance computing applications. Lattice-Boltzmann mesoscale fluid simulations of binary and ternary amphiphilic fluids in two and three dimensions are used to illustrate the substantial improvements which computational steering offers in terms of resource efficiency and time to discover new physics. We discuss details of our current steering implementations and describe their future outlook with the advent of computational grids.", "num_citations": "80\n", "authors": ["1575"]}
{"title": "Autonomic management of application workflows on hybrid computing infrastructure\n", "abstract": " In this paper, we present a programming and runtime framework that enables the autonomic management of complex application workflows on hybrid computing infrastructures. The framework is designed to address system and application heterogeneity and dynamics to ensure that application objectives and constraints are satisfied. The need for such autonomic system and application management is becoming critical as computing infrastructures become increasingly heterogeneous, integrating different classes of resources from high-end HPC systems to commodity clusters and clouds. For example, the framework presented in this paper can be used to provision the appropriate mix of resources based on application requirements and constraints. The framework also monitors the system/application state and adapts the application and/or resources to respond to changing requirements or environment. To\u00a0\u2026", "num_citations": "78\n", "authors": ["1575"]}
{"title": "Secure function evaluation with ordered binary decision diagrams\n", "abstract": " Privacy-preserving protocols allow multiple parties with private inputs to perform joint computation while preserving the privacy of their respective inputs. An important cryptographic primitive for designing privacy-preserving protocols is secure function evaluation (SFE). The classic solution for SFE by Yao uses a gate representation of the function that the two parties want to jointly compute. Fairplay is a system that implements the classic solution for SFE. In this paper, we present a new protocol for SFE that uses a graph-based representation of the function. Specifically we use the graph-based representation called ordered binary decision diagrams (OBDDs). For a large number of Boolean functions, OBDDs are more succinct than the gate-based representation. Preliminary experimental results based on a prototype implementation shows that for several functions, our protocol results in a smaller bandwidth than Fairplay\u00a0\u2026", "num_citations": "78\n", "authors": ["1575"]}
{"title": "Big data, simulations and HPC convergence\n", "abstract": " Two major trends in computing systems are the growth in high performance computing (HPC) with in particular an international exascale initiative, and big data with an accompanying cloud infrastructure of dramatic and increasing size and sophistication. In this paper, we study an approach to convergence for software and applications/algorithms and show what hardware architectures it suggests. We start by dividing applications into data plus model components and classifying each component (whether from Big Data or Big Compute) in the same way. This leads to 64 properties divided into 4 views, which are Problem Architecture (Macro pattern); Execution Features (Micro patterns); Data Source and Style; and finally the Processing (runtime) View. We discuss convergence software built around HPC-ABDS (High Performance Computing enhanced Apache Big Data Stack) and show how one can merge\u00a0\u2026", "num_citations": "73\n", "authors": ["1575"]}
{"title": "Retrofitting legacy code for authorization policy enforcement\n", "abstract": " Researchers have argued that the best way to construct a secure system is to proactively integrate security into the design of the system. However, this tenet is rarely followed because of economic and practical considerations. Instead, security mechanisms are added as the need arises, by retrofitting legacy code. Existing techniques to do so are manual and ad hoc, and often result in security holes. We present program analysis techniques to assist the process of retrofitting legacy code for authorization policy enforcement. These techniques can be used to retrofit legacy servers, such as X window, Web, proxy, and cache servers. Because such servers manage multiple clients simultaneously, and offer shared resources to clients, they must have the ability to enforce authorization policies. A developer can use our techniques to identify security-sensitive locations in legacy servers, and place reference monitor calls to\u00a0\u2026", "num_citations": "70\n", "authors": ["1575"]}
{"title": "Malware prevention system monitoring kernel events\n", "abstract": " A malware prevention system monitors kernel level events of the operating system and applies user programmable or preprepared policies to those events to detect and block malware.", "num_citations": "67\n", "authors": ["1575"]}
{"title": "Objective metrics and gradient descent algorithms for adversarial examples in machine learning\n", "abstract": " Fueled by massive amounts of data, models produced by machine-learning (ML) algorithms are being used in diverse domains where security is a concern, such as, automotive systems, finance, health-care, computer vision, speech recognition, natural-language processing, and malware detection. Of particular concern is use of ML in cyberphysical systems, such as driver-less cars and aviation, where the presence of an adversary can cause serious consequences. In this paper we focus on attacks caused by adversarial samples, which are inputs crafted by adding small, often imperceptible, perturbations to force a ML model to misclassify. We present a simple gradient-descent based algorithm for finding adversarial samples, which performs well in comparison to existing algorithms. The second issue that this paper tackles is that of metrics. We present a novel metric based on few computer-vision algorithms for\u00a0\u2026", "num_citations": "66\n", "authors": ["1575"]}
{"title": "Randomized stopping times and American option pricing with transaction costs\n", "abstract": " In a general discrete\u2010time market model with proportional transaction costs, we derive new expectation representations of the range of arbitrage\u2010free prices of an arbitrary American option. The upper bound of this range is called the upper hedging price, and is the smallest initial wealth needed to construct a self\u2010financing portfolio whose value dominates the option payoff at all times. A surprising feature of our upper hedging price representation is that it requires the use of randomized stopping times (Baxter and Chacon 1977), just as ordinary stopping times are needed in the absence of transaction costs. We also represent the upper hedging price as the optimum value of a variety of optimization problems. Additionally, we show a two\u2010player game where at Nash equilibrium the value to both players is the upper hedging price, and one of the players must in general choose a mixture of stopping times. We derive\u00a0\u2026", "num_citations": "63\n", "authors": ["1575"]}
{"title": "An autonomic approach to integrated hpc grid and cloud usage\n", "abstract": " Clouds are rapidly joining high-performance Grids as viable computational platforms for scientific exploration and discovery, and it is clear that production computational infrastructures will integrate both these paradigms in the near future. As a result, understanding usage modes that are meaningful in such a hybrid infrastructure is critical. For example, there are interesting application workflows that can benefit from such hybrid usage modes to, per- haps, reduce times to solutions, reduce costs (in terms of currency or resource allocation), or handle unexpected runtime situations (e.g., unexpected delays in scheduling queues or unexpected failures). The primary goal of this paper is to experimentally investigate, from an applications perspective, how autonomics can enable interesting usage modes and scenarios for integrating HPC Grid and Clouds. Specifically, we used a reservoir characterization application\u00a0\u2026", "num_citations": "62\n", "authors": ["1575"]}
{"title": "Extended finite state automata and systems and methods for recognizing patterns in a data stream using extended finite state automata\n", "abstract": " Deterministic finite automata (DFAs) are popular solutions to deep packet inspection because they are fast and DFAs corresponding to multiple signatures are combinable into a single DFA. Combining such DFAs causes an explosive increase in memory usage. Extended finite automata (XFAs) are an alternative to DFAs that avoids state-space explosion problems. XFAs extend DFAs with a few bytes of \u201cscratch memory\u201d used to store bits and other data structures that record progress. Simple programs associated with automaton states and/or transitions manipulate this scratch memory. XFAs are deterministic in their operation, are equivalent to DFAs in expressiveness, and require no custom hardware support. Fully functional prototype XFA implementations show that, for most signature sets, XFAs are at least 10,000 times smaller than the DFA matching all signatures. XFAs are 10 times smaller and 5 times faster or\u00a0\u2026", "num_citations": "61\n", "authors": ["1575"]}
{"title": "NEKTAR, SPICE and Vortonics: using federated grids for large scale scientific applications\n", "abstract": " In response to a joint call from US\u2019s NSF and UK\u2019s EPSRC for applications that aim to utilize the combined computational resources of the US and UK, three computational science groups from UCL, Tufts and Brown Universities teamed up with a middleware team from NIU/Argonne to meet the challenge. Although the groups had three distinct codes and aims, the projects had the underlying common feature that they were comprised of large-scale distributed applications which required high-end networking and advanced middleware in order to be effectively deployed. For example, cross-site runs were found to be a very effective strategy to overcome the limitations of a single resource.                 The seamless federation of a grid-of-grids remains difficult. Even if interoperability at the middleware and software stack levels were to exist, it would not guarantee that the federated grids can be utilized for large\u00a0\u2026", "num_citations": "61\n", "authors": ["1575"]}
{"title": "NetSpy: Automatic generation of spyware signatures for NIDS\n", "abstract": " We present NetSpy, a tool to automatically generate network-level signatures for spyware. NetSpy determines whether an untrusted program is spyware by correlating user input with network traffic generated by the untrusted program. If classified as spyware, NetSpy also generates a signature characterizing the malicious substrate of the spyware's network behavior. Such a signature can be used by network intrusion detection systems to detect spyware installations in large networks. In our experiments, NetSpy precisely identified each of the 7 spyware programs that we considered and generated network-level signatures for them. Of the 9 supposedly-benign programs that we considered, NetSpy correctly characterized 6 of them as benign. The remaining 3 programs showed network behavior that was highly suggestive of spying activity", "num_citations": "59\n", "authors": ["1575"]}
{"title": "Practical {DIFC} Enforcement on Android\n", "abstract": " Smartphone users often use private and enterprise data with untrusted third party applications. The fundamental lack of secrecy guarantees in smartphone OSes, such as Android, exposes this data to the risk of unauthorized exfiltration. A natural solution is the integration of secrecy guarantees into the OS. In this paper, we describe the challenges for decentralized information flow control (DIFC) enforcement on Android. We propose contextsensitive DIFC enforcement via lazy polyinstantiation and practical and secure network export through domain declassification. Our DIFC system, Weir, is backwards compatible by design, and incurs less than 4 ms overhead for component startup. With Weir, we demonstrate practical and secure DIFC enforcement on Android.", "num_citations": "58\n", "authors": ["1575"]}
{"title": "A formal treatment of distributed matchmaking (poster)\n", "abstract": " A formal treatment of distributed matchmaking (poster) | Proceedings of the second international conference on Autonomous agents ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search aamas Conference Proceedings Upcoming Events Authors Affiliations Award Winners More HomeConferencesAAMASProceedingsAGENTS '98A formal treatment of distributed matchmaking (poster) Article A formal treatment of distributed matchmaking (poster) Share on Authors: Somesh Jha Carnegie Mellon University Carnegie Mellon University View Profile , Prasad Chalasani Carnegie Mellon University Carnegie Mellon University View Profile , Onn Shehory Carnegie Mellon University Carnegie Mellon University View \u2026", "num_citations": "57\n", "authors": ["1575"]}
{"title": "Analysis of SPKI/SDSI certificates using model checking\n", "abstract": " SPKI/SDSI is a framework for expressing naming and authorization issues that arise in a distributed-computing environment. We establish a connection between SPKI/SDSI and a formalism known as pushdown systems (PDSs). We show that the SPKI/SDSI-to-PDS connection provides a framework for formalizing a variety of certificate-analysis problems. Moreover, the connection has computational significance: many analysis problems can be solved efficiently (i.e., in time polynomial in the size of the certificate set) using existing algorithms for model checking pushdown systems.", "num_citations": "56\n", "authors": ["1575"]}
{"title": "Semantic adversarial deep learning\n", "abstract": " Fueled by massive amounts of data, models produced by machine-learning (ML) algorithms, especially deep neural networks, are being used in diverse domains where trustworthiness is a concern, including automotive systems, finance, health care, natural language processing, and malware detection. Of particular concern is the use of ML algorithms in cyber-physical systems (CPS), such as self-driving cars and aviation, where an adversary can cause serious consequences.                 However, existing approaches to generating adversarial examples and devising robust ML algorithms mostly ignore the semantics and context of the overall system containing the ML component. For example, in an autonomous vehicle using deep learning for perception, not every adversarial example for the neural network might lead to a harmful consequence. Moreover, one may want to prioritize the search for\u00a0\u2026", "num_citations": "55\n", "authors": ["1575"]}
{"title": "Probabilistic strategies for pursuit in cluttered environments with multiple robots\n", "abstract": " In this paper, we describe a method for coordinating multiple robots in a pursuit-evasion domain. We examine the problem of multiple robotic pursuers attempting to locate a non-adversarial mobile evader in an indoor environment. Unlike many other approaches to this problem, our method seeks to minimize expected time of capture rather than guaranteeing capture. This allows us to examine the performance of our algorithm in complex and cluttered environments where guaranteed capture is difficult or impossible with limited pursuers. We present a probabilistic formulation of the problem, discretize the environment, and define cost heuristics for use in planning. We then propose a scalable algorithm using an entropy cost heuristic that searches possible movement paths to determine coordination strategies for the robotic pursuers. We present simulated results describing the performance of our algorithm against\u00a0\u2026", "num_citations": "55\n", "authors": ["1575"]}
{"title": "Locally differentially private heavy hitter identification\n", "abstract": " The notion of Local Differential Privacy (LDP) enables users to answer sensitive questions while preserving their privacy. The basic LDP frequency oracle protocol enables the aggregator to estimate the frequency of any value. But when the domain of input values is large, finding the most frequent values, also known as the heavy hitters, by estimating the frequencies of all possible values, is computationally infeasible. In this paper, we propose an LDP protocol for identifying heavy hitters. In our proposed protocol, which we call Prefix Extending Method (PEM), users are divided into groups, with each group reporting a prefix of her value. We analyze how to choose optimal parameters for the protocol and identify two design principles for designing LDP protocols with high utility. Experiments show that under the same privacy guarantee and computational cost, PEM has better utility on both synthetic and real-world\u00a0\u2026", "num_citations": "53\n", "authors": ["1575"]}
{"title": "Mining security-sensitive operations in legacy code using concept analysis\n", "abstract": " This paper presents an approach to statically retrofit legacy servers with mechanisms for authorization policy enforcement. The approach is based upon the observation that security-sensitive operations performed by a server are characterized by idiomatic resource manipulations, called fingerprints. Candidate fingerprints are automatically mined by clustering resource manipulations using concept analysis. These fingerprints are then used to identify security-sensitive operations performed by the server. Case studies with three real-world servers show that the approach can be used to identify security-sensitive operations with a few hours of manual effort and modest domain knowledge.", "num_citations": "53\n", "authors": ["1575"]}
{"title": "Malware detection\n", "abstract": " Shared resources, such as the Internet, have created a highly interconnected cyber-infrastructure. Critical infrastructures in domains such as medical, power, telecommunications, and finance are highly dependent on information systems. These two factors have exposed our critical infrastructures to malicious attacks and accidental failures. Many malicious attacks are achieved by malicious code or malware, such as viruses and worms. Given the deleterious affects of malware on our cyber infrastructure, identifying malicious programs is an important goal. Unfortunately, malware detectors have not kept pace with the evasion techniques commonly used by hackers, ie, the good guys are falling behind in the arms race. Malware Detection captures the state of the art research in the area of malicious code detection, prevention and mitigation.", "num_citations": "52\n", "authors": ["1575"]}
{"title": "A refined binomial lattice for pricing American Asian options\n", "abstract": " We present simple and fast algorithms for computing very tight upper and lower bounds on the prices of American Asian options in the binomial model. We introduce a new refined version of the Cox-Ross-Rubinstein (1979) binomial lattice of stock prices. Each node in the lattice is partitioned into \u2018nodelets\u2019, each of which represents all paths arriving at the node with a specific geometric stock price average. The upper bound uses an interpolation idea similar to the Hull-White (1993) method. From the backward-recursive upper-bound computation, we estimate a good exercise rule that is consistent with the refined lattice. This exercise rule is used to obtain a lower bound on the option price using a modification of a conditional-expectation based idea from Rogers-Shi (1995) and Chalasani-Jha-Varikooty (1998). Our algorithms run in time proportional to the number of nodelets in the refined lattice, which is\u00a0\u2026", "num_citations": "52\n", "authors": ["1575"]}
{"title": "Multi-byte regular expression matching with speculation\n", "abstract": " Intrusion prevention systems determine whether incoming traffic matches a database of signatures, where each signature in the database represents an attack or a vulnerability. IPSs need to keep up with ever-increasing line speeds, which leads to the use of custom hardware. A major bottleneck that IPSs face is that they scan incoming packets one byte at a time, which limits their throughput and latency. In this paper, we present a method for scanning multiple bytes in parallel using speculation. We break the packet in several chunks, opportunistically scan them in parallel and if the speculation is wrong, correct it later. We present algorithms that apply speculation in single-threaded software running on commodity processors as well as algorithms for parallel hardware. Experimental results show that speculation leads to improvements in latency and throughput in both cases.", "num_citations": "51\n", "authors": ["1575"]}
{"title": "Programming abstractions for data intensive computing on clouds and grids\n", "abstract": " MapReduce has emerged as an important data-parallel programming model for data-intensive computing - for Clouds and Grids. However most if not all implementations of MapReduce are coupled to a specific infrastructure. SAGA is a high-level programming interface which provides the ability to create distributed applications in an infrastructure independent way. In this paper, we show how MapReduce has been implemented using SAGA and demonstrate its interoperability across different distributed platforms - Grids, Cloud-like infrastructure and Clouds. We discuss the advantages of programmatically developing MapReduce using SAGA, by demonstrating that the SAGA-based implementation is infrastructure independent whilst still providing control over the deployment, distribution and runtime decomposition. The ability to control the distribution and placement of the computation units (workers) is critical in\u00a0\u2026", "num_citations": "51\n", "authors": ["1575"]}
{"title": "Automatic Generation of Remediation Procedures for Malware Infections.\n", "abstract": " Despite the widespread deployment of malwaredetection software, in many situations it is difficult to preemptively block a malicious program from infecting a system. Rather, signatures for detection are usually available only after malware have started to infect a large group of systems. Ideally, infected systems should be reinstalled from scratch. However, due to the high cost of reinstallation, users may prefer to rely on the remediation capabilities of malware detectors to revert the effects of an infection. Unfortunately, current malware detectors perform this task poorly, leaving users\u2019 systems in an unsafe or unstable state. This paper presents an architecture to automatically generate remediation procedures from malicious programs\u2014procedures that can be used to remediate all and only the effects of the malware\u2019s execution in any infected system. We have implemented a prototype of this architecture and used it to generate remediation procedures for a corpus of more than 200 malware binaries. Our evaluation demonstrates that the algorithm outperforms the remediation capabilities of top-rated commercial malware detectors.", "num_citations": "49\n", "authors": ["1575"]}
{"title": "Microdrivers: A new architecture for device drivers\n", "abstract": " Commodity operating systems achieve good performance by running device drivers in-kernel. Unfortunately, this architecture offers poor fault isolation. This paper introduces microdrivers, which reduce the amount of driver code running in the kernel by splitting driver functionality between a small kernel-mode component and a larger user-mode component. This paper presents the microdriver architecture and techniques to refactor existing device drivers into microdrivers, achieving most of the benefits of user-mode drivers with the performance of kernel-mode drivers. Experiments on a network driver show that 75% of its code can be removed from the kernel without affecting common-case performance.", "num_citations": "49\n", "authors": ["1575"]}
{"title": "Method and apparatus to detect malicious software\n", "abstract": " A technique for finding malicious code such as viruses in an executable binary file converts the executable binary to a function unique form to which function unique forms of virus code may be compared. By avoiding direct comparison of the expression of the viral code but looking instead at its function, obfuscation techniques intended to hide the virus code are substantially reduced in effectiveness.", "num_citations": "48\n", "authors": ["1575"]}
{"title": "Accurate approximations for European-style Asian options\n", "abstract": " In the binomial tree model, we provide efficient algorithms for computing an accurate lower bound for the value of a European-style Asian option with either a fixed or a floating strike. These algorithms are inspired by the continuous-time analysis of Rogers and Shi. Specifically we consider lower bounds on the option value that are given by the expectation of the conditional expectation of the payoff conditioned on some random variable Z. For a specific Z, Rogers and Shi estimate this conditional expectation numerically in continuous time, and show experimentally that their lower bound is very accurate. We consider a modified random variable Z that gives a strictly better lower bound. In addition, we show that this lower bound can be computed exactly in the n-step binomial tree model in time proportional to n7. We show that computing the approximation is equivalent to counting paths of various types, and that this can be done efficiently by a dynamic programming technique. We present other choices of Z that yield accurate and efficiently-computable lower bounds. We also show algorithms to compute a bound on the error of these approximations, so that we can compute an upper bound on the option value as well.Descriptors:", "num_citations": "48\n", "authors": ["1575"]}
{"title": "The survey of real time operating system: RTOS\n", "abstract": " The paper discusses the literature survey of RTOS (Real Time Operating Systems) and its contributions to the embedded world. RTOS is defined as a system in which the correctness of the system does not depend only on the logical results of computation but also on the time at which the results are produced. It has to perform critical tasks on priority basis keeping the context switching time minimum. It is often associated with few misconceptions & we have tried to throw some light on it. Since last 20 years, RTOS is undergoing continuous evolution and has resulted into development of many commercial RTOS products. We have selected few commercial RTOS of different categories of real-time applications and have discussed its real-time features. A comparison of the commercial RTOSs' is presented. We conclude by discussing the results of the survey and comparing the RTOS based on performance parameters.", "num_citations": "47\n", "authors": ["1575"]}
{"title": "A mechanism for S-adenosyl methionine assisted formation of a riboswitch conformation: a small molecule with a strong arm\n", "abstract": " The S-adenosylmethionine-1 (SAM-I) riboswitch mediates expression of proteins involved in sulfur metabolism via formation of alternative conformations in response to binding by SAM. Models for kinetic trapping of the RNA in the bound conformation require annealing of nonadjacent mRNA segments during a transcriptional pause. The entropic cost required to bring nonadjacent segments together should slow the folding process. To address this paradox, we performed molecular dynamics simulations on the SAM-I riboswitch aptamer domain with and without SAM, starting with the X-ray coordinates of the SAM-bound RNA. Individual trajectories are 200 ns, among the longest reported for an RNA of this size. We applied principle component analysis (PCA) to explore the global dynamics differences between these two trajectories. We observed a conformational switch between a stacked and nonstacked state\u00a0\u2026", "num_citations": "47\n", "authors": ["1575"]}
{"title": "Cimplifier: automatically debloating containers\n", "abstract": " Application containers, such as those provided by Docker, have recently gained popularity as a solution for agile and seamless software deployment. These light-weight virtualization environments run applications that are packed together with their resources and configuration information, and thus can be deployed across various software platforms. Unfortunately, the ease with which containers can be created is oftentimes a double-edged sword, encouraging the packaging of logically distinct applications, and the inclusion of significant amount of unnecessary components, within a single container. These practices needlessly increase the container size-sometimes by orders of magnitude. They also decrease the overall security, as each included component-necessary or not-may bring in security issues of its own, and there is no isolation between multiple applications packaged within the same container image\u00a0\u2026", "num_citations": "46\n", "authors": ["1575"]}
{"title": "Isomorph-free model enumeration: A new method for checking relational specifications\n", "abstract": " Software specifications often involve data structures with huge numbers of value, and consequently they cannot be checked using standard state exploration or model-checking techniques. Data structures can be expressed with binary relations, and operations over such structures can be expressed as formulae involving relational variables. Checking properties such as preservation of an invariant thus reduces to determining the validity of a formula or, equivalently, finding a model (of the formula's negation). A new method for finding relational models is presented. It exploits the permutation invariance of models\u2014if two interpretations are isomorphic, then neither is a model, or both are\u2014by partitioning the space into equivalence classes of symmetrical interpretations.  Representatives of these classes are constructed incrementally by using the symmetry of the partial interpretation to limit the enumeration of new\u00a0\u2026", "num_citations": "45\n", "authors": ["1575"]}
{"title": "Privacy-preserving ridge regression with only linearly-homomorphic encryption\n", "abstract": " Linear regression with 2-norm regularization (i.e., ridge regression) is an important statistical technique that models the relationship between some explanatory values and an outcome value using a linear function. In many applications (e.g., predictive modeling in personalized health-care), these values represent sensitive data owned by several different parties who are unwilling to share them. In this setting, training a linear regression model becomes challenging and needs specific cryptographic solutions. This problem was elegantly addressed by Nikolaenko et al. in S&P (Oakland)\u00a02013. They suggested a two-server system that uses linearly-homomorphic encryption (LHE) and Yao\u2019s two-party protocol (garbled circuits). In this work, we propose a novel system that can train a ridge linear regression model using only LHE (i.e., without using Yao\u2019s protocol). This greatly improves the overall performance\u00a0\u2026", "num_citations": "44\n", "authors": ["1575"]}
{"title": "End-to-end software diversification of internet services\n", "abstract": " Software diversification has been approached as a tool to provide security guarantees for programs that lack type safety (e.g., programs written in C). In this setting, diversification operates by changing the memory layout of program code or data and by changing the syntax of program code. These techniques succeed as a defense against an attacker\u2019s use of type-safety vulnerabilities (e.g., buffer overflows) because they randomize the key elements necessary to a successful low-level intrusion (memory addresses and memory contents). This chapter proposes to extend software diversification from a point technique, applied to hand-picked aspects of a single program, to an comprehensive technique applied by default to all components of an application. Internet services is used as a focused example here.", "num_citations": "44\n", "authors": ["1575"]}
{"title": "Towards hpc-abds: An initial high-performance big data stack\n", "abstract": " Many scientific problems depend on the ability to analyze and compute on large amounts of data. This analysis often does not scale well, ie its effectiveness is hampered by the increasing volume, variety and rate of change (velocity) of big data. There is a need to integrate features of traditional high-performance computing, such as scientific libraries, communication and resource management middleware, with the rich set of capabilities found in the commercial Big Data ecosystem, resulting in an integrated system generically called high-performance big data system (HPBDS). Our proposed preliminary implementation of the HPBDS\u2013includes many important software systems such as Hadoop available from the Apache open source community and thus referred to as High-Performance Computing-Big Data Stack (HPC\u2013ABDS)\u2013has two fundamental building blocks:(i) Middleware for Data-Intensive Analytics and Science (MIDAS) that will enable scalable applications with the performance of HPC (High Performance Computing) and the rich functionality of the commodity Apache Big Data Stack.(ii) The second building block will design and implement a set of cross-cutting high-performance data-analysis libraries SPIDAL (Scalable Parallel Interoperable Data Analytics Library), which will support new programming and execution models for data-intensive analysis in a wide range of science and engineering applications. These libraries will be implemented to be scalable and interoperable across a range of computing systems including clouds, clusters and supercomputers. The project libraries will have the same beneficial impact on data analytics\u00a0\u2026", "num_citations": "43\n", "authors": ["1575"]}
{"title": "Exploring connections between active learning and model extraction\n", "abstract": " Machine learning is being increasingly used by individuals, research institutions, and corporations. This has resulted in the surge of Machine Learning-as-a-Service (MLaaS)-cloud services that provide (a) tools and resources to learn the model, and (b) a user-friendly query interface to access the model. However, such MLaaS systems raise privacy concerns such as model extraction. In model extraction attacks, adversaries maliciously exploit the query interface to steal the model. More precisely, in a model extraction attack, a good approximation of a sensitive or proprietary model held by the server is extracted (ie learned) by a dishonest user who interacts with the server only via the query interface. This attack was introduced by Tram\u00e8r et. al. at the 2016 USENIX Security Symposium, where practical attacks for various models were shown. We believe that better understanding the efficacy of model extraction attacks is paramount to designing secure MLaaS systems. To that end, we take the first step by (a) formalizing model extraction and discussing possible defense strategies, and (b) drawing parallels between model extraction and established area of active learning. In particular, we show that recent advancements in the active learning domain can be used to implement powerful model extraction attacks and investigate possible defense strategies.", "num_citations": "42\n", "authors": ["1575"]}
{"title": "Theory and techniques for automatic generation of vulnerability-based signatures\n", "abstract": " In this paper, we explore the problem of creating \\emph{vulnerability signatures}. A vulnerability signature is based on a program vulnerability, and is not specific to any particular exploit. The advantage of vulnerability signatures is that their quality can be guaranteed. In particular, we create vulnerability signatures which are guaranteed to have zero false positives. We show how to automate signature creation for any vulnerability that can be detected by a runtime monitor. We provide a formal definition of a vulnerability signature, and investigate the computational complexity of creating and matching vulnerability signatures. We systematically explore the design space of vulnerability signatures. We also provide specific techniques for creating vulnerability signatures in a variety of language classes. In order to demonstrate our techniques, we have built a prototype system. Our experiments show that we can, using a\u00a0\u2026", "num_citations": "42\n", "authors": ["1575"]}
{"title": "A Logic of File Systems.\n", "abstract": " Years of innovation in file systems have been highly successful in improving their performance and functionality, but at the cost of complicating their interaction with the disk. A variety of techniques exist to ensure consistency and integrity of file system data, but the precise set of correctness guarantees provided by each technique is often unclear, making them hard to compare and reason about. The absence of a formal framework has hampered detailed verification of file system correctness. We present a logical framework for modeling the interaction of a file system with the storage system, and show how to apply the logic to represent and prove correctness properties. We demonstrate that the logic provides three main benefits. First, it enables reasoning about existing file system mechanisms, allowing developers to employ aggressive performance optimizations without fear of compromising correctness. Second, the logic simplifies the introduction and adoption of new file system functionality by facilitating rigorous proof of their correctness. Finally, the logic helps reason about smart storage systems that track semantic information about the file system. A key aspect of the logic is that it enables incremental modeling, significantly reducing the barrier to entry in terms of its actual use by file system designers. In general, we believe that our framework transforms the hitherto esoteric and error-prone \u201cart\u201d of file system design into a readily understandable and formally verifiable process.", "num_citations": "42\n", "authors": ["1575"]}
{"title": "Fusion and filtering in distributed intrusion detection systems\n", "abstract": " False alarms and timely identification of new attacks are two of the biggest challenges to the effective use of network intrusion detection systems (NIDS). A potential means for addressing these shortcomings in modern NIDS is employing multiple, distributed network intrusion detection systems (DNIDS). In this paper we consider the potential benefits of DNIDS by addressing two open problems. The first problem is how to combine data from multiple intrusion sensors in a network. This is known as the fusion problem. The second problem is how to identify the most important data provided by multiple sensors in a network. This is known as the filtering problem. We develop a series of analytic and simulation models to assess the potential benefits of DNIDS for reducing false alarms and improving timeliness of detection for different fusion and filtering strategies. Our analysis explores the trade-offs when fusion and filtering are used together and shows that significant improvements are possible.", "num_citations": "41\n", "authors": ["1575"]}
{"title": "Crypt\u03f5: Crypto-assisted differential privacy on untrusted servers\n", "abstract": " Differential privacy (DP) is currently the de-facto standard for achieving privacy in data analysis, which is typically implemented either in the\" central\" or\" local\" model. The local model has been more popular for commercial deployments as it does not require a trusted data collector. This increased privacy, however, comes at the cost of utility and algorithmic expressibility as compared to the central model. In this work, we propose, Crypt\u03b5, a system and programming framework that (1) achieves the accuracy guarantees and algorithmic expressibility of the central model (2) without any trusted data collector like in the local model. Crypt\u03b5 achieves the\" best of both worlds\" by employing two non-colluding untrusted servers that run DP programs on encrypted data from the data owners. In theory, straightforward implementations of DP programs using off-the-shelf secure multi-party computation tools can achieve the above\u00a0\u2026", "num_citations": "40\n", "authors": ["1575"]}
{"title": "Hpc-abds high performance computing enhanced apache big data stack\n", "abstract": " We review the High Performance Computing Enhanced Apache Big Data Stack HPC-ABDS and summarize the capabilities in 21 identified architecture layers. These cover Message and Data Protocols, Distributed Coordination, Security & Privacy, Monitoring, Infrastructure Management, DevOps, Interoperability, File Systems, Cluster & Resource management, Data Transport, File management, NoSQL, SQL (NewSQL), Extraction Tools, Object-relational mapping, In-memory caching and databases, Inter-process Communication, Batch Programming model and Runtime, Stream Processing, High-level Programming, Application Hosting and PaaS, Libraries and Applications, Workflow and Orchestration. We summarize status of these layers focusing on issues of importance for data analytics. We highlight areas where HPC and ABDS have good opportunities for integration.", "num_citations": "39\n", "authors": ["1575"]}
{"title": "Understanding scientific applications for cloud environments\n", "abstract": " Distributed systems and their specific incarnations have evolved significantly over the years. Most often, these evolutionary steps have been a consequence of external technology trends, such as the significant increase in network/bandwidth capabilities that have occurred. It can be argued that the single most important driver for cloud computing environments is the advance in virtualization technology that has taken place. But what implications does this advance, leading to today\u2019s cloud environments, have for scientific applications? The aim of this chapter is to explore how clouds can support scientific applications. Before we can address this important issue, it is imperative to (a) provide a working model and definition of clouds and (b) understand how they differ from other computational platforms such as grids and clusters. At a high level, cloud computing is defined by Mell and Grance [1] as a model for enabling convenient, on-demand network access to a shared pool of configurable computing resources (eg, networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction.We view clouds not as a monolithic isolated platform but as part of a large distributed ecosystem. But are clouds a natural evolution of distributed systems, or are they a fundamental new paradigm? Prima facie, cloud concepts are derived from other systems, such as the implicit model of clusters as static", "num_citations": "38\n", "authors": ["1575"]}
{"title": "Speculative parallel pattern matching\n", "abstract": " Intrusion prevention systems (IPSs) determine whether incoming traffic matches a database of signatures, where each signature is a regular expression and represents an attack or a vulnerability. IPSs need to keep up with ever-increasing line speeds, which has lead to the use of custom hardware. A major bottleneck that IPSs face is that they scan incoming packets one byte at a time, which limits their throughput and latency. In this paper, we present a method to search for arbitrary regular expressions by scanning multiple bytes in parallel using speculation. We break the packet in several chunks, opportunistically scan them in parallel, and if the speculation is wrong, correct it later. We present algorithms that apply speculation in single-threaded software running on commodity processors as well as algorithms for parallel hardware. Experimental results show that speculation leads to improvements in latency and\u00a0\u2026", "num_citations": "38\n", "authors": ["1575"]}
{"title": "Next generation workload management system for big data on heterogeneous distributed computing\n", "abstract": " The Large Hadron Collider (LHC), operating at the international CERN Laboratory in Geneva, Switzerland, is leading Big Data driven scientific explorations. Experiments at the LHC explore the fundamental nature of matter and the basic forces that shape our universe, and were recently credited for the discovery of a Higgs boson. ATLAS and ALICE are the largest collaborations ever assembled in the sciences and are at the forefront of research at the LHC. To address an unprecedented multi-petabyte data processing challenge, both experiments rely on a heterogeneous distributed computational infrastructure. The ATLAS experiment uses PanDA (Production and Data Analysis) Workload Management System (WMS) for managing the workflow for all data processing on hundreds of data centers. Through PanDA, ATLAS physicists see a single computing facility that enables rapid scientific breakthroughs for the\u00a0\u2026", "num_citations": "37\n", "authors": ["1575"]}
{"title": "Efficient runtime environment for coupled multi-physics simulations: Dynamic resource allocation and load-balancing\n", "abstract": " Coupled Multi-Physics simulations, such as hybrid CFD-MD simulations, represent an increasingly important class of scientific applications. Often the physical problems of interest demand the use of high-end computers, such as TeraGrid resources, which are often accessible only via batch-queues. Batch-queue systems are not developed to natively support the coordinated scheduling of jobs - which in turn is required to support the concurrent execution required by coupled multi-physics simulations. In this paper we develop and demonstrate a novel approach to overcome the lack of native support for coordinated job submission requirement associated with coupled runs. We establish the performance advantages arising from our solution, which is a generalization of the Pilot-Job concept - which in of itself is not new, but is being applied to coupled simulations for the first time. Our solution not only overcomes the\u00a0\u2026", "num_citations": "37\n", "authors": ["1575"]}
{"title": "Determination of free energy profiles for the translocation of polynucleotides through \u03b1-hemolysin nanopores using non-equilibrium molecular dynamics simulations\n", "abstract": " The translocation of polynucleotides through transmembrane protein pores is a fundamental biological process with important technological and medical relevance. The translocation process is complex, and it is influenced by a range of factors including the diameter and inner surface of the pore, the secondary structure of the polymer, and the interactions between the polymer and protein. In this paper, we perform nonequilibrium constant velocity-steered molecular dynamics simulations of nucleic acid molecule translocation through the protein nanopore \u03b1-hemolysin and use Jarzynski\u2019s identity to determine the associated free energy profiles. With this approach we are able to explain the observed differences in experimental translocation time through the nanopore between polyadenosine and polydeoxycytidine. The translocation of polynucleotides and single nucleotides through \u03b1-hemolysin is investigated\u00a0\u2026", "num_citations": "37\n", "authors": ["1575"]}
{"title": "Avfi: Fault injection for autonomous vehicles\n", "abstract": " Autonomous vehicle (AV) technology is rapidly becoming a reality on U.S. roads, offering the promise of improvements in traffic management, safety, and the comfort and efficiency of vehicular travel. With this increasing popularity and ubiquitous deployment, resilience has become a critical requirement for public acceptance and adoption. Recent studies into the resilience of AVs have shown that though the AV systems are improving over time, they have not reached human levels of automation. Prior work in this area has studied the safety and resilience of individual components of the AV system (e.g., testing of neural networks powering the perception function). However, methods for holistic end-to-end resilience assessment of AV systems are still non-existent.", "num_citations": "36\n", "authors": ["1575"]}
{"title": "Beyond pattern matching: A concurrency model for stateful deep packet inspection\n", "abstract": " The ever-increasing sophistication in network attacks, combined with larger and larger volumes of traffic, presents a dual challenge to network intrusion detection systems (IDSs). On one hand, to take advantage of modern multi-core processing platforms IDSs need to support scalability, by distributing traffic analysis across a large number of processing units. On the other hand, such scalability must not come at the cost of decreased effectiveness in attack detection. In this paper, we present a novel domain-specific concurrency model that addresses this challenge by introducing the notion of detection scope: a unit for partitioning network traffic such that the traffic contained in each resulting\" slice\" is independent for detection purposes. The notion of scope enables IDSs to automatically distribute traffic processing, while ensuring that information necessary to detect intrusions remains available to detector instances. We\u00a0\u2026", "num_citations": "36\n", "authors": ["1575"]}
{"title": "Towards an understanding of facets and exemplars of big data applications\n", "abstract": " We study many Big Data applications from a variety of research and commercial areas and suggest a set of characteristic features and possible kernel benchmarks that stress those features for data analytics. We draw conclusions for the hardware and software architectures that are suggested by this analysis.", "num_citations": "35\n", "authors": ["1575"]}
{"title": "Answering multi-dimensional analytical queries under local differential privacy\n", "abstract": " Multi-dimensional analytical (MDA) queries are often issued against a fact table with predicates on (categorical or ordinal) dimensions and aggregations on one or more measures. In this paper, we study the problem of answering MDA queries under local differential privacy (LDP). In the absence of a trusted agent, sensitive dimensions are encoded in a privacy-preserving (LDP) way locally before being sent to the data collector. The data collector estimates the answers to MDA queries, based on the encoded dimensions. We propose several LDP encoders and estimation algorithms, to handle a large class of MDA queries with different types of predicates and aggregation functions. Our techniques are able to answer these queries with tight error bounds and scale well in high-dimensional settings (ie, error is polylogarithmic in dimension sizes). We conduct experiments on real and synthetic data to verify our\u00a0\u2026", "num_citations": "34\n", "authors": ["1575"]}
{"title": "Design and implementation of network performance aware applications using SAGA and Cactus\n", "abstract": " This paper demonstrates the use of appropriate programming abstractions - SAGA and cactus - that facilitate the development of applications for distributed infrastructure. SAGA provides a high-level programming interface to Grid- functionality; Cactus is an extensible, component based framework for scientific applications. We show how SAGA can be integrated with cactus to develop simple, useful and easily extensible applications that can be deployed on a wide variety of distributed infrastructure, independent of the details of the resources. Our model application can gather and analyze network performance data and migrate across heterogeneous resources. We outline the architecture of our application and discuss how it imparts important features required of eScience applications. As a proof-of-concept, we present details of the successful deployment of our application over distinct and heterogeneous Grids\u00a0\u2026", "num_citations": "34\n", "authors": ["1575"]}
{"title": "Grid interoperability at the application level using SAGA\n", "abstract": " SAGA is a high-level programming abstraction, which significantly facilitates the development and deployment of Grid-aware applications. The primary aim of this paper is to discuss how each of the three main components of the SAGA landscape - interface specification, specific implementation and the different adaptors for middleware distribution - facilitate application-level interoperability. We discuss SAGA in relation to the ongoing GIN Community Group efforts and show the consistency of the SAGA approach with the GIN Group efforts. We demonstrate how interoperability can be enabled by the use of SAGA, by discussing two simple, yet meaningful applications: in the first, SAGA enables applications to utilize interoperability and in the second example SAGA adaptors provide the basis for interoperability.", "num_citations": "33\n", "authors": ["1575"]}
{"title": "Medicine bottle magnifying lens\n", "abstract": " A magnification device for assisting people to read the small print on medicine vials/bottles. The device including a magnifying lens having the ability to move towards or away from the indicia on the bottle to further aid the user to focus per their particular level of eyesight. A pair of rods are coaxial disposed within a corresponding pair of channels to enable the lens to be focused by pushing or pulling the lens. The present invention is designed to be used with virtually all manufactured bottle caps. The device can either be permanently affixed to the bottle cap, and thereby be disposable with the cap; or it can be removably connected and therefore be reusable.", "num_citations": "33\n", "authors": ["1575"]}
{"title": "Software design decisions as real options\n", "abstract": " Despite their status as foundational concepts in software engineering, many software design decision-making principles and heuristics, such as information hiding and the delaying of design decisions, are still idiosyncratic, ad hoc, poorly integrated and not clearly based on any sound theory. In this paper, we develop an economics-based approach to providing a firmer foundation for software design decision-making heuristics. We start with the premise is that many software design decisions are essentially about when if ever to make irreversible but delayable investments of valuable resources in software assets of uncertain value. This formulation reveals an analogy between software design decisions and real options, which are capital investment analogs of financial call options, for which there is a well-developed theory and body of knowledge. In particular, the theory of real options captures precisely the idea that there can be significant value in having flexibility to wait for better information before committing valuable resources to develop or obtain assets. The options-theoretic nature of many software design decisions allows us to bring option theory to bear on an analysis and refinement of critical, widely employed software design decision-making heuristics.", "num_citations": "33\n", "authors": ["1575"]}
{"title": "Libfte: A toolkit for constructing practical, format-abiding encryption schemes\n", "abstract": " Encryption schemes where the ciphertext must abide by a specified format have diverse applications, ranging from in-place encryption in databases to per-message encryption of network traffic for censorship circumvention. Despite this, a unifying framework for deploying such encryption schemes has not been developed. One consequence of this is that current schemes are ad-hoc; another is a requirement for expert knowledge that can disuade one from using encryption at all.", "num_citations": "32\n", "authors": ["1575"]}
{"title": "Secure computations on non-integer values\n", "abstract": " In this paper we present for the first time a framework that allows secure two-party computations on approximations of real valued signals. In our solution, we use a quantized logarithmic representation of the signal samples, which enables to represent both very small and very large numbers with bounded relative error. We show that numbers represented in this way can be encrypted using standard homomorphic encryption schemes; furthermore we give protocols that allow to perform all arithmetic operations on such encrypted values. Finally we demonstrate the practicality of our framework by applying it to the problem of filtering encrypted signals.", "num_citations": "32\n", "authors": ["1575"]}
{"title": "Exploring application and infrastructure adaptation on hybrid grid-cloud infrastructure\n", "abstract": " Clouds are emerging as an important class of distributed computational resources and are quickly becoming an integral part of production computational infrastructures. An important but oft-neglected question is, what new applications and application capabilities can be supported by clouds as part of a hybrid computational platform? In this paper we use the ensemble Kalman-filter based dynamic application workflow and investigate how clouds can be effectively used as an accelerator to address changing computational requirements as well as changing Quality of Service constraints (eg, deadlines). Furthermore, we explore how application and system-level adaptivity can be used to improve application performance and achieve a more effective utilization of the hybrid platform. Specifically, we adapt the ensemble Kalman-filter based application formulation (serial versus parallel, different solvers etc.) so as to\u00a0\u2026", "num_citations": "32\n", "authors": ["1575"]}
{"title": "Adaptive distributed replica\u2013exchange simulations\n", "abstract": " Owing to the loose coupling between replicas, the replica\u2013exchange (RE) class of algorithms should be able to benefit greatly from using as many resources as available. However, the ability to effectively use multiple distributed resources to reduce the time to completion remains a challenge at many levels. Additionally, an implementation of a pleasingly distributed algorithm such as replica\u2013exchange, which is independent of infrastructural details, does not exist. This paper proposes an extensible and scalable framework based on Simple API for Grid Applications that provides a general-purpose, opportunistic mechanism to effectively use multiple resources in an infrastructure-independent way. By analysing the requirements of the RE algorithm and the challenges of implementing it on real production systems, we propose a new abstraction (BigJob), which forms the basis of the adaptive redistribution and effective\u00a0\u2026", "num_citations": "32\n", "authors": ["1575"]}
{"title": "Steven Shreve: Stochastic Calculus and Finance\n", "abstract": " Steven Shreve: Stochastic Calculus and Finance Page 1 Steven Shreve: Stochastic Calculus and Finance PRASAD CHALASANI Carnegie Mellon University chal@cs.cmu.edu SOMESH JHA Carnegie Mellon University sjha@cs.cmu.edu THIS IS A DRAFT: PLEASE DO NOT DISTRIBUTE c Copyright; Steven E. Shreve, 1996 October 6, 1997 Page 2 Contents 1 Introduction to Probability Theory 11 1.1 TheBinomialAssetPricingModel . . . . . . . . . . . . . . . . . . . . . . . . . . 11 1.2 FiniteProbabilitySpaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 1.3 LebesgueMeasureandtheLebesgueIntegral . . . . . . . . . . . . . . . . . . . . 22 1.4 GeneralProbabilitySpaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 1.5 Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 1.5.1 Independenceofsets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 1.5.2 Independence of \u00a1 -algebras . . . . . . . . . . . . . . . . . . . . . . . . . 41 1.5.3 Independenceofrandomvariables . . . . . . . . . . . . . . . . . . . . \u2026", "num_citations": "32\n", "authors": ["1575"]}
{"title": "Bio-inspired machine learning based wireless sensor network security\n", "abstract": " Exploring the symbiotic nature of biological systems can result in valuable knowledge for computer networks. Biologically inspired approaches to security in networks are interesting to evaluate because of the analogies between network security and survival of human body under pathogenic attacks. Wireless Sensor Network (WSN) is a network based on multiple low-cost, low-energy sensor nodes connected to physical signals. The network is made up of sensor nodes and gateways, where the server nodes acquire physical world data, while the gateway forwards the data to the end-user. While the spread of viruses in wired systems has been studied in-depth, applying trust in wireless sensor network nodes is an emerging area. This paper uses machine learning techniques to first differentiate between fraudulent and good nodes in the system. Next, it derives inspiration from the human immune system to present an\u00a0\u2026", "num_citations": "31\n", "authors": ["1575"]}
{"title": "Towards secure bioinformatics services (short paper)\n", "abstract": " In this paper we show how privacy of genomic sequences can be protected while they are analyzed using Hidden Markov Models (HMM), which is commonly done in bioinformatics to detect certain non-beneficial patterns in the genome. Besides offering strong privacy guarantees, our solution also allows protecting the intellectual property of the parties involved, which makes the solution viable for implementation of secure bioinformatics services. In particular, we show how two mutually mistrusting parties can obliviously run the forward algorithm in a setup where one party knows a HMM and another party knows a genomic string; while the parties learn whether the model fits the genome, they neither have to disclose the parameterization of the model nor the sequence to each other. Despite the huge number of arithmetic operations required to solve the problem, we experimentally show that HMMs with\u00a0\u2026", "num_citations": "31\n", "authors": ["1575"]}
{"title": "Grid-based steered thermodynamic integration accelerates the calculation of binding free energies\n", "abstract": " The calculation of binding free energies is important in many condensed matter problems. Although formally exact computational methods have the potential to complement, add to, and even compete with experimental approaches, they are difficult to use and extremely time consuming. We describe a Grid-based approach for the calculation of relative binding free energies, which we call Steered Thermodynamic Integration calculations using Molecular Dynamics (STIMD), and its application to Src homology 2 (SH2) protein cell signalling domains. We show that the time taken to compute free energy differences using thermodynamic integration can be significantly reduced: potentially from weeks or months to days of wall-clock time. To be able to perform such accelerated calculations requires the ability to both run concurrently and control in realtime several parallel simulations on a computational Grid. We describe\u00a0\u2026", "num_citations": "31\n", "authors": ["1575"]}
{"title": "Satisfiability modulo counting: A new approach for analyzing privacy properties\n", "abstract": " Applications increasingly derive functionality from sensitive personal information, forcing developers who wish to preserve some notion of privacy or confidentiality to reason about partial information leakage. New definitions of privacy and confidentiality, such as differential privacy, address this by offering precise statements of acceptable disclosure that are useful in common settings. However, several recent published accounts of flawed implementations have surfaced, highlighting the need for verification techniques.", "num_citations": "30\n", "authors": ["1575"]}
{"title": "CrowdMine: towards crowdsourced human-assisted verification\n", "abstract": " We propose the use of crowdsourcing and human computation to help solve difficult problems in verification and debugging that can benefit from human insight. As a specific scenario, we explain how non-expert humans can assist in the verification process by finding patterns in portions of simulation or execution traces which are represented as images. Such patterns can be used in a variety of ways, including assertion-based verification, improving coverage, bug localization, and error explanation. Several related issues are discussed, including privacy and incentive mechanisms.", "num_citations": "30\n", "authors": ["1575"]}
{"title": "Smart contracts and opportunities for formal methods\n", "abstract": " Smart contracts are programs that run atop of a blockchain infrastructure. They have emerged as an important new programming model in cryptocurrencies like Ethereum, where they regulate flow of money and other digital assets according to user-defined rules. However, the most popular smart contract languages favor expressiveness rather than safety, and bugs in smart contracts have already lead to significant financial losses from accidents. Smart contracts are also appealing targets for hackers since they can be monetized. For these reasons, smart contracts are an appealing opportunity for systematic auditing and validation, and formal methods in particular. In this paper, we survey the existing smart-contract ecosystem and the existing tools for analyzing smart contracts. We then pose research challenges for formal-methods and program analysis applied to smart contracts.", "num_citations": "29\n", "authors": ["1575"]}
{"title": "Logdiver: A tool for measuring resilience of extreme-scale systems and applications\n", "abstract": " This paper presents LogDiver, a tool for the analysis of application-level resiliency in extreme-scale computing systems. The tool has been implemented to handle data generated by system monitoring tools in Blue Waters, the petascale machine in production at the University of Illinois' National Center for Supercomputing Applications. The tool is able: i) to filter, extract, and classify error data from different sources of information, such as system logs, hardware sensors and workload logs; ii) to extract signals from the categorized errors; iii) to consolidate user application data and decode application and job exit status, highlighting the reasons for the application/job exit; and iv) to correlate application failures with errors using a mix of empirical and analytical techniques. To the best of our knowledge, this is the first tool capable of measuring application-level resiliency in extreme-scale machines. We also demonstrate\u00a0\u2026", "num_citations": "29\n", "authors": ["1575"]}
{"title": "Novel approach for security in wireless sensor network using bio-inspirations\n", "abstract": " Exploring the symbiotic nature of biological systems can result in valuable knowledge for computer networks. Biologically inspired approaches to security in networks are interesting to evaluate because of the analogies between network security and survival of human body under pathogenic attacks. Wireless Sensor Network (WSN) is a network based on multiple low-cost communication and computing devices connected to sensor nodes which sense physical parameters. While the spread of viruses in wired systems has been studied in-depth, applying trust in WSN is an emerging research area. Security threats can be introduced in WSN through various means, such as a benevolent sensor node turning fraudulent after a certain period of time. The proposed research work uses biological inspirations and machine learning techniques for adding security against such threats. While it uses machine learning\u00a0\u2026", "num_citations": "29\n", "authors": ["1575"]}
{"title": "Effective blame for information-flow violations\n", "abstract": " Programs trusted with secure information should not release that information in ways contrary to system policy. However, when a program contains an illegal flow of information, current information-flow reporting techniques are inadequate for determining the cause of the error. Reasoning about information-flow errors can be difficult, as the flows involved can be quite subtle. We present a general model for information-flow blame that can explain the source of such security errors in code. This model is implemented by changing the information-flow verification procedure to:(1) generate supplementary information to reveal otherwise hidden program dependencies;(2) modify the constraint solver to construct a blame dependency graph; and (3) develop an explanation procedure that returns a complete and minimal error report. Our experiments show that information-flow errors can generally be explained and resolved\u00a0\u2026", "num_citations": "29\n", "authors": ["1575"]}
{"title": "Revisiting differentially private regression: Lessons from learning theory and their consequences\n", "abstract": " Private regression has received attention from both database and security communities. Recent work by Fredrikson et al. (USENIX Security 2014) analyzed the functional mechanism (Zhang et al. VLDB 2012) for training linear regression models over medical data. Unfortunately, they found that model accuracy is already unacceptable with differential privacy when . We address this issue, presenting an explicit connection between differential privacy and stable learning theory through which a substantially better privacy/utility tradeoff can be obtained. Perhaps more importantly, our theory reveals that the most basic mechanism in differential privacy, output perturbation, can be used to obtain a better tradeoff for all convex-Lipschitz-bounded learning tasks. Since output perturbation is simple to implement, it means that our approach is potentially widely applicable in practice. We go on to apply it on the same medical data as used by Fredrikson et al. Encouragingly, we achieve accurate models even for . In the last part of this paper, we study the impact of our improved differentially private mechanisms on model inversion attacks, a privacy attack introduced by Fredrikson et al. We observe that the improved tradeoff makes the resulting differentially private model more susceptible to inversion attacks. We analyze this phenomenon formally.", "num_citations": "28\n", "authors": ["1575"]}
{"title": "Efficient type matching\n", "abstract": " Palsberg and Zhao [17] presented an O(n                 2) time algorithm for matching two recursive types. In this paper, we present an O(n log n) algorithm for the same problem. Our algorithm works by reducing the type matching problem to the well-understood problem of finding a size-stable partition of a graph. Our result may help improve systems, such as Polyspin and Mockingbird, that are designed to facilitate interoperability of software components.We also discuss possible applications of our algorithm to Java. Issues related to subtyping of recursive types are also discussed.", "num_citations": "28\n", "authors": ["1575"]}
{"title": "Approximate option pricing\n", "abstract": " As increasingly large volumes of sophisticated options are traded in world financial markets, determining a ``fair'' price for these options has become an important and difficult computational problem. Many valuation codes use the binomial pricing model, in which the stock price is driven by a random walk. In this model, the value of an n -period option on a stock is the expected time-discounted value of the future cash flow on an n -period stock price path. Path-dependent options are particularly difficult to value since the future cash flow depends on the entire stock price path rather than on just the final stock price. Currently such options are approximately priced by Monte Carlo methods with error bounds that hold only with high probability and which are reduced by increasing the number of simulation runs.                In this article we show that pricing an arbitrary path-dependent option is \\#-P hard. We show\u00a0\u2026", "num_citations": "28\n", "authors": ["1575"]}
{"title": "Robust out-of-distribution detection for neural networks\n", "abstract": " Detecting anomalous inputs is critical for safely deploying deep learning models in the real world. Existing approaches for detecting out-of-distribution (OOD) examples work well when evaluated on natural samples drawn from a sufficiently different distribution than the training data distribution. However, in this paper, we show that existing detection mechanisms can be extremely brittle when evaluating on inputs with minimal adversarial perturbations which don't change their semantics. Formally, we introduce a novel and challenging problem, Robust Out-of-Distribution Detection, and propose an algorithm that can fool existing OOD detectors by adding small perturbations to the inputs while preserving their semantics and thus the distributional membership. We take a first step to solve this challenge, and propose an effective algorithm called ALOE, which performs robust training by exposing the model to both adversarially crafted inlier and outlier examples. Our method can be flexibly combined with, and render existing methods robust. On common benchmark datasets, we show that ALOE substantially improves the robustness of state-of-the-art OOD detection, with 58.4% AUROC improvement on CIFAR-10 and 46.59% improvement on CIFAR-100. Finally, we provide theoretical analysis for our method, underpinning the empirical results above.", "num_citations": "27\n", "authors": ["1575"]}
{"title": "Attribution-based confidence metric for deep neural networks\n", "abstract": " Deep neural networks (DNNs) have been shown to be brittle to inputs outside the distribution of training data, and to adversarial examples. This fragility is compounded by a lack of effectively computable measures of prediction confidence that correlate with the accuracy of DNNs. The direct use of logits severely overestimates confidence. These factors have impeded the adoption of DNNs in high-assurance systems. In this paper, we propose a novel confidence metric that does not require access to the training data, the use of model ensembles, or the need to train a calibration model on a held-out validation set, and hence, is usable even when only a trained model is available at inference time. A lightweight approach to quantify uncertainty in the output of a model and define a confidence metric is to measure the conformance of the model's decision in the neighborhood of the input. But measuring conformance by sampling in the neighborhood of an input becomes exponentially difficult with increase in the dimension of the input. We use the feature concentration observed in robust models for local dimensionality reduction and attribution-based sampling over the features to compute the confidence metric. We mathematically motivate the proposed metric, and evaluate its effectiveness with two sets of experiments. First, we study the change in accuracy and the associated confidence over out-of-distribution inputs and evaluate the correlation between the accuracy and computed confidence. We also compare our results with the use of logits to estimate uncertainty. Second, we consider attacks such as FGSM, CW, DeepFool, PGD, and adversarial\u00a0\u2026", "num_citations": "27\n", "authors": ["1575"]}
{"title": "Adaptive ensemble simulations of biomolecules\n", "abstract": " Recent advances in both theory and computational power have created opportunities to simulate biomolecular processes more efficiently using adaptive ensemble simulations. Ensemble simulations are now widely used to compute a number of individual simulation trajectories and analyze statistics across them. Adaptive ensemble simulations offer a further level of sophistication and flexibility by enabling high-level algorithms to control simulations-based on intermediate results. We review some of the adaptive ensemble algorithms and software infrastructure currently in use and outline where the complexities of implementing adaptive simulation have limited algorithmic innovation to date. We describe an adaptive ensemble API to overcome some of these barriers and more flexibly and simply express adaptive simulation algorithms to help realize the power of this type of simulation.", "num_citations": "27\n", "authors": ["1575"]}
{"title": "Learning neural markers of schizophrenia disorder using recurrent neural networks\n", "abstract": " Smart systems that can accurately diagnose patients with mental disorders and identify effective treatments based on brain functional imaging data are of great applicability and are gaining much attention. Most previous machine learning studies use hand-designed features, such as functional connectivity, which does not maintain the potential useful information in the spatial relationship between brain regions and the temporal profile of the signal in each region. Here we propose a new method based on recurrent-convolutional neural networks to automatically learn useful representations from segments of 4-D fMRI recordings. Our goal is to exploit both spatial and temporal information in the functional MRI movie (at the whole-brain voxel level) for identifying patients with schizophrenia.", "num_citations": "27\n", "authors": ["1575"]}
{"title": "Fast signature matching using extended finite automaton (XFA)\n", "abstract": " Automata-based representations and related algorithms have been applied to address several problems in information security, and often the automata had to be augmented with additional information. For example, extended finite-state automata (EFSA) augment finite-state automata (FSA) with variables to track dependencies between arguments of system calls. In this paper, we introduce extended finite automata (XFAs) which augment FSAs with finite scratch memory and instructions to manipulate this memory. Our primary motivation for introducing XFAs is signature matching in Network Intrusion Detection Systems (NIDS). Representing NIDS signatures as deterministic finite-state automata (DFAs) results in very fast signature matching but for several types of signatures DFAs can blowup in space. Nondeterministic finite-state automata (NFA) representation of NIDS signatures results in a succinct\u00a0\u2026", "num_citations": "27\n", "authors": ["1575"]}
{"title": "Scientific grid computing: The first generation\n", "abstract": " The scientific user's computing demands are becoming increasingly complex and can benefit from distributed resources, but effectively marshalling these distributed systems often introduces new challenges. The authors describe how researchers can exploit existing distributed grid infrastructure to get meaningful scientific results.", "num_citations": "27\n", "authors": ["1575"]}
{"title": "Robust attribution regularization\n", "abstract": " An emerging problem in trustworthy machine learning is to train models that produce robust interpretations for their predictions. We take a step towards solving this problem through the lens of axiomatic attribution of neural networks. Our theory is grounded in the recent work, Integrated Gradients (IG), in axiomatically attributing a neural network's output change to its input change. We propose training objectives in classic robust optimization models to achieve robust IG attributions. Our objectives give principled generalizations of previous objectives designed for robust predictions, and they naturally degenerate to classic soft-margin training for one-layer neural networks. We also generalize previous theory and prove that the objectives for different robust optimization models are closely related. Experiments demonstrate the effectiveness of our method, and also point to intriguing problems which hint at the need for better optimization techniques or better neural network architectures for robust attribution training.", "num_citations": "26\n", "authors": ["1575"]}
{"title": "SPICE: Simulated Pore Interactive Computing Environmen\n", "abstract": " SPICE aims to understand the vital process of translocation of biomolecules across protein pores by computing the free energy profile of the translocating biomolecule along the vertical axis of the pore. Without significant advances at the algorithmic, computing and analysis levels, understanding problems of this size and complexity will remain beyond the scope of computational science for the foreseeable future. A novel algorithmic advance is provided by a combination of Steered Molecular Dynamics and Jarzynski\u2019s Equation (SMD-JE); Grid computing provides the required new computing paradigm as well as facilitating the adoption of new analytical approaches. SPICE uses sophisticated grid infrastructure to couple distributed high performance simulations, visualization and instruments used in the analysis to the same framework. We describe how we utilize the resources of a federated trans-Atlantic Grid to use\u00a0\u2026", "num_citations": "26\n", "authors": ["1575"]}
{"title": "Symmetry and induction in model checking\n", "abstract": " With the increasing complexity of digital systems, testing of digital systems is becoming increasingly important. Perhaps, the most popular method for testing hardware is simulation. The incompleteness of simulation based testing methods has spurred the recent surge in the research on formal veri cation. In formal veri cation, one builds a precise model of the hardware under scrutiny and proves that the model satis es a speci cation of interest. For example, suppose one wants to verify that a router chip does not deadlock. In this case the user will build a precise model of the router and the speci cation will express the property of deadlock freedom. The two approaches to formal veri cation are model checking and theorem proving. In this thesis we will only discuss model checking. Most model checking procedures su er from the state explosion problem, ie, the size of the state space of the system can be exponential in the number of state variables of the system. For certain systems, exploiting the inherent symmetry can alleviate the state-explosion problem. We discuss Model Checking procedures which exploit symmetry. Current model checkers can only verify a single state-transition system at a time. We also want to extend the model checking techniques to handle in nite families of nite-state systems.In practice, nite state concurrent systems often exhibit considerable symmetry. We investigate techniques for reducing the complexity of temporal logic model checking in the presence of symmetry. In particular, we show that symmetry can frequently be used to reduce the size of the state space that must be explored during model checking. We also\u00a0\u2026", "num_citations": "26\n", "authors": ["1575"]}
{"title": "An Attack on InstaHide: Is Private Learning Possible with Instance Encoding?\n", "abstract": " A learning algorithm is private if the produced model does not reveal (too much) about its training set. InstaHide [Huang, Song, Li, Arora, ICML'20] is a recent proposal that claims to preserve privacy by an encoding mechanism that modifies the inputs before being processed by the normal learner. We present a reconstruction attack on InstaHide that is able to use the encoded images to recover visually recognizable versions of the original images. Our attack is effective and efficient, and empirically breaks InstaHide on CIFAR-10, CIFAR-100, and the recently released InstaHide Challenge. We further formalize various privacy notions of learning through instance encoding and investigate the possibility of achieving these notions. We prove barriers against achieving (indistinguishability based notions of) privacy through any learning protocol that uses instance encoding.", "num_citations": "25\n", "authors": ["1575"]}
{"title": "OAT: Attesting operation integrity of embedded devices\n", "abstract": " Due to the wide adoption of IoT/CPS systems, embedded devices (IoT frontends) become increasingly connected and mission-critical, which in turn has attracted advanced attacks (e.g., control-flow hijacks and data-only attacks). Unfortunately, IoT backends (e.g., remote controllers or in-cloud services) are unable to detect if such attacks have happened while receiving data, service requests, or operation status from IoT devices (remotely deployed embedded devices). As a result, currently, IoT backends are forced to blindly trust the IoT devices that they interact with.To fill this void, we first formulate a new security property for embedded devices, called \"Operation Execution Integrity\" or OEI. We then design and build a system, OAT, that enables remote OEI attestation for ARM-based bare-metal embedded devices. Our formulation of OEI captures the integrity of both control flow and critical data involved in an\u00a0\u2026", "num_citations": "25\n", "authors": ["1575"]}
{"title": "Automating security mediation placement\n", "abstract": " We present a framework that automatically produces suggestions to resolve type errors in security-typed programs, enabling legacy code to be retrofit with comprehensive security policy mediation. Resolving such type errors requires selecting a placement of mediation statements that implement runtime security decisions, such as declassifiers and authorization checks. Manually placing mediation statements in legacy code can be difficult, as there may be several, interacting type errors. In this paper, we solve this problem by constructing a graph that has the property that a vertex cut is equivalent to the points at which mediation statements can be inserted to allow the program to satisfy the type system. We build a framework that produces suggestions that are minimum cuts of this graph, and the framework can be customized to find suggestions that satisfy programmer requirements. Our framework\u00a0\u2026", "num_citations": "25\n", "authors": ["1575"]}
{"title": "Increasing resource utilization and task performance by agent cloning\n", "abstract": " Agents in a multi-agent system may face situations where tasks overload their computational capacities. Usually, this problem is solved by passing tasks to others or agent migration to remote hosts. We propose agent cloning as a more comprehensive approach to balancing local agent overloads. According to our paradigm, agents may clone, pass tasks to others, die or merge. We discuss the requirements of implementing a cloning mechanism and its benefits in a Multi-Agent System (MAS), and support our claims with simulation results.", "num_citations": "25\n", "authors": ["1575"]}
{"title": "Improving adversarial robustness by data-specific discretization\n", "abstract": " A recent line of research proposed (either implicitly or explicitly) gradient-masking preprocessing techniques to improve adversarial robustness. However, as shown by Athaley-Carlini-Wagner, essentially all these defenses can be circumvented if an attacker leverages approximate gradient information with respect to the preprocessing. This thus raises a natural question of whether there is a useful preprocessing technique in the context of white-box attacks, even just for only mildly complex datasets such as MNIST. In this paper we provide an affirmative answer to this question. Our key observation is that for several popular datasets, one can approximately encode entire dataset using a small set of separable codewords derived from the training set, while retaining high accuracy on natural images. The separability of the codewords in turn prevents small perturbations as in l\u221e attacks from changing feature encoding, leading to adversarial robustness. For example, for MNIST our code consists of only two codewords, 0 and 1, and the encoding of any pixel is simply 1 [x> 0.5](ie, whether a pixel x is at least 0.5). Applying this code to a naturally trained model already gives high adversarial robustness even under strong white-box attacks based on Backward Pass Differentiable Approximation (BPDA) method of Athaley-Carlini-Wagner that takes the codes into account. We give density-estimation based algorithms to construct such codes, and provide theoretical analysis and certificates of when our method can be effective. Systematic evaluation demonstrates that our method is effective in improving adversarial robustness on MNIST, CIFAR-10, and\u00a0\u2026", "num_citations": "24\n", "authors": ["1575"]}
{"title": "Security analysis of temporal RBAC under an administrative model\n", "abstract": " Security analysis of access control models is critical to confirm whether they ensure certain security properties. Administrative models specify the rules for state transition for any given access control model. While security analysis of role-based access control (RBAC) systems has been done using administrative models, work on security analysis of its temporal, spatial and spatio-temporal extensions has so far not considered the presence of any corresponding administrative model. In this paper, we present a methodology for performing security analysis of temporal RBAC (TRBAC) where state changes occur using the relations defined in a recently proposed administrative model named as AMTRAC (Administrative Model for Temporal Role-based Access Control). We initially define a number of security properties for TRBAC. These properties along with a representation of the TRBAC system and the administrative\u00a0\u2026", "num_citations": "24\n", "authors": ["1575"]}
{"title": "Symmetry and induction in model checking\n", "abstract": " Model checking is a technique for determining whether a finite state-transition system satisfies a specification expressed in temporal logic. It has been used successfully to verify a number of highly complex circuit and protocol designs. A major hurdle in using this approach to verify realistic designs is the state explosion problem. This problem tends to occur when the number of state variables is very large. In this paper we discuss two techniques for handling this problem. The first technique is based on exploiting symmetry in the state-transition graph. We show how to construct a reduced quotient graph that satisfies the same temporal properties as the original graph. The second technique applies to systems that can have an arbitrary number of processes. In this case induction at the process level can be used to avoid the state explosion problem. An invariant process can frequently be found whose\u00a0\u2026", "num_citations": "24\n", "authors": ["1575"]}
{"title": "Specification and verification of separation of duty constraints in attribute-based access control\n", "abstract": " Constraints form an important aspect of any access control system and are often regarded as one of the principle motivations behind developing different access control models. The two primary concerns related to a constraint are its specification and enforcement. Among the various types of constraints, enforcement of the Separation of Duty (SoD) constraint is considered to be the most important in commercial applications. In this paper, we introduce the problem of SoD specification, verification, and enforcement in attribute-based access control (ABAC) systems. We then demonstrate the effect of modifications in the different components of ABAC on enforcement. We also analyze the complexity of the enforcement problem and provide a methodology for solving it. Experiments on a wide range of data sets show encouraging results.", "num_citations": "23\n", "authors": ["1575"]}
{"title": "Towards a common model for pilot-jobs\n", "abstract": " Pilot-Jobs have become one of the most successful abstractions in distributed computing. In spite of extensive uptake, there does not exist a well defined, unifying conceptual model of pilot-jobs which can be used to define, compare and contrast different implementations. This presents a barrier to extensibility and interoperability. This paper is an attempt to,(i) provide a minimal but complete model (P*) of pilot-jobs,(ii) establish the generality of the P* Model by mapping various existing and well known pilot-jobs frameworks such as Condor and DIANE to P*,(iii) demonstrate the interoperable and concurrent usage of distinct pilot-job frameworks on different production distributed cyberinfrastructures via the use of an extensible API for the P* Model (Pilot-API).", "num_citations": "22\n", "authors": ["1575"]}
{"title": "Reinforcing adversarial robustness using model confidence induced by adversarial training\n", "abstract": " In this paper we study leveraging confidence information induced by adversarial training to reinforce adversarial robustness of a given adversarially trained model. A natural measure of confidence is (ie how confident  is about its prediction?). We start by analyzing an adversarial training formulation proposed by Madry et al.. We demonstrate that, under a variety of instantiations, an only somewhat good solution to their objective induces confidence to be a discriminator, which can distinguish between right and wrong model predictions in a neighborhood of a point sampled from the underlying distribution. Based on this, we propose Highly Confident Near Neighbor (HCNN) a framework that combines confidence information and nearest neighbor search, to reinforce adversarial robustness of a base model. We give algorithms in this framework and perform a detailed empirical study. We report encouraging experimental results that support our analysis, and also discuss problems we observed with existing adversarial training.", "num_citations": "21\n", "authors": ["1575"]}
{"title": "Enforcing separation of duty in attribute based access control systems\n", "abstract": " Conventional access control models like discretionary access control and role based access control are suitable for regulating access to resources by known users of an organization. However, for systems where the user population is dynamic and the identities of all users are not known in advance, attribute based access control (ABAC) can be more conveniently used. The set of constraints supported by an access control model acts as a deciding factor for the type of restrictions it can put on unauthorized access. Among the various types of constraints, enforcement of Separation of Duty (SoD) is considered to be the most important in any commercial application. In this paper, we introduce the problem of SoD enforcement in the context of ABAC. We analyze the complexity of the problem and provide a methodology for solving it. Experiments on a wide range of data sets show encouraging results.", "num_citations": "21\n", "authors": ["1575"]}
{"title": "Computational modeling of novel InN/Al0. 30In0. 70N multilayer nano-heterostructure\n", "abstract": " Most of the low dimension heterostructures that have been modeled and simulated to determine various important quantum mechanical parameters are based on GaN/AlGaN and GaAs/AlGaAs. The heterostructures of newly invented material (InN/AlInN), however, have not been well studied. In this paper, novel multilayer nano-heterostructure InN/Al0.30In0.70N of length 288\u00a0nm have been modeled and studied to compute the energy band profile within the frame work of eight band k.p method, which graphs the energy of conduction and valence band edges versus position, and potential distribution throughout the modeled and one dimensionally simulated nano-heterostructure. In addition, electron\u2013hole densities along with space charge densities have also been calculated for 30% Al concentration. The novelty of the nano-heterostructure due to unusual properties of InN studied by FP-LAPW and LCAO methods\u00a0\u2026", "num_citations": "21\n", "authors": ["1575"]}
{"title": "A Requirements Analysis for a Simple API for Grid Applications\n", "abstract": " This document distills the use cases [6] received by the Simple API for Grid Applications research group (SAGA-RG) and extracts the salient features into a set of requirements for the API In addition to the requirements drawn from the use cases, by analysing related ongoing developments in the grid community, this document tries to define further the scope and requirements of any simple API for applications.", "num_citations": "21\n", "authors": ["1575"]}
{"title": "Force field validation for nucleic acid simulations: comparing energies and dynamics of a DNA dodecamer\n", "abstract": " Important questions exist regarding the quality of force fields used in molecular dynamics (MD) simulations and their interoperable use with other available MD implementations. NAMD is one of the most efficient and scalable parallel molecular dynamics codes for large\u2010scale biomolecular simulations in the open source domain. It is the aim of this article to analyze and compare the dynamics of a benchmark DNA dodecamer d(CTTTTGCAAAAG)2 system, including its binding to a specific drug molecule arising from the use of various simulation protocols in NAMD using Amber98, with the dynamics arising from simulations of the same dodecamer using Amber98 in the AMBER package, one of the most well\u2010established simulation codes for nucleic acids. Based upon a set of validation benchmarks, the details of which are discussed, we find that nucleic acid simulations using NAMD give meaningful results and that\u00a0\u2026", "num_citations": "21\n", "authors": ["1575"]}
{"title": "Faster checking of software specifications by eliminating isomorphs\n", "abstract": " Both software specifications and their intended properties can be expressed in a simple relational language. The claim that a specification satisfies a property becomes a relational formula that can be checked automatically by enumerating the formula's interpretations. Because the number of interpretations is usually huge, this approach has not been thought to be practical. But by eliminating isomorphic interpretations, the enumeration can be reduced substantially, with a factor of roughly k! contributed by each type of k elements.", "num_citations": "21\n", "authors": ["1575"]}
{"title": "Spontaneous H2 Loss through the Interaction of Squaric Acid Derivatives and BeH2\n", "abstract": " The most stable complexes between squaric acid and its sulfur\u2010 and selenium\u2010containing analogues (C4X4H2; X=O, S, Se) with BeY2 (Y=H, F) were studied by means of the Gaussian\u200504 (G4) composite ab initio theory. Squaric acid derivatives are predicted to be very strong acids in the gas phase; their acidity increases with the size of the chalcogen, with C4Se4H2 being the strongest acid of the series and stronger than sulfuric acid. The relative stability of the C4X4H2\u22c5BeY2 (X=O, S, Se; Y=H, F) complexes changes with the nature of the chalcogen atom; but more importantly, the formation of the C4X4H2\u22c5BeF2 complexes results in a substantial acidity enhancement of the squaric moiety owing to the dramatic electron\u2010density redistribution undergone by the system when the beryllium bond is formed. The most significant consequence of this acidity enhancement is that when BeF2 is replaced by BeH2, a\u00a0\u2026", "num_citations": "20\n", "authors": ["1575"]}
{"title": "Poster: Constrained policy mining in attribute based access control\n", "abstract": " In practical access control systems, it is important to enforce an upper bound on the time taken to respond to an access request. This response time is directly influenced by the size (often called the weight) of each of the underlying access control rules. We present a constrained policy mining algorithm which takes an access control matrix as input and generates a set of attribute based access control (ABAC) rules, such that the weight of each rule is not more than a specified value and the sum of weights of all the rules is minimized. Our initial experiments show encouraging results.", "num_citations": "19\n", "authors": ["1575"]}
{"title": "High performance high functionality big data software stack\n", "abstract": " Big data is important in all areas including research, government and commercial applications. The 51 use cases gathered in NIST [1] had 34, 8 and 9 use cases in these categories. Indeed the largest datasets are possibly those associated with commercial clouds including search and social media [2]. It is estimated that this year there are approximately 6 zettabytes of stored data [3] and CISCO estimates [4] almost a zettabyte of total IP traffic this year; with largest individual science applications like the LHC with \u201conly\u201d around 0.0001 zettabytes (100 petabytes). Other research applications in the NIST study were typically fractions of petabytes with much larger astronomy (LSST, SKA) projects underway and imagery (light sources, medical, surveillance, radar such as EISCAT-3D) large and growing in size with non-cardiac medical imagery around 70 petabytes a year in the USA [2]. This broad interest in Big data has spurred a frantic software activity much of it aimed at commercial cloud deployments. This was seen in NIST study in both architecture discussions and the details given in many use cases. Here we suggest that it is valuable to understand the large scale commercial approaches and understand how they can be made use of and as appropriate be integrated in the HPC and exascale data environments. This approach is helped by the broad use of Open source technology in the consensus commercial clouds, with in particular many Apache software projects contributing to what we can term ABDS or Apache Big Data Stack. This is", "num_citations": "19\n", "authors": ["1575"]}
{"title": "Rapid, accurate, and precise calculation of relative binding affinities for the SH2 domain using a computational grid\n", "abstract": " We describe and apply a method that reduces the time taken to calculate binding free energies using thermodynamic integration. This method uses a stack of grid software, which we call STIMD, that allows the scientist to easily distribute the necessary simulations around a computational grid thereby accelerating the process. We use this method to study how a series of phosphopeptides binds to the Src SH2 domain. The binding of phosphopeptides to the Src SH2 domain is described by the \u201ctwo-pronged plug two-holed socket\u201d model, and we investigate this model by reducing the length of the aliphatic side chain that engages the second of the two sockets through two successive alchemical mutations. Seven different values of \u0394\u0394G have been calculated, and we report good agreement with experiment. We then propose an extension to this model using the insights gained from a free energy component analysis.", "num_citations": "19\n", "authors": ["1575"]}
{"title": "Uwstego: A general architecture for software watermarking\n", "abstract": " Software piracy is a cause of substantial losses for software vendors. For example, software and technology piracy is suspected to cause approximately $16 billion each year. Given the magnitude of losses due to software piracy, companies need ways to prosecute software pirates. An essential step in proving the guilt of a suspected software pirate is to trace the source of a program, ie, that a specific program originates from a certain company. Software watermarking is a technique that can be used for identifying the source of the program. In this paper, we survey various software watermarking techniques. We present several metrics to gauge the efficacy of various software watermarking schemes. Finally, we present the design and implementation of a general architecture, UWStego, for watermarking JAVA programs.", "num_citations": "19\n", "authors": ["1575"]}
{"title": "Internet Sieve: An Architecture for Generating Resilient Signatures\n", "abstract": " We present iSieve, a modular architecture for identifying intrusion profiles in packet trace data and automatically constructing resilient signatures for the profiles. The first component of the architecture organizes and normalizes packet trace data collected from honeynets. The second component classifies this data into attack profiles based upon data similarity measures. The final component uses machine learning methods to generate an automaton for each attack profile. These automata can then be used as signatures by network intrusion detection systems. We show how a large, diverse data set is effectively summarized by each component of our system and use these results to highlight implementation considerations in the architecture. Evaluation demonstrates Sieve's ability to generate resilient signatures for many different intrusion profiles. For example, our learned signatures detect 99.98% of the intrusive sessions in NetBIOS data and generate no false alarms.", "num_citations": "19\n", "authors": ["1575"]}
{"title": "Overfitting, robustness, and malicious algorithms: A study of potential causes of privacy risk in machine learning\n", "abstract": " Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models\u2019 structure or their observable behavior. This article examines the factors that can allow a training set membership inference attacker or an attribute inference attacker to learn such information. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms.", "num_citations": "18\n", "authors": ["1575"]}
{"title": "Detecting adversarial examples using data manifolds\n", "abstract": " Models produced by machine learning, particularly deep neural networks, are state-of-the-art for many machine learning tasks and demonstrate very high prediction accuracy. Unfortunately, these models are also very brittle and vulnerable to specially crafted adversarial examples. Recent results have shown that accuracy of these models can be reduced from close to hundred percent to below 5% using adversarial examples. This brittleness of deep neural networks makes it challenging to deploy these learning models in security-critical areas where adversarial activity is expected, and cannot be ignored. A number of methods have been recently proposed to craft more effective and generalizable attacks on neural networks along with competing efforts to improve robustness of these learning models. But the current approaches to make machine learning techniques more resilient fall short of their goal. Further, the\u00a0\u2026", "num_citations": "18\n", "authors": ["1575"]}
{"title": "Efficient large-scale replica-exchange simulations on production infrastructure\n", "abstract": " Replica-exchange (RE) algorithms are used to understand physical phenomena\u2014ranging from protein folding dynamics to binding affinity calculations. They represent a class of algorithms that involve a large number of loosely coupled ensembles, and are thus amenable to using distributed resources. We develop a framework for RE that supports different replica pairing (synchronous versus asynchronous) and exchange coordination mechanisms (centralized versus decentralized) and which can use a range of production cyberinfrastructures concurrently. We characterize the performance of both RE algorithms at an unprecedented number of cores employed\u2014the number of replicas and the typical number of cores per replica\u2014on the production distributed infrastructure. We find that the asynchronous algorithms outperform the synchronous algorithms, even though details of the specific implementations are\u00a0\u2026", "num_citations": "18\n", "authors": ["1575"]}
{"title": "System for automatic detection of spyware\n", "abstract": " An automatic system for spyware detection and signature generation compares packets of output from a computer in response to standard user inputs, to packets of a standard output set derived from a known clean machine. Differences between these two packet sets are analyzed with respect to whether they relate to unknown web servers and whether they incorporate user-derived information. This analysis is used to provide an automatic detection of and signature generation for spyware infecting the machine.", "num_citations": "18\n", "authors": ["1575"]}
{"title": "Guest Editors' Introduction: Scientific Applications of Grid Computing\n", "abstract": " The use of parallel computation for scientific research is so widespread today that it's easy to forget it started on a large scale as recently as the late 1980s. With the increasing attention given to parallel algorithms for scientific research, researchers soon realized that certain common applications, such as Monte Carlo algorithms, were embarrassingly parallel: they required very little interprocessor communication and could be effectively deployed on networks of modest communications bandwidth, including the Internet.", "num_citations": "18\n", "authors": ["1575"]}
{"title": "A filtering approach to anomaly and masquerade detection\n", "abstract": " A host-based intrusion detection (HIDS) monitors execution of a process to identify malicious behavior. Anomaly detection identifies a monitored activity as malicious if a deviation from a model of process behavior is observed. In statistical-anomaly detection, the model of process behavior is constructed using statistical methods on execution traces of the process. One of the major advantages of statistical-anomaly detection techniques is that they can discover new attacks. However, due to variability in behavior of a process, statistical-anomaly detection techniques suffer from the false-alarm problem, ie, normal behavior which is identified as malicious. We present a filtering-based approach to anomaly detection in which we view normal behavior as noise and anomalous behavior as signal. Filtering techniques are then used to measure the strength of the signal (anomalous behavior), which is then used to identify anomalous or malicious behavior. The filtering approach to anomaly detection has the potential of reducing false alarms because the normal behavior is \u201cfiltered out\u201d of the observed behavior. We also demonstrate that the filtering based approach can address the masquerade-detection problem, which is defined as determining the identity of the user that generated a given execution trace.", "num_citations": "18\n", "authors": ["1575"]}
{"title": "Efficient verification of security protocols using partial-order reductions\n", "abstract": " In this paper we explore how partial-order reduction can make the task of verifying security protocols more efficient. These reduction techniques have been implemented in our tool Brutus. Partial-order reductions have proved very useful in the domain of model checking reactive systems. These reductions are not directly applicable in our context because of additional complications caused by tracking knowledge of various agents. We present partial-order reductions in the context of verifying security protocols and prove their correctness. Experimental results demonstrating the effectiveness of this reduction technique are also presented.", "num_citations": "18\n", "authors": ["1575"]}
{"title": "Checking relational specifications with binary decision diagrams\n", "abstract": " Checking a specification in a language based on sets and relations (such as Z) can be reduced to the problem of finding satisfying assignments, or models, of a relational formula. A new method for finding models using ordered binary decision diagrams (BDDs) is presented that appears to scale better than existing methods.Relational terms are replaced by matrices of boolean formulae. These formulae are then composed to give a boolean translation of the entire relational formula. Throughout, boolean formulae are represented with BDDs; from the resulting BDD, models are easily extracted.The performance of the BDD method is compared to our previous method based instead on explicit enumeration. The new method performs as well or better on most of our examples, but can also handle specifications that, until now, we have been unable to analyze.", "num_citations": "18\n", "authors": ["1575"]}
{"title": "Adversarial learning and explainability in structured datasets\n", "abstract": " We theoretically and empirically explore the explainability benefits of adversarial learning in logistic regression models on structured datasets. In particular we focus on improved explainability due to significantly higher feature-concentration in adversarially-learned models: Compared to natural training, adversarial training tends to more efficiently shrink the weights of non-predictive and weakly-predictive features, while model performance on natural test data only degrades slightly (and even sometimes improves), compared to that of a naturally trained model. We provide theoretical insight into this phenomenon via a novel analysis of the expectation of the logistic model weight updates by an SGD-based adversarial learning algorithm, where examples are drawn from a random binary data-generation process. We empirically demonstrate the feature-pruning effect on a synthetic dataset, some datasets from the UCI ML Repository [1], and real-world large-scale advertising response-prediction data-sets from MediaMath. In several of the MediaMath datasets there are 10s of millions of data points, and on the order of 100,000 sparse categorical features, and adversarial learning often results in model-size reduction by a factor of 20 or higher, and yet the model performance on natural test data (measured by AUC) is comparable to (and sometimes even better) than that of the naturally trained model. We also show that traditional l1 regularization does not even come close to achieving this level of feature-concentration. We measure\" feature concentration\" using the Integrated Gradients-based feature-attribution method of [2] and derive a new closed\u00a0\u2026", "num_citations": "17\n", "authors": ["1575"]}
{"title": "Pilot-abstraction: A valid abstraction for data-intensive applications on hpc, hadoop and cloud infrastructures?\n", "abstract": " HPC environments have traditionally been designed to meet the compute demand of scientific applications and data has only been a second order concern. With science moving toward data-driven discoveries relying more on correlations in data to form scientific hypotheses, the limitations of HPC approaches become apparent: Architectural paradigms such as the separation of storage and compute are not optimal for I/O intensive workloads (e.g. for data preparation, transformation and SQL). While there are many powerful computational and analytical libraries available on HPC (e.g. for scalable linear algebra), they generally lack the usability and variety of analytical libraries found in other environments (e.g. the Apache Hadoop ecosystem). Further, there is a lack of abstractions that unify access to increasingly heterogeneous infrastructure (HPC, Hadoop, clouds) and allow reasoning about performance trade-offs in this complex environment. At the same time, the Hadoop ecosystem is evolving rapidly and has established itself as de-facto standard for data-intensive workloads in industry and is increasingly used to tackle scientific problems. In this paper, we explore paths to interoperability between Hadoop and HPC, examine the differences and challenges, such as the different architectural paradigms and abstractions, and investigate ways to address them. We propose the extension of the Pilot-Abstraction to Hadoop to serve as interoperability layer for allocating and managing resources across different infrastructures. Further, in-memory capabilities have been deployed to enhance the performance of large-scale data analytics (e.g. iterative\u00a0\u2026", "num_citations": "17\n", "authors": ["1575"]}
{"title": "Abstractions for loosely-coupled and ensemble-based simulations on Azure\n", "abstract": " Azure is an emerging cloud platform developed and operated by Microsoft. It provides a range of abstractions and building blocks for creating scalable and reliable scientific applications. In this paper we investigate the applicability of the Azure abstractions to the well-known class of loosely coupled and ensemble-based applications. We propose the BigJob API as a novel abstraction for managing groups of Azure worker roles and for remotely executing tasks on them. We demonstrate that Azure enhanced with Big Job functionality provides performance comparable to other grid and cloud offerings loosely-coupled applications.", "num_citations": "17\n", "authors": ["1575"]}
{"title": "Large scale computational science on federated international grids: The role of switched optical networks\n", "abstract": " The provision of high performance compute and data resources on a grid has often been the primary concern of grid resource providers, with the network links used to connect them only a secondary matter. Certain large scale distributed scientific simulations, especially ones which involve cross-site runs or interactive visualisation and steering capabilities, often require high quality of service, high bandwidth, low latency network interconnects between resources. In this paper, we describe three applications which require access to such network infrastructure, together with the middleware and policies needed to make them possible.", "num_citations": "17\n", "authors": ["1575"]}
{"title": "Query restart strategies for web agents\n", "abstract": " With the explosive growth of the intemet, autonomous agents will increasingly need strategies for efficiently retrieving information, The time an agent (or server) takes to answer a query issued to it is often a random variable, whose distribution can be estimated by collecting statistics. When an agent A sends a query to another agent B and the query has not completed in some period of time, agent A faces the dilemma of whether to continue waiting or reissue the query (to agent B or to a different one). When some information is available about the probability distribution of the query completion time of the agents, it is possible to devise schemes for agents to reissue queries in such a way that the expected query completion time is reduced significantly. We design such schemes for various models of query-answering ngents. Where meaningful, we take into account the cost of sending a query to an agent.", "num_citations": "17\n", "authors": ["1575"]}
{"title": "An options approach to software prototyping\n", "abstract": " Prototyping is often used to predict, or reduce the uncertainty over, the future profitability of a software design choice. Boehm [3] pioneered the use of techniques from Bayesian decision theory to provide a basis for making prototyping decisions. However, this approach does not apply to situations where the software engineer has the flexibility of waiting for more information before making a prototyping decision. Also, this framework only assumes uncertainty over one time period, and assumes a design-choice must be made immediately after prototyping. We propose a more general multi-period approach that takes into account the flexibility of being able to postpone the prototyping and design decisions. In particular, we argue that this flexibility is analogous to the flexibility of exercise of certain financial instruments called options, and that the value of the flexibility is the value of the corresponding financial option. The field of real option theory in finance provides a rigorous framework to analyze the optimal exercise of such options, and this can be applied to the prototyping decision problem. Our approach integrates the timing of prototype decisions and design decisions within a single framework.", "num_citations": "17\n", "authors": ["1575"]}
{"title": "Naphthenic Acid Corrosion Prediction Using Crude Corrosivity JIP Model\u2013Evaluation and Validation\n", "abstract": " Accurate quantification of corrosivity of crude oil fractions has been a major challenge for refiners, stemming from inadequate understanding of complex influences of naphthenic acids and type/morphology, molecular weight of acid species, sulphur content and speciation, temperature, and fluid flow on corrosion. A proprietary model has been developed encapsulating data from Honeywell\u2019s Crude Corrosivity Phase-I Joint Industry Program (JIP), conducted between 2006 and 2011. This research program, sponsored by Indian Oil Corporation Limited (IOCL) and nineteen other global refining and engineering companies, led to the development of the first ever quantitative engineering database and decision-support model to predict corrosion for common materials of construction employed in high temperature refinery crude fractionation. Data were generated in simulated refinery environments for relevant\u00a0\u2026", "num_citations": "16\n", "authors": ["1575"]}
{"title": "Botnet protocol inference in the presence of encrypted traffic\n", "abstract": " Network protocol reverse engineering of botnet command and control (C&C) is a challenging task, which requires various manual steps and a significant amount of domain knowledge. Furthermore, most of today's C&C protocols are encrypted, which prevents any analysis on the traffic without first discovering the encryption algorithm and key. To address these challenges, we present an end-to-end system for automatically discovering the encryption algorithm and keys, generating a protocol specification for the C&C traffic, and crafting effective network signatures. In order to infer the encryption algorithm and key, we enhance state-of-the-art techniques to extract this information using lightweight binary analysis. In order to generate protocol specifications we infer field types purely by analyzing network traffic. We evaluate our approach on three prominent malware families: Sality, ZeroAccess and Ramnit. Our results\u00a0\u2026", "num_citations": "16\n", "authors": ["1575"]}
{"title": "Dynamic behavior matching: A complexity analysis and new approximation algorithms\n", "abstract": " A number of advances in software security over the past decade have foundations in the behavior matching problem: given a specification of software behavior and a concrete execution trace, determine whether the behavior is exhibited by the execution trace. Despite the importance of this problem, precise descriptions of algorithms for its solution, and rigorous analyses of their complexity, are missing in the literature. In this paper, we formalize the notion of behavior matching used by the software security community, study the complexity of the problem, and give several algorithms for its solution, both exact and approximate. We find that the problem is in general not efficiently solvable, i.e. behavior matching is NP-Complete. We demonstrate empirically that our approximation algorithms can be used to efficiently find accurate solutions to real instances.", "num_citations": "16\n", "authors": ["1575"]}
{"title": "Distributed replica-exchange simulations on production environments using saga and migol\n", "abstract": " There exists a class of scientific applications for which utilizing distributed resources is critical for reducing the time-to-solution. In this paper, we discuss a specific class of applications - Replica-Exchange simulations - where the orchestration of many distributed jobs in a dynamic and inherently unreliable distributed environment is essential for a successful completion. We describe the design, development and deployment of a unique framework for constructing fault-tolerant distributed simulations. The framework consists of two primary components - SAGA and Migol. SAGA is a high-level programmatic abstraction layer that provides a standardised interface for the primary distributed functionality required for application development. We present details of a newly developed functionality in SAGA - the Checkpoint and Recovery (CPR) API. Migol is an adaptive middleware, which supports the fault-tolerance of\u00a0\u2026", "num_citations": "16\n", "authors": ["1575"]}
{"title": "Parallelization and scalability of a spectral element channel flow solver for incompressible Navier\u2013Stokes equations\n", "abstract": " Direct numerical simulation (DNS) of turbulent flows is widely recognized to demand fine spatial meshes, small timesteps, and very long runtimes to properly resolve the flow field. To overcome these limitations, most DNS is performed on supercomputing machines. With the rapid development of terascale (and, eventually, petascale) computing on thousands of processors, it has become imperative to consider the development of DNS algorithms and parallelization methods that are capable of fully exploiting these massively parallel machines. A highly parallelizable algorithm for the simulation of turbulent channel flow that allows for efficient scaling on several thousand processors is presented. A model that accurately predicts the performance of the algorithm is developed and compared with experimental data. The results demonstrate that the proposed numerical algorithm is capable of scaling well on petascale\u00a0\u2026", "num_citations": "16\n", "authors": ["1575"]}
{"title": "Model checking algorithms for the mu-calculus\n", "abstract": " The propositional mu-calculus is a powerful language for expressing properties of transition systems by using least and greatest fixpoint operators. Recently, the mu-calculus has generated much interest among researchers in computer-aided verification. This interest stems from the fact that many temporal and program logics can be encoded into the mu-calculus. In addition, important relations between transition systems, such as weak and strong bisimulation equivalence, also have fixpoint characterizations. Wide-spread use of binary decision diagrams has made fixpoint based algorithms even more important, since methods that require the manipulation of individual states do not take advantage of this representation.Descriptors:", "num_citations": "16\n", "authors": ["1575"]}
{"title": "Adaptive ensemble biomolecular applications at scale\n", "abstract": " Recent advances in both theory and methods have created opportunities to simulate biomolecular processes more efficiently using adaptive ensemble simulations. Ensemble-based simulations are used widely to compute a number of individual simulation trajectories and analyze statistics across them. Adaptive ensemble simulations offer a further level of sophistication and simulation efficacy by enabling high-level algorithms to control simulations based on intermediate results. Novel high-level algorithms for adaptive simulations require sophisticated approaches to manage the ensemble members and utilize the intermediate data during runtime. Thus, there is a need for scalable software systems to support adaptive ensemble-based methods. We describe the operations in executing adaptive workflows, classify different types of adaptations, and describe challenges in implementing them in software tools. We\u00a0\u2026", "num_citations": "15\n", "authors": ["1575"]}
{"title": "Secure integration of web content and applications on commodity mobile operating systems\n", "abstract": " A majority of today's mobile apps integrate web content of various kinds. Unfortunately, the interactions between app code and web content expose new attack vectors: a malicious app can subvert its embedded web content to steal user secrets; on the other hand, malicious web content can use the privileges of its embedding app to exfiltrate sensitive information such as the user's location and contacts. In this paper, we discuss security weaknesses of the interface between app code and web content through attacks, then introduce defenses that can be deployed without modifying the OS. Our defenses feature WIREframe, a service that securely embeds and renders external web content in Android apps, and in turn, prevents attacks between em-bedded web and host apps. WIREframe fully mediates the interface between app code and embedded web content. Un-like the existing web-embedding mechanisms\u00a0\u2026", "num_citations": "15\n", "authors": ["1575"]}
{"title": "Towards a comprehensive set of big data benchmarks\n", "abstract": " This paper reviews the Ogre classification of Big Data application with 50 facets divided into four groups or views. These four correspond to Problem Architecture, Execution mode, Data source and style, and the Processing model used. We then look at multiple existing or proposed benchmark suites and analyze their coverage of the different facets suggesting a process to obtain a complete set. We illustrate this by looking at parallel data analytics benchmarked on multicore clusters.", "num_citations": "15\n", "authors": ["1575"]}
{"title": "Provenance-aware security risk analysis for hosts and network flows\n", "abstract": " Detection of high risk network flows and high risk hosts is becoming ever more important and more challenging. In order to selectively apply deep packet inspection (DPI) one has to isolate in real time high risk network activities within a huge number of monitored network flows. To help address this problem, we propose an iterative methodology for a simultaneous assessment of risk scores for both hosts and network flows. The proposed approach measures the risk scores of hosts and flows in an interdependent manner; thus, the risk score of a flow influences the risk score of its source and destination hosts, and also the risk score of a host is evaluated by taking into account the risk scores of flows initiated by or terminated at the host. Our experimental results show that such an approach not only effective in detecting high risk hosts and flows but, when deployed in high throughput networks, is also more efficient than\u00a0\u2026", "num_citations": "15\n", "authors": ["1575"]}
{"title": "A computational steering API for scientific Grid applications: Design, implementation and lessons\n", "abstract": " We discuss three classes of scientific grid applications, with the aim of motivating the design philosophy of the RealityGrid steering library. The steering library in addition to providing functionality required for the steering of scientific applications, essentially provides a simple interface to a set of features required by a significant proportion of scientific codes in order to be deployable as grid applications. We discuss the scope and implementation details of the steering library. The latter part of the paper provides case studies for two distinct scientific grid applications, which use the aforementioned library. We conclude with a discussion, essentially capturing our thoughts and experiences, in the hope that it may provide useful feedback to the community in its effort to evolve a coherent and consistent abstraction for an API for scientific applications.", "num_citations": "15\n", "authors": ["1575"]}
{"title": "Internal Waves Observed in Lake Ontario During the International Field Year for the Great Lakes (IFYGL) 1972: I. Descriptive Survey and Preliminary Interpretation of Near\u00a0\u2026\n", "abstract": " Characteristic features of (i) records of current and temperature at various stations and depths and (ii) temperature distributions in basin cross-sections, obtained during IFYGL 1972 on Lake Ontario, are compared with features predicted by linear models of long interfacial waves in two layered, rotating, rectangular channels. The principal comparisons are between inertial motions, seen as clockwise rotations of the horizontal current vector and as oscillations in thermocline depth, and the structures and periods of the lowest five cross-channel modes of an internal Poincare wave model fitted to basin dimensions and observed stratification (predicted periods in Tables 2 and 3).Direct examination in Sections 5.3 and 6, of all the US and a preliminary portion of the Canadian records reveals that intermittent bursts of inertial waves followed strong wind impulses at all stations, except those very near shore, after stratification had become established at the station concerned. The most common of the features listed in Section 6 is a periodic (near-17 hr) rotation of-the horizontal current vector, seen at all depths except very near the bottom. Speeds of 20 cm s-1 or more were attained in the upper layer, with weaker currents below the thermocline approxi mately in antiphase with those above. Estimates by eye of average periods for individual episodes of inertial motion disclose (a) 17. 3 hr periodicity (indistinguishable from exact inertial) at individual stations (Figs. 66, 69, 70) before stratification is fully established across the basin in July, and (b) average periodicities significantly shorter than exact inertial after July, most commonly 16. 9\u00b10. 2 hr for current\u00a0\u2026", "num_citations": "15\n", "authors": ["1575"]}
{"title": "Face-Off: Adversarial Face Obfuscation.\n", "abstract": " Advances in deep learning have made face recognition technologies pervasive. While useful to social media platforms and users, this technology carries significant privacy threats. Coupled with the abundant information they have about users, service providers can associate users with social interactions, visited places, activities, and preferences\u2013some of which the user may not want to share. Additionally, facial recognition models used by various agencies are trained by data scraped from social media platforms. Existing approaches to mitigate associated privacy risks result in an imbalanced trade-off between privacy and utility. In this paper, we address this trade-off by proposing Face-Off, a privacypreserving framework that introduces strategic perturbations to images of the user\u2019s face to prevent it from being correctly recognized. To realize Face-Off, we overcome a set of challenges related to the black-box nature of commercial face recognition services, and the scarcity of literature for adversarial attacks on metric networks. We implement and evaluate Face-Off to find that it deceives three commercial face recognition services from Microsoft, Amazon, and Face++. Our user study with 423 participants further shows that the perturbations come at an acceptable cost for the users.", "num_citations": "14\n", "authors": ["1575"]}
{"title": "Priority-based coverage path planning for aerial wireless sensor networks\n", "abstract": " Aerial Wireless Sensor Networks (AWSNs) are being increasingly used in search and rescue operations for locating victims stranded in disaster areas. The primary objective of such missions is to locate the victims as quickly as possible. As such, determining the optimal flight path for UAVs is very important. In this paper, we solve the coverage problem while optimising the time to find victim when number of victims in the disaster area is unknown. Path planning problem in weighted terrain is NP-Complete problem. Next we propose a heuristic based on Partially Observable Markov Decision Processes (POMDP). Using POMDP in this problem makes sense for two reasons, first we have partial knowledge of terrain and secondly the way POMDP maximises the reward in its transition function is requirement of our problem. We conduct extensive simulations and demonstrate that our solution can reduce the time to find all\u00a0\u2026", "num_citations": "14\n", "authors": ["1575"]}
{"title": "Survey and analysis of production distributed computing infrastructures\n", "abstract": " This report has two objectives. First, we describe a set of the production distributed infrastructures currently available, so that the reader has a basic understanding of them. This includes explaining why each infrastructure was created and made available and how it has succeeded and failed. The set is not complete, but we believe it is representative. Second, we describe the infrastructures in terms of their use, which is a combination of how they were designed to be used and how users have found ways to use them. Applications are often designed and created with specific infrastructures in mind, with both an appreciation of the existing capabilities provided by those infrastructures and an anticipation of their future capabilities. Here, the infrastructures we discuss were often designed and created with specific applications in mind, or at least specific types of applications. The reader should understand how the interplay between the infrastructure providers and the users leads to such usages, which we call usage modalities. These usage modalities are really abstractions that exist between the infrastructures and the applications; they influence the infrastructures by representing the applications, and they influence the ap- plications by representing the infrastructures.", "num_citations": "14\n", "authors": ["1575"]}
{"title": "Cause: Learning granger causality from event sequences using attribution methods\n", "abstract": " We study the problem of learning Granger causality between event types from asynchronous, interdependent, multi-type event sequences. Existing work suffers from either limited model flexibility or poor model explainability and thus fails to uncover Granger causality across a wide variety of event sequences with diverse event interdependency. To address these weaknesses, we propose CAUSE (Causality from AttribUtions on Sequence of Events), a novel framework for the studied task. The key idea of CAUSE is to first implicitly capture the underlying event interdependency by fitting a neural point process, and then extract from the process a Granger causality statistic using an axiomatic attribution method. Across multiple datasets riddled with diverse event interdependency, we demonstrate that CAUSE achieves superior performance on correctly inferring the inter-type Granger causality over a range of state-of-the-art methods.", "num_citations": "13\n", "authors": ["1575"]}
{"title": "Concise explanations of neural networks using adversarial training\n", "abstract": " We show new connections between adversarial learning and explainability for deep neural networks (DNNs). One form of explanation of the output of a neural network model in terms of its input features, is a vector of feature-attributions, which can be generated by various techniques such as Integrated Gradients (IG), DeepSHAP, LIME, and CXPlain. Two desirable characteristics of an attribution-based explanation are:(1)\\emph {sparseness}: the attributions of irrelevant or weakly relevant features should be negligible, thus resulting in\\emph {concise} explanations in terms of the significant features, and (2)\\emph {stability}: it should not vary significantly within a small local neighborhood of the input. Our first contribution is a theoretical exploration of how these two properties (when using IG-based attributions) are related to adversarial training, for a class of 1-layer networks (which includes logistic regression models for binary and multi-class classification); for these networks we show that (a) adversarial training using an -bounded adversary produces models with sparse attribution vectors, and (b) natural model-training while encouraging stable explanations (via an extra term in the loss function), is equivalent to adversarial training. Our second contribution is an empirical verification of phenomenon (a), which we show, somewhat surprisingly, occurs\\emph {not only in 1-layer networks, but also DNNs trained on standard image datasets}, and extends beyond IG-based attributions, to those based on DeepSHAP: adversarial training with $\\linf $-bounded perturbations yields significantly sparser attribution vectors, with little degradation in performance\u00a0\u2026", "num_citations": "13\n", "authors": ["1575"]}
{"title": "ADMM-based multiparameter wavefield reconstruction inversion in VTI acoustic media with TV regularization\n", "abstract": " Full waveform inversion (FWI) is a nonlinear waveform matching procedure, which suffers from cycle skipping when the initial model is not kinematically accurate enough. To mitigate cycle skipping, wavefield reconstruction inversion (WRI) extends the inversion search space by computing wavefields with a relaxation of the wave equation in order to fit the data from the first iteration. Then, the subsurface parameters are updated by minimizing the source residuals the relaxation generated. Capitalizing on the wave-equation bilinearity, performing wavefield reconstruction and parameter estimation in alternating mode decomposes WRI into two linear subproblems, which can be solved efficiently with the alternating-direction method of multiplier (ADMM), leading to the so-called iteratively refined WRI (IR\u2013WRI). Moreover, ADMM provides a suitable framework to implement bound constraints and different types of\u00a0\u2026", "num_citations": "13\n", "authors": ["1575"]}
{"title": "Deep packet inspection with DFA-trees and parametrized language overapproximation\n", "abstract": " IPSs determine whether incoming traffic matches a database of vulnerability signatures defined as regular expressions. DFA representations are popular, but suffer from the state-explosion problem. We introduce a new matching structure: a tree of DFAs where the DFA associated with a node over-approximates those at its children, and the DFAs at the leaves represent the signature set. Matching works top-down, starting at the root of the tree and stopping at the first node whose DFA does not match. In the common case (benign traffic) matching does not reach the leaves. DFA-trees are built using Compact Overapproximate DFAs (CODFAs). A CODFA D' for D over-approximates the language accepted by D, has a smaller number of states than D, and has a low false-match rate. Although built from approximate DFAs, DFA-trees perform exact matching faster than a commonly used method, have a low memory\u00a0\u2026", "num_citations": "13\n", "authors": ["1575"]}
{"title": "Abstractions for large-scale distributed applications and systems\n", "abstract": " It is generally accepted that the ability to develop large-scale distributed applications has lagged seriously behind other developements in Cyber-Infrastructure. The reasons are complex and defy over-simplification. In this manuscript we provide insight into why developing applications for distributed infrastructure is notoriously hard. Although our discussion revolves around a dozen specific examples, these scientific applications are representative of the characteristics and the requirements as well as the challenges of the bulk of current distributed applications. We survey existing approaches, models, and abstractions for large-scale distributed applications. We identify commonalities and recurring structures, patterns, and environments. We find that acceptable models and abstractions are often not available. There are many ad-hoc solutions employed to write distributed applications, which results in lack of\u00a0\u2026", "num_citations": "13\n", "authors": ["1575"]}
{"title": "Opening the science doorway\n", "abstract": " Watson discusses strategies and suggestions for incorporating English language learners in the science classroom. Science activities can be modified for use with ELL students by adding visual and tactile components.", "num_citations": "13\n", "authors": ["1575"]}
{"title": "Synapse: Synthetic application profiler and emulator\n", "abstract": " Motivated by the need to emulate workload execution characteristics on high-performance and distributed heterogeneous resources, we introduce Synapse. Synapse is used as a proxy application (or \u201crepresentative application\u201d) for real workloads, with the advantage that it can be tuned in different ways and dimensions, and also at levels of granularity that are not possible with real applications. Synapse has a platform-independent application profiler, and has the ability to emulate profiled workloads on a variety of resources. Experiments show that the automated profiling performed using Synapse captures an application's characteristics with high fidelity. The emulation of an application using Synapse can reproduce the application's execution behavior in the original runtime environment, and can also reproduce those behaviors on different run-time environments.", "num_citations": "12\n", "authors": ["1575"]}
{"title": "New directions for container debloating\n", "abstract": " Application containers, such as Docker containers, are light-weight virtualization environments that\" contain\" applications together with their resources and configuration information. While they are becoming increasingly popular as a method for agile software deployment, current techniques for preparing containers add unnecessary bloat into them: they often include unneeded files that increase the container size by several orders of magnitude. This not only leads to storage and network transfer issues but also security concerns. The problem is well-recognized but available solutions are mostly ad-hoc and not largely deployed.", "num_citations": "12\n", "authors": ["1575"]}
{"title": "Conceptualizing a Computing Platform for Science Beyond 2020: To Cloudify HPC, or HPCify Clouds?\n", "abstract": " A primary challenge of the cyberinfrastructure research community is the need to define the Platforms for Science beyond 2020. We analyze major current trends and propose that in order to deliver the Platform for Science in 2020 the dominant research challenge is to manage the convergence of capabilities of traditional HPC systems with richness of Apache Big Data systems. In this vision paper, we purport to examine the relationship between infrastructure for data-intensive computing and that for High Performance Computing and examine possible \"convergence\" of capabilities.", "num_citations": "12\n", "authors": ["1575"]}
{"title": "Secure computations on non-integer values with applications to privacy-preserving sequence analysis\n", "abstract": " In this work we describe a framework which allows to perform secure computations on non-integer values. To this end, we encode values in a way similar to floating point representation and describe protocols that allow to perform efficient secure two party computations on such encoded values. We present two approaches to realize the functionality of the framework. Both approaches come with different properties and are ready to use in various application scenarios. We implemented the framework in C++ and ran several experiments. This allows for a complexity analysis and for a comparison of the two different approaches. We further describe applications to privacy-preserving computations, which greatly benefit from the use of the new framework. In particular, we show how to run an important algorithm in the context of data analysis using Hidden Markov Models (HMM), namely the Viterbi algorithm, in a secure\u00a0\u2026", "num_citations": "12\n", "authors": ["1575"]}
{"title": "Critical perspectives on large-scale distributed applications and production grids\n", "abstract": " It is generally accepted that the ability to develop large-scale distributed applications that are extensible and independent of infrastructure details has lagged seriously behind other developments in cyberinfrastructure. As the sophistication and scale of distributed infrastructure increases, the complexity of successfully developing and deploying distributed applications increases both quantitatively and in qualitatively newer ways. In this paper we trace the evolution of a representative set of \u00bfstate-of-the-art\u00bf distributed applications and production infrastructure; in doing so we aim to provide insight into the evolving sophistication of distributed applications - from simple generalizations of legacy static high-performance to applications composed of multiple loosely-coupled and dynamic components. The ultimate aim of this work is to highlight that even accounting for the fact that developing applications for distributed\u00a0\u2026", "num_citations": "12\n", "authors": ["1575"]}
{"title": "Protocol normalization using attribute grammars\n", "abstract": " Protocol parsing is an essential step in several networking-related tasks. For instance, parsing network traffic is an essential step for Intrusion Prevention Systems (IPSs). The task of developing parsers for protocols is challenging because network protocols often have features that cannot be expressed in a context-free grammar. We address the problem of parsing protocols by using attribute grammars (AGs), which allow us to factor features that are not context-free and treat them as attributes. We investigate this approach in the context of protocol normalization, which is an essential task in IPSs. Normalizers generated using systematic techniques, such as ours, are more robust and resilient to attacks. Our experience is that such normalizers incur an acceptable level of overhead (approximately 15% in the worst case) and are straightforward to implement.", "num_citations": "12\n", "authors": ["1575"]}
{"title": "Investigating autonomic behaviours in grid-basedcomputational science applications\n", "abstract": " Emerging Grid infrastructures present unprecedented opportunities for computational science and engineering, with the potential for fundamental insights into complex phenomenon. However, it also presents unprecedented challenges in terms of its scale, heterogeneity, dynamism and overall complexity that must be addressed before this potential can be realized. Autonomic computing concepts have been effectively used to address similar challenges in enterprise systems and applications; in this paper, we explore the role of autonomic computing to Grid-based computational science applications. Specifically, we use three representative computational applications to motivate Autonomic Computational Science (ACS). Using these applications, we develop a conceptual framework for ACS, consisting of mechanisms, strategies and objectives, and demonstrate how these concepts can be used to express\u00a0\u2026", "num_citations": "12\n", "authors": ["1575"]}
{"title": "Adversarially Robust Learning Could Leverage Computational Hardness.\n", "abstract": " Over recent years, devising classification algorithms that are robust to adversarial perturbations has emerged as a challenging problem. In particular, deep neural nets (DNNs) seem to be susceptible to small imperceptible changes over test instances. However, the line of work in provable robustness, so far, has been focused on information theoretic robustness, ruling out even the existence of any adversarial examples. In this work, we study whether there is a hope to benefit from algorithmic nature of an attacker that searches for adversarial examples, and ask whether there is any learning task for which it is possible to design classifiers that are only robust against polynomial-time adversaries. Indeed, numerous cryptographic tasks (eg encryption of long messages) can only be secure against computationally bounded adversaries, and are indeed impossible for computationally unbounded attackers. Thus, it is natural to ask if the same strategy could help robust learning. We show that computational limitation of attackers can indeed be useful in robust learning by demonstrating the possibility of a classifier for some learning task for which computational and information theoretic adversaries of bounded perturbations have very different power. Namely, while computationally unbounded adversaries can attack successfully and find adversarial examples with small perturbation, polynomial time adversaries are unable to do so unless they can break standard cryptographic hardness assumptions. Our results, therefore, indicate that perhaps a similar approach to cryptography (relying on computational hardness) holds promise for achieving\u00a0\u2026", "num_citations": "11\n", "authors": ["1575"]}
{"title": "An administrative model for collaborative management of ABAC systems and its security analysis\n", "abstract": " Attribute-based Access Control (ABAC) has been emerging as a suitable choice for large and federated enterprises due to its flexibility in expressing various types of security policies. Improved flexibility, however, results in higher design complexity and consequently, possibility of undesired flow of information. Reliance of access decision on the attribute values of subjects, objects and environment underscores the need for a formal way of managing attribute assignment in ABAC systems. Since large enterprises potentially have hundreds of subjects and thousands of resources, centralized management of attribute assignment is inexpedient. This paper introduces an attribute-based administrative model that supports decentralized administration of ABAC systems. The proposed model consists of a number of operations to administer the set of subjects and the set of subject attribute assignments in an ABAC system\u00a0\u2026", "num_citations": "11\n", "authors": ["1575"]}
{"title": "Reliable communications in aerial sensor networks by using a hybrid antenna\n", "abstract": " An AWSN composed of bird-sized Unmanned Aerial Vehicles (UAVs) equipped with sensors and wireless radio, enables low cost high granularity three-dimensional sensing of the physical world. The sensed data is relayed in real-time over a multi-hop wireless communication network to ground stations. The following characteristics of an AWSN make effective multi-hop communication challenging - (i) frequent link disconnections due to the inherent dynamism (ii) significant inter-node interference (iii) three dimensional motion of the UAVs. In this paper, we investigate the use of a hybrid antenna to accomplish efficient neighbor discovery and reliable communication in AWSNs. We propose the design of a hybrid Omni Bidirectional ESPAR (O-BESPAR) antenna, which combines the complimentary features of an isotropic omni radio (360 degree coverage) and directional ESPAR antennas (beamforming and reduced\u00a0\u2026", "num_citations": "11\n", "authors": ["1575"]}
{"title": "Exploring the RNA folding energy landscape using scalable distributed cyberinfrastructure\n", "abstract": " The increasing significance of RNAs in transcriptional or post-transcriptional gene regulation processes has generated considerable interest towards the prediction of RNA folding and its sensitivity to environmental factors. We use Boltzmann-weighted sampling to generate RNA secondary structures, which are used to characterize the energy landscape, via the distributions of energies and base-pair distances. Depending upon the length of an RNA, the number of sequences investigated, and the sample size of generated structures---generating and analyzing sufficient samples can be computationally challenging. We introduce and develop a lightweight and extensible runtime environment that is effective across a range of RNA sizes and other parameters, as well as over a range of infrastructure--from traditional HPC grids to clouds, without requiring any changes at the application or user level. The Adaptive\u00a0\u2026", "num_citations": "11\n", "authors": ["1575"]}
{"title": "Attribution-driven causal analysis for detection of adversarial examples\n", "abstract": " Attribution methods have been developed to explain the decision of a machine learning model on a given input. We use the Integrated Gradient method for finding attributions to define the causal neighborhood of an input by incrementally masking high attribution features. We study the robustness of machine learning models on benign and adversarial inputs in this neighborhood. Our study indicates that benign inputs are robust to the masking of high attribution features but adversarial inputs generated by the state-of-the-art adversarial attack methods such as DeepFool, FGSM, CW and PGD, are not robust to such masking. Further, our study demonstrates that this concentration of high-attribution features responsible for the incorrect decision is more pronounced in physically realizable adversarial examples. This difference in attribution of benign and adversarial inputs can be used to detect adversarial examples. Such a defense approach is independent of training data and attack method, and we demonstrate its effectiveness on digital and physically realizable perturbations.", "num_citations": "10\n", "authors": ["1575"]}
{"title": "Privacy-preserving collaborative prediction using random forests\n", "abstract": " We study the problem of privacy-preserving machine learning (PPML) for ensemble methods, focusing our effort on random forests. In collaborative analysis, PPML attempts to solve the conflict between the need for data sharing and privacy. This is especially important in privacy sensitive applications such as learning predictive models for clinical decision support from EHR data from different clinics, where each clinic has a responsibility for its patients\u2019 privacy. We propose a new approach for ensemble methods: each entity learns a model, from its own data, and then when a client asks the prediction for a new private instance, the answers from all the locally trained models are used to compute the prediction in such a way that no extra information is revealed. We implement this approach for random forests and we demonstrate its high efficiency and potential accuracy benefit via experiments on real-world datasets\u00a0\u2026", "num_citations": "10\n", "authors": ["1575"]}
{"title": "Collaborating around digital tabletops: children's physical strategies from India, the UK and Finland\n", "abstract": " We present a study of children collaborating around interactive tabletops in three different countries: India, the United Kingdom and Finland. Our data highlights the key distinctive physical strategies used by children when performing collaborative tasks during this study. Children in India employ dynamic positioning with frequent physical contact and simultaneous object movement. Children in the UK tend to prefer static positioning with minimal physical contact and simultaneous object movement. Children in Finland use a mixture of dynamic and static positioning with minimal physical contact and object movement. Our findings indicate the importance of understanding collaboration strategies and behaviours when designing and deploying interactive tabletops in heterogeneous educational environments. We conclude with a discussion on how designers of tabletops for schools can provide opportunities for children\u00a0\u2026", "num_citations": "10\n", "authors": ["1575"]}
{"title": "Formatted encryption beyond regular languages\n", "abstract": " Format-preserving and format-transforming encryption (FPE and FTE, respectively) are relatively new cryptographic primitives, yet are already being used in a broad range of real-world applications. The most flexible existing FPE and FTE implementations use regular expressions to specify plaintext and/or ciphertext formats. These constructions rely on the ability to efficiently map strings accepted by a regular expression to integers and back, called ranking and unranking, respectively.", "num_citations": "10\n", "authors": ["1575"]}
{"title": "Developing scientific applications with loosely-coupled sub-tasks\n", "abstract": " The Simple API for Grid Applications (SAGA) can be used to develop a range of applications which are in turn composed of multiple sub-tasks. In particular SAGA is an effective tool for coordinating and orchestrating the many sub-tasks of such applications, whilst keeping the application agnostic to the details of the infrastructure used. Although developed primarily in the context of distributed applications, SAGA provides an equally valid approach for applications with many sub-tasks on single high-end supercomputers, such as emerging peta-scale computers. Specifically, in this paper we describe how SAGA has been used to develop applications from two types of applications: the first with loosely-coupled homogeneous sub-tasks and, applications with loosely-coupled heterogeneous sub-tasks. We also analyse and contrast the coupling and scheduling requirements of the sub-tasks for these two\u00a0\u2026", "num_citations": "10\n", "authors": ["1575"]}
{"title": "Data-dependent differentially private parameter learning for directed graphical models\n", "abstract": " Directed graphical models (DGMs) are a class of probabilistic models that are widely used for predictive analysis in sensitive domains such as medical diagnostics. In this paper, we present an algorithm for differentially-private learning of the parameters of a DGM. Our solution optimizes for the utility of inference queries over the DGM and\\emph {adds noise that is customized to the properties of the private input dataset and the graph structure of the DGM}. To the best of our knowledge, this is the first explicit data-dependent privacy budget allocation algorithm in the context of DGMs. We compare our algorithm with a standard data-independent approach over a diverse suite of benchmarks and demonstrate that our solution requires a privacy budget that is roughly  smaller to obtain the same or higher utility.", "num_citations": "9\n", "authors": ["1575"]}
{"title": "Towards Effective Differential Privacy Communication for Users\u2019 Data Sharing Decision and Comprehension\n", "abstract": " Differential privacy protects an individual\u2019s privacy by perturbing data on an aggregated level (DP) or individual level (LDP). We report four online human-subject experiments investigating the effects of using different approaches to communicate differential privacy techniques to laypersons in a health app data collection setting. Experiments 1 and 2 investigated participants\u2019 data disclosure decisions for low-sensitive and high-sensitive personal information when given different DP or LDP descriptions. Experiments 3 and 4 uncovered reasons behind participants\u2019 data sharing decisions, and examined participants\u2019 subjective and objective comprehensions of these DP or LDP descriptions. When shown descriptions that explain the implications instead of the definition/processes of DP or LDP technique, participants demonstrated better comprehension and showed more willingness to share information with LDP than\u00a0\u2026", "num_citations": "9\n", "authors": ["1575"]}
{"title": "Practical and Robust Privacy Amplification with Multi-Party Differential Privacy.\n", "abstract": " When collecting information, local differential privacy (LDP) alleviates privacy concerns of users because their private information is randomized before being sent to the central aggregator. However, LDP results in loss of utility due to the amount of noise that is added to each individual data item. To address this issue, recent work introduced an intermediate server with the assumption that this intermediate server did not collude with the aggregator. Using this trust model, one can add less noise to achieve the same privacy guarantee; thus improving the utility. In this paper, we investigate this multiple-party setting of LDP. We first analyze the threat model and identify potential adversaries. We then make observations about existing approaches and propose new techniques that achieve a better privacy-utility tradeoff than existing ones. Finally, we perform experiments to compare different methods and demonstrate the\u00a0\u2026", "num_citations": "9\n", "authors": ["1575"]}
{"title": "Analysis of gemini interconnect recovery mechanisms: Methods and observations\n", "abstract": " This thesis focuses on the resilience of network components, and recovery capabilities of extreme-scale high-performance computing (HPC) systems, specifically petaflop-level supercomputers, aimed at solving complex science, engineering, and business problems that require high bandwidth, enhanced networking, and high compute capabilities. The resilience of the network is critical for ensuring successful execution of the applications and overall system availability. Failure of interconnect components such as links, routers, power supply, etc. pose a threat to the resilience of the interconnect network, causing application failures and, in the worst case, system-wide failure. An extreme-scale system is designed to manage these failures and automatically recover from such failures to ensure successful application execution and avoid system-wide failure. Thus, in this thesis, we characterize the success probability of the recovery procedures as well as the impact of the recovery procedures on the applications.   We developed an interconnect recovery mechanisms analysis tool (I-RAT), a plugin built on top of LogDiver  to characterize and assess the impact of recovery mechanisms. The tool was used to analyze more than two years of network/system logs from Blue Waters, a supercomputer operated by the NCSA at the University of Illinois. Our analyses show that recovery mechanisms are frequently triggered (in as little as 36 hours for link failovers) that can fail with relatively high probability (as much as 0.25 for link failover). Furthermore, the analyses show that system resilience does not equate to application resilience since executing applications\u00a0\u2026", "num_citations": "9\n", "authors": ["1575"]}
{"title": "A framework for flexible and scalable replica-exchange on production distributed CI\n", "abstract": " Replica exchange represents a powerful class of algorithms used for enhanced configurational and energetic sampling in a range of physical systems. Computationally it represents a type of application with multiple scales of communication. At a fine-grained level there is often communication with a replica, typically an MPI process. At a coarse-grained level, the replicas communicate with other replicas--both temporally as well as in amount of data exchanged. This paper outlines a novel framework developed to support the flexible execution of large-scale replica exchange. The framework is flexible in the sense that it supports different coupling schemes between replicas and is agnostic to the specific underlying simulation--classical or quantum, serial or parallel simulation. The scalability of the framework is assessed using standard simulation benchmarks. In spite of the increasing communication and coordination\u00a0\u2026", "num_citations": "9\n", "authors": ["1575"]}
{"title": "Energy landscape analysis for regulatory RNA finding using scalable distributed cyberinfrastructure\n", "abstract": " We investigate the folding energy landscape for a given RNA sequence through Boltzmann ensemble (BE) sampling of RNA secondary structures. The ensemble of sampled structures is used to derive distributions of energies and base\u2010pair distances between two configurations. We identify structural features that can be utilized for RNA gene finding. Characterization of the EL through BE sampling of secondary structures is computationally demanding and has multiple heterogeneous stages. We develop the Distributed Adaptive Runtime Environment to effectively address the computational requirements. Distributed Adaptive Runtime Environment is built upon an extensible and interoperable pilot\u2010job and supports the concurrent execution of a broad range of task sizes across a range of infrastructure. It is used to investigate two RNA systems of different sizes, S\u2010adenosyl methionine (SAM) binding RNA\u00a0\u2026", "num_citations": "9\n", "authors": ["1575"]}
{"title": "Diagnosis, prevalence, and prevention of the spread of the parasite Heterosporis sp.(Microsporida: Pleistophoridae) in yellow perch (Perca flavescens) and other freshwater fish\u00a0\u2026\n", "abstract": " A previously unknown microsporidian parasite that severely degrades muscle of yellow perch (Perca flavescens) from lakes in Minnesota, Wisconsin, and Lake Ontario was identified as belonging to the genus Heterosporis. This parasite is characterized by pyriform-shaped spores that are contained in sporophorocysts. In the wild, yellow perch, burbot (Lota lota), mottled sculpin (Cottus bairdi), trout-perch (Percopsis omiscomaycus), pumpkinseed (Lepomis gibbosus), northern pike (Esox lucius), walleye (Sander vitreus) and rock bass (Ambloplites rupestris) harbor Heterosporis naturally, but laboratory studies showed that 12 other fish species are susceptible to infection. In laboratory trials, smallmouth bass (Micropterus dolomieui) consumed significantly more fathead minnows infected with Heterosporis sp. than uninfected. Microscopically, Heterosporis sp. infection can be detected in the muscle of fish two weeks after exposure, and visually identified after seven weeks. To confirm infection, a PCR diagnostic assay was developed. Heterosporis spores are rendered noninfective by freezing, desiccation for 24 h, exposure to 2,200 mg/L bleach, and aging in air-exposed water for six months. This parasite can infect a wide range of fish species which can lead to devastating losses in commercial and sport fishing; however, there are preventative measures that may limit the spread of the parasite.", "num_citations": "9\n", "authors": ["1575"]}
{"title": "The RealityGrid computational steering API\n", "abstract": " RealityGrid partners have prior experience of computational steering in various frameworks [Love, DIVE, Stanton]. Others have also done notable work in this field [Cactus, COVISE, COVISA, Cumulvs, GRASPARC, SCIRun, VISIT].", "num_citations": "9\n", "authors": ["1575"]}
{"title": "Extensible and scalable adaptive sampling on supercomputers\n", "abstract": " The accurate sampling of protein dynamics is an ongoing challenge despite the utilization of high-performance computer (HPC) systems. Utilizing only \u201cbrute force\u201d molecular dynamics (MD) simulations requires an unacceptably long time to solution. Adaptive sampling methods allow a more effective sampling of protein dynamics than standard MD simulations. Depending on the restarting strategy, the speed up can be more than 1 order of magnitude. One challenge limiting the utilization of adaptive sampling by domain experts is the relatively high complexity of efficiently running adaptive sampling on HPC systems. We discuss how the ExTASY framework can set up new adaptive sampling strategies and reliably execute resulting workflows at scale on HPC platforms. Here, the folding dynamics of four proteins are predicted with no a priori information.", "num_citations": "8\n", "authors": ["1575"]}
{"title": "Security analysis of ABAC under an administrative model\n", "abstract": " In the present-day computing environment, where access control decisions are often dependent on contextual information like the location of the requesting user and the time of access request, attribute-based access control (ABAC) has emerged as a suitable choice for expressing security policies. In an ABAC system, access decisions depend on the set of attribute values associated with the subjects, resources, and the environment in which an access request is made. In such systems, the task of managing the set of attributes associated with the entities as well as that of analysing and understanding the security implications of each attribute assignment is of paramount importance. Here, the authors first introduce a comprehensive attribute-based administrative model, named as AMABAC (Administrative Model for ABAC), for ABAC systems and then suggest a methodology for analysing the security properties of\u00a0\u2026", "num_citations": "8\n", "authors": ["1575"]}
{"title": "Executing dynamic and heterogeneous workloads on super computers\n", "abstract": " Many scientific applications have workloads comprised of multiple heterogeneous tasks that are not known in advance and may vary in the resources needed during execution. However, high-performance computing systems are designed to support applications comprised of mostly monolithic, single-job workloads. Pilot systems decouple workload specification, resource selection, and task execution via job placeholders and late-binding. Pilot systems help to satisfy the resource requirements of workloads comprised of multiple tasks with the capabilities and usage policies of HPC systems. RADICAL-Pilot (RP) is a portable, modular and extensible Python-based Pilot system. In this paper we describe RP\u2019s design, discuss how it is engineered, characterize its performance and show its ability to execute heterogeneous and dynamic workloads on a range of high-performance computing systems. RP is capable of spawning more than 100 tasks/second and the steady-state execution of up to 8,000 concurrent tasks. RP can be used stand-alone, as well as integrated with other application-level tools as a runtime system.", "num_citations": "8\n", "authors": ["1575"]}
{"title": "Building gateways for life-science applications using the dynamic application runtime environment (DARE) framework\n", "abstract": " This work is predicated on three important trends:(i) that the importance, impact and percentage of TeraGrid/XD resources assigned to the life sciences is increasing at a rate that is probably greater than other disciplines,(ii) that gateways have proven to be a very effective access mechanism to distributed HPC resources provided by the TeraGrid/XD, and in particular a very successful model for shared/community access models, and (iii) that in spite of the previous two points there are missing capabilities and abstractions that enable the use of the collective capacity of distributed cyberinfrastructure such as TeraGrid/XD, especially those that can be used to develop gateways in an easy, extensible and scalable fashion for both compute and data-intensive applications. We introduce the SAGA-based, Dynamic Application Runtime Environment (DARE) framework from which extensible, versatile and effective gateways\u00a0\u2026", "num_citations": "8\n", "authors": ["1575"]}
{"title": "A fresh perspective on developing and executing DAG-based distributed applications: a case-study of SAGA-based montage\n", "abstract": " Most workflow based applications currently have to adapt to available tools. While this keeps the cost of development low, it can lead to performance and flexibility tradeoffs that the application developer and deployer must make. In this paper, we use the Montage astronomical image mosaicking application as prototypical DAG-based workflow application to layout the development and deployment decisions for distributed applications. We discuss and explain the lack of simple (easy-to-use), scalable, and extensible distributed applications. We then introduce SAGA as a technology that permits the construction of abstractions that aid the development and execution of the applications, and thus addresses some of common shortcomings of traditional distributed applications development. We use Montage together with SAGA to examine how legacy applications can be made to run on distributed infrastructures, to see\u00a0\u2026", "num_citations": "8\n", "authors": ["1575"]}
{"title": "On-chip micro-droplet dispenser with disposa-ble structure\n", "abstract": " We succeeded in dispensing micro-droplet by a disposable on-chip inkjet mechanism. Novelty of this paper is summarized as follows.(1) We employed a glass bonded PDMS to obtain leaf spring structure whose spring coefficient is 14 times of the conventional PDMS chip. As a result, we succeeded in dispensing of droplets by vibrating the membrane using the piezoelectric actuator attached to the PDMS disposable chip.(2) The nozzle for droplet dispensing was fabricated by SU-8 to obtain the hydrophobic surface which prevents any undesired satellite droplets produced. Consequently the accuracy of position of the droplet dispensing was achieved\u00b15 \u03bcm. The size of the droplet produced from the disposable nozzle (diameter= 100, \u03bcm) was in the range of 95-105 \u03bcm at the applied voltage of 105 V.", "num_citations": "8\n", "authors": ["1575"]}
{"title": "Distributed I/O with ParaMEDIC: Experiences with a worldwide supercomputer\n", "abstract": " Achieving high performance for distributed I/O on a wide-area network continues to be an elusive holy grail. Despite enhancements in network hardware as well as software stacks, achieving high-performance remains a challenge. In this paper, our worldwide team took a completely new and non-traditional approach to distributed I/O, called ParaMEDIC: Parallel Metadata Environment for Distributed I/O and Computing, by utilizing application-specific transformation of data to orders-of-magnitude smaller meta-data before performing the actual I/O. Specifically, this paper details our experiences in deploying a large-scale system to facilitate the discovery of missing genes and constructing a genome similarity tree by encapsulating the mpiBLAST sequence-search algorithm into ParaMEDIC. The overall project involved nine different computational sites spread across the US generating more than a petabyte of data, that was \u201cteleported\u201d to a large-scale facility in Tokyo for storage.", "num_citations": "8\n", "authors": ["1575"]}
{"title": "Programming Abstractions for Clouds\n", "abstract": " Clouds seem like\u2019Grids Done Right\u2019, including scalability, transparency, and ease of management. Virtual Machines are the dominant application environments for compute Clouds, however, that does not make application programming any less relevant than \u201cnon-virtualized\u201d environments. The limited set of successful Cloud applications show that distributed programming patterns of the type of MapReduce and All-Pairs are required to make Cloud infrastructure a viable compute environment for a large class of problems. The existence of multiple implementations of these programming paradigms also makes clear, that application portability is, even for Clouds an emerging problem which needs addressing beyond the level of system virtualization. This paper discusses these and other challenges around cloud applications programming and development, and through a discussion of several applications, demonstrates potential solutions. We discuss how using the right abstractions\u2013programming interfaces, frameworks that support commonly occurring programming and execution patterns\u2013enable efficient, extensible and importantly system-independent implementations of common programming patterns such as MapReduce, ie same application is usable seamlessly on both traditional Grids and Clouds systems. We further discuss that lessons learned from programming applications for Grid environment also apply, to some extent, to Cloud environments. 1", "num_citations": "8\n", "authors": ["1575"]}
{"title": "The Options Approach to Software Prototyping Decisions.\n", "abstract": " Prototyping is often used to predict, or reduce the uncertainty over, the future profitability of a software design choice. Boehm 1 pioneered the use of techniques from statistical decision theory to provide a basis for making prototyping decisions. However his approach does not apply to situations where the software engineer has the flexibility of waiting for more information before making a prototyping decision. Also, his framework considers only uncertainty over one time period, and assumes a design-choice must be made immediately after prototyping. We propose a more general multi-period approach that takes into account the flexibility of being able to postpone the prototyping and design decisions. In particular, we argue that this flexibility is analogous to the flexibility of exercise of certain financial instruments called options, and that the value of the flexibility is the value of the corresponding financial option. The field of real option theory in finance provides a rigorous framework to analyze the optimal exercise of such options, and this can be applied to the prototyping decision problem. Our approach integrates the timing of prototype decisions and design decisions within a single framework.", "num_citations": "8\n", "authors": ["1575"]}
{"title": "A simulated annealing based state assignment approach for control synthesis\n", "abstract": " The optimality of synthesized control designs for complex VLSI systems hinges to a great extent on the efficiency of the state assignment phase. A new system is presented for state assignment of sequential functions modelled as finite state machines. Using a simulated annealing technique and an embedded mechanism to vary the state assignment length, this scheme arrives at a synthesized logic that is efficient in terms of area occupied by both the memory elements and the combinational logic. This is in contrast to most existing methods for state assignment that use minimum code length to ensure least cost of sequential logic. Appropriate annealing schedules, perturbation functions and grouping of state codes for efficient state assignment have been arrived at. The authors present results that indicate that it is possible to achieve significant improvements in both the area and delay of the combinational logic by\u00a0\u2026", "num_citations": "8\n", "authors": ["1575"]}
{"title": "Atom: Robustifying out-of-distribution detection using outlier mining\n", "abstract": " Detecting out-of-distribution (OOD) inputs is critical for safely deploying deep learning models in an open-world setting. However, existing OOD detection solutions can be brittle in the open world, facing various types of adversarial OOD inputs. While methods leveraging auxiliary OOD data have emerged, our analysis on illuminative examples reveals a key insight that the majority of auxiliary OOD examples may not meaningfully improve or even hurt the decision boundary of the OOD detector, which is also observed in empirical results on real data. In this paper, we provide a theoretically motivated method, Adversarial Training with informative Outlier Mining (ATOM), which improves the robustness of OOD detection. We show that, by mining informative auxiliary OOD data, one can significantly improve OOD detection performance, and somewhat surprisingly, generalize to unseen adversarial attacks. ATOM\u00a0\u2026", "num_citations": "7\n", "authors": ["1575"]}
{"title": "CaPC Learning: Confidential and Private Collaborative Learning\n", "abstract": " Machine learning benefits from large training datasets, which may not always be possible to collect by any single entity, especially when using privacy-sensitive data. In many contexts, such as healthcare and finance, separate parties may wish to collaborate and learn from each other's data but are prevented from doing so due to privacy regulations. Some regulations prevent explicit sharing of data between parties by joining datasets in a central location (confidentiality). Others also limit implicit sharing of data, e.g., through model predictions (privacy). There is currently no method that enables machine learning in such a setting, where both confidentiality and privacy need to be preserved, to prevent both explicit and implicit sharing of data. Federated learning only provides confidentiality, not privacy, since gradients shared still contain private information. Differentially private learning assumes unreasonably large datasets. Furthermore, both of these learning paradigms produce a central model whose architecture was previously agreed upon by all parties rather than enabling collaborative learning where each party learns and improves their own local model. We introduce Confidential and Private Collaborative (CaPC) learning, the first method provably achieving both confidentiality and privacy in a collaborative setting. We leverage secure multi-party computation (MPC), homomorphic encryption (HE), and other techniques in combination with privately aggregated teacher models. We demonstrate how CaPC allows participants to collaborate without having to explicitly join their training sets or train a central model. Each party is able to improve the\u00a0\u2026", "num_citations": "7\n", "authors": ["1575"]}
{"title": "Analyzing and Improving Neural Networks by Generating Semantic Counterexamples through Differentiable Rendering\n", "abstract": " Even as deep neural networks (DNNs) have achieved remarkable success on vision-related tasks, their performance is brittle to transformations in the input. Of particular interest are semantic transformations that model changes that have a basis in the physical world, such as rotations, translations, changes in lighting or camera pose. In this paper, we show how differentiable rendering can be utilized to generate images that are informative, yet realistic, and which can be used to analyze DNN performance and improve its robustness through data augmentation. Given a differentiable renderer and a DNN, we show how to use off-the-shelf attacks from adversarial machine learning to generate semantic counterexamples -- images where semantic features are changed as to produce misclassifications or misdetections. We validate our approach on DNNs for image classification and object detection. For classification, we show that semantic counterexamples, when used to augment the dataset, (i) improve generalization performance (ii) enhance robustness to semantic transformations, and (iii) transfer between models. Additionally, in comparison to sampling-based semantic augmentation, our technique generates more informative data in a sample efficient manner.", "num_citations": "7\n", "authors": ["1575"]}
{"title": "Generating semantic adversarial examples with differentiable rendering\n", "abstract": " Machine learning (ML) algorithms, especially deep neural networks, have demonstrated success in several domains. However, several types of attacks have raised concerns about deploying ML in safety-critical domains, such as autonomous driving and security. An attacker perturbs a data point slightly in the pixel space and causes the ML algorithm to misclassify (eg a perturbed stop sign is classified as a yield sign). These perturbed data points are called adversarial examples, and there are numerous algorithms in the literature for constructing adversarial examples and defending against them. In this paper we explore semantic adversarial examples (SAEs) where an attacker creates perturbations in the semantic space. For example, an attacker can change the background of the image to be cloudier to cause misclassification. We present an algorithm for constructing SAEs that uses recent advances in differential rendering and inverse graphics.", "num_citations": "7\n", "authors": ["1575"]}
{"title": "A parallel unidirectional coupled DEM-PBM model for the efficient simulation of computationally intensive particulate process systems\n", "abstract": " The accurate modeling of the physics underlying particulate processes is complicated and requires significant computational capabilities to solve using particle-based models. In this work, a unidirectional multi-scale approach was used to model the high shear wet granulation process. A multi-dimensional population balance model (PBM) was developed with a mechanistic kernel, which in turn obtained collision data from the discrete element modeling (DEM) simulation. The PBM was parallelized using a hybrid OpenMP+MPI approach. The DEM simulations were performed using LIGGGHTS, which was parallelized using MPI. Speedups of approximately 14 were obtained for the PBM simulations and approximately 12 for the DEM simulations. The uni-directional coupling of DEM to PBM was performed using middle-ware components (RADICAL-Pilot) that did not require modifications of the DEM or PBM codes, yet\u00a0\u2026", "num_citations": "7\n", "authors": ["1575"]}
{"title": "Building blocks for workflow system middleware\n", "abstract": " We suggest there is a need for a fresh perspective on the design and development of middleware for high-performance workflows and workflow systems. We argue for a building blocks approach, outline a description of this approach and define their properties. We discuss RADICAL-Cybertools as one implementation of the building blocks concept, showing how they have been designed and developed in accordance with this approach. We discuss three case-studies where RADICAL-Cybertools have been used to develop new workflow systems capabilities and in-tegrated to enhance existing ones, illustrating the potential and promise of the building blocks approach.", "num_citations": "7\n", "authors": ["1575"]}
{"title": "Synthesizing near-optimal malware specifications from suspicious behaviors\n", "abstract": " Behavior-based detection techniques are a promising solution to the problem of malware proliferation. However, they require precise specifications of malicious behavior that do not result in an excessive number of false alarms, while still remaining general enough to detect new variants before traditional signatures can be created and distributed. In this paper, we present an automatic technique for extracting optimally discriminative specifications, which uniquely identify a class of programs. Such a discriminative specification can be used by a behavior-based malware detector. Our technique, based on graph mining and stochastic optimization, scales to large classes of programs. When this work was originally published, the technique yielded favorable results on malware targeted towards workstations (~86% detection rates on new malware). We believe that it can be brought to bear on emerging malware-based\u00a0\u2026", "num_citations": "7\n", "authors": ["1575"]}
{"title": "An automated video surveillance system using Viewpoint Feature Histogram and CUDA-enabled GPUs\n", "abstract": " This paper presents an automated video surveillance system which deals with content monitoring and activity change in the environment. We use Viewpoint Feature Histogram, an image descriptor for object recognition and pose estimation for the purpose of monitoring in the surveillance system. In order to enhance the performance of the system, we exploit the GPU architecture to perform data intensive task of surveillance system and implement it on CUDA-enabled devices. The experimental evaluation on the static data sets and live scenes captured from Microsoft Kinect show that Viewpoint Feature Histogram can be successfully used as an image descriptor in surveillance systems. We also test the performance of the Viewpoint Feature Histogram generation for different data sets on GPU and CPU to conclude that GPU clearly outperforms CPU for larger datasets.", "num_citations": "7\n", "authors": ["1575"]}
{"title": "Group interaction on interactive multi-touch tables by children in India\n", "abstract": " Interactive tables provide multi-touch capabilities that can enable children to collaborate face-to-face simultaneously. In this paper we extend existing understanding of children's use of interactive tabletops by examining their use by school children in a school in Delhi, India. In the study, we explore how the school children exhibit particular types of collaboration strategies and touch input techniques when dealing with digital objects. In particular, we highlight a number of behaviours of interest, such as how the children would move the same digital object on the table together. We also discuss how the children work in close proximity to each other and dynamically organize their spatial positions in order to work together, as well as establish territory and control. We go on to examine some of the finger-based interaction and manipulation strategies that arise in these contexts. Finally, the paper considers the implications\u00a0\u2026", "num_citations": "7\n", "authors": ["1575"]}
{"title": "A practical and comprehensive graduate course preparing students for research involving scientific computing\n", "abstract": " We describe a new graduate course in scientific computing that was taught during Fall 2010 at Louisiana State University. The course was designed to provide students with a broad and practical introduction to scientific computing which would provide them with the basic skills and experience to very quickly get involved in research projects involving modern cyberinfrastructure and complex real world scientific problems. The course, which was taken by thirteen graduate students, covered basic skills, networks and data, simulations and application frameworks, scientific visualization, and distributed scientific computing. Notable features of the course include a modularized team-teaching approach, and the integration of national cyberinfrastucture with teaching.", "num_citations": "7\n", "authors": ["1575"]}
{"title": "Self-adaptive architectures for autonomic computational science\n", "abstract": " Self-adaptation enables a system to modify it\u2019s behaviour based on changes in its operating environment. Such a system must utilize monitoring information to determine how to respond either through a systems administrator or automatically (based on policies pre-defined by an administrator) to such changes. In computational science applications that utilize distributed infrastructure (such as Computational Grids and Clouds), dealing with heterogeneity and scale of the underlying infrastructure remains a challenge. Many applications that do adapt to changes in underlying operating environments often utilize ad hoc, application-specific approaches. The aim of this work is to generalize from existing examples, and thereby lay the foundation for a framework for Autonomic Computational Science (ACS). We use two existing applications \u2013 Ensemble Kalman Filtering and Coupled Fusion Simulation \u2013 to\u00a0\u2026", "num_citations": "7\n", "authors": ["1575"]}
{"title": "Novel submission modes for tightly coupled jobs across distributed resources for reduced time-to-solution\n", "abstract": " The problems of scheduling a single parallel job across a large-scale distributed system are well known and surprisingly difficult to solve. In addition, because of the issues involved in distributed submission, such as co-reserving resources, and managing accounts and certificates simultaneously on multiple machines, etc., the vast number of high-performance computing (HPC) application users have been happy to remain restricted to submitting jobs to single machines. Meanwhile, the need to simulate larger and more complex physical systems continues to grow, with a concomitant increase in the number of cores required to solve the resulting scientific problems. One might reduce the demand on load per machine, and eventually the wait-time in queue, by decomposing the problem to use two resources in such circumstances, even though there might be a reduction in the peak performance. This motivates a\u00a0\u2026", "num_citations": "7\n", "authors": ["1575"]}
{"title": "Semantic adversarial deep learning\n", "abstract": " Adversarial examples have emerged as a key threat for machine-learning-based systems, especially the ones that employ deep neural networks. Unlike a large body of research in this area, this Keynote article accounts for the semantic, context, and specifications of the complete system with machine learning components in resource-constrained environments. -Muhammad Shafique, Technische Universit\u00e4t Wien.", "num_citations": "6\n", "authors": ["1575"]}
{"title": "Query-efficient physical hard-label attacks on deep learning visual classification\n", "abstract": " Existing physical adversarial examples for computer vision rely on white-box access. In this work, we investigate physical examples in the black-box hard-label case--where the attacker has only query access to the model and only receives the top-1 class label without confidence information. This threat model is more realistic for cyber-physical systems--the main target when considering physical attacks on computer vision. Key challenges in this setting include obtaining reliability against environmental variations and creating area-limited perturbations without access to model gradients. We base our work on recent advances in gradient-free optimization and present GRAPHITE, the first algorithm for black-box hard-label physical attacks on computer vision models. We evaluate GRAPHITE on a traffic sign classifier and a publicly-available Automatic License Plate Recognition (ALPR) tool using only query access. We\u00a0\u2026", "num_citations": "6\n", "authors": ["1575"]}
{"title": "Improving utility and security of the shuffler-based differential privacy\n", "abstract": " When collecting information, local differential privacy (LDP) alleviates privacy concerns of users because their private information is randomized before being sent it to the central aggregator. LDP imposes large amount of noise as each user executes the randomization independently. To address this issue, recent work introduced an intermediate server with the assumption that this intermediate server does not collude with the aggregator. Under this assumption, less noise can be added to achieve the same privacy guarantee as LDP, thus improving utility for the data collection task. This paper investigates this multiple-party setting of LDP. We analyze the system model and identify potential adversaries. We then make two improvements: a new algorithm that achieves a better privacy-utility tradeoff; and a novel protocol that provides better protection against various attacks. Finally, we perform experiments to compare different methods and demonstrate the benefits of using our proposed method.", "num_citations": "6\n", "authors": ["1575"]}
{"title": "Convergence of data generation and analysis in the biomolecular simulation community\n", "abstract": " In the biomolecular simulation (BMS) community, classical molecular dynamics (MD) simulations enable the elucidation of the relationship between the structure of biomolecules such as proteins, nucleic acids, or lipids and their function via their dynamics. MD simulations account for approximately one quarter of the service units used on XSEDE resources. Although traditionally the generation of the data has been the computational bottleneck and has been highly optimized, more and more the analysis of the data is becoming a rate limiting step. Within the NSF DIBBs SPIDAL project we have been working on leveraging HPC resources for the analysis of BMS data [1], starting from two widely adopted software packages in the community, cpptraj [2] and MDAnalysis [3, 4].Current state of the art simulations are performed at the atomic level and include the niomolecules and their environment such as water, ions, lipids, and small molecules. Typical system sizes range from O (10\u200b 3\u200b) to O (10\u200b 6\u200b) atoms with some exceptionally large systems up to~ 10\u200b 8\u200b[5]. Simulations integrate the equations of motion of all atoms using femtosecond timesteps. The positions (and possibly velocities) of the atoms are saved to a trajectory file at regular intervals, typically every 1 to 100 ps. Current simulation lengths typically achieve\u2264 10 \u00b5s although on special hardware up to 1 ms has been achieved for small systems with O (10\u200b 4\u200b) atoms [6], while massively distributed simulations can produce aggregate data of up to 6 ms [7]. Advances in hardware (GPUs, FPGA/custom hardware, exascale resources such as Summit) and software (eg GPU-optimized codes\u00a0\u2026", "num_citations": "6\n", "authors": ["1575"]}
{"title": "Enabling trade-offs between accuracy and computational cost: adaptive algorithms to reduce time to clinical insight\n", "abstract": " The efficacy of drug treatments depends on how tightly small molecules bind to their target proteins. Quantifying the strength of these interactions (the so called `binding affinity') is a grand challenge of computational chemistry, surmounting which could revolutionize drug design and provide the platform for patient specific medicine. Recently, evidence from blind challenge predictions and retrospective validation studies has suggested that molecular dynamics (MD) can now achieve useful predictive accuracy ( 1 kcal/mol) This accuracy is sufficient to greatly accelerate hit to lead and lead optimization. To translate these advances in predictive accuracy so as to impact clinical and/or industrial decision making requires that binding free energy results must be turned around on reduced timescales without loss of accuracy. This demands advances in algorithms, scalable software systems, and intelligent and efficient\u00a0\u2026", "num_citations": "6\n", "authors": ["1575"]}
{"title": "Integration Of PanDA Workload Management System With Supercomputers for ATLAS and Data Intensive Science\n", "abstract": " The.LHC, operating at CERN, is leading Big Data driven scientific explorations. Experiments at the LHC explore the fundamental nature of matter and the basic forces that shape our universe. ATLAS, one of the largest collaborations ever assembled in the sciences, is at the forefront of research at the LHC. To address an unprecedented multi-petabyte data processing challenge, the ATLAS experiment is relying on a heterogeneous distributed computational infrastructure. The ATLAS experiment uses PanDA (Production and Data Analysis) Workload Management System for managing the workflow for all data processing on over 150 data centers. Through PanDA, ATLAS physicists see a single computing facility that enables rapid scientific breakthroughs for the experiment, even though the data centers are physically scattered all over the world. While PanDA currently uses more than 250,000 cores with a peak\u00a0\u2026", "num_citations": "6\n", "authors": ["1575"]}
{"title": "Understanding performance of distributed data-intensive applications\n", "abstract": " Grids, clouds and cloud-like infrastructures are capable of supporting a broad range of data-intensive applications. There are interesting and unique performance issues that appear as the volume of data and degree of distribution increases. New scalable data-placement and management techniques, as well as novel approaches to determine the relative placement of data and computational workload, are required. We develop and study a genome sequence matching application that is simple to control and deploy, yet serves as a prototype of a data-intensive application. The application uses a SAGA-based implementation of the All-Pairs pattern. This paper aims to understand some of the factors that influence the performance of this application and the interplay of those factors. We also demonstrate how the SAGA approach can enable data-intensive applications to be extensible and interoperable over a range of\u00a0\u2026", "num_citations": "6\n", "authors": ["1575"]}
{"title": "Recent advances in intrusion detection\n", "abstract": " On behalf of the Program Committee, it is our pleasure to present the proceedings of the 11th International Symposium on Recent Advances in Intrusion Detection (RAID 2008), which took place in Cambridge, Massachusetts, USA on September 15\u201317.The symposium brought together leading researchers and practitioners from academia, government and industry to discuss intrusion detection research and practice. There were six main sessions presenting full-fledged research papers (rootkit prevention, malware detection and prevention, high performance intrusion and evasion, web application testing and evasion, alert correlation and worm detection, and anomaly detection and network traffic analysis), a session of posters on emerging research areas and case studies, and two panel discussions (\u201cGovernment Investments: Successes, Failures and the Future\u201d and \u201cLife after Antivirus-What Does the Future Hold\u00a0\u2026", "num_citations": "6\n", "authors": ["1575"]}
{"title": "Using lambda networks to enhance performance of interactive large simulations\n", "abstract": " The ability to use a visualisation tool to steer large simulations provides innovative and novel usage scenarios, e.g. the ability to use new algorithms for the computation of free energy profiles along a nanopore [1]. However, we find that the performance of interactive simulations is sensitive to the quality of service of the network with variable latency and packet loss in particular having a detrimental effect. The use of dedicated networks (provisioned in this case as a circuit-switched, point-to-point optical lightpath or lambda) can lead to significant (50% or more) performance enhancement. When running on say 128 or 256 processors of a high-end supercomputer this saving has a significant value. We perform experiments to understand the impact of network characteristics on the performance of a large parallel classical molecular dynamics simulation when coupled interactively to a remote visualisation tool. This paper\u00a0\u2026", "num_citations": "6\n", "authors": ["1575"]}
{"title": "Exact calculation of peptide-protein binding energies by steered thermodynamic integration using high performance computing grids\n", "abstract": " We describe a grid-based method to dramatically accelerate from weeks to under 48 hours the calculation of differences in binding free energy and its application to Src homology 2 (SH2) protein cell signalling domains. The method of calculation, thermodynamic integration, is briefly outlined and we indicate how the calculation process works from the perspective of an application scientist using either the UK National Grid Service or the US Teragrid. The development of a PDA-based steering client is especially useful as it gives the application scientist more freedom. Finally, we discuss our experience in developing and deploying the application on a grid.", "num_citations": "6\n", "authors": ["1575"]}
{"title": "Towards discovering and containing privacy violations in software\n", "abstract": " Malicious code can wreak havoc on our cyberinfrastructure.  Hence, discovering and containing malicious code is an important goal.  This paper focuses on privacy-violating malicious code.  Examples of privacy violations are leaking private user data to an external entity or downloading data to a user's host without their permission.  Spyware, which has recently received considerable attention in the popular literature is an important example of privacy-violating malicious code.  We propose a multi-step approach to discovering and containing privacy violations.  We have designed and implemented a dynamic slicing tool to discover dependencies between events in an execution trace.  We demonstrate that dynamic slicing can be used to discover privacy violations. Information gatbered using dynamic slicing can be used to construct security policies to contain the discovered privacy violations.  These security policies are then enforced by a sandbox.  We have implemented a sandbox for Windows, and have successfully evaluated our approach on two applications: KaZaa and RealOne Player.  For both of these applications we were able to discover privacy violations in them using our dynamic-slicing tool.  Moreover, using information gathered through dynamic slicing we were able to design policies to thwart these privacy violations.  Although our preliminary evaluation was performed on spyware, in the future we will evaluate our approach on other privacy violating malicious code.", "num_citations": "6\n", "authors": ["1575"]}
{"title": "Agent cloning\n", "abstract": " Multi agent systems are subject to performance bottlenecks in cases where agents cannot perform tasks by themselves due to insufficient resources. Solutions to such problems include passing tasks to others or agent migration to remote hosts. We propose agent cloning as a more comprehensive approach to the problem of local agent overloads. According to our paradigm, agents may clone, pass tasks to others, die or merge.", "num_citations": "6\n", "authors": ["1575"]}
{"title": "Detecting Anomalous Inputs to DNN Classifiers By Joint Statistical Testing at the Layers\n", "abstract": " Detecting anomalous inputs, such as adversarial and out-of-distribution (OOD) inputs, is critical for classifiers deployed in real-world applications, especially deep neural network (DNN) classifiers that are known to be brittle on such inputs. We propose an unsupervised statistical testing framework for detecting such anomalous inputs to a trained DNN classifier based on its internal layer representations. By calculating test statistics at the input and intermediate-layer representations of the DNN, conditioned individually on the predicted class and on the true class of labeled training data, the method characterizes their class-conditional distributions on natural inputs. Given a test input, its extent of non-conformity with respect to the training distribution is captured using p-values of the class-conditional test statistics across the layers, which are then combined using a scoring function designed to score high on anomalous inputs. We focus on adversarial inputs, which are an important class of anomalous inputs, and also demonstrate the effectiveness of our method on general OOD inputs. The proposed framework also provides an alternative class prediction that can be used to correct the DNNs prediction on (detected) adversarial inputs. Experiments on well-known image classification datasets with strong adversarial attacks, including a custom attack method that uses the internal layer representations of the DNN, demonstrate that our method outperforms or performs comparably with five recently-proposed, competing detection methods.", "num_citations": "5\n", "authors": ["1575"]}
{"title": "An investigation of data poisoning defenses for online learning\n", "abstract": " Data poisoning attacks -- where an adversary can modify a small fraction of training data, with the goal of forcing the trained classifier to high loss -- are an important threat for machine learning in many applications. While a body of prior work has developed attacks and defenses, there is not much general understanding on when various attacks and defenses are effective. In this work, we undertake a rigorous study of defenses against data poisoning for online learning. First, we study four standard defenses in a powerful threat model, and provide conditions under which they can allow or resist rapid poisoning. We then consider a weaker and more realistic threat model, and show that the success of the adversary in the presence of data poisoning defenses there depends on the \"ease\" of the learning problem.", "num_citations": "5\n", "authors": ["1575"]}
{"title": "Privacy-Preserving Ridge Regression on Distributed Data.\n", "abstract": " Linear regression is an important statistical tool that models the relationship between some explanatory values and an outcome value using a linear function. In many current applications (eg predictive modelling in personalized healthcare), these values represent sensitive data owned by several different parties that are unwilling to share them. In this setting, training a linear regression model becomes challenging and needs specific cryptographic solutions. In this work, we propose a new system that can train a linear regression model with 2-norm regularization (ie ridge regression) on a dataset obtained by merging a finite number of private datasets. Our system is composed of two phases: The first one is based on a simple homomorphic encryption scheme and takes care of securely merging the private datasets. The second phase is a new ad-hoc twoparty protocol that computes a ridge regression model solving a linear system where all coefficients are encrypted. The efficiency of our system is evaluated both on synthetically generated and real-world datasets.", "num_citations": "5\n", "authors": ["1575"]}
{"title": "Policy by example: an approach for security policy specification\n", "abstract": " Policy specification for personal user data is a hard problem, as it depends on many factors that cannot be predetermined by system developers. Simultaneously, systems are increasingly relying on users to make security decisions. In this paper, we propose the approach of Policy by Example (PyBE) for specifying user-specific security policies. PyBE brings the benefits of the successful approach of programming by example (PBE) for program synthesis to the policy specification domain. In PyBE, users provide policy examples that specify if actions should be allowed or denied in certain scenarios. PyBE then predicts policy decisions for new scenarios. A key aspect of PyBE is its use of active learning to enable users to correct potential errors in their policy specification. To evaluate PyBE's effectiveness, we perform a feasibility study with expert users. Our study demonstrates that PyBE correctly predicts policies with 76% accuracy across all users, a significant improvement over naive approaches. Finally, we investigate the causes of inaccurate predictions to motivate directions for future research in this promising new domain.", "num_citations": "5\n", "authors": ["1575"]}
{"title": "Integration of Panda Workload Management System with supercomputers\n", "abstract": " The Large Hadron Collider (LHC), operating at the international CERN Laboratory in Geneva, Switzerland, is leading Big Data driven scientific explorations. Experiments at the LHC explore the fundamental nature of matter and the basic forces that shape our universe, and were recently credited for the discovery of a Higgs boson. ATLAS, one of the largest collaborations ever assembled in the sciences, is at the forefront of research at the LHC. To address an unprecedented multi-petabyte data processing challenge, the ATLAS experiment is relying on a heterogeneous distributed computational infrastructure. The ATLAS experiment uses PanDA (Production and Data Analysis) Workload Management System for managing the workflow for all data processing on over 140 data centers. Through PanDA, ATLAS physicists see a single computing facility that enables rapid scientific breakthroughs for the\u00a0\u2026", "num_citations": "5\n", "authors": ["1575"]}
{"title": "High-level software frameworks to surmount the challenge of 100x scaling for biomolecul ar simulation science\n", "abstract": " Next-generation exascale systems will fundamentally expand the reach of biomolecular simulations and the resulting scientific insight, enabling the simulation of larger biological systems (weak scaling), longer timescales (strong scaling), more complex molecular interactions, and robust uncertainty quantification (more accurate sampling). Since currently envisioned exascale hardware architectures are essentially larger versions of systems available today, it will be challenging to solve biological problems that require longer timescales, involve more complex interactions and robust uncertainty quantification without significant algorithmic improvements. We believe that high-level simulation algorithms incorporating high-level parallelism and leveraging the statistical nature of molecular processes can provide a means to address these challenges of scaling. Proof-of-concept simulation algorithms have yielded advanced sampling and adaptive control algorithms for efficient simulation of long timescales and complex behaviors. Novel dataflow and workflow systems are needed to implement these advanced algorithms in a way that is usable by the community in exascale systems. A middleware ecosystem that provides these in a robust, scalable, reusable, and extensible framework is a key requirement for exascale infrastructure investment to result in revolutionary biological insight.In the past decade, substantial algorithmic and hardware advances have led to improvements in strong and weak scaling that permit millisecond-length simulations of moderate-sized biomolecular systems and short simulations for large assemblies [1]. Both of these have\u00a0\u2026", "num_citations": "5\n", "authors": ["1575"]}
{"title": "Mobile agent based architecture to prevent session hijacking attacks in IEEE 802.11 WLAN\n", "abstract": " Session Hijacking is one of the major threats in IEEE 802.11 Wireless LANs. Existing methodologies to overcome this attack seems to be non comprehensive. These Existing mechanisms to counter-act this attack increase the communicational overhead and load on the server and client. In this paper we propose a novel and robust mobile agent based mechanism to overcome the session hijacking attacks in IEEE 802.11 WLANs. This architecture addresses methodologies of cookie encryption, token generation and session key generation.", "num_citations": "5\n", "authors": ["1575"]}
{"title": "Optical gain of InGaAlAs quantum well with different barriers, claddings and substrates\n", "abstract": " The fundamental characteristic of the quantum well heterostructures is the optical gain. In this paper, the effect of barriers (InGaAlAs and AlGaAs), claddings (InAlAs and AlGaAs) and substrates (InP and GaAs) materials on the optical gain of InGaAlAs quantum well of 6 nm width has been studied with in TE and TM polarization modes. The overall size (width) of the STIN-SCH (step index\u2013separate confinement heterostructure) based nanoheterostructure including single quantum well along with barrier and claddings is 36 nm. In TE mode, the maximum optical gain for nano-heterostructure consisting of single quantum well (SQW) of InGaAlAs material with barriers of InGaAlAs and claddings of InAlAs is found at 1.55 \u00b5m wavelength; while for the SQW of the same material with barriers of AlGaAs and claddings of AlGaAs is found at 0.84 \u00b5m. For both types of heterostructures, the maximum gain corresponding to lasing wavelengths have been plotted on logarithmic scale and discussed. In order to support the obtained optical gain, the anti-guiding factors for both the structures have also been discussed.", "num_citations": "5\n", "authors": ["1575"]}
{"title": "Dynamic Spatial Positioning: Physical Collaboration around Interactive Table by Children in India\n", "abstract": " We present a study of how children demonstrate physicality during collaboration around interactive tables at school. Our results show that children tend to dynamically position themselves around the tabletop area to effect particular social outcomes. These movements around the tabletop allow them to enact coordination strategies in their social interactions with each other to manage their learning and task-based activities. Our analysis indicates the importance of understanding physical strategies and behaviours when designing and deploying interactive tables in classrooms. We discuss how the design of tabletops in school can embrace the extensibility of this technology, providing access for children to shape their own collaboration strategies during learning.", "num_citations": "5\n", "authors": ["1575"]}
{"title": "Scalable online comparative genomics of mononucleosomes: a BigJob\n", "abstract": " Our goal is to develop workflows for simulating arbitrary collections of mononucleosomes in atomic detail as an on demand analysis tool for online comparative genomics. The limiting factor is resource availability. The aim of this paper is to document and share our experiences in providing a general-purpose, easy-to-use and extensible solution for such computations. At the core it involves supporting the execution of high-throughput workloads of high-performance biomolecular simulations on one or more XSEDE machines. Although conceptually simple, it is still a difficult practical problem to solve, especially in a flexible, robust, scalable manner. Specifically, we employ BigJob--an interoperable Pilot-Job. The bulk of this paper is about our experience in executing a very large number of ensembles including the associated non-trivial data management problem. Our experience suggests that although a nascent and\u00a0\u2026", "num_citations": "5\n", "authors": ["1575"]}
{"title": "Advancing hydrometeorological prediction capabilities through standards-based cyberinfrastructure development: The community WRF-Hydro modeling system\n", "abstract": " The need for improved assessments and predictions of many key environmental variables is driving a multitude of model development efforts in the geosciences. The proliferation of weather and climate impacts research is driving a host of new environmental prediction model development efforts as society seeks to understand how climate does and will impact key societal activities and resources and, in turn, how human activities influence climate and the environment. This surge in model development has highlighted the role of model coupling as a fundamental activity itself and, at times, a significant bottleneck in weather and climate impacts research. This talk explores some of the recent activities and progress that has been made in assessing the attributes of various approaches to the coupling of physics-based process models for hydrometeorology. One example modeling system that is emerging from these\u00a0\u2026", "num_citations": "5\n", "authors": ["1575"]}
{"title": "Using the TeraGrid to teach scientific computing\n", "abstract": " We describe how a new graduate course in scientific computing, taught during Fall 2010 at Louisiana State University, utilized TeraGrid resources to familiarize students with some of the real world issues that computational scientists regularly deal with in their work. The course was designed to provide a broad and practical introduction to scientific computing, creating the basic skills and experience to very quickly get involved in research projects involving modern cyberinfrastructure and complex real world scientific problems. As an integral part of the course, students had to utilize various TeraGrid resources, eg, by deploying, using and extending scientific software within the national cyberinfrastructure.", "num_citations": "5\n", "authors": ["1575"]}
{"title": "Challenges in Safety Assessment of Complex Critical Infrastructures\n", "abstract": " This Position Paper addresses some problems and challenges in safety assessment of current Critical Infrastructures introduced by system complexity and by the current 'safety process framework' adopted in most high risk domains.", "num_citations": "5\n", "authors": ["1575"]}
{"title": "Euro-Par 2008 Workshops-Parallel Processing: VHPC 2008, UNICORE 2008, HPPC 2008, SGS 2008, PROPER 2008, ROIA 2008, and DPA 2008, Las Palmas de Gran Canaria, Spain, August 25-26\u00a0\u2026\n", "abstract": " Parallel and distributed processing, although within the focus of computer science research for a long time, is gaining more and more importance in a wide spectrum of applications. These proceedings aim to demonstrate the use of parallel and distributed processing concepts in different application fields, and attempt to spark interest in novel research directions to parallel and high-performance computing research in general. The objective of these workshops is to specifically address researchers coming from university, industry and governmental research organizations and application-oriented companies in order to close the gap between purely scientific research and the applicab-ity of the research ideas to real-life problems. Euro-Par is an annual series of international conferences dedicated to the pro-tion and advancement of all aspects of parallel and distributed computing. The 2008 event was the 14th issue of the conference. Euro-Par has for a long time been eager to attract colocated events sharing the same goal of promoting the dev-opment of parallel and distributed computing, both as an industrial technique and an academic discipline, extending the frontier of both the state of the art and the state of the practice. Since 2006, Euro-Par has been offering researchers the chance to co-cate advanced technical workshops back-to-back with the main conference.", "num_citations": "5\n", "authors": ["1575"]}
{"title": "Towards automated security mediation placement\n", "abstract": " We present a framework that automatically produces resolution placement suggestions for type errors in security-typed programs, enabling legacy code to be retrofit with comprehensive security policy mediation. Resolving such type errors requires selecting a placement of mediation statements that implement runtime security decisions, such as declassifiers and authorization checks. Manually placing mediation statements in legacy code can be difficult, as there may be several, interacting type errors. In this paper, we solve this problem by constructing a graph that has the property that a vertex cut is equivalent to the points at which mediation statements can be inserted to allow the program to satisfy the type system. We build a framework that produces suggestions that are minimum cuts of this graph, and the framework can be customized to find suggestions that satisfy programmer requirements. Our framework implementation for Java programs computes suggestions for 20,000 line programs in less than 100 seconds, reduces the number of locations a programmer must consider by 90%, and selects suggestions similar to those proposed by expert programmers 80% of the time. 1", "num_citations": "5\n", "authors": ["1575"]}
{"title": "Interval Universal Approximation for Neural Networks\n", "abstract": " To certify safety and robustness of neural networks, researchers have successfully applied abstract interpretation, primarily using interval bound propagation (IBP). IBP is an incomplete calculus that over-approximates the set of possible predictions of a neural network. In this paper, we introduce the interval universal approximation (IUA) theorem, which sheds light on the power and limits of IBP. First, IUA shows that neural networks not only can approximate any continuous function  (universal approximation) as we have known for decades, but we can find a neural network, using any well-behaved activation function, whose interval bounds are an arbitrary close approximation of the set semantics of  (the result of applying  to a set of inputs). We call this notion of approximation interval approximation. Our result (1) extends the recent result of Baader et al. (2020) from ReLUs to a rich class of activation functions that we call squashable functions, and (2) implies that we can construct certifiably robust neural networks under -norm using almost any practical activation function. Our construction and that of Baader et al. (2020) are exponential in the size of the function's domain. The IUA theorem additionally establishes a limit on the capabilities of IBP. Specifically, we show that there is no efficient construction of a neural network that interval-approximates any , unless P=NP. To do so, we present a novel reduction from 3SAT to interval-approximation of neural networks. It implies that it is hard to construct an IBP-certifiably robust network, even if we have a robust network to start with.", "num_citations": "4\n", "authors": ["1575"]}
{"title": "Robust out-of-distribution detection via informative outlier mining\n", "abstract": " Detecting out-of-distribution (OOD) inputs is critical for safely deploying deep learning models in an open-world setting. However, existing OOD detection solutions can be brittle under small adversarial perturbations. In this paper, we propose a simple and effective method, Adversarial Training with informative Outlier Mining (ATOM), to robustify OOD detection. Our key observation is that while unlabeled data can be used as auxiliary OOD training data, the majority of these data points are not informative to improve the decision boundary of the OOD detector. We show that, by carefully choosing which outliers to train on, one can significantly improve the robustness of the OOD detector, and somewhat surprisingly, generalize to some adversarial attacks not seen during training. We provide additionally a unified evaluation framework that allows future research examining the robustness of OOD detection algorithms. ATOM achieves state-of-the-art performance under a broad family of natural and perturbed OOD evaluation tasks, surpassing previous methods by a large margin. Finally, we provide theoretical insights for the benefit of outlier mining.", "num_citations": "4\n", "authors": ["1575"]}
{"title": "Privacy-preserving ridge regression over distributed data from lhe\n", "abstract": " Linear regression with 2-norm regularization (ie, ridge regression) is an important statistical technique that models the relationship between some explanatory values and an outcome value using a linear function. In many current applications (eg, predictive modelling in personalized health-care), these values represent sensitive data owned by several different parties who are unwilling to share them. In this setting, training a linear regression model becomes challenging and needs specific cryptographic solutions. This problem was elegantly addressed by Nikolaenko et al. in S&P (Oakland) 2013. They suggested a two-server system that uses linearly-homomorphic encryption (LHE) and Yao\u2019s two-party protocol (garbled circuits). In this work, we propose a novel system that can train a ridge linear regression model using only linearly-homomorphic encryption (ie, without using Yao\u2019s protocol). This greatly improves the overall performance (both in computation and communications) as Yao\u2019s protocol was the main bottleneck in the previous solution. The efficiency of the proposed system is validated both on synthetically-generated and real-world datasets.", "num_citations": "4\n", "authors": ["1575"]}
{"title": "Optimizing locally differentially private protocols\n", "abstract": " Protocols satisfying Local Differential Privacy (LDP) enable parties to collect aggregate information about a population while protecting each user's privacy, without relying on a trusted third party. LDP protocols (such as Google's RAPPOR) have been deployed in real-world scenarios. In these protocols, a user encodes his private information and perturbs the encoded value locally before sending it to an aggregator, who combines values that users contribute to infer statistics about the population. In this paper, we introduce a framework that generalizes several LDP protocols proposed in the literature. Our framework yields a simple and fast aggregation algorithm, whose accuracy can be precisely analyzed. Our in-depth analysis enables us to choose optimal parameters, resulting in two new protocols (i.e., Optimized Unary Encoding and Optimized Local Hashing) that provide better utility than protocols previously proposed. We present precise conditions for when each proposed protocol should be used, and perform experiments that demonstrate the advantage of our proposed protocols.", "num_citations": "4\n", "authors": ["1575"]}
{"title": "On the complexities of utilizing large\u2010scale lightpath\u2010connected distributed cyberinfrastructure\n", "abstract": " In Autumn 2013, we\u2014an international team of climate scientists, computer scientists, eScience researchers, and e\u2010Infrastructure specialists\u2014participated in the enlighten your research global competition, organized to showcase advanced lightpath technologies in support of state\u2010of\u2010the\u2010art research questions. As one of the winning entries, our enlighten your research global team embarked on a very ambitious project to run an extremely high resolution climate model on a collection of supercomputers distributed over two continents and connected using an advanced 10\u00a0G lightpath networking infrastructure. Although good progress was made, we were not able to perform all desired experiments due to a varying combination of technical problems, configuration issues, policy limitations and lack of (budget for) human resources to solve these issues. In this paper, we describe our goals, the technical and non\u00a0\u2026", "num_citations": "4\n", "authors": ["1575"]}
{"title": "Application skeleton: generating synthetic applications for infrastructure research\n", "abstract": " Application Skeleton is a simple and powerful tool to build simplified synthetic science and engineering applications (for example, modeling and simulation, data analysis) with runtime and I/O close to that of the real applications. It is intended for applied computer scientists who need to use science and engineering applications to verify the effectiveness of new systems designed to efficiently run such applications, so that they can bypass obstacles that they often encounter when accessing and building real science and engineering applications. Using the applications generated by Application Skeleton guarantees that the CS systems\u2019 effectiveness on synthetic applications will apply to the real applications.Application Skeleton can generate bag-of-task,(iterative) map-reduce, and (iterative) multistage workflow applications. These applications are represented as a set of tasks, a set of input files, and a set of dependencies. These applications can be generally considered many-task applications, and once created, can be run on single-core, single-node, multi-core, or multi-node (distributed or parallel) computers, depending on what workflow system is used to run them. The generated applications are compatible with workflow system such as Swift (Zhao et al. 2007, Wilde et al.(2009), Wilde et al.(2011)) and Pegasus (Ewa Deelman et al. 2004, E. Deelman et al.(2005)), as well as the ubiquitous UNIX shell. The application can also be created as a generic JSON object that can be used by other systems such as the AIMES (Turilli et al. 2015) middleware.", "num_citations": "4\n", "authors": ["1575"]}
{"title": "Standards based integration of advanced key management capabilities with openstack\n", "abstract": " The majority of the IT world is currently shifting to cloud platforms, and this calls for a greater measure of security for clouds. The emphasis for security originates from the need to transfer data across or within clouds without a third party being able to access the transferred information. This data is encrypted and decrypted using keys and is managed by Key Lifecycle Managers (KLMs) which communicate using a standardized protocol. Thus, to provide end user flexibility which helps in choosing the most reliable KLM, it is essential for cloud platforms to provide support for a standardized protocol so as to allow integration with external Key Lifecycle Managers.", "num_citations": "4\n", "authors": ["1575"]}
{"title": "Stream 2015 final report\n", "abstract": " \u200b 1\u200b Executive Summary\u200b 2\u200b Introduction\u200b 3\u200b State of Art\u200b 3.1\u200b Exemplar applications: Characteristics of Applications, IndustryScience differences.\u200b 3.1. 1\u200b Application Categories and Exemplars\u200b 3.1. 2\u200b Application Characteristics\u200b 3.2\u200b Current solutions Industry, Apache, DomainSpecific\u200b 3.2. 1\u200b Particular Solutions\u200b 3.2. 2\u200b Technology Challenges and Features\u200b 3.3\u200b Connections Streaming+ HPC convergence. Role of workflow.\u200b 4\u200b Next Steps and Research Directions\u200b 4.1\u200b New Algorithms\u200b 4.2\u200b Programming and Runtime Model\u037e Languages\u200b 4.3\u200b Benchmarks and Application Collections and Scenarios\u200b 4.4\u200b Streaming Software System and Algorithm Library\u200b 4.5\u200b Streaming System Infrastructure and its Characteristics\u200b 4.6\u200b Steering and Human in the Loop\u200b 5\u200b Build and sustain community\u200b 5.1\u200b Community Activities\u200b 5.2\u200b Education and Training\u200b 6\u200b Summary\u200b 7\u200b Acknowledgements\u200b 8\u200b Appendices\u200b 8.1\u200b Participants\u200b 8.2\u200b Workshop Presentations\u200b 8.3\u200b Workshop White Papers\u200b 8.4\u200b Citations", "num_citations": "4\n", "authors": ["1575"]}
{"title": "Temporal RBAC security analysis using logic programming in the presence of administrative policies\n", "abstract": " Temporal Role Based Access Control (TRBAC) is an extension of the role based access control (RBAC) model in the temporal domain. It is used by organizations needing to enforce temporal constraints on enabling and disabling of roles. For any chosen access control model, decentralization of administrative authority necessitates the use of a separate administrative model. Even with the use of an administrative model, decentralization often leads to an increased concern for security. Analysis of security properties of RBAC has been extensively done using its administrative model (ARBAC97). However, TRBAC security analysis in the presence of an administrative model so far has received limited attention. This paper proposes a method for performing formal security analysis of TRBAC considering a recently proposed administrative model named AMTRAC, which includes all the relations of ARBAC97\u00a0\u2026", "num_citations": "4\n", "authors": ["1575"]}
{"title": "BbmTTP: Beat-based parallel simulated annealing algorithm on GPGPUs for the mirrored traveling tournament problem\n", "abstract": " The problem of scheduling sports leagues has received considerable attention in recent years, especially since mathematically optimized schedules often have a large impact both economically and environmentally. The Mirrored Traveling Tournament Problem (mTTP) is an optimization problem that represents certain types of sports scheduling where the main objective is to minimize the total distance traveled by all the participating teams. In this paper, we propose a GPU based parallel simulated annealing algorithm for mTTP and test the available instances using NVIDIA's Compute Unified Device Architecture (CUDA). The approach taken here, especially keeping in mind the computationally intensive nature of mTTP, involves exploiting the thread level parallelism available in CUDA-where each thread starts with a random schedule from the discrete solution space and cooperate at regular intervals-called'Beats'\u00a0\u2026", "num_citations": "4\n", "authors": ["1575"]}
{"title": "Numerical methodologies for investigation of moderate-velocity flow using a hybrid computational fluid dynamics\u2014molecular dynamics simulation approach\n", "abstract": " Numerical approaches are presented to minimize the statistical errors inherently present due to finite sampling and the presence of thermal fluctuations in the molecular region of a hybrid computational fluid dynamics (CFD) \u2014 molecular dynamics (MD) flow solution. Near the fluid-solid interface the hybrid CFD-MD simulation approach provides a more accurate solution, especially in the presence of significant molecular-level phenomena, than the traditional continuum-based simulation techniques. It also involves less computational cost than the pure particle-based MD. Despite these advantages the hybrid CFD-MD methodology has been applied mostly in flow studies at high velocities, mainly because of the higher statistical errors associated with low velocities. As an alternative to the costly increase of the size of the MD region to decrease statistical errors, we investigate a few numerical approaches that\u00a0\u2026", "num_citations": "4\n", "authors": ["1575"]}
{"title": "Pilot abstractions for compute, data, and network\n", "abstract": " Scientific experiments in a variety of domains are producing increasing amounts of data that need to be processed efficiently. Distributed Computing Infrastructures are increasingly important in fulfilling these large-scale computational requirements.", "num_citations": "4\n", "authors": ["1575"]}
{"title": "Implementation of GIS spatial data mining based on cloud theory\n", "abstract": " The transforms between qualitative concepts and their quantitative expressions plan an important role in spatial data mining & knowledge discovery (SDMKD), the cloud theory is this kind of powerful tools. Based on the cloud model, this paper presents a expression method for uncertain direction by using two-dimensional normal cloud, builds a data mining pattern through combining cloud theory and rough sets, proposes a GIS spatial data mining structure system. In this foundation, the process of data mining for GIS spatial data by generalizing attributes based on cloud theory and reducing attributes based on rough sets is given. Finally an example is explained and confirmed this method validity", "num_citations": "4\n", "authors": ["1575"]}
{"title": "Red pandas and conservation: political ecology, tenure, livestock, and hunting in high altitude forests of Nepal\n", "abstract": " Red pandas and conservation : political ecology, tenure, livestock, and hunting in high altitude forests of Nepal Toggle navigation Repositorio Institucional de la UIA Le\u00f3n English espa\u00f1ol English English espa\u00f1ol Login Toggle navigation View Item DSpace Home REPOSITORIOS DE UNIVERSIDADES NACIONALES E INTERNACIONALES Internacionales University of Wisconsin, Madison View Item DSpace Home REPOSITORIOS DE UNIVERSIDADES NACIONALES E INTERNACIONALES Internacionales University of Wisconsin, Madison View Item Red pandas and conservation : political ecology, tenure, livestock, and hunting in high altitude forests of Nepal Thumbnail Date 2012-06-11 Author Steffens, Erik Metadata Show full item record URI https://repositorio.leon.uia.mx/xmlui/123456789/45259 Collections University of Wisconsin, Madison DSpace software copyright \u00a9 2002-2016 DuraSpace Contact Us | Send \u2026", "num_citations": "4\n", "authors": ["1575"]}
{"title": "The international dimension of farmland protection: lessons for developing countries from developed countries\n", "abstract": " Urbanization is one of the most significant land use phenomena in the world, if not the most significant. And it is has been so throughout the century. Before mid-century a rapid period of urbanization was most pronounced in the developed countries; this process has continued. Since mid-century rapid urbanization, and its consequences, have been visited upon countries in the developing world. Today, as we come to the end of the 20th century countries on every continent are grappling with significant rural to urban migration, population growth within their urban areas, and thus the physical spread of their major urban area, and most if not all of their minor ones (Clark 1998, Farvacque and McAuslan 1992). As urbanization has occurred agricultural lands in the peri-urban zone have been significantly affected. In most parts of the developing world, agricultural lands have traditionally been close to cities. This land use pattern allows producers easy access to markets, especially where transportation systems may be unreliable and expensive, the products may be perishable, and producers may be small-scale and not have the capital or facilities for storage of agricultural goods. From the urban side, this land use relationship benefits urban residents by providing them with some or all of the food they need for daily life. Urbanization has, in most cases, caused the spread of the city onto adjoining agricultural lands, often in a pattern of low-density residential development. In some parts of the world (mostly developed countries), this spread has been the site of middle and upper class housing; in other parts of the world (mostly developing countries), this\u00a0\u2026", "num_citations": "4\n", "authors": ["1575"]}
{"title": "Pandemic drugs at pandemic speed: infrastructure for accelerating COVID-19 drug discovery with hybrid machine learning-and physics-based simulations on high-performance computers\n", "abstract": " The race to meet the challenges of the global pandemic has served as a reminder that the existing drug discovery process is expensive, inefficient and slow. There is a major bottleneck screening the vast number of potential small molecules to shortlist lead compounds for antiviral drug development. New opportunities to accelerate drug discovery lie at the interface between machine learning methods, in this case, developed for linear accelerators, and physics-based methods. The two in silico methods, each have their own advantages and limitations which, interestingly, complement each other. Here, we present an innovative infrastructural development that combines both approaches to accelerate drug discovery. The scale of the potential resulting workflow is such that it is dependent on supercomputing to achieve extremely high throughput. We have demonstrated the viability of this workflow for the study of\u00a0\u2026", "num_citations": "3\n", "authors": ["1575"]}
{"title": "Parallel performance of molecular dynamics trajectory analysis\n", "abstract": " The performance of biomolecular molecular dynamics simulations has steadily increased on modern high\u2010performance computing resources but acceleration of the analysis of the output trajectories has lagged behind so that analyzing simulations is becoming a bottleneck. To close this gap, we studied the performance of trajectory analysis with message passing interface (MPI) parallelization and the Python MDAnalysis library on three different Extreme Science and Engineering Discovery Environment (XSEDE) supercomputers where trajectories were read from a Lustre parallel file system. Strong scaling performance was impeded by stragglers, MPI processes that were slower than the typical process. Stragglers were less prevalent for compute\u2010bound workloads, thus pointing to file reading as a bottleneck for scaling. However, a more complicated picture emerged in which both the computation and the data\u00a0\u2026", "num_citations": "3\n", "authors": ["1575"]}
{"title": "Informative outlier matters: Robustifying out-of-distribution detection using outlier mining\n", "abstract": " Detecting out-of-distribution (OOD) inputs is critical for safely deploying deep learning models in an open-world setting. However, existing OOD detection solutions can be brittle in the open world, facing various types of adversarial OOD inputs. While methods leveraging auxiliary OOD data have emerged, our analysis reveals a key insight that the majority of auxiliary OOD examples may not meaningfully improve the decision boundary of the OOD detector. In this paper, we provide a theoretically motivated method, Adversarial Training with informative Outlier Mining (ATOM), which improves the robustness of OOD detection. We show that, by mining informative auxiliary OOD data, one can significantly improve OOD detection performance, and somewhat surprisingly, generalize to unseen adversarial attacks. ATOM achieves state-of-the-art performance under a broad family of natural and perturbed OOD evaluation tasks. For example, on the CIFAR-10 in-distribution dataset, ATOM reduces the FPR95 by up to 57.99% under adversarial OOD inputs, surpassing the previous best baseline by a large margin.", "num_citations": "3\n", "authors": ["1575"]}
{"title": "On the Need for Topology-Aware Generative Models for Manifold-Based Defenses\n", "abstract": " Machine-learning (ML) algorithms or models, especially deep neural networks (DNNs), have shown significant promise in several areas. However, researchers have recently demonstrated that ML algorithms, especially DNNs, are vulnerable to adversarial examples (slightly perturbed samples that cause misclassification). The existence of adversarial examples has hindered the deployment of ML algorithms in safety-critical sectors, such as security. Several defenses for adversarial examples exist in the literature. One of the important classes of defenses are manifold-based defenses, where a sample is ``pulled back\" into the data manifold before classifying. These defenses rely on the assumption that data lie in a manifold of a lower dimension than the input space. These defenses use a generative model to approximate the input distribution. In this paper, we investigate the following question: do the generative models used in manifold-based defenses need to be topology-aware? We suggest the answer is yes, and we provide theoretical and empirical evidence to support our claim.", "num_citations": "3\n", "authors": ["1575"]}
{"title": "Towards a Bayesian Approach for Assessing Fault Tolerance of Deep Neural Networks\n", "abstract": " This paper presents Bayesian Deep Learning based Fault Injection (BDLFI), a novel methodology for fault injection in neural networks (NNs) and more generally differentiable programs. BDLFI uses (1) Bayesian Deep Learning to model the propagation of faults, and (2) Markov Chain Monte Carlo inference to quantify the effect of faults on the outputs of a NN. We demonstrate BDLFI on two representative networks and present our results that challenge pre-existing results in the field.", "num_citations": "3\n", "authors": ["1575"]}
{"title": "Rearchitecting Classification Frameworks For Increased Robustness\n", "abstract": " While generalizing well over natural inputs, neural networks are vulnerable to adversarial inputs. Existing defenses against adversarial inputs have largely been detached from the real world. These defenses also come at a cost to accuracy. Fortunately, there are invariances of an object that are its salient features; when we break them it will necessarily change the perception of the object. We find that applying invariants to the classification task makes robustness and accuracy feasible together. Two questions follow: how to extract and model these invariances? and how to design a classification paradigm that leverages these invariances to improve the robustness accuracy trade-off? The remainder of the paper discusses solutions to the aformenetioned questions.", "num_citations": "3\n", "authors": ["1575"]}
{"title": "MURS: practical and robust privacy amplification with multi-party differential privacy\n", "abstract": " Background System Analysis of MPDP SLH MURS Evaluation Results References Page 1 MURS: Practical and Robust Privacy Amplification with Multi-Party Differential Privacy Tianhao Wang, Min Xu, Bolin Ding, Jingren Zhou, Cheng Hong, Zhicong Huang, Ninghui Li, Somesh Jha Purdue University, University of Chicago, Alibaba Group, University of Wisconsin-Madison Background \u2660 Multi-Party Differential Privacy: Better trust than DP and better utility than LDP Data Data Data Data Data Trust boundary Noisy Data Noisy Data Noisy Data Figure 1: Multi-Party Differential Privacy System Model \u2660 Existing work either provides poor utility [5, 3, 2, 6] or high communication overhead [1, 4]. \u2660 The system model in Figure 1 is weak. System Analysis of MPDP \u2660 The server colluding with all other users. \u2660 The server with t \u2265 1 auxiliary servers. \u2660 The auxiliary servers may poison the result. SLH \u2660 Utilizing the Local Hashing \u2026", "num_citations": "3\n", "authors": ["1575"]}
{"title": "Objective driven computational experiment design: An ExaLearn perspective\n", "abstract": " A fundamental problem that currently pervades diverse areas of science and engineering is the need to design expensive computational campaigns (experiments) that are robust in the presence of substantial uncertainty. A particular interest lies in effectively achieving specific objectives for systems that cannot be completely identified. For example, there may be \u201cbig data\u201d but the data size may still pale in comparison with the complexity of the system, or the available data may be scarce due to the prohibitive cost of the relevant experiments.In current practice, the methodologies by which experiments inform theory, and theory guides experiments, remain ad hoc, particularly when the physical systems under study are multiscale, large-scale, and complex. Off-the-shelf machine learning methods are not the answer\u2014these methods have been successful in problems for which massive amounts of data are available and for which a predictive capability does not rely upon the constraints of physical laws. The need to address this fundamental problem has become urgent, as computational campaigns at pre-exascale, and soon exascale, will entail models that span wider ranges of scales, represent richer interacting physics, and inform decisions of greater societal consequence.", "num_citations": "3\n", "authors": ["1575"]}
{"title": "Explainable black-box attacks against model-based authentication\n", "abstract": " Establishing unique identities for both humans and end systems has been an active research problem in the security community, giving rise to innovative machine learning-based authentication techniques. Although such techniques offer an automated method to establish identity, they have not been vetted against sophisticated attacks that target their core machine learning technique. This paper demonstrates that mimicking the unique signatures generated by host fingerprinting and biometric authentication systems is possible. We expose the ineffectiveness of underlying machine learning classification models by constructing a blind attack based around the query synthesis framework and utilizing Explainable-AI (XAI) techniques. We launch an attack in under 130 queries on a state-of-the-art face authentication system, and under 100 queries on a host authentication system. We examine how these attacks can be defended against and explore their limitations. XAI provides an effective means for adversaries to infer decision boundaries and provides a new way forward in constructing attacks against systems using machine learning models for authentication.", "num_citations": "3\n", "authors": ["1575"]}
{"title": "Gateways to high-perfomance and distributed computing resources for global health challenges\n", "abstract": " Computational simulations for disease modeling and efficient analysis tools using large data collections have become invaluable tools for Global Health programs fighting infectious diseases. Simulations are used in many ways e.g., from predicting the effectiveness of interventions for certain diseases through Bayesian-based data-model assimilation to genomic analysis of diverse vector species in their different growth states. Even though the approaches and technologies vary, they have several common requirements on the underlying infrastructure. Simulations for infectious diseases, for example, rely on environmental data like weather, geospatial data, biodiversity and transmission complexity. Data-intensive applications need efficient distributed data management capabilities facilitating replication services or Software-as-a-Service solutions. Such solutions might follow the paradigm to transfer applications to\u00a0\u2026", "num_citations": "3\n", "authors": ["1575"]}
{"title": "Design of an intelligent security robot for collision free navigation applications\n", "abstract": " Robotics industry has replaced human efforts gradually in performing rather difficult tasks. A very pertinent aspect of an intelligent security robot is to reach the goal safely by avoiding unknown obstacles in an unknown environment. In this paper we have developed an embedded C program code to design an intelligent robot which can overcome the obstacles coming in its way. We have made use of three infrared sensors to detect the obstacles via the IR communication technique. The IR transmitter sends out infrared radiation in a direction which consequently bounces back on coming across the surface of an object and thereafter is picked up by the IR receiver. Authors have applied a multi sensor integration technique to sense the obstacles using an LED based IR transmitter and receiver module integrated with the 8051 micro controller which permits collision free navigation of robots.", "num_citations": "3\n", "authors": ["1575"]}
{"title": "A Cube-Aware Compaction Method for Scan ATPG\n", "abstract": " We propose a test-cube aware dynamic compaction method for reducing the test set size generated by scan ATPG. In the initial phase of ATPG, when the generated test cubes are significantly compatible, we start with a local cube merging approach. Then, we switch to a compacted cube-generation approach, when the compatibility of test cubes decrease. We propose efficient heuristics for merging test cubes and writing them out during the cube-merging phase. We introduce a novel reasoning analysis technique to learn cube-independent untestable faults and, avoid targeting them repeatedly during the compacted-cube generation phase. On latest Intel microprocessor designs, we are able to achieve up to 2.3X compaction with 20% run-time over-head, on top of on-chip hardware compression.", "num_citations": "3\n", "authors": ["1575"]}
{"title": "Compaction mechanism to reduce test pattern counts and segmented delay fault testing for path delay faults\n", "abstract": " With rapid advancement in science and technology and decreasing feature size of transistors, the complexity of VLSI designs is constantly increasing. With increasing density and complexity of the designs, the probability of occurrence of defects also increases. Therefore testing of designs becomes essential in order to guarantee fault-free operation of devices.", "num_citations": "3\n", "authors": ["1575"]}
{"title": "Flexible resource allocation for multicast in OFDMA based wireless networks\n", "abstract": " This paper studies an efficient resource allocation scheme for multicast in OFDMA based wireless networks. Apart from the conventional resource allocation schemes for multicast which allocate exactly the same subcarriers to the users in a multicast group, this paper proposes a more flexible scheme to divide the multicast group members into different subgroups by utilising the diversity of channel coefficient of different users. We first formulate an optimisation problem to maximise the overall transmission rate. Given the NP-hardness of the problem, we design a low-complexity heuristic, Flexible Resource Allocation with Geometric programming (FRAG). FRAG is a two-step heuristic to subdivide the multicast groups and allocate resource to corresponding subgroups. In the first step, we propose a greedy algorithm to subdivide groups and allocate subcarriers given the assumption of even power distribution. Then we\u00a0\u2026", "num_citations": "3\n", "authors": ["1575"]}
{"title": "Reliable replica exchange molecular dynamics simulation in the Grid using SAGA CPR and Migol\n", "abstract": " Reliable Replica-Exchange Molecular Dynamics Simulation in the Grid using SAGA CPR and Migol Page 1 Reliable Replica-Exchange Molecular Dynamics Simulation in the Grid using SAGA CPR and Migol Andre Luckow,1 Shantenu Jha,2,3 Andre Merzky,2 Joohyun Kim2 and Bettina Schnor1 1Institute of Computer Science, University of Potsdam, Germany 2Center for Computation & Technology, Louisiana State University, USA 3Department of Computer Science, Louisiana State University, USA 1 / 13 Page 2 A distributed system is a system on which I cannot get any work done, because some machine I have never heard of has crashed. (Leslie Lamport) 2 / 13 Page 3 Outline 1 Introduction and Challenges 2 Middleware and Abstractions 3 Replica-Exchange Framework 4 Efficient Job Scheduling \u2013 SAGA Glide-In 5 Conclusion 3 / 13 Page 4 Replica-Exchange Simulations Replica-Exchange (RE) simulations are \u2026", "num_citations": "3\n", "authors": ["1575"]}
{"title": "Efficient type matching\n", "abstract": " Palsberg and Zhao [25] presented an O(n               2) time algorithm for matching two recursive types; that is, deciding type isomorphism with associative-commutative product type constructors. In this paper, we present an O(n log n)-time algorithm for matching recursive types and an O(n)-time algorithm for matching nonrecursive types. The linear-time algorithm for nonrecursive types works without hashing or pointer arithmetic, by employing multiset discrimination techniques due to Paige et al. [9,10,21\u201324]. The O(n log n) algorithm for recursive types works by reducing the type matching problem to the problem of finding a size-stable partition of a graph, which has O(n log n) algorithms due to Cardon/Crochemore and Paige/Tarjan. The key to these algorithms is the use of a \u201cmodify-the-smaller-half\u201d approach pioneered by Hopcroft and Ullman for DFA minimization.             Our results may help improve systems\u00a0\u2026", "num_citations": "3\n", "authors": ["1575"]}
{"title": "On Automatic Placement of Declassifiers for Information-Flow Security\n", "abstract": " Security-typed languages can be used to build programs that are information-flow secure, meaning that they do not allow secret data to leak. Declassification allows programs to leak secret information in carefully prescribed ways. Manually placing declassifiers to authorize certain flows of information can be dangerous because an incorrectly placed declassifier can leak far more secure data than intended. Additionally, the sheer number of runtime flows that can cause an error means that determining where to place declassifiers can be difficult. We present a new approach for constructing information-flow secure programs where declassifiers are placed such that no unintended leakage occurs. Leakage restrictions are specified using hard constraints and potential declassifier locations are ranked using soft constraints. Finally, the placement problem is submitted to a pseudo-Boolean optimizing SAT solver that selects a minimal set of declassifiers that prevent unauthorized data leakage. These declassifiers can be reviewed by the programmer to ensure that they correspond with acceptable declassification points: if not, new hard constraints can be added and the optimization framework can be reinvoked. Our experimental results indicate that our analysis suggests declassifiers that will cause no more leakage than those placed by programmers in a fraction of the time it would take to perform a manual analysis. This work provides a foundation for less expert programmers to build information-flow secure programs and to convert existing programs to be information-flow secure.", "num_citations": "3\n", "authors": ["1575"]}
{"title": "Towards automated authorization policy enforcement\n", "abstract": " In systems with shared resources, authorization policy enforcement ensures that these resources are accessible only to users who are allowed to do so. Recently, there is growing interest to (i) extend authorization policy enforcement mechanisms provided by the operating system, and (ii) enable user-space servers to enforce authorization policies on their clients. A popular mechanism for authorization policy enforcement retrofits the code to be secured with hooks to a reference monitor. This is the basis for the Linux security modules (LSM) framework, and is also the intended usage of the recently-released security-enhanced Linux policy management framework for user-space servers. Unfortunately, reference monitor hooks are currently placed manually in operating system and user-space server code. This approach is tedious, does not scale, and as prior work has shown in the context of LSM, is error-prone. Our research is on techniques to largely automate authorization hook placement. We have devised a technique to do so, and have tested its effectiveness by applying it to determine hook placement for the Linux kernel, and cross-validating it with LSM hook placement. Our initial results are encouraging, and we have extended our technique to work with user-space servers. In particular, we have applied the technique to determine authorization hook placement for the X11 server.", "num_citations": "3\n", "authors": ["1575"]}
{"title": "SPICE: Simulated Pore Interactive Computing Environment-Using federated grids for \u201cgrand challenge\u201d biomolecular simulations\n", "abstract": " SPICE aims to understand the vital process of translocation of biomolecules across protein pores by computing the free energy profile of the translocating biomolecule along the vertical axis of the pore. Without significant advances at the algorithmic, computing and analysis levels, understanding problems of this size and complexity will remain beyond the scope of computational science for the foreseeable future. A novel algorithmic advance is provided by a combination of Steered Molecular Dynamics and Jarzynski\u2019s Equation (SMD-JE). Grid computing provides the required new computing paradigm as well as facilitating the adoption of new analytical approaches. SPICE uses sophisticated grid infrastructure to couple distributed high performance simulations, visualization and instruments used in the analysis to the same framework. This paper outlines the scientific motivation and describes why distributed resources are critical for the project. We describe how we utilize the resources of a federated trans-Atlantic Grid to use SMD-JE to enhance our understanding of the translocation phenomenon in ways that have not been possible until now. Finally, we briefly document the challenges encountered in using a grid-of-grids and some of the solutions devised in response.", "num_citations": "3\n", "authors": ["1575"]}
{"title": "The current state of the grid\n", "abstract": " Grid Computing: Making the Global Infrastructure a Reality, by Fran Berman, Geoffrey Fox, and Anthony J. G. Hey, John Wiley & Sons, 2003, ISBN 0470853190, US$105 The use of the term \"Grid computing\" covers a wide variety of efforts in distributed computing, high-performance computation, self-repairing networks, ubiquitous computing, or new techniques in high-availability computing. In recent years, the field has seen a burst of activity with many new projects and received increased attention in the technical and popular press.", "num_citations": "3\n", "authors": ["1575"]}
{"title": "A parallel quadtree algorithm for efficient assembly of stiffness matrices in meshfree Galerkin methods\n", "abstract": " For a collection of sets in Rd we consider the task of finding all sets in the collection that contain a given point. The parallel algorithm introduced in this paper is based on quadtrees and their generalizations to Rd. Our algorithm solves a difficult problem faced by meshfree discretizations, and is based on the sequential algorithm of Han, Oliveira, and Stewart [3].", "num_citations": "3\n", "authors": ["1575"]}
{"title": "Strategies for querying information agents\n", "abstract": " In a simple cooperative MAS model where a collection of \u201cquerying agents\u201d can send queries to a collection of \u201cinformation agents\u201d, we formalize the problem of designing strategies so that the expected completion time of the queries is minimized, when every querying agent uses the same strategy. We devise a provably optimal strategy for the static case with no query arrivals, and show via simulations that the same strategy performs well when queries arrive with a certain probability. We also consider issues such as whether or not the expected completion time can be reduced by sending multiple copies of queries, or by aborting copies of answered queries.", "num_citations": "3\n", "authors": ["1575"]}
{"title": "A general framework for detecting anomalous inputs to dnn classifiers\n", "abstract": " Detecting anomalous inputs, such as adversarial and out-of-distribution (OOD) inputs, is critical for classifiers (including deep neural networks or DNNs) deployed in real-world applications. While prior works have proposed various methods to detect such anomalous samples using information from the internal layer representations of a DNN, there is a lack of consensus on a principled approach for the different components of such a detection method. As a result, often heuristic and one-off methods are applied for different aspects of this problem. We propose an unsupervised anomaly detection framework based on the internal DNN layer representations in the form of a meta-algorithm with configurable components. We proceed to propose specific instantiations for each component of the meta-algorithm based on ideas grounded in statistical testing and anomaly detection. We evaluate the proposed methods on well-known image classification datasets with strong adversarial attacks and OOD inputs, including an adaptive attack that uses the internal layer representations of the DNN (often not considered in prior work). Comparisons with five recently-proposed competing detection methods demonstrates the effectiveness of our method in detecting adversarial and OOD inputs.", "num_citations": "2\n", "authors": ["1575"]}
{"title": "A Shuffling Framework for Local Differential Privacy\n", "abstract": " ldp deployments are vulnerable to inference attacks as an adversary can link the noisy responses to their identity and subsequently, auxiliary information using the order of the data. An alternative model, shuffle DP, prevents this by shuffling the noisy responses uniformly at random. However, this limits the data learnability -- only symmetric functions (input order agnostic) can be learned. In this paper, we strike a balance and propose a generalized shuffling framework that interpolates between the two deployment models. We show that systematic shuffling of the noisy responses can thwart specific inference attacks while retaining some meaningful data learnability. To this end, we propose a novel privacy guarantee, d-sigma privacy, that captures the privacy of the order of a data sequence. d-sigma privacy allows tuning the granularity at which the ordinal information is maintained, which formalizes the degree the resistance to inference attacks trading it off with data learnability. Additionally, we propose a novel shuffling mechanism that can achieve d-sigma privacy and demonstrate the practicality of our mechanism via evaluation on real-world datasets.", "num_citations": "2\n", "authors": ["1575"]}
{"title": "Is Private Learning Possible with Instance Encoding?\n", "abstract": " A private machine learning algorithm hides as much as possible about its training data while still preserving accuracy. In this work, we study whether a non-private learning algorithm can be made private by relying on an instance-encoding mechanism that modifies the training inputs before feeding them to a normal learner. We formalize both the notion of instance encoding and its privacy by providing two attack models. We first prove impossibility results for achieving a (stronger) model. Next, we demonstrate practical attacks in the second (weaker) attack model on InstaHide, a recent proposal by Huang, Song, Li and Arora [ICML\u201920] that aims to use instance encoding for privacy.", "num_citations": "2\n", "authors": ["1575"]}
{"title": "Representation Bayesian Risk Decompositions and Multi-Source Domain Adaptation\n", "abstract": " We consider representation learning (hypothesis class ) where training and test distributions can be different. Recent studies provide hints and failure examples for domain invariant representation learning, a common approach for this problem, but the explanations provided are somewhat different and do not provide a unified picture. In this paper, we provide new decompositions of risk which give finer-grained explanations and clarify potential generalization issues. For Single-Source Domain Adaptation, we give an exact decomposition (an equality) of the target risk, via a natural hybrid argument, as sum of three factors: (1) source risk, (2) representation conditional label divergence, and (3) representation covariate shift. We derive a similar decomposition for the Multi-Source case. These decompositions reveal factors (2) and (3) as the precise reasons for failure to generalize. For example, we demonstrate that domain adversarial neural networks (DANN) attempt to regularize for (3) but miss (2), while a recent technique Invariant Risk Minimization (IRM) attempts to account for (2) but does not consider (3). We also verify our observations experimentally.", "num_citations": "2\n", "authors": ["1575"]}
{"title": "Obliviousness Makes Poisoning Adversaries Weaker\n", "abstract": " Poisoning attacks have emerged as a significant security threat to machine learning (ML) algorithms. It has been demonstrated that adversaries who make small changes to the training set, such as adding specially crafted data points, can hurt the performance of the output model. Most of these attacks require the full knowledge of training data or the underlying data distribution. In this paper we study the power of oblivious adversaries who do not have any information about the training set. We show a separation between oblivious and full-information poisoning adversaries. Specifically, we construct a sparse linear regression problem for which LASSO estimator is robust against oblivious adversaries whose goal is to add a non-relevant features to the model with certain poisoning budget. On the other hand, non-oblivious adversaries, with the same budget, can craft poisoning examples based on the rest of the training data and successfully add non-relevant features to the model.", "num_citations": "2\n", "authors": ["1575"]}
{"title": "Analyzing accuracy loss in randomized smoothing defenses\n", "abstract": " Recent advances in machine learning (ML) algorithms, especially deep neural networks (DNNs), have demonstrated remarkable success (sometimes exceeding human-level performance) on several tasks, including face and speech recognition. However, ML algorithms are vulnerable to \\emph{adversarial attacks}, such test-time, training-time, and backdoor attacks. In test-time attacks an adversary crafts adversarial examples, which are specially crafted perturbations imperceptible to humans which, when added to an input example, force a machine learning model to misclassify the given input example. Adversarial examples are a concern when deploying ML algorithms in critical contexts, such as information security and autonomous driving. Researchers have responded with a plethora of defenses. One promising defense is \\emph{randomized smoothing} in which a classifier's prediction is smoothed by adding random noise to the input example we wish to classify. In this paper, we theoretically and empirically explore randomized smoothing. We investigate the effect of randomized smoothing on the feasible hypotheses space, and show that for some noise levels the set of hypotheses which are feasible shrinks due to smoothing, giving one reason why the natural accuracy drops after smoothing. To perform our analysis, we introduce a model for randomized smoothing which abstracts away specifics, such as the exact distribution of the noise. We complement our theoretical results with extensive experiments.", "num_citations": "2\n", "authors": ["1575"]}
{"title": "Incorporating Scientific Workflows in Computing Research Processes\n", "abstract": " The articles in this special section explore scientific workflows in computer research processes. The goal is to increase awareness of the benefits of workflows to enhance computational and data-enabled research and to foster the exchange of lessons learned and good practices that can benefit the community. The issue highlights some of the activities and approaches that are underway in the scientific workflow community. Scientific workflows evolved as a way to manage computation on High Performance Computing (HPC) and distributed systems. Early workflow efforts started as domain-specific efforts that managed directed acyclic graphs on high-performance and distributed systems. They considered the systems and the applications as black boxes and focused on distributed resource management, workload, and execution management.", "num_citations": "2\n", "authors": ["1575"]}
{"title": "Towards Exascale Computing for High Energy Physics: The ATLAS Experience at ORNL\n", "abstract": " Traditionally, the ATLAS experiment at Large Hadron Collider (LHC) has utilized distributed resources as provided by the Worldwide LHC Computing Grid (WLCG) to support data distribution, data analysis and simulations. For example, the ATLAS experiment uses a geographically distributed grid of approximately 200,000 cores continuously (250 000 cores at peak), (over 1,000 million core-hours per year) to process, simulate, and analyze its data (todays total data volume of ATLAS is more than 300 PB). After the early success in discovering a new particle consistent with the long-awaited Higgs boson, ATLAS is continuing the precision measurements necessary for further discoveries. Planned high-luminosity LHC upgrade and related ATLAS detector upgrades, that are necessary for physics searches beyond Standard Model, pose serious challenge for ATLAS computing. Data volumes are expected to increase at\u00a0\u2026", "num_citations": "2\n", "authors": ["1575"]}
{"title": "Enhancing Android security through app splitting\n", "abstract": " The Android operating system provides a rich security model that specifies over 100 distinct permissions. Before performing a sensitive operation, an app must obtain the corresponding permission through a request to the user. Unfortunately, an app is treated as an opaque, monolithic security principal, which is granted or denied permission as a whole. This blunts the effectiveness of the permissions model. Even the recent enhancements in Android do not account for the interactions between multiple permissions or for multiple uses of a single permission for disparate functionality.               We describe app splitting, a technique that partitions a monolithic Android app into a number of collaborating minion apps. This technique exposes information flows inside an application to OS-level mediation mechanisms to allow more expressive security and privacy policies. We implement app splitting in a tool called\u00a0\u2026", "num_citations": "2\n", "authors": ["1575"]}
{"title": "Niemann-pick disease type Ba case report\n", "abstract": " Introduction: Niemann\u2013Pick Disease (NPD) is an autosomal recessive disorder, characterized mainly by the accumulation of lipids, mostly sphingomyelin and cholesterol, in different organs such as liver, spleen, bone marrow, lungs and brain. The Sphingomyelins accumulate in lysosomes, which are responsible for carrying substances in and out of the cell. NPD is classified in 4 types: Types A, B, C and D. In type A and B, there is deficiency of acid sphingomyelinase which helps to break down Sphingomyelins, whereas Type C and D are caused by a defect in the transport of intracellular cholesterol. Thus, the 4 disease types can be grouped into 2 main categories: Type (I) which includes type A and B; and Type (II) which includes type C and D. Each type affects different organs. Symptoms depend upon severity of the disease and organs involved. The diagnosis is done by measurement of enzyme activity in peripheral white blood cells or in cultured fibroblasts. The pathologic hallmark in Niemann-Pick disease (NPD) types A and B is the characteristic lipid-laden foam cell termed as Niemann-Pick cells on bone marrow examination. Mutation analysis for detection of SMPD1 mutations is a complex procedure but can be done if facilities are available. There is no specific treatment for Niemann-Pick Disease. Type A is a severe form of disease, with average life span of 18 months. In all other types, treatment is aimed at controlling levels of cholesterol. Newer drugs like Miglustat which is able to delay neurodegeneration, \u03b2-cyclodextrin-hydroxypropyl (HBP-CD) which is claimed to attenuate clinical symptoms are being investigated in trials. Genetic\u00a0\u2026", "num_citations": "2\n", "authors": ["1575"]}
{"title": "Towards least privilege containers with cimplifier\n", "abstract": " Application containers, such as Docker containers, have recently gained popularity as a solution for agile and seamless deployment of applications. These light-weight virtualization environments run applications that are packed together with their resources and configuration information, and thus can be deployed across various software platforms. However, these software ecosystems are not conducive to the true and tried security principles of privilege separation (PS) and principle of least privilege (PLP). We propose algorithms and a tool Cimplifier, which address these concerns in the context of containers. Specifically, given a container our tool partitions them into simpler containers, which are only provided enough resources to perform their functionality. As part our solution, we develop techniques for analyzing resource usage, for performing partitioning, and gluing the containers together to preserve functionality. Our evaluation on real-world containers demonstrates that Cimplifier can preserve the original functionality, leads to reduction in image size of 58-95%, and processes even large containers in under thirty seconds.", "num_citations": "2\n", "authors": ["1575"]}
{"title": "Scalable hpc workflow infrastructure for steering scientific instruments and streaming applications\n", "abstract": " Linking scientific instruments to exascale machines and analyzing the large volumes of data produced by the instruments requires workflow infrastructure that scales along many dimensions. In this white paper we generalize this problem to include control systems, analysis of data produced by supercomputers, computational steering and data assimilation. The requirements of distributed computing problems which couple HPC and streaming data, are distinct from those familiar from largescale parallel simulations, grid computing, data repositories and orchestration, which have generated sophisticated software platforms. Our analyses points to new research directions for a scalable infrastructure, to address this generalized streaming distributed workflows problem class.", "num_citations": "2\n", "authors": ["1575"]}
{"title": "PanDA Beyond ATLAS: A Scalable Workload Management System For Data Intensive Science\n", "abstract": " The LHC experiments are today at the leading edge of large scale distributed data-intensive computational science. The LHC's ATLAS experiment processes data volumes which are particularly extreme, over 140 PB to date, distributed worldwide at over of 120 sites. An important element in the success of the exciting physics results from ATLAS is the highly scalable integrated workflow and dataflow management afforded by the PanDA workload management system, used for all the distributed computing needs of the experiment. The PanDA design is not experiment specific and PanDA is now being extended to support other data intensive scientific applications. PanDA was cited as an example of\" a high performance, fault tolerant software for fast, scalable access to data repositories of many kinds\" during the\" Big Data Research and Development Initiative\" announcement, a 200 million USD US government investment in tools to handle huge volumes of digital data needed to spur science and engineering discoveries. In this talk, a description of the new program of work to develop a generic version of PanDA will be given, as well as the progress in extending PanDA's capabilities to support supercomputers, clouds, leverage intelligent networking, while accommodating the ever growing needs of current users. In particular we will present our plans to refactor PanDA and to develop VO neutral WMS package to be used by new experiments, such as LSST and LBNE, as well as running LHC experiments. PanDA has already demonstrated at a very large scale the value of automated data-aware dynamic brokering of diverse workloads across\u00a0\u2026", "num_citations": "2\n", "authors": ["1575"]}
{"title": "Performance of EDF-BF algorithm under QoS constraint in grid heterogeneous environment\n", "abstract": " Grid computing is one of the most important domain of distributed network. Scheduling in grid is a challenging task to the researchers around the globe. Many different kind of algorithms has been proposed in the literature but because Grid scheduling is a NP-complete problem, more work is undergoing to make the process more optimized and which can provide more effective and efficient solution to the grid load balancing. This paper tries to show some different aspect of one of the most popular grid scheduling algorithm EDF-BF under a special kind of load balancing scheme based on a QoS constraint-CPU speed of the Clusters. Three different parameters were used to test the effectiveness of the approach and the output shows that this novel approach although unconventional, is very effective and a grid scheduler can user this QoS driven approach for this algorithm.", "num_citations": "2\n", "authors": ["1575"]}
{"title": "Exploring the use of hybrid HPC-Grids/Clouds infrastructure for science and engineering\n", "abstract": " Significant investments and technological advances have established highperformance computational (HPC) Grid 1 infrastructures as dominant platforms for large-scale parallel/distributed computing in science and engineering. Infrastructures such as the TeraGrid, EGEE and DEISA integrate highend computing and storage systems via high-speed interconnects, and support traditional, batch-queue-based computationally and data intensive highperformance applications.More recently, Cloud services have been playing an increasingly important role in computational research, and are poised to become an integral part of computational research infrastructures. Clouds support a different although complementary provisioning modes as compared to HPC Grids\u2014one that is based on on-demand access to computing utilities, an abstraction of unlimited computing resources, and a usage-based payment mode where users essentially \u201crent\u201d virtual resources and pay for what they use. Underneath these Cloud services are consolidated and virtualized data centers that provide virtual machine (VM) containers hosting applications from large numbers of distributed users.", "num_citations": "2\n", "authors": ["1575"]}
{"title": "Compiler construction of idempotent regions\n", "abstract": " Recovery functionality has many applications in computing systems, from speculation recovery in modern microprocessors to fault recovery in high-reliability systems.  Modern systems commonly recover using checkpoints.  However, checkpoints introduce overheads, add complexity, and often conservatively save more state than necessary.  This paper develops a compiler technique to recover program state without the overheads of explicit checkpoints.  Our technique breaks programs into idempotent regions -- regions that can be freely re-executed -- which allows recovery without checkpointed state.  Leveraging the property of idempotence, recovery can be obrained by simple re-execution.  We develop static analysis techniques to construct these regions in a compiler, and demonstrate low overheads and large region sizes using an LLVM-based implementation.  Across a set of diverse benchmark suites, we construct idempotent regions almost as large as those that could be obtained with perfect runtime information.  Although the resulting code runs slower, typical execution time overheads are in the range of just 2-12%.", "num_citations": "2\n", "authors": ["1575"]}
{"title": "Global\u2010scale distributed I/O with ParaMEDIC\n", "abstract": " Achieving high performance for distributed I/O on a wide\u2010area network continues to be an elusive holy grail. Despite enhancements in network hardware as well as software stacks, achieving high\u2010performance remains a challenge. In this paper, our worldwide team took a completely new and non\u2010traditional approach to distributed I/O, called ParaMEDIC: Parallel Metadata Environment for Distributed I/O and Computing, by utilizing application\u2010specific transformation of data to orders of magnitude smaller metadata before performing the actual I/O. Specifically, this paper details our experiences in deploying a large\u2010scale system to facilitate the discovery of missing genes and constructing a genome similarity tree by encapsulating the mpiBLAST sequence\u2010search algorithm into ParaMEDIC. The overall project involved nine computational sites spread across the U.S. and generated more than a petabyte of data that\u00a0\u2026", "num_citations": "2\n", "authors": ["1575"]}
{"title": "GRENDL: GRid ENabled Distribution and control for Laptop orchestras\n", "abstract": " Laptop Orchestras (LOs) have recently become a very popular mode of musical expression. They engage groups of performers to use ordinary laptop computers as instruments and sound sources in the performance of specially created music software. By using an orchestral metaphor, LOs provide an engaging and challenging environment to experiment with human-computer interaction, network and machine latency, and sound/signal processing. While the LOs at Princeton and Stanford are perhaps the best known, LOs have now been established at many universities in the US and UK, and as private ensembles around the world.", "num_citations": "2\n", "authors": ["1575"]}
{"title": "Parametric study of a multiscale fluidic system using a hybrid cfd-md approach\n", "abstract": " \u2020 Center for Computation and Technology, Louisiana State University, 216 Johnston Hall, Baton Rouge, LA 70803 e-mail:{sko, nykim, sjha}@ cct. lsu. edu\u2020\u2020 Department of Mechanical Engineering, Louisiana State University, Baton Rouge, LA 70803 e-mail:{meniki, moldovan}@ me. lsu. edu\u2217 Contact Author", "num_citations": "2\n", "authors": ["1575"]}
{"title": "An innovative application execution toolkit for multicluster grids\n", "abstract": " Multicluster grids provide one promising solution to satisfying growing computation demands of compute-intensive applications by collaborating various networked clusters. However, it is challenging to seamlessly integrate all participating clusters in different domains into a virtual computation platform. In order to take full advantages of multicluster grids capability, computer scientists need to deal with how to collaborate practically and efficiently participating autonomic systems to execute Grid-enabled applications. We make efforts on grid resource management and implement a toolkit called Pelecanus to improve the overall performance of application execution in multicluster grids environment. The Pelecanus takes advantages of the DA-TC (Dynamic Assignment with Task Containers) execution model to improve resource interoperability and enhance application execution and monitoring. Experiments show that it\u00a0\u2026", "num_citations": "2\n", "authors": ["1575"]}
{"title": "Recent Advances in Intrusion Detection\n", "abstract": " On behalf of the Program Committee, it is our pleasure to present the proceedings of the 12th International Symposium on Recent Advances in Intrusion Detection systems (RAID 2009), which took place in Saint-Malo, France, during September 23\u201325. As in the past, the symposium brought together leading researchers and practitioners from academia, government, and industry to discuss intrusion detection research and practice. There were six main sessions presenting full research papers on anomaly and specification-based approaches, malware detection and prevention, network and host intrusion detection and prevention, intrusion detection for mobile devices, and high-performance intrusion detection. Furthermore, there was a poster session on emerging research areas and case studies.The RAID 2009 Program Committee received 59 full paper submissions from all over the world. All submissions were\u00a0\u2026", "num_citations": "2\n", "authors": ["1575"]}
{"title": "In Computational study of conformational switching of s-box riboswitch\n", "abstract": " Computational Study of Conformational Switching of S-box Riboswitch Page 1 Conclusions and Future Work The binding of SAM stabilizes the binding pocket locally with compensation of the flexibility in other regions. The carboxyl group and amino group of SAM anchor on P3 helix and J1/2 may contribute to the tight binding of SAM to s-box riboswitch. In our future work, the trajectories will be extended to a longer time scale. And essential dynamics will be performed to study significant differences between SAM bound and SAM free trajectories. In addition, other systems with analogs of SAM bound will be submitted to MD simulation. Methods of calculating the interaction energy between ligands and s-box riboswitch, such as MM-PBSA, will be carried out to evaluate the different binding affinities. Finally, the s-box in AAT conformation will be mutated to mimic the AT conformation and to predict whether SAM can bind. \u2026", "num_citations": "2\n", "authors": ["1575"]}
{"title": "Software Security Issues in Embedded Systems\n", "abstract": " Embedded systems and networks are becoming increasingly prevalent in critical sectors. Therefore, malicious or accidental failures in embedded systems can have dire consequences. Hence, the integrity of embedded software infrastructures, such as configuration and code, is of paramount importance. The autonomous nature of embedded systems also poses new challenges in the context of system integrity. Embedded systems and networks also often have to operate autonomously in a dynamic environment. Therefore, an embedded system has to adapt its behavior to the change in environment or the overall goal. Unauthorized or unverified updates to the infrastructure of an embedded system can also compromise its integrity. In recent years, there have been significant advances in the area of software security. However, all these techniques are not directly applicable in the context of embedded systems because of following reasons 1 Embedded systems are generally deployed in environments that are highly dynamic and configurable. 2 Functional requirements of an embedded system change over time. 3 Frequently an embedded system is a complex network of components. Therefore, a malicious or accidental fault in a component can lead to a complex cascade of events in the network. 4 Embedded systems are frequently deployed in mission critical applications where consequences of failures can be dire. Therefore, recovery from failures is extremely important in the context of embedded systems. Extending existing techniques in software security to handle the four above mentioned characteristics of embedded systems is an important\u00a0\u2026", "num_citations": "2\n", "authors": ["1575"]}
{"title": "Security analysis and administrative insider threat assessment in role-based access control\n", "abstract": " Specifying and managing access control policies is a challenging problem. We propose to develop formal verification techniques for access control policies to improve the current state of the art of policy specification and management. In this paper, we formalize classes of security analysis and administrative insider threat assessment problems in the context of Role-Based Access Control. We show that in general these problems are PSPACE-complete. We also study the factors that contribute to the computational complexity by considering a lattice of various subcases of the problem with different restrictions. We show that several subcases remain PSPACE-complete, several further restricted subcases are NP-complete, and identify two subcases that are solvable in polynomial time. We also discuss our experiences and findings from experimentations that use existing formal method tools, such as model checking and logic programming, for addressing these problems.", "num_citations": "2\n", "authors": ["1575"]}
{"title": "An iterative framework for simulation conformance\n", "abstract": " MAGIC is a software verification project for C source code which verifies conformance of software components against statemachine specifications. To this aim, MAGIC extracts abstract software models using predicate abstraction, and resolves the inherent trade-off between model accuracy and scalability by an iterative abstraction refinement methodology. This paper presents the core principles implemented in the MAGIC verification engine, i.e. specification conformance using simulation and abstraction refinement. Viewing counterexamples as winning strategies in a simulation game between the implementation and the specification, we describe an algorithm where abstractions are refined on the basis of multiple winning strategies simultaneously. The refinement process is iterated until either a conformance with the specification is established, or a strategy to violate the specification is found to be\u00a0\u2026", "num_citations": "2\n", "authors": ["1575"]}
{"title": "Efficient filtering in publish-subscribe system\n", "abstract": " Implicit invocation or publish-subscribe has become an important architectural style for large-scale system design and evolution. The publish-subscribe style facilitates developing large-scale systems by composing separately developed components because the style permits loose coupling between various components. One of the major bottlenecks in using publish-subscribe systems for very large scale systems is the efciency of ltering incoming messages, ie, matching of published events with event subscriptions. This is a very challenging problem because in a realistic publishsubscribe system the number of subscriptions can be large.", "num_citations": "2\n", "authors": ["1575"]}
{"title": "Detecting errors and estimating accuracy on unlabeled data with self-training ensembles\n", "abstract": " When a deep learning model is deployed in the wild, it can encounter test data drawn from distributions different from the training data distribution and suffer drop in performance. For safe deployment, it is essential to estimate the accuracy of the pre-trained model on the test data. However, the labels for the test inputs are usually not immediately available in practice, and obtaining them can be expensive. This observation leads to two challenging tasks:(1) unsupervised accuracy estimation, which aims to estimate the accuracy of a pre-trained classifier on a set of unlabeled test inputs;(2) error detection, which aims to identify mis-classified test inputs. In this paper, we propose a principled and practically effective framework that simultaneously addresses the two tasks. The proposed framework iteratively learns an ensemble of models to identify mis-classified data points and performs self-training to improve the ensemble with the identified points. Theoretical analysis demonstrates that our framework enjoys provable guarantees for both accuracy estimation and error detection under mild conditions readily satisfied by practical deep learning models. Along with the framework, we proposed and experimented with two instantiations and achieved state-of-the-art results on 59 tasks. For example, on iWildCam, one instantiation reduces the estimation error for unsupervised accuracy estimation by at least 70% and improves the F1 score for error detection by at least 4.7% compared to existing methods.", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Continuous Release of Data Streams under both Centralized and Local Differential Privacy\n", "abstract": " We study the problem of publishing a stream of real-valued data satisfying differential privacy (DP). One major challenge is that the maximal possible value in the stream can be quite large, leading to enormous DP noise and bad utility. To reduce the maximal value and noise, one way is to estimate a threshold so that values above it can be truncated. The intuition is that, in many scenarios, only a few values are large; thus truncation does not change the original data much. We develop such a method that finds a suitable threshold with DP. Given the threshold, we then propose an online hierarchical method and several post-processing techniques.", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Fairness Properties of Face Recognition and Obfuscation Systems\n", "abstract": " The proliferation of automated face recognition in various commercial and government sectors has caused significant privacy concerns for individuals. A recent, popular approach to address these privacy concerns is to employ evasion attacks against the metric embedding networks powering face recognition systems. Face obfuscation systems generate imperceptible perturbations, when added to an image, cause the face recognition system to misidentify the user. The key to these approaches is the generation of perturbations using a pre-trained metric embedding network followed by their application to an online system, whose model might be proprietary. This dependence of face obfuscation on metric embedding networks, which are known to be unfair in the context of face recognition, surfaces the question of demographic fairness -- \\textit{are there demographic disparities in the performance of face obfuscation systems?} To address this question, we perform an analytical and empirical exploration of the performance of recent face obfuscation systems that rely on deep embedding networks. We find that metric embedding networks are demographically aware; they cluster faces in the embedding space based on their demographic attributes. We observe that this effect carries through to face obfuscation systems: faces belonging to minority groups incur reduced utility compared to those from majority groups. For example, the disparity in average obfuscation success rate on the online Face++ API can reach up to 20 percentage points. We present an intuitive analytical model to provide insights into these phenomena.", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Sample Complexity of Robust Linear Classification on Separated Data\n", "abstract": " We consider the sample complexity of learning with adversarial robustness. Most prior theoretical results for this problem have considered a setting where different classes in the data are close together or overlapping. We consider, in contrast, the well-separated case where there exists a classifier with perfect accuracy and robustness, and show that the sample complexity narrates an entirely different story. Specifically, for linear classifiers, we show a large class of well-separated distributions where the expected robust loss of any algorithm is at least , whereas the max margin algorithm has expected standard loss . This shows a gap in the standard and robust losses that cannot be obtained via prior techniques. Additionally, we present an algorithm that, given an instance where the robustness radius is much smaller than the gap between the classes, gives a solution with expected robust loss is . This shows that for very well-separated data, convergence rates of  are achievable, which is not the case otherwise. Our results apply to robustness measured in any  norm with (including ).", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Towards Adversarial Robustness via Transductive Learning\n", "abstract": " There has been emerging interest to use transductive learning for adversarial robustness (Goldwasser et al., NeurIPS 2020; Wu et al., ICML 2020). Compared to traditional \"test-time\" defenses, these defense mechanisms \"dynamically retrain\" the model based on test time input via transductive learning; and theoretically, attacking these defenses boils down to bilevel optimization, which seems to raise the difficulty for adaptive attacks. In this paper, we first formalize and analyze modeling aspects of transductive robustness. Then, we propose the principle of attacking model space for solving bilevel attack objectives, and present an instantiation of the principle which breaks previous transductive defenses. These attacks thus point to significant difficulties in the use of transductive learning to improve adversarial robustness. To this end, we present new theoretical and empirical evidence in support of the utility of transductive learning.", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Causally Constrained Data Synthesis for Private Data Release\n", "abstract": " Making evidence based decisions requires data. However for real-world applications, the privacy of data is critical. Using synthetic data which reflects certain statistical properties of the original data preserves the privacy of the original data. To this end, prior works utilize differentially private data release mechanisms to provide formal privacy guarantees. However, such mechanisms have unacceptable privacy vs. utility trade-offs. We propose incorporating causal information into the training process to favorably modify the aforementioned trade-off. We theoretically prove that generative models trained with additional causal knowledge provide stronger differential privacy guarantees. Empirically, we evaluate our solution comparing different models based on variational auto-encoders (VAEs), and show that causal information improves resilience to membership inference, with improvements in downstream utility.", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Sample Complexity of Adversarially Robust Linear Classification on Separated Data\n", "abstract": " We consider the sample complexity of learning with adversarial robustness. Most prior theoretical results for this problem have considered a setting where different classes in the data are close together or overlapping. Motivated by some real applications, we consider, in contrast, the well-separated case where there exists a classifier with perfect accuracy and robustness, and show that the sample complexity narrates an entirely different story. Specifically, for linear classifiers, we show a large class of well-separated distributions where the expected robust loss of any algorithm is at least , whereas the max margin algorithm has expected standard loss . This shows a gap in the standard and robust losses that cannot be obtained via prior techniques. Additionally, we present an algorithm that, given an instance where the robustness radius is much smaller than the gap between the classes, gives a solution with expected robust loss is . This shows that for very well-separated data, convergence rates of  are achievable, which is not the case otherwise. Our results apply to robustness measured in any  norm with  (including ).", "num_citations": "1\n", "authors": ["1575"]}
{"title": "ShadowNet: A Secure and Efficient System for On-device Model Inference\n", "abstract": " With the increased usage of AI accelerators on mobile and edge devices, on-device machine learning (ML) is gaining popularity. Consequently, thousands of proprietary ML models are being deployed on billions of untrusted devices. This raises serious security concerns about model privacy. However, protecting the model privacy without losing access to the AI accelerators is a challenging problem. In this paper, we present a novel on-device model inference system, ShadowNet. ShadowNet protects the model privacy with Trusted Execution Environment (TEE) while securely outsourcing the heavy linear layers of the model to the untrusted hardware accelerators. ShadowNet achieves this by transforming the weights of the linear layers before outsourcing them and restoring the results inside the TEE. The nonlinear layers are also kept secure inside the TEE. The transformation of the weights and the restoration of the results are designed in a way that can be implemented efficiently. We have built a ShadowNet prototype based on TensorFlow Lite and applied it on four popular CNNs, namely, MobileNets, ResNet-44, AlexNet and MiniVGG. Our evaluation shows that ShadowNet achieves strong security guarantees with reasonable performance, offering a practical solution for secure on-device model inference.", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Combining molecular simulation and machine learning to INSPIRE improved cancer therapy\n", "abstract": " Cancer is the second leading cause of death in the United States (accounting for nearly 25% of all deaths). Targeted kinase inhibitors play an increasingly prominent role in the treatment of cancer and account for a significant fraction of the $37 billion US market for oncology drugs in the last decade. Unfortunately, the development of resistance limits the amount of time patients derive benefits from their treatment. The INSPIRE project is laying the foundations for the use of molecular simulation and machine learning (ML) to guide precision cancer therapy, in which therapy is tailored to provide maximum benefit to individual patients based on genetic information about their particular cancer. It is vital that such an approach is based on predictive methods as the vast majority of clinically observed mutations are rare, rendersing catalog-building alone insufficient.", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Use Cases of Computational Reproducibility for Scientific Workflows at Exascale\n", "abstract": " We propose an approach for improved reproducibility that includes capturing and relating provenance characteristics and performance metrics, in a hybrid queriable system, the ProvEn server. The system capabilities are illustrated on two use cases: scientific reproducibility of results in the ACME climate simulations and performance reproducibility in molecular dynamics workflows on HPC computing platforms.", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Study of Effect of Strain, Quantum Well Width, and Temperature on Optical Gain in Nano-Heterostructures\n", "abstract": " In the work discussed here, we evaluate the effect of change in strain, quantum well width variation, and temperature on the optical gain of two SQW (Single quantum well) nano-heterostructures. Both the heterostructures are SCH (Separate confinement heterostructures) with STIN (Step Index) profile. We have taken a quaternary semiconductor Al0.15In0.22Ga0.63As/GaAs and compared it with a ternary semiconductor heterostructure In0.45Ga0.55As/InP. This paper is an effort to compare the effect of change of strain on the optical gain of the two heterostructures. It also analyzes the behavior of quantum well width and temperature on the gain.", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Kali: Scalable encryption fingerprinting in dynamic malware traces\n", "abstract": " Binary analysis of malware to determine uses of encryption is an important primitive with many critical applications, such as reverse-engineering of malware network communications and decryption of files encrypted by ransomware. The state of the art for encryption fingerprinting in dynamic execution traces, the ALIGOT algorithm-while effective in identifying a range of known ciphers-suffers from significant scalability limitations: in certain cases, even analyzing traces of a few thousands of machine instructions may require prohibitive time/space. In this work, we propose KALI, an enhanced algorithm based on ALIGOT which significantly reduces time/space complexity and increases scalability. Moreover, we propose a technique to focalize the analysis on encryption used for specific purposes, further improving efficiency. Results show that KALI achieves orders of magnitude reduction in execution time and memory\u00a0\u2026", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Privacy preserving linear regression on distributed data\n", "abstract": " Linear regression is an important statistical tool that models the relationship between some explanatory values and an outcome value using a linear function. In many current applications (eg predictive modelling in personalized healthcare), these values represent sensitive data owned by several different parties that are unwilling to share them. In this setting, training a linear regression model becomes challenging and needs specific cryptographic solutions. In this work, we propose a new system that can train two different variants of linear regression (ie ridge regression and lasso regression) on a dataset obtained by merging a finite number of private datasets. Our system assures that no extra information on a single private dataset is revealed to the entities performing the learning algorithm. Moreover, our solution is based on efficient cryptographic tools (eg Paillier\u2019s scheme and pseudorandom generator).", "num_citations": "1\n", "authors": ["1575"]}
{"title": "AIMES Final Technical Report\n", "abstract": " This is the final technical report for the AIMES project. Many important advances in science and engineering are due to large-scale distributed computing. Notwithstanding this reliance, we are still learning how to design and deploy large-scale production Distributed Computing Infrastructures (DCI). This is evidenced by missing design principles for DCI, and an absence of generally acceptable and usable distributed computing abstractions. The AIMES project was conceived against this backdrop, following on the heels of a comprehensive survey of scientific distributed applications. AIMES laid the foundations to address the tripartite challenge of dynamic resource management, integrating information, and portable and interoperable distributed applications. Four abstractions were defined and implemented: skeleton, resource bundle, pilot, and execution strategy. The four abstractions were implemented into software modules and then aggregated into the AIMES middleware. This middleware successfully integrates information across the application layer (skeletons) and resource layer (Bundles), derives a suitable execution strategy for the given skeleton and enacts its execution by means of pilots on one or more resources, depending on the application requirements, and resource availabilities and capabilities.", "num_citations": "1\n", "authors": ["1575"]}
{"title": "A model based connectivity and localization strategy for vehicular ad hoc networks\n", "abstract": " Nowadays, researchers have performed extensive experiments to study the feasibility and Connectivity of vehicle access. We investigate connectivity in the ad hoc network formed between vehicles that move on a typical highway. We use the common model in vehicular traffic theory. Since a vehicle spends a large portion of the connection time in this poor link quality area, \"No GPS Zone\" the data throughput can be significantly reduced. Relative location information is an important aspect in vehicular Ad hoc networks. It helps to build vehicle topology maps, also provides location tracking of nearby vehicles. In this paper, we propose a protocol of localization using VANET where no GPS information is available based on clustering and link prediction and also has the advantage to use a single coordinates system. Using the network simulator NS-2 and also checked the mobility scenario in SUMO by implementing the\u00a0\u2026", "num_citations": "1\n", "authors": ["1575"]}
{"title": "LCDs: Lane-Changing Aid System Based on Speed of Vehicles\n", "abstract": " Lane change is an important issue in microscopic traffic flow simulations and active safety. Overtaking and changing lanes are dangerous driving maneuvers. This approach presents a lane-changing system based on speed and a minimum gap between vehicles in a vehicular ad hoc network (VANET). This paper proposes a solution to ensure the safety of drivers while changing lanes on highways. Efficient routing protocols could play a crucial role in VANET applications, safeguarding both drivers and passengers, and thus, maintaining a safe on-road environment. This paper focuses on the development of an intelligent transportation system that provides timely, reliable information to drivers and the concerned authorities. A test bed is created for the techniques used in the proposed system, where analysis takes place in an on-board embedded system designed for vehicle navigation. The designed system was tested on a four-lane road in Neemrana, India. Successful simulations were conducted with real-time network parameters to maximize quality of service and performance using Simulation of Urban Mobility and Network Simulator 2 (NS-2). The system implementation, together with the findings, is presented in this paper. Illustrating the approach are results from simulation using NS-2.", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Pilot-Streaming: Design Considerations for a Stream Processing Framework for High-Performance Computing\n", "abstract": " Streaming capabilities are becoming increasingly important for scientific applications [1],[2] supporting important needs, such as the ability to act on incoming data and steering. The interoperable use of streaming data sources within HPC environments is a critical capability for an emerging set of applications. Scientific instruments, such as x-ray light sources (eg, the Advanced Photon Source (APS) and the Advanced Light Source (ALS)[3]), can generate large amounts of highvelocity data in a diverse set of experiments. Coupling data streams produced by such experiments to computational HPC capabilities is an important challenge. Supporting the processing of high data rates streams executing, eg, predictions and outlier detection algorithms on it, while running larger models in batch mode on the entire dataset, is a challenging task. The increasing demands lead to a heterogeneous landscape of infrastructures and tools supporting streaming needs on different levels. Batch frameworks, such as Spark [4] have been extended to provide streaming capabilities [5], while different native streaming frameworks, such as Storm [6] and Flink [7] have emerged. We define a streaming application as an application that processes and acts on real-time data, also referred to as event stream. Different usage modes for stream processing can be observed:\u2022 Coordination: Usage of stream processing to connect a data source and data analysis phase. Sometime this includes the pre-processing and transformation of the data before it becomes persistent (eg the Hadoop Filesystem (HDFS)) and analyzed (eg using a Hadoop processing framework).\u2022 Realtime\u00a0\u2026", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Review on SMS Encryption using MNTRU Algorithms on Android\n", "abstract": " NTRU a fast encryption algorithm was yet not implemented on chat applications. The main advantage of this algorithm is fast encryption and decryption. Since chat application needs to maintain the reliability of speed within them so its also necessary to keep in mind the security measures as well as reliability matters. The NTRU fits in all results. The execution time of proposed system is less. The decryption time is reduced.", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Analysis of Strained Al0. 15In0. 22Ga0. 63As/GaAs Graded Index\u2013Separate Confinement Lasing Nano-heterostructure\n", "abstract": " The paper deals with a theoretical insight into the various characteristics of a 0.89 \u03bcm Al0. 15In0. 22Ga0. 63As/GaAs strained single quantum well based Graded Index (GRIN)-separate confinement lasing nano-heterostructure. Major emphasis has been laid on the optical and modal gain. Both these gain have been simulated with respect to lasing wavelength, photon energy and current density. In this paper, we have also drawn a comparative picture of the two polarization modes ie Transverse Electric (TE) and Transverse Magnetic (TM). The maximum optical gain has been observed to be 5557.18 cm-1 at the lasing wavelength~ 0.90 \u03bcm and photonic energy~ 1.36 eV in TE mode and it is only 2760.70 cm-1 at the lasing wavelength~ 0.78 \u03bcm and at photonic energy~ 1.58 eV in TM mode. However, the maximum modal gain has been observed to be 54.65 cm-1 in TE mode and it is 27.16 cm-1 in TM mode at the same lasing wavelengths and photonic energies respectively at 298 K. The behavior of quasi Fermi levels for the conduction band and valence band has also been studied. Other important parameters like gain compression, differential gain and refractive index profile have also been simulated with respect to carrier density. Anti-guiding factor has been plotted against current density to observe its behavior in order to support the explanation of optical gain simulated.", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Integrating the Apache Big Data Stack with HPC for Big Data\n", "abstract": " There is perhaps a broad consensus as to important issues in practical parallel computing as applied to large scale simulations; this is reflected in supercomputer architectures, algorithms, libraries, languages, compilers and best practice for application development. However, the same is not so true for data intensive computing, even though commercially clouds devote much more resources to data analytics than supercomputers devote to simulations. We look at a sample of over 50 big data applications to identify characteristics of data intensive applications and to deduce needed runtime and architectures. We suggest a big data version of the famous Berkeley dwarfs and NAS parallel benchmarks and use these to identify a few key classes of hardware/software architectures. Our analysis builds on combining HPC and ABDS the Apache big data software stack that is well used in modern cloud computing. Initial\u00a0\u2026", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Design and implementation of obstacle detection algorithm in robotics\n", "abstract": " Robotics industry has replaced human efforts gradually in performing rather difficult tasks. A very pertinent aspect of an intelligent security robot is to reach the goal safely by avoiding unknown obstacles in an unknown environment. In this paper we have developed an embedded C program code to design an intelligent robot which can overcome the obstacles coming in its way. We have made use of three infrared sensors to detect the obstacles via the infrared communication technique. The infrared transmitter sends out infrared radiation in a direction which consequently bounces back on coming across the surface of an object and thereafter is picked up by the infrared receiver. Authors have applied a multi sensor integration technique to sense the obstacles using an LED based infrared transmitter and receiver module integrated with the 8051 micro controller which permits collision free navigation of robots.", "num_citations": "1\n", "authors": ["1575"]}
{"title": "A Parallel Simulated Annealing Approach for the Mirrored Traveling Tournament Problem\n", "abstract": " The Traveling Tournament Problem (TTP) is a benchmark problem in sports scheduling and has been extensively studied in recent years. The Mirrored Traveling Tournament Problem (mTTP) is variation of the TTP that represents certain types of sports scheduling problems where the main objective is to minimize the total distance traveled by all the participating teams. In this paper we test a parallel simulated annealing approach for solving the mTTP using OpenMP on shared memory systems and we found that this approach is superior especially with respect to the number of solution instances that are probed per second. We also see that there is significant speed up of 1.5x - 2.2x in terms of number of solutions explored per unit time.", "num_citations": "1\n", "authors": ["1575"]}
{"title": "P-HGRMS: A parallel hypergraph based root mean square algorithm for image denoising\n", "abstract": " This paper presents a parallel Salt and Pepper (SP) noise removal algorithm in a grey level digital image based on the Hypergraph Based Root Mean Square (HGRMS) approach. HGRMS is generic algorithm for identifying noisy pixels in any digital image using a two level hierarchical serial approach. However, for SP noise removal, we reduce this algorithm to a parallel model by introducing a cardinality matrix and an iteration factor, k, which helps us reduce the dependencies in the existing approach. We also observe that the performance of the serial implementation is better on smaller images, but once the threshold is achieved in terms of image resolution, its computational complexity increases drastically. We test P-HGRMS using standard images from the Berkeley Segmentation dataset on NVIDIAs Compute Unified Device Architecture (CUDA) for noise identification and attenuation. We also compare the noise removal efficiency of the proposed algorithm using Peak Signal to Noise Ratio (PSNR) to the existing approach. P-HGRMS maintains the noise removal efficiency and outperforms its sequential counterpart by 6 to 18 times (6x - 18x) in computational efficiency.", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Iterative security risk analysis for network flows based on provenance and interdependency\n", "abstract": " Discovering high risk network flows and hosts in a high throughput network is a challenging task of network monitoring. Emerging complicated attack scenarios such as DDoS attacks increase the complexity of tracking malicious and high risk network activities within a huge number of monitored network flows. To address this problem, we propose an iterative framework for assessing risk scores for hosts and network flows. To obtain risk scores of flows, we take into account two properties, flow attributes and flow provenance. Also, our iterative risk assessment measures the risk scores of hosts and flows based on an interdependency property where the risk score of a flow influences the risk of its source and destination hosts, and the risk score of a host is evaluated by risk scores of flows initiated by or terminated at the host. Moreover, the update mechanism in our framework allows flows to keep streaming into the\u00a0\u2026", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Exploring dynamic enactment of scientific workflows using pilot-abstractions\n", "abstract": " Current workflow abstractions in general lack: (a) an adequate approach to handle distributed data and (b) proper separation between logical tasks and data-flow from their mapping onto physical locations. As the complexity and dynamism of data and processing distribution have increased, optimized mapping of logical tasks to physical resources have become a necessity to avoid bottlenecks. We argue that the management of dynamic data and compute should become part of the runtime system of workflow engines to enable workflows to scale as necessary to address big data challenges and fully exploit distributed computing infrastructures (DCI). In this paper we explore how the P* model for pilot-abstractions, which proposes a clear separation between the logical compute and data units and their realization as a job or a file in some physical resource, could provide these capabilities for such a runtime\u00a0\u2026", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Exploring flexible and dynamic enactment of scientific workflows using pilot abstractions\n", "abstract": " Biomedical applications have been successfully ported and executed on distributed computing infrastructures (DCI) using workflow technology because it can hide details of the underlying infrastructure, and serve as abstraction layer to carry out high throughput computing experiments. By lowering the barriers to use such complex infrastructures, workflow management systems have been valuable allies in e-science. As heavy users of workflows to enable medical imaging and DNA sequencing data analyses, with the growth of the data size we saw challenges shift from processing to data. The complexity and dynamism of data and processing distribution have increased, and optimized mapping of logical tasks to physical resources have become a necessity to avoid bottlenecks. In practice such optimization is hard to achieve when using workflow abstractions, and based on our experience, this is not an exclusive property of the system we use. Current workflow abstractions in general lack:(a) an explicit/adequate approach to handle distributed data on a workflow and (b) proper separation between logical tasks and data flow from their mapping into physical location on a DCI. We argue that the management of dynamic data and compute should become part of the runtime system of workflow engines to enable applications implemented as workflows to scale as necessary to address big data challenges and fully exploit DCIs. We explore how the P* model for pilot-abstractions could serve as a substrate for such a runtime environment. This model proposed a clear separation between the logical compute and data units and their realization as a job or\u00a0\u2026", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Futuregrid education: Using case studies to develop a curriculum for communicating parallel and distributed computing concepts\n", "abstract": " The shift to parallel computing--including multi-core computer architectures, cloud distributed computing, and generalpurpose GPU programming--leads to fundamental changes in the design of software and systems. As a result, learning parallel, distributed, and cloud techniques in order to allow software to take advantage of the shift toward parallelism is of important significance. To this end, FutureGrid, an experimental testbed for cloud, grids, and high performance computing, provides a resource for anyone to find, share, and discuss modular teaching materials and computational platform supports.", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Guest editorial: Special issue on computer and communications security\n", "abstract": " This special issue contains extended versions of articles selected from the program of the 15th ACM Conference on Computer and Communications Security (CCS\u201908), which took place October 27 to 31, 2008 in Alexandria, Virginia (USA). This annual conference is a leading international forum for information-security researchers, practitioners, developers, and users to explore cutting-edge ideas and results, and to exchange techniques, tools, and experiences. Its mission is to promote and share novel research from academia, government, and industry covering all theoretical and practical aspects of computer security as well as case studies and implementation experiences. The selected articles represent the broad scope of the conference. Two articles focus primarily on what we can learn from novel and existing attacks on security, and the other two articles focus on the development of improved mechanisms to\u00a0\u2026", "num_citations": "1\n", "authors": ["1575"]}
{"title": "A New QoS based Load Balancing Approach with Percentage Load Conversion in Grid Heterogeneous System\n", "abstract": " In grid computing, load balancing is a technique to distribute workload evenly across two or more computing nodes, in order to get optimal resource utilization, maximize throughput, minimize response time, and avoid overload. In Grid system, there are queues of jobs waiting for getting resources like storage space, CPU, I/O devices etc. The behaviour or state of the Grid system changes dynamically ie from time to time. The bandwidth of the n/w, the no. of jobs, the no. of resources etc. in the system changes dynamically. A new approach with load balancing algorithm with load conversion has been introduced here. This algorithm is applied on different scheduling algorithms using Grid Simulator (Alea 2). With different load conversion percentages in load balancing it has been found that existing scheduling algorithms can performs better if a specified percentage of Load is reallocated depending on the CPU speed of clusters.", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Multi-species fluid flow simulations using a hybrid computational fluid dynamics-molecular dynamics approach\n", "abstract": " The constrained Lagrangian dynamics modeling in the hybrid computational fluid dynamics (CFD)-molecular dynamics (MD) approach is improved for the simulation of multi-species polyatomic fluid. The primitive formulation of the classical Lagrangian dynamics equation is replaced by conservative form to account for multi-species fluid system. Also, the equation is applied on molecules instead of individual atom, to preserve the linear momentum between continuum and particle domain without encountering the unfavorable numerical break-down of molecular bonding. We verify our hybrid CFD-MD simulation package by analyzing a nano-scale transient Couette flow of a single monatomic fluid. The multi-species polyatomic Lagrangian dynamics modeling has been evaluated by analyzing two different fluid models: the mixture of two monatomic fluids and a polyatomic molecular fluid under the short-range potential\u00a0\u2026", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Towards Enforceable Data-Driven Privacy Policies\n", "abstract": " A defining characteristic of current web applications is that they are personalized according to the interests and preferences of individual users; popular examples are Google News and Amazon. com. While this paradigm shift is generally viewed as positive by both users and content providers, it introduces privacy concerns, as the data needed to drive this functionality is often considered private. Web applications have responded by giving users the chance to deny explicit disclosure of personal information, as well as minimizing the invasiveness of the information they require. In this position paper, we address the concern that explicit disclosure alone is not sufficient to protect user privacy, as attackers can combine users\u2019 consensually-shared information with additional background information to infer private facts about individuals. We argue that to properly account for these attacks, auditors must consider not just the relationship between disclosed information and attackers\u2019 background data, but also the semantics of applications that operate over the private information.", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Numerical Experiments of Solving Moderate-Velocity Flow Field Using a Hybrid CFD-MD Approach\n", "abstract": " We propose numerical approaches to reduce the sampling noise of a hybrid computational fluid dynamics (CFD) - molecular dynamics (MD) solution. A hybrid CFD-MD approach provides higher-resolution solution near the solid obstacle and better efficiency than a pure particle-based simulation technique. However, applications up to now are limited to extreme velocity conditions, since the magnitude of statistical error in sampling particles\u2019 velocity is very large compared to the continuum velocity. Considering technical difficulties of infinitely increasing MD domain size, we propose and experiment a number of numerical alternatives to suppress the excessive sampling noise in solving moderate-velocity flow field. They are the sampling of multiple replicas, virtual stretching of sampling layers in space, and linear fitting of multiple temporal samples. We discuss the pros and cons of each technique in view of solution\u00a0\u2026", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Information Systems Security: 6th International Conference, ICISS 2010, Gandhinagar, India, December 17-19, 2010\n", "abstract": " 2.1 Web Application Vulnerabilities Many web application vulnerabilities havebeenwell documented andthemi-gation methods havealso beenintroduced [1]. The most common cause ofthose vulnerabilities isthe insu'cient input validation. Any data originated from o-side of the program code, forexample input data provided by user through a web form, shouldalwaysbeconsidered malicious andmustbesanitized before use. SQLInjection, Remote code execution orCross-site Scriptingarethe very common vulnerabilities ofthattype [3]. Below isabrief introduction toSQL-jection vulnerability though the security testingmethodpresented in thispaper is not limited toit. SQLinjectionvulnerabilityallowsanattackertoillegallymanipulatedatabase byinjectingmalicious SQL codes into the values of input parameters of http requests sentto the victim web site. 1: Fig. 1. An example of a program written in PHP which contains SQL Injection v-nerability Figure 1 showsaprogram that uses the database query function mysql query togetuserinformationcorrespondingtothe userspeci'edby the GETinput-rameterusername andthen printtheresultto the clientbrowser. Anormalhttp request with the input parameter username looks like\" http://example. com/index. php'username= bob\". The dynamically created database query at line2 is\" SELECT* FROM users WHERE username='bob'AND usertype='user'\". Thisprogram is vulnerabletoSQLInjection attacks because mysql query uses the input value of username without sanitizingmalicious codes. A malicious code can be a stringthatcontains SQL symbols ork-words. Ifan attacker sendarequest with SQL code ('alice'-')-jected\" http\u00a0\u2026", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Modelling data-driven CO2 sequestration using distributed HPC cyberinfrastructure\n", "abstract": " In this paper we lay out the computational challenges involved in effectively simulating complex phenomena such as sequestering CO 2 in oil and gas reservoirs. The challenges arise at multiple levels:(i) the computational complexity of simulating the fundamental processes;(ii) the resource requirements of the computationally demanding simulations;(iii) the need for integrating real-time data (intensive) and computationally intensive simulations;(iv) and the need to implement all of these in a robust, scalable and extensible approach. We will outline the architecture and implementation of the solution we develop in response to these requirements, and discuss results to validate claims that our solution scales to effectively solve desired problem sizes and thus provides the capability to generate novel scientific insight.", "num_citations": "1\n", "authors": ["1575"]}
{"title": "The Design, Modeling, and Evaluation of the Relax Architectural Framework\n", "abstract": " As transistor technology scales ever further, hardware reliability is becoming harder to manage.  The effects of soft errors, variability, wear-out, and yield are intensifying to the point where it becomes difficult to harness the benefits of deeper scaling without mechanisms for hardware fault detection and correction. We observe that the combination of emerging applications and emerging many-core architectures makes software recovery a viable and interesting alternative to traditional, hardware-based fault recovery. Emerging applications tend to have few I/O and memory side-effects, which limits the amount of information that needs checkpointing, and they allow discarding individual sub-computations with typically minimal qualitative impact. Software recovery can harness these properties in ways that hardware recovery cannot. Additionally, emerging many-core architectures comprised of many simple, in-order cores pay heavily in terms of power and area for hardware checkpointing resources. Software recovery can be more efficient while it simultaneously simplifies hardware design complexity.   In this paper, we describe Relax, an architectural framework for software recovery of hardware faults.  We describe Relax's language, compiler, ISA, and hardware support, develop analytical models to project performance, and evaluate an implementation of the framework on the compute kernels of seven emerging applications. Applying Relax to counter the effects of process variation, we find that Relax can enable a 20% energy efficiency improvement for more than 80% of an application's execution with only minimal source code changes.", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Recent Advances in Intrusion Detection: 12th International Symposium, RAID 2009, Saint-Malo, France, September 23-25, 2009, Proceedings\n", "abstract": " On behalf of the Program Committee, it is our pleasure to present the p-ceedings of the 12th International Symposium on Recent Advances in Intrusion Detection systems (RAID 2009), which took place in Saint-Malo, France, during September 23\u201325. As in the past, the symposium brought together leading-searchers and practitioners from academia, government, and industry to discuss intrusion detection research and practice. There were six main sessions prese-ingfullresearchpapersonanomalyandspeci? cation-basedapproaches, malware detection and prevention, network and host intrusion detection and prevention, intrusion detection for mobile devices, and high-performance intrusion det-tion. Furthermore, there was a poster session on emerging research areas and case studies. The RAID 2009ProgramCommittee received59 full paper submissionsfrom all over the world. All submissions were carefully reviewed by independent-viewers on the basis of space, topic, technical assessment, and overall balance. The? nal selection took place at the Program Committee meeting on May 21 in Oakland, California. In all, 17 papers were selected for presentation and p-lication in the conference proceedings. As a continued feature, the symposium accepted submissions for poster presentations which have been published as-tended abstracts, reporting early-stage research, demonstration of applications, or case studies. Thirty posters were submitted for a numerical review by an independent, three-person sub-committee of the Program Committee based on novelty, description, and evaluation. The sub-committee recommended the-ceptance of 16 of\u00a0\u2026", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Abstractions for distributed systems (dpa 2008)\n", "abstract": " The computing infrastructure of tomorrow will be very different from that of yesterday. In this rapidly evolving landscape, the development of applications that can remain neutral to underlying infrastrutural changes remains a challenge. The goal of the Workshop on Abstractions for Distributed Systems is to try to address how utilizing distributed systems can be made easier via the use of abstractions \u2014 support for commonly occurring patterns, which could be either programming patterns, application usage patterns or infrastructure usage patterns.               This workshop aimed to determine where programming abstractions are important and where non-programmatic abstractions are likely to make greater impact in enabling applications to effectively utilize distributed infrastructure. The workshop had a balance of applications and topical infrastructure developments (such as abstractions for Clouds).", "num_citations": "1\n", "authors": ["1575"]}
{"title": "International assistance for agricultural development: new directions?\n", "abstract": " -2-farmers.\" In the next five years, concludes McNamara,\" we expect that about 70% of our agricultural loans will contain a component for the smallholder.\" The $4.4 billion earmarked for agriculture represents about 20 percent of total lending planned by the Bank over the five year period. This is approximately the same percentage for agriculture as in the previous five years-1969-73. How much of this new lending will actually reach and benefit the small farmer? The World Bank's past performance on this score is not encouraging. Only $1 billion out of a total of $25 billion lent during the Bank's first 25 years of existence went to small farmers-a mere 4 percent.Why this new emphasis? Bi-lateral as well as multi-lateral assistance agencies are becoming increasingly conscious of the failures of past policies. Development has been rationalized, and international assistance justified, on the grounds that it Would improve the general conditions of life for the masses and not just for the few. But there are more illiterate adults, undernourished children, and unemployed workers in the less developed world today than there were twenty years ago, and the income gap between rich and poor has widened (both within the less industrialized countries and between those countries and the industrialized ones). There has been considerable economic growth but, paradoxically, little development in terms of reducing mass poverty, unemployment and inequality. The old assumption that job creation and a more egalitarian income distribution are automatic concomitants of economic growth is being widely challenged. The number of stalwart defenders of that abiding\u00a0\u2026", "num_citations": "1\n", "authors": ["1575"]}
{"title": "May 1997\n", "abstract": " As more resources are added to computer networks, and as more vendors look to the World Wide Web as a viable marketplace, the importance of being able to restrict access and to insure some kind of acceptable behaviorevenin the presenceofmaliciousintrudersbecomes paramount. Peoplehavelookedto cryptography to help solve many of these problems. However, cryptography itself is only a tool. The security of a system depends not only on the cryptosystem being used, but also on how it is used. Typically, researchers have proposed the use of security protocols to provide these security guarantees. These protocols consist of a sequenceofmessages, manywith encryptedparts. Inthis paper, wedevelopawayofverifyingthese protocols using model checking. Model checkinghasprovento beaveryuseful techniqueforverifyinghardwaredesigns. By modelling circuits as nite-state machines, and examining all possible\u00a0\u2026", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Grid Enabled Interactive Molecular Dynamic Simulations\n", "abstract": " The importance of computational approaches in providing quantitative information as well as qualitative insight has been widely acknowledged. For biomolecular systems, classical molecular dynamics (MD) simulations have the greatest ability to provide insight into specific aspects of a system at a level of detail not possible for other simulation techniques and often not even accessible experimentally [1]. However, the ability to provide such specific information comes at the price of making such simulations extremely intensive computationally. Although so far high-performance computational approaches have addressed this requirement to a great extent and in the process have played a critical role in enhancing our understanding of biomolecular systems, it has been shown [2] that if larger and more \u201creal systems\u201d are to be simulated over meaningful timescales, then advances in both the algorithms and the computing approach used are imperative. In this article we will focus on new computing approaches that facilitate interactive simulations of large-scale systems and will discuss some of the issues that require attention when attempting to use such an approach effectively and in a routine manner. By interactive simulations, we mean simulations with which the end-user can interact though a visualization component (visualizer) and/or computational steering client in near real-time.", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Mining Security-Sensitive Operations in Legacy Code\n", "abstract": " A large amount of work remains in the area of retrofitting legacy systems for security. For example, it is necessary to improve our static analysis such that our results can scale better to even larger servers such as the Linux Server. What role can domainspecific and domain-independent constraints play in improving these results?", "num_citations": "1\n", "authors": ["1575"]}
{"title": "FiE on Firmware\n", "abstract": " FIE onFirmware Page 1 FiE on Firmware Finding Vulnerabilities in Embedded Systems using Symbolic Execution Drew Davidson Ben Moench Somesh Jha Thomas Ristenpart 1 Page 2 \u2022 Symbolic execution tailored to embedded firmware \u2013 Detects common firmware vulnerabilities \u2013 Deals with domain-specific challenges \u2013 Able to verify small programs \u2022 Tested on 99 programs \u2013 Found 22 bugs \u2013 Verified memory safety for 52 programs FiE in a Nutshell 2 Page 3 [Frisby et al., 2012] Example Attack: WOOT 2012 3 Encrypted card data Page 4 [Frisby et al., 2012] Command Secret Key Example Attack: WOOT 2012 4 16-bit low power device C firmware Low-level hardware interaction Buffer Overflow! Page 5 Embedded Systems: Lots of Attacks 5 \u2026 Little Work on Detecting Vulnerabilities Page 6 Embedded Systems: Lots of Attacks 6 \u2026 Little Work on Detecting Vulnerabilities Source code analysis is helpful on desktop \u2026", "num_citations": "1\n", "authors": ["1575"]}
{"title": "Using state space exploration and a natural deduction style message derivation engine to verify security protocols\n", "abstract": " As more resources are added to computer networks, and as more vendors look to the World Wide Web as a viable marketplace, the importance of being able to restrict access and to insure some kind of acceptable behavior even in the presence of malicious adversaries becomes paramount. Many researchers have proposed the use of security protocols to provide these security guarantees. In this paper, we develop a method of verifying these protocols using a special purpose model checker which executes an exhaustive state space search of a protocol model. Our tool also includes a natural deduction style derivation engine which models the capabilities of the adversary trying to attack the protocol. Because our models are necessarily abstractions, we cannot prove a protocol correct. However, our tool is extremely useful as a debugger. We have used our tool to analyze 14 different authentication protocols, and\u00a0\u2026", "num_citations": "1\n", "authors": ["1575"]}