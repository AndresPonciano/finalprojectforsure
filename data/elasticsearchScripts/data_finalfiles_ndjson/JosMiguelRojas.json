{"title": "Compositional symbolic execution through program specialization\n", "abstract": " Scalability is a major challenge in symbolic execution. The large number of paths that need to be explored and the large size of the constraints that must be carried often compromise the effectiveness of symbolic execution for software testing in practice. Compositional symbolic execution aims to alleviate these scalability issues by executing the methods of a program separately, stowing their results in method summaries and using such summaries to incrementally execute the complete program. We present a novel compositional approach that leverages partial evaluation, a well-established technique that aims at automatically specializing a program with respect to some of its input. We report on its design and implementation in Symbolic PathFinder and on preliminary promising evaluation results.", "num_citations": "19\n", "authors": ["1912"]}
{"title": "Test case generation by symbolic execution: basic concepts, a CLP-based instance, and actor-based concurrency\n", "abstract": " The focus of this tutorial is white-box test case generation (TCG) based on symbolic execution. Symbolic execution consists in executing a program with the contents of its input arguments being symbolic variables rather than concrete values. A symbolic execution tree characterizes the set of execution paths explored during the symbolic execution of a program. Test cases can be then obtained from the successful branches of the tree. The tutorial is split into three parts: (1) The first part overviews the basic techniques used in TCG to ensure termination, handling heap-manipulating programs, achieving compositionality in the process and guiding TCG towards interesting test cases. (2) In the second part, we focus on a particular implementation of the TCG framework in constraint logic programming (CLP). In essense, the imperative object-oriented program under test is automatically transformed into an\u00a0\u2026", "num_citations": "17\n", "authors": ["1912"]}
{"title": "Parallel simulation of NEPs on clusters\n", "abstract": " This paper compares two different approaches, followed by our research group, to efficiently run NEPs on parallel platforms, as general and transparent as possible. The vague results of jNEP (our multithreaded Java simulator for multicore desktop computers) suggests the use of massively parallel platforms (clusters of computers). The good results obtained show the scalability and viability of this last approach.", "num_citations": "13\n", "authors": ["1912"]}
{"title": "Compositional CLP-based test data generation for imperative languages\n", "abstract": " Glass-box test data generation (TDG) is the process of automatically generating test input data for a program by considering its internal structure. This is generally accomplished by performing symbolic execution of the program where the contents of variables are expressions rather than concrete values. The main idea in CLP-based TDG is to translate imperative programs into equivalent CLP ones and then rely on the standard evaluation mechanism of CLP to symbolically execute the imperative program. Performing symbolic execution on large programs becomes quickly expensive due to the large number and the size of paths that need to be explored. In this paper, we propose compositional reasoning in CLP-based TDG where large programs can be handled by testing parts (such as components, modules, libraries, methods, etc.) separately and then by composing the test cases obtained for these parts to\u00a0\u2026", "num_citations": "13\n", "authors": ["1912"]}
{"title": "Resource-Driven CLP-Based test case generation\n", "abstract": " Test Data Generation (TDG) aims at automatically obtaining test inputs which can then be used by a software testing tool to validate the functional behaviour of the program. In this paper, we propose resource-aware TDG, whose purpose is to generate test cases (from which the test inputs are obtained) with associated resource consumptions. The framework is parametric w.r.t. the notion of resource (it can measure memory, steps, etc.) and allows using software testing to detect bugs related to non-functional aspects of the program. As a further step, we introduce resource-driven TDG whose purpose is to guide the TDG process by taking resource consumption into account. Interestingly, given a resource policy, TDG is guided to generate test cases that adhere to the policy and avoid the generation of test cases which violate it.", "num_citations": "7\n", "authors": ["1912"]}
{"title": "A CLP heap solver for test case generation\n", "abstract": " One of the main challenges to software testing today is to efficiently handle heap-manipulating programs. These programs often build complex, dynamically allocated data structures during execution and, to ensure reliability, the testing process needs to consider all possible shapes these data structures can take. This creates scalability issues since high (often exponential) numbers of shapes may be built due to the aliasing of references. This paper presents a novel CLP heap solver for the test case generation of heap-manipulating programs that is more scalable than previous proposals, thanks to the treatment of reference aliasing by means of disjunction, and to the use of advanced back-propagation of heap related constraints. In addition, the heap solver supports the use of heap assumptions to avoid aliasing of data that, though legal, should not be provided as input.", "num_citations": "6\n", "authors": ["1912"]}
{"title": "A framework for guided test case generation in constraint logic programming\n", "abstract": " Performing test case generation by symbolic execution on large programs becomes quickly impracticable due to the path explosion problem. A common limitation that this problem poses is the generation of unnecessarily large number of possibly irrelevant or redundant test cases even for medium-size programs. Tackling the path explosion problem and selecting high quality test cases are considered major challenges in the software testing community. In this paper we propose a constraint logic programming-based framework to guide symbolic execution and thus test case generation towards a more relevant and potentially smaller subset of paths in the program under test. The framework is realized as a tool and empirical results demonstrate its applicability and effectiveness. We show how the framework can help to obtain high quality test cases and to alleviate the scalability issues that limit most symbolic\u00a0\u2026", "num_citations": "6\n", "authors": ["1912"]}
{"title": "Automated extraction of abstract behavioural models from JMS applications\n", "abstract": " Distributed systems are hard to program, understand and analyze. Two key sources of complexity are the many possible behaviors of a system, arising from the parallel execution of its distributed nodes, and the handling of asynchronous messages exchanged between nodes. We show how to systematically construct executable models of publish/subscribe systems based on the Java Messaging Service (JMS). These models, written in the Abstract Behavioural Specification (ABS) language, capture the essentials of the messaging behavior of the original Java systems, and eliminate details not related to distribution and messages. We report on jms2abs, a tool that automatically extracts ABS models from the bytecode of JMS systems. Since the extracted models are formal and executable, they allow us to reason about the modeled JMS systems by means of tools built specifically for the modeling language\u00a0\u2026", "num_citations": "4\n", "authors": ["1912"]}
{"title": "Automatic inference of bounds on resource consumption\n", "abstract": " In this tutorial paper, we overview the techniques that underlie the automatic inference of resource consumption bounds. We first explain the basic techniques on a Java-like sequential language. Then, we describe the extensions that are required to apply our method on concurrent ABS programs. Finally, we discuss some advanced issues in resource analysis, including the inference of non-cumulative resources and the treatment of shared mutable data.", "num_citations": "3\n", "authors": ["1912"]}
{"title": "Improvements in EOR Screening, Laboratory Flood Tests and Model Description to Effectively Fast Track EOR Projects\n", "abstract": " One of the operator's main concerns on EOR projects is related to the time to reach the pilot test stage. This paper aims to support a fast tracking of the implementation of Chemical EOR Projects. The proposed techniques and tools shorten this time and reduce cost associated to carry out key phases such as the EOR Screening, the Laboratory Core Flood and the Geological and Reservoir Modeling by making use of simple yet effective techniques.", "num_citations": "2\n", "authors": ["1912"]}
{"title": "Diagnosing software faults using multiverse analysis\n", "abstract": " Spectrum-based Fault Localization (SFL) approaches aim to efficiently localize faulty components from examining program behavior. This is done by collecting the execution patterns of various combinations of components and the corresponding outcomes into a spectrum. Efficient fault localization depends heavily on the quality of the spectra. Previous approaches, including the current state-of-the-art Density-Diversity-Uniqueness (DDU) approach, attempt to generate \u201cgood\u201d testsuites by improving certain structural properties of the spectra. In this work, we propose a different approach, Multiverse Analysis, that considers multiple hypothetical universes, each corresponding to a scenario where one of the components is assumed to be faulty, to generate a spectrum that attempts to reduce the expected worst-case wasted effort over all the universes. Our experiments show that the Multiverse Analysis not just improves the efficiency of fault localization but also achieves better coverage and generates smaller test-suites over DDU, the current state-of-the-art technique. On average, our approach reduces the developer effort over DDU by over 16% for more than 92% of the instances. Further, the improvements over DDU are indeed statistically significant on the paired Wilcoxon Signed-rank test.", "num_citations": "1\n", "authors": ["1912"]}