{"title": "Mistral: Dynamically managing power, performance, and adaptation cost in cloud infrastructures\n", "abstract": " Server consolidation based on virtualization is an important technique for improving power efficiency and resource utilization in cloud infrastructures. However, to ensure satisfactory performance on shared resources under changing application workloads, dynamic management of the resource pool via online adaptation is critical. The inherent tradeoffs between power and performance as well as between the cost of an adaptation and its benefits make such management challenging. In this paper, we present Mistral, a holistic controller framework that optimizes power consumption, performance benefits, and the transient costs incurred by various adaptations and the controller itself to maximize overall utility. Mistral can handle multiple distributed applications and large-scale infrastructures through a multi-level adaptation hierarchy and scalable optimization algorithm. We show that our approach outstrips other\u00a0\u2026", "num_citations": "371\n", "authors": ["529"]}
{"title": "An exploration of L2 cache covert channels in virtualized environments\n", "abstract": " Recent exploration into the unique security challenges of cloud computing have shown that when virtual machines belonging to different customers share the same physical machine, new forms of cross-VM covert channel communication arise. In this paper, we explore one of these threats, L2 cache covert channels, and demonstrate the limits of these this threat by providing a quantification of the channel bit rates and an assessment of its ability to do harm. Through progressively refining models of cross-VM covert channels from the derived maximums, to implementable channels in the lab, and finally in Amazon EC2 itself we show how a variety of factors impact our ability to create effective channels. While we demonstrate a covert channel with considerably higher bit rate than previously reported, we assess that even at such improved rates, the harm of data exfiltration from these channels is still limited to the\u00a0\u2026", "num_citations": "301\n", "authors": ["529"]}
{"title": "Coyote: A system for constructing fine-grain configurable communication services\n", "abstract": " Communication-oriented abstractions such as atomic multicast, group RPC, and protocols for location-independent mobile computing can simplify the development of complex applications built on distributed systems. This article describes Coyote, a system that supports the construction of highly modular and configurable versions of such abstractions. Coyote extends the notion of protocol objects and hierarchical composition found in existing systems with support for finer-grain microprotocol objects and a nonhierarchical composition scheme for use within a single layer of a protocol stack. A customized service is constructed by selecting microprotocols based on their semantic guarantees and configuring them together with a standard runtime system to form a composite protocol  implementing   the service. This composite protocol is then composed hierarchically with other protocols to form a complete network\u00a0\u2026", "num_citations": "239\n", "authors": ["529"]}
{"title": "Generating adaptation policies for multi-tier applications in consolidated server environments\n", "abstract": " Creating good adaptation policies is critical to building complex autonomic systems since it is such policies that define the system configuration used in any given situation. While online approaches based on control theory and rule- based expert systems are possible solutions, each has its disadvantages. Here, a hybrid approach is described that uses modeling and optimization offline to generate suitable configurations, which are then encoded as policies that are used at runtime. The approach is demonstrated on the problem of providing dynamic management in virtualized consolidated server environments that host multiple multi-tier applications. Contributions include layered queuing models for Xen-based virtual machine environments, a novel optimization technique that uses a combination of bin packing and gradient search, and experimental results that show that automatic offline policy generation is viable\u00a0\u2026", "num_citations": "202\n", "authors": ["529"]}
{"title": "Constructing adaptive software in distributed systems\n", "abstract": " Adaptive software that can react to changes in the execution environment or user requirements by switching algorithms at run time is powerful yet difficult to implement, especially in distributed systems. This paper describes a software architecture for constructing such adaptive software and a graceful adaptive protocol that allows adaptations to be made in a coordinated manner across hosts transparently to the application. A realization of the architecture based on Cactus, a system for constructing highly configurable distributed services and protocols, is also presented. The approach is illustrated by outlining examples of adaptive components from a group communication service.", "num_citations": "193\n", "authors": ["529"]}
{"title": "A cost-sensitive adaptation engine for server consolidation of multitier applications\n", "abstract": " Virtualization-based server consolidation requires runtime resource reconfiguration to ensure adequate application isolation and performance, especially for multitier services that have dynamic, rapidly changing workloads and responsiveness requirements. While virtualization makes reconfiguration easy, indiscriminate use of adaptations such as VM replication, VM migration, and capacity controls has performance implications. This paper demonstrates that ignoring these costs can have significant impacts on the ability to satisfy response-time-based SLAs, and proposes a solution in the form of a cost-sensitive adaptation engine that weighs the potential benefits of runtime reconfiguration decisions against their costs. Extensive experimental results based on live workload traces show that the technique is able to maximize SLA fulfillment under typical time-of-day workload variations as well as flash crowds\u00a0\u2026", "num_citations": "158\n", "authors": ["529"]}
{"title": "Nfsight: netflow-based network awareness tool\n", "abstract": " Network awareness is highly critical for network and security administrators. It enables informed planning and management of network resources, as well as detection and a comprehensive understanding of malicious activity. It requires a set of tools to efficiently collect, process, and represent network data. While many such tools already exist, there is no flexible and practical solution for visualizing network activity at various granularities, and quickly gaining insights about the status of network assets. To address this issue, we developed Nfsight, a Net-Flow processing and visualization application designed to offer a comprehensive network awareness solution. Nfsight constructs bidirectional flows out of the unidirectional NetFlow flows and leverages these bidirectional flows to provide client/server identification and intrusion detection capabilities. We present in this paper the internal architecture of Nfsight, the evaluation of the service, and intrusion detection algorithms. We illustrate the contributions of Nfsight through several case studies conducted by security administrators on a large university network.", "num_citations": "123\n", "authors": ["529"]}
{"title": "A configurable and extensible transport protocol\n", "abstract": " The ability to configure transport protocols from collections of smaller software modules allows the characteristics of the protocol to be customized for a specific application or network technology. This paper describes an approach to building such customized protocols using Cactus, a system in which micro-protocols implementing individual attributes of transport can be combined into a composite protocol that realizes the desired overall functionality. In contrast with similar systems, Cactus supports non-hierarchical module composition and event-driven execution, both of which increase flexibility and allow finer-grain modules implementing orthogonal properties. To illustrate this approach, the design and implementation of a configurable transport protocol called CTP is presented. CTP allows customization of a number of properties including reliable transmission, congestion detection and control, jitter control, and\u00a0\u2026", "num_citations": "112\n", "authors": ["529"]}
{"title": "Jettison: Efficient idle desktop consolidation with partial VM migration\n", "abstract": " Idle desktop systems are frequently left powered, often because of applications that maintain network presence or to enable potential remote access. Unfortunately, an idle PC consumes up to 60% of its peak power. Solutions have been proposed that perform consolidation of idle desktop virtual machines. However, desktop VMs are often large requiring gigabytes of memory. Consolidating such VMs, creates bulk network transfers lasting in the order of minutes, and utilizes server memory inefficiently. When multiple VMs migrate simultaneously, each VM's experienced migration latency grows, and this limits the use of VM consolidation to environments in which only a few daily migrations are expected for each VM. This paper introduces Partial VM Migration, a technique that transparently migrates only the working set of an idle VM. Jettison, our partial VM migration prototype, can deliver 85% to 104% of the energy\u00a0\u2026", "num_citations": "100\n", "authors": ["529"]}
{"title": "Dynamically allocating multitier applications based upon application requirements and performance and reliability of resources\n", "abstract": " The present disclosure relates to dynamically allocating multitier applications based upon performance and reliability of resources. A controller analyzes resources and applications hosted by the resources, and collects operational data relating to the applications and resources. The controller is configured to determine an allocation scheme for allocating or reallocating the applications upon failure of a resource and/or upon rollout or distribution of a new application. The controller generates configuration data that describes steps for implementing the allocation scheme. The resources are monitored, in some embodiments, by monitoring devices. The monitoring devices collect and report the operational information and generate alarms if resources fail.", "num_citations": "97\n", "authors": ["529"]}
{"title": "Survivability through customization and adaptability: The cactus approach\n", "abstract": " Survivability, the ability of a system to tolerate intentional attacks or accidental failures or errors, is becoming increasingly important with the extended use of computer systems in society. While techniques such as cryptographic methods, intrusion detection, and traditional fault tolerance are currently being used to improve the survivability of such systems, new approaches are needed to help reach the levels that will be required in the near future. This paper proposes the use of fine-grain customization and dynamic adaptation as key enabling technologies in a new approach designed to achieve this goal. Customization not only supports software diversity, but also allows customized tradeoffs to be made between different QoS attributes including performance, security, reliability and survivability. Dynamic adaptation allows survivable services to change their behavior at runtime as a reaction to anticipated or detected\u00a0\u2026", "num_citations": "97\n", "authors": ["529"]}
{"title": "Real-time dependable channels: Customizing QoS attributes for distributed systems\n", "abstract": " Communication services that provide enhanced Quality of Service (QoS) guarantees related to dependability and real time are important for many applications in distributed systems. This paper presents real-time dependable (RTD) channels, a communication-oriented abstraction that can be configured to meet the QoS requirements of a variety of distributed applications. This customization ability is based on using CactusRT, a system that supports the construction of middleware services out of software modules called micro-protocols. Each micro-protocol implements a different semantic property or property variant and interacts with other micro-protocols using an event-driven model supported by the CactusRT runtime system. In addition to RTD channels CactusRT and its implementation are described. This prototype executes on a cluster of Pentium PCs running the OpenGroup/RI MK 7.3 Mach real-time operating\u00a0\u2026", "num_citations": "92\n", "authors": ["529"]}
{"title": "Adaptive distributed and fault-tolerant systems\n", "abstract": " An adaptive computing system is one that modifies its behavior based on changes in the environment. Since sites connected by a local-area network inherently have to deal with network congestion and the failure of other sites, distributed systems can be viewed as an important subclass of adaptive systems. As such, use of adaptive methods in this context has the same potential advantages of improved efficiency and structural simplicity as for adaptive systems in general. This paper describes a model for adaptive systems that can be applied in many scenarios arising in distributed and fault-tolerant systems. This model divides the adaptation process into three different phases\u2014change detection, agreement, and action\u2014that can be used to describe existing algorithms that deal with change, as well as to develop new adaptive algorithms. In addition to clarifying the logical structure of such algorithms, this model can also serve as a unifying implementation framework. Several adaptive algorithms are given as examples, including an adaptive network transmission protocol and a group membership protocol. A technique for implementing the model in a distributed system using small segments of code called micro-protocols and an event-driven execution paradigm is also presented.", "num_citations": "86\n", "authors": ["529"]}
{"title": "Performance and availability aware regeneration for cloud based multitier applications\n", "abstract": " Virtual machine technology enables agile system deployments in which software components can be cheaply moved, replicated, and allocated hardware resources in a controlled fashion. This paper examines how these facilities can be used to provide enhanced solutions to the classic problem of ensuring high availability while maintaining performance. By regenerating software components to restore the redundancy of a system whenever failures occur, we achieve improved availability compared to a system with a fixed redundancy level. Moreover, by smartly controlling component placement and resource allocation using information about application control flow and performance predictions from queuing models, we ensure that the resulting performance degradation is minimized. We consider an environment in which a collection of multitier enterprise applications operates across multiple hosts, racks, clusters\u00a0\u2026", "num_citations": "81\n", "authors": ["529"]}
{"title": "Kaleidoscope: cloud micro-elasticity via vm state coloring\n", "abstract": " We introduce cloud micro-elasticity, a new model for cloud Virtual Machine (VM) allocation and management. Current cloud users over-provision long-lived VMs with large memory footprints to better absorb load spikes, and to conserve performance-sensitive caches. Instead, we achieve elasticity by swiftly cloning VMs into many transient, short-lived, fractional workers to multiplex physical resources at a much finer granularity. The memory of a micro-elastic clone is a logical replica of the parent VM state, including caches, yet its footprint is proportional to the workload, and often a fraction of the nominal maximum. We enable micro-elasticity through a novel technique dubbed VM state coloring, which classifies VM memory into sets of semantically-related regions, and optimizes the propagation, allocation and deduplication of these regions. Using coloring, we build Kaleidoscope and empirically demonstrate its ability\u00a0\u2026", "num_citations": "80\n", "authors": ["529"]}
{"title": "The Cactus approach to building configurable middleware services\n", "abstract": " A number of fundamental abstractions and supporting software mechanisms have been developed for simplifying the problems associated with programming highly dependable distributed systems. For example, transactions provide all or nothing execution despite failures, while ordered atomic multicast supports the replicated state machine approach to fault tolerance by ensuring that changes to the state machine are delivered atomically and in a consistent order despite failures. All of these provide a higher level virtual machine on which to build applications by abstracting away details such as the effect of failures, message reordering and losses, and unconstrained concurrent execution. Each is appropriate for different kinds of applications with different requirements.Many of the concrete realizations of these abstractions have been built as middleware services, ie, software that is logically layered below the application and above the operating system. For each type of abstraction, however, there are often multiple different ways to define the exact semantics that it provides or other attributes, such as the type of failures it is designed to tolerate. This is especially true for multicast, which have been the basis for a large number of group communication services with varying semantics, including Isis [4], Horus [13], Transis [2], Totem [12], Consul [11], xAMP [14], and RTcast [1]. Often, each is different because it is oriented towards a particular type of application or a given execution environment. Our thesis is that the use of highly configurable software can make it easier to construct applications by simplifying the process of building custom middleware\u00a0\u2026", "num_citations": "78\n", "authors": ["529"]}
{"title": "Reliability techniques for RFID-based object tracking applications\n", "abstract": " Radio Frequency Identification (RFID) technology has the potential to dramatically improve numerous industrial practices. However, it still faces many challenges, including security and reliability, which may limit its use in many application scenarios. While security has received considerable attention, reliability has escaped much of the research scrutiny. In this work, we investigate the reliability challenges in RFID-based tracking applications, where objects (e.g., pallets, packages, and people) tagged with low-cost passive RFID tags pass by the RFID reader's read zone. Our experiments show that the reliability of tag identification is affected by several factors, including the inter-tag distance, the distance between the tag and antenna, the orientation of the tag with respect to the antenna, and the location of the tag on the object. We demonstrate that RFID system reliability can be significantly improved with the\u00a0\u2026", "num_citations": "74\n", "authors": ["529"]}
{"title": "Properties of membership services\n", "abstract": " A membership service is used in a distributed system to maintain information about which sites are functioning and which have failed at any given time. Such services have proven to be fundamental for constructing distributed applications, with many example services and algorithms defined in the literature. Despite these efforts, however, little has been done on examining the abstract properties important to membership independent of a given service. Here, these properties are identified and characterized. Message ordering graphs are used to specify the effect of each property on the message flow as seen by the application, and dependency graphs are used to characterize the relationship between properties. These graphs help differentiate existing services, as well as facilitate the design of new services in which only those properties actually required by an application are included.< >", "num_citations": "69\n", "authors": ["529"]}
{"title": "Enabling vehicular safety applications over LTE networks\n", "abstract": " Advanced Driving Assistance Systems (ADAS) for improving vehicular safety are increasingly network based, with approaches that use vehicle to vehicle (V2V) and vehicle to infrastructure (V2I) communication. Most current proposals for V2V and V2I use DSRC and a dedicated infrastructure of road side units (RSUs) for the V2I scenarios. Here, the technical feasibility of an alternative architecture is explored, one that uses a combination of LTE cellular networks and servers near the edge of the LTE network. Compared with approaches based on DSRC and RSUs, this architecture exploits an infrastructure that is already largely deployed, but requires that technical challenges related to latency and scalability be addressed. This paper outlines an architecture that addresses these challenges and shows experimental results that demonstrate its effectiveness for vehicular safety applications. The approach combines\u00a0\u2026", "num_citations": "66\n", "authors": ["529"]}
{"title": "System call monitoring using authenticated system calls\n", "abstract": " System call monitoring is a technique for detecting and controlling compromised applications by checking at runtime that each system call conforms to a policy that specifies the program's normal behavior. Here, we introduce a new approach to implementing system call monitoring based on authenticated system calls. An authenticated system call is a system call augmented with extra arguments that specify the policy for that call, and a cryptographic message authentication code that guarantees the integrity of the policy and the system call arguments. This extra information is used by the kernel to verify the system call. The version of the application in which regular system calls have been replaced by authenticated calls is generated automatically by an installer program that reads the application binary, uses static analysis to generate policies, and then rewrites the binary with the authenticated calls. This paper\u00a0\u2026", "num_citations": "65\n", "authors": ["529"]}
{"title": "iMobile EE\u2013An Enterprise Mobile Service Platform\n", "abstract": " iMobile is an enterprise mobile service platform that allows resource-limited mobile devices to communicate with each other and to securely access corporate contents and services. The original iMobile architecture consists of devlets that provide protocol interfaces to different mobile devices and infolets that access and transcode information based on device profiles. iMobile Enterprise Edition (iMobile EE) is a redesign of the original iMobile architecture to address the security, scalability, and availability requirements of a large enterprise such as AT&T. iMobile EE incorporates gateways that interact with corporate authentication services, replicated iMobile servers with backend connections to corporate services, a reliable message queue that connects iMobile gateways and servers, and a comprehensive service profile database that governs operations of the mobile service platform. The iMobile EE\u00a0\u2026", "num_citations": "65\n", "authors": ["529"]}
{"title": "Automatic model-driven recovery in distributed systems\n", "abstract": " Automatic system monitoring and recovery has the potential to provide a low-cost solution for high availability. However, automating recovery is difficult in practice because of the challenge of accurate fault diagnosis in the presence of low coverage, poor localization ability, and false positives that are inherent in many widely used monitoring techniques. In this paper, we present a holistic model-based approach that overcomes these challenges and enables automatic recovery in distributed systems. To do so, it uses theoretically sound techniques including Bayesian estimation and Markov decision theory to provide controllers that choose good, if not optimal, recovery actions according to a user-defined optimization criteria. By combining monitoring and recovery, the approach realizes benefits that could not have been obtained by using them in isolation. In this paper, we present two recovery algorithms with\u00a0\u2026", "num_citations": "63\n", "authors": ["529"]}
{"title": "Building survivable services using redundancy and adaptation\n", "abstract": " Survivable systems-that is, systems that can continue to provide service despite failures, intrusions, and other threats-are increasingly needed in a wide variety of civilian and military application areas. As a step toward realizing such systems, this paper advocates the use of redundancy and adaptation to build survivable services that can provide core functionality for implementing survivability in networked environments. An approach to building such services using these techniques is described and a concrete example involving a survivable communication service is given. This service is based on Cactus, a system for building highly configurable network protocols that offers the flexibility needed to easily add redundant and adaptive components. Initial performance results for a prototype implementation of the communication service built using Cactus/C2.1 running on Linux are also given.", "num_citations": "63\n", "authors": ["529"]}
{"title": "A configurable membership service\n", "abstract": " A membership service is used to maintain information about which sites are functioning in a distributed system at any given time. Many such services have been defined, with each implementing a unique combination of properties that simplify the construction of higher levels of the system. Despite this wealth of possibilities, however, any given service typically realizes only one set of properties, which makes it difficult to tailor the service provided to the specific needs of the application. Here, a configurable membership service that addresses this problem is described. This service is based on decomposing membership into its constituent abstract properties and then implementing these properties as separate software modules called micro-protocols that can be configured together to produce a customized membership service. A prototype C++ implementation of the membership service for a simulated distributed\u00a0\u2026", "num_citations": "60\n", "authors": ["529"]}
{"title": "Tagging a copy of memory of a virtual machine with information for fetching of relevant portions of the memory\n", "abstract": " Methods and apparatus are disclosed to provision virtual machine resources. An example method includes labeling a copy of memory associated with an established virtual machine with an execution status based on an architecture type associated with the copy, and constraining a fetch operation in response to a page fault to a labeled portion of the copy that matches an architecture type of a received processor instruction.", "num_citations": "55\n", "authors": ["529"]}
{"title": "An approach to constructing modular fault-tolerant protocols\n", "abstract": " Modularization is a well-known technique for simplifying complex software. An approach to modularizing fault-tolerant protocols such as reliable multicast and membership is described. The approach is based on implementing a protocol's individual properties as separate microprotocols and then combining selected microprotocols using an event-driven software framework. A system is constructed by composing these frameworks with traditional network protocols using standard hierarchical techniques. In addition to simplifying the software, this model helps clarify the dependencies among properties of fault-tolerant protocols and makes it possible to construct systems that are customized to the specifics of the application or underlying architecture. An example involving reliable group multicast is given, together with a description of a prototype implementation using the SR concurrent programming language.< >", "num_citations": "53\n", "authors": ["529"]}
{"title": "Content-based scheduling of virtual machines (VMs) in the cloud\n", "abstract": " Organizations of all sizes are shifting their IT infrastructures to the cloud because of its cost efficiency and convenience. Because of the on-demand nature of the Infrastructure as a Service (IaaS) clouds, hundreds of thousands of virtual machines (VMs) may be deployed and terminated in a single large cloud data center each day. In this paper, we propose a content-based scheduling algorithm for the placement of VMs in data centers. We take advantage of the fact that it is possible to find identical disk blocks in different VM disk images with similar operating systems by scheduling VMs with high content similarity on the same hosts. That allows us to reduce the amount of data transferred when deploying a VM on a destination host. In this paper, we first present our study of content similarity between different VMs, based on a large set of VMs with different operating systems that represent the majority of popular\u00a0\u2026", "num_citations": "52\n", "authors": ["529"]}
{"title": "Self-management of adaptable component-based applications\n", "abstract": " The problem of self-optimization and adaptation in the context of customizable systems is becoming increasingly important with the emergence of complex software systems and unpredictable execution environments. Here, a general framework for automatically deciding on when and how to adapt a system whenever it deviates from the desired behavior is presented. In this framework, the system's target behavior is described as a high-level policy that establishes goals for a set of performance indicators. The decision process is based on information provided independently for each component that describes the available adaptations, their impact on performance indicators, and any limitations or requirements. The technique consists of both offline and online phases. Offline, rules are generated specifying component adaptations that may help to achieve the established goals when a given change in the execution\u00a0\u2026", "num_citations": "46\n", "authors": ["529"]}
{"title": "Constructing a configurable group RPC service\n", "abstract": " Current Remote Procedure Call (RPC) services implement a variety of semantics, with many of the differences related to how communication and server failures are handled. The list increases even more when considering group RPC, a variant of RPC often used for fault-tolerance where an invocation is sent to a group of servers rather than one. This paper presents an approach to constructing group RPC in which a single configurable system is used to build different variants of the service. The approach is based on implementing each property as a separate software module called a micro-protocol, and then configuring the micro-protocols needed to implement the desired service together using a software framework based on the x-kernel. The properties of point-to-point and group RPC are identified and classified, and the general execution model described. An example consisting of a modular implementation of\u00a0\u2026", "num_citations": "44\n", "authors": ["529"]}
{"title": "A configurable and extensible transport protocol\n", "abstract": " The ability to configure transport protocols from collections of smaller software modules allows the characteristics of the protocol to be customized for a specific application or network technology. This paper describes a configurable transport protocol system called CTP in which microprotocols implementing individual attributes of transport can be combined into a composite protocol that realizes the desired overall functionality. In addition to describing the overall architecture of CTP and its microprotocols, this paper also presents experiments on both local area and wide area platforms that illustrate the flexibility of CTP and how its ability to match more closely application needs can result in better application performance. The prototype implementation of CTP has been built using the C version of the Cactus microprotocol composition framework running on Linux.", "num_citations": "43\n", "authors": ["529"]}
{"title": "Secure multi-party device pairing using sensor data\n", "abstract": " Content is securely shared between communication devices in an ad-hoc manner by employing common sensing context to establish pairing between the communication devices. In one aspect, the communication devices are within a specified distance from each other and sense common signals from their environment over a specified time period. The common signals are analyzed to determine an initialization or session key, which is utilized to secure content transfer between the communication devices. Additionally or alternatively, the key is utilized to provide access to virtual (eg, digital content) and/or physical (eg, buildings) resources.", "num_citations": "42\n", "authors": ["529"]}
{"title": "Systems, devices, and methods for initiating recovery\n", "abstract": " Certain exemplary embodiments comprise method that can comprise receiving information indicative of a fault from a monitor associated a network. The method can comprise, responsive to the information indicative of the fault, automatically determining a probability of a fault hypothesis. The method can comprise, responsive to a determination of the fault hypothesis, automatically initiating a recovery action to correct the fault.", "num_citations": "39\n", "authors": ["529"]}
{"title": "Authenticated system calls\n", "abstract": " System call monitoring is a technique for detecting and controlling compromised applications by checking at runtime that each system call conforms to a policy that specifies the program's normal behavior. A new approach to system call monitoring based on authenticated system calls is introduced. An authenticated system call is a system call augmented with extra arguments that specify the policy for that call and a cryptographic message authentication code (MAC) that guarantees the integrity of the policy and the system call arguments. This extra information is used by the kernel to verify the system call. The version of the application in which regular system calls have been replaced by authenticated calls is generated automatically by an installer program that reads the application binary, uses static analysis to generate policies, and then rewrites the binary with the authenticated calls. This paper presents the\u00a0\u2026", "num_citations": "36\n", "authors": ["529"]}
{"title": "Blackbox prediction of the impact of DVFS on end-to-end performance of multitier systems\n", "abstract": " Dynamic voltage and frequency scaling (DVFS) is a well-known technique for gaining energy savings on desktop and laptop computers. However, its use in server settings requires careful consideration of any potential impacts on end-to-end service performance of hosted applications. In this paper, we develop a simple metric called the \\frequency gradient\" that allows prediction of the impact of changes in processor frequency on the end-to-end transaction response times of multitier applications. We show how frequency gradients can be measured on a running system in a push-button manner without any prior knowledge of application semantics, structure, or configuration settings. Using experimental results, we demonstrate that the frequency gradients provide accurate predictions, and enable end-to-end performance-aware DVFS for mulitier applications.", "num_citations": "35\n", "authors": ["529"]}
{"title": "Enhancing survivability of security services using redundancy\n", "abstract": " Traditional distributed system services that provide guarantees related to confidentiality, integrity, and authenticity enhance security, but are not survivable since each attribute is implemented by a single method. This paper advocates the use of redundancy to increase survivability by using multiple methods to implement each security attribute and doing so in ways that can vary unpredictably. As a concrete example, the design and implementation of a highly configurable secure communication service called SecComm are presented. The service has been implemented using Cactus, a system for building highly configurable protocols and services for distributed systems. Initial performance results for a prototype implementation on Linux are also given.", "num_citations": "32\n", "authors": ["529"]}
{"title": "Cholla: A framework for composing and coordinating adaptations in networked systems\n", "abstract": " The ability of networked system software to adapt in a controlled manner to changes in the environment and requirements is crucial, but difficult to realize in complex systems with multiple interacting software layers/components. Typically, many components in a networked system implement adaptive behaviors and encapsulate their own adaptation logic (policies), making the whole system's adaptive behavior hard to analyze, coordinate, and test. This paper describes Cholla, a software architecture that separates the policy decisions of how and when adaptive components in networked systems react to their environment into separate centralized controllers that are constructed from composable rule sets. Centralizing policy decisions into controllers in Cholla facilitates the analysis, coordination, and testing of these policies, while the composable nature of these controllers allows them to be customized to changing\u00a0\u2026", "num_citations": "31\n", "authors": ["529"]}
{"title": "Providing QoS customization in distributed object systems\n", "abstract": " Applications built on networked collections of computers are increasingly using distributed object platforms such as CORBA, Java RMI, and DCOM to standardize object interactions. With this increased use comes the increased need for enhanced Quality of Service (QoS) attributes related to fault tolerance, security, and timeliness. This paper describes an architecture called CQoS (Configurable QoS) for implementing such enhancements in a transparent, highly customizable, and portable manner. CQoS consists of two parts: application- and platform-dependent interceptors and generic QoS components. The generic QoS components are implemented using Cactus, a system for building highly configurable protocols and services in distributed systems. The CQoS architecture and the interfaces between the different components are described, together with implementations of QoS attributes using Cactus\u00a0\u2026", "num_citations": "31\n", "authors": ["529"]}
{"title": "Supporting coordinated adaptation in networked systems\n", "abstract": " Summary form only given. Our position is that the true potential of adaptation can only be realized if support is provided for more general solutions, including adaptations that span multiple hosts and multiple system components, and algorithmic adaptations that involve changing the underlying algorithms used by the system at runtime. Such a general solution must, however, address the difficult issues related to these types of adaptations. Adaptation by multiple related components, for example, must be coordinated so that these adaptations work together to implement consistent adaptation policies. Likewise, large-scale algorithmic adaptations need to be coordinated using graceful adaptation strategies in which as much normal processing as possible continues during the changeover. Here, we summarize our approach to addressing these problems in Cactus, a system for constructing highly-configurable\u00a0\u2026", "num_citations": "31\n", "authors": ["529"]}
{"title": "Configuration management for highly customisable software\n", "abstract": " Customisable operating systems, database systems, and communication subsystems have demonstrated many advantages of customisation, including considerable performance improvements. One common approach for constructing customisable software is to implement it as a collection of modules that can be configured in different combinations to provide customised variants of the software. Typically, ad hoc methods are used to determine which modules may be combined. Such methods require intimate knowledge of the modules and their interactions or the configuration will not behave as expected. In this paper, we present a methodology that simplifies the difficult task of constructing correct custom variants of highly customisable software. The methodology is based on identifying relations between software modules that dictate which combinations are correct. We also introduce a configuration support tool\u00a0\u2026", "num_citations": "30\n", "authors": ["529"]}
{"title": "Energy-oriented partial desktop virtual machine migration\n", "abstract": " Modern offices are crowded with personal computers. While studies have shown these to be idle most of the time, they remain powered, consuming up to 60% of their peak power. Hardware-based solutions engendered by PC vendors (e.g., low-power states, Wake-on-LAN) have proved unsuccessful because, in spite of user inactivity, these machines often need to remain network active in support of background applications that maintain network presence. Recent proposals have advocated the use of consolidation of idle desktop Virtual Machines (VMs). However, desktop VMs are often large, requiring gigabytes of memory. Consolidating such VMs creates large network transfers lasting in the order of minutes and utilizes server memory inefficiently. When multiple VMs migrate concurrently, networks become congested, and the resulting migration latencies are prohibitive. We present partial VM migration, an\u00a0\u2026", "num_citations": "27\n", "authors": ["529"]}
{"title": "Supporting customized failure models for distributed software\n", "abstract": " The cost of employing software fault tolerance techniques in distributed systems is strongly related to the type of failures to be tolerated. For example, in terms of the amount of redundancy required and execution time, tolerating a processor crash is much cheaper than tolerating arbitrary (or Byzantine) failures. This paper describes an approach to constructing configurable services for distributed systems that allows easy customization of the type of failures to tolerate. Using this approach, it is possible to configure custom services across a spectrum of possibilities, from a very efficient but unreliable server group that does not tolerate any failures, to a less efficient but reliable group that tolerates crash, omission, timing, or arbitrary failures. The approach is based on building configurable services as collections of software modules called micro-protocols. Each micro-protocol implements a different semantic property or\u00a0\u2026", "num_citations": "27\n", "authors": ["529"]}
{"title": "Membership and system diagnosis\n", "abstract": " A membership service is a service in a distributed system that maintains and provides information about which sites are functioning and which have failed at any given time. System diagnosis, on the other hand, is a method for detecting faulty processing elements and distributing this information to non-faulty elements. In spite of the apparent similarity of goals, these two fields have been considered separately from their beginnings. In this paper, we attempt to compare these fields and show the fundamental differences and the similarities. We demonstrate that the problems are closely related with the major differences being the assumptions made about the failure model, the testing methods, and the type of service guarantees provided to the application. Furthermore, we demonstrate that the fields are closely enough related that some algorithms utilized in one field can easily be transformed into algorithms in the\u00a0\u2026", "num_citations": "27\n", "authors": ["529"]}
{"title": "Probabilistic model-driven recovery in distributed systems\n", "abstract": " Automatic system monitoring and recovery has the potential to provide effective, low-cost ways to improve dependability in distributed software systems. However, automating recovery is challenging in practice because accurate fault diagnosis is hampered by monitoring tools and techniques that often have low fault coverage, poor fault localization, detection delays, and false positives. In this paper, we present a holistic model-based approach that overcomes these challenges and enables automatic recovery in distributed systems. To do so, it uses theoretically sound techniques including Bayesian estimation and Markov decision theory to provide controllers that choose good, if not optimal, recovery actions according to a user-defined optimization criteria. By combining monitoring and recovery, the approach realizes benefits that could not have been obtained by using them in isolation. We experimentally validate\u00a0\u2026", "num_citations": "26\n", "authors": ["529"]}
{"title": "Link gradients: Predicting the impact of network latency on multitier applications\n", "abstract": " Geographically dispersed deployments of large and complex multitier enterprise applications introduce many challenges, including those involved in predicting the impact of network latency on end-to-end transaction response times. Here, a measurement-based approach to quantifying this impact using a new metric called the link gradient is presented. A non-intrusive technique for measuring the link gradient in running systems using delay injection and spectral analysis is presented, along with experimental results on PlanetLab that demonstrate that the link gradient can be used to predict end-to-end responsiveness, even in new and untested application configurations.", "num_citations": "25\n", "authors": ["529"]}
{"title": "Customizable service state durability for service oriented architectures\n", "abstract": " Many techniques are known for improving service dependability in service oriented architectures, but each technique has its tradeoffs relative to runtime performance overhead, resource cost, and level of assurance provided. The ability to choose the particular techniques used based on the service requirements and characteristics allows the optimization of these tradeoffs. This paper proposes making durability a customizable attribute of service states, and describes a proof of concept implementation for Web services that allows different durability mechanisms (e.g., in-memory replication or database storage) to be implemented as separate modules and then applied as needed to different variables and data structures constituting the service state. Preliminary performance results are presented using a prototype implementation of a highly available matchmaker service", "num_citations": "25\n", "authors": ["529"]}
{"title": "The impact of web service integration on grid performance\n", "abstract": " The past few years have seen an increasingly tight link between grid computing and Web services, with the latest standards defining a grid computing architecture as a set of services built using Web services standards and protocols. However, the reputation of these technologies (SOAP, XML, WSDL, HTTP) is that they are heavyweight and slow, something that is potentially a concern given the current and anticipated application mix for high performance grid architectures. This paper reports the results of a performance evaluation carried out on Globus 3.9.4, a reference implementation of the new GGF standards that are built on the Web services resource framework (WSRF). The evaluation approach combines low interference measurement (black box) techniques with more sophisticated sampling-based profiling (gray box) techniques. The results indicate possible opportunities for optimization, as well as provide\u00a0\u2026", "num_citations": "25\n", "authors": ["529"]}
{"title": "Reflections on aspects and configurable protocols\n", "abstract": " The goals of aspect oriented software development (AOSD) and frameworks for configurable protocols (CPs) are similar in many respects. AOSD allows the specification of cross-cutting concerns called aspects as separate modules that are woven with the base program as needed. CPs are oriented towards building protocols or services with different quality of service (QoS) properties and attributes out of collections of independent modules, with each configuration customizing the service for a given application and execution environment. As AOSD evolves to address issues in areas such as middleware, operating systems, and distributed computing that have traditionally been the domain of CPs, lessons learned from the development of these frameworks could be useful. The purpose of this paper is to draw parallels between AOSD and CP frameworks, with a specific focus on the Cactus framework and how it\u00a0\u2026", "num_citations": "24\n", "authors": ["529"]}
{"title": "A high performance configurable transport protocol for grid computing\n", "abstract": " Grid computing infrastructures and applications are increasingly diverse with networks ranging from very high bandwidth optical networks to wireless networks and applications ranging from remote visualization to sensor data collection. For such environments, standard transport protocols such as TCP and UDP are not always sufficient or optimal given their fixed set of properties and their lack of flexibility. As an alternative, we present H-CTP, a high-performance configurable transport protocol that can be used to build customized transport services for a wide range of grid computing scenarios. H-CTP is based on an earlier configurable transport protocol called CTP, but with a collection of optimizations that meet the challenge of providing configurability while maintaining performance that meets the requirements of such demanding applications. This paper motivates the need for customizable transport in this area\u00a0\u2026", "num_citations": "24\n", "authors": ["529"]}
{"title": "Method and apparatus for providing resource allocation policy\n", "abstract": " A method and apparatus for providing a resource allocation policy in a network are disclosed. For example, the method constructs a queuing model for each application. The method defines a utility function for each application and for each transaction type of each application, and defines an overall utility in a system. The method performs an optimization to identify an optimal configuration that maximizes the overall utility for a given workload, and determines one or more adaptation policies for configuring the system in accordance with the optimal configuration.", "num_citations": "23\n", "authors": ["529"]}
{"title": "Affordable fault tolerance through adaptation\n", "abstract": " Fault-tolerant programs are typically not only difficult to implement but also incur extra costs in tenors of performance or resource consumption. Failures are typically relatively rare but the fault-tolerance overhead must be paid regardless if any failures occur during the program execution. This paper presents an approach that reduces the cost of fault-tolerance, namely, adaptations to a change in failure model. In particular, a program that assumes no failures (or only benign failures) is combined with a component that is responsible for detecting if failures occur and then switching to a fault-tolerant algorithm. Provided that the detection and adaptation mechanisms are not too expensive, this approach results in a program with smaller fault-tolerance overhead and thus a better performance than a traditional fault-tolerant program. Thus, the high cost of fault-tolerance is only paid when failures actually occur.", "num_citations": "22\n", "authors": ["529"]}
{"title": "Why is my smartphone slow? on the fly diagnosis of underperformance on the mobile internet\n", "abstract": " The perceived end-to-end performance of the mobile Internet can be impacted by multiple factors including websites, devices, and network components. Constant changes in these factors and network complexity make identifying root causes of high latency difficult. In this paper, we propose a multidimensional diagnosis technique using passive IP flow data collected at ISPs for investigating factors that impact the performance of the mobile Internet. We implement and evaluate our technique over four days of data from a major US cellular provider's network. Our approach identifies several combinations of factors affecting performance. We investigate four combinations indepth to confirm the latency causes chosen by our technique. Our findings include a popular gaming website showing poor performance on a specific device type for over 50% of the flows and web browser traffic on older devices accounting for 99\u00a0\u2026", "num_citations": "21\n", "authors": ["529"]}
{"title": "Methods, devices, and computer program products for maintaining network presence while conserving power consumption\n", "abstract": " Network presence of a computing device in a cloud computing network is maintained while power consumption of the computing device is reduced. When the computing device is determined to enter an idle state, at least some of the operations of the computing device running in a virtual machine environment are migrated to a server within the cloud computing network while maintaining connectivity of the computing device to the cloud computing network. When the computing device is determined to be in the idle state, the computing device is put into a sleep mode to reduce power consumption of the computing device. When the computing device is determined to be in an active state, the computing device is woken, and the migrated operations are returned from the server to the computing device. This reduces power consumption of the computing device while maintaining the network presence of the computing\u00a0\u2026", "num_citations": "20\n", "authors": ["529"]}
{"title": "Mining large distributed log data in near real time\n", "abstract": " Analyzing huge amounts of log data is often a difficult task, especially if it has to be done in real time (eg, fraud detection) or when large amounts of stored data are required for the analysis. Graphs are a data structure often used in log analysis. Examples are clique analysis and communities of interest (COI). However, little attention has been paid to large distributed graphs that allow a high throughput of updates with very low latency.", "num_citations": "20\n", "authors": ["529"]}
{"title": "Understanding membership\n", "abstract": " A membership service is used in a distributed system to maintain information about which sites are functioning and which have failed at any given time. Such services have proven to be fundamental for constructing distributed applications, with many example services and algorithms defined in the literature. Despite these efforts, however, little has been done on examining the abstract properties commonly guaranteed by membership services independent of a given implementation. Here, a number of these properties are identified and defined. These properties range from agreement among sites on membership changes, consistent ordering of change notifications, and timing properties to various ways for dealing with recoveries and partitions. Message ordering graphs, which are an abstract representation of the set of messages at each site in the system and their potential delivery order, are used to define the properties. Dependency graphs, which are a graphical representation expressing when a property is defined assuming the existance of another property or when a stronger property includes a weaker property, are used to illustrate the relationships between properties. These graphs help differentiate existing services, as well as facilitate the design of new configurable services in which only those properties actually required by an application are included. Finally, a number of existing membership services are characterized in terms of the properties identified.", "num_citations": "20\n", "authors": ["529"]}
{"title": "Configuration management for highly-customizable services\n", "abstract": " The recent surge of configurable operating systems, database systems, and communication subsystems has demonstrated the importance of customization. Customization has a number of advantages, most notably performance improvements, code reuse, and meeting the quality of service requirements of the system users. One typical approach for constructing such services is to implement the service functionality as separate modules that can be configured in different combinations. Typically, ad hoc methods are used to determine which modules may be combined. Such methods require a system configurer to have intimate knowledge about the modules and their interactions or the configuration will not behave as expected. We present a methodology for configuring custom variants of configurable services. The methodology is based on identifying the relations between software modules that dictate which\u00a0\u2026", "num_citations": "18\n", "authors": ["529"]}
{"title": "The case for energy-oriented partial desktop migration\n", "abstract": " Office and home environments are increasingly crowded with personal computers. Even though these computers see little use in the course of the day, they often remain powered, even when idle. Leaving idle PCs running is not only wasteful, but with rising energy costs it is increasingly more expensive. We propose partial migration of idle desktop sessions into the cloud to achieve energyproportional computing. Partial migration only propagates the small footprint of state that will be needed during idle period execution, and returns the session to the PC when it is no longer idle. We show that this approach can reduce energy usage of an idle desktop by up to 50% over an hour and by up to 69% overnight. We show that idle desktop sessions have small working sets, up to an order of magnitude smaller than their allocated memory, enabling significant consolidation ratios. We also show that partial VM migration can save medium to large size organizations tens to hundreds of thousands of dollars annually.", "num_citations": "17\n", "authors": ["529"]}
{"title": "CPU gradients: Performance-aware energy conservation in multitier systems\n", "abstract": " Dynamic voltage and frequency scaling (DVFS) and virtual machine (VM) based server consolidation are well-known CPU scaling techniques for energy conservation that can have an adverse impact on system performance. For the responsiveness-sensitive multitier applications running in today's data centers, queuing models should ideally be used to predict the impact of CPU scaling on response time, to allow appropriate runtime trade-offs between performance and energy use. In practice, however, such models are difficult to construct and thus are often abandoned for ad-hoc solutions. In this paper, an alternative measurement-based approach that predicts the impact without requiring detailed application knowledge is presented. The approach proposes a new predictive model, the CPU gradient, that can be automatically measured on a running system using lightweight and nonintrusive CPU perturbations\u00a0\u2026", "num_citations": "16\n", "authors": ["529"]}
{"title": "Peer-to-peer error recovery for hybrid satellite-terrestrial networks\n", "abstract": " Media companies (and other organizations with large amounts of digital content) require prompt broadcast of extremely large files from a single source to a collection of geographically dispersed destinations. Due to the high cost of terrestrial networks of sufficient bandwidth, satellite networks are commonly used for such transfers. However, current satellite transfers rely on expensive error correction via forward error correction and whole-file retransmission. This paper presents a new, hybrid solution combining the advantages of satellite and terrestrial networks to provide cost-effective reliable file transfer. Specifically, we propose a new peer-to-peer scheme exploiting fast terrestrial networks and multiple receivers to recover from high loss rates (5% or more) in near real-time (latency < 400ms). This solution is efficient, robust under variable packet loss and connectivity, user tunable, scales well, and doubles\u00a0\u2026", "num_citations": "16\n", "authors": ["529"]}
{"title": "Replicating nondeterministic services on grid environments\n", "abstract": " Replication is a technique commonly used to increase the availability of services in distributed systems, including grid and Web services. While replication is relatively easy for services with fully deterministic behavior, grid and Web services often include nondeterministic operations. The traditional way to replicate such nondeterministic services is to use the primary-backup approach. While this is straightforward in synchronous systems with perfect failure detection, typical grid environments are not usually considered to be synchronous systems. This paper addresses the problem of replicating nondeterministic services by designing a protocol based on Paxos and proposing two performance optimizations suitable for replicated grid services. The first improves the performance in the case where some service operations do not change the service state, while the second optimizes grid service requests that use\u00a0\u2026", "num_citations": "16\n", "authors": ["529"]}
{"title": "A configurable membership service\n", "abstract": " A membership service is used to maintain information about which sites are functioning in a distributed system at any given time. Many such services have been defined, with each implementing a unique combination of properties that simplify the construction of higher levels of the system. Despite this wealth of possibilities, however, any given service only realizes one set of properties, which makes it difficult to tailor the service provided to the specific needs of the application. Here, a configurable membership service that addresses this problem is described. This service is based on decomposing membership into its constituent abstract properties, and then implementing these properties as separate software modules called micro-protocols that can be configured together to produce a customized membership service. A prototype C++ implementation of the membership service for a simulated distributed environment is also described.", "num_citations": "15\n", "authors": ["529"]}
{"title": "Ostro: Scalable placement optimization of complex application topologies in large-scale data centers\n", "abstract": " A complex cloud application consists of virtual machines (VMs) running software such as web servers and load balancers, storage in the form of disk volumes, and network connections that enable communication between VMs and between VMs and disk volumes. The application is also associated with various requirements, including not only quantities such as the sizes of the VMs and disk volumes, but also quality of service (QoS) attributes such as throughput, latency, and reliability. This paper presents Ostro, an Open Stack-based scheduler that optimizes the utilization of data center resources, while satisfying the requirements of the cloud applications. The novelty of the approach realized by Ostro is that it makes holistic placement decisions, in which all the requirements of an application -- described using an application topology abstraction -- are considered jointly. Specific placement algorithms for application\u00a0\u2026", "num_citations": "14\n", "authors": ["529"]}
{"title": "System and method for enforcing application security policies using authenticated system calls\n", "abstract": " Disclosed is an approach to system call monitoring in which authenticated system calls from an application are easily verified by an operating system kernel. The authenticated system call may be a system call augmented with extra arguments, which specify the policy for that call as well as a cryptographic message authentication code (MAC) that guarantees the integrity of the policy and the system call arguments. This extra information is used by the operating system kernel to verify the system call with little processing overhead. Versions of the applications in which regular system calls have been replaced by authenticated calls are generated automatically by a trusted installer program that reads the application binary, uses static analysis to generate policies, and then rewrites the binary with the authenticated calls. As a result, hacker attacks, malicious software and the like are less likely to be successful in\u00a0\u2026", "num_citations": "14\n", "authors": ["529"]}
{"title": "Real-time issues in Cactus\n", "abstract": " Services that provide real-time guarantees are important for many applications in distributed systems. While many such services have been implemented, most are targeted for speci c application areas and are correspondingly di cult to adapt to di ering requirements. This paper presents an approach to building con gurable real-time services based on software modules called micro-protocols. Each micro-protocol implements a di erent semantic property or property variant, and interacts with other micro-protocols using an event-driven model provided by a runtime system supporting real-time guarantees. The programming model is presented, together with an implementation design based on the x-kernel model for building network subsystems. The design of a highly-con gurable real-time channel abstraction built using this approach is also given. This work is part of the Cactus project the goal of which is to support ne-grained customization of QoS attributes related to dependability, realtime, and security.", "num_citations": "14\n", "authors": ["529"]}
{"title": "Community-based Analysis of Netflow for Early Detection of Security Incidents.\n", "abstract": " Detection and remediation of security incidents (eg, attacks, compromised machines, policy violations) is an increasingly important task of system administrators. While numerous tools and techniques are available (eg, Snort, nmap, netflow), novel attacks and low-grade events may still be hard to detect in a timely manner. In this paper, we present a novel approach for detecting stealthy, low-grade security incidents by utilizing information across a community of organizations (eg, banking industry, energy generation and distribution industry, governmental organizations in a specific country, etc). The approach uses netflow, a commonly available non-intrusive data source, analyzes communication to/from the community, and alerts the community members when suspicious activity is detected. A community-based detection has the ability to detect incidents that would fall below local detection thresholds while maintaining the number of alerts at a manageable level for each day.", "num_citations": "13\n", "authors": ["529"]}
{"title": "Customizing dependability attributes for mobile service platforms\n", "abstract": " Mobile service platforms are used to facilitate access to enterprise services such as email, product inventory, or design drawing databases by a wide range of mobile devices using a variety of access protocols. This paper presents a quality of service (QoS) architecture that allows flexible combinations of dependability attributes such as reliability, timeliness, and security to be enforced on a per service request basis. In addition to components that implement the underlying dependability techniques, the architecture includes policy components that evaluate a request's requirements and dynamically determine an appropriate execution strategy. The architecture has been integrated into an experimental version of iMobile, a mobile service platform being developed at AT&T. This paper describes the design and implementation of the architecture, and gives initial experimental results for the iMobile prototype.", "num_citations": "13\n", "authors": ["529"]}
{"title": "Fine-grain configurability for secure communication\n", "abstract": " Current solutions for providing communication security in network applications allow customization of certain security attributes and techniques, but in limited ways and without the benefit of a single unifying framework. Here, the design of a highly-customizable extensible service called SecComm is described in which attributes such as authenticity, privacy, integrity, and non-repudiation can be customized in arbitrary ways. With SecComm, applications can open secure communication connections in which only those attributes selected from among a wide range of possibilities are enforced, and are enforced using the strength or technique desired. SecComm has been implemented using Cactus, a system for building configurable communication services. In Cactus, different properties and techniques are implemented as software modules called micro-protocols that interact using an event-driven execution paradigm. This non-hierarchical design approach has a high degree of flexibility, ye...", "num_citations": "13\n", "authors": ["529"]}
{"title": "Configurable Fault-Tolerant Distributed Services\n", "abstract": " Fault tolerance\u2013that is, the ability of a system to continue providing its specified service despite failures\u2013is becoming more important as computers are increasingly used in application areas such as process control, air-traffic control, and banking. Distributed systems, consisting of computers connected by a network, are an important platform for many fault-tolerant systems. Unfortunately, it is difficult to construct fault-tolerant distributed software, so communication services such as multicast, RPC, membership, and transactions have been proposed as simplifying abstractions. However, although numerous versions of these services have been defined, no single implementation provides a perfect match for all applications and all execution environments.", "num_citations": "12\n", "authors": ["529"]}
{"title": "An extensible home automation architecture based on cloud offloading\n", "abstract": " Current home security and automation systems are typically structured as a collection of sensors and actuators connected to a local home controller. While new and more capable sensors are driving increasingly sophisticated applications such as video surveillance that require significant computing resources, home controllers are often resource-constrained devices that are not easy to upgrade or replace at scale. To address these limitations, we present a new Cloud-Enhanced Home Controller (CEHC) architecture where the resources of the local home controller are augmented with external cloud resources accessed over the network. While such cloud offloading has been studied in the context of other resource-constrained devices such as mobile phones, we posit that home control applications pose a new set of requirements unique to this domain. Here, we describe these requirements, propose an application\u00a0\u2026", "num_citations": "11\n", "authors": ["529"]}
{"title": "Sloth: SDN-enabled activity-based virtual machine deployment\n", "abstract": " While cloud computing is excellent at supporting elastic services that scale up to tens or hundreds of servers, its support for small-scale applications that only sporadically require one VM is lacking. To better support this sporadic usage model, we employ Software Defined Networking (SDN) technology to expose events related to network activity. Specifically, we rely on notifications when switch flow entries are removed or missing to determine resource (in) activity. Our prototype, Sloth, activates virtual machines based on incoming network traffic. Conversely, idle VMs are suspended to conserve resources. We present the design and architecture of our SDN-enabled on-demand resource deployment solution. Our empirical evaluation shows that VMs can be reactivated in less than one second, triggered by SDN events. This on-demand resource activation opens up novel applications for Cloud providers, allowing\u00a0\u2026", "num_citations": "11\n", "authors": ["529"]}
{"title": "Vision: Towards an extensible app ecosystem for home automation through cloud-offload\n", "abstract": " Home security and automation---temperature, lighting and energy management, access control, and alarming---is an area of growth in residential and office settings. These systems typically include a number of sensors and actuators connected to a controller that runs the automation software. For cost reasons, these home controllers are often resource constrained devices that are not easy to upgrade or replace at scale. But with the emergence of more capable sensors, there is a need for applications that require significant amounts of computing resources, eg, video feeds from cameras being used to identify people and their activities. The limited resources at the home controller makes it hard to deploy such applications, especially when numerous ones are being used concurrently. This problem is reminiscent of applications on mobile phones that necessitate cloud off-load. However, we posit that home control\u00a0\u2026", "num_citations": "11\n", "authors": ["529"]}
{"title": "Pscloud: a durable context-aware personal storage cloud\n", "abstract": " Personal content from mobile devices is often irreplaceable, yet current solutions for managing and synchronizing this data across multiple devices to ensure durability are often limited. A common approach is to synchronize data through a cloud storage service such as Dropbox. We argue that this model is excessively rigid because it forces users to use more expensive cloud storage than is needed. This paper proposes an alternative approach that uses storage on all of a user's mobile devices, home servers, and cloud storage accounts to create a single unified personal storage system called PSCloud in which data is automatically cached, replicated, and placed to enable reliable access across all devices while minimizing network access and storage costs. This approach is based on a per-device network context-graph that tracks connectivity relationships between a user's devices and storage options over time\u00a0\u2026", "num_citations": "11\n", "authors": ["529"]}
{"title": "Using CPU gradients for performance-aware energy conservation in multitier systems\n", "abstract": " Dynamic voltage and frequency scaling (DVFS) and virtual machine (VM) based server consolidation are techniques that hold promise for energy conservation, but can also have adverse impacts on system performance. For the responsiveness-sensitive multitier applications running in today\u2019s data centers, queuing models should ideally be used to predict the impact of CPU scaling on response time, to allow appropriate runtime trade-offs between performance and energy use. In practice, however, such models are difficult to construct and thus are often abandoned for ad hoc solutions. In this paper, an alternative measurement-based approach that predicts the impacts without requiring detailed application knowledge is presented. The approach uses a new set of metrics, the CPU gradients, that can be automatically measured on a running system using lightweight and nonintrusive CPU perturbations. The practical\u00a0\u2026", "num_citations": "11\n", "authors": ["529"]}
{"title": "On the comparison of network attack datasets: An empirical analysis\n", "abstract": " Network malicious activity can be collected and reported by various sources using different attack detection solutions. The granularity of these solutions provides either very detailed information (intrusion detection systems, honeypots) or high-level trends (CAIDA, SANS). The problem for network security operators is often to select the sources of information to better protect their network. How much information from these sources is redundant and how much is unique? The goal of this paper is to show empirically that while some global attack events can be correlated across various sensors, the majority of incoming malicious activity has local specificities. This study presents a comparative analysis of four different attack datasets offering three different levels of granularity: 1) two high interaction honeynets deployed at two different locations (i.e., a corporate and an academic environment); 2) ATLAS which is a\u00a0\u2026", "num_citations": "11\n", "authors": ["529"]}
{"title": "Implementing integrated fine-grain customizable qos using cactus\n", "abstract": " The concept of quality of service (QoS) has traditionally focused on performance-centric metrics such as throughput, delay, jitter, and loss rate (eg,[2, 8]) primarily in the field of data communication. Recently, however, the use of the concept has extended to all types of services in computer systems, and the scope of the QoS concept has expanded to include other attributes of service quality such as faulttolerance, availability, reliability, timeliness, consistency, accuracy, and security. Such an integrated QoS concept provides users the ability to specify exactly what aspects of the service quality are important, and\u2014in case of tradeoffs between different QoS attributes\u2014what balance between the different attributes should be chosen. For example, strengthening communication security through cryptographic methods typically reduces throughput and increases latency. Providing users the power to specify the desired\u00a0\u2026", "num_citations": "11\n", "authors": ["529"]}
{"title": "A customized communication subsystem for FT-Linda\n", "abstract": " Distributed fault-tolerant systems usually impose much stronger requirements on the underlying communication protocols than do applications developed without fault-tolerance in mind. That is true, for example, of applications composed of processes replicated on multiple hosts, where all replicas must keep the same view of the state of the communication. This paper describes how the communication substrate for a speci c application with strong communication requirements was developed. The application, the runtime system for a fault-tolerant version of the Linda language called FT-Linda, requires a communication substrate capable of providing ordered atomic multicast, failure detection and membership services. The implementation relies on a new framework for the composition of event-driven microprotocols that is used with the x-kernel.", "num_citations": "11\n", "authors": ["529"]}
{"title": "A model for adaptive fault-tolerant systems\n", "abstract": " An adaptive computing system is one that modifies its behavior based on changes in the environment. Since one common type of environment change in a distributed system is network or processor failure, fault-tolerant distributed systems can be viewed as an important subclass of adaptive systems. As such, use of adaptive methods for dealing with failures in this context has the same potential advantages of improved efficiency and structural simplicity as for adaptive systems in general. This paper describes a model for adaptive systems that can be applied in many failure scenarios arising in distributed systems. This model divides the adaptation process into three different phases\u2014change detection, agreement, and action\u2014that can be used as a common means for describing various fault-tolerance algorithms such as reliable transmission and membership protocols. This serves not only to clarify the\u00a0\u2026", "num_citations": "11\n", "authors": ["529"]}
{"title": "Method and apparatus for controlling a roadway source\n", "abstract": " A method and computer-readable storage device and apparatus for controlling a roadway resource are disclosed. For example, the method receives from a device associated with a vehicle a request to reach a target destination by a target arrival time and a maximum payment amount to reach the target destination by the target arrival time. The method next determining a route to the target destination and determines a roadway resource along the route that is controllable. The method then sends an instruction to the device associated with the vehicle, where the instruction indicates to navigate the vehicle along the route and controls the roadway resource to enable the vehicle to reach the target destination by the target arrival time.", "num_citations": "10\n", "authors": ["529"]}
{"title": "Automatic recovery using bounded partially observable markov decision processes\n", "abstract": " This paper provides a technique, based on partially observable Markov decision processes (POMDPs), for building automatic recovery controllers to guide distributed system recovery in a way that provides provable assurances on the quality of the generated recovery actions even when the diagnostic information may be imprecise. Lower bounds on the cost of recovery are introduced and proved, and it is shown how the characteristics of the recovery process can be used to ensure that the lower bounds converge even on undiscounted models. The bounds used in an appropriate online controller provide it with provable termination properties. Simulation-based experimental results on a realistic e-commerce system demonstrate that the proposed bounds can be improved iteratively, and the resulting controller convincingly outperforms a controller that uses heuristics instead of bounds", "num_citations": "10\n", "authors": ["529"]}
{"title": "Creating cross-service chains of virtual network functions in a wide area network\n", "abstract": " Concepts and technologies are disclosed herein for creating cross-service chains of virtual network functions in a wide area network. A controller can receive a chain request from a requestor. The chain request can specify functionality that is to be included in a service chain. The functionality can include a first function and a second function. The controller can compute a route associated with the service chain. The route can specify a first site that hosts a first service that provides the first function and a second site that hosts a second service that provides the second function. A first virtual network function can be located at the first site and a second virtual network function can be located at the second site. The controller can configure edge devices and forwarding devices to various entities at the two sites to enable the cross-service virtual network function chain.", "num_citations": "9\n", "authors": ["529"]}
{"title": "Collaborative QoS for service oriented architectures\n", "abstract": " Methods, systems, and computer-readable media for providing collaborative quality of service (\u201cQoS\u201d) for service-oriented architectures are described. The application services in the service-oriented architecture implement a QoS application-programming interface (\u201cAPI\u201d) in addition to the traditional service API. The QoS API may include a QoS reporting interface for reporting values of QoS metrics of the application service, a QoS mechanism interface that exposes details of dependability mechanisms utilized by the application service, and/or a QoS negotiation interface that allows dynamic negotiation of what QoS properties the application service provides and how the QoS properties are provided. Service consumers may utilize the QoS API of the various application services in the service-oriented architecture to collect dependability information regarding the application services and make service provider\u00a0\u2026", "num_citations": "9\n", "authors": ["529"]}
{"title": "Methods and systems for transferring data over electronic networks\n", "abstract": " Methods and systems for managing the transfer of large data files across electronic data networks optimally in accordance with the desired results of the users. The present invention takes into consideration the user-defined transfer requirements, the data characteristics, and the characteristics of the entirety of the network, including both the access links and the backbone and processing and storage resources in the backbone. The present invention the enables users to more optimally transfer data within the limitations of the existing network capabilities, negating requirements to update local or remote network facilities.", "num_citations": "9\n", "authors": ["529"]}
{"title": "Performance aware regeneration in virtualized multitier applications\n", "abstract": " Virtual machine technology enables highly agile system deployments in which components can be cheaply moved, cloned, and allocated controlled hardware resources. In this paper, we examine in the context of multitier Enterprise applications, how these facilities can be used to provide enhanced solutions to the classic problem of ensuring high availability without a loss in performance on a fixed amount of resources. By using virtual machine clones to restore the redundancy of a system whenever component failures occur, we achieve improved availability compared to a system with a fixed redundancy level. By smartly controlling component placement and colocation using information about the multitier system\u2019s flows and predictions made by queuing models, we ensure that the resulting performance degradation is minimized. Simulation results show that our proposed approach provides better availability and significantly lower degradation of system response times compared to traditional approaches.", "num_citations": "9\n", "authors": ["529"]}
{"title": "DarkNOC: Dashboard for Honeypot Management.\n", "abstract": " Protecting computer and information systems from security attacks is becoming an increasingly important task for system administrators. Honeypots are a technology often used to detect attacks and collect information about techniques and targets (eg, services, ports, operating systems) of attacks. However, managing a large and complex network of honeypots becomes a challenge given the amount of data collected as well as the risk that the honeypots may become infected and start attacking other machines. In this paper, we present DarkNOC, a management and monitoring tool for complex honeynets consisting of different types of honeypots as well as other data collection devices. DarkNOC has been actively used to manage a honeynet consisting of multiple subnets and hundreds of IP addresses. This paper describes the architecture and a number of case studies demonstrating the use of DarkNOC.", "num_citations": "8\n", "authors": ["529"]}
{"title": "QoS customization in distributed object systems\n", "abstract": " Applications built on networked collections of computers are increasingly using distributed object platforms such as CORBA,Java Remote Method Invocation (RMI), and DCOM to standardize object interactions. With this increased use comes the increased need for enhanced quality of service (QoS) attributes related to fault tolerance, security, and timeliness. This paper describes an architecture called CQoS (configurable QoS) for implementing such enhancements in a transparent, highly customizable, and portable manner. CQoS consists of two parts: application\u2010 and platform\u2010dependent interceptors and generic QoS components. The generic QoS components are implemented using Cactus, a system for building highly configurable protocols and services in distributed systems. The CQoS architecture and the interfaces between the different components are described, together with implementations of QoS\u00a0\u2026", "num_citations": "8\n", "authors": ["529"]}
{"title": "Application deployment engine\n", "abstract": " Concepts and technologies are disclosed herein for an application deployment engine. A processor that executes an application deployment engine can receive an application request. The processor can obtain network topology data that indicates availability of resources of a data center, an application template associated with the application, and a running time during which an application placement plan is to be identified out of a large number of placement scenarios within the running time. The application template can describe an application flow path associated with the application. The processor can identify the application placement plan, where the application placement plan can include an optimal placement of the application at the data center, before a given running time expires by pruning the large search space. The processor can generate a command to effect deployment of the application in\u00a0\u2026", "num_citations": "7\n", "authors": ["529"]}
{"title": "Virtual redundancy for active-standby cloud applications\n", "abstract": " VM redundancy is the foundation of resilient cloud applications. While active-active approaches combined with load balancing and autoscaling are usually resource efficient, the stateful nature of many cloud applications often necessitates 1+1 (or 1+n) active-standby approaches. Keeping the standbys, however, could result in inefficient utilization of cloud resources. We explore an intriguing cloud-based solution, where standby VMs from active-standby applications are selectively overbooked to reduce resources reserved for failures. The approach requires careful VM placement to avoid a situation where multiple standby VMs activate simultaneously on the same host and thus cannot get the full resource entitlement. Indeed today's clouds do not have this visibility to the applications. We rectify this situation through ShadowBox, a novel redundancy-aware VM scheduler that optimizes the placement and activation of\u00a0\u2026", "num_citations": "7\n", "authors": ["529"]}
{"title": "Locational prediction of failures\n", "abstract": " Methods, systems, and products predict locations of failures. Messages historically received are analyzed to predict a future failure. A pattern in the messages historically received is used to retrieve a template. A location of the future failure is predicted from the template.", "num_citations": "7\n", "authors": ["529"]}
{"title": "Q-opt: Self-tuning quorum system for strongly consistent software defined storage\n", "abstract": " This paper presents Q-OPT, a system for automatically tuning the configuration of quorum systems in strongly consistent Software Defined Storage (SDS) systems. Q-OPT is able to assign different quorum systems to different items and can be used in a large variety of settings, including systems supporting multiple tenants with different profiles, single tenant systems running applications with different requirements, or systems running a single application that exhibits non-uniform access patterns to data. Q-OPT supports automatic and dynamic reconfiguration, using a combination of complementary techniques, including top-k analysis to prioritise quorum adaptation, machine learning to determine the best quorum configuration, and a non-blocking quorum reconfiguration protocol that preserves consistency during reconfiguration. Q-OPT has been implemented as an extension to one of the most popular open-source\u00a0\u2026", "num_citations": "7\n", "authors": ["529"]}
{"title": "Airfoil: A topology aware distributed load balancing service\n", "abstract": " Load balancing is one of the most basic services needed by cloud applications. While today's clouds and load balancers provide highly customizable load distribution policies, they are forced out of necessity to ignore the impact of load balancing on the network. However, such network agnostic behavior can lead to inefficient utilization of cloud resources and poor performance, especially for upcoming network centric applications such as network function virtualization (NFV). But can network topology, a cloud property that is hidden from tenants, be made to effectively influence load balancing, a function that is intimately tied to per-tenant application structure that is largely invisible to the cloud? We answer that question in this paper by presenting Airfoil, a novel topology-aware distributed load balancer as a service (LBaaS) that takes network topology into consideration while providing cloud tenants with application\u00a0\u2026", "num_citations": "7\n", "authors": ["529"]}
{"title": "Online model-based adaptation for optimizing performance and dependability\n", "abstract": " Constructing adaptive software that is capable of changing behavior at runtime is a challenging software engineering problem. However, the problem of determining when and how such a system should adapt, ie, the system's adaptation policy, can be even more challenging. To optimize the behavior of a system over its lifetime, the policy must often take into account not only the current system state, but also the anticipated future behavior of the system. This paper presents a systematic approach based on using Markov Decision Processes to model the system and to generate optimal adaptation policies for it. In our approach, we update the model on-line based on system measurements and generate updated adaptation policies at runtime when necessary. We present the general approach and then outline its application to a distributed message dissemination system based on AT&T's iMobile platform.", "num_citations": "7\n", "authors": ["529"]}
{"title": "Access control in wide-area networks\n", "abstract": " Access control involves maintaining information about which users can access system resources and ensuring that access is restricted to authorized users. In wide-area networks such as the Internet, implementing access control is difficult, since resources may be replicated, the task of managing access rights may be distributed among multiple sites, and events such as host failures, host recoveries, and network partitions must be dealt with. This paper explores the problem of access control in such an environment, and in particular the inherent tradeoff between security, availability, and performance. Techniques for dealing with access control in the presence of partitions are presented and used as the basis for an algorithm that allows application control over these tradeoffs.", "num_citations": "7\n", "authors": ["529"]}
{"title": "Topology aware load balancing engine\n", "abstract": " Concepts and technologies are disclosed herein for a topology aware load balancing engine. A processor that executes a load balancing engine can receive a request for a load balancing plan for an application. The processor can obtain network topology data that describes elements of a data center and links associated with the elements. The processor can obtain an application flow graph associated with the application and create a load balancing plan to use in balancing traffic associated with the application. The processor can create the load balancing plan to use in balancing traffic associated with the application and distribute commands to the data center to balance traffic over the links.", "num_citations": "6\n", "authors": ["529"]}
{"title": "Online Failure Prediction with Accurate Failure Localization in Cloud Infrastructures\n", "abstract": " (in English) The ability to predict the occurrence of a failure in a large scale IT infrastructure, such as a cloud data center, opens up possibilities to alleviate or completely hide the impact of the failure by taking corrective actions before the failure actually occurs. For example, a VM can be migrated from a failing physical machine to another. However, existing failure prediction methods often do not provide enough information about the location of the failure (eg, which physical machine) to take meaningful actions. We propose an integrated method that combines the prediction of the occurrence of a failure (of a given type) with the prediction of the location of the failure enabling automated or operator actions. We describe the approach and show initial results in a commercial cloud infrastructure.", "num_citations": "6\n", "authors": ["529"]}
{"title": "Draco: statistical diagnosis of chronic problems in distributed systems\n", "abstract": " Chronics are recurrent problems that often fly under the radar of operations teams because they do not affect enough users or service invocations to set off alarm thresholds. In contrast with major outages that are rare, often have a single cause, and as a result are relatively easy to detect and diagnose quickly, chronic problems are elusive because they are often triggered by complex conditions, persist in a system for days or weeks, and coexist with other problems active at the same time. In this paper, we present Draco, a scalable engine to diagnose chronics that addresses these issues by using a \u201ctopdown\u201d approach that starts by heuristically identifying user interactions that are likely to have failed, eg, dropped calls, and drills down to identify groups of properties that best explain the difference between failed and successful interactions by using a scalable Bayesian learner. We have deployed Draco in production for the VoIP operations of a major ISP. In addition to providing examples of chronics that Draco has helped identify, we show via a comprehensive evaluation on production data that Draco provided 97% coverage, had fewer than 4% false positives, and outperformed state-of-the-art diagnostic techniques by up to 56% for complex chronics. I.", "num_citations": "6\n", "authors": ["529"]}
{"title": "Using link gradients to predict the impact of network latency on multitier applications\n", "abstract": " Managing geographically dispersed deployments of complex multitier applications involves dealing with the substantial effects of network latency. However, the effects of network latency on an application's end-to-end performance can be far from obvious, thus making it difficult to predict the true impact of infrastructure changes such as network upgrades or server relocation on the users of an application. In this paper, we propose a new metric to quantify this impact called the link gradient. We develop a novel noise-resistant, nonintrusive technique to measure the link gradients in running systems without requiring knowledge of the system structure by using a combination of run-time delay injection and spectral analysis. We evaluate the intrusiveness and accuracy of our approach using micro-benchmarks and a deployment of two benchmark multitier Web applications on PlanetLab. Using these results, we show that\u00a0\u2026", "num_citations": "6\n", "authors": ["529"]}
{"title": "Methods and systems for transferring data over electronics networks\n", "abstract": " Methods and systems for managing the transfer of large data files across electronic data networks optimally in accordance with the desired results of the users. The present invention takes into consideration the user-defined transfer requirements, the data characteristics, and the characteristics of the entirety of the network, including both the access links and the backbone and processing and storage resources in the backbone. The present invention the enables users to more optimally transfer data within the limitations of the existing network capabilities, negating requirements to update local or remote network facilities.", "num_citations": "6\n", "authors": ["529"]}
{"title": "From local impact functions to global adaptation of service compositions\n", "abstract": " The problem of self-optimization and adaptation in the context of customizable systems is becoming increasingly important with the emergence of complex software systems and unpredictable execution environments. Here, a general framework for automatically deciding on when and how to adapt a system whenever it deviates from the desired behavior is presented. In this framework, the adaptation targets of the system are described in terms of a high-level policy that establishes goals for a set of performance indicators. The decision process is based on information provided independently for each service that describes the available adaptations, their impact on performance indicators, and any limitations or requirements. The technique consists of both offline and online phases. Offline, rules are generated specifying service adaptations that may help to achieve the specified goals when a given change in\u00a0\u2026", "num_citations": "6\n", "authors": ["529"]}
{"title": "Applying grid technology to Web application systems\n", "abstract": " Grid middleware such as the Globus toolkit and grid standards such as the open grid services architecture (OGSA) are intended to be expressive enough for building distributed enterprise applications, yet their use in this context remains largely unexplored. Here, two specific issues related to the use of grid technology for Web application systems, an important class of enterprise applications is examined. The first is whether existing middleware can be used to move legacy Web applications to a grid platform, and if so, how; the second is whether such grid-enabling brings advantages in terms of additional functionality, enhanced performance, or simplified management. These issues are addressed by grid-enabling a sample J2EE Internet banking application and comparing its performance with the original version, and then designing a scalable software architecture that can use the dynamic resource allocation\u00a0\u2026", "num_citations": "6\n", "authors": ["529"]}
{"title": "A Study of Network-Side 5G User Localization Using Angle-Based Fingerprints\n", "abstract": " This paper explores network-side cellular user localization using fingerprints created from the angle measurements enabled by 5G. Our key idea is a binning-based fingerprinting technique that leverages multipath propagation to create fingerprint vectors based on angles of arrival of signals along multiple paths at each user. In network simulations that recreate urban environments with 3D building geometry and base station locations for a major city, our binning-based fingerprinting for 5G achieves significantly lower localization errors with a single base station than signal strength-based fingerprinting for LTE.", "num_citations": "5\n", "authors": ["529"]}
{"title": "Is collaborative QoS the solution to the SOA dependability dilemma?\n", "abstract": " Service-oriented architectures (SOAs) are an approach to structuring software in which distributed applications are constructed as collections of interacting services. While they promise many benefits including significant cost savings through service reuse and faster application design and implementation, many of the very aspects that make SOAs attractive amplify the dependability challenges faced by distributed applications. This dependability dilemma becomes especially pronounced when the services making up an application are owned and managed by different organizations or are executed on resources owned and operated by third parties, such as cloud computing or utility computing providers. This paper reviews the vision of SOAs, and discusses the characteristics that make them particularly challenging for dependability. It then discusses techniques that have been proposed for building\u00a0\u2026", "num_citations": "5\n", "authors": ["529"]}
{"title": "Supporting configurability and real time in RTD channels\n", "abstract": " Network communication services that can be configured to customize functionality provide significant advantages over monolithic versions, but can be difficult to construct if the services must also provide real\u2010time guarantees on message delivery. This paper describes how practical issues related to the combination of configurability and real time have been addressed in real\u2010time dependable (RTD) channels, a communication abstraction that has been prototyped using the Cactus system. The architecture and implementation of RTD channels are described, including facilities for translating an application's quality of service requirements into configuration\u2010specific resource requirements, and an admission control architecture that uses system\u2010wide information to determine whether sufficient resources exist to create new channels. In addition, the results of experiments that demonstrate that the desired channel\u00a0\u2026", "num_citations": "5\n", "authors": ["529"]}
{"title": "Using redundancy to increase survivability\n", "abstract": " Secure communication services\u2014that is, communication services that provide attributes such as confidentiality, integrity, and authenticity\u2014typically implement each attribute using a single method for each connection. For example, confidentiality may be provided by DES and integrity by keyed MD5. Although such an approach may be secure in the traditional sense, it is not survivable\u2014once a method is compromised, all security guarantees on the connection related to that attribute are gone. Each method is, in essence, a single point of vulnerability very much analogous to a single point of system failure when considering faulttolerance attributes. This problem is the same for many other aspects of security, including authentication and access control.This position paper advocates the use of a standard fault-tolerance technique\u2014redundancy\u2014to increase the survivability of communication. For example, using this approach, message integrity can be implemented by calculating redundant independent signatures, while confidentiality can be implemented by encrypting the message with a combination of methods with keys established using different methods. As a result, even if an intruder manages to find one key or break one algorithm, the security guarantees may remain intact. The task of the intruder can be complicated further by using secret combinations of methods or by dynamically altering the set of methods during the lifetime of the connection. By using multiple methods and doing so in ways that can vary unpredictably, the space of possibilities that must be considered by an attacker and the effort expended to compromise the attribute\u00a0\u2026", "num_citations": "5\n", "authors": ["529"]}
{"title": "Designing and implementing adaptive distributed systems\n", "abstract": " Objective: To develop fundamental new techniques for designing and implementing adaptive computing systems on distributed platforms.", "num_citations": "5\n", "authors": ["529"]}
{"title": "Supercell: adaptive software-defined storage for cloud storage workloads\n", "abstract": " The explosive growth of data due to the increasing adoption of cloud technologies in the enterprise has created a strong demand for more flexible, cost-effective, and scalable storage solutions. Many storage systems, however, are not well matched to the workloads they service due to the difficulty of configuring the storage system optimally a priori with only approximate knowledge of the workload characteristics. This paper shows how cloud-based orchestration can be leveraged to create flexible storage solutions that use continuous adaptation to tailor themselves to their target application workloads, and in doing so, provide superior performance, cost, and scalability over traditional fixed designs. To demonstrate this approach, we have built \"SuperCell,\" a Ceph-based distributed storage solution with a recommendation engine for the storage configuration. SuperCell provides storage operators with real-time\u00a0\u2026", "num_citations": "4\n", "authors": ["529"]}
{"title": "Using web service transformations to implement cooperative fault tolerance\n", "abstract": " Developing techniques to increase the availability of web services in the event of failure has become increasingly important given their key role in providing access to online information, financial, and retail resources. This paper describes an approach to improving availability by using failover between similar but not identical services, and the use of cooperative fault tolerance between the providers of these services. With this approach, a similar service can be used as a backup, with the protocol and service differences between the two services masked by the use of transformation web services that are generated semi-automatically. The basic idea of cooperative fault-tolerance using similar services is presented based on an example involving two stock broker services. The software architecture and the process for generating the transformation web services using a code generation tool are also described\u00a0\u2026", "num_citations": "4\n", "authors": ["529"]}
{"title": "A compiler-enabled model-and measurement-driven adaptation environment for dependability and performance\n", "abstract": " Traditional techniques for building dependable, high-performance distributed systems are too expensive for most non-critical systems, often causing dependability to be sidelined as a design goal. Nevertheless, systems are expected to be dependable, and if dependability could be provided at a lower cost, many applications would stand to benefit. We believe that compiler techniques can be used to create novel and enhance existing dependability mechanisms to create a wider range of cost/dependability tradeoffs than is currently available. Similarly, compilers can assist in the area of error detection by expanding the range of errors that can be detected. New compiler techniques, combined with model-driven adaptation and control mechanisms, can be used to dynamically guide a system as it makes choices, with cost, dependability, and performance tradeoffs, in response to the occurrence of faults and changes in\u00a0\u2026", "num_citations": "4\n", "authors": ["529"]}
{"title": "Corba-as-needed: A technique to construct high performance corba applications\n", "abstract": " This paper proposes a new optimization technique called CORBA-as-needed to improve the performance of distributed CORBA applications. This technique is based on the observation that in many cases the client and the server of a distributed application run on compatible computing platforms, and do not need the interoperability functionality of CORBA. CORBA-as-needed dynamically determines if the interoperability functionality is needed for a specific application invocation, and bypasses this functionality if it is not needed. Performance measurements from a prototype implementation in omniORB show that CORBA-as-needed achieves a very significant performance improvement.", "num_citations": "4\n", "authors": ["529"]}
{"title": "The first international workshop on dependability of clouds, data centers and virtual computing environments\n", "abstract": " Cloud computing can be characterized as the culmination of the integration of computing and data infrastructures to provide a scalable, agile and cost-effective approach to support the ever-growing critical IT needs (in terms of computation and storage) of both enterprises and the general public. Cloud computing introduces a paradigm shift in computing where the ownership of computing resources is no more necessary for businesses and individuals to provide services to their end-users over the Internet. Cloud computing relieves its users from the burdens of provisioning and managing their own data centers and allows them to pay for resources only when they are actually needed and used. However, a shared cloud infrastructure introduces a number of new dependability challenges both for the cloud providers and users. Indeed all the data gets created, stored, shared and manipulated within the cloud.", "num_citations": "3\n", "authors": ["529"]}
{"title": "Modularizing fault-tolerant protocols\n", "abstract": " Fault-tolerant protocols such as group multicast and membership are important abstractions that help simplify the development of distributed operating systems with fault-tolerance requirements. Unfortunately, these protocols are often very complicated in their own right, which makes their design and implementation a non-trivial task. If the power of these abstractions is to be effectively utilized in future systems, further efforts to improve our understanding of these protocols and their fundamental properties are needed. Our current research is addressing these issues by applying modularization techniques to fault-tolerant protocols. Our approach is based on identifying orthogonal properties of a given protocol, and then realizing these properties as separate modules within a standard system framework. For example, group multicast protocols are often defined to be some combination of reliability, atomicity, and\u00a0\u2026", "num_citations": "3\n", "authors": ["529"]}
{"title": "Managing physical resources of an application\n", "abstract": " A method includes, for components of an application, identifying a plurality of groups including a first group and a second group, The method also includes nesting the first group into the second group to create a nested group, based at least on a level of the first group not exceeding a level of the second group. The method includes deploying the application by placing the components of the nested group within a network in accordance with a restriction of the nested group. The components include at least one virtual machine.", "num_citations": "2\n", "authors": ["529"]}
{"title": "Feasibility Study of Location-Conscious Multi-Site Erasure-Coded Ceph Storage for Disaster Recovery\n", "abstract": " A multi-site, distributed storage system deployment is frequently needed for disaster recovery. Traditional master-slave backup/restore solutions have their limitations and cannot meet the high data durability, system availability, and low latency required in many use cases. Software-defined storage (SDS) achieves flexibility, scalability, and cost-effectiveness through disaggregation of storage hardware from software and enables innovations in storage architectures. The open-source SDS Ceph solution can be used to build storage systems with high reliability using a disaster recovery (DR) configuration among multiple sites while maintaining cost-effectiveness. However, using multi-site erasure coding has the potential issue of longer read latency because of communication delays caused by data chunks distributed among multiple sites. This paper proposes location-conscious, multi-site erasure-coded Ceph\u00a0\u2026", "num_citations": "2\n", "authors": ["529"]}
{"title": "Discovering the Structure of Cloud Applications using Sampled Packet Traces\n", "abstract": " Accurate and up-to-date knowledge of how a cloud tenant's VMs utilize the underlying cloud infrastructure is essential for many cloud management tasks including tenant onboarding, optimized VM placement, performance optimization, and debugging. Unfortunately, existing solutions such as instrumentation at the hypervisors or standard networking protocols such as LLDP only provide a partial picture of cloud tenant's application structures and how they stress the underlying infrastructure. In this paper, we consider whether it is possible to use sFlow, a standardized mechanism for packet header sampling available in most commodity network switches, to extract such information in an accurate and scalable manner. We overcome the challenges posed by the purely passive and highly sampled nature of sFlow data, and describe a tool, sFinder, that automatically and continuously extracts such information. Our\u00a0\u2026", "num_citations": "2\n", "authors": ["529"]}
{"title": "Methods, systems, and products for estimating answers to questions\n", "abstract": " Methods, systems, and products answer questions using heuristics. A question and an input are received. A set of heuristic rules is retrieved, and the question is answered by evaluating an accuracy of each heuristic rule. Multiple accuracies from the set of heuristic rules are combined to produce a confidence level for the answer to the question.", "num_citations": "2\n", "authors": ["529"]}
{"title": "Quantifying the impact of network latency on the end-to-end response time of distributed applications\n", "abstract": " A method for measuring system response sensitivity, using live traffic and an analysis that converts randomly arriving stimuli and reactions to the stimuli to mean measures over chosen intervals, thereby creating periodically occurring samples that are processed. The system is perturbed in a chosen location of the system in a manner that is periodic with frequency p, and the system's response to arriving stimuli is measured at frequency p. The perturbation, illustratively, is with a square wave pattern.", "num_citations": "2\n", "authors": ["529"]}
{"title": "Transaction dependency graph construction using signal injection\n", "abstract": " Understanding the runtime behavior and dependencies between components in complex transaction-based enterprise systems enables the system administrators to identify performance bottlenecks, allocate resources, and detect failures. This paper introduces a novel method for extracting dependency information between system components at runtime by using delay injection on individual links and Fast Fourier Transforms. Our proposed method introduces minimal disturbance in the system and its execution time is independent of the system workload. Thus, it can be used at runtime in production systems. Furthermore, it avoids false positives introduced by other methods. We present preliminary experimental results that demonstrate that our approach is able to identify dependencies, avoid false positives, while ensuring low perturbation to the target system.", "num_citations": "2\n", "authors": ["529"]}
{"title": "RFT: Scalable and fault-tolerant microservices for the O-RAN control plane\n", "abstract": " The Open Radio Access Network (O-RAN) Alliance is opening up traditionally closed RAN elements by defining a new open communication interface (E2) that allows the behavior of a RAN element to be customized and controlled in real time. The RAN Intelligent Controller (RIC for short) is a platform for implementing RAN control functions as microservices called xApps. In this work, we propose and evaluate techniques to enable xApps in the RIC platform to be fault-tolerant while preserving high scalability. The key premise of our work is that traditional replication techniques cannot sustain high throughput and low latency as required by RAN elements. We propose techniques that use state partitioning, partial replication, and fast re-route with role awareness to decrease the overhead. We implemented the fault tolerance techniques as a library, called RFT (RIC Fault Tolerance), that xApp writers can employ to\u00a0\u2026", "num_citations": "1\n", "authors": ["529"]}
{"title": "RIC: A RAN Intelligent Controller Platform for AI-Enabled Cellular Networks\n", "abstract": " With the emergence of 5G, network densification, and richer and more demanding applications, the radio access network (RAN)\u2014a key component of the cellular network infrastructure\u2014will become increasingly complex. To tackle this complexity, it is critical for the RAN to be able to automate the process of deploying, optimizing, and operating while leveraging novel data-driven technologies to ultimately improve the end-user quality of experience. In this article, we disaggregate the traditional monolithic control plane (CP) RAN architecture and introduce a RAN Intelligent Controller (RIC) platform decoupling the control and data planes of the RAN driving an intelligent and continuously evolving radio network by fostering network openness and empowering network intelligence with AI-enabled applications. We provide functional and software architectures of the RIC and discuss its design challenges. We elaborate\u00a0\u2026", "num_citations": "1\n", "authors": ["529"]}
{"title": "Switchboard: A Middleware for Wide-Area Service Chaining\n", "abstract": " Production networks are transitioning from the use of physical middleboxes to virtual network functions (VNFs), which makes it easy to construct highly-customized service chains of VNFs dynamically using software. Wide-area service chains are increasingly important given the emergence of heterogeneous execution platforms consisting of customer premise equipment (CPE), small edge cloud sites, and large centralized cloud data centers, since only part of the service chain can be deployed at the CPE and even the closest edge site may not always be able to process all the customers' traffic. Switchboard is a middleware for realizing and managing such an ecosystem of diverse VNFs and cloud platforms. It exploits principles from service-oriented architectures to treat VNFs as independent services, and provides a traffic routing platform shared by all VNFs. Moreover, Switchboard's global controller optimizes wide\u00a0\u2026", "num_citations": "1\n", "authors": ["529"]}
{"title": "Workshop on Dependability Issues on SDN and NFV (DISN)\n", "abstract": " Software-Defined Networks (SDN) and Network Function Virtualization (NFV) are two technologies that have already had a deep impact on computer and telecommunication networks. Software Defined Networks (SDN) decouple network control from forwarding functions, enabling network control to become directly programmable and the underlying infrastructure to be abstracted from applications and network services. Network Function Virtualization (NFV) is a network architecture concept where IT virtualization techniques are used to implement network node functions as building blocks that may be combined, or chained, together to create communication services. SDN and NFV make it simpler and faster to deploy and manage new services, avoiding the cost and the long time frame required to design and implement hardwarebased network services. SDN and NVF introduce numerous dependability challenges\u00a0\u2026", "num_citations": "1\n", "authors": ["529"]}
{"title": "Finding the Needle in the Haystack: Identifying Business Communities in Internet Traffic\n", "abstract": " Identifying real-world business communities, e.g., Energy, finance, defense, in Internet traffic is a challenging problem but would be valuable for the construction of better in-trusion detection techniques, for example. Seed-based community detection identifies a community in a graph by iteratively adding the 'closest' vertices to an initial set of seed-vertices which are known to belong to the community. Previous research focused on unambiguous networks, where edges describe a specific intention in a fixed domain (e.g., A 'friend' in a social network) and tightly-knit communities whose members are better connected to each other ('close') than to the rest of the network. However, looking at a complete day of raw Internet traffic, we found that (1) the intend of a communication is ambiguous (e.g., ad-downloads are indistinguishable from web-page downloads) and (2) real-world industries manifest themselves as loosely\u00a0\u2026", "num_citations": "1\n", "authors": ["529"]}
{"title": "The first international workshop on dependability of clouds, data centers and virtual computing environments: DCDV 2011\n", "abstract": " Cloud computing can be characterized as the culmination of the integration of computing and data infrastructures to provide a scalable, agile and cost-effective approach to support the ever-growing critical IT needs (in terms of computation and storage) of both enterprises and the general public. Cloud computing introduces a paradigm shift in computing where the ownership of computing resources is no more necessary for businesses and individuals to provide services to their end-users over the Internet. Cloud computing relieves its users from the burdens of provisioning and managing their own data centers and allows them to pay for resources only when they are actually needed and used. However, a shared cloud infrastructure introduces a number of new dependability challenges both for the cloud providers and users. Indeed all the data gets created, stored, shared and manipulated within the cloud.", "num_citations": "1\n", "authors": ["529"]}
{"title": "Performance Evaluation of an Alert Dissemination Engine based on the AT&T Enterprise Messaging Network\n", "abstract": " The recent surge in the variety and number of mobile devices used as communication end points has created a significant challenge for messaging applications that aim to reach their target recipients regardless of their location and available devices. Alerting services used to notify a potentially large number of recipients about an emergency, or other important events, are an important class of applications enabled by the prevalence of such mobile devices. A middleware platform that arbitrates content delivery and adaptation between mobile devices and backend messaging applications is crucial in reducing the software complexity on both the client device and server side applications. For an alerting application, the Quality of Service (QoS) of the middleware platform becomes crucial-alerts must be delivered quickly, reliably, and securely to all their recipients. As the first step in achieving such QoS, this paper evaluates the performance of a commercial mobile middleware platform, the AT&T Enterprise Messaging Network (EMN), in the context of an alerting service. EMN is used as an Alert Dissemination Engine (ADE) to provision users, disseminate alerts, collect acknowledgments, and prepare reports on the status of alert dissemination and acknowledgments. The evaluation is based on a combination of component benchmarking and end-to-end benchmarking.", "num_citations": "1\n", "authors": ["529"]}
{"title": "The lost art of abstraction\n", "abstract": " System abstractions such as virtual memory simplify the construction of software by hiding details of the underlying system and by providing higher-level functionality on which to build. While the value of building systems as layers or hierarchies of abstractions has long been known, the application of this principle has been uneven when it comes to using it as the basis for architecting dependable distributed systems. This paper gives an overview of issues that arise when using abstractions in this area and proposes some approaches to addressing these issues. The latter include the use of translucent abstractions that expose some of the internal workings of the abstraction implementation, customizable abstractions that allow attributes to be matched to the application requirements and execution scenario, and an intrusion-stop process abstraction that potentially provides a basis for architecting survivable\u00a0\u2026", "num_citations": "1\n", "authors": ["529"]}
{"title": "Resource allocation for an enterprise mobile services platform\n", "abstract": " With rapid technological advances being made in the area of wireless communications it is ex-pected that, in the near future, mobile users will be able to access a wide variety of services that will be available over a heterogeneous network. The qualities of these services are essential to the overall end user experience and it is the responsibility of the network and the mo-bile service platform to deliver on these expectations. In the context of mobile computing environments, limited and dynamically varying available resources, strin-gent application requirements and user mobility make it difficult to provide sustained quality of service to applications. This paper addresses the above issues, in particular a) a queuing model is derived for a mobile service platform (iMobile) to identify its resource allocation needs, b) a network simulation of a mobile service platform is provided for estimating the expected response time and c) a testbed implementation of a mobile service platform to negotiate adaptive service support. cesses internet/intranet data on behalf of the user, keeps track of user interactions by means of ses-sions, performs content transformations appropriate to the user and device needs and last but not least performing AAA (authentication, authorization and accounting) services. Figure 1 shows a reference architecture of such a mobile service paltform, iMobile. A mobile device always interacts with an iMobile gateway before ac-cessing iMobile services. A gateway authenticates a mobile user and puts each service request on the message queue. Any iMobile server can then pick up the request. Each server hosts a set of infolets for backend\u00a0\u2026", "num_citations": "1\n", "authors": ["529"]}
{"title": "Supporting Configurable Real-Time Communication Services\n", "abstract": " Constructing communication services that provide real-time guarantees is important for many applications built on distributed systems. While a variety of such services have been designed and implemented, most are targeted for specific applications and are correspondingly difficult to adapt to differing requirements in other areas. This paper presents an approach to building configurable and customized versions of real-time communication services based on software modules called micro-protocols. Each micro-protocol implements a different semantic property or property variant, and interacts with other micro-protocols using an event-driven model supported by a runtime system providing real-time guarantees. The programming model is presented, together with an implementation design based on the x-kernel model for building network subsystems and the OSF/RI MK 7.2 operating system. The design of a highly-configurable real-time channel abstraction built using this approach is also given. Prototype implementations of the runtime system and channel abstraction are currently underway.", "num_citations": "1\n", "authors": ["529"]}
{"title": "Fine-Grain QoS Customization in Distributed Middleware Services\n", "abstract": " Quality of service (QoS) is increasingly being used to describe a broad collection of attributes, ranging from the performance-centric metrics used in the networking community to other metrics related to reliability, timeliness, and security. Moreover, QoS is also being applied to guarantees provided by a wider variety of services, ranging from communication services to higher level middleware services. In such a context, constructing a service that provides a fixed set of QoS guarantees is typically not sufficient since different applications that use the service may have very different requirements. The Cactus project is addressing these issues by providing facilities for performing fine-grain customization of QoS in distributed middleware services.", "num_citations": "1\n", "authors": ["529"]}
{"title": "Using Diversity Techniques to Enhance Communication Security\u00a3\n", "abstract": " Secure communication services\u2014that is, communication services that provide attributes such as confidentiality, integrity and authenticity\u2014typically implement each attribute using a single method for each message or connection. For example, confidentiality is implemented by applying a single algorithm such as DES [1], IDEA [18], or Blowfish [25] to a given message. When stronger security is needed, parameters such as key lengths may be changed or a more secure method substituted, but always within the constraint of applying a single method. While some techniques can be viewed as moving in the direction of applying techniques repeatedly\u2014TDEA (triple DES) for instance [1]\u2014even here the combinations are typically fixed and not altered or dynamically varied during execution. As a result, an attacker attempting to compromise a security attribute can focus their efforts on breaking just one method, which can decrease the effort required to break the system [12].In this position paper, we advocate a new paradigm based on using dynamic secret combinations of methods to increase security, an example of a diversity-based approach [23]. For example, with this approach, message integrity can be implemented by calculating two independent signatures or confidentiality implemented by encrypting the message sequentially with a secret combination of methods. The basic rationale for using such a technique is straightforward. By using multiple methods and doing so in ways that can vary unpredictably, the space of possibilities that must be considered by an attacker and the effort expended to compromise the attribute expands combinatorially\u00a0\u2026", "num_citations": "1\n", "authors": ["529"]}