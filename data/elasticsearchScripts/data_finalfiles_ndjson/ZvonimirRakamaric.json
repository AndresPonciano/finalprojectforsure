{"title": "Rigorous estimation of floating-point round-off errors with symbolic taylor expansions\n", "abstract": " Rigorous estimation of maximum floating-point round-off errors is an important capability central to many formal verification tools. Unfortunately, available techniques for this task often provide very pessimistic overestimates, causing unnecessary verification failure. We have developed a new approach called Symbolic Taylor Expansions that avoids these problems, and implemented a new tool called FPTaylor embodying this approach. Key to our approach is the use of rigorous global optimization, instead of the more familiar interval arithmetic, affine arithmetic, and/or SMT solvers. FPTaylor emits per-instance analysis certificates in the form of HOL Light proofs that can be machine checked. In this article, we present the basic ideas behind Symbolic Taylor Expansions in detail. We also survey as well as thoroughly evaluate six tool families, namely, Gappa (two tool options studied), Fluctuat, PRECiSA, Real2Float\u00a0\u2026", "num_citations": "138\n", "authors": ["692"]}
{"title": "Rigorous floating-point mixed-precision tuning\n", "abstract": " Virtually all real-valued computations are carried out using floating-point data types and operations. The precision of these data types must be set with the goals of reducing the overall round-off error, but also emphasizing performance improvements. Often, a mixed-precision allocation achieves this optimum; unfortunately, there are no techniques available to compute such allocations and conservatively meet a given error target across all program inputs. In this work, we present a rigorous approach to precision allocation based on formal analysis via Symbolic Taylor Expansions, and error analysis based on interval functions. This approach is implemented in an automated tool called FPTuner that generates and solves a quadratically constrained quadratic program to obtain a precision-annotated version of the given expression. FPTuner automatically introduces all the requisite precision up and down casting\u00a0\u2026", "num_citations": "102\n", "authors": ["692"]}
{"title": "Towards Formal Approaches to System Resilience\n", "abstract": " Technology scaling and techniques such as dynamic voltage/frequency scaling are predicted to increase the number of transient faults in future processors. Error detectors implemented in hardware are often energy inefficient, as they are \"always on.\" While software-level error detection can augment hardware-level detectors, creating detectors in software that are highly effective remains a challenge. In this paper, we first present anew LLVM-level fault injector called KULFI that helps simulate faults occurring within CPU state elements in a versatile manner. Second, using KULFI, we study the behavior of a family of well-known and simple algorithms under error injection. (We choose a family of sorting algorithms for this study.) We then propose a promising way to interpret our empirical results using a formal model that builds on the idea of predicate state transition diagrams. After introducing the basic abstraction\u00a0\u2026", "num_citations": "82\n", "authors": ["692"]}
{"title": "Evaluation of Android Malware Detection Based on System Calls\n", "abstract": " With Android being the most widespread mobile platform, protecting it against malicious applications is essential. Android users typically install applications from large remote repositories, which provides ample opportunities for malicious newcomers. In this paper, we evaluate a few techniques for detecting malicious Android applications on a repository level. The techniques perform automatic classification based on tracking system calls while applications are executed in a sandbox environment. We implemented the techniques in the maline tool, and performed extensive empirical evaluation on a suite of around 12,000 applications. The evaluation considers the size and type of inputs used in analyses. We show that simple and relatively small inputs result in an overall detection accuracy of 93% with a 5% benign application classification error, while results are improved to a 96% detection accuracy with up\u00a0\u2026", "num_citations": "78\n", "authors": ["692"]}
{"title": "Efficient search for inputs causing high floating-point errors\n", "abstract": " Tools for floating-point error estimation are fundamental to program understanding and optimization. In this paper, we focus on tools for determining the input settings to a floating point routine that maximizes its result error. Such tools can help support activities such as precision allocation, performance optimization, and auto-tuning. We benchmark current abstraction-based precision analysis methods, and show that they often do not work at scale, or generate highly pessimistic error estimates, often caused by non-linear operators or complex input constraints that define the set of legal inputs. We show that while concrete-testing-based error estimation methods based on maintaining shadow values at higher precision can search out higher error-inducing inputs, suit able heuristic search guidance is key to finding higher errors. We develop a heuristic search algorithm called Binary Guided Random Testing (BGRT). In\u00a0\u2026", "num_citations": "78\n", "authors": ["692"]}
{"title": "System Programming in Rust: Beyond Safety\n", "abstract": " Rust is a new system programming language that offers a practical and safe alternative to C. Rust is unique in that it enforces safety without runtime overhead, most importantly, without the overhead of garbage collection. While zero-cost safety is remarkable on its own, we argue that the superpowers of Rust go beyond safety. In particular, Rust's linear type system enables capabilities that cannot be implemented efficiently in traditional languages, both safe and unsafe, and that dramatically improve security and reliability of system software. We show three examples of such capabilities: zero-copy software fault isolation, efficient static information flow analysis, and automatic checkpointing. While these capabilities have been in the spotlight of systems research for a long time, their practical use is hindered by high cost and complexity. We argue that with the adoption of Rust these mechanisms will become commoditized.", "num_citations": "64\n", "authors": ["692"]}
{"title": "Symbolic Learning of Component Interfaces\n", "abstract": " Given a white-box component with specified unsafe states, we address the problem of automatically generating an interface that captures safe orderings of invocations of\u2019s public methods. Method calls in the generated interface are guarded by constraints on their parameters. Unlike previous work, these constraints are generated automatically through an iterative refinement process. Our technique, named Psyco (P redicate-based SY mbolic CO mpositional reasoning), employs a novel combination of the L* automata learning algorithm with symbolic execution. The generated interfaces are three-valued, capturing whether a sequence of method invocations is safe, unsafe, or its effect on the component state is unresolved by the symbolic execution engine. We have implemented Psyco as a new prototype tool in the JPF open-source software model checking platform, and we have successfully applied it to several\u00a0\u2026", "num_citations": "61\n", "authors": ["692"]}
{"title": "JDart: A Dynamic Symbolic Analysis Framework\n", "abstract": " We describe JDart, a dynamic symbolic analysis framework for Java. A distinguishing feature of JDart is its modular architecture: the main component that performs dynamic exploration communicates with a component that efficiently constructs constraints and that interfaces with constraint solvers. These components can easily be extended or modified to support multiple constraint solvers or different exploration strategies. Moreover, JDart has been engineered for robustness, driven by the need to handle complex NASA software. These characteristics, together with its recent open sourcing, make JDart an ideal platform for research and experimentation. In the current release, JDart supports the CORAL, SMTInterpol, and Z3 solvers, and is able to handle NASA software with constraints containing bit operations, floating point arithmetic, and complex arithmetic operations (e.g., trigonometric and nonlinear\u00a0\u2026", "num_citations": "57\n", "authors": ["692"]}
{"title": "A logic and decision procedure for predicate abstraction of heap-manipulating programs\n", "abstract": " An important and ubiquitous class of programs are heap-manipulating programs (HMP), which manipulate unbounded linked data structures by following pointers and updating links. Predicate abstraction has proved to be an invaluable technique in the field of software model checking; this technique relies on an efficient decision procedure for the underlying logic. The expression and proof of many interesting HMP safety properties require transitive closure predicates; such predicates express that some node can be reached from another node by following a sequence of (zero or more) links in the data structure. Unfortunately, adding support for transitive closure often yields undecidability, so one must be careful in defining such a logic. Our primary contributions are the definition of a simple transitive closure logic for use in predicate abstraction of HMPs, and a decision procedure for this logic. Through\u00a0\u2026", "num_citations": "56\n", "authors": ["692"]}
{"title": "A scalable memory model for low-level code\n", "abstract": " Because of its critical importance underlying all other software, low-level system software is among the most important targets for formal verification. Low-level systems software must sometimes make type-unsafe memory accesses, but because of the vast size of available heap memory in today\u2019s computer systems, faithfully representing each memory allocation and access does not scale when analyzing large programs. Instead, verification tools rely on abstract memory models to represent the program heap. This paper reports on two related investigations to develop an accurate (i.e., providing a useful level of soundness and precision) and scalable memory model: First, we compare a recently introduced memory model, specifically designed to more accurately model low-level memory accesses in systems code, to an older, widely adopted memory model. Unfortunately, we find that the newer memory\u00a0\u2026", "num_citations": "41\n", "authors": ["692"]}
{"title": "An inference-rule-based decision procedure for verification of heap-manipulating programs with mutable data and cyclic data structures\n", "abstract": " Research on the automatic verification of heap-manipulating programs (HMPs) \u2014 programs that manipulate unbounded linked data structures via pointers \u2014 has blossomed recently, with many different approaches all showing leaps in performance and expressiveness. A year ago, we proposed a small logic for specifying predicates about HMPs and demonstrated that an inference-rule-based decision procedure could be performance-competitive, and in many cases superior to other methods known at the time. That work, however, was a proof-of-concept, with a logic fragment too small to verify most real programs. In this work, we generalize our previous results to be practically useful: we allow the data in heap nodes to be mutable, we allow more than a single pointer field, and we add new primitives needed to verify cyclic structures. Each of these extensions necessitates new or changed inference rules\u00a0\u2026", "num_citations": "40\n", "authors": ["692"]}
{"title": "Hybrid Learning: Interface Generation through Static, Dynamic, and Symbolic Analysis\n", "abstract": " This paper addresses the problem of efficient generation of component interfaces through learning. Given a white-box component C with specified unsafe states, an interface captures safe orderings of invocations of C's public methods. In previous work we presented Psyco, an interface generation framework that combines automata learning with symbolic component analysis: learning drives the framework in exploring different combinations of method invocations, and symbolic analysis computes method guards corresponding to constraints on the method parameters for safe execution. In this work we propose solutions to the two main bottlenecks of Psyco. The explosion of method sequences that learning generates to validate its computed interfaces is reduced through partial order reduction resulting from a static analysis of the component. To avoid scalability issues associated with symbolic analysis, we propose\u00a0\u2026", "num_citations": "39\n", "authors": ["692"]}
{"title": "Context-bounded translations for concurrent software: An empirical evaluation\n", "abstract": " Context-Bounded Analysis has emerged as a practical automatic formal analysis technique for fine-grained, shared-memory concurrent software. Two recent papers (in CAV 2008 and 2009) have proposed ingenious translation approaches that promise much better scalability, backed by compelling, but differing, theoretical and conceptual advantages. Empirical evidence comparing the translations, however, has been lacking. Furthermore, these papers focused exclusively on Boolean model checking, ignoring the also widely used paradigm of verification-condition checking. In this paper, we undertake a methodical, empirical evaluation of the three main source-to-source translations for context-bounded analysis of concurrent software, in a verification-condition-checking paradigm. We evaluate their scalability under a wide range of experimental conditions. Our results show: (1) The newest, CAV\u00a02009\u00a0\u2026", "num_citations": "36\n", "authors": ["692"]}
{"title": "Fast and Precise Symbolic Analysis of Concurrency Bugs in Device Drivers\n", "abstract": " Concurrency errors, such as data races, make device drivers notoriously hard to develop and debug without automated tool support. We present Whoop, a new automated approach that statically analyzes drivers for data races. Whoop is empowered by symbolic pairwise lockset analysis, a novel analysis that can soundly detect all potential races in a driver. Our analysis avoids reasoning about thread interleavings and thus scales well. Exploiting the race-freedom guarantees provided by Whoop, we achieve a sound partial-order reduction that significantly accelerates Corral, an industrial-strength bug-finder for concurrent programs. Using the combination of Whoop and Corral, we analyzed 16 drivers from the Linux 4.0 kernel, achieving 1.5 -- 20\u00d7 speedups over standalone Corral.", "num_citations": "35\n", "authors": ["692"]}
{"title": "Verifying heap-manipulating programs in an SMT framework\n", "abstract": " Automated software verification has made great progress recently, and a key enabler of this progress has been the advances in efficient, automated decision procedures suitable for verification (Boolean satisfiability solvers and satisfiability-modulo-theories (SMT) solvers). Verifying general software, however, requires reasoning about unbounded, linked, heap-allocated data structures, which in turn motivates the need for a logical theory for such structures that includes unbounded reachability. So far, none of the available SMT solvers supports such a theory. In this paper, we present our integration of a decision procedure that supports unbounded heap reachability into an available SMT solver. Using the extended SMT solver, we can efficiently verify examples of heap-manipulating programs that we could not verify before.", "num_citations": "34\n", "authors": ["692"]}
{"title": "Android malware detection based on system calls\n", "abstract": " With Android being the most widespread mobile platform, protecting it against malicious applications is essential. Android users typically install applications from large remote repositories, which provides ample opportunities for malicious newcomers. In this paper, we propose a simple, and yet highly effective technique for detecting malicious Android applications on a repository level. Our technique performs automatic classification based on tracking system calls while applications are executed in a sandbox environment. We implemented the technique in a tool called MALINE, and performed extensive empirical evaluation on a suite of around 12,000 applications. The evaluation yields an overall detection accuracy of 93% with a 5% benign application classification error, while results are improved to a 96% detection accuracy with up-sampling. This indicates that our technique is viable to be used in practice. Finally, we show that even simplistic feature choices are highly effective, suggesting that more heavyweight approaches should be thoroughly (re) evaluated.", "num_citations": "30\n", "authors": ["692"]}
{"title": "Portable inter-workgroup barrier synchronisation for GPUs\n", "abstract": " Despite the growing popularity of GPGPU programming, there is not yet a portable and formally-specified barrier that one can use to synchronise across workgroups. Moreover, the occupancy-bound execution model of GPUs breaks assumptions inherent in traditional software execution barriers, exposing them to deadlock. We present an occupancy discovery protocol that dynamically discovers a safe estimate of the occupancy for a given GPU and kernel, allowing for a starvation-free (and hence, deadlock-free) inter-workgroup barrier by restricting the number of workgroups according to this estimate. We implement this idea by adapting an existing, previously non-portable, GPU inter-workgroup barrier to use OpenCL 2.0 atomic operations, and prove that the barrier meets its natural specification in terms of synchronisation.", "num_citations": "29\n", "authors": ["692"]}
{"title": "Verifying Rust Programs with SMACK\n", "abstract": " Rust is an emerging systems programming language with guaranteed memory safety and modern language features that has been extensively adopted to build safety-critical software. However, there is currently a lack of automated software verifiers for Rust. In this work, we present our experience extending the SMACK verifier to enable its usage on Rust programs. We evaluate SMACK on a set of Rust programs to demonstrate a wide spectrum of language features it supports.", "num_citations": "25\n", "authors": ["692"]}
{"title": "Formal Analysis of GPU Programs with Atomics via Conflict-Directed Delay-Bounding\n", "abstract": " GPU based computing has made significant strides in recent years. Unfortunately, GPU program optimizations can introduce subtle concurrency errors, and so incisive formal bug-hunting methods are essential. This paper presents a new formal bug-hunting method for GPU programs that combine barriers and atomics. We present an algorithm called c                 onflict-directed                  d                 elay-bounded scheduling algorithm (CD) that exploits the occurrence of conflicts among atomic synchronization commands to trigger the generation of alternate schedules; these alternate schedules are executed in a delay-bounded manner. We formally describe CD, and present two correctness checking methods, one based on final state comparison, and the other on user assertions. We evaluate our implementation on realistic GPU benchmarks, with encouraging results.", "num_citations": "19\n", "authors": ["692"]}
{"title": "Verifying relative safety, accuracy, and termination for program approximations\n", "abstract": " Approximate computing is an emerging area for trading off the accuracy of an application for improved performance, lower energy costs, and tolerance to unreliable hardware. However, developers must ensure that the leveraged approximations do not introduce significant, intolerable divergence from the reference implementation, as specified by several established robustness criteria. In this work, we show the application of automated differential verification towards verifying relative safety, accuracy, and termination criteria for a class of program approximations. We use mutual summaries to express relative specifications for approximations, and SMT-based invariant inference to automate the verification of such specifications. We perform a detailed feasibility study showing promise of applying automated verification to the domain of approximate computing in a cost-effective manner.", "num_citations": "17\n", "authors": ["692"]}
{"title": "Taming test inputs for separation assurance\n", "abstract": " The Next Generation Air Transportation System (NextGen) advocates the use of innovative algorithms and software to address the increasing load on air-traffic control. AutoResolver [12] is a large, complex NextGen component that provides separation assurance between multiple airplanes up to 20 minutes ahead of time. Our work targets the development of a light-weight, automated testing environment for AutoResolver. The input space of AutoResolver consists of airplane trajectories, each trajectory being a sequence of hundreds of points in the three-dimensional space. Generating meaningful test cases for AutoResolver that cover its behavioral space to a satisfactory degree is a major challenge. We discuss how we tamed this input space to make it amenable to test case generation techniques, as well as how we developed and validated an extensible testing environment around AutoResolver.", "num_citations": "16\n", "authors": ["692"]}
{"title": "STORM: static unit checking of concurrent programs\n", "abstract": " Concurrency is inherent in today's software. Unexpected interactions between concurrently executing threads often cause subtle bugs in concurrent programs. Such bugs are hard to discover using traditional testing techniques since they require executing a program on a particular unit test (ie input) through a particular thread interleaving. A promising solution to this problem is static program analysis since it can simultaneously check a concurrent program on all inputs as well as through all possible thread interleavings. This paper describes a scalable, automatic, and precise approach to static unit checking of concurrent programs implemented in a tool called Storm. Storm has been applied on a number of real-world Windows device drivers, and the tool found a previously undiscovered concurrency bug in a driver from Microsoft's Driver Development Kit.", "num_citations": "15\n", "authors": ["692"]}
{"title": "Automated Differential Program Verification for Approximate Computing\n", "abstract": " Approximate computing is an emerging area for trading off the accuracy of an application for improved performance, lower energy costs, and tolerance to unreliable hardware. However, care has to be taken to ensure that the approximations do not cause significant divergence from the reference implementation. Previous research has proposed various metrics to guarantee several relaxed notions of safety for the design and verification of such approximate applications. However, current approximation verification approaches often lack in either precision or automation. On one end of the spectrum, type-based approaches lack precision, while on the other, proofs in interactive theorem provers require significant manual effort. In this work, we apply automated differential program verification (as implemented in SymDiff) for reasoning about approximations. We show that mutual summaries naturally express many relaxed specifications for approximations, and SMT-based checking and invariant inference can substantially automate the verification of such specifications. We demonstrate that the framework significantly improves automation compared to previous work on using Coq, and improves precision when compared to path-insensitive analysis. Our results indicate the feasibility of applying automated verification to the domain of approximate computing in a cost-effective manner.", "num_citations": "10\n", "authors": ["692"]}
{"title": "JPF-Doop: Combining Concolic and Random Testing for Java\n", "abstract": " Achieving high code coverage during software testing is important because it gives a measure of how thoroughly the software has been tested. However, reaching high code coverage in testing of real-world software is challenging due to its size and complexity. Our paper addresses this challenge by proposing an automatic multipronged approach. In particular, we propose an iterative algorithm for generating unit tests that meaningfully combines concolic execution and random testing. The algorithm aims to exploit the advantages of both random testing and systematic software verification techniques. We implemented the algorithm by integrating a Java Pathfinder\u2019s concolic execution engine and Randoop, and dubbed the implementation JPF-Doop. Preliminary experimental results show that JPF-Doop outperforms Randoop in terms of code coverage.", "num_citations": "10\n", "authors": ["692"]}
{"title": "Automatic inference of frame axioms using static analysis\n", "abstract": " Many approaches to software verification are currently semi-automatic: a human must provide key logical insights - e.g., loop invariants, class invariants, and frame axioms that limit the scope of changes that must be analyzed. This paper describes a technique for automatically inferring frame axioms of procedures and loops using static analysis. The technique builds on a pointer analysis that generates limited information about all data structures in the heap. Our technique uses that information to over-approximate a potentially unbounded set of memory locations modified by each procedure/loop; this over- approximation is a candidate frame axiom. We have tested this approach on the buffer-overflow benchmarks from ASE 2007. With manually provided specifications and invariants/axioms, our tool could verify/falsify 226 of the 289 benchmarks. With our automatically inferred frame axioms, the tool could verify\u00a0\u2026", "num_citations": "10\n", "authors": ["692"]}
{"title": "Java simulator of real-time scheduling algorithms\n", "abstract": " This document describes the authors' research in application of object-oriented language (Java) in development of real-time system simulation. It also describes advantages and disadvantages of Java, and gives a critical overview of necessary modifications to make Java an acceptable choice for real-time systems. Because of inherent constraints of existing runtime environments, this paper deals only with a part of complete problem, which was possible to simulate on the existing Java platform. We have developed a modular aperiodic scheduling algorithm simulator. The simulator implements EDF scheduling algorithm, but it can easily be extended to support any aperiodic scheduling algorithm.", "num_citations": "10\n", "authors": ["692"]}
{"title": "Counterexample-Guided Bit-Precision Selection\n", "abstract": " Static program verifiers based on satisfiability modulo theories (SMT) solvers often trade precision for scalability to be able to handle large programs. A popular trade-off is to model bitwise operations, which are expensive for SMT solving, using uninterpreted functions over integers. Such an over-approximation improves scalability, but can introduce undesirable false alarms in the presence of bitwise operations that are common in, for example, low-level systems software. In this paper, we present our approach to diagnose the spurious counterexamples caused by this trade-off, and leverage the learned information to lazily and gradually refine the precision of reasoning about bitwise operations in the whole program. Our main insight is to employ a simple and fast type analysis to transform both a counterexample and program into their more precise versions that block the diagnosed spurious\u00a0\u2026", "num_citations": "9\n", "authors": ["692"]}
{"title": "Practical Floating-point Divergence Detection\n", "abstract": " Reducing floating-point precision allocation in HPC programs is of considerable interest from the point of view of obtaining higher performance. However, this can lead to unexpected behavioral deviations from the programmer\u2019s intent. In this paper, we focus on the problem of divergence detection: when a given floating-point program exhibits different control flow (or differs in terms of other discrete outputs) with respect to the same program interpreted under reals. This problem has remained open even for everyday programs such as those that compute convex-hulls. We propose a classification of the divergent behaviors exhibited by programs, and propose efficient heuristics to generate inputs causing divergence. Our experimental results demonstrate that our input generation heuristics are far more efficient than random input generation for divergence detection, and can exhibit divergence even for\u00a0\u2026", "num_citations": "9\n", "authors": ["692"]}
{"title": "A Mixed Real and Floating-Point Solver\n", "abstract": " Reasoning about mixed real and floating-point constraints is essential for developing accurate analysis tools for floating-point programs. This paper presents FPRoCK, a prototype tool for solving mixed real and floating-point formulas. FPRoCK transforms a mixed formula into an equisatisfiable one over the reals. This formula is then solved using an off-the-shelf SMT solver. FPRoCK is also integrated with the PRECiSA static analyzer, which computes a sound estimation of the round-off error of a floating-point program. It is used to detect infeasible computational paths, thereby improving the accuracy of PRECiSA.", "num_citations": "8\n", "authors": ["692"]}
{"title": "Asynchronously communicating visibly pushdown systems\n", "abstract": " We introduce an automata-based formal model suitable for specifying, modeling, analyzing, and verifying asynchronous task-based and message-passing programs. Our model consists of visibly pushdown automata communicating over unbounded reliable point-to-point first-in-first-out queues. Such a combination unifies two branches of research, one focused on task-based models, and the other on models of message-passing programs. Our model generalizes previously proposed models that have decidable reachability in several ways. Unlike task-based models of asynchronous programs, our model allows sending and receiving of messages even when stacks are not empty, without imposing restrictions on the number of context-switches or communication topology. Our model also generalizes the well-known communicating finite-state machines with recognizable channel property allowing (1\u00a0\u2026", "num_citations": "7\n", "authors": ["692"]}
{"title": "Releasing the PSYCO: Using Symbolic Search in Interface Generation for Java\n", "abstract": " The Java PathFinder extension Psyco generates interfaces of Java components using a combination of dynamic symbolic execution and automata learning to explore different combinations of method invocations on a component. Such interfaces are useful in contract-based compositional verification of component-based systems. Psyco relies on testing for validating learned interfaces and currently cannot guarantee that a generated interface is correct. Instead, it simply returns the most recent learned interface once a user-defined time limit is exceeded. In this paper, we report on work that was performed during the 2016 Google Summer of Code. The aim of this work is to extend Psyco with symbolic search. During symbolic search, Psyco uses fully symbolic method summaries for exploring the state space of a component symbolically. We plan to eventually use symbolic search to compute a termination criterion for\u00a0\u2026", "num_citations": "6\n", "authors": ["692"]}
{"title": "Systematic Debugging Methods for Large Scale HPC Computational Frameworks\n", "abstract": " Parallel computational frameworks for high-performance computing are central to the advancement of simulation-based studies in science and engineering. Unfortunately, finding and fixing bugs in these frameworks can be extremely time consuming. Left unchecked, these bugs can drastically diminish the amount of new science that can be performed. This article presents a systematic study of the Uintah Computational Framework and approaches to debug it more incisively. A key insight is to leverage the modular structure of Uintah, which lends itself to systematic debugging. In particular, the authors have developed a new approach based on coalesced stack trace graphs (CSTG) that summarize the system behavior in terms of key control flows manifested through function invocation chains. They illustrate several scenarios for how CSTGs could help efficiently localize bugs, and present a case study of how they\u00a0\u2026", "num_citations": "6\n", "authors": ["692"]}
{"title": "The Dart, the Psyco, and the Doop: Concolic Execution in Java PathFinder and its Applications\n", "abstract": " JDart is a concolic execution extension for Java PathFinder. Concolic execution executes programs with concrete values while recording symbolic constraints. In this way, it combines the benefits of fast concrete execution, with the possibility of generating new concrete values, triggered by symbolic constraints, in order to exercise additional, potentially rare, program behaviors. As is typical with concolic execution engines, JDart can be used for test-case generation. Beyond this basic mode, it has also been used as a component of other tools. In this paper, we describe the main features of JDart, provide usage examples, and give an overview of applications that use JDart. We particularly concentrate on our efforts into making JDart robust enough to handle large, complex systems.", "num_citations": "5\n", "authors": ["692"]}
{"title": "An SMT Theory of Fixed-Point Arithmetic\n", "abstract": " Fixed-point arithmetic is a popular alternative to floating-point arithmetic on embedded systems. Existing work on the verification of fixed-point programs relies on custom formalizations of fixed-point arithmetic, which makes it hard to compare the described techniques or reuse the implementations. In this paper, we address this issue by proposing and formalizing an SMT theory of fixed-point arithmetic. We present an intuitive yet comprehensive syntax of the fixed-point theory, and provide formal semantics for it based on rational arithmetic. We also describe two decision procedures for this theory: one based on the theory of bit-vectors and the other on the theory of reals. We implement the two decision procedures, and evaluate our implementations using existing mature SMT solvers on a benchmark suite we created. Finally, we perform a case study of using the theory we propose to verify properties of\u00a0\u2026", "num_citations": "4\n", "authors": ["692"]}
{"title": "Leveraging Compiler Intermediate Representation for Multi-and Cross-Language Verification\n", "abstract": " Developers nowadays regularly use numerous programming languages with different characteristics and trade-offs. Unfortunately, implementing a software verifier for a new language from scratch is a large and tedious undertaking, requiring expert knowledge in multiple domains, such as compilers, verification, and constraint solving. Hence, only a tiny fraction of the used languages has readily available software verifiers to aid in the development of correct programs. In the past decade, there has been a trend of leveraging popular compiler intermediate representations (IRs), such as LLVM IR, when implementing software verifiers. Processing IR promises out-of-the-box multi- and cross-language verification since, at least in theory, a verifier ought to be able to handle a program in any programming language (and their combination) that can be compiled into the IR. In practice though, to the best of our knowledge\u00a0\u2026", "num_citations": "4\n", "authors": ["692"]}
{"title": "RedLeaf: Towards An Operating System for Safe and Verified Firmware\n", "abstract": " RedLeaf is a new operating system being developed from scratch to utilize formal verification for implementing provably secure firmware. RedLeaf is developed in a safe language, Rust, and relies on automated reasoning using satisfiability modulo theories (SMT) solvers for formal verification. RedLeaf builds on two premises:(1) Rust's linear type system enables practical language safety even for systems with tightest performance and resource budgets (eg, firmware), and (2) a combination of SMT-based reasoning and pointer discipline enforced by linear types provides a unique way to automate and simplify verification effort scaling it to the size of a small OS kernel.", "num_citations": "4\n", "authors": ["692"]}
{"title": "Moving the Needle on Rigorous Floating-point Precision Tuning\n", "abstract": " Virtually all real-valued computations are carried out using floating-point data types and operations. With increasing emphasis on overall computational efficiency, compilers are increasingly attempting to optimize floating-point expressions. Practical reasoning about the correctness of these optimizations requires error analysis procedures that are rigorous (ideally, they can generate proof certificates), can handle a wide variety of operators (eg, transcendentals), and handle all normal programmatic constructs (eg, conditionals and loops). Unfortunately, none of today\u2019s approaches can achieve this combination. This position paper summarizes recent progress achieved in the community on this topic. It then showcases the component techniques present within our own rigorous floating-point precision tuning framework called FPTuner\u2014essentially offering a collection of \u201cgrab and go\u201d tools that others can benefit from. Finally, we present FPTuner\u2019s limitations and describe how we can exploit contemporaneous research to improve it.", "num_citations": "4\n", "authors": ["692"]}
{"title": "FUSED: A Low-cost Online Soft-Error Detector\n", "abstract": " The growth in soft error rates caused by shrinking device geometries and transistor variability can undermine system reliability, requiring cross-layer resilience solutions. In this paper, we make following contributions to this area. First, we introduce a new framework called FUSED in which softerror detectors are automatically compiled from and inserted into application code through the Rose compilation framework that is widely used in HPC. Our error detectors are based on control-flow tracking through predicate transitions. Second, we develop a new heuristic based on the idea of invalid predicate transitions to identify sensitive-code-blocks causing silent data corruption (SDC) in a program\u2019s execution output. New results report in this paper include showing the feasibility of using likely program invariants (in the form of predicate transitions) in realistic code, automation of error detector insertion, use of our empirical\u00a0\u2026", "num_citations": "4\n", "authors": ["692"]}
{"title": "Systematic debugging of concurrent systems using coalesced stack trace graphs\n", "abstract": " A central need during software development of large-scale parallel systems is tools that help to quickly identify the root causes of bugs. Given the massive scale of these systems, tools that highlight changes\u2014say introduced across software versions or their operating conditions (e.g., inputs, schedules)\u2014can prove to be highly effective in practice. Conventional debuggers, while good at presenting details at the problem-site (e.g., crash), often omit contextual information to identify the root causes of the bug. We present a new approach to collect and coalesce stack traces, leading to an efficient summary display of salient system control flow differences in a graphical form called Coalesced Stack Trace Graphs (CSTG). CSTGs have helped us debug situations within a computational framework called Uintah that has been deployed at very large scale. In this paper, we detail CSTGs through case studies in the\u00a0\u2026", "num_citations": "3\n", "authors": ["692"]}
{"title": "Bytecode optimization\n", "abstract": " Today, when Java is entering the embedded market it needs performance enhancements more than ever. Large-scale enterprise applications would also benefit from better code optimization techniques. In this paper we present the results of optimization with an optimizing framework we have developed. We measure the impact of different optimizations on run times in different execution environments and propose further enhancements.", "num_citations": "3\n", "authors": ["692"]}
{"title": "Stochastic Local Search for Solving Floating-Point Constraints\n", "abstract": " We present OL1V3R, a solver for the SMT floating-point theory that is based on stochastic local search (SLS). We adapt for OL1V3R the key ingredients of related work on leveraging SLS to solve the SMT fixed-sized bit-vector theory, and confirm its effectiveness by comparing it with mature solvers. Finally, we discuss the limitations of OL1V3R and propose solutions to make it more powerful.", "num_citations": "2\n", "authors": ["692"]}
{"title": "Consistency-aware scheduling for weakly consistent programs\n", "abstract": " Modern geo-replicated data stores provide high availability by relaxing the underlying consistency requirements. Programs layered over such data stores are called weakly consistent programs. Due to the reduced consistency requirements, they exhibit highly nondeterministic behaviors, some of which might violate program invariants. Therefore, implementing correct weakly consistent programs and reasoning about them is challenging. In this paper, we present a systematic scheduling approach that is aware of the underlying consistency model. Our approach dynamically explores all possible program behaviors allowed by the used data store consistency model, and it evaluates program invariants during the exploration. We implement the approach in a prototype model checker for Antidote, which is a causally consistent key-value data store with convergent con ict handling. We evaluate our tool on several\u00a0\u2026", "num_citations": "2\n", "authors": ["692"]}
{"title": "Towards Automated Differential Program Verification for Approximate Computing\n", "abstract": " Approximate computing is an emerging area for trading off the accuracy of an application for improved performance, lower energy costs, and tolerance to unreliable hardware. However, care has to be taken to ensure that the approximations do not cause significant divergence from the reference implementation. Previous research has proposed various metrics to guarantee several relaxed notions of safety for the design and verification of such approximate applications. However, current approximation verification approaches often lack in either precision or automation. On one end of the spectrum, type-based approaches lack precision, while on the other, proofs in interactive theorem provers require significant manual effort.In this work, we apply automated differential program verification (as implemented in SymDiff) for reasoning about approximations. We show that mutual summaries naturally express many relaxed specifications for approximations, and SMT-based checking and invariant inference can substantially automate the verification of such specifications. We demonstrate that the framework significantly improves automation compared to previous work on using Coq. Our results indicate the feasibility of applying automated verification to the domain of approximate computing in a cost-effective manner.", "num_citations": "2\n", "authors": ["692"]}
{"title": "Unsafe Floating-point to Unsigned Integer Casting Check for GPU Programs\n", "abstract": " Numerical programs usually include type-casting instructions which convert data among different types. Identifying unsafe type-casting is important for preventing undefined program behaviors which cause serious problems such as security vulnerabilities and result non-reproducibility. While many tools had been proposed for handling sequential programs, to our best knowledge, there isn't a tool geared toward GPUs. In this paper, we propose a static analysis based method which points out all potentially unsafe type-casting instructions in a program. To reduce false alarms (which are commonly raised by static analysis), we employ two techniques, manual hints and pre-defined function contracts, and we empirically show that these techniques are effective in practice. We evaluated our method with artificial programs and samples from CUDA SDK. Our implementation is currently being integrated into a GPU program\u00a0\u2026", "num_citations": "2\n", "authors": ["692"]}
{"title": "A better logic and decision procedure for predicate abstraction of heap-manipulating programs\n", "abstract": " Heap-manipulating programs (HMP), which manipulate unbounded linked data structures via pointers, are a major frontier for software model checking. In recent work, we proposed a small logic and inference-rule-based decision procedure and demonstrated their potential by verifying, via predicate abstraction, some simple HMPs. In this work, we generalize and improve our previous results to be practically useful: we allow more than a single pointer field, we permit updating the data stored in heap nodes, we add new primitives and inference rules for cyclic structures, and we greatly improve the performance of our implementation. Experimentally, we are able to verify many more HMP examples, including three small container functions from the Linux kernel. On the theoretical front, we prove NP-hardness for a small fragment of our logic, completeness of our inference rules for a large fragment, and soundness for the full logic.", "num_citations": "2\n", "authors": ["692"]}
{"title": "Study of Integrating Random and Symbolic Testing for Object-Oriented Software\n", "abstract": " Testing is currently the main technique adopted by the industry for improving the quality, reliability, and security of software. In order to lower the cost of manual testing, automatic testing techniques have been devised, such as random and symbolic testing, with their respective trade-offs. For example, random testing excels at fast global exploration of software, while it plateaus when faced with hard-to-hit numerically-intensive execution paths. On the other hand, symbolic testing excels at exploring such paths, while it struggles when faced with complex heap class structures. In this paper, we describe an approach for automatic unit testing of object-oriented software that integrates the two techniques. We leverage feedback-directed unit testing to generate meaningful sequences of constructor+method invocations that create rich heap structures, and we in turn further explore these sequences using dynamic\u00a0\u2026", "num_citations": "1\n", "authors": ["692"]}
{"title": "JDART: A Dynamic Symbolic Analysis Framework\n", "abstract": " We describe JDART, a dynamic symbolic analysis framework for Java. A distinguishing feature of JDART is its modular architecture: the main component that performs dynamic exploration communicates with a component that efficiently constructs constraints and that interfaces with constraint solvers. These components can easily be extended or modified to support multiple constraint solvers or different exploration strategies. Moreover, JDART has been engineered for robustness, driven by the need to handle complex NASA software. These characteristics, together with its recent open sourcing, make JDART an ideal platform for research and experimentation. In the current release, JDART supports the CORAL, SMTInterpol, and Z3 solvers, and is able to handle NASA software with constraints containing bit operations, floating point arithmetic, and complex arithmetic operations (eg, trigonometric and nonlinear). We illustrate how JDART has been used to support other analysis techniques, such as automated interface generation and testing of libraries. Finally, we demonstrate the versatility and effectiveness of JDART, and compare it with state-of-the-art dynamic or pure symbolic execution engines through an extensive experimental evaluation.", "num_citations": "1\n", "authors": ["692"]}