{"title": "Factors that affect software systems development project outcomes: A survey of research\n", "abstract": " Determining the factors that have an influence on software systems development and deployment project outcomes has been the focus of extensive and ongoing research for more than 30 years. We provide here a survey of the research literature that has addressed this topic in the period 1996\u20132006, with a particular focus on empirical analyses. On the basis of this survey we present a new classification framework that represents an abstracted and synthesized view of the types of factors that have been asserted as influencing project outcomes.", "num_citations": "333\n", "authors": ["177"]}
{"title": "A comparison of techniques for developing predictive models of software metrics\n", "abstract": " The use of regression analysis to derive predictive equations for software metrics has recently been complemented by increasing numbers of studies using non-traditional methods, such as neural networks, fuzzy logic models, case-based reasoning systems, and regression trees. There has also been an increasing level of sophistication in the regression-based techniques used, including robust regression methods, factor analysis, and more effective validation procedures. This paper examines the implications of using these methods and provides some recommendations as to when they may be appropriate. A comparison of the various techniques is also made in terms of their modelling capabilities with specific reference to software metrics.", "num_citations": "285\n", "authors": ["177"]}
{"title": "A perspective-based understanding of project success\n", "abstract": " Answering the call for alternative approaches to researching project management, we explore the evaluation of project success from a subjectivist perspective. An in-depth, longitudinal case study of information systems development in a large manufacturing company was used to investigate how various project stakeholders subjectively perceived the project outcome and what evaluation criteria they drew on in doing so. A conceptual framework is developed for understanding and analyzing evaluations of project success, both formal and informal. The framework highlights how different stakeholder perspectives influence the perceived outcome(s) of a project, and how project evaluations may differ between stakeholders and across time.", "num_citations": "268\n", "authors": ["177"]}
{"title": "Software forensics: extending authorship analysis techniques to computer programs\n", "abstract": " Software forensics is the analysis of the syntactic, structural and semantic form of software in order to identify, characterise and discriminate between the authors of software products for some legal purpose. Determining software authorship may be important in several contexts: civil litigation involving allegations of software theft or plagiarism or apportioning liability for software failure; criminal litigation in relation to computer fraud or software attacks on computer systems using viruses and other similar means. Our focus is on forensic analysis of software source code, the structured English-like implementation of the algorithm selected to undertake the task at hand. We use a fictionalised version of a recent case to illustrate the potential of software forensics to provide evidence and also review in detail the judicial reception of such material.", "num_citations": "154\n", "authors": ["177"]}
{"title": "Applications of fuzzy logic to software metric models for development effort estimation\n", "abstract": " Software metrics are measurements of the software development process and product that can be used as variables (both dependent and independent) in models for project management. The most common types of these models are those used for predicting the development effort for a software system based on size, complexity, developer characteristics, and other metrics. Despite the financial benefits from developing accurate and usable models, there are a number of problems that have not been overcome using the traditional techniques of formal and linear regression models. These include the nonlinearities and interactions inherent in complex real-world development processes, the lack of stationarity in such processes, over-commitment to precisely specified values, the small quantities of data often available, and the inability to use whatever knowledge is available where exact numerical values are\u00a0\u2026", "num_citations": "110\n", "authors": ["177"]}
{"title": "A baseline model for software effort estimation\n", "abstract": " Software effort estimation (SEE) is a core activity in all software processes and development lifecycles. A range of increasingly complex methods has been considered in the past 30 years for the prediction of effort, often with mixed and contradictory results. The comparative assessment of effort prediction methods has therefore become a common approach when considering how best to predict effort over a range of project types. Unfortunately, these assessments use a variety of sampling methods and error measurements, making comparison with other work difficult. This article proposes an automatically transformed linear model (ATLM) as a suitable baseline model for comparison against SEE methods. ATLM is simple yet performs well over a range of different project types. In addition, ATLM may be used with mixed numeric and categorical data and requires no parameter tuning. It is also deterministic, meaning\u00a0\u2026", "num_citations": "82\n", "authors": ["177"]}
{"title": "Source code authorship analysis for supporting the cybercrime investigation process\n", "abstract": " Nowadays, in a wide variety of situations, source code authorship identification has become an issue of major concern. Such situations include authorship disputes, proof of authorship in court, cyber attacks in the form of viruses, trojan horses, logic bombs, fraud, and credit card cloning. Source code author identification deals with the task of identifying the most likely author of a computer program, given a set of predefined author candidates. We present a new approach, called the SCAP (Source Code Author Profiles) approach, based on byte-level n-grams in order to represent a source code author\u2019s style. Experiments on data sets of different programming-language (Java, C++ and Common Lisp) and varying difficulty (6 to 30 candidate authors) demonstrate the effectiveness of the proposed approach. A comparison with a previous source code authorship identification study based on more complicated information\u00a0\u2026", "num_citations": "80\n", "authors": ["177"]}
{"title": "A comparison of modeling techniques for software development effort prediction\n", "abstract": " Software metrics are playing an increasingly important role in software development project management, with the need to effectively control the expensive investment of software development of paramount concern. Research examining the estimation of software development effort has been particularly extensive. In this work, regression analysis has been used almost exclusively to derive equations for predicting software process effort. This approach, whilst useful in some cases, also suffers from a number of limitations in relation to data set characteristics. In an attempt to overcome some of these problems, some recent studies have adopted less common modeling methods, such as neural networks, fuzzy logic models and case-based reasoning. In this paper some consideration is given to the use of neural networks and fuzzy models in terms of their appropriateness for the task of effort estimation. A comparison of techniques is also made with specific reference to statistical modeling and to function point analysis, a popular formal method for estimating development size and effort.", "num_citations": "76\n", "authors": ["177"]}
{"title": "Software metrics data analysis\u2014exploring the relative performance of some commonly used modeling techniques\n", "abstract": " Whilst some software measurement research has been unquestionably successful, other research has struggled to enable expected advances in project and process management. Contributing to this lack of advancement has been the incidence of inappropriate or non-optimal application of various model-building procedures. This obviously raises questions over the validity and reliability of any results obtained as well as the conclusions that may have been drawn regarding the appropriateness of the techniques in question. In this paper we investigate the influence of various data set characteristics and the purpose of analysis on the effectiveness of four model-building techniques\u2014three statistical methods and one neural network method. In order to illustrate the impact of data set characteristics, three separate data sets, drawn from the literature, are used in this analysis. In terms of predictive accuracy, it is\u00a0\u2026", "num_citations": "73\n", "authors": ["177"]}
{"title": "Comparative review of functional complexity assessment methods for effort estimation\n", "abstract": " Budgetary constraints are placing increasing pressure on project managers to effectively estimate development effort requirements at the earliest opportunity. With the rising impact of automation on commercial software development, the attention of researchers developing effort estimation models has recently been focused on functional representations of systems, in response to the assertion that development effort is a function of specification content. A number of such models exist; several, however, have received almost no research or industry attention. Project managers wishing to implement a functional assessment and estimation programme are therefore unlikely to be aware of the various methods or how they compare. This paper therefore provides this information, as well as forming a basis for the development and improvement of new methods.< >", "num_citations": "65\n", "authors": ["177"]}
{"title": "Understanding the attitudes, knowledge sharing behaviors and task performance of core developers: A longitudinal study\n", "abstract": " ContextPrior research has established that a few individuals generally dominate project communication and source code changes during software development. Moreover, this pattern has been found to exist irrespective of task assignments at project initiation.ObjectiveWhile this phenomenon has been noted, prior research has not sought to understand these dominant individuals. Previous work considering the effect of team structures on team performance has found that core communicators are the gatekeepers of their teams\u2019 knowledge, and the performance of these members was correlated with their teams\u2019 success. Building on this work, we have employed a longitudinal approach to study the way core developers\u2019 attitudes, knowledge sharing behaviors and task performance change over the course of their project, based on the analysis of repository data.MethodWe first used social network analysis (SNA) and\u00a0\u2026", "num_citations": "64\n", "authors": ["177"]}
{"title": "A systematic mapping study on dynamic metrics and software quality\n", "abstract": " Several important aspects of software product quality can be evaluated using dynamic metrics that effectively capture and reflect the software's true runtime behavior. While the extent of research in this field is still relatively limited, particularly when compared to research on static metrics, the field is growing, given the inherent advantages of dynamic metrics. The aim of this work is to systematically investigate the body of research on dynamic software metrics to identify issues associated with their selection, design and implementation. Mapping studies are being increasingly used in software engineering to characterize an emerging body of research and to identify gaps in the field under investigation. In this study we identified and evaluated 60 works based on a set of defined selection criteria. These studies were further classified and analyzed to identify their relativity to future dynamic metrics research. The\u00a0\u2026", "num_citations": "64\n", "authors": ["177"]}
{"title": "Supporting agile team composition: A prototype tool for identifying personality (In) compatibilities\n", "abstract": " Extensive work in the behavioral sciences tells us that team composition is a complex activity in many disciplines, given the variations inherent across individuals' personalities. The composition of teams to undertake software development is subject to this same complexity. Furthermore, the building of a team to undertake agile software development may be particularly challenging, given the inclusive yet fluid nature of teams in this context. We describe here the development and preliminary evaluation of a prototype tool intended to assist software engineers and project managers in forming agile teams, utilizing information concerning members' personalities as input to this process. Initial assessment of the tool's capabilities by agile development practitioners suggests that it would be of value in supporting the team composition activity in real projects.", "num_citations": "60\n", "authors": ["177"]}
{"title": "Software forensics: old methods for a new science\n", "abstract": " Over the past few years there has been a renewed interest in the science of software authorship identification; this area of research has been termed 'software forensics'. This paper examines the range of possible measures that can be used to establish commonality and variance in programmer style, with a view to determining program authorship. It also describes some applications of these techniques, particularly for establishing the originator of programs in cases of security breach, plagiarism and computer fraud.", "num_citations": "60\n", "authors": ["177"]}
{"title": "Communication and personality profiles of global software developers\n", "abstract": " ContextPrior research has established that a small proportion of individuals dominate team communication during global software development. It is not known, however, how these members\u2019 contributions affect their teams\u2019 knowledge diffusion process, or whether their personality profiles are responsible for their dominant presence.ObjectiveWe set out to address this gap through the study of repository artifacts.MethodArtifacts from ten teams were mined from the IBM Rational Jazz repository. We employed social network analysis (SNA) to group practitioners into two clusters, Top Members and Others, based on the numbers of messages they communicated and their engagement in task changes. SNA metrics (density, in-degree and closeness) were then used to study practitioners\u2019 importance in knowledge diffusion. Thereafter, we performed psycholinguistic analysis on practitioners\u2019 messages using linguistic\u00a0\u2026", "num_citations": "53\n", "authors": ["177"]}
{"title": "Examining the significance of high-level programming features in source code author classification\n", "abstract": " The use of Source Code Author Profiles (SCAP) represents a new, highly accurate approach to source code authorship identification that is, unlike previous methods, language independent. While accuracy is clearly a crucial requirement of any author identification method, in cases of litigation regarding authorship, plagiarism, and so on, there is also a need to know why it is claimed that a piece of code is written by a particular author. What is it about that piece of code that suggests a particular author? What features in the code make one author more likely than another? In this study, we describe a means of identifying the high-level features that contribute to source code authorship identification using as a tool the SCAP method. A variety of features are considered for Java and Common Lisp and the importance of each feature in determining authorship is measured through a sequence of experiments in which we\u00a0\u2026", "num_citations": "52\n", "authors": ["177"]}
{"title": "Software source code sizing using fuzzy logic modeling\n", "abstract": " Knowing the likely size of a software product before it has been constructed is potentially beneficial in project management: for instance, size can be an important factor in determining an appropriate development/integration schedule, and it can be a significant input in terms of the allocation of personnel and other resources. In this study we consider the applicability of fuzzy logic modeling methods to the task of software source code sizing, using a previously published data set. Our results suggest that, particularly with refinement using data and knowledge, fuzzy predictive models can outperform their traditional regression-based counterparts.", "num_citations": "50\n", "authors": ["177"]}
{"title": "A fuzzy logic approach to computer software source code authorship analysis\n", "abstract": " Software source code authorship analysis has become an important area in recent years with promising applications in both the legal sector (such as proof of ownership and software forensics) and the education sector (such as plagiarism detection and assessing style). Authorship analysis encompasses the sub-areas of author discrimination, author characterization, and similarity detection (also referred to as plagiarism detection). While a large number of metrics have been proposed for this task, many borrowed or adapted from the area of computational linguistics, there is a difficulty with capturing certain types of information in terms of quantitative measurement. Here it is proposed that existing numerical metrics should be supplemented with fuzzy-logic linguistic variables to capture more subjective elements of authorship, such as the degree to which comments match the actual source code\u2019s behavior. These variables avoid the need for complex and subjective rules, replacing these with an expert\u2019s judgement. Fuzzy-logic models may also help to overcome problems with small data sets for calibrating such models. Using authorship discrimination as a test case, the utility of objective and fuzzy measures, singularly and in combination, is assessed as well as the consistency of the measures between counters.", "num_citations": "47\n", "authors": ["177"]}
{"title": "Qualitative research on software development: a longitudinal case study methodology\n", "abstract": " This paper reports the use of a qualitative methodology for conducting longitudinal case study research on software development. We provide a detailed description and explanation of appropriate methods of qualitative data collection and analysis that can be utilized by other researchers in the software engineering field. Our aim is to illustrate the utility of longitudinal case study research, as a complement to existing methodologies for studying software development, so as to enable the community to develop a fuller and richer understanding of this complex, multi-dimensional phenomenon. We discuss the insights gained and lessons learned from applying a longitudinal qualitative approach to an empirical case study of a software development project in a large multi-national organization. We evaluate the methodology used to emphasize its strengths and to address the criticisms traditionally made of\u00a0\u2026", "num_citations": "45\n", "authors": ["177"]}
{"title": "A taxonomy of data quality challenges in empirical software engineering\n", "abstract": " Reliable empirical models such as those used in software effort estimation or defect prediction are inherently dependent on the data from which they are built. As demands for process and product improvement continue to grow, the quality of the data used in measurement and prediction systems warrants increasingly close scrutiny. In this paper we propose a taxonomy of data quality challenges in empirical software engineering, based on an extensive review of prior research. We consider current assessment techniques for each quality issue and proposed mechanisms to address these issues, where available. Our taxonomy classifies data quality issues into three broad areas: first, characteristics of data that mean they are not fit for modeling, second, data set characteristics that lead to concerns about the suitability of applying a given model to another data set, and third, factors that prevent or limit data accessibility\u00a0\u2026", "num_citations": "41\n", "authors": ["177"]}
{"title": "Software forensics for discriminating between program authors using case-based reasoning, feedforward neural networks and multiple discriminant analysis\n", "abstract": " Software forensics is the field that, by treating pieces of program source code as linguistically and stylistically analyzable entities, attempts to investigate computer program authorship. This can be performed with the goal of identification, discrimination, or characterization of authors. In this paper we extract a set of 26 standard authorship metrics from 351 programs by 7 different authors. The use of feedforward neural networks, multiple discriminant analysis, and case-based reasoning is then investigated in terms of classification accuracy for the authors on both training and testing samples. The first two techniques produce remarkably similar results, with the best results coming from the case-based reasoning models. All techniques have high prediction accuracy rates, supporting the feasibility of the task of discriminating program authors based on source-code measurements.", "num_citations": "41\n", "authors": ["177"]}
{"title": "FULSOME: fuzzy logic for software metric practitioners and researchers\n", "abstract": " There has been increasing interest in recent times for using fuzzy logic techniques to represent software metric models, especially those predicting the software development effort. The use of fuzzy logic for this application area offers several advantages when compared to other commonly-used techniques. These include the use of a single model with different levels of precision for the inputs and outputs used throughout the development life-cycle, the possibility of model development with little or no data, and its effectiveness when used as a communication tool. The use of fuzzy logic in any applied field, however, requires that suitable tools are available for both practitioners and researchers-satisfying both interface- and functionality-related requirements. After outlining some of the specific needs of the software metrics community, including results from a survey of software developers on this topic, this paper\u00a0\u2026", "num_citations": "41\n", "authors": ["177"]}
{"title": "Contextual relevance feedback in web information retrieval\n", "abstract": " In this paper, we present an alternative approach to the problem of contextual relevance feedback in web-based information retrieval. Our approach utilises a rich contextual model that exploits a user's implicit and explicit data. Each user's implicit data are gathered from their Internet search histories on their local machine. The user's explicit data are captured from a lexical database, a shared contextual knowledge base and domain-specific concepts using data mining techniques and a relevance feedback approach. This data is later used by our approach to modify queries to more accurately reflect the user's interests as well as to continually build the user's contextual profile and a shared contextual knowledge base. Finally, the approach retrieves personalised or contextual search results from the search engine using the modified/expanded query. Preliminary experiments indicate that our approach has the\u00a0\u2026", "num_citations": "39\n", "authors": ["177"]}
{"title": "Alternatives to regression models for estimating software projects\n", "abstract": " The use of \u2018standard\u2019 regression analysis to derive predictive equations for software development has recently been complemented by increasing numbers of analyses using less common methods, such as neural networks, fuzzy logic models, and regression trees.  This paper considers the implications of using these methods and provides some recommendations as to when they may  be appropriate.  A comparison of techniques is also made in terms of their modelling capabilities with specific reference to function point analysis.", "num_citations": "39\n", "authors": ["177"]}
{"title": "IDENTIFIED (Integrated Dictionary-based Extraction of Non-language-dependent Token Information for Forensic Identification, Examination, and Discrimination): A dictionary-based\u00a0\u2026\n", "abstract": " The frequency and severity of computer-based attacks such as viruses and worms, logic bombs, trojan horses, computer fraud, and plagiarism of software code have all become of increasing concern to many of those involved with information systems. Part of the difficulty experienced in collecting evidence regarding the attack or theft in such situations has been the definition and collection of appropriate measurements to use in models of authorship, With this purpose in mind a system called IDENTIFIED is being developed to assist with the task of software forensics which is the use of software code authorship analysis for legal or official purposes. IDENTIFIED uses combinations of wildcards and special characters to define count-based metrics, allows for hierarchical metametric definitions, automates much of the file handling task, extracts metric values from source code, and assists with the analysis and modelling\u00a0\u2026", "num_citations": "37\n", "authors": ["177"]}
{"title": "User participation in contemporary IS development: An IS management perspective\n", "abstract": " User participation in IS development has become an established practice perceived to improve both the development process and its outcomes. Recently, however, Markus and Mao (2004) have highlighted the need to revisit user participation in light of the changing IS development environment. A survey of New Zealand organisations with 200 or more full-time employees was undertaken in order to obtain an updated assessment of the actual practice of user participation in IS development projects. The results suggest that user participation continues to be a dominant aspect of IS development. The paper provides empirical data on the common reasons for having users participate and the form and types of user activities in development. The responding organisations perceived user participation to be beneficial to IS development in their recent IS projects, and many intended continuing or increasing their current levels of user participation in the future.", "num_citations": "36\n", "authors": ["177"]}
{"title": "Evolving connectionist system versus algebraic formulas for prediction of renal function from serum creatinine\n", "abstract": " Evolving connectionist system versus algebraic formulas for prediction of renal function from serum creatinine.BackgroundIn clinical trials, equation 7 from the Modification of Diet in Renal Disease (MDRD) Study is the most accurate formula for the prediction of glomerular filtration rate (GFR) from serum creatinine. An alternative approach has been developed using evolving connectionist systems (ECOS), which are novel computing structures that can be trained to generate accurate output from a given set of input variables. This study aims to compare the prediction errors associated with each method, using data that reproduce routine clinical practice as opposed to the artificial setting of clinical trials.MethodsThe methods were compared using 441 radioisotope measurements of GFR in 178 chronic kidney disease patients from 12 centers in Australia and New Zealand. All clinical and laboratory measurements were\u00a0\u2026", "num_citations": "33\n", "authors": ["177"]}
{"title": "GQM++ a full life cycle framework for the development and implementation of software metric programs\n", "abstract": " One of the more challenging aspects concerning the development of a sound and maturing software process is the determination of an appropriate and relevant supporting measurement program. At one time process and product measurement was a fantasy, something that was undertaken by other organisations. With the advent of process improvement models, metric program development has now gained extensive support. Moreover, the use of frameworks such as the Goal/Question/Metric (GQM) approach has enabled managers to more effectively and confidently select candidate metrics or measures. It is our contention however, that GQM only takes us some way to the development of a feasible program, and that there are other considerations that should be made before embarking on organisation-wide collection schemes. These considerations include in particular an explicit acknowledgment of the costs and benefits of data collection, the specification of potential modelling and analysis methods, and the determination of how any results might be \u2018fed back\u2019into the process. We therefore propose an extension to the GQM framework, which we have called GQM++, to enable the more formal consideration of such issues. Although as yet untested, it is our opinion that explicit specification of these other metric program characteristics will result in the development of more comprehensive, pragmatic and feasible data collection and analysis processes.", "num_citations": "33\n", "authors": ["177"]}
{"title": "An ontology driven approach for knowledge discovery in biomedicine\n", "abstract": " The explosion of biomedical data and the growing number of disparate data sources are exposing researchers to a new challenge - how to acquire, maintain and share knowledge from large and distributed databases in the context of rapidly evolving research. This paper describes research in progress on a new methodology for leveraging the semantic content of ontologies to improve knowledge discovery in complex and dynamic domains. It aims to build a multi-dimensional ontology able to share knowledge from different experiments undertaken across aligned research communities in order to connect areas of science seemingly unrelated to the area of immediate interest. We analyze how ontologies and data mining may facilitate biomedical data analysis and present our efforts to bridge the two fields, knowledge discovery in Biomedicine, and ontology learning for successful data mining in large databases. In particular we present an initial biomedical ontology case study and how we are integrating that with a data mining environment.", "num_citations": "32\n", "authors": ["177"]}
{"title": "Establishing relationships between specification size and software process effort in CASE environments\n", "abstract": " Advances in software process technology have rendered some existing methods of size assessment and effort estimation inapplicable. The use of automation in the software process, however, provides an opportunity for the development of more appropriate software size-based effort estimation models. A specification-based size assessment method has therefore been developed and tested in relation to process effort on a preliminary set of systems. The results of the analysis confirm the assertion that, within the automated environment class, specification size indicators (that may be automatically and objectively derived) are strongly related to process effort requirements.", "num_citations": "32\n", "authors": ["177"]}
{"title": "Evolving ontologies for intelligent decision support\n", "abstract": " The explosive growth in volumes of data and the growing number of disparate data sources are exposing researchers to a \u2018new\u2019 challenge \u2014 how to acquire, maintain and share knowledge from large and distributed databases in the context of rapidly evolving research. Our approach addresses such questions by integrating soft computing techniques and ontology engineering. The primary outcome of this work is a process that facilitates the creation of evolving ontologies that are able to represent the dynamic and uncertain nature of domains in order to provide constant and ongoing support for the decision making process over time.", "num_citations": "31\n", "authors": ["177"]}
{"title": "A visual analysis approach to update systematic reviews\n", "abstract": " Context: In order to preserve the value of Systematic Reviews (SRs), they should be frequently updated considering new evidence that has been produced since the completion of the previous version of the reviews. However, the update of an SR is a time consuming, manual task. Thus, many SRs have not been updated as they should be and, therefore, they are currently outdated. Objective: The main contribution of this paper is to support the update of SRs. Method: We propose USR-VTM, an approach based on Visual Text Mining (VTM) techniques, to support selection of new evidence in the form of primary studies. We then present a tool, named Revis, which supports our approach. Finally, we evaluate our approach through a comparison of outcomes achieved using USR-VTM versus the traditional (manual) approach. Results: Our results show that USR-VTM increases the number of studies correctly included\u00a0\u2026", "num_citations": "30\n", "authors": ["177"]}
{"title": "Fuzzy logic for software metric models throughout the development life-cycle\n", "abstract": " One problem faced by managers who are using project management models is the elicitation of numerical inputs. Obtaining these with any degree of confidence early in a project is not always feasible. Related to this difficulty is the risk of precisely specified outputs from models leading to overcommitment. These problems can be seen as the collective failure of software measurements to represent the inherent uncertainties in managers' knowledge of the development products, resources, and processes. It is proposed that fuzzy logic techniques can help to overcome some of these difficulties by representing the imprecision in inputs and outputs, as well as providing a more expert-knowledge based approach to model building. The use of fuzzy logic for project management however should not be the same throughout the development life cycle. Different levels of available information and desired precision suggest\u00a0\u2026", "num_citations": "30\n", "authors": ["177"]}
{"title": "Exploring software developers\u2019 work practices: Task differences, participation, engagement, and speed of task resolution\n", "abstract": " In seeking to understand the processes enacted during software development, an increasing number of studies have mined software repositories. In particular, studies have endeavored to show how teams resolve software defects. Although much of this work has been useful, we contend that large-scale examinations across the range of activities that are commonly performed, beyond defect-related issues alone, would help us to more fully understand the reasons why defects occur as well as their consequences. More generally, these explorations would reveal how team processes occur during all software development efforts. We thus extend such studies by investigating how software practitioners work while undertaking the range of software tasks that are typically performed. Multiple forms of analyses of a longitudinal case study reveal that software practitioners were mostly involved in fixing defects, and that\u00a0\u2026", "num_citations": "29\n", "authors": ["177"]}
{"title": "Data quality in empirical software engineering: a targeted review\n", "abstract": " Context: The utility of prediction models in empirical software engineering (ESE) is heavily reliant on the quality of the data used in building those models. Several data quality challenges such as noise, incompleteness, outliers and duplicate data points may be relevant in this regard. Objective: We investigate the reporting of three potentially influential elements of data quality in ESE studies: data collection, data pre-processing, and the identification of data quality issues. This enables us to establish how researchers view the topic of data quality and the mechanisms that are being used to address it. Greater awareness of data quality should inform both the sound conduct of ESE research and the robust practice of ESE data collection and processing. Method: We performed a targeted literature review of empirical software engineering studies covering the period January 2007 to September 2012. A total of 221\u00a0\u2026", "num_citations": "28\n", "authors": ["177"]}
{"title": "Rigor in software complexity measurement experimentation\n", "abstract": " The lack of widespread industry acceptance of much of the research into the measurement of software complexity must be due at least in part to the lack of experimental rigor associated with many of the studies. This article examines 13 areas in which previous empirical problems have arisen, citing examples where appropriate, and provides recommendations regarding more adequate procedures.", "num_citations": "27\n", "authors": ["177"]}
{"title": "The true role of active communicators: an empirical study of Jazz core developers\n", "abstract": " Context: Interest in software engineering (SE) methodologies and tools has been complemented in recent years by research efforts oriented towards understanding the human processes involved in software development. This shift has been imperative given reports of inadequately performing teams and the consequent growing emphasis on individuals and team relations in contemporary SE methods. Objective: While software repositories have frequently been studied with a view to explaining such human processes, research has tended to use primarily quantitative analysis approaches. There is concern, however, that such approaches can provide only a partial picture of the software process. Given the way human behavior is nuanced within psychological and social contexts, it has been asserted that a full understanding may only be achieved through deeper contextual enquiries. Method: We have followed such\u00a0\u2026", "num_citations": "26\n", "authors": ["177"]}
{"title": "FULSOME: A fuzzy logic modeling tool for software metricians\n", "abstract": " There has been a growing body of literature suggesting that some of the problems faced by software development project managers can be at least partially overcome by using fuzzy logic techniques. However, one issue that has been generally overlooked in this recommendation is the means by which these \"software metricians\" can collect data for, develop, and interpret fuzzy logic models in practice. We describe a freely available system that has been built with this in mind called FULSOME (FUzzy Logic for SOftware MEtrics). While there are many tools available for developing fuzzy models, it is suggested that before there will be real adoption of such techniques by project managers there will need to be suitable tools that support their particular workflows and that use appropriate terminology. Another requirement will be the development of some standard procedures and definitions for such models. Issues\u00a0\u2026", "num_citations": "25\n", "authors": ["177"]}
{"title": "Modelling software processes: a focus on objectives\n", "abstract": " Existing software process models such as Waterfall and XP are characterised by unstated assumptions, a consequence of which is that we can not easily compare models or transfer data from one model to another. This means that software planners have no mechanism for selecting process activities that are best suited to individual projects. In this paper, we propose a framework for modelling software processes that supports representation and comparison of different kinds of software process. Our framework is based on a lift in focus from'choosing activities' to'identifying project objectives and selecting activities to meet those objectives'. We overview some evidence to support the claims of representation and comparison and discuss benefits and limitations of the approach.", "num_citations": "22\n", "authors": ["177"]}
{"title": "Availability of antidotes, antivenoms, and antitoxins in New Zealand hospital pharmacies\n", "abstract": " Aim To assess the adequacy of the types and quantities of antidotes, antivenoms and antitoxins held by New Zealand hospital pharmacies.Methods A list of 61 antidotes, antivenoms, antitoxins and their various forms was developed following literature review and consideration of national pharmaceutical listings. An Internet-accessible survey was then developed, validated and, during the period 28 February to 7 April 2014, sent to 24 hospital pharmacies nationally for completion. Results were assessed and compared with published guidelines for adequate stocking of antidotes in hospitals that provide emergency care.Results The response rate for the survey was 100%. Wide variation in stock levels were reported with only Nacetylcysteine and octreotide held in adequate quantities by all hospitals to manage a single patient for 24 hours. While archaic compounds were still stocked, newer and more effective\u00a0\u2026", "num_citations": "21\n", "authors": ["177"]}
{"title": "Investigating a conceptual construct for software context\n", "abstract": " A growing number of empirical software engineering researchers suggest that a complementary focus on theory is required if the discipline is to mature. A first step in theory-building involves the establishment of suitable theoretical constructs. For researchers studying software projects, the lack of a theoretical construct for context is problematic for both experimentation and effort estimation. For experiments, insufficiently understood contextual factors confound results, and for estimation, unstated contextual factors affect estimation reliability. We have earlier proposed a framework that we suggest may be suitable as a construct for context ie represents a minimal, spanning set for the space of software contexts. The framework has six dimensions, described as Who, Where, What, When, How and Why. In this paper, we report the outcomes of a pilot study to test its suitability by categorising contextual factors from the\u00a0\u2026", "num_citations": "20\n", "authors": ["177"]}
{"title": "An ontology engineering approach for knowledge discovery from data in evolving domains\n", "abstract": " An ontology engineering approach for knowledge discovery from data in evolving domains P. Gottgtroy, N. Kasabov & S. Macdonell Knowledge Engineering and Discovery Institute, Auckland University of Technology, New Zealand Abstract Knowledge discovery in evolving domains presents several challenges in information extraction and knowledge acquisition from heterogeneous, distributed, dynamic data sources. We define an evolving process if the process is developing, changing over time in a continuous manner. Examples of such domains include biological sciences, medical sciences, and social sciences, among others. This paper describes research in progress on a new methodology for leveraging the semantic content of ontologies to improve knowledge discovery in complex and dynamical domains. We consider in this initial stage the problem of how to acquire previous knowledge from data and then use this information in the context of ontology engineering. The first part of this paper concerns some aspects", "num_citations": "20\n", "authors": ["177"]}
{"title": "Identified: Software authorship analysis with case-based reasoning\n", "abstract": " Software forensics is the use of authorship analysis techniques to analyse computer programs for a legal or official purpose. This generally consists of plagiarism detection and malicious code analysis. IDENTIFIED is a system that has been designed to assist with the extraction of count based metrics from source code, and with the development of models of authorship using statistical and machine learning approaches. Software forensic models can be used for identification, classification, characterisation, and intent analysis. One of the more promising methods for identification is case-based reasoning, where samples of code can be compared to those collected from known authors.", "num_citations": "20\n", "authors": ["177"]}
{"title": "Metric selection for effort assessment in multimedia systems development\n", "abstract": " This paper describes ongoing research directed at formulating a set of appropriate metrics for assessing effort requirements for multimedia systems development. An exploratory investigation of the factors that are considered by industry to be influential in determining development effort is presented. This work incorporates the use of a GQM framework to assist the metric selection process from a literature basis, followed by an industry questionnaire. The results provide some useful insights into contemporary project management practices in relation to multimedia systems.", "num_citations": "19\n", "authors": ["177"]}
{"title": "Data gathering for actor analyses: A research note on the collection and aggregation of individual respondent data for MACTOR\n", "abstract": " The augmentation of future studies with data on actors and their interactions is suggested as a means to reduce uncertainty and to account for extreme or unexpected future outcomes due to the involvement of multiple actors and their competing perspectives and options. In the context of New Zealand\u2019s health workforce forecasting environment, this research note presents a systematic method to gather and aggregate actor data developed for a recent foresight study. The method identifies the issues encountered and solutions developed when gathering data from time poor respondents representing diverse and sometimes oppositional actors, and for the coding and aggregation of these data for use in LIPSOR\u2019s actor analysis tool, MACTOR. Worked examples are provided to demonstrate the method\u2019s application with the software.", "num_citations": "18\n", "authors": ["177"]}
{"title": "Software forensics applied to the task of discriminating between program authors\n", "abstract": " Software forensics is here regarded as the particular field of inquiry that, by treating pieces of program source code as linguistically and stylistically analyzable entities, attempts to investigate various aspects of computer program authorship. These inquiries could be performed with any number of goals in mind, including those of intensification, discrimination and characterization of authors. In this paper we extract a set of 26 authorship-related metrics from 351 source code programs, written by 7 different authors. The use of feed-forward neural network (FFNN), multiple discriminant analysis (MDA), and case-based reasoning (CBR) models for discriminating these programs are then investigated in terms of classification accuracy for the authors on both training and testing (holdout) samples. The first two techniques (FFNN and MDA) produce remarkably similar results, with the overall best results coming from the CBR models. All of the examined modelling techniques have prediction accuracy rates of over 80% supporting the claim that it is feasible to use such techniques for the task of discriminating program authors based on source-code measurements in a majority of cases.", "num_citations": "17\n", "authors": ["177"]}
{"title": "Personality profiles of global software developers\n", "abstract": " Context: Individuals' personality traits have been shown to influence their behavior during team work. In particular, positive group attitudes are said to be essential for distributed and global software development efforts where collaboration is critical to project success. Objective: Given this, we have sought to study the influence of global software practitioners' personality profiles from a psycholinguistic perspective. Method: Artifacts from ten teams were selected from the IBM Rational Jazz repository and mined. We employed social network analysis (SNA) techniques to identify and group practitioners into two clusters based on the numbers of messages they communicated, Top Members and Others, and used standard statistical techniques to assess practitioners' engagement in task changes associated with work items. We then performed psycholinguistic analysis on practitioners' messages using linguistic\u00a0\u2026", "num_citations": "16\n", "authors": ["177"]}
{"title": "What affects team behavior? Preliminary linguistic analysis of communications in the Jazz repository\n", "abstract": " There is a growing belief that understanding and addressing the human processes employed during software development is likely to provide substantially more value to industry than yet more recommendations for the implementation of various methods and tools. To this end, considerable research effort has been dedicated to studying human issues as represented in software artifacts, due to its relatively unobtrusive nature. We have followed this line of research and have conducted a preliminary study of team behaviors using data mining techniques and linguistic analysis. Our data source, the IBM Rational Jazz repository, was mined and data from three different project areas were extracted. Communications in these projects were then analyzed using the LIWC linguistic analysis tool. We found that although there are some variations in language use among teams working on project areas dedicated to different\u00a0\u2026", "num_citations": "16\n", "authors": ["177"]}
{"title": "Towards a metrics suite for Object-Relational mappings\n", "abstract": " Object-relational (O/R) middleware is frequently used in practice to bridge the semantic gap (the \u2018impedance mismatch\u2019) between object-oriented application systems and relational database management systems (RDBMSs). If O/R middleware is employed, the object model needs to be linked to the relational schema. Following the so-called forward engineering approach, the developer is faced with the challenge of choosing from a variety of mapping strategies for class associations and inheritance relationships. These mapping strategies have different impacts on the characteristics of application systems, such as their performance or maintainability. Quantifying these mapping impacts via metrics is considered beneficial in the context of O/R mapping tools since such metrics enable an automated and differentiated consideration of O/R mapping strategies. In this paper, the foundation of a metrics suite for\u00a0\u2026", "num_citations": "16\n", "authors": ["177"]}
{"title": "On satisfying the Android OS community: User feedback still central to developers' portfolios\n", "abstract": " End-users play an integral role in identifying requirements, validating software features' usefulness, locating defects, and in software product evolution in general. Their role in these activities is especially prominent in online application distribution platforms (OADPs), where software is developed for many potential users, and for which the traditional processes of requirements gathering and negotiation with a single group of end-users do not apply. With such vast access to end-users, however, comes the challenge of how to prioritize competing requirements in order to satisfy previously unknown user groups, especially with early releases of a product. One highly successful product that has managed to overcome this challenge is the Android Operating System (OS). While the requirements of early versions of the Android OS likely benefited from market research, new features in subsequent releases appear to have\u00a0\u2026", "num_citations": "15\n", "authors": ["177"]}
{"title": "An automatic architecture reconstruction and refactoring framework\n", "abstract": " A variety of sources have noted that a substantial proportion of non trivial software systems fail due to unhindered architectural erosion. This design deterioration leads to low maintainability, poor testability and reduced development speed. The erosion of software systems is often caused by inadequate understanding, documentation and maintenance of the desired implementation architecture. If the desired architecture is lost or the deterioration is advanced, the reconstruction of the desired architecture and the realignment of this desired architecture with the physical architecture both require substantial manual analysis and implementation effort. This paper describes the initial development of a framework for automatic software architecture reconstruction and source code migration. This framework offers the potential to reconstruct the conceptual architecture of software systems and to automatically migrate\u00a0\u2026", "num_citations": "15\n", "authors": ["177"]}
{"title": "Exploring the links between software development task type, team attitudes and task completion performance: Insights from the Jazz repository\n", "abstract": " ContextIn seeking to better understand the impact of various human factors involved in software development, and how teams\u2019 attitudes relate to their performance, increasing attention is being given to the study of team-related artefacts. In particular, researchers have conducted numerous studies on a range of team communication channels to explore links between developers\u2019 language use and the incidence of software bugs in the products they delivered. Comparatively limited attention has been paid, however, to the full range of software tasks that are commonly performed during the development and delivery of software systems, in spite of compelling evidence pointing to the need to understand teams\u2019 attitudes more widely.ObjectiveWe were therefore motivated to study the relationships between task type and team attitudes, and how attitudes expressed in teams\u2019 communications might be related to their task\u00a0\u2026", "num_citations": "14\n", "authors": ["177"]}
{"title": "Managing Requirements Change the Informal Way\n", "abstract": " Software has always been considered as malleable. Changes to software requirements are inevitable during the development process. Despite many software engineering advances over several decades, requirements changes are a source of project risk, particularly when businesses and technologies are evolving rapidly. Although effectively managing requirements changes is a critical aspect of software engineering, conceptions of requirements change in the literature and approaches to their management in practice still seem rudimentary. The overall goal of this study is to better understand the process of requirements change management. We present findings from an exploratory case study of requirements change management in a globally distributed setting. In this context we noted a contrast with the traditional models of requirements change. In theory, change control policies and formal processes are considered as a natural strategy to deal with requirements changes. Yet we observed that \u201cinformal requirements changes\u201d(InfRc) were pervasive and unavoidable. Our results reveal an equally \u2018natural\u2019informal change management process that is required to handle InfRc in parallel. We present a novel model of requirements change which, we argue, better represents the phenomenon and more realistically incorporates both the informal and formal types of change.", "num_citations": "14\n", "authors": ["177"]}
{"title": "Early experiences in measuring multimedia systems development effort\n", "abstract": " The development of multimedia information systems must be managed and controlled just as it is for other generic system types. This paper proposes an approach for assessing multimedia component and system characteristics with a view to ultimately using these features to estimate the associated development effort. Given the different nature of multimedia systems, existing metrics do not appear to be entirely useful in this domain; however, some general principles can still be applied in analysis. Some basic assertions concerning the influential characteristics of multimedia systems are made and a small preliminary set of data is evaluated.", "num_citations": "14\n", "authors": ["177"]}
{"title": "Understanding class-level testability through dynamic analysis\n", "abstract": " It is generally acknowledged that software testing is both challenging and time-consuming. Understanding the factors that may positively or negatively affect testing effort will point to possibilities for reducing this effort. Consequently there is a significant body of research that has investigated relationships between static code properties and testability. The work reported in this paper complements this body of research by providing an empirical evaluation of the degree of association between runtime properties and class-level testability in object-oriented (OO) systems. The motivation for the use of dynamic code properties comes from the success of such metrics in providing a more complete insight into the multiple dimensions of software quality. In particular, we investigate the potential relationships between the runtime characteristics of production code, represented by Dynamic Coupling and Key Classes, and\u00a0\u2026", "num_citations": "13\n", "authors": ["177"]}
{"title": "Maximising data retention from the ISBSG repository\n", "abstract": " Background: In 1997 the International Software Benchmarking Standards Group (ISBSG) began to collect data on software projects. Since then they have provided copies of their repository to researchers and practitioners, through a sequence of releases of increasing size.  Problem: Questions over the quality and completeness of the data in the repository have led some researchers to discard substantial proportions of the data in terms of observations, and to discount the use of some variables in the modelling of, among other things, software development effort. In some cases the details of the discarding of data has received little mention and minimal justification.  Method: We describe the process we used in attempting to maximise the amount of data retained for modelling software development effort at the project level, based on previously completed projects that had been sized using IFPUG/NESMA function point analysis (FPA) and recorded in the repository.  Results: Through justified formalisation of the data set and domain-informed refinement we arrive at a final usable data set comprising 2862 (of 3024) observations across thirteen variables.  Conclusion: a methodical approach to the pre-processing of data can help to ensure that as much data is retained for modelling as possible. Assuming that the data does reflect one or more underlying models, such retention should increase the likelihood of robust models being developed.", "num_citations": "13\n", "authors": ["177"]}
{"title": "Finding faults: A scoping study of fault diagnostics for Industrial Cyber\u2013Physical Systems\n", "abstract": " Context:As Industrial Cyber\u2013Physical Systems (ICPS) become more connected and widely-distributed, often operating in safety-critical environments, we require innovative approaches to detect and diagnose the faults that occur in them.Objective:We profile fault identification and diagnosis techniques employed in the aerospace, automotive, and industrial control domains. Each of these sectors has adopted particular methods to meet their differing diagnostic needs. By examining both theoretical presentations as well as case studies from production environments, we present a profile of the current approaches being employed and identify gaps.Methodology:A scoping study was used to identify and compare fault detection and diagnosis methodologies that are presented in the current literature. We created categories for the different diagnostic approaches via a pilot study and present an analysis of the trends that\u00a0\u2026", "num_citations": "12\n", "authors": ["177"]}
{"title": "What can developers' messages tell us? a psycholinguistic analysis of jazz teams' attitudes and behavior patterns\n", "abstract": " Reports that communication and behavioral issues contribute to inadequately performing software teams have fuelled a wealth of research aimed at understanding the human processes employed during software development. The increasing level of interest in human issues is particularly relevant for agile and global software development approaches that emphasize the importance of people and their interactions during projects. While mature analysis techniques in behavioral psychology have been recommended for studying such issues, particularly when using archives and artifacts, these techniques have rarely been used in software engineering research. We utilize these techniques under an embedded case study approach to examine whether IBM Rational Jazz practitioners' behaviors change over project duration and whether certain tasks affect teams' attitudes and behaviors. We found highest levels of\u00a0\u2026", "num_citations": "12\n", "authors": ["177"]}
{"title": "A simulation framework to support software project (re) planning\n", "abstract": " Planning and replanning software projects involves selecting activities according to organisational policies, project goals and contexts, deciding how to effect the activities, and dealing with uncertainty in activity outputs. There is at the present time no general model to support project managers with all of these tasks. The contributions of this paper are to propose a set of properties that are desirable in a model for (re)planning and to create a framework based on these properties. The purpose of the framework is to support the modelling and simulation of (re)planning during software projects. Key aspects of the framework are a focus on project objectives as drivers of activity selection, and activity prediction that supports uncertainty and that may be based on previous activity data, expert opinion or experimental evidence. We present a 'proof-of-concept' case study to illustrate how the framework can be applied to\u00a0\u2026", "num_citations": "12\n", "authors": ["177"]}
{"title": "Integrating local and personalised modelling with global ontology knowledge bases for biomedical and bioinformatics decision support\n", "abstract": " A novel ontology based decision support framework and a development platform are described, which allow for the creation of global knowledge representation for local and personalised modelling and decision support. The main modules are: an ontology module; and a machine learning module. Both modules evolve through continuous learning from new data. Results from the machine learning procedures can be entered back to the ontology thus enriching its knowledge base and facilitating new discoveries. This framework supports global, local and personalised modelling. The latter is a process of model creation for a single person, based on their personal data and the information available in the ontology. Several methods for local and personalised modelling, both traditional and new, are described. A case study is presented on brain-gene-disease ontology, where a set of 12 genes related to central\u00a0\u2026", "num_citations": "12\n", "authors": ["177"]}
{"title": "Experience: quality benchmarking of datasets used in software effort estimation\n", "abstract": " Data is a cornerstone of empirical software engineering (ESE) research and practice. Data underpin numerous process and project management activities, including the estimation of development effort and the prediction of the likely location and severity of defects in code. Serious questions have been raised, however, over the quality of the data used in ESE. Data quality problems caused by noise, outliers, and incompleteness have been noted as being especially prevalent. Other quality issues, although also potentially important, have received less attention. In this study, we assess the quality of 13 datasets that have been used extensively in research on software effort estimation. The quality issues considered in this article draw on a taxonomy that we published previously based on a systematic mapping of data quality issues in ESE. Our contributions are as follows: (1) an evaluation of the \u201cfitness for purpose\u201d of\u00a0\u2026", "num_citations": "11\n", "authors": ["177"]}
{"title": "The promise of complementarity: using the methods of foresight for health workforce planning\n", "abstract": " Health workforce planning aims to meet a health system\u2019s needs with a sustainable and fit-for-purpose workforce, although its efficacy is reduced in conditions of uncertainty. This PhD breakthrough article offers foresight as a means of addressing this uncertainty and models its complementarity in the context of the health workforce planning problem. The article summarises the findings of a two-case multi-phase mixed method study that incorporates actor analysis, scenario development and policy Delphi. This reveals a few dominant actors of considerable influence who are in conflict over a few critical workforce issues. Using these to augment normative scenarios, developed from existing clinically developed model of care visions, a number of exploratory alternative descriptions of future workforce situations are produced for each case. Their analysis reveals that these scenarios are a reasonable facsimile of\u00a0\u2026", "num_citations": "11\n", "authors": ["177"]}
{"title": "Investigating the significance of bellwether effect to improve software effort estimation\n", "abstract": " Bellwether effect refers to the existence of exemplary projects (called the Bellwether) within a historical dataset to be used for improved prediction performance. Recent studies have shown an implicit assumption of using recently completed projects (referred to as moving window) for improved prediction accuracy. In this paper, we investigate the Bellwether effect on software effort estimation accuracy using moving windows. The existence of the Bellwether was empirically proven based on six postulations. We apply statistical stratification and Markov chain methodology to select the Bellwether moving window. The resulting Bellwether moving window is used to predict the software effort of a new project. Empirical results show that Bellwether effect exist in chronological datasets with a set of exemplary and recently completed projects representing the Bellwether moving window. Result from this study has shown that\u00a0\u2026", "num_citations": "11\n", "authors": ["177"]}
{"title": "Packaged software implementation requirements engineering by small software enterprises\n", "abstract": " Small to medium sized business enterprises (SMEs) generally thrive because they have successfully done something unique within a niche market. For this reason, SMEs may seek to protect their competitive advantage by avoiding any standardization encouraged by the use of packaged software (PS). Packaged software implementation at SMEs therefore presents challenges relating to how best to respond to mismatches between the functionality offered by the packaged software and each SME's business needs. An important question relates to which processes small software enterprises - or Small to Medium-Sized Software Development Companies (SMSSDCs) - apply in order to identify and then deal with these mismatches. To explore the processes of packaged software (PS) implementation, an ethnographic study was conducted to gain in-depth insights into the roles played by analysts in two SMSSDCs\u00a0\u2026", "num_citations": "11\n", "authors": ["177"]}
{"title": "Assessing the degree of spatial isomorphism for exploratory spatial analysis\n", "abstract": " This research continues with current innovative geocomputational research trends that aim to provide enhanced spatial analysis tools. The coupling of case-based reasoning (CBR) with GIS provides the focus of this paper. This coupling allows the retrieval, reuse, revision and retention of previous similar spatial cases. CBR is therefore used to develop more complex spatial data modelling methods (by using the CBR modules for improved spatial data manipulation) and provide enhanced exploratory geographical analysis tools (to find and assess certain patterns and relationships that may exist in spatial databases). This paper details the manner in which spatial similarity is assessed, for the purpose of re-using previous spatial cases. The authors consider similarity assessment a useful concept for retrieving and analysing spatial information as it may help researchers describe and explore a certain phenomena, its immediate environment and its relationships to other phenomena. This paper will address the following questions: What makes phenomena similar? What is the definition of similarity? What principles govern similarity? and How can similarity be measured?  Generally, phenomena are similar when they share common attributes and circumstances. The degree of similarity depends on the type and number of commonalties they share. Within this research, similarity is examined from a spatial perspective. Spatial similarity is broadly defined by the authors as the spatial matching and ranking according to a specific context and scale. More specifically, similarity is governed by context (function, use, reason, goal, users frame-of mind), scale\u00a0\u2026", "num_citations": "11\n", "authors": ["177"]}
{"title": "Rethinking health workforce planning: Capturing health system social and power interactions through actor analysis\n", "abstract": " Future health systems will be required to accommodate changing social and treatment environments along with new and not-before-contemplated health care roles. Thus, health workforce planning is likely to benefit from improved problem identification, response formulation and data and methods that provide deeper understandings of socially influenced systems. Actor analysis is able to facilitate this through its examination of actor goals, interactions, and influences. We explore the use of this infrequently reported method in the context of health workforce planning. Through an embedded mixed methods design, we draw on data from inductive document analysis, deductively coded semi-structured interview responses from two separate but interconnected health sub sectors and numerically transform these to comply with the selected actor analysis software\u2019s input requirements. Our findings underline the\u00a0\u2026", "num_citations": "10\n", "authors": ["177"]}
{"title": "Categorising software contexts\n", "abstract": " A growing number of researchers suggest that software process must be tailored to a project\u2019s context to achieve maximal performance. Researchers have studied \u2018context\u2019in an ad-hoc way, with focus on those contextual factors that appear to be of significance. The result is that we have no useful basis upon which to contrast and compare studies. We are currently researching a theoretical basis for software context for the purpose of tailoring and note that a deeper consideration of the meaning of the term \u2018context\u2019is required before we can proceed. In this paper, we examine the term and present a model based on insights gained from our initial categorisation of contextual factors from the literature. We test our understanding by analysing a further six documents. Our contribution thus far is a model that we believe will support a theoretical operationalisation of software context for the purpose of process tailoring.", "num_citations": "10\n", "authors": ["177"]}
{"title": "Mobile services and applications: towards a balanced adoption model\n", "abstract": " This paper synthesizes prior research to develop a novel model for the study of the adoption of mobile business services and applications incorporating a demand and supply perspective. The model complements and extends existing models while also leveraging data from industry reports; in particular, it focuses on the interrelationships between participants in the mobile services value chain and the impact of these interrelationships on the adoption of new services in a competitive and technology-saturated service market. There has been to date limited research reported that has considered the dynamics of the interrelationships between customers and (layers of) multiple service providers as a factor in the adoption and acceptance process; the proposed model addresses this gap and advocates the use of a combination of design science and service science methodologies. It is concluded that not mobility per se but the way mobility is used to create value plays a significant role as an adoption driver, and that the quality of the service and its relevance to personal or business lifestyle are the most important decision making factors. It is also asserted that while innovative mobile services (i.e., services that are not already offered using a different technology) may be compelling if they meet lifestyle needs, mobile services replacing or complementing existing ones will be favored by customers only if their quality is exceptional and motivates \u2018switching\u2019 to the mobile service.", "num_citations": "10\n", "authors": ["177"]}
{"title": "Improving web search using contextual retrieval\n", "abstract": " Contextual retrieval is a critical technique for todaypsilas search engines in terms of facilitating queries and returning relevant information. This paper reports on the development and evaluation of a system designed to tackle some of the challenges associated with contextual information retrieval from the World Wide Web (WWW). The developed system has been designed with a view to capturing both implicit and explicit user data which is used to develop a personal contextual profile. Such profiles can be shared across multiple users to create a shared contextual knowledge base. These are used to refine search queries and improve both the search results for a user as well as their search experience. An empirical study has been undertaken to evaluate the system against a number of hypotheses. In this paper, results related to one are presented that support the claim that users can find information more readily\u00a0\u2026", "num_citations": "10\n", "authors": ["177"]}
{"title": "Software Development, CASE Tools and 4GLs\u2014A Survey of New Zealand Usage (Part 1)\n", "abstract": " This paper reports the results of a recent national survey which considered the use of CASE tools and 4GLs in commercial software development. Responses from just over 750 organisations show a high degree of product penetration, along with extensive use of package solutions. Use of 3GLs in general, and of COBOL in particular, is still relatively widespread, however. In terms of systems analysis and design techniques under a CASE/4GL environment, screen and report definition is the most preferred technique, although both data flow analysis and data modelling also feature strongly.", "num_citations": "10\n", "authors": ["177"]}
{"title": "Investigating the significance of the bellwether effect to improve software effort prediction: Further empirical study\n", "abstract": " Context: In addressing how best to estimate how much effort is required to develop software, a recent study found that using exemplary and recently completed projects [forming Bellwether moving windows (BMW)] in software effort prediction (SEP) models leads to relatively improved accuracy. More studies need to be conducted to determine whether the BMW yields improved accuracy in general, since different sizing and aging parameters of the BMW are known to affect accuracy. Objective: To investigate the existence of exemplary projects (Bellwethers) with defined window size and age parameters, and whether their use in SEP improves prediction accuracy. Method: We empirically investigate the moving window assumption based on the theory that the prediction outcome of a future event depends on the outcomes of prior events. Sampling of Bellwethers was undertaken using three introduced Bellwether\u00a0\u2026", "num_citations": "9\n", "authors": ["177"]}
{"title": "Causal factors, benefits and challenges of test-driven development: Practitioner perceptions\n", "abstract": " This report describes the experiences of one organization's adoption of Test Driven Development (TDD) practices as part of a medium-term software project employing Extreme Programming as a methodology. Three years into this project the team's TDD experiences are compared with their non-TDD experiences on other ongoing projects. The perceptions of the benefits and challenges of using TDD in this context are gathered through five semi-structured interviews with key team members. Their experiences indicate that use of TDD has generally been positive and the reasons for this are explored to deepen the understanding of TDD practice and its effects on code quality, application quality and development productivity. Lessons learned are identified to aid others with the adoption and implementation of TDD practices, and some potential further research areas are suggested.", "num_citations": "9\n", "authors": ["177"]}
{"title": "Insights into domain knowledge sharing in software development practice in SMEs\n", "abstract": " The collaborative development of shared understanding is crucial to the success of software development projects. It is also a challenging and volatile process in practice. Small organizations may be especially vulnerable due to reliance on key individuals and insufficient resource to employ several domain specialists. There is, however, minimal empirical research on sharing domain understanding in the context of small software organizations. In this paper we present the results of a field study of commercial software development practice in which we conducted semi-structured interviews with practitioners from ten such organizations. The study provides insights into practices, perceptions, and challenges related to developing shared domain understanding. Our results show that smaller organizations place particular emphasis on the use of prototypes or existing products to refine and verify domain understanding\u00a0\u2026", "num_citations": "9\n", "authors": ["177"]}
{"title": "Quantitative functional complexity analysis of commercial software systems\n", "abstract": " British Library EThOS: Quantitative functional complexity analysis of commercial software systems New search | Advanced search | Search results Login / Register | About | Help | FAQ | Follow dividing line Use this URL to cite or link to this record in EThOS: https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.260551 Title: Quantitative functional complexity analysis of commercial software systems Author: MacDonell, Stephen Gerard ISNI: 0000 0001 3615 0789 Awarding Body: University of Cambridge Current Institution: University of Cambridge Date of Award: 1993 Availability of Full Text: Access from EThOS: Full text unavailable from EThOS. Please try the link below. Access from Institution: https://doi.org/10.17863/CAM.Abstract: No abstract available Supervisor: Not available Sponsor: Not available Qualification Name: Thesis (Ph.D.) Qualification Level: Doctoral EThOS ID: uk.bl.ethos.260551 DOI: 10.17863/CAM.: & : \u2026", "num_citations": "9\n", "authors": ["177"]}
{"title": "Synchronised visualisation of software process and product artefacts: Concept, design and prototype implementation\n", "abstract": " ContextMost prior software visualisation (SV) research has focused primarily on making aspects of intangible software product artefacts more evident. While undoubtedly useful, this focus has meant that software process visualisation has received far less attention.ObjectiveThis paper presents Conceptual Visualisation, a novel SV approach that builds on the well-known CodeCity metaphor by situating software code artefacts alongside their software development processes, in order to link and synchronise these typically separate components.MethodWhile the majority of prior SV research has focused on re-presenting what is already available in the code (i.e., the implementation) or information derived from it (i.e., various metrics), the presented approach instead makes the design concepts and original developers\u2019 intentions \u2013 both significant sources of information in terms of software development and\u00a0\u2026", "num_citations": "8\n", "authors": ["177"]}
{"title": "An empirical study into the relationship between class features and test smells\n", "abstract": " While a substantial body of prior research has investigated the form and nature of production code, comparatively little attention has examined characteristics of test code, and, in particular, test smells in that code. In this paper, we explore the relationship between production code properties (at the class level) and a set of test smells, in five open source systems. Specifically, we examine whether complexity properties of a production class can be used as predictors of the presence of test smells in the associated unit test. Our results, derived from the analysis of 975 production class-unit test pairs, show that the Cyclomatic Complexity (CC) and Weighted Methods per Class (WMC) of production classes are strong indicators of the presence of smells in their associated unit tests. The Lack of Cohesion of Methods in a production class (LCOM) also appears to be a good indicator of the presence of test smells. Perhaps\u00a0\u2026", "num_citations": "8\n", "authors": ["177"]}
{"title": "How Do Globally Distributed Agile Teams Self-organise?-Initial Insights from a Case Study\n", "abstract": " Agile software developers are required to self-organize, occupying various informal roles as needed in order to successfully deliver software features. However, previous research has reported conflicting evidence about the way teams actually undertake this activity. The ability to self-organize is particularly necessary for software development in globally distributed environments, where distance has been shown to exacerbate human-centric issues. Understanding the way successful teams self-organise should inform distributed team composition strategies and software project governance. We have used psycholinguistics to study the way IBM Rational Jazz practitioners enacted various roles, expressed attitudes and shared competencies to successfully self-organize in their global projects. Among our findings, we uncovered that practitioners enacted various roles depending on their teams' cohort of features; and that team leaders were most critical to IBM Jazz teams' self-organisation. We discuss these findings and highlight their implications for software project governance.", "num_citations": "8\n", "authors": ["177"]}
{"title": "IS development practice in New Zealand organisations\n", "abstract": " A survey of New Zealand organisations with 200 or more full-time employees was undertaken in order to obtain an updated assessment of IS development practice. Over the period surveyed (2001-2003), larger organisations (500 or more FTEs) or those with larger IS functions (10 or more IS FTEs) undertook significantly more IS projects, more expensive projects, more projects in which users participated and more projects in which a standard method was used, than their smaller counterparts. In the same period, there has been a trend towards increased use of packaged software solutions and outsourced development or customisation of packaged solutions. Factors perceived as most important to facilitating or inhibiting development in actual IS projects were related to availability of resources, definition of user requirements, communication between developers and users, project management, management of IS\u00a0\u2026", "num_citations": "8\n", "authors": ["177"]}
{"title": "Evolving connectionist systems for adaptive sport coaching\n", "abstract": " Contemporary computer assisted coaching software operates either on a particular sub-space of the wider problem or requires expert(s) to operate and provide explanations and recommendations. This paper introduces a novel motion data processing methodology oriented to the provision of future generation sports coaching software. The main focus of investigation is the development of techniques that facilitate processing automation, incremental learning from initially small data sets, and robustness of architecture with a degree of interpretation on individual sport performers\u2019 motion techniques. Findings from a case study using tennis motion data verify the prospect of building similar models and architectures for other sports or entertainment areas in which the aims are to improve human motion efficacy and to prevent injury. A central feature is the decoupling of the high-level analytical architecture from\u00a0\u2026", "num_citations": "8\n", "authors": ["177"]}
{"title": "Building evolving ontology maps for data mining and knowledge discovery in biomedical informatics\n", "abstract": " The explosion of biomedical data and the growing number of disparate data sources are exposing researchers to a new challenge-how to acquire, maintain and share knowledge from large and distributed databases in the context of rapidly evolving research.This paper describes research in progress on a new methodology for leveraging the semantic content of ontologies to improve knowledge discovery in complex and dynamic domains. It aims to build a multi-dimensional ontology able to share knowledge from different experiments undertaken across aligned research communities in order to connect areas of science seemingly unrelated to the area of immediate interest. We analyze how ontologies and data mining may facilitate biomedical data analysis and present our efforts to bridge the two fields, knowledge discovery in databases, and ontology learning for successful data mining", "num_citations": "8\n", "authors": ["177"]}
{"title": "Augmenting text mining approaches with social network analysis to understand the complex relationships among users' requests: A case study of the android operating system\n", "abstract": " Text mining approaches are being used increasingly for business analytics. In particular, such approaches are now central to understanding users' feedback regarding systems delivered via online application distribution platforms such as Google Play. In such settings, large volumes of reviews of potentially numerous apps and systems means that it is infeasible to use manual mechanisms to extract insights and knowledge that could inform product improvement. In this context of identifying software system improvement options, text mining techniques are used to reveal the features that are mentioned most often as being in need of correction (e.g., GPS), and topics that are associated with features perceived as being defective (e.g., inaccuracy of GPS). Other approaches may supplement such techniques to provide further insights for online communities and solution providers. In this work we augment text mining\u00a0\u2026", "num_citations": "7\n", "authors": ["177"]}
{"title": "Adopting softer approaches in the study of repository data: a comparative analysis\n", "abstract": " Context: Given the acknowledged need to understand the people processes enacted during software development, software repositories and mailing lists have become a focus for many studies. However, researchers have tended to use mostly mathematical and frequency-based techniques to examine the software artifacts contained within them. Objective: There is growing recognition that these approaches uncover only a partial picture of what happens during software projects, and deeper contextual approaches may provide further understanding of the intricate nature of software teams' dynamics. We demonstrate the relevance and utility of such approaches in this study. Method: We use psycholinguistics and directed content analysis (CA) to study the way project tasks drive teams' attitudes and knowledge sharing. We compare the outcomes of these two approaches and offer methodological advice for\u00a0\u2026", "num_citations": "7\n", "authors": ["177"]}
{"title": "Raising healthy software systems\n", "abstract": " We elaborate on the analogy between humans and bespoke software systems and we use this analogy to inform an alternative perspective on the development and management of such systems.", "num_citations": "7\n", "authors": ["177"]}
{"title": "The viability of fuzzy logic modeling in software development effort estimation: Opinions and expectations of project managers\n", "abstract": " There is a growing body of evidence to suggest that significant benefits may be gained from augmenting current approaches to software development effort estimation, and indeed other project management activities, with models developed using fuzzy logic and other soft computing methods. The tasks undertaken by project managers early in a development process would appear to be particularly amenable to such a strategy, particularly if fuzzy logic models are used in a complementary manner with other algorithmic approaches, thus providing a range of predictions as opposed to a single point value. As well as providing a more intuitively acceptable set of estimates, this would help to reduce or remove the unwarranted level of certainty associated with a point estimate. Furthermore, such an approach would enable organizations to \"store\" their project management knowledge, making them less susceptible to\u00a0\u2026", "num_citations": "7\n", "authors": ["177"]}
{"title": "An Ontological Analysis of a Proposed Theory for Software Development\n", "abstract": " There is growing acknowledgement within the software engineering community that a theory of software development is needed to integrate the myriad methodologies that are currently popular, some of which are based on opposing perspectives. We have been developing such a theory for a number of years. In this paper, we overview our theory and report on a recent ontological analysis of the theory constructs. We suggest that, once fully developed, this theory, or one similar to it, may be applied to support situated software development, by providing an overarching model within which software initiatives might be categorised and understood. Such understanding would inevitably lead to greater predictability with respect to outcomes.", "num_citations": "6\n", "authors": ["177"]}
{"title": "A systems approach to software process improvement in small organisations\n", "abstract": " There is, at the present time, no model to effectively support context-aware process change in small software organisations. The assessment reference models, for example, SPICE and CMMI, provide a tool for identifying gaps with best practice, but do not take into account group culture and environment, and do not help with prioritisation. These approaches thus do not support the many small software organisations that need to make effective changes that are linked to business objectives in short time periods. In this paper, we propose a model on an analogy of \u2018software system as human\u2019 and suggest that we can apply the idea of human health to help identify business objectives and improvement steps appropriate for these objectives. We describe a \u2018proof-of-concept\u2019 case study in which the model is retrospectively applied to a process improvement effort with a local software group.", "num_citations": "6\n", "authors": ["177"]}
{"title": "Enhancing data analysis with Ontologies and Olap\n", "abstract": " Knowledge Discovery in Databases (KDD) is an iterative process based on the analysis of current facts or data, pre-processing to clean and transform that data, application of mining algorithms, and deployment using the mining results on new data. Computer scientists have been working on feature selection in order to improve the quality of data process. Their objectives include: building simpler and more comprehensible models, improving data mining performance, and helping to prepare, to clean, and to understand data. In largely parallel research, ontologies have been widely used by the Artificial Intelligence community to represent domain knowledge and to integrate different database models. This work investigates the application of ontologies in the data analysis step of the KDD process. Our objective is to support the data preparation phase adding a semantic level to the data in order to select the most\u00a0\u2026", "num_citations": "6\n", "authors": ["177"]}
{"title": "Effective team onboarding in agile software development: techniques and goals\n", "abstract": " Context: It is not uncommon for a new team member to join an existing Agile software development team, even after development has started. This new team member faces a number of challenges before they are integrated into the team and can contribute productively to team progress. Ideally, each newcomer should be supported in this transition through an effective team onboarding program, although prior evidence suggests that this is challenging for many organisations. Objective: We seek to understand how Agile teams address the challenge of team onboarding in order to inform future onboarding design. Method: We conducted an interview survey of eleven participants from eight organisations to investigate what onboarding activities are common across Agile software development teams. We also identify common goals of onboarding from a synthesis of literature. A repertory grid instrument is used to map\u00a0\u2026", "num_citations": "5\n", "authors": ["177"]}
{"title": "Differences in Jazz Project Leaders\u2019 Competencies and Behaviors: A Preliminary Empirical Investigation\n", "abstract": " Studying the human factors that impact on software development, and assigning individuals with specific competencies and qualities to particular software roles, have been shown to aid software project performance. For instance, prior evidence suggests that extroverted software project leaders are most successful. Role assignment based on individuals' competencies and behaviors may be especially relevant in distributed software development contexts where teams are often affected by distance, cultural, and personality issues. Project leaders in these environments need to possess high levels of interpersonal, intra-personal and organizational competencies if they are to appropriately manage such issues and maintain positive project performance. With a view to understanding and explaining the specific competencies and behaviors that are required of project leaders in these settings, we used psycholinguistic\u00a0\u2026", "num_citations": "5\n", "authors": ["177"]}
{"title": "Stakeholder perceptions of software project outcomes: an industry case study\n", "abstract": " Background: In spite of their limited scope, measures reflecting adherence to schedule, budget and specification continue to dominate the assessment and reporting of project outcomes.Objective: We set out to explore how the parties involved in the acquisition and deployment of a self-contained software system viewed the project's outcomes, and the measures they considered.Method: Large volumes of empirical data were collected as part of a longitudinal case study conducted in a large multi-national company and were analyzed using qualitative methods.Results: While the conventional criteria remain of interest, the evidence reported here indicates that a richer set of contributors influence perceptions of project success and failure.Conclusions: The evaluation of project outcomes needs to become far more sophisticated and, at the very least, other measures should be considered alongside traditional measures.", "num_citations": "5\n", "authors": ["177"]}
{"title": "Standard method use in contemporary IS development: an empirical investigation\n", "abstract": " Purpose \u2013 The purpose of this research is to obtain an updated assessment of the use of standard methods in IS development practice in New Zealand, and to compare these practices to those reported elsewhere.Design/methodology/approach \u2013 A web\u2010based survey of IS development practices in New Zealand organisations with 200 or more full\u2010time employees was conducted. The results of the survey were compared to prior studies from other national contexts.Findings \u2013 The results suggest that levels of standard method use continue to be high in New Zealand organisations, although methods are often used in a pragmatic or ad hoc way. Further, the type of method used maps to a shift from bespoke development to system acquisition or outsourcing. Organisations that reported using standard methods perceived them to be beneficial to IS development in their recent IS projects, and generally disagreed with\u00a0\u2026", "num_citations": "5\n", "authors": ["177"]}
{"title": "Contextual and concept-based interactive query expansion\n", "abstract": " In this paper, we present a novel approach for contextual and concept based query formulation in web-based information retrieval, which is an on-going PhD project being undertaken at the Software Engineering Research Lab (SERL) at Auckland University of Technology (AUT). Various query formulation approaches have been studied for a long time with varying degree of success. To the best of our knowledge none of the existing approaches offer a similar service to the one discussed in this paper. Our novel approach centres on the formulation of a high quality search query using a user\u2019s contextual profile, a shared contextual knowledge based, lexical databases and domain-specific concepts. A user\u2019s contextual profile is constructed by monitoring and capturing user\u2019s implicit and explicit data. A shared contextual knowledge based is built by consolidating various users\u2019 contextual profiles. A machine learning technique is employed to learn user\u2019s specific information needs and support the iterative development of a search query by suggesting alternative terms/ concepts for query formulation. Early results indicate that the system has the potential to not only aid in the formulation of high quality search queries but also contribute towards the long term goal of intelligent contextual information retrieval from the WWW.", "num_citations": "5\n", "authors": ["177"]}
{"title": "Industry practices in project management for multimedia information systems\n", "abstract": " This paper describes ongoing research directed at formulating a set of appropriate measures for assessing and ultimately predicting effort requirements for multimedia systems development.  Whilst significant advances have been made in the determination of measures for both transaction-based and process-intensive systems, very little work has been undertaken in relation to measures for multimedia systems.  A small preliminary empirical study is reviewed as a precursor to a more exploratory investigation of the factors that are considered by industry to be influential in determining development effort.  This work incorporates the development and use of a goal-based framework to assist the measure selection process from a literature basis, followed by an industry questionnaire.  The results provide a number of preliminary but nevertheless useful insights into contemporary project management practices with\u00a0\u2026", "num_citations": "5\n", "authors": ["177"]}
{"title": "Deriving relevant functional measures for automated development projects\n", "abstract": " The increasing use of computer aided software engineering (CASE) tools, fourth-generation languages (4GLs) and other similar development automation techniques, has reduced the impact of implementation methods and individual ability on development task difficulty. It has therefore been suggested that measures derived from software specification representations may provide a consistent basis for relatively accurate estimation of subsequent development attributes. To this end, this paper describes the development of a functional complexity analysis scheme that is applicable to system specification products, rather than to the traditional products of the lower-level design and construction phases.", "num_citations": "5\n", "authors": ["177"]}
{"title": "Features that predict the acceptability of java and javascript answers on stack overflow\n", "abstract": " Context: Stack Overflow is a popular community question and answer portal used by practitioners to solve problems during software development. Developers can focus their attention on answers that have been accepted or where members have recorded high votes in judging good answers when searching for help. However, the latter mechanism (votes) can be unreliable, and there is currently no way to differentiate between an answer that is likely to be accepted and those that will not be accepted by looking at the answer's characteristics. Objective: In potentially providing a mechanism to identify acceptable answers, this study examines the features that distinguish an accepted answer from an unaccepted answer. Methods: We studied the Stack Overflow dataset by analyzing questions and answers for the two most popular tags (Java and JavaScript). Our dataset comprised 249,588 posts drawn from 2014-2016\u00a0\u2026", "num_citations": "4\n", "authors": ["177"]}
{"title": "Revisiting the size effect in software fault prediction models\n", "abstract": " BACKGROUND: In object oriented (OO) software systems, class size has been acknowledged as having an indirect effect on the relationship between certain artifact characteristics, captured via metrics, and fault-proneness, and therefore it is recommended to control for size when designing fault prediction models.AIM: To use robust statistical methods to assess whether there is evidence of any true effect of class size on fault prediction models.METHOD: We examine the potential mediation and moderation effects of class size on the relationships between OO metrics and number of faults. We employ regression analysis and bootstrapping-based methods to investigate the mediation and moderation effects in two widely-used datasets comprising seventeen systems.RESULTS: We find no strong evidence of a significant mediation or moderation effect of class size on the relationships between OO metrics and faults. In\u00a0\u2026", "num_citations": "4\n", "authors": ["177"]}
{"title": "Studying Expectation Violations in Socio-Technical Systems: a Case Study of the Mobile App Community.\n", "abstract": " With information technology mediating most aspects of contemporary societies, it is important to explore how human-oriented concepts may be leveraged to explore human actions in this new dispensation. One such concept is expectation violations. Expectations govern nearly all aspect of human interactions. However, while this phenomenon has been studied in human-human contexts where violations are expressed through verbal or non-verbal forms, little effort has been dedicated to the study of expectations in human-software contexts in socio-technical systems. We have thus studied expectation violations in one such instance, the mobile app community. Using Expectation Violation and Expectation Confirmation theories, we studied users\u2019 reviews of four apps in the health and fitness domain to understand how this app community responds to expectation violations, and if users in a similar domain will express dissatisfaction about similar expectation violations. Our outcomes confirm that the mobile app community responded to expectation violations, just as individuals do in human-human settings. In addition, we observed that users of different health and fitness apps reported similar expectation violations, as is the case for individuals and groups sharing culture-specific expectations. Beyond being of practical relevance for the app community, our outcomes also highlight opportunities for extending the abovementioned theories.", "num_citations": "4\n", "authors": ["177"]}
{"title": "Improving web information retrieval using shared contexts\n", "abstract": " The effective utilisation of a user\u2019s context in improving the performance of web search engines is a subject of intense research interest. In particular, much attention has been directed to the enhancement of queries and the provision of more relevant information by taking user context into account. Progress in this field has been limited to date, however, due to ongoing challenges in capturing and representing contextual information. We describe here the development and evaluation of a web-based contextual information retrieval that addresses some of these challenges and makes progress in defining the information required to create contextual profiles. Our system collects and leverages implicit and explicit user data to modify queries with the aim of more accurately reflecting the user\u2019s interests. This data is maintained dynamically in each user\u2019s contextual profile and utilised to improve the quality of information found during web searches. Where enabled, this data also contributes to the development of a shared contextual knowledge base that can also be used to augment queries. This shared contextual knowledge base is a key aspect of this research. The system has been tested in an observational study that has considered its ability to improve the user\u2019s web search experience. This paper presents experimental data to provide evidence of the system\u2019s performance, demonstrating that the shared contextual knowledge base extends the functionality associated with the individual contextual profile.", "num_citations": "4\n", "authors": ["177"]}
{"title": "Visualization and analysis of software engineering data using self-organizing maps\n", "abstract": " There is no question that accuracy is an important requirement of classification and prediction models used in software engineering management. It is, however, just one of a number of attributes that contribute to a model being 'useful'. Understandably much research has been undertaken with the objective of maximizing model accuracy, but this has often occurred with little regard for these other model attributes, which might include cost-effectiveness, credibility and, for want of a better term, meaningfulness. The research described in this paper addresses both model accuracy and meaningfulness as conveyed by self-organizing maps (SOMs). SOMs are neural-network based representations of data distributions that provide two-dimensional depictions of multi-dimensional relationships. As such they can enable developers and project managers (and researchers) to visualize often complex interactions among and\u00a0\u2026", "num_citations": "4\n", "authors": ["177"]}
{"title": "Determining delivered functional error content based on the complexity of CASE specifications\n", "abstract": " Despite problems of definition, software complexity has long been recognised as a significant determinant of software product error content. Consequently a large number of empirical studies have attempted to quantify this relationship, using code and design structure measures as complexity indicators. With the increasing use of automated development tools in business software development, however, assessment of the complexity of functional specifications is likely to be more appropriate. This paper therefore describes relevant assessment methods for both complexity and functional errors in this environment. Statistical analysis of significant relationships produced encouraging results based on the development of an error prediction equation.", "num_citations": "4\n", "authors": ["177"]}
{"title": "Reliance on correlation data for complexity metric use and validation\n", "abstract": " This paper reports the results of an experiment to illustrate the hazards of using correlation data as the sole determinant for software metric use and validation. Three widely cited complexity metrics have been examined in relation to the frequency of software development errors.", "num_citations": "4\n", "authors": ["177"]}
{"title": "New Zealand's health workforce planning should embrace complexity and uncertainty\n", "abstract": " Recently, concerns have been raised over the sufficiency, distribution and sustainability of New Zealand\u2019s medical workforce, with competing views being offered on the responses that should be taken. 1, 2 The arguments presented for and against proposed solutions tend to be medically orientated and supported by evidence and the analysis of trends that are framed by the present health system\u2019s organisation. This has the outcome of further embedding the present\u2019s infrastructures and service delivery methods, which limit alternative consideration of how future services could be provided and peopled. 3 In the main, suggested solutions seek to address New Zealand\u2019s medical workforce imbalances by reducing reliance on international medical graduates (IMGs), to increase the numbers and placements of locally trained doctors through medical school roll increases, and by continuing or improving a range of incentives and informational programmes as a means to promote and attract trainees for hard-to-staff vocational specialities. 4\u20136Yet despite numerous similar policy interventions over past decades, the issues of persistent shortages and misdistributions continue. 2 This is in part due to a traditional medically focused and a by-profession approach to health workforce policy and planning, as well as a reliance on quantitative forecasts made under conditions of uncertainty. 7 In New Zealand, these forecasts, made available by academics and professional organisations, 8, 9 with regularly collected workforce survey data are also used as an aid to project the future numbers of particular professions. 10, 11 However, this approach tends to neglect\u00a0\u2026", "num_citations": "3\n", "authors": ["177"]}
{"title": "Evolving a Model for Software Process Context: An Exploratory Study.\n", "abstract": " In the domain of software engineering, our efforts as researchers to advise industry on which software practices might be applied most effectively are limited by our lack of evidence based information about the relationships between context and practice efficacy. In order to accumulate such evidence, a model for context is required. We are in the exploratory stage of evolving a model for context for situated software practices. In this paper, we overview the evolution of our proposed model. Our analysis has exposed a lack of clarity in the meanings of terms reported in the literature. Our base model dimensions are People, Place, Product and Process. Our contributions are a deepening of our understanding of how to scope contextual factors when considering software initiatives and the proposal of an initial theoretical construct for context. Study limitations relate to a possible subjectivity in the analysis and a restricted evaluation base. In the next stage in the research, we will collaborate with academics and practitioners to formally refine the model.", "num_citations": "3\n", "authors": ["177"]}
{"title": "On Design-time Security in IEC 61499 Systems: Conceptualisation, Implementation, and Feasibility\n", "abstract": " Cyber-attacks on Industrial Automation and Control Systems (IACS) are rising in numbers and sophistication. Embedded controller devices such as Programmable Logic Controllers (PLCs), which are central to controlling physical processes, must be secured against attacks on confidentiality, integrity and availability. The focus of this paper is to add design-level support for security in IACS applications, especially around inter-PLC communications. We propose an end-to-end solution to develop IACS applications with inherent, and parametric support for security. Built using the IEC 61499 Function Blocks standard, this solution allows us to annotate certain communications as `secure' during design time. When the application is compiled, these annotations are transformed into a security layer that implements encrypted communication between PLCs. In this paper, we implement a part of this security layer focussed\u00a0\u2026", "num_citations": "3\n", "authors": ["177"]}
{"title": "2nd workshop on hybrid development approaches in software systems development\n", "abstract": " Software and system development is complex and diverse, and a multitude of development approaches is used and combined with each other to address the manifold challenges companies face today. To study the current state of the practice and to build a sound understanding about the utility of different development approaches and their application to modern software system development, in 2016, we launched the HELENA initiative. This paper introduces the 2nd HELENA workshop and provides an overview of the current project state. In the workshop, six teams present initial findings from their regions, impulse talk are given, and further steps of the HELENA roadmap are discussed.", "num_citations": "3\n", "authors": ["177"]}
{"title": "Combining Dynamic Analysis and Visualization to Explore the Distribution of Unit Test Suites\n", "abstract": " As software systems have grown in scale and complexity the test suites built alongside those systems have also become increasingly complex. Understanding key aspects of test suites, such as their coverage of production code, is important when maintaining or reengineering systems. This work investigates the distribution of unit tests in Open Source Software (OSS) systems through the visualization of data obtained from both dynamic and static analysis. Our long-term aim is to support developers in their understanding of test distribution and the relationship of tests to production code. We first obtain dynamic coupling information from five selected OSS systems and we then map the test and production code results. The mapping is shown in graphs that depict both the dependencies between classes and static test information. We analyze these graphs using Centrality metrics derived from graph theory and SNA\u00a0\u2026", "num_citations": "3\n", "authors": ["177"]}
{"title": "Combining text mining and visualization techniques to study teams' behavioral processes\n", "abstract": " There is growing interest in mining software repository data to understand, and predict, various aspects of team processes. In particular, text mining and natural-language processing (NLP) techniques have supported such efforts. Visualization may also supplement text mining to reveal unique multi-dimensional insights into software teams' behavioral processes. We demonstrate the utility of combining these approaches in this study. Future application of these methods to the study of teams' behavioral processes offers promise for both research and practice.", "num_citations": "3\n", "authors": ["177"]}
{"title": "Understanding Feasibility Study Approach for Packaged Software Implementation by SMEs\n", "abstract": " Software engineering often no longer involves building systems from scratch, but rather integrating functionality from existing software and components or implementing packaged software. Conventional software engineering comprises a set of influential approaches that are often considered good practice, including structured programming, and collecting a complete set of test cases. However, these approaches do not apply well for packaged software (PS) implementation; hence this phenomenon requires independent consideration. To explore PS implementation, we conducted ethnographic studies in packaged software development companies, in particular, to understand aspects of the feasibility study approach for PS implementation. From an analysis of these cases, we conclude that firstly; the analyst has more of a hybrid analyst-sales-marketing role than the analyst in traditional RE feasibility study\u00a0\u2026", "num_citations": "3\n", "authors": ["177"]}
{"title": "Stochastic cost estimation and risk analysis in managing software projects\n", "abstract": " This paper presents an overview of the use of stochastic modelling as an approach to assessing the impact of uncertainty in effort and cost estimations in software projects. Uncertainty in input values is modelled using probability distributions and this uncertainty is propagated through the model to provide risk information using Monte Carlo simulation. Statistical analysis of the outputs of the simulation provides a means for identifying where the highest risk in the estimates lies. Understanding this risk, in terms of both its impact and its likelihood, allows activities to be undertaken to mitigate the risk prior to submitting a tender, therefore increasing the confidence with which the bid/no-bid decision is made.", "num_citations": "3\n", "authors": ["177"]}
{"title": "Applying Fuzzy Logic Modeling to Software Project Management\n", "abstract": " In this paper we provide evidence to support the use of fuzzy sets, fuzzy rules and fuzzy inference in modeling predictive relationships of relevance to software project management. In order to make such an approach accessible to managers we have constructed a software toolset that enables data, classes and rules to be defined for any such relationship (e.g. determination of project risk, or estimation of product size, based on a variety of input parameters). We demonstrate the effectiveness of the approach by applying our fuzzy logic modeling toolset to two previously published data sets. It is shown that the toolset does indeed facilitate the creation and refinement of classes of data and rules mapping input values or classes to outputs. This in itself represents a positive outcome, in that the approach is shown to be capable of incorporating data and knowledge in a single model. The predictive results\u00a0\u2026", "num_citations": "3\n", "authors": ["177"]}
{"title": "Diagnosable-by-Design Model-Driven Development for IEC 61499 Industrial Cyber-Physical Systems\n", "abstract": " Integrating the design and creation of fault identification and diagnostic capabilities into Model-Driven Development methodologies is one approach to enhancing the resilience of Industrial Cyber-Physical SystemsWe present a Fault Diagnostic Engine designed to recognise and diagnose faults in IEC 61499 Function Block Applications. Using diagnostic agents that interact directly with the target application, we demonstrate fault monitoring and analysis tech-niques and as well as failure scenario interventionBy designing and building fault diagnostic resources during early phases of Model-Driven Development, both iterative testing and long-term fault management capabilities can be created. While applying and refining appropriate model artifacts, we demonstrate that the concurrent development of function blocks alongside fault management capabilities is both feasible and worthwhile.", "num_citations": "2\n", "authors": ["177"]}
{"title": "Multi-objective reconstruction of software architecture\n", "abstract": " Design erosion is a persistent problem within the software engineering discipline. Software designs tend to deteriorate over time and there is a need for tools and techniques that support software architects when dealing with legacy systems. This paper presents an evaluation of a search-based software engineering (SBSE) approach intended to recover high-level architecture designs of software systems by structuring low-level artifacts into high-level architecture artifact configurations. In particular, this paper describes the performance evaluation of a number of metaheuristic search algorithms applied to architecture reconstruction problems with high dimensionality in terms of objectives. These problems have been selected as representative of the typical challenges faced by software architects dealing with legacy systems and the results inform the ongoing development of a software tool that supports the analysis of\u00a0\u2026", "num_citations": "2\n", "authors": ["177"]}
{"title": "Progress report on a proposed theory for software development\n", "abstract": " There is growing acknowledgement within the software engineering community that a theory of software development is needed to integrate the myriad methodologies that are currently popular, some of which are based on opposing perspectives. We have been developing such a theory for a number of years. In this position paper, we overview our theory along with progress made thus far. We suggest that, once fully developed, this theory, or one similar to it, may be applied to support situated software development, by providing an overarching model within which software initiatives might be categorised and understood. Such understanding would inevitably lead to greater predictability with respect to outcomes.", "num_citations": "2\n", "authors": ["177"]}
{"title": "A Study of the Relationship Between Class Testability and Runtime Properties\n", "abstract": " Software testing is known to be expensive, time consuming and challenging. Although previous research has investigated relationships between several software properties and software testability the focus has been on static software properties. In this work we present the results of an empirical investigation into the possible relationship between runtime properties (dynamic coupling and key classes) and class testability. We measure both properties using dynamic metrics and argue that data gathered using dynamic metrics are both broader and more precise than data gathered using static metrics. Based on statistical analysis, we find that dynamic coupling and key classes are significantly correlated with class testability. We therefore suggest that these properties could be used as useful indicators of class testability.", "num_citations": "2\n", "authors": ["177"]}
{"title": "A prototype tool to support extended team collaboration in agile project feature management\n", "abstract": " In light of unacceptable rates of software project failure agile development methodologies have achieved widespread industry prominence, aimed at reducing software project risks and improving the likelihood of project success. However, the highly collaborative processes embedded in agile methodologies may themselves introduce other risks. In particular, the fluid and diverse nature of agile team structures may mean that collaboration regarding what is to be delivered becomes more challenging. We have therefore developed a prototype tool intended to enable all stakeholders to have greater access to the features of the emerging system irrespective of their location, via remote feature management functionality. Software engineering experts have evaluated the initial prototype, verifying that it would enhance collaboration and is likely to assist teams in their handling of feature management.", "num_citations": "2\n", "authors": ["177"]}
{"title": "Using historical data in stochastic estimation of software project duration\n", "abstract": " This paper presents a framework for the representation of uncertainty in the estimates used to predict the duration of software design projects. The modelling framework utilises Monte Carlo simulation to compute the propagation of uncertainty in estimates towards the total project uncertainty and therefore gives a project manager the means to make informed decisions throughout the project life. The framework also provides a mechanism for accumulating project knowledge through the use of a historical database, allowing effort estimates to be informed by, or indeed upon, the outcome of previous projects.", "num_citations": "2\n", "authors": ["177"]}
{"title": "An empirical investigation into IS development practice in New Zealand\n", "abstract": " A Web-based survey of 106 large New Zealand organisations was undertaken to gain an understanding of their IS development practices. The survey focussed on the contribution of standard methods and user participation to IS development. Among the findings were that 91% of the respondents used a standard method in the development process in at least some of projects undertaken in the last three years. All organisations reported using some level of user participation. The majority of organisations agreed that organisational issues had been more important than technical issues in determining the outcome of the IS development in these projects.", "num_citations": "2\n", "authors": ["177"]}
{"title": "Software Engineering Management\n", "abstract": " This is the current draft (version 0.9) of the knowledge area description for Software Engineering Management. The primary goals of this document are to:", "num_citations": "2\n", "authors": ["177"]}
{"title": "Fuzzy Logic Techniques for Software Metric Models of Development Effort\n", "abstract": " Software metrics are measurements that are made of software development processes and the resulting products. Once recorded they can be used as variables (both dependent and independent) in models for project management purposes, including prediction, monitoring, and assessing the development process. The most common types of these software metric models are those used for predicting the development effort required for a software system, usually in terms of developer hours/days, based on size, complexity, developer characteristics, and other relevant metrics. Despite the well-known, and considerable, financial and strategic benefits that can be acquired from developing accurate and usable models of effort estimation, there are a number of problems that have not yet been overcome by those using the traditional techniques of formal and linear regression models. These include taking full consideration of the non-linearities and interactions inherent in complex real-world processes such as software development and the lack of stationarity in such processes (as people, tools, and methodologies change). Other problems include over-commitment to precisely specified values, the small quantities of data often available, and the inability to use whatever knowledge is available where numerical values are unknown or are only known in some approximate manner. The usefulness of alternative techniques, especially that of fuzzy logic modeling, are investigated, some comparisons between the techniques are made, and some recommendations are suggested regarding the use of fuzzy logic for project management.", "num_citations": "2\n", "authors": ["177"]}
{"title": "Assessing the graphical and algorithmic structure of hierarchical coloured Petri net models\n", "abstract": " Petri nets, as a modelling formalism, are utilised for the analysis of processes, whether for explicit understanding, database design or business process re-engineering. The formalism, however, can be represented on a virtual continuum from highly graphical to largely algorithmic. The use and understanding of the formalism will, in part, therefore depend on the resultant complexity and power of the representation and, on the graphical or algorithmic preference of the user. This paper develops a metric which will indicate the graphical or algorithmic tendency of hierarchical coloured Pctri nets.", "num_citations": "2\n", "authors": ["177"]}
{"title": "Statistical analysis procedures for software complexity assessment data\n", "abstract": " This paper describes certain statistical procedures that can be used with confidence in the analysis of software product/process assessment data. Problems have occurred with this activity in the past, mainly because the nature of underlying data distributions has not been given adequate consideration. Recently, however, software engineering researchers have started to realise that more appropriate procedures, including non-parametric and robust techniques, should be used in order to obtain valid results and conclusions. Although this paper deals mainly with the relationship between software complexity and development effort, many of the comments and recommendations made are also relevant to the analysis of other software assessment data sets.", "num_citations": "2\n", "authors": ["177"]}
{"title": "Does class size matter? An in-depth assessment of the effect of class size in software defect prediction\n", "abstract": " In the past 20 years, defect prediction studies have generally acknowledged the effect of class size on software prediction performance. To quantify the relationship between object-oriented (OO) metrics and defects, modelling has to take into account the direct, and potentially indirect, effects of class size on defects. However, some studies have shown that size cannot be simply controlled or ignored, when building prediction models. As such, there remains a question whether, and when, to control for class size. This study provides a new in-depth examination of the impact of class size on the relationship between OO metrics and software defects or defect-proneness. We assess the impact of class size on the number of defects and defect-proneness in software systems by employing a regression-based mediation (with bootstrapping) and moderation analysis to investigate the direct and indirect effect of class size in count and binary defect prediction. Our results show that the size effect is not always significant for all metrics. Of the seven OO metrics we investigated, size consistently has significant mediation impact only on the relationship between Coupling Between Objects (CBO) and defects/defect-proneness, and a potential moderation impact on the relationship between Fan-out and defects/defect-proneness. Based on our results we make three recommendations. One, we encourage researchers and practitioners to examine the impact of class size for the specific data they have in hand and through the use of the proposed statistical mediation/moderation procedures. Two, we encourage empirical studies to investigate the indirect effect of\u00a0\u2026", "num_citations": "1\n", "authors": ["177"]}
{"title": "Rethinking workforce planning for integrated care: using scenario analysis to facilitate policy development\n", "abstract": " Background                 A goal of health workforce planning is to have the most appropriate workforce available to meet prevailing needs. However, this is a difficult task when considering integrated care, as future workforces may require different numbers, roles and skill mixes than those at present. With this uncertainty and large variations in what constitutes integrated care, current health workforce policy and planning processes are poorly placed to respond. In order to address this issue, we present a scenario-based workforce planning approach.                                               Methods                 We propose a novel mixed methods design, incorporating content analysis, scenario methods and scenario analysis through the use of a policy Delphi. The design prescribes that data be gathered from workforce documents and studies that are used to develop scenarios, which are then assessed by a panel of\u00a0\u2026", "num_citations": "1\n", "authors": ["177"]}
{"title": "Examining convolutional feature extraction using Maximum Entropy (ME) and Signal-to-Noise Ratio (SNR) for image classification\n", "abstract": " Convolutional Neural Networks (CNNs) specialize in feature extraction rather than function mapping. In doing so they form complex internal hierarchical feature representations, the complexity of which gradually increases with a corresponding increment in neural network depth. In this paper, we examine the feature extraction capabilities of CNNs using Maximum Entropy (ME) and Signal-to-Noise Ratio (SNR) to validate the idea that, CNN models should be tailored for a given task and complexity of the input data. SNR and ME measures are used as they can accurately determine in the input dataset, the relative amount of signal information to the random noise and the maximum amount of information respectively.We use two well known benchmarking datasets, MNIST and CIFAR-10 to examine the information extraction and abstraction capabilities of CNNs. Through our experiments, we examine convolutional\u00a0\u2026", "num_citations": "1\n", "authors": ["177"]}
{"title": "Designing Actively Secure, Highly Available Industrial Automation Applications\n", "abstract": " Programmable Logic Controllers (PLCs) execute critical control software that drives Industrial Automation and Control Systems (IACS). PLCs can become easy targets for cyber-adversaries as they are resource-constrained and are usually built using legacy, less-capable security measures. Security attacks can significantly affect system availability, which is an essential requirement for IACS. We propose a method to make PLC applications more security-aware. Based on the well-known IEC 61499 function blocks standard for developing IACS software, our method allows designers to annotate critical parts of an application during design time. On deployment, these parts of the application are automatically secured using appropriate security mechanisms to detect and prevent attacks. We present a summary of availability attacks on distributed IACS applications that can be mitigated by our proposed method. Security\u00a0\u2026", "num_citations": "1\n", "authors": ["177"]}
{"title": "3rd Workshop on Hybrid Development Approaches in Software System Development\n", "abstract": " Evidence shows that software development methods, frameworks, and even practices are seldom applied in companies by following the book. Combinations of different methodologies into home-grown processes are being constantly uncovered. Nonetheless, an academic understanding and investigation of this phenomenon is very limited. In 2016, the HELENA initiative was launched to research hybrid development approaches in software system development. This paper introduces the 3rd HELENA workshop and provides a detailed description of the instrument used and the available data sets.", "num_citations": "1\n", "authors": ["177"]}
{"title": "Valuing evaluation: methodologies to bridge research and practice\n", "abstract": " The potential disconnect between research and practice in software engineering (SE) means that the uptake of research outcomes has at times been limited. In this paper we seek to identify research approaches that are rigorous in terms of method but that are also relevant to software engineering practitioners. After considering the correspondence of several approaches to software systems research and practice we recommend a framework for applying grounded theory in SE research, as a means of delivering both robust and useful outcomes.", "num_citations": "1\n", "authors": ["177"]}
{"title": "Heuristic and rule-based knowledge acquisition: classification of numeral strings in text\n", "abstract": " This paper describes the rule-based classification of numerals and strings that include numerals, composed of a number and semantic unit(s) that indicate a SPEED, NUMBER, or other measure, at three levels: morphological, syntactic, and semantic. The approach employs three interpretation processes: word trigram construction with tokeniser, rule-based processing of number strings, and n-gram based classification. We extracted numeral strings from 378 online newspaper articles, finding that, on average, they comprised about 2.2% of the words in the articles. To manually extract n-gram rules to disambiguate the number strings\u2019 meanings, our approach was trained on 886 numeral strings and tested on the remaining 3251 strings. We implemented two heuristic disambiguation methods based on each category\u2019s frequency statistics collected from the sample data, and precision ratios of both methods\u00a0\u2026", "num_citations": "1\n", "authors": ["177"]}
{"title": "Towards the development of a documentation structure for modelling spatial processes\n", "abstract": " This paper describes issues relating to the formation of a documentation structure for modelling spatial processes. The limitations and desirable attributes of such a structure are discussed with particular reference to the requirements of model designers and implementers. The definition of their structure is regarded as pivotal to the continued research and implementation of technology for modelling spatial processes previously conducted and published, in which extensive benefits were identified. In this paper, particular reference is made to highlight existing research in the area of model documentation that although requiring adaptation, may satisfy the specified criteria for the development of a generic system for modelling spatial processes.", "num_citations": "1\n", "authors": ["177"]}
{"title": "Software process engineering for measurement-driven software quality programs\u2014realism and idealism\n", "abstract": " This paper brings together a set of commonsense recommendations relating to the delivery of software quality, with some emphasis on the adoption of realistic perspectives for software process/product stakeholders in the area of process improvement. The use of software measurement is regarded as an essential component for a quality development program, in terms of prediction, control, and adaptation as well as the communication necessary for stakeholders\u2019 realistic perspectives. Some recipes for failure are briefly considered so as to enable some degree of contrast between what is currently perceived to be good and bad practices. This is followed by an evaluation of the quality-at-all-costs model, including a brief pragmatic investigation of quality in other, more mature, disciplines. Several programs that claim to assist in the pursuit of quality are examined, with some suggestions made as to how they may best be used in practice.", "num_citations": "1\n", "authors": ["177"]}
{"title": "CASE and 4GL Product Users\u2019 Participation in Software Engineering Research\n", "abstract": " A recent graduate student project required access to a number of real-world systems developed using computer aided software engineering (CASE) tools and/or fourth generation languages (4GLs) at business sites in the United Kingdom and New Zealand. Obtaining cooperation for this type of participation, however, was unexpectedly time-consuming and difficult. This short paper reports the results of the mail/telephone campaign that was undertaken at the outset of the project when expectations were high. It is hoped that the experiences reported here may make other researchers more aware of the problems that can occur in attempting to obtain software engineering data from this environment. Recommendations concerning the most useful avenues are also provided.", "num_citations": "1\n", "authors": ["177"]}