{"title": "Time-aware test suite prioritization\n", "abstract": " Regression test prioritization is often performed in a time constrained execution environment in which testing only occurs for a fixed time period. For example, many organizations rely upon nightly building and regression testing of their applications every time source code changes are committed to a version control repository. This paper presents a regression test prioritization technique that uses a genetic algorithm to reorder test suites in light of testing time constraints. Experiment results indicate that our prioritization approach frequently yields higher average percentage of faults detected (APFD) values, for two case study applications, when basic block level coverage is used instead of method level coverage. The experiments also reveal fundamental trade offs in the performance of time-aware prioritization. This paper shows that our prioritization technique is appropriate for many regression testing environments\u00a0\u2026", "num_citations": "462\n", "authors": ["564"]}
{"title": "A family of test adequacy criteria for database-driven applications\n", "abstract": " Although a software application always executes within a particular environment, current testing methods have largely ignored these environmental factors. Many applications execute in an environment that contains a database. In this paper, we propose a family of test adequacy criteria that can be used to assess the quality of test suites for database-driven applications. Our test adequacy criteria use dataflow information that is associated with the entities in a relational database. Furthermore, we develop a unique representation of a database-driven application that facilitates the enumeration of database interaction associations. These associations can reflect an application's definition and use of database entities at multiple levels of granularity. The usage of a tool to calculate intraprocedural database interaction associations for two case study applications indicates that our adequacy criteria can be computed with\u00a0\u2026", "num_citations": "177\n", "authors": ["564"]}
{"title": "Software testing\n", "abstract": " I shall not deny that the construction of these testing programs has been a major intellectual effort: to convince oneself that one has not overlooked \u201ca relevant state\u201d and to convince oneself that the testing programs generate them all is no simple matter. The encouraging thing is that (as far as we know!) it could be done. Edsger W. Dijkstra [Dijkstra, 1968] 1", "num_citations": "92\n", "authors": ["564"]}
{"title": "An empirical study of incorporating cost into test suite reduction and prioritization\n", "abstract": " Software developers use testing to gain and maintain confidence in the correctness of a software system. Automated reduction and prioritization techniques attempt to decrease the time required to detect faults during test suite execution. This paper uses the Harrold Gupta Soffa, delayed greedy, traditional greedy, and 2-optimal greedy algorithms for both test suite reduction and prioritization. Even though reducing and reordering a test suite is primarily done to ensure that testing is cost-effective, these algorithms are normally configured to make greedy choices with coverage information alone. This paper extends these algorithms to greedily reduce and prioritize the tests by using both test cost (eg, execution time) and the ratio of code coverage to test cost. An empirical study with eight real world case study applications shows that the ratio greedy choice metric aids a test suite reduction method in identifying a smaller\u00a0\u2026", "num_citations": "90\n", "authors": ["564"]}
{"title": "Test suite reduction and prioritization with call trees\n", "abstract": " This paper presents a tool that (i) constructs tree-based models of a program's behavior during testing and (ii) employs these trees while reordering and reducing a test suite. Using either a dynamic call tree or a calling context tree, the test reduction component identifies a subset of the original tests that covers the same call tree paths. The prioritization technique reorders a test suite so that it covers the call tree paths more rapidly than the initial test ordering. In support of program and test suite understanding, the tool also visualizes the call trees and the coverage relationships. For a chosen case study application, the experimental results show that call tree construction only increases testing time by 13%. In comparison to the original test suite, the experiments show that (i) a prioritized suite achieves coverage much faster and (ii) a reduced test suite contains 45% fewer tests and consumes 82% less time", "num_citations": "56\n", "authors": ["564"]}
{"title": "Efficient time-aware prioritization with knapsack solvers\n", "abstract": " Regression testing is frequently performed in a time constrained environment. This paper explains how 0/1 knapsack solvers (eg, greedy, dynamic programming, and the core algorithm) can identify a test suite reordering that rapidly covers the test requirements and always terminates within a specified testing time limit. We conducted experiments that reveal fundamental trade-offs in the (i) time and space costs that are associated with creating a reordered test suite and (ii) quality of the resulting prioritization. We find knapsack-based prioritizers that ignore the overlap in test case coverage incur a low time overhead and a moderate to high space overhead while creating prioritizations exhibiting a minor to modest decrease in effectiveness. We also find that the most sophisticated 0/1 knapsack solvers do not always identify the most effective prioritization, suggesting that overlap-aware prioritizers with a higher time\u00a0\u2026", "num_citations": "50\n", "authors": ["564"]}
{"title": "An approach for understanding and testing third party software components\n", "abstract": " In this paper we present an approach to mitigating software risk by understanding and testing third party, or commercial-off-the-shelf (COTS), software components. Our approach, based on the notion of software wrapping, gives system integrators an improved understanding of how a COTS component behaves within a particular system. Our approach to wrapping allows the data flowing into and out of the component at the public interface level to be intercepted. Using our wrapping approach, developers can apply testing techniques such as fault injection, data collection and, assertion checking to components whose source code is unavailable. We have created a methodology for using software wrapping in conjunction with data collection, fault injection, and assertion checking to test the interaction between a component and the rest of the application. The methodology seeks to identify locations in the program\u00a0\u2026", "num_citations": "44\n", "authors": ["564"]}
{"title": "Database-aware test coverage monitoring\n", "abstract": " Unlike traditional programs, a database-centric application interacts with a database that has a complex state and structure. Even though the database is an important component of modern software, there are few tools to support the testing of database-centric applications. This paper presents a test coverage monitoring technique that tracks a program's definition and use of database entities during test suite execution. The paper also describes instrumentation probes that construct a coverage tree that records how the program and the tests cover the database. We conducted experiments to measure the costs that are associated with (i) instrumenting the program and the tests and (ii) monitoring coverage. For all of the applications, the experiments demonstrate that the instrumentation mechanism incurs an acceptable time overhead. While the use of statically inserted probes may increase the size of an application\u00a0\u2026", "num_citations": "43\n", "authors": ["564"]}
{"title": "History-based test case prioritization with software version awareness\n", "abstract": " Test case prioritization techniques schedule the test cases in an order based on some specific criteria so that the tests with better fault detection capability are executed at an early position in the regression test suite. Many existing test case prioritization approaches are code-based, in which the testing of each software version is considered as an independent process. Actually, the test results of the preceding software versions may be useful for scheduling the test cases of the later software versions. Some researchers have proposed history-based approaches to address this issue, but they assumed that the immediately preceding test result provides the same reference value for prioritizing the test cases of the successive software version across the entire lifetime of the software development process. Thus, this paper describes ongoing research that studies whether the reference value of the immediately preceding\u00a0\u2026", "num_citations": "42\n", "authors": ["564"]}
{"title": "Test suite reduction methods that decrease regression testing costs by identifying irreplaceable tests\n", "abstract": " ContextIn software development and maintenance, a software system may frequently be updated to meet rapidly changing user requirements. New test cases will be designed to ensure the correctness of new or modified functions, thus gradually increasing the test suite\u2019s size. Test suite reduction techniques aim to decrease the cost of regression testing by removing the redundant test cases from the test suite and then obtaining a representative set of test cases that still yield a high level of code coverage.ObjectiveMost of the existing reduction algorithms focus on decreasing the test suite\u2019s size. Yet, the differences in execution costs among test cases are usually significant and it may take a lot of execution time to run a test suite consisting of a few long-running test cases. This paper presents and empirically evaluates cost-aware algorithms that can produce the representative sets with lower execution costs\u00a0\u2026", "num_citations": "41\n", "authors": ["564"]}
{"title": "Towards the prioritization of regression test suites with data flow information\n", "abstract": " Regression test prioritization techniques re-order the execution of a test suite in an attempt to ensure that defects are revealed earlier in the test execution phase. In prior work, test suites were prioritized with respect to their ability to satisfy control flow-based and mutation-based test adequacy criteria. In this paper, we propose an approach to regression test prioritization that leverages the all-DUs test adequacy criterion that focuses on the definition and use of variables within the program under test. Our prioritization scheme is motivated by empirical studies that have shown that (i) tests fulfilling the all-DUs test adequacy criteria are more likely to reveal defects than those that meet the control flow-based criteria,(ii) there is an unclear relationship between all-DUs and mutation-based criteria, and (iii) mutation-based testing is significantly more expensive than testing that relies upon all-DUs. In support of our\u00a0\u2026", "num_citations": "40\n", "authors": ["564"]}
{"title": "Empirically studying the role of selection operators during search-based test suite prioritization\n", "abstract": " Regression test suite prioritization techniques reorder test cases so that, on average, more faults will be revealed earlier in the test suite's execution than would otherwise be possible. This paper presents a genetic algorithm-based test prioritization method that employs a wide variety of mutation, crossover, selection, and transformation operators to reorder a test suite. Leveraging statistical analysis techniques, such as tree model construction through binary recursive partitioning and kernel density estimation, the paper's empirical results highlight the unique role that the selection operators play in identifying an effective ordering of a test suite. The study also reveals that, while truncation selection consistently outperformed the tournament and roulette operators in terms of test suite effectiveness, increasing selection pressure consistently produces the best results within each class of operator. After further explicating\u00a0\u2026", "num_citations": "36\n", "authors": ["564"]}
{"title": "Reducing the cost of regression testing by identifying irreplaceable test cases\n", "abstract": " Test suite reduction techniques decrease the cost of software testing by removing the redundant test cases from the test suite while still producing a reduced set of tests that yields the same level of code coverage as the original suite. Most of the existing approaches to reduction aim to decrease the size of the test suite. Yet, the difference in the execution cost of the tests is often significant and it may be costly to use a test suite consisting of a few long-running test cases. Thus, this paper proposes an algorithm, based on the concept of test irreplaceability, which creates a reduced test suite with a decreased execution cost. Leveraging widely used benchmark programs, the empirical study shows that, in comparison to existing techniques, the presented algorithm is the most effective at reducing the cost of running a test suite.", "num_citations": "27\n", "authors": ["564"]}
{"title": "Using coverage effectiveness to evaluate test suite prioritizations\n", "abstract": " Regression test suite prioritization techniques reorder a test suite with the goal of ensuring that the reorganized test suite finds faults faster than the initial ordering. It is challenging to empirically evaluate the effectiveness of a new test case arrangement because existing metrics (i) require fault seeding or (ii) ignore test case costs. This paper presents a coverage effectiveness (CE) metric that (i) obviates the need to seed faults into the program under test and (ii) incorporates available data about test case execution times. A test suite is awarded a high CE value when it quickly covers the test requirements. It is possible to calculate coverage effectiveness regardless of the coverage criterion that is chosen to evaluate test case quality. The availability of an open source CE calculator enables future case studies and controlled experiments to use coverage effectiveness when evaluating different approaches to test suite\u00a0\u2026", "num_citations": "27\n", "authors": ["564"]}
{"title": "Towards the measurement of tuple space performance\n", "abstract": " Many applications rely upon a tuple space within distributed system middleware to provide loosely coupled communication and service coordination. This paper describes an approach for measuring the throughput and response time of a tuple space when it handles concurrent local space interactions. Furthermore, it discusses a technique that populates a tuple space with tuples before the execution of a benchmark in order to age the tuple space and provide a worst-case measurement of space performance. We apply the tuple space benchmarking and aging methods to the measurement of the performance of a JavaSpace, a current example of a tuple space that integrates with the Jini network technology. The experiment results indicate that: (i) the JavaSpace exhibits limited scalability as the number of concurrent interactions from local space clients increases, (ii) the aging technique can operate with acceptable\u00a0\u2026", "num_citations": "24\n", "authors": ["564"]}
{"title": "A comprehensive framework for testing database-centric applications\n", "abstract": " CiteSeerX \u2014 A Comprehensive Framework for Testing Database-Centric Applications Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA A Comprehensive Framework for Testing Database-Centric Applications (2007) Cached Download as a PDF Download Links [www.cs.virginia.edu] [etd.library.pitt.edu] [www.cs.allegheny.edu] Save to List Add to Collection Correct Errors Monitor Changes by Gregory M. Kapfhammer Citations: 15 - 7 self Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract Keyphrases database-centric application comprehensive framework Powered by: Apache Solr About CiteSeerX Submit and Index Documents Privacy Policy Help Data Source Contact Us \u2026", "num_citations": "21\n", "authors": ["564"]}
{"title": "Creation and analysis of a Javaspace-based distributed genetic algorithm\n", "abstract": " The island model for distributed genetic algorithms (GAs) is a natural match for the master-worker paradigm in distributed computation. We explore the benefits and drawbacks of several distributed system architectures in developing an implementation of a distributed GA that exploits the Jini and JavaSpace technologies. Our results, using the knapsack problem as an illustration, show that there is an unavoidable price to pay in terms of decreasing computation-to-communication ratios as a function of instance size. However, we can diminish these effects by expanding the number of JavaSpaces beyond those required for the obvious implementation. Our results also indicate that as the number of remote machines increases the potential for a better solution also rises. Even though our distributed GAs did not always exploit this potential for a higher quality solution, we believe that the combination of Java, Jini, and JavaSpaces presents avenues for easily distributing the computation of genetic algorithms.", "num_citations": "21\n", "authors": ["564"]}
{"title": "Regression testing\n", "abstract": " reduction and prioritization, efficiency and effectiveness trade-offs Abstract. Regression testing techniques execute a test suite whenever the addition of defect fixes or new functionality changes the program under test. The repeated exe-cution of a test suite aims to establish a confidence in the correctness of the software application and identify defects that were introduced by the program modifications. Industry experiences suggest that regression testing often improves the quality of the application under test. However, testing teams may not always perform regression testing because the frequent execution of the tests often incurs high time and space overheads. Test suite selection techniques try to reduce the cost of testing by running a subset of the tests, such as those that execute the modified source code, in order to ensure that the updated program still operates correctly. Alternatively, reduction meth-ods decrease testing time overheads by discarding the tests that redundantly cover the test requirements. Approaches to test suite prioritization reorder the test cases in an attempt to maximize the rate at which the tests achieve a testing goal such as code coverage. After describing a wide variety of metrics for empirically evaluating differ-ent regression testing methods, this chapter considers the efficiency and effectiveness trade-offs associated with these techniques. The conclusion of this article summarizes the state-of-the-art in the field of regression testing and then offers suggestions for future work and resources for further study. 1", "num_citations": "20\n", "authors": ["564"]}
{"title": "Parameter tuning for search-based test-data generation revisited: Support for previous results\n", "abstract": " Although search-based test-data generators, like EvoSuite, efficiently and automatically create effective JUnit test suites for Java classes, these tools are often difficult to configure. Prior work by Arcuri and Fraser revealed that the tuning of EvoSuite with response surface methodology (RSM) yielded a configuration of the test data generator that did not outperform the default configuration. Following the experimental design and protocol described by Arcuri and Fraser, this paper presents the results of a study that lends further support to prior results: like RSM, the EvoSuite configuration identified by the well-known Sequential Parameter Optimization Toolbox (SPOT) failed to significantly outperform the default settings. Although this result is negative, it furnishes further empirical evidence of the challenge associated with tuning a complex search-based test data generator. Moreover, the outcomes of the presented\u00a0\u2026", "num_citations": "19\n", "authors": ["564"]}
{"title": "Testing in resource constrained execution environments\n", "abstract": " Software for resource constrained embedded devices is often implemented in the Java programming language because the Java compiler and virtual machine provide enhanced safety, portability, and the potential for run-time optimization. It is important to verify that a software application executes correctly in the environment in which it will normally execute, even if this environment is an embedded one that severely constrains memory resources. Testing can be used to isolate defects within and establish a confidence in the correctness of a Java application that executes in a resource constrained environment. However, executing test suites with a Java virtual machine (JVM) that uses dynamic compilation to create native code bodies can introduce significant testing time overheads if memory resources are highly constrained. This paper describes an approach that uses adaptive code unloading to ensure that it is\u00a0\u2026", "num_citations": "19\n", "authors": ["564"]}
{"title": "Empirically evaluating regression testing techniques: Challenges, solutions, and a potential way forward\n", "abstract": " The published studies of regression testing methods often contain many of the hallmarks of high quality empirical research. Beyond features like clear descriptions of the methodology and the visualization and statistical analysis of the data sets, certain papers in this field also provide some of the artifacts used in and/or produced by the experiments. Yet, the limited industrial adoption of regression testing techniques is due in part to a lack of comprehensive empirical evaluations. Moreover, the regression testing community has not achieved a level of experimental reproducibility that would fully establish it as a science. After identifying the challenges associated with evaluating regression testing methods, this paper advocates a way forward involving a mutually beneficial increased sharing of the inputs, outputs, and procedures used in experiments.", "num_citations": "18\n", "authors": ["564"]}
{"title": "Empirically evaluating Greedy-based test suite reduction methods at different levels of test suite complexity\n", "abstract": " Test suite reduction is an important approach that decreases the cost of regression testing. A test suite reduction technique operates based on the relationship between the test cases in the regression test suite and the test requirements in the program under test. Thus, its effectiveness should be closely related to the complexity of a regression test suite \u2013 the product of the number of test cases and the number of test requirements. Our previous work has shown that cost-aware techniques (i.e., the test suite reduction techniques that aim to decrease the regression test suite's execution cost) generally outperform the others in terms of decreasing the cost of running the regression test suite. However, the previous empirical studies that evaluated cost-aware techniques did not take into account test suite complexity. That is, prior experiments do not reveal if the cost-aware techniques scale and work effectively on test suites\u00a0\u2026", "num_citations": "13\n", "authors": ["564"]}
{"title": "A genetic algorithm to improve Linux kernel performance on resource-constrained devices\n", "abstract": " As computers become increasingly mobile, users demand more functionality, longer battery-life, and better performance from mobile devices. In response, chipset fabricators are focusing on elegant architectures to provide solutions that are both low-power and high-performance. Since these architectures rely on unique x86 extensions rather than fast clock speeds and large caches, careful thought must be placed into effective optimization strategies for not only user applications, but also the kernel itself, as the typical default optimizations used by modern compilers do not often take advantage of these specialized features. Focusing on the Intel Diamondville platform, this paper presents a genetic algorithm that evolves the compiler flags needed to build a Linux kernel that exhibits reduced response times.", "num_citations": "12\n", "authors": ["564"]}
{"title": "Towards a method for reducing the test suites of database applications\n", "abstract": " Database applications are commonly implemented and used in both industry and academia. These complex and rapidly evolving applications often have frequent changes in the source code of the program and the state and structure of the database. This paper describes and empirically evaluates a test suite reduction technique that improves the efficiency of regression testing for database applications by removing redundant tests. The experimental results show that the reduced test suites are between 30% and 80% smaller than the original test suite, thus enabling a decrease in testing time ranging from 7% to 78%. These empirical outcomes suggest that test suite reduction is a viable method for controlling the costs associated with testing rapidly-evolving database applications.", "num_citations": "11\n", "authors": ["564"]}
{"title": "A comprehensive framework for testing database-centric software applications\n", "abstract": " The database is a critical component of many modern software applications. Recent reports indicate that the vast majority of database use occurs from within an application program. Indeed, database-centric applications have been implemented to create digital libraries, scientific data repositories, and electronic commerce applications. However, a database-centric application is very different from a traditional software system because it interacts with a database that has a complex state and structure. This dissertation formulates a comprehensive framework to address the challenges that are associated with the efficient and effective testing of database-centric applications. The database-aware approach to testing includes:(i) a fault model,(ii) several unified representations of a program's database interactions,(iii) a family of test adequacy criteria,(iv) a test coverage monitoring component, and (v) tools for reducing\u00a0\u2026", "num_citations": "10\n", "authors": ["564"]}
{"title": "An examination of the run-time performance of GUI creation frameworks\n", "abstract": " The graphical user interface (GUI) is an important component of many software systems. Past surveys indicate that the development of a GUI is a significant undertaking and that the GUI\u2019s source code often comprises a substantial portion of the program\u2019s overall source base. Graphical user interface creation frameworks for popular objectoriented programming languages enable the rapid construction of simple and complex GUIs. In this paper, we examine the run-time performance of two GUI creation frameworks, Swing and Thinlet, that are tailored for the Java programming language. Using a simple model of a Java GUI, we formally define the difficulty of a GUI manipulation event. After implementing a case study application, we conducted experiments to measure the event handling latency for GUI manipulation events of varying difficulties. During our investigation of the run-time performance of the Swing and Thinlet GUI creation frameworks, we also measured the CPU and memory consumption of our candidate application during the selected GUI manipulation events. Our experimental results indicate that Thinlet often outperformed Swing in terms of both event handling latency and memory consumption. However, Swing appears to be better suited, in terms of event handling latency and CPU consumption, for the construction of GUIs that require manipulations of high difficulty levels.", "num_citations": "10\n", "authors": ["564"]}
{"title": "Empirically identifying the best genetic algorithm for covering array generation\n", "abstract": " With their many interacting parameters, modern software systems are highly configurable. Combinatorial testing is a widely used and practical technique that can detect the failures triggered by the parameters and their interactions. One of the key challenges in combinatorial testing is covering array generation, an often expensive process that is not always amenable to automation with greedy methods. The 2-way covering array is the most common and can be defined as follows [1]:", "num_citations": "8\n", "authors": ["564"]}
{"title": "Testing commercial-off-the-shelf software components\n", "abstract": " Today, developers do not rely solely on custom built software systems to construct large scale information systems. The systems being built and maintained are made up of a combination of commercial-off-the-shelf (COTS) components, legacy software, and custom built components [Ghosh99]. Corporate down-sizing and decreased government budgets as well as the spiraling costs of building and maintaining large software systems have made the development of a system completely from original software an impossibility [VoasPayne95]. A result of this transition has been that developers are relying on the use of commercial-off-the-shelf (COTS) components with increased frequency.COTS components are pieces of software that ideally can be integrated into a system with far less effort and cost than would be necessary to build a custom component and integrate it into the system in question. Unfortunately, the very\u00a0\u2026", "num_citations": "8\n", "authors": ["564"]}
{"title": "An approach to identifying and understanding problematic COTS components\n", "abstract": " The usage of Commercial off the Shelf (COTS) components in software systems presents the possibility of temporal savings and efficiency increases. However, this temporal savings might come at the expense of system quality. When a system integrator relies upon COTS software, trust is placed in unknown, black-box components. We present a methodology that identifies problematic COTS components and then attempts to augment a system integrator\u2019s understanding of these components. Our technique uses software fault injection to expose COTS components to new failure scenarios. When these unique failure scenarios cause a COTS component to act in an unpredictable manner, our approach records the injected fault and the anomalous behavior. Next, we employ different machine learning techniques to build a representation of the anomalous behavior of the COTS component. These machine learning algorithms analyze the collected data, which describes the diverse conditions that cause a COTS component to behave unpredictably, and produce a comprehensive model of the combinations of input and component state that normally result in deviant behavior. A system integrator can inspect a graphical representation of this model in order to gain a better understanding of the anomalous COTS components. We believe our approach to isolating and understanding problematic COTS components will allow a system integrator to realize the temporal savings of reusable COTS software while also mitigating the associated risks.", "num_citations": "7\n", "authors": ["564"]}
{"title": "Implementation and analysis of a JavaSpace supported by a relational database\n", "abstract": " JavaSpaces object repository provides an exceptional environment for the design and implementation of loosely coupled distributed systems. We explore the strengths and weaknesses of the most popular implementation of a persistent JavaSpace. We report on the design, implementation, and analysis of RDBSpace, a JavaSpace that is supported by a relational database. Our experimental results indicate that it is possible to build an efficient and scalable JavaSpace that relies on a relational database back-end.", "num_citations": "6\n", "authors": ["564"]}
{"title": "A framework to support research in and encourage industrial adoption of regression testing techniques\n", "abstract": " When software developers make changes to a program, it is possible that they will introduce faults into previously working parts of the code. As software grows, a regression test suite is run to ensure that the old functionality still works as expected. Yet, as the number of test cases increases, it becomes more expensive to execute the test suite. Reduction and prioritization techniques enable developers to manage large and unwieldy test suites. However, practitioners and researchers do not always use and study these methods due, in part, to a lack of availability. In response to this issue, this paper describes an already released open-source framework that supports both research and practice in regression testing. The sharing of this framework will enable the replication of empirical studies in regression testing and encourage faster industrial adoption of these useful, yet rarely used, techniques.", "num_citations": "5\n", "authors": ["564"]}
{"title": "Using synthetic test suites to empirically compare search-based and greedy prioritizers\n", "abstract": " The increase in the complexity of modern software has led to the commensurate growth in the size and execution time of the test suites for these programs. In order to address this alarming trend, developers use test suite prioritization to reorder the test cases so that faults can be detected at an early stage of testing. Yet, the implementation and evaluation of greedy and search-based test prioritizers requires access to case study applications and their associated test suites, which are often difficult to find, configure, and use in an empirical study. This paper presents two types of synthetically generated test suites that support this process of experimentally evaluating prioritization methods. Using synthetic test suites affords greater control over test case characteristics and supports the identification of empirical trends that contradict the established wisdom about search-based and greedy prioritization. For instance, we\u00a0\u2026", "num_citations": "4\n", "authors": ["564"]}
{"title": "Interactive coverage effectiveness multiplots for evaluating prioritized regression test suites\n", "abstract": " \u25b6 Enable software testers to quickly find the best test suite order for their own applications\u25b6 Interactively pick prioritizers, comparing CE values and the actual ordering of the tests\u25b6 Utilize prioritization techniques such as greedy (GRD), 2-optimal greedy (2OPT), delayed greedy (DGR), and Harrold Gupta Soffa (HGS) which use greedy choice metrics (GCMs) to efficiently construct new test orders [7]", "num_citations": "3\n", "authors": ["564"]}
{"title": "Prioritizing test suites by finding Hamiltonian paths: Preliminary studies and initial results\n", "abstract": " Introduction. This paper describes a technique for prioritizing a test suite by finding the least weight Hamiltonian path in a complete graph that represents relative testing costs. Our technique is especially useful when testing confronts constraints such as quotas in a Web service [3], memory overhead [4], or test execution time [5]. During the testing of modern mobile computing devices (eg, handsets running Google Android), it is often challenging to properly handle memory constraints. Thus, even though we anticipate that our approach is valuable in a wide variety of limited resource environments, this paper focuses on prioritizing test suites for memory constrained execution. In summary, the important contributions of this paper include:", "num_citations": "3\n", "authors": ["564"]}
{"title": "An experimental study of methods for executing test suites in memory constrained environments\n", "abstract": " Software for memory constrained mobile devices is often implemented in the Java programming language because the Java compiler and virtual machine (JVM) provide enhanced safety, portability, and the potential for run-time optimization. However, testing time may increase substantially when memory is limited and the JVM employs a compiler to create native code bodies. This paper furnishes an empirical study that identifies the fundamental trade-offs associated with a method that uses adaptive native code unloading to perform memory constrained testing. The experimental results demonstrate that code unloading can reduce testing time by 17% and the code size of the test suite and application under test by 68% while maintaining the overall size of the JVM. We also find that the goal of reducing the space overhead of an automated testing technique is often at odds with the objective of decreasing the time\u00a0\u2026", "num_citations": "2\n", "authors": ["564"]}
{"title": "Building a Distributed Genetic Algorithm with the Jini Network Technology\n", "abstract": " \u2022 Quality of Solution: The QoS for the CDSM is always higher than the SGA. Generally, the QoS is higher in the CDSM as we add machines.\u2022 Generations\u2013per\u2013Second: The CDSM can compute more Gen/Sec than the SDSM. Generally, adding more machines to the CDSM increases the Gen/Sec.", "num_citations": "2\n", "authors": ["564"]}
{"title": "Creating a free, dependable software engineering environment for building Java applications\n", "abstract": " As open source software engineering becomes more prevalent, employing sound software engineering practices and the tools used to implement these practices becomes more important. This paper examines the current status of free software engineering tools. For each set of tools, we determined the important attributes that would best assist a developer in each stage of the waterfall model. We rated each tool based on predetermined attributes. We used the creation of a graphical user interface based email client in Java to assist in evaluating each tool. Our findings show that there is still a need for free tools to extract UML diagrams, test graphical user interfaces, make configuring Emacs easier, and profile Java applications. In other areas there are free tools that provide satisfactory functionality such as Concurrent Versions System (CVS), GVim, JUnit, JRefactory, GNU Make, Jakarta Ant, Javadoc, and Doc++.", "num_citations": "1\n", "authors": ["564"]}