{"title": "Instruction set compiled simulation: A technique for fast and flexible instruction set simulation\n", "abstract": " Instruction set simulators are critical tools for the exploration and validation of new programmable architectures. Due to increasing complexity of the architectures and time-to-market pressure, performance is the most important feature of an instruction-set simulator. Interpretive simulators are flexible but slow, whereas compiled simulators deliver speed at the cost of flexibility. This paper presents a novel technique for generation of fast instruction-set simulators that combines the benefit of both compiled and interpretive simulation. We achieve fast instruction accurate simulation through two mechanisms. First, we move the time-consuming decoding process from run-time to compile time while maintaining the flexibility of the interpretive simulation. Second, we use a novel instruction abstraction technique to generate aggressively optimized decoded instructions that further improves simulation performance. Our\u00a0\u2026", "num_citations": "188\n", "authors": ["1805"]}
{"title": "LOSSLESS DATA COMPRESSION AND REAL-TIME DECOMPRESSION\n", "abstract": " A method, information processing system, and computer program storage product store data in an information processing system. Uncompressed data is received and the uncompressed data is divided into a series of vectors. A sequence of profitable bitmask patterns is identified for the vectors that maximizes compression efficiency while minimizes decompression penalty. Matching patterns are created using multiple bit masks based on a set of maximum values of the frequency distribution of the vectors. A dictionary is built based upon the set of maximum values in the frequency distribution and a bit mask savings which is a number of bits reduced using each of the multiple bit masks. Each of the vectors is compressed using the dictionary and the matching patterns with having high bit mask savings. The compressed vectors are stored into memory. Also, an efficient placement is developed to enable parallel\u00a0\u2026", "num_citations": "154\n", "authors": ["1805"]}
{"title": "Functional coverage driven test generation for validation of pipelined processors\n", "abstract": " Functional verification of microprocessors is one of the most complex and expensive tasks in the current system-on-chip design process. A significant bottleneck in the validation of such systems is the lack of a suitable functional coverage metric. The paper presents a functional coverage based test generation technique for pipelined architectures. The proposed methodology makes three important contributions. First, a general graph-theoretic model is developed that can capture the structure and behavior (instruction-set) of a wide variety of pipelined processors. Second, we propose a functional fault model that is used to define the functional coverage for pipelined architectures. Finally, test generation procedures are presented that accept the graph model of the architecture as input and generate test programs to detect all the faults in the functional fault model. Our experimental results on two pipelined processor\u00a0\u2026", "num_citations": "147\n", "authors": ["1805"]}
{"title": "Dynamic cache reconfiguration and partitioning for energy optimization in real-time multi-core systems\n", "abstract": " Multicore architectures, especially chip multi-processors, have been widely acknowledged as a successful design paradigm. Existing approaches primarily target application-driven partitioning of the shared cache to alleviate inter-core cache interference so that both performance and energy efficiency are improved. Dynamic cache reconfiguration is a promising technique in reducing energy consumption of the cache subsystem for uniprocessor systems. In this paper, we present a novel energy optimization technique which employs both dynamic reconfiguration of private caches and partitioning of the shared cache for multicore systems with real-time tasks. Our static profiling based algorithm is designed to judiciously find beneficial cache configurations (of private caches) for each task as well as partition factors (of the shared cache) for each core so that the energy consumption is minimized while task deadline is\u00a0\u2026", "num_citations": "105\n", "authors": ["1805"]}
{"title": "Architecture description languages for programmable embedded systems\n", "abstract": " Embedded systems present a tremendous opportunity to customise designs by exploiting the application behaviour. Shrinking time-to-market, coupled with short product lifetimes, create a critical need for rapid exploration and evaluation of candidate architectures. Architecture description languages (ADL) enable exploration of programmable architectures for a given set of application programs under various design constraints such as area, power and performance. The ADL is used to specify programmable embedded systems, including processor, coprocessor and memory architectures. The ADL specification is used to generate a variety of software tools and models facilitating exploration and validation of candidate architectures. The paper surveys the existing ADLs in terms of (a) the inherent features of the languages and (b) the methodologies they support to enable simulation, compilation, synthesis, test\u00a0\u2026", "num_citations": "103\n", "authors": ["1805"]}
{"title": "Bitmask-based code compression for embedded systems\n", "abstract": " Embedded systems are constrained by the available memory. Code-compression techniques address this issue by reducing the code size of application programs. It is a major challenge to develop an efficient code-compression technique that can generate substantial reduction in code size without affecting the overall system performance. We present a novel code-compression technique using bitmasks, which significantly improves the compression efficiency without introducing any decompression penalty. This paper makes three important contributions. 1) It develops an efficient bitmask-selection technique that can create a large set of matching patterns. 2) It develops an efficient dictionary-selection technique based on bitmasks. 3) It proposes a dictionary-based code-compression algorithm using the bitmask- and dictionary-selection techniques that can significantly reduce the memory requirement. To\u00a0\u2026", "num_citations": "91\n", "authors": ["1805"]}
{"title": "MERS: statistical test generation for side-channel analysis based Trojan detection\n", "abstract": " Hardware Trojan detection has emerged as a critical challenge to ensure security and trustworthiness of integrated circuits. A vast majority of research efforts in this area has utilized side-channel analysis for Trojan detection. Functional test generation for logic testing is a promising alternative but it may not be helpful if a Trojan cannot be fully activated or the Trojan effect cannot be propagated to the observable outputs. Side-channel analysis, on the other hand, can achieve significantly higher detection coverage for Trojans of all types/sizes, since it does not require activation/propagation of an unknown Trojan. However, they have often limited effectiveness due to poor detection sensitivity under large process variations and small Trojan footprint in side-channel signature. In this paper, we address this critical problem through a novel side-channel-aware test generation approach, based on a concept of Multiple\u00a0\u2026", "num_citations": "90\n", "authors": ["1805"]}
{"title": "RATS: Restoration-aware trace signal selection for post-silicon validation\n", "abstract": " Post-silicon validation is one of the most important and expensive tasks in modern integrated circuit design methodology. The primary problem governing post-silicon validation is the limited observability due to storage of a small number of signals in a trace buffer. The signals to be traced should be carefully selected in order to maximize restoration of the remaining signals. Existing approaches have two major drawbacks. They depend on partial restorability computations that are not effective in restoring maximum signal states. They also require long signal selection time due to inefficient computation as well as operating on gate-level netlist. We have proposed a signal selection approach based on total restorability at gate-level, which is computationally more efficient (10 times faster) and can restore up to three times more signals compared to existing methods. We have also developed a register transfer level\u00a0\u2026", "num_citations": "87\n", "authors": ["1805"]}
{"title": "Handbook of Energy-Aware and Green Computing, Volume 1\n", "abstract": " Implementing energy-efficient CPUs and peripherals as well as reducing resource consumption have become emerging trends in computing. As computers increase in speed and power, their energy issues become more and more prevalent. The need to develop and promote environmentally friendly computer technologies and systems has also come to the forefront", "num_citations": "87\n", "authors": ["1805"]}
{"title": "Graph-based functional test program generation for pipelined processors\n", "abstract": " Functional verification is widely acknowledged as a major bottleneck in microprocessor design. While early work on specification driven functional test program generation has proposed several promising ideas, many challenges remain in applying them to realistic embedded processors. We present a graph coverage based functional test program generation approach for pipelined processors. The proposed methodology makes three important contributions. First, it automatically generates the graph model of the pipelined processor from the specification using functional abstraction. Second, it generates functional test programs based on the coverage of the pipeline behaviour. Finally, the test generation time is drastically reduced due to the use of module level property checking. We applied this methodology on the DLX processor to demonstrate the usefulness of our approach.", "num_citations": "85\n", "authors": ["1805"]}
{"title": "A survey of side-channel attacks on caches and countermeasures\n", "abstract": " With the increasing proliferation of Internet-of-Things (IoT) in our daily lives, security and trustworthiness are key considerations in designing computing devices. A vast majority of IoT devices use shared caches for improved performance. Unfortunately, the data sharing introduces the vulnerability in these systems. Side-channel attacks in shared caches have been explored for over a decade. Existing approaches utilize side-channel (non-functional) behaviors such as time, power, and electromagnetic radiation to attack encryption schemes. In this paper, we survey the widely used target encryption algorithms, the common attack techniques, and recent attacks that exploit the features of cache. In particular, we focus on the cache timing attacks against the cloud computing and embedded systems. We also survey existing countermeasures at different abstraction levels.", "num_citations": "81\n", "authors": ["1805"]}
{"title": "Test data compression using efficient bitmask and dictionary selection methods\n", "abstract": " Higher circuit densities in system-on-chip (SOC) designs have led to drastic increase in test data volume. Larger test data size demands not only higher memory requirements, but also an increase in testing time. Test data compression addresses this problem by reducing the test data volume without affecting the overall system performance. This paper proposes a novel test data compression technique using bitmasks which provides a substantial improvement in the compression efficiency without introducing any additional decompression penalty. The major contributions of this paper are as follows: 1) it develops an efficient bitmask selection technique for test data in order to create maximum matching patterns; 2) it develops an efficient dictionary selection method which takes into account the bitmask based compression; and 3) it proposes a test compression technique using efficient dictionary and bitmask selection\u00a0\u2026", "num_citations": "78\n", "authors": ["1805"]}
{"title": "Functional abstraction driven design space exploration of heterogeneous programmable architectures\n", "abstract": " Rapid Design Space Exploration (DSE) of a programmable architecture is feasible using an automatic toolkit (compiler, simulator, assembler) generation methodology driven by an Architecture Description Language (ADL). While many contemporary ADLs can effectively capture one class of architecture, they are typically unable to capture a wide spectrum of processor and memory features present in DSP, VLIW, EPIC and Superscalar processors. The main bottleneck has been the lack of an abstraction underlying the ADL (covering a diverse set of architectural features) that permits reuse of the abstraction primitives to compose the heterogeneous architectures. We present in this paper the functional abstraction needed to capture such wide variety of programmable architectures. We illustrate the usefulness of this approach by specifying two very different architectures using functional abstraction. Our DSE results\u00a0\u2026", "num_citations": "76\n", "authors": ["1805"]}
{"title": "Scalable test generation for Trojan detection using side channel analysis\n", "abstract": " Hardware Trojan detection has emerged as a critical challenge to ensure security and trustworthiness of integrated circuits. A vast majority of research efforts in this area has utilized side-channel analysis for Trojan detection. Functional test generation for logic testing is a promising alternative but it may not be helpful if a Trojan cannot be fully activated or the Trojan effect cannot be propagated to the observable outputs. Side-channel analysis, on the other hand, can achieve significantly higher detection coverage for Trojans of all types/sizes, since it does not require activation/propagation of an unknown Trojan. However, they have often limited effectiveness due to poor detection sensitivity under large process variations and small Trojan footprint in side-channel signature. In this paper, we address this critical problem through a novel side-channel-aware test generation approach, based on a concept of multiple\u00a0\u2026", "num_citations": "73\n", "authors": ["1805"]}
{"title": "Hardware IP security and trust\n", "abstract": " \u00a9 Springer International Publishing AG 2017", "num_citations": "73\n", "authors": ["1805"]}
{"title": "Efficient Trace Signal Selection for Post Silicon Validation and Debug\n", "abstract": " Post-silicon validation is an essential part of modern integrated circuit design to capture bugs and design errors that escape pre-silicon validation phase. A major problem governing post-silicon debug is the observability of internal signals since the chip has already been manufactured. Storage requirements limit the number of signals that can be traced, therefore, a major challenge is how to reconstruct the majority of the remaining signals based on traced values. Existing approaches focus on selecting signals with an emphasis on partial restorability, which does not guarantee a good signal restoration. We propose an approach that efficiently selects a set of signals based on total restorability criteria. Our experimental results demonstrate that our signal selection algorithm is both computationally more efficient and can restore up to three times more signals compared to existing methods.", "num_citations": "73\n", "authors": ["1805"]}
{"title": "An efficient retargetable framework for instruction-set simulation\n", "abstract": " Instruction-set structure (ISA) simulators are an integral part of today's processor and software design process. While increasing complexity of the architectures demands high performance simulation, the increasing variety of available architectures makes retargetability a critical feature of an instruction-set simulator. Retargetability requires generic models while high performance demands target specific customizations. To address these contradictory requirements, we have developed a generic instruction model and a generic decode algorithm that facilitates easy and efficient retargetability of the ISA-simulator for a wide range of processor architectures such as RISC, CISC, VLIW and variable length instruction set processors. The instruction model is used to generate compact and easy to debug instruction descriptions that are very similar to that of architecture manual. These descriptions are used to generate high\u00a0\u2026", "num_citations": "64\n", "authors": ["1805"]}
{"title": "Dypo: Dynamic pareto-optimal configuration selection for heterogeneous mpsocs\n", "abstract": " Modern multiprocessor systems-on-chip (MpSoCs) offer tremendous power and performance optimization opportunities by tuning thousands of potential voltage, frequency and core configurations. As the workload phases change at runtime, different configurations may become optimal with respect to power, performance or other metrics. Identifying the optimal configuration at runtime is infeasible due to the large number of workloads and configurations. This paper proposes a novel methodology that can find the Pareto-optimal configurations at runtime as a function of the workload. To achieve this, we perform an extensive offline characterization to find classifiers that map performance counters to optimal configurations. Then, we use these classifiers and performance counters at runtime to choose Pareto-optimal configurations. We evaluate the proposed methodology by maximizing the performance per watt for 18\u00a0\u2026", "num_citations": "62\n", "authors": ["1805"]}
{"title": "Efficient trace signal selection using augmentation and ILP techniques\n", "abstract": " A key problem in post-silicon validation is to identify a small set of traceable signals that are effective for debug during silicon execution. Most signal selection techniques rely on a metric based on circuit structure. Simulation-based signal selection is promising but have major drawbacks in computation overhead and restoration quality. In this paper, we propose an efficient simulation-based signal selection technique to address these bottlenecks. Our approach uses (1) bounded mock simulations to determine state restoration effectiveness, and (2) an ILP-based algorithm for refining selected signals over different simulation runs. Experimental results demonstrate that our algorithm can provide significantly better restoration ratio (up to 515%, 51% on average) compared to the state-of-the-art techniques.", "num_citations": "60\n", "authors": ["1805"]}
{"title": "System-level validation: high-level modeling and directed test generation techniques\n", "abstract": " This book covers state-of-the art techniques for high-level modeling and validation of complex hardware/software systems, including those with multicore architectures. Readers will learn to avoid time-consuming and error-prone validation from the comprehensive coverage of system-level validation, including high-level modeling of designs and faults, automated generation of directed tests, and efficient validation methodology using directed tests and assertions. The methodologies described in this book will help designers to improve the quality of their validation, performing as much validation as possible in the early stages of the design, while reducing the overall validation effort and cost.", "num_citations": "59\n", "authors": ["1805"]}
{"title": "System-wide leakage-aware energy minimization using dynamic voltage scaling and cache reconfiguration in multitasking systems\n", "abstract": " System optimization techniques are widely used to improve energy efficiency as well as overall performance. Dynamic voltage scaling (DVS) is well studied and known to be successful in reducing processor energy consumption. Due to the increasing significance of the memory subsystem's energy consumption, dynamic cache reconfiguration (DCR) techniques are recently proposed at the aim of improving cache subsystem's energy efficiency. As the manufacturing technology scales into the order of nanometers, leakage current, which leads to static power consumption, becomes a significant contributor in the overall power dissipation. In this paper, we consider various system components and study their impact on system-wide energy consumption under different processor voltage levels as well as cache configurations. Based on the observation, we efficiently integrate DVS and DCR techniques together to make\u00a0\u2026", "num_citations": "57\n", "authors": ["1805"]}
{"title": "Functional test generation using efficient property clustering and learning techniques\n", "abstract": " Functional verification is one of the major bottlenecks in system-on-chip design due to the combined effects of increasing complexity and lack of automated techniques for generating efficient tests. Several promising ideas using bounded model checking are proposed over the years to efficiently generate counterexamples (tests). The existing researchers have used incremental satisfiability to improve the counterexample generation, involving only one property by sharing knowledge across instances of the same property with incremental bounds. In this paper, we present a framework that can efficiently reduce the overall test generation time by exploiting the similarity among different properties. This paper makes two primary contributions: (1) it proposes novel methods to cluster similar properties; and (2) it develops efficient learning techniques that can significantly reduce the overall test generation time for the\u00a0\u2026", "num_citations": "56\n", "authors": ["1805"]}
{"title": "Leakage-aware energy minimization using dynamic voltage scaling and cache reconfiguration in real-time systems\n", "abstract": " System optimization techniques are widely used to improve energy efficiency as well as overall performance. Dynamic voltage scaling (DVS) is acknowledged to be successful in reducing processor energy consumption. Due to the increasing significance of the memory subsystem's energy consumption, dynamic cache reconfiguration (DCR) techniques are recently proposed at the aim of saving cache subsystem's energy consumption. As the manufacturing technology scales into the order of nanometers, leakage current, both in the processor and cache subsystem, becomes a significant contributor in the overall power dissipation. In this paper, we efficiently integrate processor voltage scaling and cache reconfiguration together that is aware of leakage power to minimize overall system energy consumption. Experimental results demonstrate that our approach outperforms existing techniques by on average 12 - 23%.", "num_citations": "54\n", "authors": ["1805"]}
{"title": "Coverage-driven automatic test generation for UML activity diagrams\n", "abstract": " Due to the increasing complexity of today's embedded systems, the analysis and validation of such systems is becoming a major challenge. UML is gradually adopted in the embedded system design as a system level specification. One of the major bottlenecks in the validation of UML activity diagrams is the lack of automated techniques for directed test generation. This paper proposes an automated test generation approach for the UML activity diagrams. The contribution of this paper is the use of specification coverage to generate properties as well as design models to enable directed test generation using model checking. Our experimental results demonstrate that our approach can drastically reduce the validation effort in both specification and implementation levels.", "num_citations": "53\n", "authors": ["1805"]}
{"title": "Trojan localization using symbolic algebra\n", "abstract": " Growing reliance on reusable hardware Intellectual Property (IP) blocks, severely affects the security and trustworthiness of System-on-Chips (SoCs) since untrusted third-party vendors may deliberately insert malicious components to incorporate undesired functionality. Malicious implants may also work as hidden backdoor and leak protected information. In this paper, we propose an automated approach to identify untrustworthy IPs and localize malicious functional modifications (if any). The technique is based on extracting polynomials from gate-level implementation of the untrustworthy IP and comparing them with specification polynomials. The proposed approach is applicable when the specification is available. Our approach is scalable due to manipulation of polynomials instead of BDD-based analysis used in traditional equivalence checking techniques. Experimental results using Trust-HUB benchmarks\u00a0\u2026", "num_citations": "52\n", "authors": ["1805"]}
{"title": "Rapid exploration of pipelined processors through automatic generation of synthesizable RTL models\n", "abstract": " As embedded systems continue to face increasingly higher performance requirements, deeply pipelined processor architectures are being employed to meet desired system performance. System architects critically need modeling techniques to rapidly explore and evaluate candidate architectures based on area, power, and performance constraints. We present an exploration framework for pipelined processors. We use the EXPRESSION Architecture Description Language (ADL) to capture a wide spectrum of processor architectures. The ADL has been used to enable performance driven exploration by generating a software toolkit from the ADL specification. In this paper, we present a functional abstraction technique to automatically generate synthesizable RTL from the ADL specification. Automatic generation of RTL enables rapid exploration of candidate architectures under given design constraints such as area\u00a0\u2026", "num_citations": "52\n", "authors": ["1805"]}
{"title": "Dynamic cache reconfiguration for soft real-time systems\n", "abstract": " In recent years, efficient dynamic reconfiguration techniques have been widely employed for system optimization. Dynamic cache reconfiguration is a promising approach for reducing energy consumption as well as for improving overall system performance. It is a major challenge to introduce cache reconfiguration into real-time multitasking systems, since dynamic analysis may adversely affect tasks with timing constraints. This article presents a novel approach for implementing cache reconfiguration in soft real-time systems by efficiently leveraging static analysis during runtime to minimize energy while maintaining the same service level. To the best of our knowledge, this is the first attempt to integrate dynamic cache reconfiguration in real-time scheduling techniques. Our experimental results using a wide variety of applications have demonstrated that our approach can significantly reduce the cache energy\u00a0\u2026", "num_citations": "51\n", "authors": ["1805"]}
{"title": "Specification-driven directed test generation for validation of pipelined processors\n", "abstract": " Functional validation is a major bottleneck in pipelined processor design due to the combined effects of increasing design complexity and lack of efficient techniques for directed test generation. Directed test vectors can reduce overall validation effort, since shorter tests can obtain the same coverage goal compared to the random tests. This article presents a specification-driven directed test generation methodology. The proposed methodology makes three important contributions. First, a general graph model is developed that can capture the structure and behavior (instruction set) of a wide variety of pipelined processors. The graph model is generated from the processor specification. Next, we propose a functional fault model that is used to define the functional coverage for pipelined architectures. Finally, we propose two complementary test generation techniques: test generation using model checking, and test\u00a0\u2026", "num_citations": "50\n", "authors": ["1805"]}
{"title": "Post-silicon validation in the SoC era: A tutorial introduction\n", "abstract": " Editor's note: Post-silicon validation is a complex and critical component of a modern system-on-chip (SoC) design verification. It includes a large number of inter-related activities each with its own nuance and subtleties, requires extensive planning, and spans the entire system design lifecycle. This article provides a comprehensive high-level overview of the various facets of post-silicon validation, and includes industrial case studies illustrating their real-life application.", "num_citations": "49\n", "authors": ["1805"]}
{"title": "Hybrid-compiled simulation: An efficient technique for instruction-set architecture simulation\n", "abstract": " Instruction-set simulators are critical tools for the exploration and validation of new processor architectures. Due to the increasing complexity of architectures and time-to-market pressure, performance is the most important feature of an instruction-set simulator. Interpretive simulators are flexible but slow, whereas compiled simulators deliver speed at the cost of flexibility and compilation overhead. This article presents a hybrid instruction-set-compiled simulation (HISCS) technique for generation of fast instruction-set simulators that combines the benefit of both compiled and interpretive simulation. This article makes two important contributions: (i) it improves the interpretive simulation performance by applying compiled simulation at the instruction level using a novel template-customization technique to generate optimized decoded instructions during compile time; and (ii) it reduces the compile-time overhead by\u00a0\u2026", "num_citations": "48\n", "authors": ["1805"]}
{"title": "Test generation using SAT-based bounded model checking for validation of pipelined processors\n", "abstract": " Functional verification is one of the major bottlenecks in microprocessor design. Simulation-based techniques are the most widely used form of processor verification. Efficient test generation is crucial for the simulation-based verification. We present an efficient test generation methodology using SAT-based bounded model checking (BMC). This paper addresses two important challenges in test generation using SAT-based BMC: determination of bound for each property, and application of design and property decompositions to improve test generation time as well as memory requirement. Our experimental results using a MIPS processor demonstrate the feasibility and usefulness of our approach.", "num_citations": "45\n", "authors": ["1805"]}
{"title": "Hardware Trojan detection using ATPG and model checking\n", "abstract": " The threat of hardware Trojans' existence in inte-gratedcircuits has become a major concern in System-on-Chip (SoC) design industry as well as in military/defense organizations. There is an increased emphasis on finding effective ways to detect and activate hardware Trojans in current research efforts. However, state-of-the-art approaches suffer from the lack of completeness and scalability. Moreover, most of the existing methods cannot generate efficient tests to activate the potential hidden Trojan. In this paper, we propose an effective test generation approach which is capable of activating malicious functionality hidden in large sequential designs. Automatic test pattern generation (ATPG) works well on full-scan designs, whereas model checking is suitable for logic blocks without scan chain. Due to overhead considerations, partial-scan chain insertion is the standard practice today. Unfortunately, neither ATPG\u00a0\u2026", "num_citations": "44\n", "authors": ["1805"]}
{"title": "Dynamic reconfiguration of two-level caches in soft real-time embedded systems\n", "abstract": " Cache reconfiguration is a promising optimization technique for reducing memory hierarchy energy consumption with little or no impact on overall system performance. While cache reconfiguration is successful in desktop-based systems, it is not directly applicable in real-time systems due to timing constraints. Existing scheduling-aware cache reconfiguration techniques consider only one-level cache. It is a major challenge to dynamically tune multi-level caches since the exploration space is prohibitively large. This paper efficiently integrates cache reconfiguration in soft real-time systems with a unified two-level cache hierarchy. We utilize a set of exploration heuristics during our static analysis which effectively decreases the exploration time while keeps the generated profile results beneficial to be leveraged during runtime. Our experimental results have demonstrated 32 - 49% energy savings with minor impact on\u00a0\u2026", "num_citations": "43\n", "authors": ["1805"]}
{"title": "Processor-memory co-exploration driven by a memory-aware architecture description language\n", "abstract": " Memory represents a major bottleneck in modern embedded systems. Traditionally, memory organizations for programmable systems assumed a fixed cache hierarchy. With the widening processor-memory gap, more aggressive memory technologies and organizations have appeared, allowing customization of a heterogeneous memory architecture tuned for the application. However, such a processor-memory co-exploration approach critically needs the ability to explicitly capture heterogeneous memory architectures. We present in this paper a language-based approach to explicitly capture the memory subsystem configuration, and perform exploration of the memory architecture to trade-off cost versus performance. We present a set of experiments using our Memory-Aware Architectural Description Language to drive the exploration of the memory subsystem for the TIC6211 processor architecture, demonstrating\u00a0\u2026", "num_citations": "43\n", "authors": ["1805"]}
{"title": "Functional test generation using property decompositions for validation of pipelined processors\n", "abstract": " Functional validation is a major bottleneck in pipelined processor design. Simulation using functional test vectors is the most widely used form of processor validation. While existing model checking based approaches have proposed several promising ideas for efficient test generation, many challenges remain in applying them to realistic pipelined processors. The time and resources required for test generation using existing model checking based techniques can be extremely large. This paper presents an efficient test generation technique using decompositional model checking. The contribution of the paper is the development of both property and design decomposition procedures for efficient test generation of pipelined processors. Our experimental results using a multi-issue MIPS processor demonstrate several orders-of-magnitude reduction in memory requirement and test generation time", "num_citations": "42\n", "authors": ["1805"]}
{"title": "Architecture description language (ADL)-driven software toolkit generation for architectural exploration of programmable SOCs\n", "abstract": " Advances in semiconductor technology permit increasingly complex applications to be realized using programmable systems-on-chips (SOCs). Furthermore, shrinking time-to-market demands, coupled with the need for product versioning through software modification of SOC platforms, have led to a significant increase in the software content of these SOCs. However, designer productivity is greatly hampered by the lack of automated software generation tools for the exploration and evaluation of different architectural configurations. Traditional hardware-software codesign flows do not support effective exploration and customization of the embedded processors used in programmable SOCs. The inherently application-specific nature of embedded processors and the stringent area, power, and performance constraints in embedded systems design critically require a fast and automated architecture exploration\u00a0\u2026", "num_citations": "42\n", "authors": ["1805"]}
{"title": "An automated configurable Trojan insertion framework for dynamic trust benchmarks\n", "abstract": " Malicious hardware modification, also known as hardware Trojan attack, has emerged as a serious security concern for electronic systems. Such attacks compromise the basic premise of hardware root of trust. Over the past decade, significant research efforts have been directed to carefully analyze the trust issues arising from hardware Trojans and to protect against them. This vast body of work often needs to rely on well-defined set of trust benchmarks that can reliably evaluate the effectiveness of the protection methods. In recent past, efforts have been made to develop a benchmark suite to analyze the effectiveness of pre-silicon Trojan detection and prevention methodologies. However, there are only a limited number of Trojan inserted benchmarks available. Moreover, there is an inherent bias as the researcher is aware of Trojan properties such as location and trigger condition since the current benchmarks are\u00a0\u2026", "num_citations": "39\n", "authors": ["1805"]}
{"title": "A bitmask-based code compression technique for embedded systems\n", "abstract": " Embedded systems are constrained by the available memory. Code compression techniques address this issue by reducing the code size of application programs. Dictionary-based code compression techniques are popular because they offer both good compression ratio and fast decompression scheme. Recently proposed techniques by J. Prakash et al. (2003) improve standard dictionary-based compression by considering mismatches. This paper makes two important contributions: i) it provides a cost-benefit analysis framework for improving the compression ratio by creating more matching patterns, and ii) it develops an efficient code compression technique using bitmasks to improve the compression ratio without introducing any decompression penalty. To demonstrate the usefulness of our approach we have used applications from various domains and compiled for a wide variety of architectures. Our\u00a0\u2026", "num_citations": "39\n", "authors": ["1805"]}
{"title": "Efficient combination of trace and scan signals for post silicon validation and debug\n", "abstract": " Post-silicon validation is as an important aspect of any integrated circuit design methodology. The primary objective is to capture the bugs that have escaped the pre-silicon validation phase. A major challenge in post-silicon debug is the limited observability of internal signals in the circuit. Recent technological advances, such as embedded logic analysis, allow to store some signal states in a trace buffer. A promising direction to improve observability is to combine a small set of signals traced every cycle with a large set of scan signals stored across several cycles. The limited size of the trace buffer constrains the number of trace and scan signals that can be stored. In this paper, we propose an efficient algorithm to select a profitable combination of trace and scan signals to maximize the overall signal restoration performance. Our experimental results using ISCAS'89 benchmarks demonstrate that our approach can\u00a0\u2026", "num_citations": "38\n", "authors": ["1805"]}
{"title": "Functional test generation using design and property decomposition techniques\n", "abstract": " Functional verification of microprocessors is one of the most complex and expensive tasks in the current system-on-chip design methodology. Simulation using functional test vectors is the most widely used form of processor validation. A significant bottleneck in the validation of such systems is the lack of automated techniques for directed test generation. While existing model checking--based approaches have proposed several promising ideas for automated test generation, many challenges remain in applying them to industrial microprocessors. The time and resources required for test generation using existing model checking--based techniques can be prohibitively large. This article presents an efficient test generation technique using decompositional model checking. The contribution of the article is the development of both property and design decomposition procedures for efficient test generation of pipelined\u00a0\u2026", "num_citations": "38\n", "authors": ["1805"]}
{"title": "Modeling and validation of pipeline specifications\n", "abstract": " Verification is one of the most complex and expensive tasks in the current Systems-on-Chip design process. Many existing approaches employ a bottom-up approach to pipeline validation, where the functionality of an existing pipelined processor is, in essence, reverse-engineered from its RT-level implementation. Our validation technique is complementary to these bottom-up approaches. Our approach leverages the system architect's knowledge about the behavior of the pipelined architecture, through architecture description language (ADL) constructs, and thus allows a powerful top-down approach to pipeline validation. The most important requirement in top-down validation process is to ensure that the specification (reference model) is golden. This paper addresses automatic validation of processor, memory, and coprocessor pipelines described in an ADL. We present a graph-based modeling that captures\u00a0\u2026", "num_citations": "38\n", "authors": ["1805"]}
{"title": "PreDVS: Preemptive dynamic voltage scaling for real-time systems using approximation scheme\n", "abstract": " System optimization techniques based on dynamic voltage scaling (DVS) are widely used with the aim of reducing processor energy consumption. Inter-task DVS assigns the same voltage level to all the instances of each task. Its intra-task counterpart exploits more energy savings by assigning multiple voltage levels within each task. In this paper, we propose a voltage scaling technique, named PreDVS, which assigns voltage levels based on the task set's preemptive scheduling for hard real-time systems. Our approach is based on an approximation scheme hence can guarantee to generate solutions within a specified quality bound (e.g., within 1% of the optimal) and is different from any existing inter- or intra-task DVS techniques. PreDVS exploits static time slack at a finer granularity and achieves more energy saving than inter-task scaling without introducing any extra voltage switching overhead. Moreover, it can\u00a0\u2026", "num_citations": "37\n", "authors": ["1805"]}
{"title": "Automatic verification of in-order execution in microprocessors with fragmented pipelines and multicycle functional units\n", "abstract": " As embedded systems continue to face increasingly higher performance requirements, deeply pipelined processor architectures are being employed to meet desired system performance. System architects critically need modeling techniques that allow exploration, evaluation, customization and validation of different processor pipeline configurations, tuned for a specific application domain. We propose a novel finite state machine (FSM) based modeling of pipelined processors and define a set of properties that can be used to verify the correctness of in-order execution in the presence of fragmented pipelines and multicycle functional units. Our approach leverages the system architect's knowledge about the behavior of the pipelined processor through architecture description language (ADL) constructs, and thus allows a powerful top-down approach to pipeline verification. We applied this methodology to the DLX\u00a0\u2026", "num_citations": "37\n", "authors": ["1805"]}
{"title": "Automated test generation for debugging arithmetic circuits\n", "abstract": " Optimized and custom arithmetic circuits are widely used in embedded systems such as multimedia applications, cryptography systems, signal processing and console games. Debugging of arithmetic circuits is a challenge due to increasing complexity coupled with non-standard implementations. Existing equivalence checking techniques produce a remainder to indicate the presence of a potential bug. However, bug localization remains a major bottleneck. Simulation-based validation using random or constrained-random tests are not effective and can be infeasible for complex arithmetic circuits. In this paper, we present an automated test generation and bug localization technique for debugging arithmetic circuits. This paper makes two important contributions. We propose an automated approach for generating directed tests by suitable assignments of input variables to make the reminder non-zero. The generated\u00a0\u2026", "num_citations": "35\n", "authors": ["1805"]}
{"title": "Efficient test generation for Trojan detection using side channel analysis\n", "abstract": " Detection of hardware Trojans is vital to ensure the security and trustworthiness of System-on-Chip (SoC) designs. Side-channel analysis is effective for Trojan detection by analyzing various side-channel signatures such as power, current and delay. In this paper, we propose an efficient test generation technique to facilitate side-channel analysis utilizing dynamic current. While early work on current-aware test generation has proposed several promising ideas, there are two major challenges in applying it on large designs: (i) the test generation time grows exponentially with the design complexity, and (ii) it is infeasible to detect Trojans since the side-channel sensitivity is marginal compared to the noise and process variations. Our proposed work addresses both challenges by effectively exploiting the affinity between the inputs and rare (suspicious) nodes. We formalize the test generation problem as a searching\u00a0\u2026", "num_citations": "34\n", "authors": ["1805"]}
{"title": "Decoding-aware compression of FPGA bitstreams\n", "abstract": " Bitstream compression is important in reconfigurable system design since it reduces the bitstream size and the memory requirement. It also improves the communication bandwidth and thereby decreases the reconfiguration time. Existing research in this field has explored two directions: efficient compression with slow decompression or fast decompression at the cost of compression efficiency. This paper proposes a novel decode-aware compression technique to improve both compression and de compression efficiencies. The three major contributions of this paper are: 1) smart placement of compressed bitstreams that can significantly decrease the overhead of decompression engine; 2) selection of profitable parameters for bitstream compression; and 3) efficient combination of bitmask-based compression and run length encoding of repetitive patterns. Our proposed technique outperforms the existing\u00a0\u2026", "num_citations": "34\n", "authors": ["1805"]}
{"title": "An efficient code compression technique using application-aware bitmask and dictionary selection methods\n", "abstract": " Memory plays a crucial role in designing embedded systems. A larger memory can accommodate more and large applications but increases cost, area, as well as energy requirements. Code compression techniques address this problem by reducing the size of the applications. While early work on bitmask-based compression has proposed several promising ideas, many challenges remain in applying them to embedded system design. This paper makes two important contributions to address these challenges by developing application-specific bitmask selection and bitmask-aw are dictionary selection techniques. The authors applied these techniques for code compression of TI and MediaBench applications to demonstrate the usefulness of the approach", "num_citations": "34\n", "authors": ["1805"]}
{"title": "A retargetable framework for instruction-set architecture simulation\n", "abstract": " Instruction-set architecture (ISA) simulators are an integral part of today's processor and software design process. While increasing complexity of the architectures demands high-performance simulation, the increasing variety of available architectures makes retargetability a critical feature of an instruction-set simulator. Retargetability requires generic models while high-performance demands target specific customizations. To address these contradictory requirements, we have developed a generic instruction model and a generic decode algorithm that facilitates easy and efficient retargetability of the ISA-simulator for a wide range of processor architectures, such as RISC, CISC, VLIW, and variable length instruction-set processors. The instruction model is used to generate compact and easy to debug instruction descriptions that are very similar to that of architecture manual. These descriptions are used to generate\u00a0\u2026", "num_citations": "34\n", "authors": ["1805"]}
{"title": "Directed test generation using concolic testing on RTL models\n", "abstract": " Functional validation is one of the most time consuming steps in System-on-Chip (SoC) design methodology. In today's industrial practice, simulating designs using billions of random or constrained-random tests can lead to high functional coverage. However, it is hard to cover the remaining small fraction of corner cases and rare functional scenarios. While formal methods are promising in such cases, it is infeasible to apply them on large designs. In this paper, we propose a fully automated and scalable approach for generating directed tests using concolic testing of RTL models. While application of concolic testing on hardware designs has shown some promising results, existing approaches are tuned for improving overall coverage, rather than covering a specific target. We developed a Control Flow Graph (CFG) assisted directed test generation method that can efficiently generate a test to activate a given target\u00a0\u2026", "num_citations": "33\n", "authors": ["1805"]}
{"title": "Automatic RTL test generation from SystemC TLM specifications\n", "abstract": " SystemC transaction-level modeling (TLM) is widely used to enable early exploration for both hardware and software designs. It can reduce the overall design and validation effort of complex system-on-chip (SOC) architectures. However, due to lack of automated techniques coupled with limited reuse of validation efforts between abstraction levels, SOC validation is becoming a major bottleneck. This article presents a novel top-down methodology for automatically generating register transfer-level (RTL) tests from SystemC TLM specifications. It makes two important contributions: i) it proposes a method that can automatically generate TLM tests using various coverage metrics, and (ii) it develops a test refinement specification for automatically converting TLM tests to RTL tests in order to reduce overall validation effort. We have developed a tool which incorporates these activities to enable automated RTL test\u00a0\u2026", "num_citations": "33\n", "authors": ["1805"]}
{"title": "Scalable hardware Trojan activation by interleaving concrete simulation and symbolic execution\n", "abstract": " Intellectual Property (IP) based System-on-Chip (SoC) design is a widely used practice today. The IPs gathered from third-party vendors may not be trustworthy since they may contain malicious implants (hardware Trojans). To avoid the detection of the Trojan, adversaries usually hide it under rare branches or rare assignments triggered under extremely rare input sequences. Due to exponential input space complexity, state-of-the-art constrained-random test generation methods are not suitable for activating these rare scenarios. While existing model checking based directed test generation approaches are promising, they are not capable of generating tests for large RTL designs due to the capacity restrictions of formal methods. In this paper, we propose an automated and scalable test generation approach for activation of hardware Trojans in RTL designs. This paper makes three important contributions. First, it\u00a0\u2026", "num_citations": "32\n", "authors": ["1805"]}
{"title": "Property learning techniques for efficient generation of directed tests\n", "abstract": " Property falsification in model checking is widely used for automated generation of directed tests. Due to state space explosion problem, traditional model checking techniques cannot handle large scale designs. SAT-based bounded model checking is promising to address the prohibitively large time and resource requirements during the property falsification. This article presents several efficient learning techniques that can improve the overall test generation time for a single property as well as a cluster of similar properties. The goal is to exploit both variable assignments and common conflict clauses of the prechecked partial or similar SAT instances for property falsification. Our method makes three novel contributions: 1) investigates the decision ordering-based learnings for a single SAT instance; 2) applies the decision ordering learnings between similar SAT instances; and 3) exploits the relation between the\u00a0\u2026", "num_citations": "32\n", "authors": ["1805"]}
{"title": "System-on-Chip Security\n", "abstract": " We are living in a connected world, where a wide variety of computing and sensing components interact with each other. Secure computation and trusted communication are essential as intelligent computing devices are increasingly embedded in every possible device in our daily life such as wearable devices, autonomous vehicles, and smart homes. Any failure of security and trust requirements of these devices may endanger human life and environment by causing damages to critical infrastructure, violating personal privacy, or undermining the credibility of a business. Attacks on hardware can be more critical than traditional attacks on software since patching is extremely difficult (almost impossible) on hardware designs. Note that hardware designs are fixed after fabrication, and any existing vulnerability in their implementations can be exploited by attackers. Moreover, the same attack can be repeated on every\u00a0\u2026", "num_citations": "30\n", "authors": ["1805"]}
{"title": "Real-time detection and localization of DoS attacks in NoC based SoCs\n", "abstract": " Network-on-Chip (NoC) is widely employed by multi-core System-on-Chip (SoC) architectures to cater to their communication requirements. The increased usage of NoC and its distributed nature across the chip has made it a focal point of potential security attacks. Denial-of-Service (DoS) is one such attack that is caused by a malicious intellectual property (IP) core flooding the network with unnecessary packets causing significant performance degradation through NoC congestion. In this paper, we propose a lightweight and real-time DoS attack detection mechanism. Once a potential attack has been flagged, our approach is also capable of localizing the malicious IP using latency data gathered by NoC components. Experimental results demonstrate the effectiveness of our approach with timely attack detection and localization while incurring minor area and power overhead (less than 6% and 4%, respectively).", "num_citations": "30\n", "authors": ["1805"]}
{"title": "Directed test generation for validation of cache coherence protocols\n", "abstract": " Computing systems utilize multicore processors with complex cache coherence protocols to meet the increasing need for performance and energy improvement. It is a major challenge to verify the correctness of a cache coherence protocol since the number of reachable states grows exponentially with the number of cores. In this paper, we propose an efficient test generation technique, which can be used to achieve full state and transition coverage in simulation-based verification for a wide variety of cache coherence protocols. Based on effective analysis of the state space structure, our method can generate more efficient test sequences (50% shorter) on-the-fly compared with tests generated by BFS. While our on-the-fly method can reduce the numbers of required tests by half, it can still be impractical to verify all possible transitions in the presence of large number of cores. We propose scalable on-the-fly test\u00a0\u2026", "num_citations": "29\n", "authors": ["1805"]}
{"title": "Postsilicon trace signal selection using machine learning techniques\n", "abstract": " A key problem in postsilicon validation is to identify a small set of traceable signals that are effective for debug during silicon execution. Structural analysis used by traditional signal selection techniques leads to a poor restoration quality. In contrast, simulation-based selection techniques provide superior restorability but incur significant computation overhead. In this paper, we propose an efficient signal selection technique using machine learning to take advantage of simulation-based signal selection while significantly reducing the simulation overhead. The basic idea is to train a machine learning framework with a few simulation runs and utilize its effective prediction capability (instead of expensive simulation) to identify beneficial trace signals. Specifically, our approach uses: (1) bounded mock simulations to generate training vectors for the machine learning technique and (2) a compound search-space\u00a0\u2026", "num_citations": "29\n", "authors": ["1805"]}
{"title": "Efficient test case generation for validation of UML activity diagrams\n", "abstract": " Unified Modeling Language (UML) is widely used as a system level specification language in embedded system design. Due to the increasing complexity of embedded systems, the analysis and validation of UML specifications is becoming a challenge. UML activity diagram is promising to modeling the overall system behavior. However, lack of techniques for automated test case generation is one major bottleneck in the UML activity diagram validation. This article presents a methodology for automatically generating test cases based on various model checking techniques. It makes three primary contributions: First, we propose coverage-driven mapping rules that can automatically translate activity diagram to formal models. Next, we present a procedure for automatic property generation according to error models. Finally, we apply various model checking based test case generation techniques to enable\u00a0\u2026", "num_citations": "29\n", "authors": ["1805"]}
{"title": "Efficient selection of trace and scan signals for post-silicon debug\n", "abstract": " Post-silicon validation is a critical part of integrated circuit design methodology. The primary objective is to detect and eliminate the bugs that have escaped pre-silicon validation phase. One of the key challenges in post-silicon validation is the limited observability of internal signals in manufactured chips. A promising direction to improve observability is to combine trace and scan signals-a small set of trace signals are stored in every cycle, whereas a large set of scan signals are dumped across multiple cycles. Existing techniques are not very effective, since they explore a coarse-grained combination of trace and scan signals. In this paper, we propose a fine-grained architecture that addresses this issue using various scan chains with different dumping periods. We also propose efficient algorithms to select beneficial signals based on this architecture. Our experimental results demonstrate that our approach can\u00a0\u2026", "num_citations": "28\n", "authors": ["1805"]}
{"title": "Automated generation of directed tests for transition coverage in cache coherence protocols\n", "abstract": " Processors with multiple cores and complex cache coherence protocols are widely employed to improve the overall performance. It is a major challenge to verify the correctness of a cache coherence protocol since the number of reachable states grows exponentially with the number of cores. In this paper, we propose an efficient test generation technique, which can be used to achieve full state and transition coverage in simulation based verification for a wide variety of cache coherence protocols. Based on effective analysis of the state space structure, our method can generate more efficient test sequences (50% shorter) compared with tests generated by breadth first search. Moreover, our proposed approach can generate tests on-the-fly due to its space efficient design.", "num_citations": "28\n", "authors": ["1805"]}
{"title": "Automated test generation for debugging multiple bugs in arithmetic circuits\n", "abstract": " Optimized and custom arithmetic circuits are widely used in embedded systems such as multimedia applications, cryptography systems, signal processing and console games. Debugging of arithmetic circuits is a challenge due to increasing complexity coupled with non-standard implementations. Existing algebraic rewriting techniques produce a remainder to indicate the presence of a potential bug. However, bug localization remains a major bottleneck. Simulation-based validation using random or constrained-random tests are not effective for complex arithmetic circuits due to bit-blasting. In this paper, we present an automated test generation and bug localization technique for debugging arithmetic circuits. This paper makes four important contributions. We propose an automated approach for generating directed tests by suitable assignments of input variables to make the remainder non-zero. The generated tests\u00a0\u2026", "num_citations": "27\n", "authors": ["1805"]}
{"title": "Assertion-based functional consistency checking between TLM and RTL models\n", "abstract": " Transaction Level Modeling (TLM) is promising for functional validation at an early stage of System-on-Chip (SoC) design. However, raising the abstraction level brings a major challenge - how to guarantee the functional consistency between TLM specifications and Register Transfer Level (RTL) implementations? This paper proposes an efficient mechanism for functional consistency checking using assertion observability. The experimental results using several industrial designs demonstrate that our method can automatically check the functional consistency between different abstraction levels.", "num_citations": "27\n", "authors": ["1805"]}
{"title": "Efficient trace data compression using statically selected dictionary\n", "abstract": " Post-silicon validation and debug have gained importance in recent years to track down errors that have escaped the pre-silicon phase. Limited observability of internal signals during post-silicon debug necessitates the storage of signal states in real time. Trace buffers are used to store these states. To increase the debug observation window, it is essential to compress these trace signals, so that trace data over larger number of cycles can be stored in the trace buffer while keeping its size constant. In this paper, we propose several dictionary based compression techniques for trace data compression that takes account of the fact that the difference between golden and erroneous trace data is small. Therefore, the static dictionary selected based on golden trace data can provide notably better compression performance than the dynamic dictionaries selected in the current approaches. This will also significantly reduce\u00a0\u2026", "num_citations": "27\n", "authors": ["1805"]}
{"title": "Security-aware FSM design flow for identifying and mitigating vulnerabilities to fault attacks\n", "abstract": " The security of a system-on-chip (SoC) can be compromised by exploiting the vulnerabilities of the finite state machines (FSMs) in the SoC controller modules through fault injection attacks. These vulnerabilities may be unintentionally introduced by traditional FSM design practices or by CAD tools during synthesis. In this paper, we first analyze how the vulnerabilities in an FSM can be exploited by fault injection attacks. Then, we propose a security-aware FSM design flow for ASIC designs to mitigate them and prevent fault attacks on FSM. Our proposed FSM design flow starts with a security-aware encoding scheme which makes the FSM resilient against fault attacks. However, the vulnerabilities introduced by the CAD tools cannot be addressed by encoding schemes alone. To analyze for such vulnerabilities, we develop a novel technique named analyzing vulnerabilities in FSM. If any vulnerability exists, we\u00a0\u2026", "num_citations": "26\n", "authors": ["1805"]}
{"title": "Scalable test generation by interleaving concrete and symbolic execution\n", "abstract": " Functional validation is widely acknowledged as a major challenge for System-on-Chip (SoC) designs. Directed tests are superior compared to random tests since a significantly less number of directed tests can achieve the same coverage goal. Existing test generation techniques have inherent limitations due to use of formal methods. First, these approaches expect formal specification and do not directly support Hardware Description Language (HDL) models. Most importantly, the complexity of real world designs usually exceeds the capacity of model checking tools. In this paper, we propose a scalable technique to enable directed test generation for HDL models by combining static analysis and simulation based validation. Unlike existing approaches that support a limited set of HDL features, our approach covers a wide variety of features including dynamic array references. We have compared our approach with\u00a0\u2026", "num_citations": "26\n", "authors": ["1805"]}
{"title": "Functional verification of programmable embedded architectures: a top-down approach\n", "abstract": " It is widely acknowledged that the cost of validation and testing comprises a s-nificant percentage of the overall development costs for electronic systems today, and is expected to escalate sharply in the future. Many studies have shown that up to 70% of the design development time and resources are spent on functional verification. Functional errors manifest themselves very early in the design flow, and unless they are detected up front, they can result in severe consequence-both financially and from a safety viewpoint. Indeed, several recent instances of high-profile functional errors (eg, the Pentium FDIV bug) have resulted in-creased attention paid to verifying the functional correctness of designs. Recent efforts have proposed augmenting the traditional RTL simulation-based validation methodology with formal techniques in an attempt to uncover hard-to-find c-ner cases, with the goal of trying to reach RTL functional verification closure. However, what is often not highlighted is the fact that in spite of the tremendous time and effort put into such efforts at the RTL and lower levels of abstraction, the complexity of contemporary embedded systems makes it difficult to guarantee functional correctness at the system level under all possible operational scenarios. The problem is exacerbated in current System-on-Chip (SOC) design meth-ologies that employ Intellectual Property (IP) blocks composed of processor cores, coprocessors, and memory subsystems. Functional verification becomes one of the major bottlenecks in the design of such systems.", "num_citations": "26\n", "authors": ["1805"]}
{"title": "Exploration of memory and cluster modes in directory-based many-core cmps\n", "abstract": " Networks-on-chip have become the standard interconnect solution to address the communication requirements of many-core chip multiprocessors. It is well-known that network performance and power consumption depend critically on the traffic load. The network traffic itself is a function of not only the application, but also the cache coherence protocol, and memory controller/directory locations. Communication between the distributed directory to memory can introduce hotspots, since the number of memory controllers is much smaller than the number of cores. Therefore, it is critical to account for directorymemory communication, and model them accurately in architecture simulators. This paper analyzes the impact of directorymemory traffic and different memory and cluster modes on the NoC traffic and system performance. We demonstrate that unrealistic models in a widely used multiprocessor simulator produce\u00a0\u2026", "num_citations": "25\n", "authors": ["1805"]}
{"title": "SACR: Scheduling-aware cache reconfiguration for real-time embedded systems\n", "abstract": " Dynamic reconfiguration techniques are widely used for efficient system optimization. Dynamic cache reconfiguration is a promising approach for reducing energy consumption as well as for improving overall system performance. It is a major challenge to introduce cache reconfiguration into real-time embedded systems since dynamic analysis may adversely affect tasks with real-time constraints. This paper presents a novel approach for implementing cache reconfiguration in soft real-time systems by efficiently leveraging static analysis during execution to both minimize energy and maximize performance. To the best of our knowledge, this is the first attempt to integrate dynamic cache reconfiguration in real-time scheduling techniques. Our experimental results using a wide variety of applications have demonstrated that our approach can significantly (up to 74%) reduce the overall energy consumption of the cache\u00a0\u2026", "num_citations": "25\n", "authors": ["1805"]}
{"title": "Securing network-on-chip using incremental cryptography\n", "abstract": " Network-on-chip (NoC) has become the standard communication fabric for on-chip components in modern System-on-chip (SoC) designs. Since NoC has visibility to all communications in the SoC, it has been one of the primary targets for security attacks. While packet encryption can provide secure communication, it can introduce unacceptable energy and performance overhead due to the resource-constrained nature of SoC designs. In this paper, we propose a lightweight encryption scheme that is implemented on the network interface. Our approach improves the performance of encryption without compromising security using incremental cryptography, which exploits the unique NoC traffic characteristics. Experimental results demonstrate that our proposed approach significantly (up to 57%, 30% on average) reduces the encryption time compared to traditional approaches with negligible (less than 2%) impact on\u00a0\u2026", "num_citations": "24\n", "authors": ["1805"]}
{"title": "Automated activation of multiple targets in RTL models using concolic testing\n", "abstract": " Simulation is widely used for validation of Register-Transfer-Level (RTL) models. While simulating with millions of random (or constrained-random) tests can cover majority of the targets (functional scenarios), the number of remaining targets can still be huge (hundreds or thousands) in case of today's industrial designs. Prior work on directed test generation using concolic testing can cover only one target at a time. A naive extension of prior work to activate the remaining targets would be infeasible due to wasted effort in multiple overlapping searches. In this paper, we propose an automated test generation technique for activating multiple targets in RTL models using concolic testing. This paper makes three important contributions. First, it efficiently prunes the targets that can be covered by the tests generated for activating the other targets. Next, it minimizes the overlapping searches while trying to generate tests for\u00a0\u2026", "num_citations": "24\n", "authors": ["1805"]}
{"title": "Automated debugging of arithmetic circuits using incremental gr\u00f6bner basis reduction\n", "abstract": " Symbolic algebra is a promising approach to verify large and complex arithmetic circuits. Existing algebraic-based verification methods generate a remainder to indicate buggy implementation. The remainder is beneficial for debugging of the faulty implementation since it can be used for automated test generation, bug localization, and bug correction. However, existing equivalence checking approaches are not scalable and lead to explosion in size of the remainder when the design is faulty. To make the matters worse, the location of the bug can also lead to the explosion in the number of remainder terms. In this paper, we propose an incremental equivalence checking method to address the scalability challenges by solving the verification problem in the increasing order of design's input complexity. Our proposed approach makes two important contributions. It is able to generate smaller and compact remainders for\u00a0\u2026", "num_citations": "24\n", "authors": ["1805"]}
{"title": "Dynamic selection of trace signals for post-silicon debug\n", "abstract": " Post-silicon validation is one of the most expensive and complex tasks in today's System-on-Chip (SoC) design methodology. A major challenge in post-silicon debug is limited observability of the internal signals. Existing approaches address this issue by selecting a small set of useful signals. These signal states are stored in an on-chip trace buffer during execution. The applicability of existing methods is limited to a specific debug scenario where every component has equal importance all the time. In reality, a verification engineer would like to focus on a specific set of components (functional regions). Some regions can be ignored in a certain duration during execution due to clock gating and other considerations. Similarly, certain regions may be well verified datapath and less likely to have errors compared to other control-intensive regions. In this paper, we propose an efficient signal selection algorithm and a low\u00a0\u2026", "num_citations": "24\n", "authors": ["1805"]}
{"title": "Scalable trace signal selection using machine learning\n", "abstract": " A key problem in post-silicon validation is to identify a small set of traceable signals that are effective for debug during silicon execution. Structural analysis used by traditional signal selection techniques leads to poor restoration quality. In contrast, simulation-based selection techniques provide superior restorability but incur significant computation overhead. In this paper, we propose an efficient signal selection technique using machine learning to take advantage of simulation-based signal selection while significantly reducing the simulation overhead. Our approach uses (1) bounded mock simulations to generate training vectors set for the machine learning technique, and (2) an elimination approach to identify the most profitable signals set. Experimental results indicate that our approach can improve restorability by up to 63.3% (17.2% on average) with a faster or comparable runtime.", "num_citations": "24\n", "authors": ["1805"]}
{"title": "Processor-memory coexploration using an architecture description language\n", "abstract": " Memory represents a major bottleneck in modern embedded systems in terms of cost, power, and performance. Traditionally, memory organizations for programmable embedded systems assume a fixed cache hierarchy. With the widening processor--memory gap, more aggressive memory technologies and organizations have appeared, allowing customization of a heterogeneous memory architecture tuned for specific target applications. However, such a processor--memory coexploration approach critically needs the ability to explicitly capture heterogeneous memory architectures. We present in this paper a language-based approach to explicitly capture the memory subsystem configuration, generate a memory-aware software toolkit, and perform coexploration of the processor--memory architectures. We present a set of experiments using our memory-aware architectural description language (ADL) to drive the\u00a0\u2026", "num_citations": "24\n", "authors": ["1805"]}
{"title": "Directed test generation for validation of multicore architectures\n", "abstract": " Functional validation is widely acknowledged as a major challenge for multicore architectures. Directed tests are promising since a significantly smaller number of directed tests can achieve the same coverage goal compared to constrained-random tests. SAT-based bounded model checking is effective for automated generation of directed tests (counterexamples). While existing approaches focus on clause forwarding between different bounds to reduce the test generation time, this article proposes a novel technique that exploits temporal, structural, and spatial symmetry in multicore designs at the same time. Our proposed technique enables the reuse of the knowledge learned from one core to the remaining cores in multicore architectures (structural symmetry), from one bound to the next for a give property (temporal symmetry), as well as from one property to other properties (spatial symmetry). The experimental\u00a0\u2026", "num_citations": "23\n", "authors": ["1805"]}
{"title": "Dynamic Reconfiguration in Real-Time Systems\n", "abstract": " Computing has transformed humankind. General-purpose computers such as desktops and laptops are used by a larger fraction of the world population. However, in many scenarios, the user may not be aware of inherent computations since these are hidden inside other products. For example, when someone is taking a picture using a digital camera or driving a car, a large amount of data processing and decision making computations are taking place without the user\u2019s knowledge. These systems are commonly termed as embedded systems. Embedded systems are present everywhere, from everyday appliances to biomedical, military, geological, and space equipments.The complexity of these systems is increasing at an exponential rate due to both advances in technology and demand for realization of ever more complex applications in the areas of communication, multimedia, networking, entertainment, military\u00a0\u2026", "num_citations": "23\n", "authors": ["1805"]}
{"title": "Efficient techniques for directed test generation using incremental satisfiability\n", "abstract": " Functional validation is a major bottleneck in the current SOC design methodology. While specification-based validation techniques have proposed several promising ideas, the time and resources required for directed test generation can be prohibitively large. This paper presents an efficient test generation methodology using incremental satisfiability. The existing researches have used incremental SAT to improve counterexample (test) generation involving only one property with different bounds. This paper is the first attempt to utilize incremental satisfiability in directed test generation involving multiple properties. The contribution of this paper is a novel methodology to share learning across multiple properties by developing efficient techniques for property clustering, name substitution, and selective forwarding of conflict clauses. Our experimental results using both software and hardware benchmarks demonstrate\u00a0\u2026", "num_citations": "23\n", "authors": ["1805"]}
{"title": "Modeling and verification of pipelined embedded processors in the presence of hazards and exceptions\n", "abstract": " Embedded systems present a tremendous opportunity to customize designs by exploiting the application behavior. Due to increasing design complexity deeply pipelined high performance embedded processors are common today. In the presence of hazards and exceptions the validation of pipelined embedded processors is a major challenge. We extend a Finite State Machine (FSM) based modeling of pipelined processors to verify the pipeline specification in the presence of hazards and multiple exceptions. Our approach leverages the system architect\u2019s knowledge about the behavior of the pipelined processor, through Architecture Description Language (ADL) constructs, and thus allows a powerful top-down approach to pipeline verification. We applied this methodology to the DLX processor to demonstrate the usefulness of our approach", "num_citations": "23\n", "authors": ["1805"]}
{"title": "Automatic modeling and validation of pipeline specifications driven by an architecture description language [SoC]\n", "abstract": " Verification is one of the most complex and expensive tasks in the current systems-on-chip (SOC) design process. Many existing approaches employ a bottom-up approach to pipeline validation, where the functionality of an existing pipelined processor is, in essence, reverse-engineered from its RT-level implementation. Our approach leverages the system architect's knowledge about the behavior of the pipelined architecture, through architecture description language (ADL) constructs, and thus allows a powerful top-down approach to pipeline validation. This paper addresses automatic validation of processor, memory, and co-processor pipelines described in an ADL. We present a graph-based modeling of architectures which captures both structure and behavior of the architecture. Based on this model, we present formal approaches for automatic validation of the architecture described in the ADL. We applied our\u00a0\u2026", "num_citations": "23\n", "authors": ["1805"]}
{"title": "Real-Time Detection and Localization of Distributed DoS Attacks in NoC-Based SoCs\n", "abstract": " Network-on-chip (NoC) is widely employed by multicore system-on-chip (SoC) architectures to cater to their communication requirements. Increasing NoC complexity coupled with its widespread usage has made it a focal point of potential security attacks. Distributed denial-of-service (DDoS) is one such attack that is caused by malicious intellectual property (IP) cores flooding the network with unnecessary packets causing significant performance degradation through NoC congestion. In this article, we propose an efficient framework for real-time detection and localization of DDoS attacks. This article makes three important contributions. We propose a real-time and lightweight DDoS attack detection technique for NoC-based SoCs by monitoring packets to detect any violations. Once a potential attack has been flagged, our approach is also capable of localizing the malicious IPs using the latency data in the NoC\u00a0\u2026", "num_citations": "22\n", "authors": ["1805"]}
{"title": "Efficient signal selection using fine-grained combination of scan and trace buffers\n", "abstract": " Post-silicon validation is a critical part of integrated circuit design methodology. The primary objective is to detect and eliminate the bugs that has escaped pre-silicon validation phase. One of the key challenges in post-silicon validation is the limited observability of internal signals in manufactured chips. Leveraging on-chip buffers addresses this issue by storing some of the internal signal states during runtime. A promising direction to improve observability is to combine trace and scan signals - a small set of trace signals are stored every cycle, whereas a large set of scan signals are dumped across multiple cycles. Existing techniques are not very effective since they explore a coarse-grained combination of trace and scan signals. In this paper, we propose a fine-grained architecture that addresses this issue by using various scan chains with different dumping periods. We also propose an efficient algorithm to select\u00a0\u2026", "num_citations": "22\n", "authors": ["1805"]}
{"title": "Energy-aware dynamic slack allocation for real-time multitasking systems\n", "abstract": " Dynamic voltage scaling (DVS) has been a very effective technique for processor energy reduction. It adjusts processor voltage and frequency level during runtime. In this article, we propose a general and flexible processor voltage scaling algorithm for real-time multitasking systems. Our approach focuses on exploiting dynamic slack that is created when a task finishes earlier than its estimated worst-case execution time (WCET). Our algorithm is efficient enough to execute at runtime and can be configured flexibly to make tradeoffs between running time and energy savings. By rescheduling tasks effectively, we can achieve almost as much energy savings as if there is no arrival time constraints. Furthermore, our approach can effectively incorporate both leakage power consumption as well as variable scaling overhead. Also, it is relatively independent of task characteristics and scheduling policy. Experimental\u00a0\u2026", "num_citations": "22\n", "authors": ["1805"]}
{"title": "Intra-task dynamic cache reconfiguration\n", "abstract": " Optimization techniques are widely used in embedded systems design to improve overall area, performance and energy requirements. Dynamic cache reconfiguration (DCR) is very effective to reduce energy consumption of cache subsystems. Finding the right reconfiguration points in a task and selecting appropriate cache configurations for each phase are the primary challenges in phase-based DCR. In this paper, we present a novel intra-task dynamic cache reconfiguration technique using a detailed cache model, and tune a highly-configurable cache on a per-phase basis compared to tuning once per application. Experimental results demonstrate that our intra-task DCR can achieve up to 27% (12% on average) and 19% (7% on average) energy savings for instruction and data caches, respectively, without introducing any performance penalty.", "num_citations": "22\n", "authors": ["1805"]}
{"title": "QUEBS: Qualifying event based search in concolic testing for validation of RTL models\n", "abstract": " Input vector generation is an important step during validation and debugging of hardware designs. Validation using random and directed random tests are widely used today. However, these methods can lead to unacceptable functional coverage under tight deadlines. Concolic testing is a semi-formal method to address this issue. It combines concrete simulation guided by symbolic execution. Application of concolic testing in hardware domain is still in its infancy due to the lack of effective traversal strategies. In this paper, we present Qualifying Event Based Search (QUEBS) heuristic for concolic testing. During exhaustive concolic testing, same branch may be selected many times for traversal. Our heuristic limits the number of times a branch can be selected. By preventing repeated selection, it facilitates wider coverage within limited time. Also, whenever a previously uncovered branch is encountered, this limit is\u00a0\u2026", "num_citations": "21\n", "authors": ["1805"]}
{"title": "Efficient decision ordering techniques for SAT-based test generation\n", "abstract": " Model checking techniques are promising for automated generation of directed tests. However, due to the prohibitively large time and resource requirements, conventional model checking techniques do not scale well when checking complex designs. In SAT-based BMC, many variable ordering heuristics have been investigated to improve counterexample (test) generation involving only one property. This paper presents efficient decision ordering techniques that can improve the overall test generation time of a cluster of similar properties. Our method exploits the assignments of previously generated tests and incorporates it in the decision ordering heuristic for current test generation. Our experimental results using both software and hardware benchmarks demonstrate that our approach can drastically reduce the overall test generation time.", "num_citations": "21\n", "authors": ["1805"]}
{"title": "Letter to the Editor: Staphylococcal Infection in Histamine and 5\u2010Hydroxytryptamine Depleted Rats\n", "abstract": " P. SCHUFF\u2010WERNER, W. SPLETTST\u00d6SSER, F. SCHMIDT and G. HUETHER, Serotonin acts as a radical scavenger and is oxidized to a dimer during the respiratory burst of human mononuclear and polymorphonuclear phagocytes*, European Journal of Clinical Investigation, 25, 7,(477-484),(2008).", "num_citations": "21\n", "authors": ["1805"]}
{"title": "Lightweight and trust-aware routing in NoC-based SoCs\n", "abstract": " Increasing System-on-Chip (SoC) design complexity coupled with time-to-market constraints have motivated manufacturers to integrate several third-party Intellectual Property (IP) cores in their SoC designs. IPs acquired from potentially untrusted vendors can be a serious threat to the trusted IPs when they are connected using the same Network-on-Chip (NoC). For example, the malicious IPs can tamper packets as well as degrade SoC performance by launching DoS attacks. While existing authentication schemes can check the data integrity of packets, it can introduce unacceptable overhead on resource-constrained SoCs. In this paper, we propose a lightweight and trust-aware routing mechanism to bypass malicious IPs during packet transfers. This reduces the number of re-transmissions due to tampered data, minimizes DoS attack risk, and as a result, improves SoC performance even in the presence of\u00a0\u2026", "num_citations": "20\n", "authors": ["1805"]}
{"title": "Efficient cache reconfiguration using machine learning in noc-based many-core cmps\n", "abstract": " Dynamic cache reconfiguration (DCR) is an effective technique to optimize energy consumption in many-core architectures. While early work on DCR has shown promising energy saving opportunities, prior techniques are not suitable for many-core architectures since they do not consider the interactions and tight coupling between memory, caches, and network-on-chip (NoC) traffic. In this article, we propose an efficient cache reconfiguration framework in NoC-based many-core architectures. The proposed work makes three major contributions. First, we model a distributed directory based many-core architecture similar to Intel Xeon Phi architecture. Next, we propose an efficient cache reconfiguration framework that considers all significant components, including NoC, caches, and main memory. Finally, we propose a machine learning--based framework that can reduce the exploration time by an order of\u00a0\u2026", "num_citations": "20\n", "authors": ["1805"]}
{"title": "Cost-effective analysis of post-silicon functional coverage events\n", "abstract": " Post-silicon validation is a major challenge due to the combined effects of debug complexity and observability constraints. Assertions as well as a wide variety of checkers are used in pre-silicon stage to monitor certain functional scenarios. Pre-silicon checkers can be synthesized to coverage monitors in order to capture the coverage of certain events and improve the observability during post-silicon debug. Synthesizing thousands of coverage monitors can introduce unacceptable area and energy overhead. On the other hand, absence of coverage monitors would negatively impact post-silicon coverage analysis. In this paper, we propose a framework for cost-effective post-silicon coverage analysis by identifying hard-to-detect events coupled with trace-based coverage analysis. This paper makes three major contributions. We propose a method to utilize existing debug infrastructure to enable coverage analysis in\u00a0\u2026", "num_citations": "20\n", "authors": ["1805"]}
{"title": "Lightweight anonymous routing in noc based socs\n", "abstract": " System-on-Chip (SoC) supply chain is widely acknowledged as a major source of security vulnerabilities. Potentially malicious third-party IPs integrated on the same Network-on-Chip (NoC) with the trusted components can lead to security and trust concerns. While secure communication is a well studied problem in computer networks domain, it is not feasible to implement those solutions on resource-constrained SoCs. In this paper, we present a lightweight anonymous routing protocol for communication between IP cores in NoC based SoCs. Our method eliminates the major overhead associated with traditional anonymous routing protocols while ensuring that the desired security goals are met. Experimental results demonstrate that existing security solutions on NoC can introduce significant (1.5X) performance degradation, whereas our approach provides the same security features with minor (4%) impact on\u00a0\u2026", "num_citations": "19\n", "authors": ["1805"]}
{"title": "Trace buffer attack: Security versus observability study in post-silicon debug\n", "abstract": " Since the standardization of AES/Rijndael symmetric-key cipher by NIST in 2001, it gained widespread acceptance in various protocols and withstood intense scrutiny from the theoretical cryptanalysts. From the physical implementation point of view, however, AES remained vulnerable. Practical attacks on AES via fault injection, differential power analysis, scan-chain and cache-access timing have been demonstrated so far. Along this line, in this paper, we propose a novel and effective attack, termed Trace Buffer Attack. Trace buffers are extensively used for post-silicon debug of digital designs. We identify this as a source of information leakage and show that, unless proper countermeasure is taken, Trace Buffer Attack is capable of partially recovering the secret keys of different AES implementations. We report the detailed process of trace-buffer attack with experimental results. We also propose a countermeasure\u00a0\u2026", "num_citations": "19\n", "authors": ["1805"]}
{"title": "Decision ordering based property decomposition for functional test generation\n", "abstract": " SAT-based BMC is promising for directed test generation since it can locate the reason of an error within a small bound. However, due to the state space explosion problem, BMC cannot handle complex designs and properties. Although various optimization methods are proposed to address a single complex property, the test generation process cannot be fully automated. This paper presents an efficient automated approach that can scale down the falsification complexity using property decomposition and learning techniques. Our experimental results using both software and hardware benchmarks demonstrate that our approach can drastically reduce the overall test generation effort.", "num_citations": "19\n", "authors": ["1805"]}
{"title": "FSM anomaly detection using formal analysis\n", "abstract": " Finite state machines (FSMs) control the functionality of the overall design. Any deviation from the specified FSM behavior can endanger the trustworthiness of the design. This is a critical concern when an FSM is responsible for controlling the usage or propagation of protected information (e.g. secret keys) in a secure component. FSM vulnerabilities can be created by a rogue designer or an attacker by inserting hardware Trojans in the FSM implementation. The vulnerability can also be introduced unintentionally by a CAD tool (e.g., when a synthesis tool is trying to optimize a gate-level netlist). In this paper, we present an efficient formal analysis framework based on symbolic algebra to find FSM vulnerabilities. The proposed method tries to find inconsistencies between the specification and FSM implementation through manipulation of respective polynomials. Security properties (such as a safe transition to a\u00a0\u2026", "num_citations": "18\n", "authors": ["1805"]}
{"title": "Dynamic cache tuning for efficient memory based computing in multicore architectures\n", "abstract": " Memory-based computing (MBC) is a promising approach to improve overall system reliability when few functional units are defective or unreliable under process-induced or thermal variations. A major challenge in using MBC for reliability improvement is that it can introduce significant energy and performance overhead. In this paper, we present an efficient dynamic cache reconfiguration and partitioning technique to improve performance and energy efficiency in MBC-enabled reliable multicore systems. We use genetic algorithm to search effectively in a large and complex design space. Experimental results demonstrate that the proposed cache reconfiguration and partitioning approach can significantly improve both performance and energy efficiency for on-demand memory based computing without sacrificing reliability.", "num_citations": "18\n", "authors": ["1805"]}
{"title": "Synchronized generation of directed tests using satisfiability solving\n", "abstract": " Directed test generation is important for the functional verification of complex system-on-chip designs. SAT based bounded model checking is promising for counter example generation which can be used in directed testing. Existing research has explored two directions to accelerate the SAT solving process: learning during solving of one property with different bounds, or solving multiple properties with known bounds. This paper combines the advantages of both approaches by introducing a novel SAT-solving technique which exploits the similarities among SAT instances for multiple properties and bounds on the same design. The proposed technique ensures that the knowledge obtained in previous solving iterations be shared across different bounds as well as between different properties. Our experimental results demonstrate that our approach can significantly reduce overall test generation time (on average 10\u00a0\u2026", "num_citations": "18\n", "authors": ["1805"]}
{"title": "A top-down methodology for validation of microprocessors\n", "abstract": " Validation of microprocessors is one of the most complex and expensive tasks in the current System-on-Chip (SOC) design flow. A significant bottleneck in the validation of such systems is the lack of a golden reference model. Thus, many existing approaches employ a bottom-up validation methodology by using a combination of simulation techniques and formal methods. We present a top-down validation approach using an Architecture Description Language (ADL) based specification of the microprocessors. The reference model of the microprocessor is generated from the ADL specification. An important aspect of our methodology is the ability to perform both model (property) checking and equivalence checking depending on the generated reference model. We use a symbolic simulator to perform property checking, and a commercial equivalence checker to perform equivalence checking using the generated\u00a0\u2026", "num_citations": "18\n", "authors": ["1805"]}
{"title": "Automated test generation for activation of assertions in RTL models\n", "abstract": " A major challenge in assertion-based validation is how to activate the assertions to ensure that they are valid. While existing test generation using model checking is promising, it cannot generate directed tests for large designs due to state space explosion. We propose an automated and scalable mechanism to generate directed tests using a combination of symbolic execution and concrete simulation of RTL models. Experimental results show that the directed tests are able to activate assertions non-vacuously.", "num_citations": "17\n", "authors": ["1805"]}
{"title": "Feature-based signal selection for post-silicon debug using machine learning\n", "abstract": " A key challenge of post-silicon validation methodology is to select a limited number of trace signals that are effective during post-silicon debug. Structural analysis used by traditional signal selection techniques are fast but lead to poor restoration quality. In contrast, simulation-based selection techniques provide superior restorability but incur significant computation overhead. While early work on machine learning based signal selection is promising [1] , it is still not applicable on large industrial designs since it needs thousands of simulations of large and complex designs. In this paper, we propose a signal selection technique that addresses the scalability issue of simulation-based techniques while maintaining a high restoration performance. The basic idea is to train a machine learning framework using a small set of circuits, and apply the trained model to the bigger circuit under test, without any need for simulating\u00a0\u2026", "num_citations": "17\n", "authors": ["1805"]}
{"title": "Variation-aware evaluation of MPSoC task allocation and scheduling strategies using statistical model checking\n", "abstract": " To maximize the overall performance yield, variation-aware analysis is becoming a key step in Multiprocessor System-on-Chip (MP-SoC) Task Allocation and Scheduling (TAS). Although various approaches have been investigated to improve performance yields, most of them cannot perform quantitative comparison among existing TAS heuristics, which is important for MPSoC designers to make decisions. Based on the statistical model checker UPPAAL-SMC, we propose a framework that can automatically evaluate the performance yield of TAS strategies under time and power constraints with variations. Experimental results show that our approach can not only filter inferior strategies efficiently, but also support the automated tuning of architecture and constraint parameters to achieve the required performance yield.", "num_citations": "17\n", "authors": ["1805"]}
{"title": "Towards RTL test generation from SystemC TLM specifications\n", "abstract": " SystemC transaction level modeling (TLM) is widely used to reduce the overall design and validation effort of complex system-on-chip (SOC) architectures. Due to lack of efficient techniques, the amount of reuse between abstraction levels is limited in many scenarios such as reuse of TLM level tests for RTL validation. This paper presents a top-down methodology for generation of RTL tests from SystemC TLM specifications. This paper makes two important contributions: automatic test generation from TLM specification using a transition-based coverage metric and automatic translation of TLM tests into RTL tests using a set of transformation rules. Our initial results using a router design demonstrate the usefulness of our approach by capturing various functional errors as well as inconsistencies in the implementation.", "num_citations": "17\n", "authors": ["1805"]}
{"title": "Reconfigurable network-on-chip security architecture\n", "abstract": " Growth of the Internet-of-things has led to complex system-on-chips (SoCs) being used in the edge devices in IoT applications. The increased complexity is demanding designers to consider several critical factors, such as dynamic requirement changes, long application life, mass production, and tight time-to-market deadlines. These requirements lead to more complex security concerns. SoC manufacturers outsource some of the intellectual property cores integrated on the SoC to untrusted third-party vendors. The untrusted intellectual properties can contain malicious implants, which can launch attacks using the resources provided by the on-chip interconnection network, commonly known as the network-on-chip (NoC). Existing efforts on securing NoC have considered lightweight encryption, authentication, and other attack detection mechanisms such as denial-of-service and buffer overflows. Unfortunately, these\u00a0\u2026", "num_citations": "16\n", "authors": ["1805"]}
{"title": "A novel test-data compression technique using application-aware bitmask and dictionary selection methods\n", "abstract": " Higher circuit densities in System-on-Chip (SOC) designs have led to enhancement in the test data volume. Larger test data size demands not only greater memory requirements, but also an increase in the testing time. Test data compression addresses this problem by reducing the test data volume without affecting the overall system performance. This paper proposes a novel test data compression technique using bitmasks which provides a significant enhancement in the compression efficiency without introducing any additional decompression penalty. The major contributions of this paper are as follows: i) it develops an efficient bitmask selection technique for test data in order to create maximum matching patterns; ii) it develops an efficient dictionary selection method which takes into account the speculated results of compressed codes and iii) it proposes a suitable code compression technique using dictionary\u00a0\u2026", "num_citations": "16\n", "authors": ["1805"]}
{"title": "Automated test generation for hardware trojan detection using reinforcement learning\n", "abstract": " Due to globalized semiconductor supply chain, there is an increasing risk of exposing System-on-Chip (SoC) designs to malicious implants, popularly known as hardware Trojans. Unfortunately, traditional simulation-based validation using millions of test vectors is unsuitable for detecting stealthy Trojans with extremely rare trigger conditions due to exponential input space complexity of modern SoCs. There is a critical need to develop efficient Trojan detection techniques to ensure trustworthy SoCs. While there are promising test generation approaches, they have serious limitations in terms of scalability and detection accuracy. In this paper, we propose a novel logic testing approach for Trojan detection using an effective combination of testability analysis and reinforcement learning. Specifically, this paper makes three important contributions. 1) Unlike existing approaches, we utilize both controllability and\u00a0\u2026", "num_citations": "15\n", "authors": ["1805"]}
{"title": "Scalable activation of rare triggers in hardware Trojans by repeated maximal clique sampling\n", "abstract": " Hardware Trojans are serious threat to security and reliability of computing systems. It is hard to detect these malicious implants using traditional validation methods since an adversary is likely to hide them under rare trigger conditions. While existing statistical test generation methods are promising for Trojan detection, they are not suitable for activating extremely rare trigger conditions in stealthy Trojans. To address the fundamental challenge of activating rare triggers, we propose a new test generation paradigm for Trigger Activation by Repeated Maximal Clique sampling (TARMAC). The basic idea is to utilize a satisfiability modulo theories (SMT) solver to construct a test corresponding to each maximal clique. This paper makes three fundamental contributions: (1) it proves that the trigger activation problem can be mapped to clique cover problem, and the test vectors generated by covering maximal cliques are\u00a0\u2026", "num_citations": "15\n", "authors": ["1805"]}
{"title": "Automated trigger activation by repeated maximal clique sampling\n", "abstract": " Hardware Trojans are serious threat to security and reliability of computing systems. It is hard to detect these malicious implants using traditional validation methods since an adversary is likely to hide them under rare trigger conditions. While existing statistical test generation methods are promising for Trojan detection, they are not suitable for activating extremely rare trigger conditions in stealthy Trojans. To address the fundamental challenge of activating rare triggers, we propose a new test generation paradigm by mapping trigger activation problem to clique cover problem. The basic idea is to utilize a satisfiability solver to construct a test corresponding to each maximal clique. This paper makes two fundamental contributions: 1) it proves that the trigger activation problem can be mapped to clique cover problem, 2) it proposes an efficient test generation algorithm to activate trigger conditions by repeated maximal\u00a0\u2026", "num_citations": "15\n", "authors": ["1805"]}
{"title": "Vulnerability-aware energy optimization for reconfigurable caches in multitasking systems\n", "abstract": " Cache vulnerability due to soft errors is one of the reliability concerns in embedded systems. Dynamic reconfiguration techniques are widely studied for improving cache energy without considering the implications of cache vulnerability. Maintaining a useful data longer in the cache can be beneficial for energy improvement due to reduction in miss rates, however, longer data retention negatively impacts the vulnerability due to soft errors. This paper studies the tradeoff between energy efficiency improvement and reduction in cache vulnerability during cache reconfiguration. We propose a heuristic approach for both intertask and intratask cache reconfiguration in multitasking systems. Experimental results demonstrate that our proposed approach can significantly improve both vulnerability (25% on average) and energy efficiency (21% on average) for data cache without violating real-time constraints.", "num_citations": "15\n", "authors": ["1805"]}
{"title": "Cache reconfiguration using machine learning for vulnerability-aware energy optimization\n", "abstract": " Dynamic cache reconfiguration has been widely explored for energy optimization and performance improvement for single-core systems. Cache partitioning techniques are introduced for the shared cache in multicore systems to alleviate inter-core interference. While these techniques focus only on performance and energy, they ignore vulnerability due to soft errors. In this article, we present a static profiling based algorithm to enable vulnerability-aware energy-optimization for real-time multicore systems. Our approach can efficiently search the space of cache configurations and partitioning schemes for energy optimization while task deadlines and vulnerability constraints are satisfied. A machine learning technique has been employed to minimize the static profiling time without sacrificing the accuracy of results. Our experimental results demonstrate that our approach can achieve 19.2% average energy savings\u00a0\u2026", "num_citations": "15\n", "authors": ["1805"]}
{"title": "Reliability and energy-aware cache reconfiguration for embedded systems\n", "abstract": " Cache vulnerability due to soft errors is one of the reliability concerns in embedded systems. Dynamic reconfiguration techniques are widely studied for improving cache energy without considering the implications of cache vulnerability. Maintaining a useful data longer in the cache can be beneficial for energy improvement due to reduction in miss rates, however, longer data retention negatively impacts the vulnerability due to soft errors. This paper studies the trade-off between energy efficiency improvement and reduction in cache vulnerability during cache reconfiguration. We propose two heuristic approaches for reliability- and energy-aware dynamic cache reconfiguration. Experimental results demonstrate that our proposed approaches can provide drastic reduction in cache vulnerability with minor impact on energy and performance.", "num_citations": "15\n", "authors": ["1805"]}
{"title": "Dynamic reconfiguration of two-level cache hierarchy in real-time embedded systems\n", "abstract": " System optimization techniques based on efficient dynamic reconfiguration have been widely adopted in recent years. Cache reconfiguration is a promising optimization technique for reducing memory hierarchy energy consumption with little or no impact on overall system performance. While cache reconfiguration is successful in desktop-based and embedded systems, it is not directly applicable in real-time systems due to timing constraints. Existing scheduling-aware cache reconfiguration techniques consider only one-level cache. It is a major challenge to dynamically tune multi-level caches since the exploration space is prohibitively large. This paper efficiently integrates cache reconfiguration in real-time systems with a unified two-level cache hierarchy. We propose a set of exploration heuristics for our static analysis which effectively reduces the exploration time while keeps the generated profile results\u00a0\u2026", "num_citations": "14\n", "authors": ["1805"]}
{"title": "A general algorithm for energy-aware dynamic reconfiguration in multitasking systems\n", "abstract": " System optimization techniques based on dynamic reconfiguration are widely adopted for energy conservation. While dynamic voltage scaling (DVS) techniques have been extensively studied for processor energy conservation, dynamic cache reconfiguration (DCR) for reducing cache energy consumption in multitasking systems is still in its infancy. In this paper, we propose a general and flexible algorithm for energy optimization based on dynamic reconfiguration in multitasking systems. Our algorithm is flexibly parameterized and can be used to provide tradeoffs between running time and solution quality. Furthermore, it can easily incorporate variable reconfiguration overhead. Experimental results show that our technique can generate near-optimal solutions with significantly low running time and memory requirements.", "num_citations": "14\n", "authors": ["1805"]}
{"title": "Temperature-and energy-constrained scheduling in multitasking systems: A model checking approach\n", "abstract": " The ongoing scaling of semiconductor technology is causing severe increase of on-chip power density and temperature in microprocessors. This has raised urgent requirement for both power and thermal management during each level of system design. In this paper, we propose a formal technique based on model checking using extended timed automata to solve the processor frequency assignment problem in a temperature- and energy-constrained multitasking system. The state space explosion problem is alleviated by transforming and solving a Pseudo-Boolean satisfiability problem. Our approach is capable of finding efficient solutions under various constraints and applicable to other problem variants as well. Our method is independent of any system and task characteristics. Experimental results demonstrate the usefulness of our approach.", "num_citations": "14\n", "authors": ["1805"]}
{"title": "Exploiting transaction level models for observability-aware post-silicon test generation\n", "abstract": " A major challenge in post-silicon debug is to generate efficient tests that activate requisite coverage goals on the target hardware while also producing results that are observable through a given on-chip design-for-debug (DfD) architecture. Unfortunately, such tests cannot be generated by analysis of RTL models, both because of design complexity and since the implementation can be buggy. In this paper, we propose an approach to address this problem by exploiting transaction-level models (TLM). Our approach involves mapping tests and observability requirements between TLM and RTL, enabling TLM analysis to generate post-silicon tests. We provide two case studies to demonstrate the flexibility and effectiveness of our proposed approach.", "num_citations": "13\n", "authors": ["1805"]}
{"title": "TCEC: Temperature and energy-constrained scheduling in real-time multitasking systems\n", "abstract": " The ongoing scaling of semiconductor technology is causing severe increase of on-chip power density and temperature in microprocessors. This urgently requires both power and thermal management during system design. In this paper, we propose a model checking-based technique using extended timed automata to solve the processor frequency assignment problem in a temperature and energy-constrained multitasking system. We also develop a polynomial time-approximation algorithm to address the state-space explosion problem caused by symbolic model checker. Our approximation scheme is guaranteed to not generate any false-positive answer, while it may return false-negative answer in rare cases. Our method is universally applicable since it is independent of any system and task characteristics. Experimental results demonstrate the usefulness of our approach.", "num_citations": "13\n", "authors": ["1805"]}
{"title": "Energy-aware dynamic reconfiguration algorithms for real-time multitasking systems\n", "abstract": " System optimization techniques based on dynamic reconfiguration are widely adopted for energy conservation. While dynamic voltage scaling (DVS) techniques have been extensively studied for processor energy conservation, dynamic cache reconfiguration (DCR) for reducing cache energy consumption in multitasking systems is still in its infancy. In this paper, we propose a general and flexible algorithm for energy optimization based on dynamic reconfiguration in multitasking systems. Our algorithm is flexibly parameterized and can be used to provide tradeoffs between running time and solution quality. Furthermore, it can easily incorporate variable reconfiguration overhead. Experimental results show that our technique can generate near-optimal solutions with significantly low running time and memory requirements.", "num_citations": "13\n", "authors": ["1805"]}
{"title": "Lossless Compression Using Efficient Encoding of Bitmasks\n", "abstract": " Lossless compression is widely used to improve both memory requirement and communication bandwidth in embedded systems. Dictionary based compression techniques are very popular because of their good compression efficiency and fast decompression mechanism. Bitmask based compression improves the effectiveness of the dictionary based approaches by recording minor differences using bitmasks. This paper proposes an efficient encoding of bitmasks used in bitmask-based compression. We prove that a n-bit bitmask (records n differences) can be encoded using only n-1 bits. This encoding improves compression efficiency while reduces decompression hardware overhead. We have applied our approach in a wide a variety of domains including code compression, FPGA bitstream compression as well as control word compression. Our experimental results using a wide variety of benchmarks\u00a0\u2026", "num_citations": "13\n", "authors": ["1805"]}
{"title": "Specification-based compaction of directed tests for functional validation of pipelined processors\n", "abstract": " Functional validation is a major bottleneck in microprocessor design methodology. Simulation is the widely used method for functional validation using billions of random and biased-random test programs. Although directed tests require a smaller test set compared to random tests to achieve the same functional coverage goal, there is a lack of automated techniques for directed test generation. Furthermore, the number of directed tests can still be prohibitively large. This paper presents a methodology for specification-based coverage analysis and test generation. The primary contribution of this paper is a compaction technique that can drastically reduce the required number of directed test programs to achieve a coverage goal. Our experimental results using a MIPS processor and an industrial processor (e500) demonstrate more than 90% reduction in number of directed tests without sacrificing the functional\u00a0\u2026", "num_citations": "13\n", "authors": ["1805"]}
{"title": "Algorithm, Design\n", "abstract": " The ongoing scaling of semiconductor technology is causing se-vere increase of on-chip power density and temperature in micro-processors. This has raised urgent requirement for both power and thermal management during each level of system design. In this paper, we propose a formal technique based on model checking us-ing extended timed automata to solve the processor frequency as-signment problem in a temperature-and energy-constrained multi-tasking system. The state space explosion problem is alleviated by transforming and solving a Pseudo-Boolean satisfiability problem. Our approach is capable of finding efficient solutions under vari-ous constraints and applicable to other problem variants as well. Our method is independent of any system and task characteristics. Experimental results demonstrate the usefulness of our approach.", "num_citations": "13\n", "authors": ["1805"]}
{"title": "Towards automatic validation of dynamic behavior in pipelined processor specifications\n", "abstract": " As embedded systems continue to face increasingly higher performance requirements, deeply pipelined processor architectures are being employed to meet desired system performance. A significant bottleneck in the validation of such systems is the lack of a golden reference model. Thus, many existing techniques employ a bottom-up approach to architecture validation, where the functionality of an existing pipelined architecture is, in essence, reverse-engineered from its implementation. Our validation technique is complementary to these bottom-up approaches. Our approach leverages the system architect's knowledge about the behavior of the pipelined architecture, through Architecture Description Language (ADL) constructs, and thus allows a powerful top\u2013down approach to architecture validation. The most important requirement in top\u2013down validation process is to ensure that the specification\u00a0\u2026", "num_citations": "13\n", "authors": ["1805"]}
{"title": "Automatic validation of pipeline specifications\n", "abstract": " Recent approaches on language-driven Design Space Exploration (DSE) use Architectural Description Languages (ADL) to capture the processor architecture, generate automatically a software toolkit (including compiler, simulator, and assembler) for that processor, and provide feedback to the designer on the quality of the architecture. It is important to verify the ADL description of the processor to ensure the correctness of the software toolkit. We present in this paper an automatic validation framework, driven by an ADL. We present algorithms for automatic validation of ADL specification of the processor pipelines. We applied our methodology to verify several realistic processor cores to demonstrate the usefulness of our approach.", "num_citations": "13\n", "authors": ["1805"]}
{"title": "Architecture description language driven design space exploration in the presence of coprocessors\n", "abstract": " Embedded systems present a tremendous opportunity to customize designs by exploiting the application behavior. Shrinking time-tomarket, coupled with short product lifetimes create a critical need for rapid exploration and evaluation of candidate System-on-Chip SOC architectures. Recent work on language driven Design Space Exploration DSE uses Architecture Description Languages ADL to capture the processor-memory architecture and automatically generate a software toolkit from the ADL description. We propose in this paper an ADL-based approach to explicitly capture the coprocessor configuration, and perform exploration of the coprocessor architecture along with processor and memory subsystem. We present a set of experiments using the TI C6x architecture to demonstrate the usefulness of our approach.", "num_citations": "13\n", "authors": ["1805"]}
{"title": "Proactive thermal management using memory-based computing in multicore architectures\n", "abstract": " Reliability is a major concern in modern electronic systems due to high defect rates and large parametric variations. A major contributor to reliability concerns is the potential thermal violations due to increasing transistor count coupled with the high clock rate in multicore System-on-Chip (SoC) designs. Dynamic thermal management is widely used to reduce the SoC temperature. Early work on using memory-based computing has shown promising results in improving SoC reliability when few functional units are defective or unreliable under process-induced or thermal variations. However, there are no prior efforts to explore the effectiveness of MBC for thermal management in multicore architectures. In this paper, we present a novel dynamic thermal management technique using proactive memory-based computing to reduce the peak temperature of applications in multicore architectures. The basic idea is to\u00a0\u2026", "num_citations": "12\n", "authors": ["1805"]}
{"title": "Electronic design automation for IC system design, verification, and testing\n", "abstract": " The first of two volumes in the Electronic Design Automation for Integrated Circuits Handbook, Second Edition, Electronic Design Automation for IC System Design, Verification, and Testing thoroughly examines system-level design, microarchitectural design, logic verification, and testing. Chapters contributed by leading experts authoritatively discuss processor modeling and design tools, using performance metrics to select microprocessor cores for integrated circuit (IC) designs, design and verification languages, digital simulation, hardware acceleration and emulation, and much more. New to This Edition: Major updates appearing in the initial phases of the design flow, where the level of abstraction keeps rising to support more functionality with lower non-recurring engineering (NRE) costs Significant revisions reflected in the final phases of the design flow, where the complexity due to smaller and smaller geometries is compounded by the slow progress of shorter wavelength lithography New coverage of cutting-edge applications and approaches realized in the decade since publication of the previous edition\u2014these are illustrated by new chapters on high-level synthesis, system-on-chip (SoC) block-based design, and back-annotating system-level models Offering improved depth and modernity, Electronic Design Automation for IC System Design, Verification, and Testing provides a valuable, state-of-the-art reference for electronic design automation (EDA) students, researchers, and professionals.", "num_citations": "12\n", "authors": ["1805"]}
{"title": "Trace buffer attack on the AES cipher\n", "abstract": " Since the standardization of AES/Rijndael symmetric-key cipher by NIST in 2001, it gained widespread acceptance in various protocols and withstood intense scrutiny from the theoretical cryptanalysts. From the physical implementation point of view, however, AES remained vulnerable. Practical attacks on AES via fault injection, differential power analysis, scan-chain and cache-access timing have been demonstrated so far. In this paper, we propose a novel and effective attack, termed Trace Buffer Attack. Trace buffers are extensively used for post-silicon debug of integrated circuits. We identify the trace buffer as a source of information leakage. We first report the detailed process of trace buffer attack assuming that the register-transfer level (RTL) implementation is available. We further analyze the AES encryption algorithm and Rijndael\u2019s key expansion algorithm, and illustrate that trace buffer attack is\u00a0\u2026", "num_citations": "12\n", "authors": ["1805"]}
{"title": "Content-aware encoding for improving energy efficiency in multi-level cell resistive random access memory\n", "abstract": " Memory is an integral and important component of both general-purpose and embedded systems. It is widely acknowledged that energy of the memory structure is a major contributor in overall system energy. Recent advances with emerging non-volatile memory (NVM) technologies can potentially alleviate the issue of memory leakage power. However, they introduce new challenges and opportunities for dynamic power management in memory. In this paper, we consider resistive random access memory (RRAM), a promising NVM technology, and observe that a specific feature of the memory, namely, its multi-level cell (MLC) structure, can be used to significantly reduce its read access energy. Unlike conventional CMOS static random access memory (SRAM), the read access energy in RRAM largely depend on the stored content. Based on this observation, we present an efficient encoding technique for improving\u00a0\u2026", "num_citations": "12\n", "authors": ["1805"]}
{"title": "Synergistic integration of dynamic cache reconfiguration and code compression in embedded systems\n", "abstract": " Optimization techniques are widely used in embedded systems design to improve overall area, performance and energy requirements. Dynamic cache reconfiguration is very effective to reduce energy consumption of cache subsystems which accounts for about half of the total energy consumption in embedded systems. Various studies have shown that code compression can significantly reduce memory requirements, and may improve performance in many scenarios. In this paper, we study the challenges and associated opportunities in integrating dynamic cache reconfiguration with code compression to retain the advantages of both approaches. Experimental results demonstrate that synergistic combination of cache reconfiguration and code compression can significantly reduce both energy consumption (65% on average) and memory requirements while drastically improve the overall performance (up to 75\u00a0\u2026", "num_citations": "12\n", "authors": ["1805"]}
{"title": "Efficient directed test generation for validation of multicore architectures\n", "abstract": " Functional verification of multicore architectures is widely acknowledged as a major challenge. Directed tests are promising since a significantly smaller number of directed tests can achieve the same coverage goal compared to constrained-random tests. SAT-based bounded model checking is effective for automated generation of directed tests (counterexamples). While existing approaches focus on clause forwarding between different bounds to reduce the test generation time, this paper proposes a novel technique that exploits the structural similarity within the same bound as well as between different bounds. Our proposed technique enables the reuse of the knowledge learned from one core to the remaining cores in multicore architectures. The experimental results demonstrate that our approach can significantly (2-10 times) reduce overall test generation time compared to existing approaches.", "num_citations": "12\n", "authors": ["1805"]}
{"title": "A universal placement technique of compressed instructions for efficient parallel decompression\n", "abstract": " Instruction compression is important in embedded system design since it reduces the code size (memory requirement) and thereby improves the overall area, power, and performance. Existing research in this field has explored two directions: efficient compression with slow decompression, or fast decompression at the cost of compression efficiency. This paper combines the advantages of both approaches by introducing a novel bitstream placement method. Our contribution in this paper is a novel compressed bitstream placement technique to support parallel decompression without sacrificing the compression efficiency. The proposed technique enables splitting a single bitstream (instruction binary) fetched from memory into multiple bitstreams, which are then fed into different decoders. As a result, multiple slow decoders can simultaneously work to produce the effect of high decode bandwidth. We prove that our\u00a0\u2026", "num_citations": "12\n", "authors": ["1805"]}
{"title": "Restructuring field layouts for embedded memory systems\n", "abstract": " In many computer systems with large data computations, the delay of memory access is one of the major performance bottlenecks. In this paper, we propose an enhanced field remapping scheme for dynamically allocated structures in order to provide better locality than conventional field layouts. Our proposed scheme reduces cache miss rates drastically by aggregating and grouping fields from multiple instances of the same structure, which implies the performance improvement and power reduction. Our methodology will become more important in the design space exploration, especially as the embedded systems for data oriented application become prevalent. Experimental results show that average L1 and L2 data cache misses are reduced by 23% and 17%, respectively. Due to the enhanced localities, our remapping achieves 13% faster execution time on average than original programs. It also reduces\u00a0\u2026", "num_citations": "12\n", "authors": ["1805"]}
{"title": "Test generation using reinforcement learning for delay-based side-channel analysis\n", "abstract": " Reliability and trustworthiness are dominant factors in designing System-on-Chips (SoCs) for a variety of applications. Malicious implants, such as hardware Trojans, can lead to undesired information leakage or system malfunction. To ensure trustworthy computing, it is critical to develop efficient Trojan detection techniques. While existing delay-based side-channel analysis is promising, it is not effective due to two fundamental limitations: (i) The difference in path delay between the golden design and Trojan inserted design is negligible compared with environmental noise and process variations. (ii) Existing approaches rely on manually crafted rules for test generation, and require a large number of simulations, making it impractical for industrial designs. In this paper, we propose a novel test generation method using reinforcement learning for delay-based Trojan detection. This paper makes three important\u00a0\u2026", "num_citations": "11\n", "authors": ["1805"]}
{"title": "Scalable concolic testing of RTL models\n", "abstract": " Simulation is widely used for validation of Register-Transfer-Level (RTL) models. While simulating with millions of random or constrained-random tests can cover majority of the functional scenarios, the number of remaining scenarios can still be huge (hundreds or thousands) in case of today's industrial designs. Hard-to-activate branches are one of the major contributors for such remaining/untested scenarios. While directed test generation techniques using formal methods are promising in activating branches, it is infeasible to apply them on large designs due to state space explosion. In this paper, we propose a fully automated and scalable approach to cover the hard-to-activate branches using concolic testing of RTL models. While application of concolic testing on hardware designs has shown some promising results in improving the overall coverage, they are not designed to activate specific targets such as\u00a0\u2026", "num_citations": "11\n", "authors": ["1805"]}
{"title": "Automated test generation for Trojan detection using delay-based side channel analysis\n", "abstract": " Side-channel analysis is widely used for hardware Trojan detection in integrated circuits by analyzing various side-channel signatures, such as timing, power and path delay. Existing delay-based side-channel analysis techniques have two major bottlenecks: (i) they are not suitable in detecting Trojans since the delay difference between the golden design and a Trojan inserted design is negligible, and (ii) they are not effective in creating robust delay signatures due to reliance on random and ATPG based test patterns. In this paper, we propose an efficient test generation technique to detect Trojans using delay-based side channel analysis. This paper makes two important contributions. (1) We propose an automated test generation algorithm to produce test patterns that are likely to activate trigger conditions, and change critical paths. Compared to existing approaches where delay difference is solely based on extra\u00a0\u2026", "num_citations": "11\n", "authors": ["1805"]}
{"title": "Temperature-aware task partitioning for real-time scheduling in embedded systems\n", "abstract": " Both power and heat density of on-chip systems are in- creasing exponentially with Moore's Law. High temperature negatively affects reliability as well the costs of cooling and packaging. In this paper, we propose task partitioning as an effective way to reduce the peak temperature in embedded systems running either a set of periodic heterogeneous tasks with common period or periodic heterogeneous tasks with individual period. For task sets with common period, experimental results show that our task partitioning algorithms is able to reduce the peak temperature by as much as 5.8\u00b0C as compared to algorithms that only use task sequencing. For task sets with individual period, EDF scheduling with task partitioning can also lower the peak temperature, as compared to simple EDF scheduling, by as much as 6\u00b0C. Our analysis indicates that the numbers of additional context switches (overhead) is less than 2 per\u00a0\u2026", "num_citations": "11\n", "authors": ["1805"]}
{"title": "Rexsim: A Retargetable framework for instruction-set architecture simulation\n", "abstract": " Instruction-set simulators are an integral part of today\u2019s processor and software design process. Due to increasing complexity of the architectures and time-to-market pressure, performance and retargetability are the most important features of an instruction-set simulator. Dynamic behavior of applications and processors requires the ISA simulators to be flexible. Flexible interpretive simulators are slow while fast compiled simulators are not flexible enough. Retargetability and flexibility require generic models while high performance demands target specific customizations. To address these contradictory requirements, we propose a generic model as well as an efficient and flexible implementation technique. The contribution of this paper is a simulation framework that is retargetable, fast and flexible. We have developed a generic instruction model and a generic decode algorithm to generate retargetable simulators that supports wide spectrum of processor architectures including RISC, DSP, VLIW and Superscalar. We have also developed the Instruction-Set Compiled Simulation (IS-CS) technique that combines the performance of compiled simulation with the flexibility of interpretive simulation. The generated simulator delivers up to 46% performance improvement over JIT-CCS [2], the best known result in this category. We illustrate the applicability of our approach using two different state-of-theart real world architectures: the Sparc and the ARM.", "num_citations": "11\n", "authors": ["1805"]}
{"title": "Layout-aware selection of trace signals for post-silicon debug\n", "abstract": " Post-silicon debug is widely acknowledged as a bottleneck in SoC design methodology. A major challenge during post-silicon debug is the limited observability of internal signals. Existing approaches try to select a small set of beneficial trace signals that can maximize observability. Unfortunately, these techniques do not consider design constraints such as routability of the selected signals or routing congestion. Therefore, in reality, it may not be possible to route the selected signals. We propose a layout-aware signal selection algorithm that takes into account both observability and routing congestion. Our experimental results demonstrate that our proposed approach can select routing friendly trace signals with negligible impact on observability.", "num_citations": "10\n", "authors": ["1805"]}
{"title": "Observability-aware directed test generation for soft errors and crosstalk faults\n", "abstract": " Post-silicon validation has emerged as an important component of any chip design methodology to detect both functional and electrical errors that have escaped the pre-silicon validation phase. In order to detect these escaped errors, both controllability and observability factors should be considered. Soft errors and crosstalk faults are two important electrical faults that can adversely affect the correct functionality of the chip. A major bottleneck with the existing approaches is that they do not consider the inter-dependence of the selected trace signals and test generation. In this paper, we explore the synergy between trace signal selection and observability-aware test generation to enable efficient detection of electrical errors including soft errors and crosstalk faults. Our experimental results demonstrate that our approach can significantly improve error detection performance - on an average 58% for crosstalk faults and\u00a0\u2026", "num_citations": "10\n", "authors": ["1805"]}
{"title": "Introduction to architecture description languages\n", "abstract": " Publisher SummaryThis chapter focuses on the architecture description languages (ADLs). The phrase \u201carchitecture description language\u201d is used in the context of designing both software and hardware architectures. Software ADLs are used for representing and analyzing software architectures where they capture the behavioral specifications of the components and their interactions that comprise the software architecture. The hardware ADLs capture the structure (hardware components and their connectivity) and the behavior (instruction set) of processor architectures and these ADLs have been successfully used as a specification language for processor development. The ADL specification is used to generate various executable models such as simulator, compiler, and hardware implementation, and these models can enable various design automation tasks such as exploration, simulation, compilation\u00a0\u2026", "num_citations": "10\n", "authors": ["1805"]}
{"title": "Directed micro-architectural test generation for an industrial processor: A case study\n", "abstract": " Simulation-based validation of the current industrial processors typically use huge number of test programs generated at instruction set architecture (ISA) level. However, architectural test generation techniques have limitations in terms of exercising intricate micro-architectural artifacts. Therefore, it is necessary to use micro-architectural details during test generation. Furthermore, there is a lack of automated techniques for directed test generation targeting micro-architectural faults. To address these challenges, we present a directed test generation technique at micro-architectural level for functional validation of microprocessors. A processor model is described in a temporal specification language at micro-architecture level. The desired behaviors of micro-architecture mechanisms are expressed as temporal logic properties. We use decompositional model checking for systematic test generation. Our experiments\u00a0\u2026", "num_citations": "10\n", "authors": ["1805"]}
{"title": "Specification-driven validation of programmable embedded systems\n", "abstract": " Validation of programmable embedded systems, consisting of processor cores, coprocessors, and memory subsystems, is one of the major bottleneck in current System-on-Chip (SOC) design methodology. One of the most important problems in validation of such systems is the lack of a golden reference model. As a result, many existing validation techniques employ a bottom-up approach to design verification, where the functionality of an existing architecture is, in essence, reverse-engineered from its implementation. This thesis presents a top-down validation methodology that complements the existing bottom-up approaches. It leverages the system architect's knowledge about the behavior of the design through architecture specification. We have developed validation techniques to ensure that the static and dynamic behaviors of the specified architecture is well formed. The validated specification is used as a\u00a0\u2026", "num_citations": "10\n", "authors": ["1805"]}
{"title": "Architecture description language driven functional test program generation for microprocessors using SMV\n", "abstract": " Formal techniques offer an opportunity to significantly reduce the cost of microprocessor verification. We propose a model checking based approach to automatically generate functional test programs for pipelined processors. We specify the processor architecture in an Architecture Description Language (ADL). The processor model is extracted from the ADL specification. Specific properties are applied to the processor model using SMV model checker to generate test programs. We applied this methodology on a single-issue DLX processor to demonstrate the usefulness of our approach.", "num_citations": "10\n", "authors": ["1805"]}
{"title": "Functional abstraction of programmable embedded systems\n", "abstract": " Rapid Design Space Exploration (DSE) of a processor-memory architecture is feasible using automatic toolkit (compiler, simulator, assembler) generation methodology driven by an Architecture Description Language (ADL). While many contemporary ADLs can effectively capture one class of architecture, they are typically unable to capture a wide spectrum of architecture and memory features present in DSP, VLIW, EPIC and Superscalar processors. The main bottleneck has been the lack of a functional abstraction underlying the ADL covering a diverse set of heterogeneous architectures. We present in this report the functional abstraction needed to capture such wide variety of programmable embedded systems. We demonstrate the power of this approach by specifying two very different processor-memory architecture using functional abstraction approach. We outline the automatic software toolkit generation from the given ADL description using functional abstractions. We show initial results of rapid design space exploration of architectures specified using functional abstraction based ADL approach.", "num_citations": "10\n", "authors": ["1805"]}
{"title": "Efficient peak power estimation using probabilistic cost-benefit analysis\n", "abstract": " Estimation of peak power consumption is an essential task in order to design reliable systems. Optimistic design choices can make the circuit unreliable and vulnerable to power attacks, whereas pessimistic design can lead to unacceptable design overhead. The power virus problem is defined as finding input patterns that can maximize switching activity (dynamic power dissipation) in digital circuits. In this paper, we present a fast and simple to implement power virus generation technique utilizing a probabilistic cost-benefit analysis. To maximize switching activity, our proposed algorithm iteratively enables transitions in high fan-out gates while considering the trade-off between switching of new gates (benefit) and blocking of gate transitions in the future iterations (cost) due to switching of the currently selected one. Extensive experiments using both combinational and sequential benchmarks demonstrate that our\u00a0\u2026", "num_citations": "8\n", "authors": ["1805"]}
{"title": "Compression-aware dynamic cache reconfiguration for embedded systems\n", "abstract": " Optimization techniques are widely used in embedded systems design to improve overall area, performance and energy requirements. Dynamic cache reconfiguration is very effective to reduce energy consumption of cache subsystems which accounts for about half of the total energy consumption in embedded systems. Various studies have shown that code compression can significantly reduce memory requirements, and may improve performance in many scenarios. In this paper, we study the challenges and associated opportunities in integrating dynamic cache reconfiguration with code compression to retain the advantages of both approaches. We developed efficient heuristics to explore large space of two-level cache hierarchy in order to study the effect of a two-level cache on energy consumption. Experimental results demonstrate that synergistic combination of cache reconfiguration and code compression\u00a0\u2026", "num_citations": "8\n", "authors": ["1805"]}
{"title": "Bitmask-based control word compression for NISC architectures\n", "abstract": " Implementing a custom hardware is not always feasible due to cost and time considerations. No instruction set computer (NISC) architecture is one of the promising direction to design a custom datapath for each application using its execution characteristics. A major challenge with NISC control word is that they tend to be at least 4 to 5 times larger than regular instruction size, thereby imposing higher memory requirement. A promising approach is to compress these control words to reduce the code size of the application. This article proposes an efficient bitmask-based compression technique to drastically reduce the control word size while keeping the decompression overhead minimal. The main contributions of our approach are: i) efficient don't care resolution for maximum bitmask coverage using limited dictionary entries, ii) run length encoding to significantly reduce repetitive control words, and iii) smart\u00a0\u2026", "num_citations": "8\n", "authors": ["1805"]}
{"title": "Efficient placement of compressed code for parallel decompression\n", "abstract": " Code compression is important in embedded systems design since it reduces the code size (memory requirement) and thereby improves overall area, power and performance. Existing researches in this field have explored two directions: efficient compression with slow decompression, or fast decompression at the cost of compression efficiency. This paper combines the advantages of both approaches by introducing a novel bitstream placement method. The contribution of this paper is a novel code placement technique to enable parallel decompression without sacrificing the compression efficiency. The proposed technique splits a single bitstream (instruction binary) fetched from memory into multiple bitstreams, which are then fed into different decoders. As a result, multiple slow-decoders can work simultaneously to produce the effect of high decode bandwidth. Our experimental results demonstrate that our\u00a0\u2026", "num_citations": "8\n", "authors": ["1805"]}
{"title": "Memory subsystem description in EXPRESSION\n", "abstract": " Memory represents a major bottleneck in modern embedded systems. Traditionally, memory organizations for programmable systems assumed a fixed cache hierarchy. With the widening processormemory gap, more aggressive memory technologies and organizations have appeared, allowing customization of a heterogeneous memory architecture tuned for the application. However, such a processor-memory co-exploration approach critically needs the ability to explicitly capture heterogeneous memory architectures. We present in this report the mechanism for describing memory subsystems in EXPRESSION, an Architecture Description Language (ADL) for processor-memory systems. The memory subsystem for the retargetable simulator can be generated from the description automatically. We have demonstrated the technique by generating memory subsystems for C6x, R10K, Itanium and PowerPC architectures. We present a set of experiments using our memory aware ADL Language to drive the exploration of the memory subsystem for the TIC6211 processor architecture, demonstrating a range of cost and performance attributes.", "num_citations": "8\n", "authors": ["1805"]}
{"title": "Security validation in modern soc designs\n", "abstract": " Modern SoC designs include a large number of sensitive data and collateral that must be protected against unauthorized or malicious access. Unauthorized access can happen in the design/integration supply chain as well as on-field through exploitation of system and platform errors, physical access, malicious software execution, etc. Validation entails ensuring that the system is robust against all of these attacks, and even unanticipated ones. It is one of the most critical and time-consuming activities in the SoC design methodology. In this chapter, we provide a general overview of validation activities and challenges, discuss limitations in our current validation practices, and identify requirements and opportunities for research in this space.", "num_citations": "7\n", "authors": ["1805"]}
{"title": "Security and trust vulnerabilities in third-party IPs\n", "abstract": " Reusable hardware Intellectual Property (IP)-based System-on-Chip (SoC) design has emerged as a pervasive design practice in the industry to dramatically reduce design and verification cost while meeting aggressive time-to-market constraints. Growing reliance on these pre-verified hardware IPs, often gathered from untrusted third-party vendors, severely affects the security and trustworthiness of SoC computing platforms. An important emerging concern with the hardware IPs acquired from external sources is that they may come with deliberate malicious implants to incorporate undesired functionality, undocumented test and debug interface working as hidden backdoor, or other integrity issues. This chapter describes various security and trust vulnerabilities in third-party hardware IPs.", "num_citations": "7\n", "authors": ["1805"]}
{"title": "Efficient task partitioning and scheduling for thermal management in multicore processors\n", "abstract": " Power and heat density of integrated circuits (ICs) are rising exponentially over the years. The overheating of ICs leads to higher cost of cooling and packaging as well as reliability concerns and shorter lifetime. While existing task-partitioning based approaches are promising for reducing peak temperature in uniprocessor systems, there are no previous efforts in exploring temperature-aware task partitioning in multicore architectures. In this paper, we propose a task partitioning and scheduling algorithm to reduce the hot-spot in multicore embedded systems running a set of independent tasks. Experimental results using real benchmarks show that our approach is able to reduce the peak temperature by as much as 4. 52oC compared to the stateof-the-art thermal-aware task scheduling algorithm PDTM [22] while requires 31% less time to finish all the tasks.", "num_citations": "7\n", "authors": ["1805"]}
{"title": "Learning-oriented property decomposition for automated generation of directed tests\n", "abstract": " SAT-based Bounded Model Checking (BMC) is promising for automated generation of directed tests. Due to the state space explosion problem, SAT-based BMC is unsuitable to handle complex properties with large SAT instances or large bounds. In this paper, we propose a framework to automatically scale down the SAT falsification complexity by utilizing the decision ordering based learning from decomposed sub-properties. Our framework makes three important contributions: i) it proposes learning-oriented decomposition techniques for complex property falsification, ii) it proposes an efficient approach to accelerate the complex property falsification using the learning from decomposed sub-properties, and iii) it combines the advantages of both property decomposition and property clustering to reduce the overall test generation time. The experimental results using both software and hardware\u00a0\u2026", "num_citations": "7\n", "authors": ["1805"]}
{"title": "Memory-based computing for performance and energy improvement in multicore architectures\n", "abstract": " Memory-based computing (MBC) is promising for improving performance and energy efficiency in both data-and compute-intensive applications. In this paper, we propose a novel reconfigurable MBC framework for multicore architectures where each core uses caches for computation using Look Up Tables (LUTs). Experimental results demonstrate that on-demand memory-based computing in each core can significantly improve performance (up to 4.7 X, 3.3 X on average) as well as reduce energy consumption (up to 4.7 X, 2X on average) in multicore architectures.", "num_citations": "7\n", "authors": ["1805"]}
{"title": "Performance analysis of high performance K-Mean data mining algorithm for multicore heterogeneous compute cluster\n", "abstract": " In this paper, we have study the performance of k-Mean data-mining algorithm (k-Mean), which is implemented on the heterogeneous compute cluster with the multi core programming. The multicore program is implemented with MPI and C for the parallel computing and utilizing the maximum compute power of the heterogeneous cluster. The heterogeneous cluster is established with the help of MPICH2.We have also analyzed the efficiency and performance of k-Mean data mining algorithm for the large dataset. The dataset, which we have used, is chess. txt [1]. The dataset is divided into the number of cores and core compute the dataset independently and makes a data cluster of similar dataset on each processor core.", "num_citations": "7\n", "authors": ["1805"]}
{"title": "Dual code compression for embedded systems\n", "abstract": " Computer architects aim to make embedded systems more powerful and space efficient. Code compression is traditionally used to reduce the code size by compressing the instructions with higher static frequency. However, it may introduce decompression overhead. Performance-aware compression techniques try to improve performance through reduction of cache misses by utilizing the dynamic instruction frequency, but it sacrifices code size. We propose a dual compression scheme that aims to simultaneously optimize both code size reduction and performance improvement. Experimental results show that our approach can simultaneously achieve best of both scenarios - achieves up to 40% compression efficiency and an average performance improvement of 50%.", "num_citations": "7\n", "authors": ["1805"]}
{"title": "Contemporary Computing: Third International Conference, IC3 2010, Noida, India, August 9-11, 2010. Proceedings, Part I\n", "abstract": " \u200b This book constitutes the first part of the refereed proceedings of the Third International Conference, IC3 2010, held in Noida, India, in August 2010. The 23 revised full papers presented were carefully reviewed and selected from numerous submissions.", "num_citations": "7\n", "authors": ["1805"]}
{"title": "Functional validation of programmable architectures\n", "abstract": " Validation of programmable architectures, consisting of processor cores, coprocessors, and memory subsystems, is one of the major bottlenecks in current system-on-chip design methodology. A critical challenge in validation of such systems is the lack of a golden reference model. Traditional validation techniques employ different reference models depending on the abstraction level and verification task (e.g., functional simulation or property checking), resulting in potential inconsistencies between multiple reference models. This paper presents a validation methodology that uses an architecture description language (ADL) based specification as a golden reference model for validation of programmable architectures, and generation of executable models such as simulators and hardware prototypes. We present a validation framework that uses the generated hardware as a reference model to verify the hand-written\u00a0\u2026", "num_citations": "7\n", "authors": ["1805"]}
{"title": "A top-down methodology for microprocessor validation\n", "abstract": " A major challenge in today's functional verification is the lack of a formal specification with which to compare the RTL model. We propose a novel top-down verification approach that allows specification of a design above the RTL. From this specification, it is possible to automatically generate assertion models and RTL reference models. We also demonstrate that symbolic simulation and equivalence checking can be applied to verify an RTL design against its specification.", "num_citations": "7\n", "authors": ["1805"]}
{"title": "A framework for GUI-Driven design space exploration of a MIPS4K-like processor\n", "abstract": " EXPRESSION is an ADL (Architecture Description Language) that can capture a processormemory architecture description and generate a compiler and simulator automatically from this description. Previously many case studies have been undertaken with the goal of architecture exploration using a framework that involves manual specification of an architecture in EXPRESSION and subsequent manual intervention at various steps in the ADL to the back-end compiler and simulator flow. In this technical report we present a framework for capturing the EXPRESSION description for processors using a GUI front end tool and transforming the generated description into intermediate code to be used by the compiler and simulator engines. This automated flow requires no manual intervention at any point and allows rapid Design Space Exploration by modifying the graphical specification of the processor using the GUI. We give the example of a MIPS 4K like processor called acesMIPS which was completely captured using our automated framework and present some sample exploration studies.", "num_citations": "7\n", "authors": ["1805"]}
{"title": "Hardware-assisted malware detection using explainable machine learning\n", "abstract": " Malicious software, popularly known as malware, is widely acknowledged as a serious threat to modern computing systems. Software-based solutions, such as anti-virus software, are not effective since they rely on matching patterns that can be easily fooled by carefully crafted malware with obfuscation or other deviation capabilities. While recent malware detection methods provide promising results through effective utilization of hardware features, the detection results cannot be interpreted in a meaningful way. In this paper, we propose a hardware-assisted malware detection framework using explainable machine learning. This paper makes three important contributions. First, we theoretically establish that our proposed method can provide interpretable explanation of classification results to address the challenge of transparency. Next, we show that the explainable outcome can lead to accurate localization of\u00a0\u2026", "num_citations": "6\n", "authors": ["1805"]}
{"title": "SECTAR: Secure NoC using Trojan Aware Routing\n", "abstract": " System-on-Chips (SoCs) are designed using different Intellectual Property (IP) blocks from multiple third-party vendors to reduce design cost while meeting aggressive time-to-market constraints. Designing trustworthy SoCs need to address the increasing concerns related to supply-chain security vulnerabilities. Malicious implants on IPs, such as Hardware Trojans (HTs) are one of the significant security threats in designing trustworthy SoCs. It is a major challenge to detect Trojans in complex multi-processor SoCs using conventional pre- and post-silicon validation methodologies. Packet-based Network-on-Chip (NoC) is a widely used solution for on-chip communication between IPs in complex SoCs. The focus of this paper is to enable trusted NoC communication in the presence of potentially untrusted IPs. This paper makes three key contributions. (1) We model an HT in NoC router that activates misrouting of the\u00a0\u2026", "num_citations": "6\n", "authors": ["1805"]}
{"title": "The Metric Matters: The Art of Measuring Trust in EIectronics\n", "abstract": " Electronic hardware trust is an emerging concern for all stake-holders in the semiconductor industry. Trust issues in electronic hardware span all stages of its life cycle-from creation of intellectual property (IP) blocks to manufacturing, test and deployment of hardware components and all abstraction levels-from chips to printed circuit boards (PCBs) to systems. The trust issues originate from a horizontal business model that promotes reliance of third-party untrusted facilities, tools, and IPs in the hardware life cycle. Today, designers are tasked with verifying the integrity of third-party IPs before incorporating them into system-on-chip (SoC) designs. Existing trust metric frameworks have limited applicability since they are not comprehensive. They capture only a subset of vulnerabilities such as potential vulnerabilities introduced through design mistakes and CAD tools, or quantify features in a design that target a particular\u00a0\u2026", "num_citations": "6\n", "authors": ["1805"]}
{"title": "Vulnerability-aware energy optimization using reconfigurable caches in multicore systems\n", "abstract": " Dynamic cache reconfiguration has been widely explored for energy optimization and performance improvement for single-core systems. Cache partitioning techniques are introduced for the shared cache in multicore systems to alleviate inter-core interference. While these techniques focus only on performance and energy, they ignore vulnerability due to soft errors. In this paper, we present a static profiling based algorithm to enable vulnerability-aware energy-optimization for real-time multicore systems. Our approach can efficiently search the space of cache configurations and partitioning schemes for energy optimization while task deadlines and vulnerability constraints are satisfied. Our experimental results demonstrate that our approach can achieve 19.2% average energy savings compared with the base configuration, while drastically reduce the vulnerability (49.3% on average) compared to state-of-the-art\u00a0\u2026", "num_citations": "6\n", "authors": ["1805"]}
{"title": "Tecs: Temperature-and energy-constrained scheduling for multicore systems\n", "abstract": " The widespread use of multicore architectures with decreasing feature size is causing severe increase of on-chip power dissipation in modern embedded systems. This introduces both thermal and energy management problems that need to be addressed during the system level design. In this paper, we explore the DVS scheduling problem on multicore systems under both temperature and energy constraints. We present an exact algorithm as well as a polynomial time approximation scheme, since this problem is NP-hard. When the original problem is schedulable, our approximation algorithm is guaranteed to generate a solution, which will not violate the temperature constraint, and consume no more time or energy than a specified approximation bound, e.g., within 1%, of the optimal time consumption and energy constraints. We evaluate our approach using both real and synthetic benchmarks mapped on DVS\u00a0\u2026", "num_citations": "6\n", "authors": ["1805"]}
{"title": "Branch-and-bound style resource constrained scheduling using efficient structure-aware pruning\n", "abstract": " Branch-and-bound approaches are promising in pruning infeasible search space during the resource constrained scheduling (RCS). However, such methods only compare the estimated upper and lower bounds of an incomplete schedule to the length of the best feasible schedule at that iteration. This paper proposes an efficient pruning technique which can identify the fruitless search space based on the detailed structural scheduling information of the obtained best feasible schedule. The proactive nature of our pruning technique enables the pruning of the space which cannot be identified by the state-of-the-art branch-and-bound techniques. The experimental results demonstrate that our approach can drastically (up to two orders-of-magnitude) reduce the overall RCS time under a wide variety of resource constraints.", "num_citations": "6\n", "authors": ["1805"]}
{"title": "Constrained signal selection for post-silicon validation\n", "abstract": " Limited signal observability is a major concern during post-silicon validation. On-chip trace buffers store a small number of signal states every cycle. Existing signal selection techniques are designed to select a set of signals based on the trace buffer width. In a real-life scenario, it is reasonable that a designer has determined some important signals that must be traced. In this paper, we study the constrained signal selection problem where a set of trace signals are already provided by the designer and the remaining signals have to be determined to improve overall restoration performance. Our experimental results using ISCAS'89 benchmarks demonstrate that up to 5% improvement can be obtained in restoration performance compared to existing approaches.", "num_citations": "6\n", "authors": ["1805"]}
{"title": "Reliability improvement in multicore architectures through computing in embedded memory\n", "abstract": " Nanoscale devices provide the capability of gigascale integration in modern electronic systems. However, such systems suffer from high defect rates and large parametric variations that can adversely affect system reliability. Hardware duplication is an obvious direction but it incurs significant area overhead that is unacceptable in many scenarios. Memory-based computing (MBC) is a promising alternative to improve overall system reliability when few functional units are defective or unreliable under process-induced or thermal variations. Existing works demonstrated the utility of MBC in single-core based designs. In this paper, we explore the effectiveness of MBC in multicore architectures where each core uses a small private cache while a set of cores share a large second-level cache. The private as well as shared caches are used to perform computation on demand using a lookup table. When a functional unit\u00a0\u2026", "num_citations": "6\n", "authors": ["1805"]}
{"title": "Architecture description languages\n", "abstract": " Publisher SummaryArchitecture description languages (ADL) enable design automation of embedded processors. The ADL specification is used to generate various executable models, including simulator, compiler, and hardware implementation. The generated models enable various design automation tasks, including exploration, simulation, compilation, synthesis, test generation, and validation. This chapter reviews the existing ADLs in terms of their capabilities in capturing a variety of embedded processors available currently. Existing ADLs can be classified based on two aspects: content and objective. The content-oriented classification is based on the nature of the information an ADL can capture, whereas the objective-oriented classification is based on the purpose of an ADL. Existing ADLs can be classified into various content-based categories, such as structural, behavioral, and mixed ADLs. Similarly\u00a0\u2026", "num_citations": "6\n", "authors": ["1805"]}
{"title": "A property checking approach to microprocessor verification using symbolic simulation\n", "abstract": " Several bottom-up validation techniques have been proposed to formally verify the implementation of a microprocessor by comparing the pipelined implementation with its Instruction-Set Architecture (ISA) specification model, or by deriving the ISA model from the implementation. We present a top-down validation approach using symbolic simulation. We define a set of properties and verify the correctness of the processor by verifying if the properties are met. We applied our methodology to verify several properties on a Memory Management Unit (MMU) of a microprocessor that is compliant with the PowerPC instruction-set architecture to demonstrate the usefulness of our approach.", "num_citations": "6\n", "authors": ["1805"]}
{"title": "Coprocessor codesign for programmable architectures\n", "abstract": " Embedded systems present a tremendous opportunity to customize the designs by exploiting the application behavior. Shrinking time-to-market, coupled with short product lifetimes create a critical need for rapidly explore and evaluate candidate System-on-Chip (SOC) architectures. Recent work on language driven Design Space Exploration (DSE) uses Architecture Description Language (ADL) to capture the processor-memory architecture and generate automatically a software toolkit for that architecture. We present in this report an ADL-based approach to explicitly capture the coprocessor configuration, and perform exploration of the coprocessor architecture along with processor and memory subsystem. We present a set of experiments using our coprocessoraware ADL to drive the exploration of the TI C6x processor-memory architecture in the presence of coprocessors, demonstrating a range of cost and\u00a0\u2026", "num_citations": "6\n", "authors": ["1805"]}
{"title": "Directed test generation for hybrid systems\n", "abstract": " Validation of hybrid systems is complex due to interactions of both continuous and discrete dynamics. Simulation is the most widely used form of system validation using a combination of random and constrained-random tests. Directed tests are promising since orders-of-magnitude less number of directed tests can achieve the same coverage goal compared to random tests. While directed test generation is well studied for digital designs, it is still in its infancy for hybrid systems. In this paper, we propose a method for automatically generating directed tests for hybrid systems. The test generation scheme is based on the Rapidly Exploring Random Tree (RRT) algorithm. In contrast to existing methods of using RRT for validation that tries to reach targets (functional scenarios) from the initial state, we propose to employ reverse RRT that starts from a target and tries to reach the initial state. This enables us to generate an\u00a0\u2026", "num_citations": "5\n", "authors": ["1805"]}
{"title": "Proactive thermal management using memory based computing\n", "abstract": " Nanoscale devices provide the capability of gigascale integration in modern electronic systems. However, such systems suffer from high defect rates and large parametric variations. The surge of transistor count with the increased clock rate elevates the processor temperature which makes these systems even more unreliable and unstable. Dynamic Thermal Management (DTM) approaches considerably increase application's run-time in order to lower the peak temperature. Memory-based computing (MBC) is a promising approach to improve overall system reliability when few functional units are defective or unreliable under process-induced or thermal variations. In this paper, we present a novel DTM technique using proactive MBC to reduce the peak temperature of applications. We propose an efficient technique to proactively transfer the instructions with frequent operand pairs to memory. Experimental results\u00a0\u2026", "num_citations": "5\n", "authors": ["1805"]}
{"title": "EXPRESSION: An ADLfor Software Toolkit Generation, Exploration, and Validation of Programmable SOC Architectures\n", "abstract": " Publisher SummaryThis chapter focuses on the EXPRESSION architecture description language (ADL) and its associated design automation methodologies. EXPRESSION is used for modeling, software toolkit generation, rapid prototyping, design space exploration, and functional verification of System-on-Chip (SOC) architectures and it was developed at the University of California, Irvine. It follows a mixed-level approach where it captures both the structure and behavior that supports a natural specification of the programmable architectures consisting of processor cores, coprocessors, and memories. The powerful constructs in EXPRESSION allow specification of a wide variety of processor, coprocessor, and memory architectures. Automatic generation of a compiler and simulator enables fast and efficient design space exploration of architectures. The elegant formalism in EXPRESSION also enables top-down\u00a0\u2026", "num_citations": "5\n", "authors": ["1805"]}
{"title": "HDLGen: Architecture description language driven HDL generation for pipelined processors\n", "abstract": " As embedded systems continue to face increasingly higher performance requirements, deeply pipelined processor architectures are being employed to meet desired system performance. System architects critically need modeling techniques to rapidly explore and evaluate candidate architectures based on area, clock frequency, power, and performance constraints. We present an exploration framework for pipelined processors. We use the EXPRESSION Architecture Description Language (ADL) to capture a wide spectrum of processor architectures. The ADL has been used to enable performance driven exploration by generating a software toolkit from the ADL specification. In this paper, we present how to automatically generate synthesizable RTL from the ADL specification using a functional abstraction technique. Automatic generation of RTL enables rapid exploration of candidate architectures under given design constraints such as area, clock frequency, power, and performance. Our exploration results demonstrate the power of reuse in composing heterogeneous architectures using functional abstraction primitives allowing for a reduction in the time for specification and exploration by at least an order of magnitude.", "num_citations": "5\n", "authors": ["1805"]}
{"title": "Architecture description language driven verification of in-order execution in pipelined processors\n", "abstract": " As embedded systems continue to face increasingly higher performance requirements, deeply pipelined processor architectures are being employed to meet desired system performance. System architects critically need modeling techniques that allow exploration, evaluation, customization and validation of different processor pipeline configurations, tuned for a specific application domain. We propose a novel FSM-based modeling of pipelined processors and define a set of properties that can be used to verify the correctness of in-order execution in the pipeline. Our approach leverages the system architect\u2019s knowledge about the behavior of the pipelined processor (through our ADL constructs) and thus allows a powerful top-down approach to pipeline verification.", "num_citations": "5\n", "authors": ["1805"]}
{"title": "Specification of hazards, stalls, interrupts, and exceptions in EXPRESSION\n", "abstract": " Recent work on language-driven Design Space Exploration (DSE)([1],[2],[3],[4],[6],[7],[9],[11],[12]), uses Architectural Description Languages (ADL) to capture the processor architecture, generate automatically a software toolkit (including compiler, simulator and assembler) for that processor, and provide feedback to the designer on the quality of the architecture. However none of these ADLs have explicit way of describing hazards and interrupts for wide variety of processors and memory architectures. The nML [6], LISA [5] and RADL [10] processor description languages are closet to our work. We describe in detail the hazard and interrupt specification techniques for these languages.The RADL [10] processor description language supports interrupts and hazards specification. Hazard/Stall specification is closely tied to the architecture and hence not good candidate for architectural exploration. Moreover, the paper\u00a0\u2026", "num_citations": "5\n", "authors": ["1805"]}
{"title": "A study of out-of-order completion for the MIPS R10K superscalar processor\n", "abstract": " Instruction level parallelism (ILP) improves performance for VLIW, EPIC, and Superscalar processors. Out-of-order execution improves performance further. The advantage of out-of-order execution is not fully utilized due to in-order completion. In this report we study the performance loss due to in-order completion for MIPS R10000 processor.", "num_citations": "5\n", "authors": ["1805"]}
{"title": "A Survey of Network-on-Chip Security Attacks and Countermeasures\n", "abstract": " With the advances of chip manufacturing technologies, computer architects have been able to integrate an increasing number of processors and other heterogeneous components on the same chip. Network-on-Chip (NoC) is widely employed by multicore System-on-Chip (SoC) architectures to cater to their communication requirements. NoC has received significant attention from both attackers and defenders. The increased usage of NoC and its distributed nature across the chip has made it a focal point of potential security attacks. Due to its prime location in the SoC coupled with connectivity with various components, NoC can be effectively utilized to implement security countermeasures to protect the SoC from potential attacks. There is a wide variety of existing literature on NoC security attacks and countermeasures. In this article, we provide a comprehensive survey of security vulnerabilities in NoC-based SoC\u00a0\u2026", "num_citations": "4\n", "authors": ["1805"]}
{"title": "Editorial TVLSI positioning\u2014Continuing and accelerating an upward trajectory\n", "abstract": " I. VLSI Systems: A Glance Into The Last Decades Since their inception in 1970s, VLSI systems have enabled several new technological capabilities and made them accessible to an unceasingly wider range of users, reaching a scale that has been exponentially increasing over the decades    (see   ). Relentless integration of more complex systems has driven such remarkable evolution, as made possible by the inexorable miniaturization. As shown in   , more functionality has been crammed in a consistently smaller form factor, as exemplified by the physical volume shrinking of computers by 100 X/decade   ,   . At the same time, the energy per task has been decreasing at 10\u2013100 X/decade, as shown in   , for several systems and system-on-chip subsystems   . This allowed packing more capabilities into the same power envelope, as generally observed in the electronic systems, even before the advent of the\u00a0\u2026", "num_citations": "4\n", "authors": ["1805"]}
{"title": "Test generation for hybrid systems using clustering and learning techniques\n", "abstract": " In this paper, we propose a test generation method that employs clustering and learning techniques to reduce test generation time in hybrid systems. While learning-oriented test generation is a well-studied problem for digital systems, there are limited efforts for utilizing learning during generation of directed tests for hybrid systems. This paper makes two important contributions: i) it develops an efficient technique to cluster a set of functional scenarios that are expected to have similar test generation trajectory, and ii) it employs efficient learning mechanism such that beneficial information is shared during test generation of similar functional scenarios in a cluster. Our experimental results using two popular hybrid systems demonstrate that our approach can significantly (up to 3:8 times, 2:9 times on average) reduce the overall test generation time.", "num_citations": "4\n", "authors": ["1805"]}
{"title": "Lossles audio compression: A case study\n", "abstract": " Audio compression is used everywhere\u2013largely due to the rise of the internet, computers, and embedded systems. Since original, uncompressed digital recordings are very large in size, efficient ways are needed to compress large audio signals into small, high-quality, convenient formats. While lossy audio formats, like MP3, are widely popular in many everyday applications, lossless formats are also important to retain the original audio signals. It is feasible to perform lossless compression due to decreasing memory cost and increasing internet bandwidth. This paper investigates improvements to the Free Lossless Audio Codec (FLAC), one of the best lossless audio formats, by conducting tests on nineteen quality benchmarks. We have also studied lossless compression techniques from other domains and applied dictionary-based encoding for audio compression. The results show that dictionary-based compression is not useful, while modifying the blocking size in FLAC shows minor improvements in compression efficiency.", "num_citations": "4\n", "authors": ["1805"]}
{"title": "Memory access optimizations in instruction-set simulators\n", "abstract": " Design of programmable processors and embedded applications requires instruction-set simulators for early exploration and validation of candidate architectures. Interpretive simulators are widely used in embedded systems design. One of the key performance bottlenecks in interpretive simulation is the instruction and data memory access translation between host and target machines. The simulators must maintain and update the status of the simulated processor including memory and register values. A major challenge in the simulation is to efficiently map the target address space to the host address space. This paper presents two optimization techniques that aggressively utilize the spatial locality of the instruction and data accesses in interpretive simulation: signature based address mapping for optimizing general memory accesses; and incremental instruction fetch for optimizing instruction accesses. To\u00a0\u2026", "num_citations": "4\n", "authors": ["1805"]}
{"title": "A methodology for validation of microprocessors using symbolic simulation\n", "abstract": " Functional validation is one of the most complex and expensive tasks in the current processor design methodology. A significant bottleneck in the validation of processors is the lack of a golden reference model. Thus, many existing approaches employ a bottom-up methodology by using a combination of simulation techniques and formal methods. We present a top-down validation approach using a language-based specification. The specification is used to generate the necessary reference models for processor validation using symbolic simulation. We applied our methodology for property checking as well as equivalence checking of microprocessors.", "num_citations": "4\n", "authors": ["1805"]}
{"title": "Network-on-Chip Security and Privacy\n", "abstract": " This book provides comprehensive coverage of Network-on-Chip (NoC) security vulnerabilities and state-of-the-art countermeasures, with contributions from System-on-Chip (SoC) designers, academic researchers and hardware security experts. Readers will gain a clear understanding of the existing security solutions for on-chip communication architectures and how they can be utilized effectively to design secure and trustworthy systems.", "num_citations": "3\n", "authors": ["1805"]}
{"title": "Hardware Acceleration of Explainable Machine Learning using Tensor Processing Units\n", "abstract": " Machine learning (ML) is successful in achieving human-level performance in various fields. However, it lacks the ability to explain an outcome due to its black-box nature. While existing explainable ML is promising, almost all of these methods focus on formatting interpretability as an optimization problem. Such a mapping leads to numerous iterations of time-consuming complex computations, which limits their applicability in real-time applications. In this paper, we propose a novel framework for accelerating explainable ML using Tensor Processing Units (TPUs). The proposed framework exploits the synergy between matrix convolution and Fourier transform, and takes full advantage of TPU's natural ability in accelerating matrix computations. Specifically, this paper makes three important contributions. (1) To the best of our knowledge, our proposed work is the first attempt in enabling hardware acceleration of explainable ML using TPUs. (2) Our proposed approach is applicable across a wide variety of ML algorithms, and effective utilization of TPU-based acceleration can lead to real-time outcome interpretation. (3) Extensive experimental results demonstrate that our proposed approach can provide an order-of-magnitude speedup in both classification time (25x on average) and interpretation time (13x on average) compared to state-of-the-art techniques.", "num_citations": "3\n", "authors": ["1805"]}
{"title": "Directed Test Generation for Activation of Security Assertions in RTL Models\n", "abstract": " Assertions are widely used for functional validation as well as coverage analysis for both software and hardware designs. Assertions enable runtime error detection as well as faster localization of errors. While there is a vast literature on both software and hardware assertions for monitoring functional scenarios, there is limited effort in utilizing assertions to monitor System-on-Chip (SoC) security vulnerabilities. We have identified common SoC security vulnerabilities and defined several classes of assertions to enable runtime checking of security vulnerabilities. A major challenge in assertion-based validation is how to activate the security assertions to ensure that they are valid. While existing test generation using model checking is promising, it cannot generate directed tests for large designs due to state space explosion. We propose an automated and scalable mechanism to generate directed tests using a\u00a0\u2026", "num_citations": "3\n", "authors": ["1805"]}
{"title": "Hardware-Assisted Malware Detection using Machine Learning\n", "abstract": " Malicious software, popularly known as malware, is a serious threat to modern computing systems. A comprehensive cybercrime study by Ponemon Institute highlights that malware is the most expensive attack for organizations, with an average revenue loss of $2.6 million per organization in 2018 (11% increase compared to 2017). Recent high-profile malware attacks coupled with serious economic implications have dramatically changed our perception of threat from malware. Software-based solutions, such as anti-virus programs, are not effective since they rely on matching patterns (signatures) that can be easily fooled by carefully crafted malware with obfuscation or other deviation capabilities. Moreover, softwarebased solutions are not fast enough for real-time malware detection in safety-critical systems. In this paper, we investigate promising approaches for hardware-assisted malware detection using machine learning. Specifically, we explore how machine learning can be effective for malware detection utilizing hardware performance counters, embedded trace buffer as well as on-chip network traffic analysis.", "num_citations": "3\n", "authors": ["1805"]}
{"title": "System-on-Chip Security Assertions\n", "abstract": " Assertions are widely used for functional validation as well as coverage analysis for both software and hardware designs. Assertions enable runtime error detection as well as faster localization of errors. While there is a vast literature on both software and hardware assertions for monitoring functional scenarios, there is limited effort in utilizing assertions to monitor System-on-Chip (SoC) security vulnerabilities. In this paper, we identify common SoC security vulnerabilities by analyzing the design. To monitor these vulnerabilities, we define several classes of assertions to enable runtime checking of security vulnerabilities. Our experimental results demonstrate that the security assertions generated by our proposed approach can detect all the inserted vulnerabilities while the functional assertions generated by state-of-the-art assertion generation techniques fail to detect most of them.", "num_citations": "3\n", "authors": ["1805"]}
{"title": "Test generation for detection of malicious parametric variations\n", "abstract": " Reusable hardware Intellectual Property (IP) based System-on-Chip (SoC) design has emerged as a pervasive design practice in the industry to dramatically reduce design/verification cost while meeting aggressive time-to-market constraints. It is crucial to ensure that an IP block is not vulnerable to input conditions that violate its non-functional (parametric) constraints, such as power, temperature, or performance. Power supply voltages, increased integration densities, and higher operating frequencies, among other factors, are producing devices that are more sensitive to power dissipation and reliability problems. Power viruses which have excessive power dissipation can lead to overheating, electromigration, and a reduced chip lifetime. Moreover, large instantaneous power consumption causes voltage drop and ground bounce, resulting in circuit delays and soft errors. As a result, reliability analysis of\u00a0\u2026", "num_citations": "3\n", "authors": ["1805"]}
{"title": "Intrusion detection system for classification of attacks with cross validation\n", "abstract": " Now days, due to rapidly uses of internet, the patterns of network attacks are increasing. There are various organizations and institutes are using internet and access or share the sensitive information in network. To protect information from unauthorized or intruders is one of the important issues. In this paper, we have used decision tree techniques like C4. 5 and CART as classifier for classification of attacks. We have proposed an ensemble model that is combination of C4. 5 and Classification and Regression Tree (CART) as robust classifier for classification of attacks. We have used NSL-KDD data set with binary and multiclass problem with 10-fold cross validation. The proposed ensemble model gives satisfactory accuracy as 99.67% and 99.53% in case of binary class and multiclass NSL-KDD data set respectively.", "num_citations": "3\n", "authors": ["1805"]}
{"title": "Efficient resource constrained scheduling using parallel structure-aware pruning techniques\n", "abstract": " Branch-and-bound approaches are promising in pruning fruitless search space during the resource constrained scheduling. However, such approaches only compare the estimated upper and lower bounds of an incomplete schedule to the length of the best feasible schedule at that iteration, which does not fully exploit the potential of the pruning during the search. Aiming to improve the performance of resource constrained scheduling, this paper proposes a parallel structure-aware pruning approach that can traverse the search space significantly faster than state-of-the-art branch-and-bound techniques. This paper makes three major contributions: i) it proposes an efficient pruning technique using the structural scheduling information of the obtained best feasible schedules; ii) it investigates how to perform parallel search to enable efficient multi-directional search and generation of effective fences by tuning the\u00a0\u2026", "num_citations": "3\n", "authors": ["1805"]}
{"title": "Layout-aware signal selection in reconfigurable architectures\n", "abstract": " Post-silicon validation is an important and increasingly complex task in SoC design methodology. One of the major challenges in post-silicon debug is the limited observability of internal signals. Existing signal selection techniques try to maximize observability by selecting a small set of profitable trace signals. Unfortunately, these techniques do not consider design constraints such as routing congestion in reconfigurable architectures. In this paper, we propose a layout-aware signal selection algorithm that takes into account both observability and routing congestion in field-programmable gate array (FPGA). Our experimental results demonstrate that our approach can tradeoff between observability and wire-length reduction in FPGA-based designs.", "num_citations": "3\n", "authors": ["1805"]}
{"title": "Guest Editors' Introduction: Special Section on System-Level Design and Validation of Heterogeneous Chip Multiprocessors\n", "abstract": " Chip (MPSoC), are used in a wide variety of systems. They are necessary to avoid the unsustainable power consumption profile of increasing clock speed of uniprocessors in the early part of the last decade. The designers are thus forced to employ innovative design alternatives such as heterogeneous cores, novel network-on-chips, GPUs, and reconfigurable fabrics. At the same time, the design teams face shortened design cycles and the increasing pool of immediate users, as a large variety of such systems are already placed in the hands of consumers (including Motorola Xoom, Apple iPad, to name a few). To meet such stringent constraints and to guarantee quality of the products, the emerging CMPs need to be designed by making verification of the functionality as an integral part of the design methodology from inception to final product delivery, which presents a design paradigm shift. Until now the architectural exploration and initial performance evaluation phases were prior to the design, whereas the validation tasks were employed after the design is completed. Now, more than ever, validation must be done at every stage of the design, and as the designs move from one level of abstraction to the next, consistency must be validated as well.Various studies indicate that the functional verification of complex CMPs consumes majority (as much as 70 percent) of the overall design time and resources and yet many CMP designs exhibit first silicon failure, primarily due to the functional errors. Functional verification complexity is expected to increase further due to the shift to heterogeneous multicore architectures that increasingly use novel\u00a0\u2026", "num_citations": "3\n", "authors": ["1805"]}
{"title": "System-wide Energy Optimization with DVS and DCR\n", "abstract": " In Chaps. 3 and 5, we described approaches and algorithms for employing DCR and DVS in real-time systems separately. However, as shown in Fig. 1.5, both processor and cache subsystem as well as other components contribute to the system\u2019s overall power dissipation. Therefore, it is promising to employ DVS and DCR simultaneously to perform system-wide energy optimization.", "num_citations": "3\n", "authors": ["1805"]}
{"title": "Challenges of rapidly emerging consumer space multiprocessors\n", "abstract": " In the computer systems industry, roughly between the years 1950 and 1980, the advances in technology were largely aimed at high-end or enterprise-class hardware.For the next 30 years, the personal computing industry was the arena in which technological advances were applied. For the past couple years, technological trailblazing has been led by consumer electronics, such as smartphones, tablets, and game consoles.Accordngly, as the user base has widened, design, validation, and quality assurance practice must be strengthened. At the same time, the difficulties in making sure systems are correct are increasingly complex and spread across all levels, including that of application development.This article examines the difficulties presented by the industry's essentially having cornered itself such that there is little room for error.", "num_citations": "3\n", "authors": ["1805"]}
{"title": "A brief history of multiprocessors and eda\n", "abstract": " The Last Byte takes a brief look at the recent history of multiprocessor design and EDA to see where the community stands today. Turning sophisticated systems into commodity items has presented its share of problems, but innovation continues to thrive, nonetheless, and offers considerable promise for validation and testing solutions.", "num_citations": "3\n", "authors": ["1805"]}
{"title": "Architecture description languages for programmable embedded systems\n", "abstract": " Embedded systems are everywhere\u2013they run the computing devices hidden inside a vast array of everyday products and appliances such as cell phones, toys, handheld PDAs, camerasandmicrowaveovens. Carsarefullofthem, asare airplanes, satellites and advanced military and medical equipments. As applications growincreasingly complex, so do the complexities of the embedded computing devices. Figure 6.1 shows an example embedded system, consisting of programmable components including a processor core, coprocessors and memory subsystem. The programmable components are used to execute the application programs. Depending on the application domain, the embedded system can have application specific hardwares, interfaces, controllers and peripherals. The programmable components, consisting of a processor core, coprocessors and memory subsystem, are referred to as \u2018programmable embedded systems\u2019. They are also referred to as \u2018programmable architectures\u2019.As embedded systems become ubiquitous, there is an urgent need to facilitate rapid design space exploration (DSE) of programmable architectures. This need for rapid DSE becomes even more critical given the dual pressures of shrinking timeto-market and ever-shrinking product lifetimes. Architecture Description Languages (ADLs) areusedtoperformearlyexploration, synthesis, testgenerationandvalidation of processor-based designs as shown in Figure 6.2. ADLs are used to specify programmable architectures. The specification can be used for generation of a software toolkit including the compiler, assembler, simulator and debugger. The application\u00a0\u2026", "num_citations": "3\n", "authors": ["1805"]}
{"title": "Language-driven validation of pipelined processors using satisfiability solvers\n", "abstract": " Due to increasing demand for faster computations, deeply pipelined processor architectures are being employed to meet desired system performance. Functional validation of such pipelined processors is one of the most complex and expensive tasks in the current systems-on-chip design methodology. While language-based validation techniques have proposed several promising ideas, many challenges remain in applying them to realistic pipelined processors. This paper describes two practical challenges in this methodology: test generation and equivalence checking. The time and resources required for test generation using the existing approaches can be extremely large for today's pipelined processors. Similarly, traditional equivalence checkers are not useful in the context of language-driven model generation and functional validation. This paper outlines our plan to address these challenges using\u00a0\u2026", "num_citations": "3\n", "authors": ["1805"]}
{"title": "Processor validation: a top-down approach\n", "abstract": " This work presents the design validation techniques for microprocessors. The verification effectiveness validates the design implementation using a combination of simulation techniques and formal methods. This article presents a top-down validation methodology that complements existing bottom-up verification techniques. The validation team applies model checking to a high-level description of the design abstracted from the RTL implementation. Formal verification uses a formal language to describe the system. The specification for the formal verification comes from the architecture description; the implementation can come from either the architecture specification or the abstracted design. A top-down methodology for validating microprocessors using a combination of symbolic simulation and equivalence checking is presented. Specification-driven hardware generation and validation of design implementation\u00a0\u2026", "num_citations": "3\n", "authors": ["1805"]}
{"title": "Automatic functional test program generation for microprocessor verification\n", "abstract": " A novel specification driven and constraints solving based method to automatically generate test programs from simple to complex ones for advanced microprocessors is presented in this paper. Our microprocessor architectural automatic test program generator (MA 2 TG) can produce not only random test programs but also a sequence of instructions for a specific constraint by specifying a user constraints file. The proposed methodology makes three important contributions. First, it simplifies the microprocessor architecture modeling and eases adoption of architecture modification via architecture description language (ADL) specification. Second, it generates test programs for specific constraints utilizing the power of state-to-art constraints solving techniques. Finally, the number of test program for microprocessor verification and the verification time are dramatically reduced. We applied this method on DLX processor\u00a0\u2026", "num_citations": "3\n", "authors": ["1805"]}
{"title": "Functional verification of pipelined processors: a case study\n", "abstract": " Functional verification of pipelined processors is one of the major bottlenecks in current system-on-chip (SOC) design methodology. A significant bottleneck in the validation of such systems is the lack of a suitable functional coverage metric. This paper presents a test generation and functional coverage estimation framework for pipelined processors using Specman Elite. We have applied this methodology on a VLIW DLX architecture to demonstrate the usefulness of our approach.", "num_citations": "3\n", "authors": ["1805"]}
{"title": "Architecture Description Language driven validation of dynamic behavior in pipelined processor specifications\n", "abstract": " As embedded systems continue to face increasingly higher performance requirements, deeply pipelined processor architectures are being employed to meet desired system performance. A significant bottleneck in the validation of such systems is the lack of a golden reference model. Thus, many existing techniques employ a bottom-up approach to architecture validation, where the functionality of an existing pipelined architecture is, in essence, reverse-engineered from its implementation. Our validation technique is complementary to these bottom-up approaches. Our approach leverages the system architect\u2019s knowledge about the behavior of the pipelined architecture, through Architecture Description Language (ADL) constructs, and thus allows a powerful top-down approach to architecture validation. The most important requirement in top-down validation process is to ensure that the specification (reference model) is golden. Earlier, we have developed validation techniques to ensure that the static behavior of the pipeline is well-formed by analyzing the structural aspects of the specification using a graph based model. In this paper, we verify the dynamic behavior by analyzing the instruction flow in the pipeline using a Finite State Machine (FSM) based model to validate several important architectural properties such as determinism, finiteness, and execution style (eg, in-order execution) in the presence of hazards and multiple exceptions. We applied this methodology to the specification of a representative pipelined processor to demonstrate the usefulness of our approach.", "num_citations": "3\n", "authors": ["1805"]}
{"title": "A methodology for validation of microprocessors using equivalence checking\n", "abstract": " As embedded systems continue to face increasingly higher performance requirements, deeply pipelined processor architectures are being employed to meet desired system performance. Validation of such processor architectures is one of the most complex and expensive tasks in the current systems-on-chip design process. A significant bottleneck in the validation of such systems is the lack of a golden reference model. We present an architecture description language (ADL) driven methodology for generating golden reference model. We use EXPRESSION ADL to capture the structure and behavior of the processor. The synthesizable register transfer language (RTL) description of the architecture is generated from the ADL specification. The generated RTL description is used as a golden reference model for verifying the correctness of the implementation using equivalence checking. We applied our methodology\u00a0\u2026", "num_citations": "3\n", "authors": ["1805"]}
{"title": "Architecture description language driven validation of processor, memory, and co-processor pipelines\n", "abstract": " Verification is one of the most complex and expensive tasks in the current Systems-on-Chip (SOC) design process. Many existing approaches employ a bottom-up approach to pipeline validation, where the functionality of an existing pipelined processor is, in essence, reverse-engineered from its RT-level implementation. Our approach leverages the system architect\u2019s knowledge about the behavior of the pipelined architecture, through Architecture Description Language (ADL) constructs, and thus allows a powerful top-down approach to pipeline validation. This report addresses automatic validation of processor, memory, and co-processor pipelines described in an ADL. We present a graph-based modeling of architectures which captures both structure and behavior of the architecture. Based on this model, we present formal approaches for automatic validation of the architecture described in the ADL. We applied our methodology to verify several realistic architectures from different architectural domains to demonstrate the usefulness of our approach.", "num_citations": "3\n", "authors": ["1805"]}
{"title": "MaxSense: Side-Channel Sensitivity Maximization for Trojan Detection using Statistical Test Patterns\n", "abstract": " Detection of hardware Trojans is vital to ensure the security and trustworthiness of System-on-Chip (SoC) designs. Side-channel analysis is effective for Trojan detection by analyzing various side-channel signatures such as power, current, and delay. In this article, we propose an efficient test generation technique to facilitate side-channel analysis utilizing dynamic current. While early work on current-aware test generation has proposed several promising ideas, there are two major challenges in applying it on large designs: (i) The test generation time grows exponentially with the design complexity, and (ii) it is infeasible to detect Trojans, since the side-channel sensitivity is marginal compared to the noise and process variations. Our proposed work addresses both challenges by effectively exploiting the affinity between the inputs and rare (suspicious) nodes. The basic idea is to quickly find the profitable ordered\u00a0\u2026", "num_citations": "2\n", "authors": ["1805"]}
{"title": "Hardware trojan detection schemes using path delay and side-channel analysis\n", "abstract": " Power-side channel attacks use the amount of power consumption and transient/dynamic current leakage to attack the design. A device like an oscilloscope can be used to collect power traces, and those traces are statistically analyzed using correlation analysis to derive secret information of the design. Therefore, it is very important to develop automated security validation methods that can identify power side-channel leakage. We need to detect the parts of a design that is responsible for power side-channel leakage in an automated fashion. Chapter  10   presents techniques to detect these vulnerabilities.           Hardware Trojans are malicious changes in the electronic device that adds or removes functionality or reduces reliability of an integrated circuit, printed circuit board, or system. This chapter describes the threat model of semiconductor supply chain, vulnerabilities, and impact of Trojan attacks. We\u00a0\u2026", "num_citations": "2\n", "authors": ["1805"]}
{"title": "Formal Approaches to Hardware Trust Verification\n", "abstract": " Trust establishment in semiconductor designs has become a major challenge for design houses and government since several countries and companies are involved during different stages of a design life cycle. The variety of vendors increases the risk of security vulnerabilities within the supply chain of integrated circuits. Hardware Trojans are malfunctions which can be inserted during any stage of design such as defining specification, designing intellectual properties (e.g., high-level models, RTL modules, and gate-level netlists), layout extraction, and manufacturing. A triggered hardware Trojan can severely affect the integrity and security of the circuit by causing system failures such as deadlock, denial of service, or granting an unauthorized access to secret information. Hardware Trojans are designed in a way that they are inactive most of the time and can be triggered with a very rare input sequence\u00a0\u2026", "num_citations": "2\n", "authors": ["1805"]}
{"title": "Tutorial T2: Validation and Debug of Security and Trust Issues in Embedded Systems\n", "abstract": " Summary form only given. Reusable hardware intellectual property (IP) based System-on-Chip (SoC) design has emerged as a pervasive design practice in the industry to dramatically reduce design/verification cost while meeting aggressive time-to-market constraints. However, growing reliance on reusable pre-verified hardware IPs and wide array of CAD tools during SoC design - often gathered from untrusted 3rd party vendors - severely affects the security and trustworthiness of SoC computing platforms. Major security issues in the hardware IPs at different stages of SoC life cycle include piracy during IP evaluation, reverse engineering, cloning, counterfeiting, as well as malicious hardware modifications. The global electronic piracy market is growing rapidly and is now estimated to be $1B/day, of which a significant part is related to hardware IPs. Furthermore, use of untrusted foundry in a fabless business\u00a0\u2026", "num_citations": "2\n", "authors": ["1805"]}
{"title": "Directed Test Generation for Cache Coherence Protocols\n", "abstract": " Processors with multiple cores and complex cache coherence protocols are widely employed to improve the overall performance. Since the number of reachable states in a cache coherence protocol grows exponentially with the number of cores, the verification team are facing a dramatic challenge to verify the correctness of the protocol. In this paper, we propose an efficient test generation technique, which can be used to achieve full state and transition coverage in simulation based verification for a wide variety of cache coherence protocols. Based on effective analysis of the state space structure, our method can generate more efficient test sequences (50% shorter) compared with tests generated by breadth first search. Moreover, the proposed approach can generate tests on-the-fly due to its space efficient design. Experimental results demonstrate that our approach can significantly reduce the validation effort compared to existing methods.", "num_citations": "2\n", "authors": ["1805"]}
{"title": "A partitioned bitmask-based technique for lossless seismic data compression\n", "abstract": " Seismic data costs companies and institutes millions of dollars for massive storage equipment as well as for huge data transfer bandwidth. At the same time, precision of seismic data is becoming critical for scientific analysis and research. Many existing techniques have achieved significant compression at the cost of accuracy (loss of information), or lossless compression at the cost of high computation complexity. This report addressed the problem by applying partitioned bitmaskbased compression to seismic data in order to produce a significant compression without losing any accuracy. To demonstrate our approach, we compressed real world seismic data set and obtained an average compression ratio of 74%(1.35).", "num_citations": "2\n", "authors": ["1805"]}
{"title": "Functional Test Generation using SAT-based Bounded Model Checking\n", "abstract": " Functional validation is one of the major bottlenecks in processor design methodology due to combined effects of increasing complexity and decreasing time-to-market. Increasing complexity of designs leads to larger set of design errors. Shorter time-tomarket requires a faster validation scheme. Simulation using functional test vectors is the most widely used form of processor validation. While existing model checking based approaches have proposed several promising ideas for efficient test generation, many challenges remain in applying them to realistic pipelined processors. The time and resources required for test generation using existing model checking based techniques can be extremely large. This report presents an efficient test generation technique using SAT-based bounded model checking. To demonstrate the usefulness of this approach, we have applied this technique to generate test programs for validation of the VLIW MIPS processor.", "num_citations": "2\n", "authors": ["1805"]}
{"title": "A framework for memory subsystem exploration\n", "abstract": " Memory represents a major bottleneck in modern embedded systems in terms of cost, power and performance. Traditionally, memory organizations for programmable systems assume a fixed cache hierarchy. With the widening processor-memory gap, more aggressive memory technologies and organizations have appeared, allowing customization of a heterogeneous memory architecture tuned for the application. However, such a processor-memory co-exploration approach critically needs the ability to explicitly capture heterogeneous memory architectures. We present in this paper a language-based approach to explicitly capture the memory subsystem configuration, and perform exploration of the memory architecture to meet diverse requirements: low power, better performance, smaller die size etc. We present a set of experiments using our Memory-Aware Architectural Description Language to drive the exploration of the memory subsystem for the TI C6211 processor architecture, demonstrating a range of cost, performance, and energy attributes.", "num_citations": "2\n", "authors": ["1805"]}
{"title": "Data Criticality in Multithreaded Applications: An Insight for Many-Core Systems\n", "abstract": " Multithreaded applications are capable of exploiting the full potential of many-core systems. However, network-on-chip (NoC)-based intercore communication in many-core systems is responsible for 60%\u201375% of the miss latency experienced by multithreaded applications. Delay in the arrival of critical data at the requesting core severely hampers performance. This brief presents some interesting insights about how critical data are requested from the memory by multithreaded applications. Then it investigates the cause of delay in NoC and how it affects the performance. Finally, this brief shows how NoC-aware memory access optimizations can significantly improve performance. Our experimental evaluation considers  Early Restart  memory access optimization and demonstrates that by exploiting available NoC resources, critical data can be prioritized to reduce miss penalty by 11% and improve overall system\u00a0\u2026", "num_citations": "1\n", "authors": ["1805"]}
{"title": "Trojan Aware Network-on-Chip Routing\n", "abstract": " With the noting growth in Internet-of-Things (IoT) devices and embedded systems, outsourcing of circuit design and fabrication process has significantly accelerated over the years. The race for bringing more devices into the market made semiconductor industries paying less attention to the hardware security of these devices. Due to the reduced emphasis on security standards, new hardware vulnerabilities are uncovered every now and then. For example, one of the recently exposed glitches in modern Intel processors allows an adversary to access kernel memory [46]. The flaw is able to bypass most of the hardware level protections available in the system. Due to the increasing demand for data and compute-intensive tasks in the era of IoT and embedded systems, design of Multi-Processor System-on-Chips (MPSoCs) gained popularity. The use of packet-based on-chip interconnect technology called Network-on-Chip (NoC) in MPSoCs outperforms the existing bus-based interconnect, thereby circumventing the low wire routing congestion and low operation frequencies of the system. NoC provides separation between computation and communication, supports modularity and Intellectual Property (IP) reuse via standard interfaces and handles synchronisation issues, which in turn improve the performance of the system. However, the long and globally distributed supply chain of hardware IPs makes MPSoC design increasingly vulnerable to diverse trust/integrity issues. For example, MPSoCs built with third party NoCs create more vulnerabilities due to its emphasis on performance and backward compatibility", "num_citations": "1\n", "authors": ["1805"]}
{"title": "LIGHTWEIGHT ENCRYPTION AND ANONYMOUS ROUTING IN NoC BASED SoCs\n", "abstract": " Various examples are provided related to software and hardware architectures that enable lightweight encryption and anonymous routing in a network-on-chip (NoC) based system-on-chip (SoC). In one example, among others, method for lightweight encryption and anonymous routing includes identifying, by a source node in a network-on-chip (NoC) based system-on-chip (SoC) architecture, a routing path from the source node to a destination node in the NoC-based SoC architecture, where the routing path comprises the source node, a plurality of intermediate nodes in the NoC-based SoC architecture, and the destination node; generating, by the source node, a plurality of tuples, a number of tuples in the plurality of tuples being based on a threshold; and distributing, by the source node, the plurality of tuples to the plurality of intermediate nodes and the destination node.", "num_citations": "1\n", "authors": ["1805"]}
{"title": "Trigger activation by repeated maximal clique sampling\n", "abstract": " An exemplary method for generating a test vector to activate a Trojan triggering condition includes the operations of obtaining a design graph representation of an electronic circuit; constructing a satisfiability graph from the design graph representation, wherein the satisfiability graph includes a set of vertices representing rare signals of the electronic circuit and satisfiability connections between the vertices; finding a plurality of maximal satisfiable cliques in the satisfiability graph, wherein a maximal satisfiable clique corresponds to a triggering condition for a payload of the electronic circuit; generating a test vector for each of the maximal satisfiable cliques; and performing a test for the presence of a hardware Trojan circuit in the electronic circuit using the generated test vectors as input signals.", "num_citations": "1\n", "authors": ["1805"]}
{"title": "Special Session: Impact of Noise on Quantum Algorithms in Noisy Intermediate-Scale Quantum Systems\n", "abstract": " A major challenge in realizing efficient and powerful quantum algorithms is quantum noise. Quantum noise itself is a sophisticated topic that is not seen in the classical domain. In this paper, we explore the impact of noise on quantum algorithms in Noisy Intermediate-Scale Quantum (NISQ) systems. This paper first introduces the origins of quantum noise. Next, it proposes a common treatment to simplify the view of quantum noise. Finally, it presents a case study to investigate the impact of noise on quantum Fourier transform algorithm.", "num_citations": "1\n", "authors": ["1805"]}
{"title": "The future of security validation and verification\n", "abstract": " Trustworthy System-on-Chip (SoC) design is vital to provide the hardware root-of-trust to enable a truly secure cyberspace. This book presented a wide variety of state-of-the-art SoC security validation and verification techniques for designing trustworthy SoCs. This chapter concludes the book with a summary of ideas presented in the previous chapters, and outlines the road map of future security validation challenges and opportunities.", "num_citations": "1\n", "authors": ["1805"]}
{"title": "Automated Test Generation for Detection of Malicious Functionality\n", "abstract": " Test generation has been widely used during pre-silicon as well as post-silicon validation to detect design bugs. By applying specific test vectors, we can compare the design outputs with the golden (expected) outputs to detect a violation. Hardware Trojans can be viewed as covert bugs that are maliciously implanted into a design such that they can be activated only under very rare conditions. Due to their stealthy nature, it is challenging to generate effective tests to detect hardware Trojans. In this chapter, we describe different test generation approaches for detection of hardware Trojans. We first introduce random test generation and formal methods based test generation, followed by two hybrid approaches: test generation using ATPG and model checking, test generation using concrete simulation and symbolic execution.", "num_citations": "1\n", "authors": ["1805"]}
{"title": "SoC Trust Metrics and Benchmarks\n", "abstract": " Malicious hardware modification, also known as, hardware Trojan attack, has emerged as a serious security concern for electronic systems. Such attacks compromise the basic premise of hardware root-of-trust. Over the past decade, significant research efforts have been directed to carefully analyze the trust issues arising from hardware Trojans and to protect against them. This vast body of work often needs to rely on well-defined set of trust benchmarks that can reliably evaluate the effectiveness of the protection methods. In recent past, efforts have been made to develop a benchmark suite to analyze the effectiveness of pre-silicon Trojan detection and prevention methodologies. In this chapter, we first introduce widely used benchmarks for hardware Trojan. However, there are only a limited number of Trojan-inserted benchmarks available. Moreover, there is an inherent bias as the researcher is aware of Trojan\u00a0\u2026", "num_citations": "1\n", "authors": ["1805"]}
{"title": "Trojan Detection Using Machine Learning\n", "abstract": " Machine learning techniques have the capability to explore high-dimensional feature space and find patterns that are not intuitive for analytic approaches. It is natural to apply these techniques for hardware Trojan detection to distinguish Trojan-infected designs from good designs. Almost in all aspects of hardware Trojan detection, we can tune machine learning for hardware Trojan detection. For logic testing, machine learning can help generate test vectors that are more likely to have Trojans triggered or partially activated. For side-channel analysis, machine learning can build the pattern of side-channel fingerprints of normal circuits and any outlier will be a Trojan circuit. For approaches based on structural or functional analysis, we can extract the structural or functional properties as features and train machine learning for classification. For runtime Trojan detection or monitoring, machine learning can help as long\u00a0\u2026", "num_citations": "1\n", "authors": ["1805"]}
{"title": "SoC Security Verification Using Property Checking\n", "abstract": " In this chapter, we focus on developing security properties for automatically identifying System-on-Chip (SoC) vulnerabilities as well as measuring the security of the SoC. We show how to develop a comprehensive set of security properties covering a broad set of vulnerabilities, metrics, design functionality, and security requirements to create an automatic platform for security assessments among different designs, implementations, and abstraction levels.", "num_citations": "1\n", "authors": ["1805"]}
{"title": "How to measure trust in microelectronics: The metric matters\n", "abstract": " Electronic hardware trust is an emerging concern for designers in the semiconductor industry. Trust issues in electronic hardware span all stages in its life cycle-from creation of intellectual property (IP) blocks to manufacturing, test and deployment of hardware components and all abstraction levels-from chips to printed circuit boards (PCBs) to systems. The trust issues originate from a horizontal business model that promotes reliance of third-party untrusted facilities, tools, and IPs in the hardware life cycle. Today, designers are tasked with verifying the integrity of third-party IPs before incorporating them into system-on-chip (SoC) designs. Existing trust metric frameworks have limited applicability since they are not comprehensive. They capture only a subset of vulnerabilities such as potential vulnerabilities introduced through design mistakes and CAD tools, or quantify features in a design that target a particular\u00a0\u2026", "num_citations": "1\n", "authors": ["1805"]}
{"title": "Tutorial T2B: Hardware Intellectual Property (IP) Security and Trust: Challenges and Solutions\n", "abstract": " Reusable hardware intellectual property (IP) based System-on-Chip (SoC) design has emerged as a pervasive design practice in the industry to dramatically reduce design/verification cost while meeting aggressive time-to-market constraints. Growing reliance on reusable, functionally pre-verified hardware IPs and wide array of CAD tools during SoC design-often gathered from untrusted 3rd party vendors-severely affects the security and trustworthiness of SoC computing platforms. Major security issues in the hardware IPs at different stages of SoC life cycle include piracy during IP evaluation, reverse engineering, and cloning, counterfeiting, as well as malicious, hard-to-detect hardware modifications in the hardware IPs. The global electronic piracy market is growing rapidly and is now estimated to be over $1 B/day [1], of which a significant part is related to hardware IPs. Due to evergrowing computing demands\u00a0\u2026", "num_citations": "1\n", "authors": ["1805"]}
{"title": "Validation of IP Security and Trust\n", "abstract": " System-on-chip (SoC) designers may have to outsource Intellectual Properties (IPs) to meet time-to-market constraints. The IPs may be collected from untrusted third parties and it raises major security concerns. An adversary can embed malicious components that are hard to detect. However, when the malicious components are triggered, the correct functionality of the design is deviated and critical information such as secret keys may be leaked. Therefore, it is vital to validate IPs from security aspects alongside with functionality, timing, and other requirements. In this chapter, we review the existing security validation methods for soft IP cores using a combination of simulation-based validation and formal methods.", "num_citations": "1\n", "authors": ["1805"]}
{"title": "Modeling and Specification of SoC Designs\n", "abstract": " System-level specifications are widely used to capture a wide spectrum of SoC designs. To enable early stage exploration, it is required that system-level specifications should have both formal (unambiguous) semantics and easy correlation with the architecture manual. However, most system-level specifications are still written in an informal manner. Since informal specifications are not amenable to automated analysis, there are possibilities of ambiguity, incompleteness, and contradiction, which can lead to different interpretations of specifications. This chapter introduces two of the most widely used system-level specifications: SystemC TLMs for hardware modeling, and UML activity diagrams for software modeling. To enable the automated validation, this chapter presents how to extract formal models from these specifications.", "num_citations": "1\n", "authors": ["1805"]}
{"title": "Energy-Aware Scheduling with Dynamic Voltage Scaling\n", "abstract": " Dynamic voltage scaling (DVS) [43] is widely acknowledged as one of the most effective processor energy saving techniques. The reason behind its capability to save energy is that linear reduction in the supply voltage leads to approximately linear slow down of performance while the power can be decreased quadratically. Many general as well as specific-purpose processors nowadays support DVS [60, 80, 81] with multiple available voltage levels.", "num_citations": "1\n", "authors": ["1805"]}
{"title": "Energy Optimization of Cache Hierarchy in Multicore Real-Time Systems\n", "abstract": " Computation using single-core processors has hit the power wall on its way of performance improvement. Chip multiprocessor (CMP) architectures, which integrates multiple processing units on a single integrated circuit (IC), have been widely adopted by major vendors like Intel, AMD, IBM and ARM in both general-purpose computers (e.g., [48]) and embedded systems (e.g., [2, 83]). Multicore processors are able to run multiple threads in parallel at lower power dissipation per unit of performance. Despite the inherent advantages, energy conservation is still a primary concern in multicore system optimization. While power consumption is a key concern in designing any computing devices, energy efficiency is especially critical for embedded systems. Real-time systems that run applications with timing constraints require unique considerations. Due to the ever growing demands for parallel computing, real-time\u00a0\u2026", "num_citations": "1\n", "authors": ["1805"]}
{"title": "Guest Editors' Introduction: Multicore SoC Validation with Transaction-Level Models\n", "abstract": " This issue of IEEE Design and Test presents four special-theme articles that highlight challenges and recent trends of multicore architecture validation using transaction-level models. The articles cover theoretical as well as practical aspects related to high-level validation including transaction-level modeling of multicore architectures, validation, and debug of TLM models, and industrial case studies.", "num_citations": "1\n", "authors": ["1805"]}
{"title": "Sustainable Computing: Informatics and Systems\n", "abstract": " Understanding the power dissipation behavior of an application/workload is the key to writing power-efficient software and designing energy-efficient computer systems. Power modeling based on performance monitoring counters (PMCs) is an effective approach to analyze and quantify power dissipation behaviors on a real computer system. One of the potential benefits is that software developers are able to optimize the power behavior of an application by adjusting its source code implementations. However, it is challenging to relate power dissipation to the execution of specific segments of source code directly. In addition, existing power models need to be further investigated by reconsidering multicore architecture processors with on-chip shared resources. Therefore, we need to adjust PMC-based power models from the developers\u2019 perspective, and reevaluate them on multicore computer systems. In this paper, followed by a detailed classification of previous efforts on power profiling, we propose a two-level power model that estimates per-core power dissipation on chip multiprocessor (CMP) on-thefly by using only one PMC and frequency information from CPUs. The model attempts to satisfy the basic requirements from developer point of view: simplicity and applicability. Based on this model, we design and implement SPAN, a software power analyzer, to identify power behavior associated with source code. Given an application, SPAN is able to determine its power dissipation rate at the function-block level. We evaluate both the power model and SPAN on two general purpose multicore computer systems. The experimental results based\u00a0\u2026", "num_citations": "1\n", "authors": ["1805"]}
{"title": "Coverage-driven Functional Test Generation for Processor Validation using Formal Methods\n", "abstract": " Functional validation is one of the major bottlenecks in processor design: up to 70% of the design development time and resources are spent on functional verification. Simulation is the most widely used form of microprocessor validation. A major challenge in simulation-based validation is how to reduce the overall validation time and resources. Traditionally, billions of random tests are used during simulation and the lack of a comprehensive functional coverage metric makes it difficult to measure the verification progress. Directed test generation is a promising approach in terms of the volume of test set but poses several challenges. One of the major challenges is how to generate directed tests for complex processor designs for efficient functional validation. This paper presents functional coverage-driven test generation techniques using formal methods. We present fault models based on the functionality of pipelined processors. These fault models are used to define the functional coverage metric to measure the verification progress. We have developed automatic test generation techniques using model checking and SAT solving methods. The experiments using MIPS processor demonstrate the feasibility and usefulness of the proposed functional validation methodology.", "num_citations": "1\n", "authors": ["1805"]}
{"title": "Automated micro-architectural test generation for validation of modern processors\n", "abstract": " Design complexity of todays microprocessors is in-creasing at an alarming rate to cope up with the required performance improvement by adopting complicated micro-architectural features such as deep pipelines, dynamic scheduling, out-oforder and superscalar execution, and dynamic speculation. Since verification complexity is directly proportional to the design complexity, considerable amount of time and resources are spent on design validation. In the current industrial practice, billions of random test programs generated at instruction set architecture (ISA) level are used during simulation-based validation. However, architectural test generation techniques have limitations in terms of exercising intricate micro-architectural artifacts. Therefore, it is necessary to use micro-architectural details during test generation. Furthermore, there is a lack of automated techniques for directed test generation targeting micro-architectural faults. To address these challenges, we present a directed test generation technique at micro-architectural level for functional validation of microprocessors. A processor model is described in a temporal specification language at micro-architecture level. The desired behaviors of micro-architecture mechanisms are expressed as temporal logic properties. We use decompositional model checking for systematic test generation. Our experiments using a processor based on the Power Architecture TM Technology1 shows very promising results in terms of test generation time as well as test program length.", "num_citations": "1\n", "authors": ["1805"]}
{"title": "SAT-based combinational equivalence checking\n", "abstract": " Combinational equivalence checking is one of the key components in today\u2019s hardware verification methodology. Structural similarity of the two designs are exploited by existing BDD, SAT, or ATPG based methods. This report presents a technique for improving the performance of the existing SAT-based combinational equivalence checkers by adding new constraints based on the structural similarity. Our experimental results using ISCAS benchmarks demonstrate the usefulness of this approach.", "num_citations": "1\n", "authors": ["1805"]}