{"title": "Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems\n", "abstract": " MXNet is a multi-language machine learning (ML) library to ease the development of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is computation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters. This paper describes both the API design and the system implementation of MXNet, and explains how embedding of both symbolic expression and tensor operation is handled in a unified fashion. Our preliminary experiments reveal promising results on large scale deep neural network applications using multiple GPU machines.", "num_citations": "1951\n", "authors": ["1817"]}
{"title": "The application of two-level attention models in deep convolutional neural network for fine-grained image classification\n", "abstract": " Fine-grained classification is challenging because categories can only be discriminated by subtle and local differences. Variances in the pose, scale or rotation usually make the problem more difficult. Most fine-grained classification systems follow the pipeline of finding foreground object or object parts (where) to extract discriminative features (what). In this paper, we propose to apply visual attention to fine-grained classification task using deep neural network. Our pipeline integrates three types of attention: the bottom-up attention that propose candidate patches, the object-level top-down attention that selects relevant patches to a certain object, and the part-level top-down attention that localizes discriminative parts. We combine these attentions to train domain-specific deep nets, then use it to improve both the what and where aspects. Importantly, we avoid using expensive annotations like bounding box or part information from end-to-end. The weak supervision constraint makes our work easier to generalize. We have verified the effectiveness of the method on the subsets of ILSVRC2012 dataset and CUB200_2011 dataset. Our pipeline delivered significant improvements and achieved the best accuracy under the weakest supervision condition. The performance is competitive against other methods that rely on additional annotations.", "num_citations": "708\n", "authors": ["1817"]}
{"title": "Corey: An Operating System for Many Cores.\n", "abstract": " Multiprocessor application performance can be limited by the operating system when the application uses the operating system frequently and the operating system services use data structures shared and modified by multiple processing cores. If the application does not need the sharing, then the operating system will become an unnecessary bottleneck to the application\u2019s performance. This paper argues that applications should control sharing: the kernel should arrange each data structure so that only a single processor need update it, unless directed otherwise by the application. Guided by this design principle, this paper proposes three operating system abstractions (address ranges, kernel cores, and shares) that allow applications to control inter-core sharing and to take advantage of the likely abundance of cores by dedicating cores to specific operating system functions.Measurements of microbenchmarks on the Corey prototype operating system, which embodies the new abstractions, show how control over sharing can improve performance. Application benchmarks, using MapReduce and a Web server, show that the improvements can be significant for overall performance: MapReduce on Corey performs 25% faster than on Linux when using 16 cores. Hardware event counters confirm that these improvements are due to avoiding operations that are expensive on multicore machines.", "num_citations": "527\n", "authors": ["1817"]}
{"title": "ReVive: cost-effective architectural support for rollback recovery in shared-memory multiprocessors\n", "abstract": " This paper presents ReVive, a novel general-purpose rollback recovery mechanism for shared-memory multiprocessors. ReVive carefully balances the conflicting requirements of availability, performance, and hardware cost. ReVive performs checkpointing, logging, and distributed parity protection, all memory-based. It enables recovery from a wide class of errors, including the permanent loss of an entire node. To maintain high performance, ReVive includes specialized hardware that performs frequent operations in the background, such as log and parity updates. To keep the cost low, more complex checkpointing and recovery functions are performed in software, while the hardware modifications are limited to the directory controllers of the machine. Our simulation results on a 16-processor system indicate that the average error-free execution time overhead of using ReVive is only 6.3%, while the achieved\u00a0\u2026", "num_citations": "344\n", "authors": ["1817"]}
{"title": "Deep graph library: Towards efficient and scalable deep learning on graphs\n", "abstract": " Accelerating research in the emerging field of deep graph learning requires new tools. Such systems should support graph as the core abstraction and take care to maintain both forward (ie supporting new research ideas) and backward (ie integration with existing components) compatibility. In this paper, we present Deep Graph Library (DGL). DGL enables arbitrary message handling and mutation operators, flexible propagation rules, and is framework agnostic so as to leverage high-performance tensor, autograd operations, and other feature extraction modules already available in existing frameworks. DGL carefully handles the sparse and irregular graph structure, deals with graphs big and small which may change dynamically, fuses operations, and performs auto-batching, all to take advantages of modern hardware. DGL has been tested on a variety of models, including but not limited to the popular Graph\u00a0\u2026", "num_citations": "292\n", "authors": ["1817"]}
{"title": "Building topology-aware overlays using global soft-state\n", "abstract": " Distributed hash table (DHT) based overlay networks offer an administration-free and fault-tolerant storage space that maps \"keys\" to \"values\". For these systems to function efficiently, their structures must fit that of the underlying network. Existing techniques for discovering network proximity information, such as landmark clustering and expanding-ring search are either inaccurate or expensive. The lack of global proximity information in overlay construction and maintenance can result in bad proximity approximation or excessive communication. To address these problems, we propose the following: (1) Combining landmark clustering and round-trip time (RTT) measurements to generate proximity information, achieving both efficiency and accuracy. (2) Controlled placement of global proximity information on the system itself as soft-state, such that nodes can independently access relevant information efficiently. (3\u00a0\u2026", "num_citations": "261\n", "authors": ["1817"]}
{"title": "System and method for a distributed object store\n", "abstract": " An improved system and method for flexible object placement and soft-state indexing of objects in a distributed object store is provided. A distributed object store may be provided by a large number of system nodes operably coupled to a network. A system node provided may include an access module for communicating with a client, an index module for building an index of a replicated data object, a data module for storing a data object on a computer readable medium, and a membership and routing module for detecting the configuration of operable nodes in the distributed system. Upon failure of an index node, the failure may be detected at other nodes, including those nodes that store the replicas of the object. These nodes may then send new index rebuilding requests to a different node that may rebuild the index for servicing any access request to the object.", "num_citations": "245\n", "authors": ["1817"]}
{"title": "Timestream: Reliable stream computation in the cloud\n", "abstract": " TimeStream is a distributed system designed specifically for low-latency continuous processing of big streaming data on a large cluster of commodity machines. The unique characteristics of this emerging application domain have led to a significantly different design from the popular MapReduce-style batch data processing. In particular, we advocate a powerful new abstraction called resilient substitution that caters to the specific needs in this new computation model to handle failure recovery and dynamic reconfiguration in response to load changes. Several real-world applications running on our prototype have been shown to scale robustly with low latency while at the same time maintaining the simple and concise declarative programming model. TimeStream handles an on-line advertising aggregation pipeline at a rate of 700,000 URLs per second with a 2-second delay, while performing sentiment analysis of\u00a0\u2026", "num_citations": "240\n", "authors": ["1817"]}
{"title": "Strider: A black-box, state-based approach to change and configuration management and support\n", "abstract": " We describe a new approach, called Strider, to Change and Configuration Management and Support\u00a0(CCMS). Strider is a black-box approach: without relying on specifications, it uses state differencing to identify potential causes of differing program behaviors, uses state tracing to identify actual, run-time state dependencies, and uses statistical behavior modeling for noise filtering. Strider is a state-based approach: instead of linking vague, high level descriptions and symptoms to relevant actions, it models management and support problems in terms of individual, named pieces of low level configuration state and provides precise mappings to user-friendly information through a computer genomics database. We use troubleshooting of configuration failures to demonstrate that the Strider approach reduces problem complexity by several orders of magnitude, making root-cause analysis possible.", "num_citations": "237\n", "authors": ["1817"]}
{"title": "An empirical study of collusion behavior in the Maze P2P file-sharing system\n", "abstract": " Peer-to-peer networks often use incentive policies to encourage cooperation between nodes. Such systems are generally susceptible to collusion by groups of users in order to gain unfair advantages over others. While techniques have been proposed to combat Web spam collusion, there are few measurements of real collusion in deployed systems. In this paper, we report analysis and measurement results of user collusion in Maze, a large-scale peer-to-peer file sharing system with a non-net-zero point-based incentive policy. We search for colluding behavior by examining complete user logs, and incrementally refine a set of collusion detectors to identify common collusion patterns. We find collusion patterns similar to those found in Web spamming. We evaluate how proposed reputation systems would perform on the Maze system. Our results can help guide the design of more robust incentive schemes.", "num_citations": "235\n", "authors": ["1817"]}
{"title": "Error-Driven Incremental Learning in Deep Convolutional Neural Network for Large-Scale Image Classification\n", "abstract": " Supervised learning using deep convolutional neural network has shown its promise in large-scale image classification task. As a building block, it is now well positioned to be part of a larger system that tackles real-life multimedia tasks. An unresolved issue is that such model is trained on a static snapshot of data. Instead, this paper positions the training as a continuous learning process as new classes of data arrive. A system with such capability is useful in practical scenarios, as it gradually expands its capacity to predict increasing number of new classes. It is also our attempt to address the more fundamental issue: a good learning system must deal with new knowledge that it is exposed to, much as how human do.", "num_citations": "228\n", "authors": ["1817"]}
{"title": "Automated known problem diagnosis with event traces\n", "abstract": " Computer problem diagnosis remains a serious challenge to users and support professionals. Traditional troubleshooting methods relying heavily on human intervention make the process inefficient and the results inaccurate even for solved problems, which contribute significantly to user's dissatisfaction. We propose to use system behavior information such as system event traces to build correlations with solved problems, instead of using only vague text descriptions as in existing practices. The goal is to enable automatic identification of the root cause of a problem if it is a known one, which would further lead to its resolution. By applying statistical learning techniques to classifying system call sequences, we show our approach can achieve considerable accuracy of root cause recognition by studying four case examples.", "num_citations": "222\n", "authors": ["1817"]}
{"title": "R2: An application-level kernel for record and replay\n", "abstract": " Library-based record and replay tools aim to reproduce an application\u2019s execution by recording the results of selected functions in a log and during replay returning the results from the log rather than executing the functions. These tools must ensure that a replay run is identical to the record run. The challenge in doing so is that only invocations of a function by the application should be recorded, recording the side effects of a function call can be difficult, and not executing function calls during replay, multithreading, and the presence of the tool may change the application\u2019s behavior from recording to replay. These problems have limited the use of such tools. R2 allows developers to choose functions that can be recorded and replayed correctly. Developers annotate the chosen functions with simple keywords so that R2 can handle calls with side effects and multithreading. R2 generates code for record and replay from templates, allowing developers to avoid implementing stubs for hundreds of functions manually. To track whether an invocation is on behalf of the application or the implementation of a selected function, R2 maintains a mode bit, which stubs save and restore.We have implemented R2 on Windows and annotated large parts (1,300 functions) of the Win32 API, and two higher-level interfaces (MPI and SQLite). R2 can replay multithreaded web and database servers that previous library-based tools cannot replay. By allowing developers to choose high-level interfaces, R2 can also keep recording overhead small; experiments show that its recording overhead for Apache is approximately 10%, that recording and replaying at the SQLite\u00a0\u2026", "num_citations": "217\n", "authors": ["1817"]}
{"title": "D3S: Debugging Deployed Distributed Systems.\n", "abstract": " Testing large-scale distributed systems is a challenge, because some errors manifest themselves only after a distributed sequence of events that involves machine and network failures. D  S is a checker that allows developers to specify predicates on distributed properties of a deployed system, and that checks these predicates while the system is running. When D  S finds a problem it produces the sequence of state changes that led to the problem, allowing developers to quickly find the root cause.", "num_citations": "201\n", "authors": ["1817"]}
{"title": "Logical volume-level migration in a partition-based distributed file system\n", "abstract": " Method and system for migrating a logical volumes in a distributed file system having multiple partitions servers. Each partition server owns one or more volumes, and each volume including a subset of logically related objects of the file system. In response to a migration request that requests migration of a selected logical volume from a source partition server to the target partition server, an ownership map is updated to indicate the selected logical volume is owned by the target partition server. At the source partition server, forwarding information is stored to reference the target partition server. When the source partition server receives a request for access to the object, the forwarding information is transmitted to the requester.", "num_citations": "194\n", "authors": ["1817"]}
{"title": "Multiple granularity descriptors for fine-grained categorization\n", "abstract": " Fine-grained categorization, which aims to distinguish subordinate-level categories such as bird species or dog breeds, is an extremely challenging task. This is due to two main issues: how to localize discriminative regions for recognition and how to learn sophisticated features for representation. Neither of them is easy to handle if there is insufficient labeled data. We leverage the fact that a subordinate-level object already has other labels in its ontology tree. These\" free\" labels can be used to train a series of CNN-based classifiers, each specialized at one grain level. The internal representations of these networks have different region of interests, allowing the construction of multi-grained descriptors that encode informative and discriminative features covering all the grain levels. Our multiple granularity framework can be learned with the weakest supervision, requiring only image-level label and avoiding the use of labor-intensive bounding box or part annotations. Experimental results on three challenging fine-grained image datasets demonstrate that our approach outperforms state-of-the-art algorithms, including those requiring strong labels.", "num_citations": "185\n", "authors": ["1817"]}
{"title": "Updating references to a migrated object in a partition-based distributed file system\n", "abstract": " Method and arrangement for updating references to a migrated object in a distributed file system. A migrated object is an object has moved from a source partition server to a target partition server. Each object has an associated parent object and each partition server owns a subset of objects of the file system. A set of forward pointers that reference one or more child objects of the parent object is maintained in each parent object. In the migrated object a set of back pointers is maintained that reference one or more parent objects having forward pointers to the migrated object. To each partition server that owns a parent object of the migrated object, an update request is transmitted. The forward pointers in each parent object are updated in response to the update request.", "num_citations": "138\n", "authors": ["1817"]}
{"title": "The memory performance of DSS commercial workloads in shared-memory multiprocessors\n", "abstract": " Although cache-coherent shared-memory multiprocessors are often used to run commercial workloads, little work has been done to characterize how well these machines support such workloads. In particular, we do not have much insight into the demands of commercial workloads on the memory subsystem of these machines. In this paper, we analyze in detail the memory access patterns of several queries that are representative of Decision Support System (DSS) databases. Our analysis shows that the memory use of queries differs largely depending on how the queries access the database data, namely via indices or by sequentially scanning the records. The former queries, which we call Index queries, suffer most of their shared-data misses on indices and on lock-related metadata structures. The latter queries, which we call Sequential queries, suffer most of their shared-data misses on the database records as\u00a0\u2026", "num_citations": "134\n", "authors": ["1817"]}
{"title": "An empirical study of free-riding behavior in the Maze P2P file-sharing system\n", "abstract": " Maze is a P2P file-sharing system with an active and large user base. It is developed, deployed and operated by an academic research team. As such, it offers ample opportunities to conduct experiments to under-stand user behavior. Embedded in Maze is a set of incentive policies designed to encourage sharing and contribution. This paper presents an in-depth analysis of the effectiveness of the incentive policies and how users react to them. We found that in general the policies have been effective. But they also encourage the more selfish users to cheat by whitewashing their ac-counts as a variation of Sybil attack. We examine multiple factors that may contribute to the free-riding behavior. Our conclusions are that upload speed, NAT and amount of shared files are not the problems, and selfish behavior is demonstrated more by shorter online time. Since free-riders are also avid consumers of popular files\u00a0\u2026", "num_citations": "132\n", "authors": ["1817"]}
{"title": "On the impact of replica placement to the reliability of distributed brick storage systems\n", "abstract": " Data reliability of distributed brick storage systems critically depends on the replica placement policy, and the two governing forces are repair speed and sensitivity to multiple concurrent failures. In this paper, the authors provided an analytical framework to reason and quantify the impact of replica placement policy to system reliability. The novelty of the framework is its consideration of the bounded network bandwidth for data maintenance. The framework was applied to two popular schemes, namely sequential placement and random placement, and showed that both have drawbacks that significantly degrade data reliability. Then the stripe placement scheme was proposed and find the near-optimal configuration parameter such that it provides much better reliability. The possibility of addressing the problem of correlated brick failures in the analytical framework was further discussed", "num_citations": "130\n", "authors": ["1817"]}
{"title": "SOMO: Self-organized metadata overlay for resource management in P2P DHT\n", "abstract": " In this paper, we first describe the concept of data overlay, which is a mechanism to implement arbitrary data structure on top of any structured P2P DHT. With this abstraction, we developed a highly scalable, efficient and robust infrastructure, called SOMO, to perform resource management for P2P DHT. It does so by gathering and disseminating system metadata in O(logN) time with a self-organizing and self-healing data overlay. Our preliminary results of using SOMO to balance routing traffic with node capacities in a prefix-based overlay have demonstrated the utility of data overlay as well as the potential of SOMO.", "num_citations": "127\n", "authors": ["1817"]}
{"title": "A Measurement Study of a Peer-to-Peer Video-on-Demand System.\n", "abstract": " Despite strong interest in P2P video-on-demand (VoD) services, existing studies are mostly based on simulation and focus on areas such as overlay topology. Little is known about the effectiveness of P2P in VoD systems and the end user experience. In this paper we present a comprehensive study of these issues using the two-month logs from a deployed experimental P2P VoD system over CERNET1. Our key findings are:(1) the key factor is the popularity of channels and a moderate number of concurrent users can derive satisfactory user experience. However, good network bandwidth at peers and adequate server provisioning are critical.(2) a simple prefetching algorithm can be effective to improve random seeks. Overall, we believe that it is feasible to provide a costeffective P2P VoD service with acceptable user experience, and there is a fundamental tradeoff between good experience and system scalability.", "num_citations": "117\n", "authors": ["1817"]}
{"title": "WiDS Checker: Combating Bugs in Distributed Systems.\n", "abstract": " Despite many efforts, the predominant practice of debugging a distributed system is still printf-based log mining, which is both tedious and error-prone. In this paper, we present WiDS Checker, a unified framework that can check distributed systems through both simulation and reproduced runs from real deployment. All instances of a distributed system can be executed within one simulation process, multiplexed properly to observe the \u201chappensbefore\u201d relationship, thus accurately reveal full system state. A versatile script language allows a developer to refine system properties into straightforward assertions, which the checker inspects for violations. Combining these two components, we are able to check distributed properties that are otherwise impossible to check. We applied WiDS Checker over a suite of complex and real systems and found non-trivial bugs, including one in a previously proven Paxos specification. Our experience demonstrates the usefulness of the checker and allows us to gain insights beneficial to future research in this area.", "num_citations": "116\n", "authors": ["1817"]}
{"title": "Building low-maintenance expressways for p2p systems\n", "abstract": " Recent P2P systems, represented by Oceanstore and PAST, offer an administration-free and fault-tolerant storage utility. Nodes in these systems collectively contribute towards a storage space, in a self-organizing fashion. While elegant from a theoretical perspective, they can be improved in three important areas:(i) low maintenance cost;(ii) the ability to make discriminative use of the nodes in the system that has different capacity and resource constraints;(iii) the ability to adapt to the underlying network conditions and the applications\u2019 needs.In this paper, we explore \u201cexpressways\u201d as an auxiliary mechanism to deliver high routing performance, leaving the baseline infrastructure concentrating on tasks such as efficient resource utilization and simple management. The basic ideas of our proposal are to divide the total space in the overlay network into topology areas of different spans; we then select candidates in these areas to collectively construct a balanced, self-organized expressway system. By archiving the relevant system information (eg, physical coordinates of the nodes) as objects stored on the system itself, we can further tune the expressway dynamically to fit both physical network conditions and application needs.", "num_citations": "115\n", "authors": ["1817"]}
{"title": "Scale-Invariant Convolutional Neural Networks\n", "abstract": " Even though convolutional neural networks (CNN) has achieved near-human performance in various computer vision tasks, its ability to tolerate scale variations is limited. The popular practise is making the model bigger first, and then train it with data augmentation using extensive scale-jittering. In this paper, we propose a scaleinvariant convolutional neural network (SiCNN), a modeldesigned to incorporate multi-scale feature exaction and classification into the network structure. SiCNN uses a multi-column architecture, with each column focusing on a particular scale. Unlike previous multi-column strategies, these columns share the same set of filter parameters by a scale transformation among them. This design deals with scale variation without blowing up the model size. Experimental results show that SiCNN detects features at various scales, and the classification result exhibits strong robustness against object scale variations.", "num_citations": "114\n", "authors": ["1817"]}
{"title": "Practical defenses against BGP prefix hijacking\n", "abstract": " Prefix hijacking, a misbehavior in which a misconfigured or malicious BGP router originates an IP prefix that the router does not own, is becoming an increasingly serious security problem on the Internet. In this paper, we conduct a first comprehensive study on incrementally deployable mitigation solutions against prefix hijacking. We first propose a novel reactive detection-assisted solution based on the idea of bogus route purging and valid route promotion. Our simulations based on realistic settings show that purging bogus routes at 20 highest-degree ASes reduces the polluted portion of the Internet by a random prefix hijack from 50% down to 24%, and adding promotion further reduces the remaining pollution by 33%~ 57%, We prove that our proposed route purging and promotion scheme preserve the convergence properties of BGP regardless of the number of promoters. We are the first to demonstrate that\u00a0\u2026", "num_citations": "101\n", "authors": ["1817"]}
{"title": "Speeding up irregular applications in shared-memory multiprocessors: Memory binding and group prefetching\n", "abstract": " While many parallel applications exhibit good spatial locality, other important codes in areas like graph problem-solving or CAD do not. Often, these irregular codes contain small records accessed via pointers. Consequently, while the former applications benefit from long cache lines, the latter prefer short lines. One good solution is to combine short lines with prefetching. In this way, each application can exploit the amount of spatial locality that it has. However, prefetching, if provided, should also work for the irregular codes. This paper presents a new prefetching scheme that, while usable by regular applications, is specifically targeted to irregular ones: memory binding and group prefetching. The idea is to hardware-bind and prefetch together groups of data that the programmer suggests are strongly related to each other. Examples are the different fields in a record or two records linked by a permanent pointer. This\u00a0\u2026", "num_citations": "101\n", "authors": ["1817"]}
{"title": "Robust incentives via multi\u2010level Tit\u2010for\u2010Tat\n", "abstract": " Much work has been done to address the need for incentive models in real deployed peer\u2010to\u2010peer networks. In this paper, we discuss problems found with the incentive model in a large, deployed peer\u2010to\u2010peer network, Maze. We evaluate several alternatives, and propose an incentive system that generates preferences for well\u2010behaved nodes while correctly punishing colluders. We discuss our proposal as a hybrid between Tit\u2010for\u2010Tat and EigenTrust, and show its effectiveness through simulation of real traces of the Maze system. Copyright \u00a9 2007 John Wiley & Sons, Ltd.", "num_citations": "97\n", "authors": ["1817"]}
{"title": "GridCast: Improving peer sharing for P2P VoD\n", "abstract": " Video-on-Demand (VoD) is a compelling application, but costly. VoD is costly due to the load it places on video source servers. Many have proposed using peer-to-peer (P2P) techniques to shift load from servers to peers. Yet, nobody has implemented and deployed a system to openly and systematically evaluate how these techniques work. This article describes the design, implementation and evaluation of GridCast, a real deployed P2P VoD system. GridCast has been live on CERNET since May of 2006. It provides seek, pause, and play operations, and employs peer sharing to improve system scalability. In peak months, GridCast has served videos to 23,000 unique users. From the first deployment, we have gathered information to understand the system and evaluate how to further improve peer sharing through caching and replication. We first show that GridCast with single video caching (SVC) can decrease\u00a0\u2026", "num_citations": "96\n", "authors": ["1817"]}
{"title": "Object-level migration in a partition-based distributed file system\n", "abstract": " Method and system for moving an object from a source partition server to a target partition server in a distributed file system having multiple partition servers. Each object has at least one associated and linked parent object, and each partition server owns a subset of objects of the file system. The object migration is accomplished by creating a copy in a target partition server of a selected object from a source partition server. At the source partition server, a forwarding link is created that references the copy in the target partition server. The copy is linked to the parent object of the selected object, and requests for access to the selected object are responded to with the forwarding link.", "num_citations": "93\n", "authors": ["1817"]}
{"title": "MadLINQ: large-scale distributed matrix computation for the cloud\n", "abstract": " The computation core of many data-intensive applications can be best expressed as matrix computations. The MadLINQ project addresses the following two important research problems: the need for a highly scalable, efficient and fault-tolerant matrix computation system that is also easy to program, and the seamless integration of such specialized execution engines in a general purpose data-parallel computing system.", "num_citations": "90\n", "authors": ["1817"]}
{"title": "Reducing remote conflict misses: NUMA with remote cache versus COMA\n", "abstract": " Many future applications for scalable shared-memory multiprocessors are likely to have large working sets that overflow secondary or tertiary caches. Two possible solutions to this problem are to add a very large cache called remote cache that caches remote data (NUMA-RC), or organize the machine as a cache-only memory architecture (COMA). This paper tries to determine which solution is best. To compare the performance of the two organizations for the same amount of total memory, we introduce a model of data sharing. The model uses three data sharing patterns: replication, read-mostly migration, and read-writs migration. Replication data is accessed in read-mostly mode by several processors, while migration data is accessed largely by one processor at a time. For large working sets, the weight of the migration data largely determines whether COMA outperforms NUMA-RC. Ideally, COMA only needs to\u00a0\u2026", "num_citations": "84\n", "authors": ["1817"]}
{"title": "Towards cinematic internet video-on-demand\n", "abstract": " Video-on-demand (VoD) is increasingly popular with Internet users. It gives users greater choice and more control than live streaming or file downloading. Systems such as MSN Video and YouTube deliver content at low bitrates. This may suit short clips, but great films and 5-minute bloopers are as different as symphonies and jingles. For cinema, poor quality and high jitter are less acceptable. Combining user control with high bitrate is compelling, but technically challenging. VoD is expensive due to the load it places on video source servers. Many researchers have proposed using peer-to-peer (P2P) techniques to shift load from sources to peers (peer-assistance), yet none have implemented and deployed a system with the first purpose of openly and systematically evaluating this approach. To fill this void, we have built and deployed GridCast1. GridCast doubles the bitrates of current popular internet VoD systems\u00a0\u2026", "num_citations": "80\n", "authors": ["1817"]}
{"title": "MPIWiz: Subgroup reproducible replay of MPI applications\n", "abstract": " Message Passing Interface (MPI) is a widely used standard for managing coarse-grained concurrency on distributed computers. Debugging parallel MPI applications, however, has always been a particularly challenging task due to their high degree of concurrent execution and non-deterministic behavior. Deterministic replay is a potentially powerful technique for addressing these challenges, with existing MPI replay tools adopting either data-replay or order-replay approaches. Unfortunately, each approach has its tradeoffs. Data-replay generates substantial log sizes by recording every communication message. Order-replay generates small logs, but requires all processes to be replayed together. We believe that these drawbacks are the primary reasons that inhibit the wide adoption of deterministic replay as the critical enabler of cyclic debugging of MPI applications.", "num_citations": "75\n", "authors": ["1817"]}
{"title": "Evaluation of edge caching/off loading for dynamic content delivery\n", "abstract": " As dynamic content becomes increasingly dominant, it becomes an important research topic as how the edge resources such as client-side proxies, which are otherwise underutilized for such content, can be put into use. However, it is unclear what will be the best strategy, and the design/deployment trade offs lie therein. In this paper, using one representative e-commerce benchmark, we report our experience of an extensive investigation of different offloading and caching options. Our results point out that, while great benefits can be reached in general, advanced offloading strategies can be overly complex and even counterproductive. In contrast, simple augmentation at proxies to enable fragment caching and page composition achieves most of the benefit without compromising important considerations such as security. We also present proxy+ architecture which supports such capabilities for existing Web\u00a0\u2026", "num_citations": "73\n", "authors": ["1817"]}
{"title": "BitVault: A highly reliable distributed data retention platform\n", "abstract": " This paper summarizes our experience designing and implementing BitVault: a content-addressable retention platform for large volumes of reference data -- seldom-changing information that needs to be retained for a long time. BitVault uses \"smart bricks\" as the building block to lower the hardware cost. The challenges are to keep management costs low in a system that scales from one brick to tens of thousands, to ensure reliability, and to deliver a simple design. Our design incorporates peer-to-peer (P2P) technologies for self-managing and self-healing and uses massively parallel repair to reduce system vulnerability to data loss. The simplicity of the architecture relies on an eventually reliable membership service provided by a perfect one-hop distributed hash table (DHT). Its object-driven repair model yields last-replica recall guarantee independent of the failure scenario. So long as the last copy of a data\u00a0\u2026", "num_citations": "72\n", "authors": ["1817"]}
{"title": "Routing in peer-to-peer networks\n", "abstract": " Routing in a peer-to-peer network is described. In an implementation, a method includes receiving at one of a plurality of nodes in a peer-to-peer network, an indication of a change in membership in the peer-to-peer network by another node in the peer-to-peer network. A report is broadcast that describes the change. The report is for receipt by each node referenced in a routing table included in the one node.", "num_citations": "71\n", "authors": ["1817"]}
{"title": "Deployment of a Large-scale Peer-to-Peer Social Network.\n", "abstract": " We present the design and architecture of the Maze file-sharing and social network. Maze is one of the first large-scale deployments of an academic research project, with over 210,000 registered users and more than 10,000 users online at any time, sharing over 140 million files. Maze includes an evolving incentive structure, and simple mechanisms for providing network locality. We outline the Maze architecture and describe initial results from a measurement study.", "num_citations": "70\n", "authors": ["1817"]}
{"title": "Namespace service in a distributed file system using a database management system\n", "abstract": " Namespace service in a distributed file system using a database management system. A namespace database is configured on a namespace server with namespace identifiers and associated file location information. The namespace server is separate from the data servers in the distributed file system. A client proxy arrangement interfaces with client applications and with the namespace server to obtain from the namespace server location information associated with files referenced in file access requests and submit storage access requests to the appropriate data servers. The separate namespace server and data servers enhances scalability of the distributed file system.", "num_citations": "70\n", "authors": ["1817"]}
{"title": "Optimization of traffic routing for data center services\n", "abstract": " Techniques and systems for providing optimization of traffic routing for data centers are disclosed herein. In some embodiments, a method may include identifying a plurality of available source sites and paths or routes between an online service provider (OSP) and a destination prefix. A traffic manager may measure a cost for each of the plurality of available paths. In some instances, the traffic manager may also measure a performance value for each of the available paths. The traffic manager may then select one of the available source sites and paths as a preferred source site and path that has a minimized cost for an instance of the performance value when compared to other paths of the plurality of available source sites and paths. In further embodiments, the traffic manager may rewrite a routing table and DNS mapping to implement the preferred source site and path as the default source site and path between\u00a0\u2026", "num_citations": "68\n", "authors": ["1817"]}
{"title": "A practical distributed mutual exclusion protocol in dynamic peer-to-peer systems\n", "abstract": " Mutual exclusion is one of the well-studied fundamental primitives in distributed systems. However, the emerging P2P systems bring forward several challenges that can\u2019t be completely solved by previous approaches. In this paper, we propose the Sigma protocol that is implemented inside a dynamic P2P DHT and circumvents those issues. The basic idea is to adopt queuing and cooperation between clients and replicas so as to enforce quorum consensus scheme. We demonstrate that this protocol is scalable with system size, robust to contention, and resilient to network latency variance and fault-tolerant.", "num_citations": "68\n", "authors": ["1817"]}
{"title": "Data overlay, self-organized metadata overlay, and associated methods\n", "abstract": " A data overlay is described which is built as a data structure on top of a logical space defined by a distributed hash table (DHT) in a peer-to-peer (P2P) network environment. The data overlay can assume a topology of a tree having a plurality of tree nodes. Each of the tree nodes has a zone allocated to it that maps to a corresponding zone associated with a DHT node in the logical space of the DHT. Procedures are described for \u201cgrowing\u201d this tree structure, such that the tree structure is self-organizing and self-healing on the same scale as the underlying DHT. Other procedures are described for using the tree structure to gather information from the DHT nodes and to disseminate information to the DHT nodes.", "num_citations": "66\n", "authors": ["1817"]}
{"title": "Reperasure: Replication protocol using erasure-code in peer-to-peer storage network\n", "abstract": " Peer-to-peer overlay networks offer a convenient way to host an infrastructure that can scale to the size of the Internet and yet stay manageable. These overlays are essentially self-organizing distributed hash tables (DHT). The dynamic nature of the system, however, poses serious challenges of data reliability. Furthermore, in order to see wider adoption, it is time to design support for generic replication mechanisms capable of handling arbitrary update requests - most of the existing proposals are deep archival systems in nature. Utilizing the fact that DHT can function as a super-reliable and high performance disk when data stored inside are erasure coded, we believe practical and simple protocols can be designed. In this paper, we introduce the reperasure protocol, a layer on top of the basic DHT, which efficiently supports strong consistency semantic with high availability guarantee. By relieving the DHT layer out\u00a0\u2026", "num_citations": "61\n", "authors": ["1817"]}
{"title": "First step toward model-free, anonymous object tracking with recurrent neural networks\n", "abstract": " In this paper, we propose and study a novel visual object tracking approach based on convolutional networks and recurrent networks. The proposed approach is distinct from the existing approaches to visual object tracking, such as filtering-based ones and tracking-by-detection ones, in the sense that the tracking system is explicitly trained off-line to track anonymous objects in a noisy environment. The proposed visual tracking model is end-to-end trainable, minimizing any adversarial effect from mismatches in object representation and between the true underlying dynamics and learning dynamics. We empirically show that the proposed tracking approach works well in various scenarios by generating artificial video sequences with varying conditions; the number of objects, amount of noise and the match between the training shapes and test shapes.", "num_citations": "60\n", "authors": ["1817"]}
{"title": "Attentional Neural Network: Feature Selection Using Cognitive Feedback\n", "abstract": " Attentional Neural Network is a new framework that integrates top-down cognitive bias and bottom-up feature extraction in one coherent architecture. The top-down influence is especially effective when dealing with high noise or difficult segmentation problems. Our system is modular and extensible. It is also easy to train and cheap to run, and yet can accommodate complex behaviors. We obtain classification accuracy better than or competitive with state of art results on the MNIST variation dataset, and successfully disentangle overlaid digits with high success rates. We view such a general purpose framework as an essential foundation for a larger system emulating the cognitive abilities of the whole brain.", "num_citations": "56\n", "authors": ["1817"]}
{"title": "Designing a Robust Namespace for Distributed File Services\n", "abstract": " A number of ongoing research projects follow a partition-based approach to provide highly scalable distributed storage services. These systems maintain namespaces that reference objects distributed across multiple locations in the system. Typically, atomic commitment protocols, such as 2-phase commit, are used for updating the namespace, in order to guarantee its consistency even in the presence of failures. Atomic commitment protocols are known to impose a high overhead to failure-free execution. Furthermore, they use conservative recovery procedures and may considerably restrict the concurrency of overlapping operations in the system. This paper proposes a set of new protocols implementing the fundamental operations in a distributed namespace. The protocols impose a minimal overhead to failure-free execution. They are robust against both communication and host failures, and use aggressive\u00a0\u2026", "num_citations": "56\n", "authors": ["1817"]}
{"title": "Namespace management in a distributed file system\n", "abstract": " Method and system for performing a namespace operation in a distributed file system. The file system is disposed on a plurality of partition servers, and each partition server controls access to a subset of hierarchically-related, shared storage objects. Each namespace operation involves a namespace object and a target object that are part of the shared storage objects. Namespace operations received at each partition server are serialized. In response to an unlink namespace operation, a reference in the namespace object to the target object is removed, and after removal the target object is modified in accordance with the unlink operation. In response to a link operation, the target object is modified consistent with the link operation. After modification of the target object, a reference to the target object is inserted in the namespace object.", "num_citations": "55\n", "authors": ["1817"]}
{"title": "Namespace management in a distributed file system\n", "abstract": " Method and system for performing a namespace operation in a distributed file system. The file system is disposed on a plurality of partition servers, and each partition server controls access to a subset of hierarchically-related, shared storage objects. Each namespace operation involves a namespace object and a target object that are part of the shared storage objects. Namespace operations received at each partition server are serialized. In response to an unlink namespace operation, a reference in the namespace object to the target object is removed, and after removal the target object is modified in accordance with the unlink operation. In response to a link operation, the target object is modified consistent with the link operation. After modification of the target object, a reference to the target object is inserted in the namespace object. A log record is stored in association with each namespace operation when the\u00a0\u2026", "num_citations": "55\n", "authors": ["1817"]}
{"title": "Cluster storage collection based data management\n", "abstract": " Cluster storage collection-based data management is described. In one aspect, and in a distributed system for storing data across a network to multiple data storage nodes, a bounded bandwidth available for data repair in the distributed system is determined. A specific number of stripes are then created on each data storage node of the multiple data storage nodes. The stripes are for placement and replication of data objects across respective ones of the data storage nodes. The specific number of stripes created on each data storage node is a function of the determined bounded data repair bandwidth.", "num_citations": "54\n", "authors": ["1817"]}
{"title": "An Architecture for Scalable and Manageable File Services\n", "abstract": " Monolithic file servers are limited by the power of an individual system. Cluster file servers are limited by resource sharing and recovery issues as the number of cluster nodes increases. DiFFS is a file service architecture that allows system resources to be added (or removed) dynamically, eg, storage and processors. Resources are partitioned in such a way that contention is avoided, while maintaining a single namespace. Resources may be heterogeneous, and geographically dispersed.This architecture has several advantages. A file\u2019s physical location is decoupled from its location in the namespace. This decoupling enables a powerful and flexible mechanism for the placement of file system objects. For example, different types of files, eg, text or video, may reside anywhere in the namespace while being hosted by servers best suited to handling their content type. DiFFS also provides lightweight protocols for online dynamic reconfiguration (volume reassignment and object migration) to address fluctuating demand and potentially mobile file system entities. A DiFFS prototype has been implemented in Linux. Performance results indicate that the architecture achieves its flexibility and scalability goals without sacrificing performance.", "num_citations": "52\n", "authors": ["1817"]}
{"title": "Collection-based object replication\n", "abstract": " Collection-based object replication for a system that includes a client computing device (client) connected to a server and multiple data storage nodes. In certain cases, a data storage node generates a replica of multiple replicas of a collection. The collection is a unit of data placement, access, replication, and repair. Other data storage nodes are also configured with a respective replica of the multiple replicas. The data storage node verifies whether an object received directly from the client for storage in the collection has been fully replicated by the other data storage nodes in respective replicas.", "num_citations": "51\n", "authors": ["1817"]}
{"title": "Method, apparatus, and system for expressway routing among peers\n", "abstract": " In a method for creating expressway for overlay routing, an existing peer-to-peer network is organized into a plurality of zones. A neighboring zone to a destination peer is selected. A plurality of residents of the neighboring zone are retrieved. A candidate peer is selected from the plurality of residents based a physical distance value and an estimated distance value.", "num_citations": "51\n", "authors": ["1817"]}
{"title": "Error-bounded sampling for analytics on big sparse data\n", "abstract": " Aggregation queries are at the core of business intelligence and data analytics. In the big data era, many scalable shared-nothing systems have been developed to process aggregation queries over massive amount of data. Microsoft's SCOPE is a well-known instance in this category. Nevertheless, aggregation queries are still expensive, because query processing needs to consume the entire data set, which is often hundreds of terabytes. Data sampling is a technique that samples a small portion of data to process and returns an approximate result with an error bound, thereby reducing the query's execution time. While similar problems were studied in the database literature, we encountered new challenges that disable most of prior efforts: (1) error bounds are dictated by end users and cannot be compromised, (2) data is sparse, meaning data has a limited population but a wide range. For such cases\u00a0\u2026", "num_citations": "49\n", "authors": ["1817"]}
{"title": "Friends troubleshooting network: Towards privacy-preserving, automatic troubleshooting\n", "abstract": " Content sharing is a popular usage of peer-to-peer systems for its inherent scalability and low cost of maintenance. In this paper, we leverage this nature of peer-to-peer systems to tackle a new problem: automatic misconfiguration troubleshooting. In this setting, machine configurations from the peers are \u201cshared\u201d to diagnose the misconfigurations on a sick machine. A key challenge for such a troubleshooting system is privacy preservation. To this end, we construct Friends Troubleshooting Network (FTN), a peer-to-peer overlay network, where the links between peer machines reflect the friendship of their owners. To preserve privacy, we use historyless and futureless random-walk in the FTN, during which search along with parameter aggregation are carried out for the purpose of troubleshooting. Many of our techniques can be applied to other application scenarios that require privacy-preserving distributed\u00a0\u2026", "num_citations": "49\n", "authors": ["1817"]}
{"title": "RepStore: A self-managing and self-tuning storage backend with smart bricks\n", "abstract": " With the continuously improving price-performance ratio, building large, smart-brick based distributed storage system becomes increasingly attractive. The challenges, however, include not only reliability, adequate cost-performance ratio, online upgrades and so on, but also the system's ability to achieve these goals in as self-managing and self-adaptive a manner as possible. In this paper, we describe RepStore, a system that fulfills these goals. RepStore unites the self-organizing capability of P2P DHT and the completely autonomous, per-brick tuning mechanism to derive a scalable and cost-effective architecture. RepStore employs replication for active write-intensive data and erasure-coding for the rest, strives to achieve the best cost-performance balance automatically and transparent to application, and does so in a completely distributed manner. Our preliminary evaluations reveal that the system performs\u00a0\u2026", "num_citations": "48\n", "authors": ["1817"]}
{"title": "Trading replication consistency for performance and availability: an adaptive approach\n", "abstract": " Replication system is one of the most fundamental building blocks of wide-area applications. Due to the inevitable dependencies on wide-area communication, trade-off between performance, availability and replication consistency is often a necessity. While a number of proposals have been made to provide a tunable consistency bound between strong and weak extremes, many of them rely on a statically specified enforcement across replicas. This approach, while easy to implement, neglects the dynamic contexts within which replicas are operating, delivering sub-optimal performance and/or system availability. In this paper we analyze the problem of optimal performance/availability for a given consistency level under heterogeneous workload and network condition. We prove several optimization rules for different goals. Based on these results, we developed an adaptive update window protocol in which\u00a0\u2026", "num_citations": "48\n", "authors": ["1817"]}
{"title": "Overlay node placement: Analysis, algorithms and impact on applications\n", "abstract": " Overlay routing has emerged as a promising approach to improving performance and reliability of Internet paths. To fully realize the potential of overlay routing under the constraints of deployment costs in terms of hardware, network connectivity and human effort, it is critical to carefully place infrastructure overlay nodes to balance the trade-off between performance and resource constraints. In this paper, we investigate approaches to perform intelligent placement of overlay nodes to facilitate (i) resilient routing and (ii) TCP performance improvement. We formulate objective functions to accurately capture application behavior: reliability and TCP performance, and develop several placement algorithms, which offer a wide range of trade-offs in complexity and required knowledge of the client- server location and traffic load. Using simulations on synthetic and real Internet topologies, and PlanetLab experiments, we\u00a0\u2026", "num_citations": "46\n", "authors": ["1817"]}
{"title": "WiDS: An Integrated Toolkit for Distributed System Development.\n", "abstract": " Faced with a proliferation of distributed systems in research and production groups, we have devised the WiDS ecosystem of technologies to optimize the development and testing process for such systems. WiDS optimizes the process of developing an algorithm, testing its correctness in a debuggable environment, and testing its behavior at large scales in a distributed simulation. We have developed many distributed protocols and systems using WiDS, including a large-scale backup service that is robust enough to be deployed. We have also used WiDS to perform ultra-large scale (> 1million instances) simulation of a production protocol. In this paper, we describe the principles and design of WiDS, share the lessons that we learned, and discuss on-going research that will further reduce programming and debugging difficulties of distributed systems.", "num_citations": "41\n", "authors": ["1817"]}
{"title": "Evaluation and optimization of a peer-to-peer video-on-demand system\n", "abstract": " Video-on-demand (VoD) is increasingly popular with internet users. However, VoD is costly due to the load placed on video servers. Peer-to-peer (P2P) techniques are an approach to alleviating server load through peer-assisted sharing. Existing studies on P2P VoD are mostly based on simulation and focus on areas such as overlay topology, but little is known about the effectiveness of P2P in a real VoD system.In this paper we present a comprehensive measurement study of GridCast, a deployed experimental P2P VoD system. Using a 2-month log of GridCast, we evaluate its scalability and end user experience. Motivated by the observations on user behavior and unused peer resource, we further optimize its performance. Our key findings are: (1) a moderate number of concurrent users can derive satisfactory user experience. However, good network bandwidth at peers and adequate server provisioning are still\u00a0\u2026", "num_citations": "39\n", "authors": ["1817"]}
{"title": "Checkpoint computer system utilizing a FIFO buffer to re-synchronize the memory systems on the detection of an error\n", "abstract": " A computer system having a checkpoint error recovery system. The computer system includes a first computer having a first memory and a second computer having a second memory and a buffer. The first and second memories are updated by memory updates that include an address specifying a location and data to be written to the memory receiving the update at the location. The computer system also includes an interface for providing the second computer with a copy of each memory update received by the first memory. Upon receiving each of the copies of the memory updates, the second computer generates a recovery memory update corresponding to that copy of the memory update. The recovery memory update includes the data stored in the second memory at the address specified in the first memory update and the address specified in the received copy. The second computer then updates the second\u00a0\u2026", "num_citations": "39\n", "authors": ["1817"]}
{"title": "Enforcing file authorization access\n", "abstract": " A method and system of enforcing file authorization access. The method may include generating an authorization combination at a metadata server and encrypting the authorization combination. The authorization combination may include a block combination including a block list for accessing user requested data from a storage server system and an authorization prefix. The authorization prefix may indicate at least one operation which the user requesting data access is authorized to perform. The method may further include receiving the encrypted authorization combination at the storage server, and decrypting the encrypted authorization combination to recover the block list for retrieving the user requested data.", "num_citations": "37\n", "authors": ["1817"]}
{"title": "Extending a standard-based remote file access protocol and maintaining compatibility with a standard protocol stack\n", "abstract": " Apparatus and method for extending a standard-based remote file access protocol and maintaining compatibility with the standard protocol stack. Network file system (NFS) remote procedure calls (RPCs) that are submitted by an NFS client application are intercepted in a manner that is transparent to the client operating system. The intercepted NFS-RPCs are sent to a file interface card. The file interface card includes a processor that executes code that implements the standard NFS-RPC protocol, along with extensions to the standard NFS-RPC protocol. Non-NFS RPCs are sent to a conventional network interface card for processing.", "num_citations": "37\n", "authors": ["1817"]}
{"title": "The performance of the cedar multistage switching network\n", "abstract": " While multistage switching networks for vector multiprocessors have been studied extensively, detailed evaluations of their performance are rare. Indeed, analytical models, simulations with pseudosynthetic loads, studies focused on average-value parameters, and measurements of networks disconnected from the machine, all provide limited information. In this paper, instead, we present an in-depth empirical analysis of a multistage switching network in a realistic setting: We use hardware probes to examine the performance of the omega network of the Cedar shared-memory machine executing real applications. The machine is configured with 16 vector processors. The analysis suggests that the performance of multistage switching networks is limited by traffic nonuniformities. We identify two major nonuniformities that degrade Cedar's performance and are likely to slow down other networks too. The first one is the\u00a0\u2026", "num_citations": "37\n", "authors": ["1817"]}
{"title": "The performance of the Cedar multistage switching network\n", "abstract": " While multistage switching networks for vector multiprocessors have been studied extensively, detailed evaluations of their performance are rare. Indeed, analytical models, simulations with pseudo-synthetic loads, studies focused on average-value parameters, and measurements of networks disconnected from the machine all provide limited information. In this paper, instead, we present an in-depth empirical analysis of a multistage switching network in a realistic setting: we use hardware probes to examine the performance of the omega network of the Cedar shared-memory machine executing real applications. The machine is configured with 16 vector processors. The analysis suggests that the performance of multistage switching networks is limited by traffic non-uniformities. We identify two major non-uniformities that degrade Cedar's performance and are likely to slow down other networks too. The first one is\u00a0\u2026", "num_citations": "37\n", "authors": ["1817"]}
{"title": "Extensible browser platform for web applications\n", "abstract": " An enhancement to a web browser offers an extension mechanism for web applications to utilize client-side resources, thereby moving more extensibility and flexibility to the client-side. The web browser may expose some control interfaces to meet various requirements from web applications. Using the extension mechanism, the web applications are able to offload the cloud servers, function when offline, and improve performance. The extension mechanism also provides users with full control to the data passing through their web browsers.", "num_citations": "36\n", "authors": ["1817"]}
{"title": "On the placement of infrastructure overlay nodes\n", "abstract": " Overlay routing has emerged as a promising approach to improving performance and reliability of Internet paths. To fully realize the potential of overlay routing under the constraints of deployment costs in terms of hardware, network connectivity and human effort, it is critical to carefully place infrastructure overlay nodes to balance the tradeoff between performance and resource constraints. In this paper, we investigate approaches to perform intelligent placement of overlay nodes to facilitate (i) resilient routing and (ii) TCP performance improvement. We formulate objective functions to capture application behavior: reliability and TCP performance, and develop several placement algorithms, which offer a wide range of tradeoffs in complexity and required knowledge of the client-server location and traffic load. Using simulations on synthetic and real Internet topologies, and PlanetLab experiments, we demonstrate the\u00a0\u2026", "num_citations": "36\n", "authors": ["1817"]}
{"title": "Low-power distributed event detection in wireless sensor networks\n", "abstract": " In this paper we address the problem of energy-efficient event detection in wireless sensor networks (WSNs). Duty cycling is a fundamental approach to conserving energy in WSNs. However, it brings challenges to event detection in the sense that an event may be undetected or undergo a certain delay before it is detected, in particular when sensors are low duty-cycled. We investigate the fundamental relationship between event detection and energy efficiency. Based on a simplified network model, we quantify event detection performance by deriving the closed forms of detection delay and detectability. We also characterize the intrinsic tradeoff that exists between detection performance and system lifetime, which helps flexible design decisions for WSNs. In addition, we propose a completely localized algorithm, CAS, to cooperatively determine sensor wakeups. Without relying on location information, CAS is easy\u00a0\u2026", "num_citations": "36\n", "authors": ["1817"]}
{"title": "A framework for lazy replication in P2P VoD\n", "abstract": " Video-on-Demand (VoD) is a compelling application, but costly due to the load it places on servers. Peer-to-peer (P2P) techniques hold the potential to reduce centralized costs by sharing data between peers. There are many difficult design issues associated with P2P for VoD. Viewing the problem as designing a large distributed cache, many of the issues can be expressed in terms of caching algorithms.", "num_citations": "35\n", "authors": ["1817"]}
{"title": "A multi-dimensional reputation system combined with trust and incentive mechanisms in P2P file sharing systems\n", "abstract": " Free-riders and fake files are two important problems in P2P file sharing systems. Previous works have always used incentive mechanisms and trust mechanisms to address them respectively. In real systems however, a trust mechanism without incentive would face lack of users' enthusiasm and thus cause sparse relationship of direct trust while an incentive mechanism without trust could induce users' bad behavior. A novel reputation system is proposed in this paper that combines trust and incentive mechanisms. It uses files' vote and retention time, download volume and users' rank to construct a more extensive direct trust relationship and calculates a user's reputation with a multi-trust algorithm. It can identify fake files and provide service differentiation with reputation. Implementation and some security consideration in DHT are also discussed.", "num_citations": "35\n", "authors": ["1817"]}
{"title": "Event-based automated diagnosis of known problems\n", "abstract": " System events preceding occurrence of a problem are likely to be similar to events preceding occurrence of the same problem at other times or on other systems. Thus, the cause of a problem may be identified by comparing a trace of events preceding occurrence of the problem with previously diagnosed traces. Traces of events preceding occurrences of a problem arising from a known cause are reduced to a series of descriptive elements. These elements are aligned to correlate differently timed but otherwise similar traces of events, converted into symbolic representations, and archived. A trace of events leading to an undiagnosed a problem similarly is converted to a symbolic representation. The representation of the undiagnosed trace is then compared to the archived representations to identify a similar archived representation. The cause of the similar archived representation is presented as a diagnosis of the\u00a0\u2026", "num_citations": "35\n", "authors": ["1817"]}
{"title": "Placing an object at a node in a peer-to-peer system based on storage utilization\n", "abstract": " A peer-to-peer system includes a plurality of nodes. The plurality of nodes includes at least an initial node and a neighboring node. The initial node is operable to determine whether to place an object at either the initial node or another node in the system based on a comparison of storage utilizations for the initial node and the neighboring node.", "num_citations": "34\n", "authors": ["1817"]}
{"title": "Simulating large-scale p2p systems with the wids toolkit\n", "abstract": " Current simulation technologies support at most hundreds of thousands of nodes, and fall short on the emerging large-scale networking systems that usually involve millions of nodes. We meet this challenge with our distributed simulation engine that is able to run millions of instances and is tested with a production P2P protocol, using commodity PC clusters. This simulation engine is part of the WiDS toolkit, which takes a holistic approach to the research and development of distributed systems. We also propose a critical optimization, called slow message relaxation (SMR), to trade simulation accuracy for performance. By taking advantage of the fact that distributed protocols are resilient to network fluctuation, SMR executes events in a logical time window much wider than the conventional look ahead scheme allows. We analyze and bound the potential effect of the distortion on application logic and other general\u00a0\u2026", "num_citations": "33\n", "authors": ["1817"]}
{"title": "Hang analysis: Fighting responsiveness bugs\n", "abstract": " Soft hang is an action that was expected to respond instantly but instead drives an application into a coma. While the application usually responds eventually, users cannot issue other requests while waiting. Such hang problems are widespread in productivity tools such as desktop applications; similar issues arise in server programs as well. Hang problems arise because the software contains blocking or time-consuming operations in graphical user interface (GUI) and other time-critical call paths that should not. This paper proposes HangWiz to find hang bugs in source code, which are difficult to eliminate before release by testing, as they often depend on a user's environment. HangWiz finds hang bugs by finding hang points: an invocation that is expected to complete quickly, such as a GUI action, but calls a blocking function. HangWiz collects hang patterns from runtime traces supplemented with expert\u00a0\u2026", "num_citations": "31\n", "authors": ["1817"]}
{"title": "Distance-adaptive update protocols for scalable shared-memory multiprocessors\n", "abstract": " While update protocols generally induce lower miss rates than invalidate protocols, they tend to generate much traffic. This is one of the reasons why they are considered less cost-effectively scalable than invalidate protocols and, as a result, are avoided in most existing designs of scalable shared-memory multiprocessors. However, given the increasing relative cost of cache misses, update protocols are becoming more worthy of exploration. In this paper, we present a model of sharing that is key to investigating the performance of optimized update protocols: the update distance model. The model gives insight into the update patterns that optimized protocols need to handle. Using this model, we design a new family of protocols that we call distance-adaptive protocols. In these schemes, the directory records the update patterns observed and then uses them to selectively send updates and invalidations to processors\u00a0\u2026", "num_citations": "31\n", "authors": ["1817"]}
{"title": "Separate read and write servers in a distributed file system\n", "abstract": " A system and method for providing a plurality of client applications access to data in a distributed file system. In various embodiments, read requests are separated from write requests and the read requests are processed by dedicated read servers. A plurality of read servers are coupled to the client applications and each read server reads file data from the distributed file system and returns the file data to the client applications. A write server writes data to the distributed file system. Various embodiments are described for separating read requests from write requests and transmitting read requests to the read servers write requests to the write server.", "num_citations": "30\n", "authors": ["1817"]}
{"title": "Distribution of physical file systems\n", "abstract": " Data coherency and lock services for distributed physical file systems. Client applications are coupled to a virtual file system, which is coupled to one or more physical file systems. A lock agent is coupled to one or more lock servers and to the virtual file system. The virtual file system, in response to a file access request from a client application, submits a data validation request to the lock agent. The lock agent, in turn, submits a lease request to the lock server. When the lock server grants the lease, the lease and a validation code are returned to the lock agent. The validation code indicates whether the data in the buffer cache of the virtual file system are valid. The lock agent then returns the lease to the virtual file system, which then submits the file access request to a selected one of the physical file systems.", "num_citations": "30\n", "authors": ["1817"]}
{"title": "Minerva: A Scalable and Highly Efficient Training Platform for Deep Learning\n", "abstract": " The tooling landscape of deep learning is fragmented by a growing gap between the generic and productivity-oriented tools that optimize for algorithm development and the task-specific ones that optimize for speed and scale. This creates an artificial barrier to bring new innovations into real-world applications. Minerva addresses this issue with a layered design that provides language flexibility and execution efficiency simultaneously within one coherent framework. It proposes a matrix-based API, resulting in compact codes and the Matlab-like, imperative and procedural coding style. The code is dynamically translated into an internal dataflow representation, which is then efficiently executed against different hardware. The same user code runs on modern laptop and workstation, high-end multi-core server, or server clusters, with and without GPU acceleration, delivering performance and scalability better than or competitive with existing tools on different platforms.", "num_citations": "29\n", "authors": ["1817"]}
{"title": "Machine Bank: Own your virtual personal computer\n", "abstract": " In this paper, we report the design, implementation and experimental results of Machine Bank, a system engineered towards the popular shared-lab scenario, where users outnumber available PCs and may get different PCs in different sessions. Machine Bank allows users to preserve their entire working environment across sessions. Each client runs virtual machine, which is saved to and reinstantiated from a content-addressable backend storage. We carefully designed lightweight hooks at client side that implements caching and tracking logics to improve reinstantiation speed as well as to remove unnecessary network and disk traffic. Our detailed evaluation demonstrates that these techniques are effective, and the overall performance fits well with the shared-lab usage.", "num_citations": "29\n", "authors": ["1817"]}
{"title": "Proxy+: Simple proxy augmentation for dynamic content processing\n", "abstract": " Caching dynamic content can bring many benefits to the performance and scalability of Web application servers. However, such mechanisms are usually tightly coupled to individual application servers (or even applications) that prevent caching at more advantageous points. In this paper we propose an approach to enable dynamic content caching at enhanced Web proxies which requires only simple modifications to existing applications.", "num_citations": "28\n", "authors": ["1817"]}
{"title": "Method and apparatus for mapping peers to an overlay network\n", "abstract": " In a method of mapping peers in a peer-to-peer network to an overlay network, network coordinates are determined for a selected peer. The logical coordinates in the overlay network are determined based on the network coordinates. A zone is determined based on the logical coordinates. The network coordinates, a network address of the selected peer and the zone is stored as an object at a peer owning the zone, where associated information is stored in the peer that has the network coordinate and using the network coordinate as a key.", "num_citations": "24\n", "authors": ["1817"]}
{"title": "Impression store: Compressive sensing-based storage for big data analytics\n", "abstract": " For many big data analytics workloads, approximate results suffice. This begs the question, whether and how the underlying system architecture can take advantage of such relaxations, thereby lifting constraints inherent in today\u2019s architectures. This position paper explores one of the possible directions. Impression Store is a distributed storage system with the abstraction of big data vectors. It aggregates updates internally and responds to the retrieval of top-K high-value entries. With proper extension, Impression Store supports various aggregations, top-K queries, outlier and major mode detection. While restricted in scope, such queries represent a substantial and important portion of many production workloads. In return, the system has unparalleled scalability; any node in the system can process any query, both reads and updates. The key technique we leverage is compressive sensing, a technique that substantially reduces the amount of active memory state, IO, and traffic volume needed to achieve such scalability.", "num_citations": "23\n", "authors": ["1817"]}
{"title": "Mutual exclusion techniques in a dynamic peer-to-peer environment\n", "abstract": " Mutual exclusion techniques for use in a dynamic peer-to-peer environment are described. In an implementation, a method includes receiving, at each of a plurality of logical replicas, a request from a client. Each of the logical replicas includes a queue and is for exclusive association with one of the clients. The request is for accessing one of a plurality of resources. When a particular one of the logical replicas is exclusively associated with another one of the clients, the request is stored in the queue of the particular logical replica.", "num_citations": "22\n", "authors": ["1817"]}
{"title": "Language-based replay via data flow cut\n", "abstract": " A replay tool aiming to reproduce a program's execution interposes itself at an appropriate replay interface between the program and the environment. During recording, it logs all non-deterministic side effects passing through the interface from the environment and feeds them back during replay. The replay interface is critical for correctness and recording overhead of replay tools.", "num_citations": "20\n", "authors": ["1817"]}
{"title": "Hybrid object placement in a distributed storage system\n", "abstract": " Described is the differentiation of replicas in a large distributed object store as either being smoothing replicas based on an amount of load on storage nodes (bricks), or as spreading replicas based on a substantially random distribution among the system's bricks. The smoothing replicas are placed among the lowest usage bricks, while the spreading replicas are placed randomly throughout other bricks in the system independent of load. As a result, fast, primarily parallel data repair is facilitated by selecting a spreading replica when repair is needed, while load balancing is facilitated by placing a smoothing replica on a low-usage brick when a new replica is checked in, and selecting a smoothing replica when load balancing is triggered by overloading of a brick and/or addition of a new brick. Check-in, data repair and load balancing policies specify how to use smoothing replicas and spreading replicas.", "num_citations": "20\n", "authors": ["1817"]}
{"title": "An analytical framework and its applications for studying brick storage reliability\n", "abstract": " The reliability of a large-scale storage system is influenced by a complex set of inter-dependent factors. This paper presents a comprehensive and extensible analytical framework that offers quantitative answers to many design tradeoffs. We apply the framework to a number of important design strategies that a designer and/or administrator must face in reality, including topology-aware replica placement, proactive replication that uses small background network bandwidth and unused disk space to create additional copies. We also quantify the impact of slow (but potentially more accurate) failure detection and lazy replacement of failed disks. We use detailed simulation to verify and refine our analytical model. These results demonstrate the versatility of the framework and serve as a solid step towards more quantitative studies of fundamental system tradeoffs between reliability, performance, and cost in large-scale\u00a0\u2026", "num_citations": "20\n", "authors": ["1817"]}
{"title": "Distributed Outlier Detection using Compressive Sensing\n", "abstract": " Computing outliers and related statistical aggregation functions from large-scale big data sources is a critical operation in many cloud computing scenarios, eg service quality assurance, fraud detection, or novelty discovery. Such problems commonly have to be solved in a distributed environment where each node only has a local slice of the entirety of the data. To process a query on the global data, each node must transmit its local slice of data or an aggregated subset thereof to a global aggregator node, which can then compute the desired statistical aggregation function. In this context, reducing the total communication cost is often critical to the overall efficiency.", "num_citations": "18\n", "authors": ["1817"]}
{"title": "Z-Ring: Fast prefix routing via a low maintenance membership protocol\n", "abstract": " In this paper, we introduce Z-ring, a fast prefix routing protocol for peer-to-peer overlay networks. Z-ring incorporates cost-efficient membership protocol to achieve fast routing with small maintenance cost. Z-ring achieves routing in logGN steps, where N is the network size and G is the size of a group that can be maintained by a membership protocol with low cost. With G=4096, it translates to one-hop routing for intranet environments (N<4096), two-hop routing for mid-scale internet applications (N<16 million), and three-hop routing for ultra-large Internet applications (N<64 billion). Z-ring maintains good routing success rate under churn and low maintenance cost even at large network size. Its modularized use of the membership protocol also makes it adaptive to dynamic and wide-range network size changes.", "num_citations": "18\n", "authors": ["1817"]}
{"title": "Saliency-based Sequential Image Attention with Multiset Prediction\n", "abstract": " Humans process visual scenes selectively and sequentially using attention. Central to models of human visual attention is the saliency map. We propose a hierarchical visual architecture that operates on a saliency map and uses a novel attention mechanism to sequentially focus on salient regions and take additional glimpses within those regions. The architecture is motivated by human visual attention, and is used for multi-label image classification on a novel multiset task, demonstrating that it achieves high precision and recall while localizing objects with its attention. Unlike conventional multi-label image classification models, the model supports multiset prediction due to a reinforcement-learning based training process that allows for arbitrary label permutation and multiple instances per label.", "num_citations": "16\n", "authors": ["1817"]}
{"title": "Distributed system simulation: slow message relaxation\n", "abstract": " Distributed system simulation is enhanced by extending the simulation window. In a described implementation, the simulation window extension is facilitated with a slow message relaxation scheme. For example, especially when the simulation window is extended, slow unscheduled events can arrive at a logical process with a timestamp that is prior to (eg, less than) the local time of a receiving logical process that is participating in a simulation. To ameliorate issues created by a slow unscheduled message and its corresponding slow unscheduled event, a current logical time of the receiving logical process is substituted for the original timestamp of the slow unscheduled event to transform it into a punctual unscheduled event.", "num_citations": "15\n", "authors": ["1817"]}
{"title": "Method and apparatus for generating a routing table\n", "abstract": " In a method of generating a routing table for a selected peer, a zone of the selected peer is compared to a target zone. A current entry associated with the zone of the selected peer is created in a routing table of the selected peer in response to the zone of the selected peer being one of smaller and equal to the target zone.", "num_citations": "15\n", "authors": ["1817"]}
{"title": "DiFFS: a Scalable Distributed File System\n", "abstract": " Industry analysts see no limits to the world\u2019s expanding appetite for data-storage services. Emerging networking technologies allow incremental scaling of bandwidth and capacity of storage, which is attached to the network. A key challenge is to devise the software that provides transparent shared access to decentralized storage resources. Existing network file systems will not meet the scalability requirements of future storage services. This paper introduces DiFFS, a distributed file system designed for storage area networks. DiFFS achieves high scalability by following a partitioning approach to sharing storage resources. The architecture is robust against failures and unfavorable access patterns. It is independent of the physical file system (s) used for the placement of data; multiple file systems can co-exist in a DiFFS system.", "num_citations": "14\n", "authors": ["1817"]}
{"title": "Expressway routing among peers\n", "abstract": " In a method for expressway routing among peers, a request is received to forward data. The destination is determined from said request. A routing table is searched for an expressway route based on zones to the destination. The data is transmitted across the expressway route to the destination.", "num_citations": "13\n", "authors": ["1817"]}
{"title": "Loss Functions for Multiset Prediction\n", "abstract": " We study the problem of multiset prediction. The goal of multiset prediction is to train a predictor that maps an input to a multiset consisting of multiple items. Unlike existing problems in supervised learning, such as classification, ranking and sequence generation, there is no known order among items in a target multiset, and each item in the multiset may appear more than once, making this problem extremely challenging. In this paper, we propose a novel multiset loss function by viewing this problem from the perspective of sequential decision making. The proposed multiset loss function is empirically evaluated on two families of datasets, one synthetic and the other real, with varying levels of difficulty, against various baseline loss functions including reinforcement learning, sequence, and aggregated distribution matching loss functions. The experiments reveal the effectiveness of the proposed loss function over the others.", "num_citations": "12\n", "authors": ["1817"]}
{"title": "Checkpoint computer system utilizing a FIFO buffer to re-synchronize and recover the system on the detection of an error\n", "abstract": " A fault-tolerant computer system having an application memory organized as a plurality of cache lines, each cache line being identified by an address in the memory. A FIFO buffer stores a plurality of such cache lines. The system includes at least one CPU for executing instructions stored in the application memory. A checkpoint controller defines a series of repeating checkpoint cycles. The application memory and FIFO buffer are operated under the control of a memory controller. The checkpoint controller also has access to a plurality of registers in the CPU that define the state of that CPU at a point in each checkpoint cycle that is controllable by the controller. When the memory controller receives a cache line from the CPU in response to a write command specifying an address A in the application memory at which the cache line is to be stored, a copy of the cache line as stored in the application memory at A is\u00a0\u2026", "num_citations": "12\n", "authors": ["1817"]}
{"title": "The power of dht as a logical space\n", "abstract": " P2P DHT has fast become a rather \"classical\" research field. The currently popular mindsets are mostly routing-centric, or storage-centric. In this paper, we argue that it maybe more interesting to simply view DHT as a logical space that can dynamically size itself with potentially unlimited amount resources. This is in someway analogous to the virtual memory in any contemporary operating system. We believe that exploring such a perspective will bring about new insights as well as applications. We illustrate the power of this abstraction with a few examples, including self-scaling, self-healing fat tree, self-tune storage system, and a lightweight quorum-based distributed lock protocol which does not assume a constant number of members to start with. While their individual utilizations differ radically, all of them are united under this viewpoint.", "num_citations": "12\n", "authors": ["1817"]}
{"title": "Method for reducing coherent misses in shared-memory multiprocessors utilizing lock-binding prefetchs\n", "abstract": " A method for operating a shared memory computer system to reduce the latency times associated with lock/unlock code sequences. The computer system includes a shared memory and a plurality of processors. When one of the processors wishes to modify a shared variable stored in the shared memory, the processor must first request and receive a lock from the shared memory. The lock prevents any other processor in the computer system from modifying data in the shared memory during the locked period. In the present invention, a list of variables in the shared memory that are shared by two or more of the processors is generated. When one of the processors is granted a lock, a prefetch instruction is executed for each variable in the list. Each prefetch instruction specifies the processor receiving the lock as the destination of the data specified in that prefetch instruction. The list may be generated by a compiler\u00a0\u2026", "num_citations": "12\n", "authors": ["1817"]}
{"title": "A scalable and topology configurable protocol for distributed parameter synchronization\n", "abstract": " This paper addresses the problem of model synchronization in data-parallelism of deep-learning systems. In such systems, workers on different machines continuously update their local copies of the model, and the updates need to be merged so that the copies are roughly consistent to each other. In modern implementations using GPUs, workers generate very high updates, posing significant scalability challenges.", "num_citations": "11\n", "authors": ["1817"]}
{"title": "Creating expressway for overlay routing\n", "abstract": " In a method for creating expressway for overlay routing, an existing peer-to-peer network is organized into a plurality of zones. The plurality of zones is organized into a plurality of levels. Neighboring zones are identified for each zone of the plurality of zones. One or more representatives are identified for each neighboring zone. A routing table is created based the plurality of zones, the neighboring zones, the one or more representatives, and the plurality of levels.", "num_citations": "11\n", "authors": ["1817"]}
{"title": "Sigma: A fault-tolerant mutual exclusion algorithm in dynamic distributed systems subject to process crashes and memory losses\n", "abstract": " This paper introduces the Sigma algorithm that solves fault-tolerant mutual exclusion problem in dynamic systems where the set of processes may be large and change dynamically, processes may crash, and the recovery or replacement of crashed processes may lose all state information (memory losses). Sigma algorithm includes new messaging mechanisms to tolerate process crashes and memory losses. It does not require any extra cost for process recovery. The paper also shows that the threshold used by the Sigma algorithm is necessary for systems with process crashes and memory losses.", "num_citations": "11\n", "authors": ["1817"]}
{"title": "A hybrid model checking and runtime monitoring method for c++ web services\n", "abstract": " Traditional model checking methods to find safety and liveness violations are usually applied in the abstract model, while existing runtime monitoring methods to check the predicates can not obtain the high path coverage. This paper presents a hybrid model checking and runtime monitoring method to check the safety and liveness properties in real complex distributed C/C++ Web service systems,which can offer both a richer and more efficient way to search violations specified by linear temporal logic. Based on our model checking engine to walk different paths, we adopt the finite linear temporal logic engine to dynamically verify properties in C/C++ Web service systems. Then, we use the practical examples to illustrate its usage and applications in real C/C++ Web service systems.", "num_citations": "10\n", "authors": ["1817"]}
{"title": "Self-balanced P2P expressway: when Marxism meets Confucian\n", "abstract": " The potential of a P2P system to become an ultra-scalable and yet manageable infrastructure lies in its self-organizing nature. Being composed by increasingly powerful commodity devices, these systems must also endeavor into being not merely self-organizing, but self-adaptation as well.", "num_citations": "10\n", "authors": ["1817"]}
{"title": "File virtualization with directnfs\n", "abstract": " There is a definite trend in the enterprise storage industry to move from Network Attached Storage (NAS) solutions to high performance Storage Area Networks (SAN). This transition is not easy because of the well-entrenched NAS infrastructure that has already been deployed. This paper attempts to define a file system that can leverage the existing NAS software infrastructure along with evolving SAN technology to provide the benefits of high performance storage access while reducing the cost of migrating to these networks.In this paper, we propose a new network file system, DirectNFS, which allows NAS clients to take full advantage of the performance and scalability benefits of SANs. In order to achieve this goal, the system presents a NAS interface to existing NAS clients while allowing DirectNFS clients to access storage directly over shared SAN, ie clients bypass the server for data access. A server maintains the NAS interface for legacy clients and arbitrates access to metadata by DirectNFS (SAN aware) clients. This metadata server ensures that the system is operable for both legacy NAS clients as well as DirectNFS clients. The communication protocol of DirectNFS is designed as an extension of traditional network file systems protocols, such as NFS and CIFS. A prototype of DirectNFS has been built for Linux, as an extension to the native NFSv2 implementation. Initial results demonstrate that the performance of data intensive operations such as read and write is comparable to that of local file systems, such as ext2.", "num_citations": "10\n", "authors": ["1817"]}
{"title": "AutoLog: facing log redundancy and insufficiency\n", "abstract": " Logs are valuable for failure diagnosis and software debugging in practice. However, due to the ad-hoc style of inserting logging statements, the quality of logs can hardly be guaranteed. In case of a system failure, the log file may contain a large number of irrelevant logs, while crucial clues to the root cause may still be missing.", "num_citations": "9\n", "authors": ["1817"]}
{"title": "Conditional correlation analysis for safe region-based memory management\n", "abstract": " Region-based memory management is a popular scheme in systems software for better organization and performance. In the scheme, a developer constructs a hierarchy of regions of different lifetimes and allocates objects in regions. When the developer deletes a region, the runtime will recursively delete all its subregions and simultaneously reclaim objects in the regions. The developer must construct a consistent placement of objects in regions; otherwise, if a region that contains pointers to other regions is not always deleted before pointees, an inconsistency will surface and cause dangling pointers, which may lead to either crashes or leaks. This paper presents a static analysis tool RegionWiz that can find such lifetime inconsistencies in large C programs using regions. The tool is based on an analysis framework that generalizes the relations and constraints over regions and objects as conditional correlations\u00a0\u2026", "num_citations": "9\n", "authors": ["1817"]}
{"title": "Analytical Framework for Multinode Storage Reliability Analysis\n", "abstract": " A analytical framework is described for quantitatively analyzing reliability of a multinode storage system, such as a brick storage system. The framework defines a multidimensional state space of the multinode storage system and uses a stochastic process (such as Markov process) to determine a transition time-based metric measuring the reliability of the multinode storage system. The analytical framework is highly scalable and may be used for quantitatively predicting or comparing the reliability of storage systems under various configurations without requiring experimentation and large-scale simulations.", "num_citations": "9\n", "authors": ["1817"]}
{"title": "XRing: Achieving high-performance routing adaptively in structured P2P\n", "abstract": " P2P DHT has emerged as a scalable way to pool together potentially an unlimited amount of resources in a complete self-organizing manner. Many of the current proposals, however, are dominated by the minimalist principle, ie achieving O (logN) performance with O (logN) state. These designs, while elegant, undershoot in a number of interesting deployment situations where either the churn rate is low or system size is moderate.We argue that the amount of state is not an issue, the quality and the associated efforts are. From this perspective, an ideal DHT should reach the optimal performance for a given bandwidth budget. It should also be robust, in the sense that the system should weather storm of events during which the performance degrades gracefully and rapidly return to normal level afterwards.", "num_citations": "9\n", "authors": ["1817"]}
{"title": "Hybrid programming\n", "abstract": " Hybrid programming combines certain aspects of the synchronous calling nature of a thread-oriented programming model with certain aspects of the asynchronous calling nature of an event-oriented programming model by creating parallelized calls. In a described implementation, multiple synchronous calls are transformed into multiple asynchronous calls and encapsulated within a barrier time period. A hybrid programming model or protocol may be employed, for example, in conjunction with communication exchanges in a multiple-phase and multiple-party distributed programming environment.", "num_citations": "8\n", "authors": ["1817"]}
{"title": "FiLM: a runtime monitoring tool for distributed systems\n", "abstract": " It is well recognized that debugging or testing a distributed system is a great challenge. FiLM is a runtime monitoring tool that can monitor the execution of distributed applications against LTL specifications on finite traces. Implemented within the online predicate checking infrastructure D 3 S, FiLM models the execution of distributed applications as a trace of consistent global snapshots with global timestamps, and it employs finite automata constructed from LTL specifications to evaluate the trace of distributed systems. We proved that the generated automata accept exactly the traces which satisfy LTL specifications. Our case study shows that FiLM successfully detected an important and intricate liveness bug in a real Paxos implementation.", "num_citations": "8\n", "authors": ["1817"]}
{"title": "Placing an object at a node in a peer-to-peer system based on a zoom-in algorithm\n", "abstract": " A peer-to-peer system is divided into a plurality of zones for placing an object. A parent zone having a parent node is identified. The parent zone is divided into the plurality of zones, ie, subzones, and one of subzones is selected. A node in the subzone is selected for placing the object.", "num_citations": "8\n", "authors": ["1817"]}
{"title": "Scalable, structured data placement over p2p storage utilities\n", "abstract": " P2P overlays offer a convenient way to host an infrastructure that can scale to the size of the Internet and yet be manageable. Current proposals, however, do not offer support for structuring data, other than assuming a distributed hash table. In reality, both applications and users typically organize data in a structured form. One such popular structure is the tree as employed in a file system, and a database. A naive approach such as hashing the pathname not only ignores locality in important operations such as file/directory lookup, but also results in uncontrollable, massive object relocations when rename on a path component occur. In this paper, we investigate policies and strategies that place a tree onto the flat storage space of P2P systems. We found that, in general, there exists a tradeoff between lookup performance and balanced storage utilization, and attempt to balance these two requirements calls for an\u00a0\u2026", "num_citations": "8\n", "authors": ["1817"]}
{"title": "Towards pragmatic library-based replay\n", "abstract": " Deterministic replay is a powerful approach for debugging multithreaded and distributed applications that involve complex nondeterminism. Recently, several replay tools based on user-space libraries were proposed. Such library-based tools are lightweight and efficient, and focus on the application being debugged. However, several significant problems remain, such as in-address-space tool interference, absence of support for advanced features in modern operating systems (eg asynchronous I/O), and error-prone engineering efforts for wrapping a large set of API functions to enable logging and replaying.We propose three mechanisms to cope with these problems. First, we resolve the tool interference problem by space separation, using a virtual execution layer composed of intercepted functions. The virtual execution layer isolates both memory and execution of the application from supporting libraries and in-address-space replay tool, hence systematically removes the interference. Second, based on an event-view of the execution, we are able to handle advanced operating features such as asynchronous I/O and exceptional control flow (eg signal). Third, instead of manually coding the wrappers, we use an annotation-aware code generator so that the process of producing the wrappers is more robust and efficient. We have handled more than 750 API functions and the code generator is responsible for more than 37,000 lines of wrapper code. To the best of our knowledge, these three mechanisms, space separation, event-based replay, and annotation-aware code generation are novel for library-based deterministic replay. We have\u00a0\u2026", "num_citations": "7\n", "authors": ["1817"]}
{"title": "A dataflow model for .NET-based Grid Computing systems\n", "abstract": " This paper presents the design, implementation and evaluation of a dataflow system, including a dataflow programming model and a dataflow engine, for coarse-grained distributed data intensive applications. The dataflow programming model provides users with a transparent interface for application programming and execution management in a parallel and distributed computing environment. The dataflow engine dispatches the tasks onto candidate distributed computing resources in the system, and manages failures and load balancing problems in a transparent manner. The system has been implemented over .NET platform and deployed in a Windows Desktop Grid. This paper uses two benchmarks to demonstrate the scalability and fault tolerance properties of our system.", "num_citations": "7\n", "authors": ["1817"]}
{"title": "Recovery of Memory and Process in DSM Systems: HA Issue#\n", "abstract": " The faulty scenario of the shared-memory multiprocessor system investigated in this report is single-node (or single protection-domain) failure. We assume a robust, HA-enabled IO subsystem is already in place. We describe the impact of the fault with dependency arcs originated from elsewhere but end at the faulty unit. One example is shown in Figure 1.", "num_citations": "6\n", "authors": ["1817"]}
{"title": "Learning Word Embeddings from Intrinsic and Extrinsic Views\n", "abstract": " While word embeddings are currently predominant for natural language processing, most of existing models learn them solely from their contexts. However, these context-based word embeddings are limited since not all words' meaning can be learned based on only context. Moreover, it is also difficult to learn the representation of the rare words due to data sparsity problem. In this work, we address these issues by learning the representations of words by integrating their intrinsic (descriptive) and extrinsic (contextual) information. To prove the effectiveness of our model, we evaluate it on four tasks, including word similarity, reverse dictionaries,Wiki link prediction, and document classification. Experiment results show that our model is powerful in both word and document modeling.", "num_citations": "5\n", "authors": ["1817"]}
{"title": "BOX: Icing the APIs\n", "abstract": " This paper presents BOX, an API-centric debugging and testing platform that uses API calling boundary as the manipulation surface to install and extend a variety of important debugging tools transparently for legacy applications. We deal with the problem of instrumenting large amount of APIs by using annotation-aware code generation; we carefully design the runtime to eliminate interference from hosted tools to the target application. The framework is highly extensible, thanks to the signal-slot model to process intercepted APIs. We demonstrate the power of BOX by prototyping a number of tools, ranging from monitoring, logging, dependency tracking, time-suspending-debugging and deterministic replay. We have successfully replayed several large and complex software package, including MySQL and Apache with low overhead. Our experience has validated the main design points of BOX.", "num_citations": "5\n", "authors": ["1817"]}
{"title": "Traversing runtime spanning trees\n", "abstract": " The traversal of runtime spanning trees is facilitated in a distributed operational environment. Distributed traversal of runtime spanning trees may be implemented in different scenarios. However, by way of example only, distributed traversal of runtime spanning trees is described herein primarily in the context of a distributed system simulation scenario. Ensuring that each unscheduled event is processed within a simulation round (ie, within a quantum barrier) in which it is created is especially challenging when executing an operation (eg, performing a simulation) with a distributed apparatus. To address this challenge, unscheduled events are set to correspond to event nodes in a tree. Parent events that beget child events are assigned token values. The token value of a parent event is split and assigned to its child events such that a runtime spanning tree may be distributively traversed by summing the token values\u00a0\u2026", "num_citations": "5\n", "authors": ["1817"]}
{"title": "Placing an object at a node within a logical space in a peer-to-peer system\n", "abstract": " A peer-to-peer system includes a plurality of nodes functioning as a distributed, shared, file system. A node of the plurality of nodes is operable to randomly place an object within a logical space relative to a parent node hosting a parent object in the peer-to-peer system.", "num_citations": "4\n", "authors": ["1817"]}
{"title": "Excel-NUMA: toward programmability, simplicity, and high performance\n", "abstract": " While hardware-coherent scalable shared-memory multiprocessors are relatively easy to program, they still require substantial programming effort to deliver high performance. Specifically, to minimize remote accesses, data must be carefully laid out in memory for locality and application working sets carefully tuned for caches. It has been claimed that this programming effort is less necessary in hardware COMA machines like Flat-COMA thanks to automatic line-based data migration. Unfortunately, Flat-COMA is complex to design. Consequently, we would like a machine as programmable as Flat-COMA, as simple as plain CC-NUMA, and that outperforms both. This paper presents our proposal: Excel-NUMA (EX-NUMA). The idea is to exploit the fact that, after a memory line is written and cached, the storage that kept the line in memory is unutilized. We use that storage to temporarily hold remote data displaced from\u00a0\u2026", "num_citations": "4\n", "authors": ["1817"]}
{"title": "Dependability, access diversity, low cost: pick two\n", "abstract": " Storage systems and data centers are growing rapidly and costing more, with higher energy bills. Users want dependable access to a wide variety of diverse content, but providers want to lower costs. Many studies have looked at the tradeoffs between cost and dependability, but few have looked carefully at how content request diversity changes this relationship. In this paper, we model a disk array and develop an analytical framework to study the relationships between dependability, access diversity, and low cost. We show how access diversity changes the relationship between cost and dependability and that all three are in tension with one another. It is possible to improve any two together, but not all three simultaneously.", "num_citations": "3\n", "authors": ["1817"]}
{"title": "Using model checker and replay facility to debug complex distributed system\n", "abstract": " A correct system is only derived from a correct implementation of a correct specification. Unfortunately, this imposes a heavy burden in the development process, especially for complex, distributed system ranging from machine room computing and storage services as well as large-scale P2P applications. A specification, if authored in formal language such as TLA+, Spec#, SPIN etc., is ready for model checking. The state explosion problem, however, prohibits all specification states to be thoroughly traversed. Often ad hoc heuristics are applied to drastically reduce the scale so as to make the model checking phase tractable. A correct implementation can be even more challenging, especially when we encounter non-deterministic bugs that are hard to reproduce. The gap between spec and implementation often leaves one to wonder whether the implementation or the spec is faulty, or even both. Motivated by our\u00a0\u2026", "num_citations": "3\n", "authors": ["1817"]}
{"title": "Data Migration in a Distributed File Service\n", "abstract": " As distributed systems span the globe, placing data near the point where the data are accessed is becoming important to improve client performance and to reduce network load. With the advent of the utility model in storage services, it is necessary for storage service companies to provide quality of service guarantees to meet customer needs. Adaptive data migration will be required by storage service providers to guarantee these service-level agreements. Storage customers will require the ability to transparently migrate data among different storage service providers based on, for example, pricing differences or QoS issues.", "num_citations": "3\n", "authors": ["1817"]}
{"title": "Cross-Partition Protocols in a Distributed File Service\n", "abstract": " DiFFS is a distributed file service architecture designed for storage area networks [1]. DiFFS achieves high scalability by following a partitioning approach to sharing storage resources. The architecture is robust against failures and unfavorable access patterns. It is independent of the physical file system (s) used for the placement of data; multiple file systems can co-exist in a DiFFS system.Much of DiFFS scalability is attributed to its unique partitioning approach. Each partition is controlled by one partition server, which coordinates non-idempotent operations that may affect the state of the resources (allocate or de-allocate blocks, for example) of the SAN partition under its responsibility. Other operations can bypass the partition server and directly access the SAN storage (Figure 1-a)", "num_citations": "3\n", "authors": ["1817"]}
{"title": "Segtree Transformer: Iterative Refinement of Hierarchical Features\n", "abstract": " The building block of Transformer can be seen as inducing message passing over a complete graph whose nodes correspond to input tokens. Such dense connections make the Transformer data-hungry. Star-Transformer exploits short-term dependencies more heavily by keeping the connections between adjacent tokens but relaying long dependencies via a central node, thereby reducing the number of connections from O (n2) to O (n). This centralized structure has trouble handling long sentences. This paper proposes Segment Tree Transformer (SegTree-Transformer), a middle ground that organizes input tokens into a tree of word spans, and extends attentions to those spans as well. It yields O (n log n) connections which greatly improves space and time complexity, and outperforms alternatives in a number of NLP tasks with moderately-sized data.", "num_citations": "2\n", "authors": ["1817"]}
{"title": "MoonBox: debugging with online slicing and dryrun\n", "abstract": " Efficient tools are indispensable in the battle against software bugs. In this short paper, we introduce two techniques that target different phases of an interactive and iterative debugging session. To make slice-assisted log analysis practical to help fault diagnosis, slicing itself must be done instantaneously. We split the costly slicing computation into online and offline, and employ incremental updates after program edits. The result is a vast reduction of slicing cost. For the benchmarks we tested, slices can be computed in the range of seconds, which is 0.02%~ 6.5% of the unmodified slicing algorithm.", "num_citations": "2\n", "authors": ["1817"]}
{"title": "Corey: an Operating System for Many Cores\n", "abstract": " Multiprocessor operating system kernels typically provide complex abstractions implemented with shared data structures protected by locks. On multicore systems this design may cause the kernel to be a bottleneck due to the costs of contention for shared data and locks, and the costs of inter-core TLB invalidation. Corey is a new operating system based on the principle that applications should control all sharing: all kernel data structures should be local to a processor core unless directed otherwise by the application.", "num_citations": "2\n", "authors": ["1817"]}
{"title": "Islands in the MSN Messenger buddy network\n", "abstract": " The MSN messenger buddy network comprises a giant component and many small islands. Being curious about the existence of the small islands, we study their topologies and find that they favor part of the possible topologies. Specifically, fully connected and star-shape topologies become more and more popular as the size of the islands increases. The tendency towards certain topologies suggests specific ways of interactions between users in the islands. Using egonetworks as a tool, we also compare the local structures in islands and those in the giant component. The comparison shows that the giant component ego-networks are larger but sparser than those in the islands. The difference implies different interaction behaviors between users in the islands and the giant component.", "num_citations": "2\n", "authors": ["1817"]}
{"title": "Can a file system virtualize processors?\n", "abstract": " Supercomputers are comprising more and more processors and these processors are increasingly heterogeneous, with differing performance characteristics. The conventional programming models assume that all nodes run in lockstep. Thus, applications run at the speed of the least powerful processor. We introduce DesyncFS, a new programming model based on the block abstraction of traditional file systems. It virtualizes the performance characteristics of processors; this allows their heterogeneity to be hidden. We show that DesyncFS allows cluster throughput to scale with average processor throughput instead of being limited by the slowest processor.", "num_citations": "2\n", "authors": ["1817"]}
{"title": "New routing update mechanisms of kademlia\n", "abstract": " Putting forward new routing update mechanisms, improved lookup performance of standard kademlia protocol: each node would advertise others in a range when it is leaving overlay network; and each node should backward notice its routing providers about the failed links founded in its routing lookup process, then lookup initiator and routing providers could update their k-buckets corporately. In the new mechanisms, the dead nodes would be accessed in very low possibility so that the lookup process can be more effective, especially under churn state. Determine arguments of mechanisms and analyze the lookup performance through simulation, prove the validity of the mechanisms.", "num_citations": "1\n", "authors": ["1817"]}
{"title": "A dataflow system for unreliable computing environments\n", "abstract": " This paper presents the design, implementation and evaluation of a dataflow system, including a dataflow programming model and a dataflow engine, for coarse-grained distributed data intensive applications. The dataflow programming model provides users with a transparent interface for application programming and execution management in a parallel and distributed computing environment. The dataflow engine dispatches the tasks onto candidate distributed computing resources in the system, and manages failures and load balancing problems in a transparent manner. The system has been implemented over .NET platform and deployed in a Windows Desktop Grid. This paper uses two benchmarks to demonstrate the scalability and fault tolerance properties of our system.", "num_citations": "1\n", "authors": ["1817"]}
{"title": "High Availability Issues in DSM Systems: Research Opportunities\n", "abstract": " This report is a summary of several weeks\u2019 study of the High Availability (HA) issues of DSM system. In this I am integrating relevant materials that I learn from various literature resources and many notes that I have taken during the period. The intent is to give a first-cut understanding of the issues and discuss some potential research opportunities. They are by no means final. The report is written only in the hope of getting my thoughts more organized and to welcome critiques and suggestions from my colleagues, who have already helped me a great deal to get me started.", "num_citations": "1\n", "authors": ["1817"]}