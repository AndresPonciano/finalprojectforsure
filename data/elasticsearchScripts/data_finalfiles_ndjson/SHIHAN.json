{"title": "Extracting top-k insights from multi-dimensional data\n", "abstract": " OLAP tools have been extensively used by enterprises to make better and faster decisions. Nevertheless, they require users to specify group-by attributes and know precisely what they are looking for. This paper takes the first attempt towards automatically extracting top-kinsights from multi-dimensional data. This is useful not only for non-expert users, but also reduces the manual effort of data analysts. In particular, we propose the concept of insight which captures interesting observation derived from aggregation results in multiple steps (eg, rank by a dimension, compute the percentage of measure by a dimension). An example insight is:``Brand B's rank (across brands) falls along the year, in terms of the increase in sales''. Our problem is to compute the top-k insights by a score function. It poses challenges on (i) the effectiveness of the result and (ii) the efficiency of computation. We propose a meaningful scoring\u00a0\u2026", "num_citations": "68\n", "authors": ["236"]}
{"title": "Systematically debugging IoT control system correctness for building automation\n", "abstract": " Advances and standards in Internet of Things (IoT) have simplified the realization of building automation. However, non-expert IoT users still lack tools that can help them to ensure the underlying control system correctness: user-programmable logics match the user intention. In fact, non-expert IoT users lack the necessary know-how of domain experts. This paper presents our experience in running a building automation service based on the Salus framework. Complementing efforts that simply verify the IoT control system correctness, Salus takes novel steps to tackle practical challenges in automated debugging of identified policy violations, for non-expert IoT users. First, Salus leverages formal methods to localize faulty user-programmable logics. Second, to debug these identified faults, Salus selectively transforms the control system logics into a set of parameterized equations, which can then be solved by popular\u00a0\u2026", "num_citations": "42\n", "authors": ["236"]}
{"title": "Tablesense: Spreadsheet table detection with convolutional neural networks\n", "abstract": " Spreadsheet table detection is the task of detecting all tables on a given sheet and locating their respective ranges. Automatic table detection is a key enabling technique and an initial step in spreadsheet data intelligence. However, the detection task is challenged by the diversity of table structures and table layouts on the spreadsheet. Considering the analogy between a cell matrix as spreadsheet and a pixel matrix as image, and encouraged by the successful application of Convolutional Neural Networks (CNN) in computer vision, we have developed TableSense, a novel end-to-end framework for spreadsheet table detection. First, we devise an effective cell featurization scheme to better leverage the rich information in each cell; second, we develop an enhanced convolutional neural network model for table detection to meet the domain-specific requirement on precise table boundary detection; third, we propose an effective uncertainty metric to guide an active learning based smart sampling algorithm, which enables the efficient build-up of a training dataset with 22,176 tables on 10,220 sheets with broad coverage of diverse table structures and layouts. Our evaluation shows that TableSense is highly effective with 91.3% recall and 86.5% precision in EoB-2 metric, a significant improvement over both the current detection algorithm that are used in commodity spreadsheet tools and state-of-the-art convolutional neural networks in computer vision.", "num_citations": "30\n", "authors": ["236"]}
{"title": "Quickinsights: Quick and automatic discovery of insights from multi-dimensional data\n", "abstract": " Discovering interesting data patterns is a common and important analytical need in data, with increasing user demand for automated discovery abilities. However, automatically discovering interesting patterns from multi-dimensional data remains challenging. Existing techniques focus on mining individual types of patterns. There is a lack of unified formulation for different pattern types, as well as general mining frameworks to derive them effectively and efficiently. We present a novel technique QuickInsights, which quickly and automatically discovers interesting patterns from multi-dimensional data. QuickInsights proposes a unified formulation of interesting patterns, called insights, and designs a systematic mining framework to discover high-quality insights efficiently. We demonstrate the effectiveness and efficiency of QuickInsights through our evaluation on 447 real datasets as well as user studies on both expert\u00a0\u2026", "num_citations": "30\n", "authors": ["236"]}
{"title": "Systematically ensuring the confidence of real-time home automation IoT systems\n", "abstract": " Recent advances and industry standards in Internet of Things (IoT) have accelerated the real-world adoption of connected devices. To manage this hybrid system of digital real-time devices and analog environments, the industry has pushed several popular home automation IoT (HA-IoT) frameworks, such as If-This-Then-That (IFTTT), Apple HomeKit, and Google Brillo. Typically, users author device interactions by specifying the triggering sensor event and the triggered device command. In this seemingly simple software system, two dominant factors govern the system confidence properties with respect to the physical world. First, IoT users are largely nonexperts who lack the comprehensive consideration regarding potential impact and joint effect with existing rules. Second, while the increasing complexity of IoT devices enables fine-grained control (e.g., heater temperature) of continuous real-time environments\u00a0\u2026", "num_citations": "29\n", "authors": ["236"]}
{"title": "Feature design for HMM based Eastern Asian character recognition\n", "abstract": " An exemplary method for online character recognition of East Asian characters includes acquiring time sequential, online ink data for a handwritten East Asian character, conditioning the ink data to produce conditioned ink data where the conditioned ink data includes information as to writing sequence of the handwritten East Asian character and extracting features from the conditioned ink data where the features include a tangent feature, a curvature feature, a local length feature, a connection point feature and an imaginary stroke feature. Such a method may determine neighborhoods for ink data and extract features for each neighborhood. An exemplary Hidden Markov Model based character recognition system may use various exemplary methods for training and character recognition.", "num_citations": "28\n", "authors": ["236"]}
{"title": "Cross-trace scalable issue detection and clustering\n", "abstract": " Techniques and systems for cross-trace scalable issue detection and clustering that scale-up trace analysis for issue detection and root-cause clustering using a machine learning based approach are described herein. These techniques enable a scalable performance analysis framework for computing devices addressing issue detection, which is designed as a multiple scale feature for learning based on issue detection, and root cause clustering. In various embodiments the techniques employ a cross-trace similarity model, which is defined to hierarchically cluster problems detected in the learning based issue detection via butterflies of trigram stacks. The performance analysis framework is scalable to manage millions of traces, which include high problem complexity.", "num_citations": "22\n", "authors": ["236"]}
{"title": "Analyzing software performance issues\n", "abstract": " Execution traces are collected from multiple execution instances that exhibit performance issues such as slow execution. Call stacks are extracted from the execution traces, and the call stacks are mined to identify frequently occurring function call patterns. The call patterns are then clustered, and used to identify groups of execution instances whose performance issues may be caused by common problematic program execution patterns.", "num_citations": "19\n", "authors": ["236"]}
{"title": "Expandable group identification in spreadsheets\n", "abstract": " Spreadsheets are widely used in various business tasks. Spreadsheet users may put similar data and computations by repeating a block of cells (a unit) in their spreadsheets. We name the unit and all its expanding ones as an expandable group. All units in an expandable group share the same or similar formats and semantics. As a data storage and management tool, expandable groups represent the fundamental structure in spreadsheets. However, existing spreadsheet systems do not recognize any expandable groups. Therefore, other spreadsheet analysis tools, eg, data integration and fault detection, cannot utilize this structure of expandable groups to perform precise analysis. In this paper, we propose ExpCheck to automatically extract expandable groups in spreadsheets. We observe that continuous units that share the similar formats and semantics are likely to be an expandable group. Inspired by this, we\u00a0\u2026", "num_citations": "13\n", "authors": ["236"]}
{"title": "Semantic structure extraction for spreadsheet tables with a multi-task learning architecture\n", "abstract": " Semantic structure extraction for spreadsheets includes detecting table regions, recognizing structural components and classifying cell types. Automatic semantic structure extraction is key to automatic data transformation from various table structures into canonical schema so as to enable data analysis and knowledge discovery. However, they are challenged by the diverse table structures and the spatial-correlated semantics on cell grids. To learn spatial correlations and capture semantics on spreadsheets, we have developed a novel learning-based framework for spreadsheet semantic structure extraction. First, we propose a multi-task framework that learns table region, structural components and cell types jointly; second, we leverage the advances of the recent language model to capture semantics in each cell value; third, we build a large human-labeled dataset with broad coverage of table structures. Our evaluation shows that our proposed multi-task framework is highly effective that outperforms the results of training each task separately.", "num_citations": "11\n", "authors": ["236"]}
{"title": "Uncovering JavaScript performance code smells relevant to type mutations\n", "abstract": " In dynamic typing languages such as JavaScript, object types can be mutated easily such as by adding a field to an object. However, compiler optimizations rely on a fixed set of types, unintentional type mutations can invalidate the speculative code generated by the type-feedback JIT and deteriorate the quality of compiler optimizations. Since type mutations are invisible, finding and understanding the performance issues relevant to type mutations can be an overwhelming task to programmers. We develop a tool JSweeter to detect performance bugs incurred by type mutations based on the type evolution graphs extracted from program execution. We apply JSweeter to the Octane benchmark suite and identify 46 performance issues, where 19 issues are successfully fixed with the refactoring hints generated by JSweeter and the average performance gain is 5.3\u00a0% (up\u00a0to 23\u00a0%). The result is persuasive\u00a0\u2026", "num_citations": "11\n", "authors": ["236"]}
{"title": "Radical set determination for HMM based east asian character recognition\n", "abstract": " Exemplary techniques are described for selecting radical sets for use in probabilistic East Asian character recognition algorithms. An exemplary technique includes applying a decomposition rule to each East Asian character of the set to generate a progressive splitting graph where the progressive splitting graph comprises radicals as nodes, formulating an optimization problem to find an optimal set of radicals to represent the set of East Asian characters using maximum likelihood and minimum description length and solving the optimization problem for the optimal set of radicals. Another exemplary technique includes selecting an optimal set of radicals by using a general function that characterizes a radical with respect to other East Asian characters and a complex function that characterizes complexity of a radical.", "num_citations": "11\n", "authors": ["236"]}
{"title": "A unified framework for recognizing handwritten chemical expressions\n", "abstract": " Chemical expressions have more variant structures in 2-D space than that in math equations. In this paper we propose a unified framework for recognizing handwritten chemical expressions including both inorganic and organic expressions. A set of novel statistical algorithms is presented in two key components of this framework: symbol grouping and structure analysis. Non-symbol modeling and inter-group modeling are proposed to achieve better grouping result, and bond modeling is proposed to group the special bond symbols in the unified framework. A graph-based representation (CESG) is defined for representing generic chemical expressions, and the structure analysis problem is formulated as a search problem for CESG over a weighted direction graph. Experiments on a database of more than 35,000 expressions were conducted and results are presented.", "num_citations": "11\n", "authors": ["236"]}
{"title": "Handwriting Recognition System Using Multiple Path Recognition Framework\n", "abstract": " Described is a multi-path handwriting recognition framework based upon stroke segmentation, symbol recognition, two-dimensional structure analysis and semantic structure analysis. Electronic pen input corresponding to handwritten input (eg, a chemical expression) is recognized and output via a data structure, which may include multiple recognition candidates. A recognition framework performs stroke segmentation and symbol recognition on the input, and analyzes the structure of the input to output the data structure corresponding to recognition results. For chemical expressions, the structural analysis may perform a conditional sub-expression analysis for inorganic expressions, or organic bond detection, connection relationship analysis, organic atom determination and/or conditional sub-expression analysis for organic expressions. The structural analysis also performs subscript, superscript analysis and\u00a0\u2026", "num_citations": "9\n", "authors": ["236"]}
{"title": "Structure-aware pre-training for table understanding with tree-based transformers\n", "abstract": " Tables are widely used with various structures to organize and present data. Recent attempts on table understanding mainly focus on relational tables, yet overlook to other common table structures. In this paper, we propose TUTA, a unified pre-training architecture for understanding generally structured tables. Since understanding a table needs to leverage both spatial, hierarchical, and semantic information, we adapt the self-attention strategy with several key structure-aware mechanisms. First, we propose a novel tree-based structure called a bi-dimensional coordinate tree, to describe both the spatial and hierarchical information in tables. Upon this, we extend the pre-training architecture with two core mechanisms, namely the tree-based attention and tree-based position embedding. Moreover, to capture table information in a progressive manner, we devise three pre-training objectives to enable representations at the token, cell, and table levels. TUTA pre-trains on a wide range of unlabeled tables and fine-tunes on a critical task in the field of table structure understanding, i.e. cell type classification. Experiment results show that TUTA is highly effective, achieving state-of-the-art on four well-annotated cell type classification datasets.", "num_citations": "6\n", "authors": ["236"]}
{"title": "Analyzing Program Execution\n", "abstract": " A call pattern database is mined to identify frequently occurring call patterns related to program execution instances. An SVM classifier is iteratively trained based at least in part on classifications provided by human analysts; at each iteration, the SVM classifier identifies boundary cases, and requests human analysis of these cases. The trained SVM classifier is then applied to call pattern pairs to produce similarity measures between respective call patterns of each pair, and the call patterns are clustered based on the similarity measures.", "num_citations": "6\n", "authors": ["236"]}
{"title": "Radical-based HMM modeling for handwritten East Asian characters\n", "abstract": " Exemplary methods, systems, and computer-readable media for developing, training and/or using models for online handwriting recognition of characters are described. An exemplary method for building a trainable radical-based HMM for use in character recognition includes defining radical nodes, where a radical node represents a structural element of an character, and defining connection nodes, where a connection node represents a spatial relationship between two or more radicals. Such a method may include determining a number of paths in the radical-based HMM using subsequence direction histogram vector (SDHV) clustering and determining a number of states in the radical-based HMM using curvature scale space-based (CSS) corner detection.", "num_citations": "6\n", "authors": ["236"]}
{"title": "Systematic multi-path HMM topology design for online handwriting recognition of east asian characters\n", "abstract": " This paper presents a systematic multi-path HMM topology design algorithm to better model online handwriting of East Asian characters. This data-driven algorithm solves three key problems in HMM topology design. First, HMM path number determination is formalized as a clustering problem using subsequence direction histogram vector (SDHV) as feature of both writing order and style. Second, curvature scale space-based (CSS-based) substroke segmentation is used to calculate the optimal state number and initial state parameters. Third, self-rotation restricted corner state and imaginary stroke state are designed to determine state connectivity and Gaussian mixture number in order to achieve better state alignment. Experiments on large character sets demonstrate both a significant relative error reduction rate and high recognition accuracy using the proposed algorithm.", "num_citations": "5\n", "authors": ["236"]}
{"title": "TUTA: Tree-based Transformers for Generally Structured Table Pre-training\n", "abstract": " We propose TUTA, a unified pre-training architecture for understanding generally structured tables. Noticing that understanding a table requires spatial, hierarchical, and semantic information, we enhance transformers with three novel structure-aware mechanisms. First, we devise a unified tree-based structure, called a bi-dimensional coordinate tree, to describe both the spatial and hierarchical information of generally structured tables. Upon this, we propose tree-based attention and position embedding to better capture the spatial and hierarchical information. Moreover, we devise three progressive pre-training objectives to enable representations at the token, cell, and table levels. We pre-train TUTA on a wide range of unlabeled web and spreadsheet tables and fine-tune it on two critical tasks in the field of table structure understanding: cell type classification and table type classification. Experiments show that\u00a0\u2026", "num_citations": "3\n", "authors": ["236"]}
{"title": "TabularNet: A Neural Network Architecture for Understanding Semantic Structures of Tabular Data\n", "abstract": " Tabular data are ubiquitous for the widespread applications of tables and hence have attracted the attention of researchers to extract underlying information. One of the critical problems in mining tabular data is how to understand their inherent semantic structures automatically. Existing studies typically adopt Convolutional Neural Network (CNN) to model the spatial information of tabular structures yet ignore more diverse relational information between cells, such as the hierarchical and paratactic relationships. To simultaneously extract spatial and relational information from tables, we propose a novel neural network architecture, TabularNet. The spatial encoder of TabularNet utilizes the row/column-level Pooling and the Bidirectional Gated Recurrent Unit (Bi-GRU) to capture statistical information and local positional correlation, respectively. For relational information, we design a new graph construction method\u00a0\u2026", "num_citations": "3\n", "authors": ["236"]}
{"title": "Table2Charts: Recommending Charts by Learning Shared Table Representations\n", "abstract": " It is common for people to create different types of charts to explore a multi-dimensional dataset (table). However, to recommend commonly composed charts in real world, one should take the challenges of efficiency, imbalanced data and table context into consideration. In this paper, we propose Table2Charts framework which learns common patterns from a large corpus of (table, charts) pairs. Based on deep Q-learning with copying mechanism and heuristic searching, Table2Charts does table-to-sequence generation, where each sequence follows a chart template. On a large spreadsheet corpus with 165k tables and 266k charts, we show that Table2Charts could learn a shared representation of table fields so that recommendation tasks on different chart types could mutually enhance each other. Table2Charts outperforms other chart recommendation systems in both multi-type task (with doubled recall numbers R@3=0.61 and R@1=0.43) and human evaluations.", "num_citations": "2\n", "authors": ["236"]}
{"title": "Hitab: A hierarchical table dataset for question answering and natural language generation\n", "abstract": " Tables are often created with hierarchies, but existing works on table reasoning mainly focus on flat tables and neglect hierarchical tables. Hierarchical tables challenge existing methods by hierarchical indexing, as well as implicit relationships of calculation and semantics. This work presents HiTab, a free and open dataset to study question answering (QA) and natural language generation (NLG) over hierarchical tables. HiTab is a cross-domain dataset constructed from a wealth of statistical reports (analyses) and Wikipedia pages, and has unique characteristics: (1) nearly all tables are hierarchical, and (2) both target sentences for NLG and questions for QA are revised from original, meaningful, and diverse descriptive sentences authored by analysts and professions of reports. (3) to reveal complex numerical reasoning in statistical analyses, we provide fine-grained annotations of entity and quantity alignment. HiTab provides 10,686 QA pairs and descriptive sentences with well-annotated quantity and entity alignment on 3,597 tables with broad coverage of table hierarchies and numerical reasoning types. Targeting hierarchical structure, we devise a novel hierarchy-aware logical form for symbolic reasoning over tables, which shows high effectiveness. Targeting complex numerical reasoning, we propose partially supervised training given annotations of entity and quantity alignment, which helps models to largely reduce spurious predictions in the QA task. In the NLG task, we find that entity and quantity alignment also helps NLG models to generate better results in a conditional generation setting. Experiment results of state-of-the-art baselines\u00a0\u2026", "num_citations": "1\n", "authors": ["236"]}
{"title": "Table2Charts: Recommending Charts by Learning Shared Table Representations\n", "abstract": " It is common for people to create different types of charts to explore a multi-dimensional dataset (table). However, to recommend commonly composed charts in real world, one should take the challenges of efficiency, imbalanced data and table context into consideration. In this paper, we propose Table2Charts framework which learns common patterns from a large corpus of (table, charts) pairs. Based on deep Q-learning with copying mechanism and heuristic searching, Table2Charts does table-to-sequence generation, where each sequence follows a chart template. On a large spreadsheet corpus with 165k tables and 266k charts, we show that Table2Charts could learn a shared representation of table fields so that recommendation tasks on different chart types could mutually enhance each other. Table2Charts outperforms other chart recommendation systems in both multi-type task (with doubled recall numbers\u00a0\u2026", "num_citations": "1\n", "authors": ["236"]}
{"title": "Neural Code Summarization: How Far Are We?\n", "abstract": " Source code summaries are important for the comprehension and maintenance of programs. However, there are plenty of programs with missing, outdated, or mismatched summaries. Recently, deep learning techniques have been exploited to automatically generate summaries for given code snippets. To achieve a profound understanding of how far we are from solving this problem, in this paper, we conduct a systematic and in-depth analysis of five state-of-the-art neural source code summarization models on three widely used datasets. Our evaluation results suggest that: (1) The BLEU metric, which is widely used by existing work for evaluating the performance of the summarization models, has many variants. Ignoring the differences among the BLEU variants could affect the validity of the claimed results. Furthermore, we discover an important, previously unknown bug about BLEU calculation in a commonly-used software package. (2) Code pre-processing choices can have a large impact on the summarization performance, therefore they should not be ignored. (3) Some important characteristics of datasets (corpus size, data splitting method, and duplication ratio) have a significant impact on model evaluation. Based on the experimental results, we give some actionable guidelines on more systematic ways for evaluating code summarization and choosing the best method in different scenarios. We also suggest possible future research directions. We believe that our results can be of great help for practitioners and researchers in this interesting area.", "num_citations": "1\n", "authors": ["236"]}
{"title": "On the Evaluation of Commit Message Generation Models: An Experimental Study\n", "abstract": " Commit messages are natural language descriptions of code changes, which are important for program understanding and maintenance. However, writing commit messages manually is time-consuming and laborious, especially when the code is updated frequently. Various approaches utilizing generation or retrieval techniques have been proposed to automatically generate commit messages. To achieve a better understanding of how the existing approaches perform in solving this problem, this paper conducts a systematic and in-depth analysis of the state-of-the-art models and datasets. We find that: (1) Different variants of the BLEU metric are used in previous works, which affects the evaluation and understanding of existing methods. (2) Most existing datasets are crawled only from Java repositories while repositories in other programming languages are not sufficiently explored. (3) Dataset splitting strategies can influence the performance of existing models by a large margin. Some models show better performance when the datasets are split by commit, while other models perform better when the datasets are split by timestamp or by project. Based on our findings, we conduct a human evaluation and find the BLEU metric that best correlates with the human scores for the task. We also collect a large-scale, information-rich, and multi-language commit message dataset MCMD and evaluate existing models on this dataset. Furthermore, we conduct extensive experiments under different dataset splitting strategies and suggest the suitable models under different scenarios. Based on the experimental results and findings, we provide feasible\u00a0\u2026", "num_citations": "1\n", "authors": ["236"]}
{"title": "Is a single model enough? mucos: A multi-model ensemble learning for semantic code search\n", "abstract": " Recently, deep learning methods have become mainstream in code search since they do better at capturing semantic correlations between code snippets and search queries and have promising performance. However, code snippets have diverse information from different dimensions, such as business logic, specific algorithm, and hardware communication, so it is hard for a single code representation module to cover all the perspectives. On the other hand, as a specific query may focus on one or several perspectives, it is difficult for a single query representation module to represent different user intents. In this paper, we propose MuCoS, a multi-model ensemble learning architecture for semantic code search. It combines several individual learners, each of which emphasizes a specific perspective of code snippets. We train the individual learners on different datasets which contain different perspectives of code information, and we use a data augmentation strategy to get these different datasets. Then we ensemble the learners to capture comprehensive features of code snippets.", "num_citations": "1\n", "authors": ["236"]}
{"title": "Learning Formatting Style Transfer and Structure Extraction for Spreadsheet Tables with a Hybrid Neural Network Architecture\n", "abstract": " Table formatting is a typical task for spreadsheet users to better exhibit table structures and data relationships. But quickly and effectively formatting tables is a challenge for users. Lots of manual operations are needed, especially for complex tables. In this paper, we propose techniques for table formatting style transfer, ie, to automatically format a target table according to the style of a reference table. Considering the latent many-to-many mappings between table structures and formats, we propose CellNet, which is a novel end-to-end, multi-task model leveraging conditional Generative Adversarial Networks (cGANs) with three key components to (1) model and recognize table structures;(2) encode formatting styles;(3) learn and apply the latent mapping based on recognized table structure and encoded style, respectively. Moreover, we build up a spreadsheet table corpus containing 5,226 tables with high-quality\u00a0\u2026", "num_citations": "1\n", "authors": ["236"]}
{"title": "Neural Formatting for Spreadsheet Tables\n", "abstract": " Spreadsheets are popular and widely used for data presentation and management, where users create tables in various structures to organize and present data. Table formatting is an important yet tedious task for better exhibiting table structures and data relationships. However, without the aid of intelligent tools, manual formatting remains a tedious and time-consuming task. In this paper, we propose CellGAN, a neural formatting model for learning and recommending formats of spreadsheet tables. Based on a novel conditional generative adversarial network (cGAN) architecture, CellGAN learns table formatting from real-world spreadsheet tables in a self-supervised fashion without requiring human labeling. In CellGAN we devise two mechanisms, row/column-wise pooling and local refinement network, to address challenges from the spreadsheet domain. We evaluate the effectiveness of CellGAN against real\u00a0\u2026", "num_citations": "1\n", "authors": ["236"]}
{"title": "Reliable and Efficient Anytime Skeleton Learning\n", "abstract": " Skeleton Learning (SL) is the task for learning an undirected graph from the input data that captures their dependency relations. SL plays a pivotal role in causal learning and has attracted growing attention in the research community lately. Due to the high time complexity, anytime SL has emerged which learns a skeleton incrementally and improves it overtime. In this paper, we first propose and advocate the reliability requirement for anytime SL to be practically useful. Reliability requires the intermediately learned skeleton to have precision and persistency. We also present REAL, a novel Reliable and Efficient Anytime Learning algorithm of skeleton. Specifically, we point out that the commonly existing Functional Dependency (FD) among variables could make the learned skeleton violate faithfulness assumption, thus we propose a theory to resolve such incompatibility. Based on this, REAL conducts SL on a reduced set of variables with guaranteed correctness thus drastically improves efficiency. Furthermore, it employs a novel edge-insertion and best-first strategy in anytime fashion for skeleton growing to achieve high reliability and efficiency. We prove that the skeleton learned by REAL converges to the correct skeleton under standard assumptions. Thorough experiments were conducted on both benchmark and real-world datasets demonstrate that REAL significantly outperforms the other state-of-the-art algorithms.", "num_citations": "1\n", "authors": ["236"]}
{"title": "Multi-dimensional data insight interaction\n", "abstract": " A computing apparatus of an insight interfacing system receives from a user a request for a dataset comprising a plurality of subspaces of a multi-dimensional data structure. Insights are received based on the received request then presented on a display device. Also, a chart may be presented based on the received request. The computing apparatus receives a selection of at least a portion of the presented one or more insights or a portion of the chart, then receives contextual insights based on the selected portion and presents the contextual insights on the display device.", "num_citations": "1\n", "authors": ["236"]}
{"title": "Feature design for character recognition\n", "abstract": " An exemplary method for online character recognition of characters includes acquiring time sequential, online ink data for a handwritten character, conditioning the ink data to produce conditioned ink data where the conditioned ink data includes information as to writing sequence of the handwritten character and extracting features from the conditioned ink data where the features include a tangent feature, a curvature feature, a local length feature, a connection point feature and an imaginary stroke feature. Such a method may determine neighborhoods for ink data and extract features for each neighborhood. An exemplary character recognition system may use various exemplary methods for training and character recognition.", "num_citations": "1\n", "authors": ["236"]}