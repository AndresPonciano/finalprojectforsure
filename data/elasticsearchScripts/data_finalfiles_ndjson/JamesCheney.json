{"title": "Cyclone: a safe dialect of C.\n", "abstract": " Cyclone is a safe dialect of C. It has been designed from the ground up to prevent the buffer overflows, format string attacks, and memory management errors that are common in C programs, while retaining C's syntax and semantics. This paper examines safety violations enabled by C's design, and shows how Cyclone avoids them, without giving up C's hallmark control over low-level details such as data representation and memory management.", "num_citations": "1001\n", "authors": ["1881"]}
{"title": "Provenance in databases: Why, how, and where\n", "abstract": " In September 2008, Google News promoted an undated article about United Airlines' near bankruptcy in 2002. In the ensuing panic, the share price of United Airlines dropped by around 75% in a few hours. This problem was due in part to the fact that the article lacked provenance that readers could have used to determine that it was out of date. In an increasingly networked world, understanding of provenance is essential for establishing trust in data stored in databases and exchanged among Web sites. It is also critical to the process of making key business, scientific, and governmental decisions. Modern database systems are capable of producing answers efficiently. However, they are generally lacking capabilities to explain provenance such as why and how the answers were produced, or where the data in the result came from. In recent years, different notions of provenance for database queries have been studied by the authors and a growing community of researchers in databases and scientific computation. Provenance in Databases reviews research over the past ten years on why, how, and where provenance, clarifies the relationships among these notions of provenance, and describes some of their applications in confidence computation, view maintenance and update, debugging, and annotation propagation. Provenance in Databases is intended for engineers and researchers who would like to familiarize themselves with the foundations, as well as the many challenges in the field of database provenance.", "num_citations": "694\n", "authors": ["1881"]}
{"title": "Prov-o: The prov ontology\n", "abstract": " The PROV Ontology (PROV-O) expresses the PROV Data Model using the OWL2 Web Ontology Language. It provides a set of classes, properties, and restrictions that can be used to represent and interchange provenance information generated in different systems and under different contexts. It can also be specialized to create new classes and properties to model provenance information for different applications and domains.", "num_citations": "570\n", "authors": ["1881"]}
{"title": "Region-based memory management in Cyclone\n", "abstract": " Cyclone is a type-safe programming language derived from C. The primary design goal of Cyclone is to let programmers control data representation and memory management without sacrificing type-safety. In this paper, we focus on the region-based memory management of Cyclone and its static typing discipline. The design incorporates several advancements, including support for region subtyping and a coherent integration with stack allocation and a garbage collector. To support separate compilation, Cyclone requires programmers to write some explicit region annotations, but a combination of default annotations, local type inference, and a novel treatment of region effects reduces this burden. As a result, we integrate C idioms in a region-based framework. In our experience, porting legacy C to Cyclone has required altering about 8% of the code; of the changes, only 6%(of the 8%) were region annotations.", "num_citations": "490\n", "authors": ["1881"]}
{"title": "Provenance management in curated databases\n", "abstract": " Curated databases in bioinformatics and other disciplines are the result of a great deal of manual annotation, correction and transfer of data from other sources. Provenance information concerning the creation, attribution, or version history of such data is crucial for assessing its integrity and scientific value. General purpose database systems provide little support for tracking provenance, especially when data moves among databases. This paper investigates general-purpose techniques for recording provenance for data that is copied among databases. We describe an approach in which we track the user's actions while browsing source databases and copying data into a curated database, in order to record the user's actions in a convenient, queryable form. We present an implementation of this technique and use it to evaluate the feasibility of database support for provenance management. Our experiments show\u00a0\u2026", "num_citations": "401\n", "authors": ["1881"]}
{"title": "Compressing XML with multiplexed hierarchical PPM models\n", "abstract": " We established a working Extensible Markup Language (XML) compression benchmark based on text compression, and found that bzip2 compresses XML best, albeit more slowly than gzip. Our experiments verified that T/sub XMILL/ speeds up and improves compression using gzip and bounded-context PPM by up to 15%, but found that it worsens the compression for bzip2 and PPM. We describe alternative approaches to XML compression that illustrate other tradeoffs between speed and effectiveness. We describe experiments using several text compressors and XMILL to compress a variety of XML documents. Using these as a benchmark, we describe our two main results: an online binary encoding for XML called Encoded SAX (ESAX) that compresses better and faster than existing methods; and an online, adaptive, XML-conscious encoding based on prediction by partial match (PPM) called multiplexed\u00a0\u2026", "num_citations": "295\n", "authors": ["1881"]}
{"title": "First-class phantom types\n", "abstract": " Classical phantom types are datatypes in which type constraints are  expressed using type variables that do not appear in the datatype cases themselves.  They can be used to embed typed languages into Haskell or ML.  However,  while such encodings guarantee that only well-formed data can be constructed, they do not permit type-safe deconstruction without additional tagging and run-time checks.  We introduce first-class phantom types, which make such constraints explicit via type equations.  Examples of first-class phantom types include typed type representations and typed higher-order abstract syntax trees. These types can be used to support typed generic functions, dynamic typing, and staged compilation in higher-order, statically typed languages such as Haskell or Standard ML.  In our system, type constraints can be equations between type constructors as well as type functions of higher-order kinds.  We prove type soundness and decidability for a Haskell-like language extended by first-class phantom types.", "num_citations": "281\n", "authors": ["1881"]}
{"title": "The W3C PROV family of specifications for modelling provenance metadata\n", "abstract": " Provenance, a form of structured metadata designed to record the origin or source of information, can be instrumental in deciding whether information is to be trusted, how it can be integrated with other diverse information sources, and how to establish attribution of information to authors throughout its history. The PROV set of specifications, produced by the World Wide Web Consortium (W3C), is designed to promote the publication of provenance information on the Web, and offers a basis for interoperability across diverse provenance management systems. The PROV provenance model is deliberately generic and domain-agnostic, but extension mechanisms are available and can be exploited for modelling specific domains. This tutorial provides an account of these specifications. Starting from intuitive and informal examples that present idiomatic provenance patterns, it progressively introduces the relational\u00a0\u2026", "num_citations": "190\n", "authors": ["1881"]}
{"title": "Curated databases\n", "abstract": " Curated databases are databases that are populated and updated with a great deal of human effort. Most reference works that one traditionally found on the reference shelves of libraries--dictionaries, encyclopedias, gazetteers etc.--are now curated databases. Since it is now easy to publish databases on the web, there has been an explosion in the number of new curated databases used in scientific research. The value of curated databases lies in the organization and the quality of the data they contain. Like the paper reference works they have replaced, they usually represent the efforts of a dedicated group of people to produce a definitive description of some subject area.", "num_citations": "178\n", "authors": ["1881"]}
{"title": "On the expressiveness of implicit provenance in query and update languages\n", "abstract": " Information describing the origin of data, generally referred to as provenance, is important in scientific and curated databases where it is the basis for the trust one puts in their contents. Since such databases are constructed using operations of both query and update languages, it is of paramount importance to describe the effect of these languages on provenance. In this article we study provenance for query and update languages that are closely related to SQL, and compare two ways in which they can manipulate provenance so that elements of the input are rearranged to elements of the output: implicit provenance, where a query or update only provides the rearranged output, and provenance is provided implicitly by a default provenance semantics; and explicit provenance, where a query or update provides both the output and the description of the provenance of each component of the output. Although explicit\u00a0\u2026", "num_citations": "154\n", "authors": ["1881"]}
{"title": "A lightweight implementation of generics and dynamics\n", "abstract": " The recent years have seen a number of proposals for extending statically typed languages by dynamics or generics. Most proposals---if not all---require significant extensions to the underlying language. In this paper we show that this need not be the case. We propose a particularly lightweight extension that supports both dynamics and generics. Furthermore, the two features are smoothly integrated: dynamic values, for instance, can be passed to generic functions. Our proposal makes do with a standard Hindley-Milner type system augmented by existential types. Building upon these ideas we have implemented a small library that is readily usable both with Hugs and with the Glasgow Haskell compiler.", "num_citations": "143\n", "authors": ["1881"]}
{"title": "Provenance: a future history\n", "abstract": " Science, industry, and society are being revolutionized by radical new capabilities for information sharing, distributed computation, and collaboration offered by the World Wide Web. This revolution promises dramatic benefits but also poses serious risks due to the fluid nature of digital information. One important cross-cutting issue is managing and recording provenance, or metadata about the origin, context, or history of data. We posit that provenance will play a central role in emerging advanced digital infrastructures. In this paper, we outline the current state of provenance research and practice, identify hard open research problems involving provenance semantics, formal modeling, and security, and articulate a vision for the future of provenance.", "num_citations": "129\n", "authors": ["1881"]}
{"title": "YesWorkflow: a user-oriented, language-independent tool for recovering workflow information from scripts\n", "abstract": " Scientific workflow management systems offer features for composing complex computational pipelines from modular building blocks, for executing the resulting automated workflows, and for recording the provenance of data products resulting from workflow runs. Despite the advantages such features provide, many automated workflows continue to be implemented and executed outside of scientific workflow systems due to the convenience and familiarity of scripting languages (such as Perl, Python, R, and MATLAB), and to the high productivity many scientists experience when using these languages. YesWorkflow is a set of software tools that aim to provide such users of scripting languages with many of the benefits of scientific workflow systems. YesWorkflow requires neither the use of a workflow engine nor the overhead of adapting code to run effectively in such a system. Instead, YesWorkflow enables scientists to annotate existing scripts with special comments that reveal the computational modules and dataflows otherwise implicit in these scripts. YesWorkflow tools extract and analyze these comments, represent the scripts in terms of entities based on the typical scientific workflow model, and provide graphical renderings of this workflow-like view of the scripts. Future versions of YesWorkflow also will allow the prospective provenance of the data products of these scripts to be queried in ways similar to those available to users of scientific workflow systems.", "num_citations": "109\n", "authors": ["1881"]}
{"title": "A practical theory of language-integrated query\n", "abstract": " Language-integrated query is receiving renewed attention, in part because of its support through Microsoft's LINQ framework. We present a practical theory of language-integrated query based on quotation and normalisation of quoted terms. Our technique supports join queries, abstraction over values and predicates, composition of queries, dynamic generation of queries, and queries with nested intermediate data. Higher-order features prove useful even for constructing first-order queries. We prove a theorem characterising when a host query is guaranteed to generate a single SQL query. We present experimental results confirming our technique works, even in situations where Microsoft's LINQ framework either fails to produce an SQL query or, in one case, produces an avalanche of SQL queries.", "num_citations": "109\n", "authors": ["1881"]}
{"title": "Prov-dm: The prov data model\n", "abstract": " Provenance is information about entities, activities, and people involved in producing a piece of data or thing, which can be used to form assessments about its quality, reliability or trustworthiness. PROV-DM is the conceptual data model that forms a basis for the W3C provenance (PROV) family of specifications. PROV-DM distinguishes core structures, forming the essence of provenance information, from extended structures catering for more specific uses of provenance. PROVDM is organized in six components, respectively dealing with:(1) entities and activities, and the time at which they were created, used, or ended;(2) derivations of entities from entities;(3) agents bearing responsibility for entities that were generated and activities that happened;(4) a notion of bundle, a mechanism to support provenance of provenance;(5) properties to link entities that refer to the same thing; and,(6) collections forming a logical structure for its members.", "num_citations": "96\n", "authors": ["1881"]}
{"title": "The rationale of PROV\n", "abstract": " The prov family of documents are the final output of the World Wide Web Consortium Provenance Working Group, chartered to specify a representation of provenance to facilitate its exchange over the Web. This article reflects upon the key requirements, guiding principles, and design decisions that influenced the prov family of documents. A broad range of requirements were found, relating to the key concepts necessary for describing provenance, such as resources, activities, agents and events, and to balancing prov\u2019s ease of use with the facility to check its validity. By this retrospective requirement analysis, the article aims to provide some insights into how prov turned out as it did and why. Benefits of this insight include better inter-operability, a roadmap for alternate investigations and improvements, and solid foundations for future standardization activities.", "num_citations": "93\n", "authors": ["1881"]}
{"title": "Requirements for provenance on the web\n", "abstract": " From where did this tweet originate? Was this quote from the New York Times modified? Daily, we rely on data from the Web, but often it is difficult or impossible to determine where it came from or how it was produced. This lack of provenance is particularly evident when people and systems deal with Web information or with any environment where information comes from sources of varying quality. Provenance is not captured pervasively in information systems. There are major technical, social, and economic impediments that stand in the way of using provenance effectively. This paper synthesizes requirements for provenance on the Web for a number of dimensions, focusing on three key aspects of provenance: the content of provenance, the management of provenance records, and the uses of provenance information. To illustrate these requirements, we use three synthesized scenarios that encompass provenance problems faced by Web users today.", "num_citations": "93\n", "authors": ["1881"]}
{"title": "Provenance as dependency analysis\n", "abstract": " Provenance is information recording the source, derivation, or history of some information. Provenance tracking has been studied in a variety of settings; however, although many design points have been explored, the mathematical or semantic foundations of data provenance have received comparatively little attention. In this paper, we argue that dependency analysis techniques familiar from program analysis and program slicing provide a formal foundation for forms of provenance that are intended to show how (part of) the output of a query depends on (parts of) its input. We introduce a semantic characterization of such dependency provenance, show that this form of provenance is not computable, and provide dynamic and static approximation techniques.", "num_citations": "90\n", "authors": ["1881"]}
{"title": "Nominal logic programming\n", "abstract": " Nominal logic is an extension of first-order logic which provides a simple foundation for formalizing and reasoning about abstract syntax modulo consistent renaming of bound names (that is, \u03b1-equivalence). This article investigates logic programming based on nominal logic. We describe some typical nominal logic programs, and develop the model-theoretic, proof-theoretic, and operational semantics of such programs. Besides being of interest for ensuring the correct behavior of implementations, these results provide a rigorous foundation for techniques for analysis and reasoning about nominal logic programs, as we illustrate via examples.", "num_citations": "86\n", "authors": ["1881"]}
{"title": "A formal framework for provenance security\n", "abstract": " Provenance, or information about the origin, derivation, or history of data, is becoming an important topic especially for shared scientific or public data on the Web. It clearly has implications on security (and vice versa) yet these implications are not well-understood. A great deal of work has focused on mechanisms for recording, managing or using some kind of provenance information, but relatively little progress has been made on foundational models that define provenance and relate it to security goals such as availability, confidentiality or privacy. We argue that such foundations are essential to making meaningful progress on these problems and should be developed. In this paper, we outline a formal model of provenance, propose formalizations of security properties for provenance such as disclosure and obfuscation, and explore their implications in domains based on automata, database queries and workflow\u00a0\u2026", "num_citations": "72\n", "authors": ["1881"]}
{"title": "Row-based effect types for database integration\n", "abstract": " We present CoreLinks, a call-by-value variant of System F with row polymorphism, row-based effect types, and implicit subkinding, which forms the basis for the Links web programming language. We focus on extensions to CoreLinks for database programming. The effect types support abstraction over database queries, while ensuring that queries are translated predictably to idiomatic and efficient SQL at run-time. Subkinding statically enforces the constraint that queries must return a list of records of base type. Polymorphism over the presence of record labels supports abstraction over database queries, inserts, deletes and updates.", "num_citations": "61\n", "authors": ["1881"]}
{"title": "A provenance model for manually curated data\n", "abstract": " Many curated databases are constructed by scientists integrating various existing data sources \u201cby hand\u201d, that is, by manually entering or copying data from other sources. Capturing provenance in such an environment is a challenging problem, requiring a good model of the process of curation. Existing models of provenance focus on queries/views in databases or computations on the Grid, not updates of databases or Web sites. In this paper we motivate and present a simple model of provenance for manually curated databases and discuss ongoing and future work.", "num_citations": "56\n", "authors": ["1881"]}
{"title": "A Graph Model of Data and Workflow Provenance.\n", "abstract": " Provenance has been studied extensively in both database and workflow management systems, so far with little convergence of definitions or models. Provenance in databases has generally been defined for relational or complex object data, by propagating fine-grained annotations or algebraic expressions from the input to the output. This kind of provenance has been found useful in other areas of computer science: annotation databases, probabilistic databases, schema and data integration, etc. In contrast, workflow provenance aims to capture a complete description of evaluation\u2013or enactment\u2013of a workflow, and this is crucial to verification in scientific computation. Workflows and their provenance are often presented using graphical notation, making them easy to visualize but complicating the formal semantics that relates their run-time behavior with their provenance records. We bridge this gap by extending a previously-developed dataflow language which supports both database-style querying and workflow-style batch processing steps to produce a workflow-style provenance graph that can be explicitly queried. We define and describe the model through examples, present queries that extract other forms of provenance, and give an executable definition of the graph semantics of dataflow expressions.", "num_citations": "54\n", "authors": ["1881"]}
{"title": "Functional programs that explain their work\n", "abstract": " We present techniques that enable higher-order functional computations to\" explain\" their work by answering questions about how parts of their output were calculated. As explanations, we consider the traditional notion of program slices, which we show can be inadequate, and propose a new notion: trace slices. We present techniques for specifying flexible and rich slicing criteria based on partial expressions, parts of which have been replaced by holes.", "num_citations": "52\n", "authors": ["1881"]}
{"title": "A sequent calculus for nominal logic\n", "abstract": " Nominal logic is a theory of names and binding based on the primitive concepts of freshness and swapping, with a self-dual N- (or \"new\")-quantifier, originally presented as a Hilbert-style axiom system extending first-order logic. We present a sequent calculus for nominal logic called fresh logic, or FL, admitting cut-elimination. We use FL to provide a proof-theoretic foundation for nominal logic programming and show how to interpret FO/spl lambda//spl nabla/, another logic with a self-dual quantifier, within FL.", "num_citations": "51\n", "authors": ["1881"]}
{"title": "Provenance XG final report\n", "abstract": " Provenance XG Final Report (2010) | www.narcis.nl KNAW KNAW Narcis Back to search results VU University Amsterdam Publication Provenance XG Final Report (2010) Pagina-navigatie: Main Save publication Save as MODS Export to Mendeley Save as EndNote Export to RefWorks Title Provenance XG Final Report Series W3C Incubator Group Reports Author Gil, Y; Cheney, J; Groth, PT; Hartig, O; Miles, S; Moreau, L; Pinheiro da Silva, P Publisher Artificial intelligence; Network Institute; Knowledge Representation and Reasoning Date issued 2010 Access Restricted Access Language English Type Report Publisher W3C Abstract http://www.w3.org/2005/Incubator/prov/XGR-prov/ Publication https://research.vu.nl/en/publications/03cc58ac-d3f5-4179-97... Persistent Identifiers NBN urn:nbn:nl:ui:31-03cc58ac-d3f5-4179-9744-51c1eec5c8f7 Handle 1871.1/03cc58ac-d3f5-4179-9744-51c1eec5c8f7 Metadata XML \u2026", "num_citations": "50\n", "authors": ["1881"]}
{"title": "The complexity of equivariant unification\n", "abstract": " Nominal logic is a first-order theory of names and binding based on a primitive operation of swapping rather than substitution. Urban, Pitts, and Gabbay have developed a nominal unification algorithm that unifies terms up to nominal equality. However, because of nominal logic\u2019s equivariance principle, atomic formulas can be provably equivalent without being provably equal as terms, so resolution using nominal unification is sound but incomplete. For complete resolution, a more general form of unification called equivariant unification, or \u201cunification up to a permutation\u201d is required. Similarly, for rewrite rules expressed in nominal logic, a more general form of matching called equivariant matching is necessary.               In this paper, we study the complexity of the decision problem for equivariant unification and matching. We show that these problems are NP-complete in general. However, when one of the\u00a0\u2026", "num_citations": "48\n", "authors": ["1881"]}
{"title": "Provenance as dependency analysis\n", "abstract": " Provenance is information recording the source, derivation or history of some information. Provenance tracking has been studied in a variety of settings, particularly database management systems. However, although many candidate definitions of provenance have been proposed, the mathematical or semantic foundations of data provenance have received comparatively little attention. In this paper, we argue that dependency analysis techniques familiar from program analysis and program slicing provide a formal foundation for forms of provenance that are intended to show how (part of) the output of a query depends on (parts of) its input. We introduce a semantic characterisation of such dependency provenance for a core database query language, show that minimal dependency provenance is not computable, and provide dynamic and static approximation techniques. We also discuss preliminary implementation\u00a0\u2026", "num_citations": "47\n", "authors": ["1881"]}
{"title": "FLUX: functional updates for XML\n", "abstract": " XML database query languages have been studied extensively, but XML database updates have received relatively little attention, and pose many challenges to language design. We are developing an XML update language called FLUX, which stands for FunctionaL Updates for XML, drawing upon ideas from functional programming languages. In prior work, we have introduced a core language for FLUX with a clear operational semantics and a sound, decidable static type system based on regular expression types.", "num_citations": "47\n", "authors": ["1881"]}
{"title": "Program slicing and data provenance.\n", "abstract": " Provenance is information that aids understanding and troubleshooting database queries by explaining the results in terms of the input. Slicing is a program analysis technique for debugging and understanding programs that has been studied since the early 1980s, in which program results are explained in terms of parts of the program that contributed to the results. This paper will briefly review ideas and techniques from program slicing and show how they might be useful for improving our understanding of provenance in databases.", "num_citations": "47\n", "authors": ["1881"]}
{"title": "A core calculus for provenance\n", "abstract": " Provenance is an increasing concern due to the ongoing revolution in sharing and processing scientific data on the Web and in other computer systems. It is proposed that many computer systems will need to become provenance-aware in order to provide satisfactory accountability, reproducibility, and trust for scientific or other high-value data. To date, there is not a consensus concerning appropriate formal models or security properties for provenance. In previous work, we introduced a formal framework for provenance security and proposed formal definitions of properties called disclosure and obfuscation.", "num_citations": "45\n", "authors": ["1881"]}
{"title": "Relating nominal and higher-order pattern unification\n", "abstract": " Higher-order pattern unification and nominal unification are two approaches to unifying modulo some form of \u03b1-equivalence (consistent renaming of bound names). The higher-order and nominal approaches seem superficially dissimilar. However, we show that a natural concretion (or name-application) operation for nominal terms can be used to simulate the behavior of higherorder patterns. We describe a form of nominal terms called nominal patterns that includes concretion and for which unification is equivalent to a special case of higher-order pattern unification, and then show that full higher-order pattern unification can be reduced to nominal unification via nominal patterns.", "num_citations": "45\n", "authors": ["1881"]}
{"title": "Mechanizing the metatheory of LF\n", "abstract": " LF is a dependent type theory in which many other formal systems can be conveniently embedded. However, correct use of LF relies on nontrivial metatheoretic developments such as proofs of correctness of decision procedures for LF's judgments. Although detailed informal proofs of these properties have been published, they have not been formally verified in a theorem prover. We have formalized these properties within Isabelle/HOL using the Nominal Datatype Package, closely following a recent article by Harper and Pfenning. In the process, we identified and resolved a gap in one of the proofs and a small number of minor lacunae in others. We also formally derive a version of the type checking algorithm from which Isabelle/HOL can generate executable code. Besides its intrinsic interest, our formalization provides a foundation for studying the adequacy of LF encodings, the correctness of Twelf-style\u00a0\u2026", "num_citations": "44\n", "authors": ["1881"]}
{"title": "Equivariant unification\n", "abstract": " Nominal logic is a variant of first-order logic with special facilities for reasoning about names and binding based on the underlying concepts of swapping and freshness. It serves as the basis of logic programming, term rewriting, and automated theorem proving techniques that support reasoning about languages with name-binding. These applications often require nominal unification, or equational reasoning and constraint solving in nominal logic. Urban, Pitts and Gabbay developed an algorithm for a broadly applicable class of nominal unification problems. However, because of nominal logic\u2019s equivariance property, these applications also require a different form of unification, which we call equivariant unification. In this article, we first study the complexity of the decision problem for equivariant unification and equivariant matching. We show that these problems are NP-hard in general, as is nominal\u00a0\u2026", "num_citations": "42\n", "authors": ["1881"]}
{"title": "Scrap your nameplate: (functional pearl)\n", "abstract": " Recent research has shown how boilerplate code, or repetitive code for traversing datatypes, can be eliminated using generic programming techniques already available within some implementations of Haskell. One particularly intractable kind of boilerplate is nameplate, or code having to do with names, name-binding, and fresh name generation. One reason for the difficulty is that operations on data structures involving names, as usually implemented, are not regular instances of standard map, fold, or zip operations. However, in nominal abstract syntax, an alternative treatment of names and binding based on swapping, operations such as \u03b1-equivalence, capture-avoiding substitution, and free variable set functions are much better-behaved.In this paper, we show how nominal abstract syntax techniques similar to those of FreshML can be provided as a Haskell library called FreshLib. In addition, we show how\u00a0\u2026", "num_citations": "41\n", "authors": ["1881"]}
{"title": "Query shredding: efficient relational evaluation of queries over nested multisets\n", "abstract": " Nested relational query languages have been explored extensively, and underlie industrial language-integrated query systems such as Microsoft's LINQ. However, relational databases do not natively support nested collections in query results. This can lead to major performance problems: if programmers write queries that yield nested results, then such systems typically either fail or generate a large number of queries. We present a new approach to query shredding, which converts a query returning nested data to a fixed number of SQL queries. Our approach, in contrast to prior work, handles multiset semantics, and generates an idiomatic SQL: 1999 query directly from a normal form for nested queries. We provide a detailed description of our translation and present experiments showing that it offers comparable or better performance than a recent alternative approach on a range of examples.", "num_citations": "40\n", "authors": ["1881"]}
{"title": "A simple sequent calculus for nominal logic\n", "abstract": " Nominal logic is a variant of first-order logic that provides support for reasoning about bound names in abstract syntax. A key feature of nominal logic is the new-quantifier, which quantifies over fresh names (names not appearing in any values considered so far). Previous attempts have been made to develop convenient rules for reasoning with the new-quantifier, but we argue that none of these attempts is completely satisfactory. In this article we develop a new sequent calculus for nominal logic in which the rules for the new-quantifier are much simpler than in previous attempts. We also prove several structural and metatheoretic properties, including cut-elimination, consistency and equivalence to Pitts' axiomatization of nominal logic.", "num_citations": "40\n", "authors": ["1881"]}
{"title": "PROV-N: The provenance notation\n", "abstract": " Provenance is information about entities, activities, and people involved in producing a piece of data or thing, which can be used to form assessments about its quality, reliability or trustworthiness. PROV-DM is the conceptual data model that forms a basis for the W3C provenance (PROV) family of specifications. PROV-DM distinguishes core structures, forming the essence of provenance information, from extended structures catering for more specific uses of provenance. PROV-DM is organized in six components, respectively dealing with:(1) entities and activities, and the time at which they were created, used, or ended;(2) derivations of entities from entities;(3) agents bearing responsibility for entities that were generated and activities that happened;(4) a notion of bundle, a mechanism to support provenance of provenance; and,(5) properties to link entities that refer to the same thing;(6) collections forming a logical structure for its members. To provide examples of the PROV data model, the PROV notation (PROV-N) is introduced: aimed at human consumption, PROV-N allows serializations of PROV instances to be created in a compact manner. PROV-N facilitates the mapping of the PROV data model to concrete syntax, and is used as the basis for a formal semantics of PROV. The purpose of this document is to define the PROV-N notation.", "num_citations": "39\n", "authors": ["1881"]}
{"title": "Causality and the semantics of provenance\n", "abstract": " Provenance, or information about the sources, derivation, custody or history of data, has been studied recently in a number of contexts, including databases, scientific workflows and the Semantic Web. Many provenance mechanisms have been developed, motivated by informal notions such as influence, dependence, explanation and causality. However, there has been little study of whether these mechanisms formally satisfy appropriate policies or even how to formalize relevant motivating concepts such as causality. We contend that mathematical models of these concepts are needed to justify and compare provenance techniques. In this paper we review a theory of causality based on structural models that has been developed in artificial intelligence, and describe work in progress on a causal semantics for provenance graphs.", "num_citations": "38\n", "authors": ["1881"]}
{"title": "Completeness and Herbrand theorems for nominal logic\n", "abstract": " Nominal logic is a variant of first-order logic in which abstract syntax with names and binding is formalized in terms of two basic operations: name-swapping andfreshness. It relies on two important principles: equivariance (validity is preserved by name-swapping), and fresh name generation (\u201cnew\u201d or fresh names can always be chosen). It is inspired by a particular class of models for abstract syntax trees involving names and binding, drawing on ideas from Fraenkel-Mostowski set theory: finite-support models in which each value can depend on only finitely many names.Although nominal logic is sound with respect to such models, it is not complete. In this paper we review nominal logic and show why finite-support models are insufficient both in theory and practice. We then identify (up to isomorphism) the class of models with respect to which nominal logic is complete: ideal-supported models in which the supports\u00a0\u2026", "num_citations": "37\n", "authors": ["1881"]}
{"title": "Recording Provenance for SQL Queries and Updates.\n", "abstract": " Knowing the origin of data (ie, where the data was copied or created from)\u2014its provenance\u2014is vital for assessing the trustworthiness of contemporary scientific databases such as UniProt [16] and SWISSPROT [14]. Unfortunately, provenance information must currently be recorded manually, by added effort of the database maintainer. Since such maintenance is tedious and error-prone, it is desirable to provide support for recording provenance in the database system itself. We review a recent proposal for incorporating such support, as well as its theoretical properties.", "num_citations": "33\n", "authors": ["1881"]}
{"title": "Towards a theory of information preservation\n", "abstract": " Digital preservation is a pressing challenge to the library community. In this paper, we describe the initial results of our efforts towards understanding digital (as well as traditional) preservation problems from first principles. Our approach is to use the language of mathematics to formalize the concepts that are relevant to preservation. Our theory of preservation spaces draws upon ideas from logic and programming language semantics to describe the relationship between concrete objects and their information contents. We also draw on game theory to show how objects change over time as a result of uncontrollable environment effects and directed preservation actions. In the second half of this paper, we show how to use the mathematics of universal algebra as a language for objects whose information content depends on many components. We use this language to describe both migration and emulation\u00a0\u2026", "num_citations": "33\n", "authors": ["1881"]}
{"title": "A dependent nominal type theory\n", "abstract": " Nominal abstract syntax is an approach to representing names and binding pioneered by Gabbay and Pitts. So far nominal techniques have mostly been studied using classical logic or model theory, not type theory. Nominal extensions to simple, dependent and ML-like polymorphic languages have been studied, but decidability and normalization results have only been established for simple nominal type theories. We present a LF-style dependent type theory extended with name-abstraction types, prove soundness and decidability of beta-eta-equivalence checking, discuss adequacy and canonical forms via an example, and discuss extensions such as dependently-typed recursion and induction principles.", "num_citations": "32\n", "authors": ["1881"]}
{"title": "Dynamic provenance for SPARQL updates\n", "abstract": " While the Semantic Web currently can exhibit provenance information by using the W3C PROV standards, there is a \u201cmissing link\u201d in connecting PROV to storing and querying for dynamic changes to RDF graphs using SPARQL. Solving this problem would be required for such clear use-cases as the creation of version control systems for RDF. While some provenance models and annotation techniques for storing and querying provenance data originally developed with databases or workflows in mind transfer readily to RDF and SPARQL, these techniques do not readily adapt to describing changes in dynamic RDF datasets over time. In this paper we explore how to adapt the dynamic copy-paste provenance model of Buneman et al.[2] to RDF datasets that change over time in response to SPARQL updates, how to represent the resulting provenance records themselves as RDF in a manner compatible with\u00a0\u2026", "num_citations": "30\n", "authors": ["1881"]}
{"title": "An analytical survey of provenance sanitization\n", "abstract": " Security is likely to be a critical factor in the future adoption of provenance technology, because of the risk of inadvertent disclosure of sensitive information. In this survey paper we review the state of the art in secure provenance, considering mechanisms for controlling access, and the extent to which these mechanisms preserve provenance integrity. We examine seven systems or approaches, comparing features and identifying areas for future work.", "num_citations": "29\n", "authors": ["1881"]}
{"title": "A simple nominal type theory\n", "abstract": " Nominal logic is an extension of first-order logic with features useful for reasoning about abstract syntax with bound names. For computational applications such as programming and formal reasoning, it is desirable to develop constructive type theories for nominal logic that extend standard type theories for propositional, first- or higher-order logic. This has proven difficult, largely because of complex interactions between nominal logic's name-abstraction operation and ordinary functional abstraction. This difficulty already arises in the case of propositional logic and simple type theory. In this paper we show how this difficulty can be overcome, and present a simple nominal type theory that enjoys properties such as type soundness and strong normalization, and that can be soundly interpreted using existing nominal set models of nominal logic. We also sketch how recursion combinators for languages with binding\u00a0\u2026", "num_citations": "28\n", "authors": ["1881"]}
{"title": "Toward a general theory of names: binding and scope\n", "abstract": " High-level formalisms for reasoning about names and binding such as de Bruijn indices, various flavors of higher-order abstract syntax, the Theory of Contexts, and nominal abstract syntax address only one relatively restrictive form of scoping: namely, unary lexical scoping, in which the scope of a (single) bound name is a subtree of the abstract syntax tree (possibly with other subtrees removed due to shadowing). Many languages exhibit binding or renaming structure that does not fit this mold. Examples include binding transitions in the \u03c0-calculus; unique identifiers in contexts, memory heaps, and XML documents; declaration scoping in modules and namespaces; anonymous identifiers in automata, type schemes, and Horn clauses; and pattern matching and mutual recursion constructs in functional languages. In these cases, it appears necessary to either rearrange the abstract syntax so that lexical scoping can\u00a0\u2026", "num_citations": "28\n", "authors": ["1881"]}
{"title": "Tradeoffs in XML database compression\n", "abstract": " Large XML data files, or XML databases, are now a common way to distribute scientific and bibliographic data, and storing such data efficiently is an important concern. A number of approaches to XML compression have been proposed in the last five years. The most competitive approaches employ one or more statistical text compressors based on PPM or arithmetic coding in which some of the context is provided by the XML document structure. The purpose of this paper is to investigate the relationship between the extant proposals in more detail. We review the two main statistical modeling approaches proposed so far, and evaluate their performance on two representative XML databases. Our main finding is that while a recently-proposed multiple-model approach can provide better overall compression for large databases, it uses much more memory and converges more slowly than an older single-model approach.", "num_citations": "27\n", "authors": ["1881"]}
{"title": "Towards a Repository of Bx Examples.\n", "abstract": " We argue for the creation of a curated repository of examples of bidirectional transformations (bx). In particular, such a resource may support research on bx, especially cross-fertilisation between the different communities involved. We have initiated a bx repository, which is introduced in this paper. We discuss our design decisions and their rationale, and illustrate them using the now classic Composers example. We discuss the difficulties that this undertaking may face, and comment on how they may be overcome.", "num_citations": "26\n", "authors": ["1881"]}
{"title": "An Empirical Evaluation of Simple DTD-Conscious Compression Techniques.\n", "abstract": " The term \u201cXML compression\u201d has been used to describe techniques addressing several different (though related) problems, all relevant to Web data management:1. minimum-length coding for efficient XML document storage and transmission [13, 5, 10, 1]; 2. compact binary formats for efficient (streaming) XML message processing and transmission [8, 9]; and3. storage techniques for efficient XML database query processing [11, 17, 3, 2].", "num_citations": "24\n", "authors": ["1881"]}
{"title": "Avoiding equivariance in alpha-prolog\n", "abstract": " \u03b1Prolog is a logic programming language which is well-suited for rapid prototyping of type systems and operational semantics of typed \u03bb-calculi and many other languages involving bound names. In \u03b1Prolog, the nominal unification algorithm of Urban, Pitts and Gabbay is used instead of first-order unification. However, although \u03b1Prolog can be viewed as Horn-clause logic programming in Pitts\u2019 nominal logic, proof search using nominal unification is incomplete in nominal logic. Because of nominal logic\u2019s equivariance principle, complete proof search would require solving NP-hard equivariant unification problems. Nevertheless, the \u03b1Prolog programs we studied run correctly without equivariant unification. In this paper, we give several examples of \u03b1Prolog programs that do not require equivariant unification, develop a test for identifying such programs, and prove the correctness of this test via a\u00a0\u2026", "num_citations": "24\n", "authors": ["1881"]}
{"title": "Provenance traces\n", "abstract": " Provenance is information about the origin, derivation, ownership, or history of an object. It has recently been studied extensively in scientific databases and other settings due to its importance in helping scientists judge data validity, quality and integrity. However, most models of provenance have been stated as ad hoc definitions motivated by informal concepts such as \"comes from\", \"influences\", \"produces\", or \"depends on\". These models lack clear formalizations describing in what sense the definitions capture these intuitive concepts. This makes it difficult to compare approaches, evaluate their effectiveness, or argue about their validity. We introduce provenance traces, a general form of provenance for the nested relational calculus (NRC), a core database query language. Provenance traces can be thought of as concrete data structures representing the operational semantics derivation of a computation; they are related to the traces that have been used in self-adjusting computation, but differ in important respects. We define a tracing operational semantics for NRC queries that produces both an ordinary result and a trace of the execution. We show that three pre-existing forms of provenance for the NRC can be extracted from provenance traces. Moreover, traces satisfy two semantic guarantees: consistency, meaning that the traces describe what actually happened during execution, and fidelity, meaning that the traces \"explain\" how the expression would behave if the input were changed. These guarantees are much stronger than those contemplated for previous approaches to provenance; thus, provenance traces provide a general semantic\u00a0\u2026", "num_citations": "23\n", "authors": ["1881"]}
{"title": "ACCOn: checking consistency of XML write-access control policies\n", "abstract": " XML access control policies involving updates may contain security flaws, here called inconsistencies, in which a forbidden operation may be simulated by performing a sequence of allowed operations. ACCOn implements i) consistency checking algorithms that examine whether a write-access control policy defined over a DTD is inconsistent and ii) repair algorithms that propose repairs to an inconsistent policy to obtain a consistent one.", "num_citations": "22\n", "authors": ["1881"]}
{"title": "Mechanized metatheory model-checking\n", "abstract": " The problem of mechanically formalizing and proving metatheoretic properties of programming language calculi, type systems, operational semantics, and related formal systems has received considerable attention recently. However, the dual problem of searching for errors in such formalizations has received comparatively little attention. In this paper, we consider the problem of bounded model-checking for metatheoretic properties of formal systems specified using nominal logic. In contrast to the current state of the art for metatheory verification, our approach is fully automatic, does not require expertise in theorem proving on the part of the user, and produces counterexamples in the case that a flaw is detected. We present two implementations of this technique, one based on negation-as-failure and one based on negation elimination, along with experimental results showing that these techniques are fast enough\u00a0\u2026", "num_citations": "22\n", "authors": ["1881"]}
{"title": "System description: Alpha-Prolog, a fresh approach to logic programming modulo alpha-equivalence\n", "abstract": " \u03b1Prolog is a prototype logic programming language with a built-in notion of binders and unification modulo \u03b1-equivalence. It is based on a mild extension of first-order Horn formulae: instead of the usual first-order terms and first-order unification, \u03b1Prolog uses nominal terms and nominal unification introduced in [3]. In this paper, we give three examples that demonstrate the advantages of \u03b1Prolog and describe our current implementation.", "num_citations": "22\n", "authors": ["1881"]}
{"title": "Introduction to bidirectional transformations\n", "abstract": " Bidirectional transformations (BX) serve to maintain consistency between different representations of related and often overlapping information, translating changes in one representation to the others. We present a brief introduction to the field, in order to provide some common background to the remainder of this volume, which constitutes the lecture notes from the Summer School on Bidirectional Transformations, held in Oxford in July 2016 as one of the closing activities of the UK EPSRC-funded project A Theory of Least Change for Bidirectional Transformations.", "num_citations": "21\n", "authors": ["1881"]}
{"title": "Equivariant unification\n", "abstract": " Nominal logic is a variant of first-order logic with special facilities for reasoning about names and binding based on the underlying concepts of swapping and freshness. It serves as the basis of logic programming and term rewriting techniques that provide similar advantages to, but remain simpler than, higher-order logic programming or term rewriting systems. Previous work on nominal rewriting and logic programming has relied on nominal unification, that is, unification up to equality in nominal logic. However, because of nominal logic\u2019s equivariance property, these applications require a stronger form of unification, which we call equivariant unification. Unfortunately, equivariant unification and matching are NP-hard decision problems. This paper presents an algorithm for equivariant unification that produces a complete set of finitely many solutions, as well as NP decision procedure and a version that\u00a0\u2026", "num_citations": "21\n", "authors": ["1881"]}
{"title": "Towards a Principle of Least Surprise for Bidirectional Transformations.\n", "abstract": " In software engineering and elsewhere, it is common for different people to work intensively with different, but related, artefacts, eg models, documents, or code. They may use bidirectional transformations (bx) to maintain consistency between them. Naturally, they do not want their deliberate decisions disrupted, or their comprehension of their artefact interfered with, by a bx that makes changes to their artefact beyond the strictly necessary. This gives rise to a desire for a principle of Least Change, which has been often alluded to in the field, but seldom addressed head on. In this paper we present examples, briefly survey what has been said about least change in the context of bx, and identify relevant notions from elsewhere that may be applicable. We identify that what is actually needed is a Principle of Least Surprise, to limit a bx to reasonable behaviour. We present candidate formalisations of this, but none is obviously right for all circumstances. We point out areas where further work might be fruitful, and invite discussion.", "num_citations": "20\n", "authors": ["1881"]}
{"title": "Language-integrated provenance\n", "abstract": " Provenance, or information about the origin or derivation of data, is important for assessing the trustworthiness of data and identifying and correcting mistakes. Most prior implementations of data provenance have involved heavyweight modifications to database systems and little attention has been paid to how the provenance data can be used outside such a system. We present extensions to the Links programming language that build on its support for language-integrated query to support provenance queries by rewriting and normalizing monadic comprehensions and extending the type system to distinguish provenance metadata from normal data. The main contribution of this article is to show that the two most common forms of provenance can be implemented efficiently and used safely as a programming language feature with no changes to the database system.", "num_citations": "19\n", "authors": ["1881"]}
{"title": "A linearly typed assembly language\n", "abstract": " Today's type-safe low-level languages rely on garbage collection to  recycle heap-allocated objects safely.  We present LTAL, a safe, low-level, yet simple language that ``stands on its own'': it guarantees safe execution within a fixed memory space, without relying on external run-time support.  We demonstrate the expressiveness of LTAL by giving a type-preserving compiler for the functional core of ML.  But this independence comes  at a steep price: LTAL's type system imposes a draconian discipline of linearity that ensures that memory can be reused safely, but prohibits any useful kind of sharing.  We present the results of experiments with a prototype LTAL system that show just how high the price of linearity can be.", "num_citations": "19\n", "authors": ["1881"]}
{"title": "Principles of Provenance (Dagstuhl Seminar 12091)\n", "abstract": " This report documents the program and the outcomes of Dagstuhl Seminar 12091``Principles of Provenance''. The term``provenance''refers to information about the origin, context, derivation, ownership or history of some artifact. In both art and science, provenance information is crucial for establishing the value of a real-world artifact, guaranteeing for example that the artifact is an original work produced by an important artist, or that a stated scientific conclusion is reproducible. Since it is much easier to copy or alter digital information than it is to copy or alter real-world artifacts, the need for tracking and management of provenance information to testify the value and correctness of digital information has been firmly established in the last few years. As a result, provenance tracking and management has been studied in many settings, ranging from databases, scientific workflows, business process modeling, and security to social networking and the Semantic Web, but with relatively few interaction between these areas. This Dagstuhl seminar has focused on bringing together researchers from the above and other areas to identify the commonalities and differences of dealing with provenance; improve the mutual understanding of these communities; and identify main areas for further foundational provenance research.", "num_citations": "18\n", "authors": ["1881"]}
{"title": "\u03b1Check: A mechanized metatheory model-checker\n", "abstract": " The problem of mechanically formalizing and proving metatheoretic properties of programming language calculi, type systems, operational semantics, and related formal systems has received considerable attention recently. However, the dual problem of searching for errors in such formalizations has attracted comparatively little attention. In this article, we present \u03b1Check, a bounded model checker for metatheoretic properties of formal systems specified using nominal logic. In contrast to the current state of the art for metatheory verification, our approach is fully automatic, does not require expertise in theorem proving on the part of the user, and produces counterexamples in the case that a flaw is detected. We present two implementations of this technique, one based on negation-as-failure and one based on negation elimination, along with experimental results showing that these techniques are fast enough to be\u00a0\u2026", "num_citations": "17\n", "authors": ["1881"]}
{"title": "Reflections on monadic lenses\n", "abstract": " Bidirectional transformations (bx) have primarily been modeled as pure functions, and do not account for the possibility of the side-effects that are available in most programming languages. Recently several formulations of bx that use monads to account for effects have been proposed, both among practitioners and in academic research. The combination of bx with effects turns out to be surprisingly subtle, leading to problems with some of these proposals and increasing the complexity of others. This paper reviews the proposals for monadic lenses to date, and offers some improved definitions, paying particular attention to the obstacles to naively adding monadic effects to existing definitions of pure bx such as lenses and symmetric lenses, and the subtleties of equivalence of symmetric bidirectional transformations in the presence of effects.", "num_citations": "17\n", "authors": ["1881"]}
{"title": "Database queries that explain their work\n", "abstract": " Provenance for database queries or scientific workflows is often motivated as providing explanation, increasing understanding of the underlying data sources and processes used to compute the query, and reproducibility, the capability to recompute the results on different inputs, possibly specialized to a part of the output. Many provenance systems claim to provide such capabilities; however, most lack formal definitions or guarantees of these properties, while others provide formal guarantees only for relatively limited classes of changes. Building on recent work on provenance traces and slicing for functional programming languages, we introduce a detailed tracing model of provenance for multiset-valued Nested Relational Calculus, define trace slicing algorithms that extract subtraces needed to explain or recompute specific parts of the output, and define query slicing and differencing techniques that support\u00a0\u2026", "num_citations": "17\n", "authors": ["1881"]}
{"title": "Consistency and repair for XML write-access control policies\n", "abstract": " XML access control policies involving updates may contain security flaws, here called inconsistencies, in which a forbidden operation may be simulated by performing a sequence of allowed operations. This article investigates the problem of deciding whether a policy is consistent, and if not, how its inconsistencies can be repaired. We consider total and partial policies expressed in terms of annotated schemas defining which operations are allowed or denied for the XML trees that are instances of the schema. We show that consistency is decidable in PTIME for such policies and that consistent partial policies can be extended to unique least-privilege consistent total policies. We also consider repair problems based on deleting privileges to restore consistency, show that finding minimal repairs is NP-complete, and give heuristics for finding repairs. Finally, we experimentally evaluate these algorithms in\u00a0\u2026", "num_citations": "17\n", "authors": ["1881"]}
{"title": "On principles of Least Change and Least Surprise for bidirectional transformations\n", "abstract": " In software engineering and elsewhere, different people may work intensively with different, but related, artefacts, eg models, documents, or code. They may use bidirectional transformations (bx) to maintain consistency between them. Naturally, they do not want their deliberate decisions disrupted, or their comprehension of their artefact interfered with, by a bx that makes changes to their artefact beyond the strictly necessary. This gives rise to a desire for a principle of Least Change, which has been often alluded to in the field, but seldom addressed head on. In this paper we present examples, briefly survey what has been said about least change in the context of bx, and identify relevant notions from elsewhere that may be applicable. We conclude that we cannot expect a Principle of Least Change to determine the optimal behaviour of a bx based on the consistency relation it embodies alone. Any such principle would bind the hands of the bx developer too tightly: the specification of how consistency is restored is as important a part of the development of a bx as the specification of what consistency means. Rather, what is required is a notion of reasonable behaviour of a bx that captures the idea that the bx\u2019s consistency restoration does not gratuitously surprise its user. We suggest considering continuity variants, particularly Ho\u0308lder continuity. Such properties are too strong to expect them to hold universally, so we introduce the idea of a property holding piecewise on an atlas of subspace pairs.", "num_citations": "16\n", "authors": ["1881"]}
{"title": "The Database Wiki Project: A General-Purpose Platform for Data Curation and Collaboration\n", "abstract": " Databases and wikis have complementary strengths and weaknesses for use in collaborative data management and data curation. Relational databases, for example, offer advantages such as scalability, query optimization and concurrency control, but are not easy to use and lack other features needed for collaboration. Wikis have proved enormously successful as a means to collaborate because they are easy to use, encourage sharing, and provide built-in support for archiving, history-tracking and annotation. However, wikis lack support for structured data, efficiently querying data at scale, and localized provenance and annotation. To achieve the best of both worlds, we are developing a general-purpose platform for collaborative data management, called DBWIKI. Our system not only facilitates the collaborative creation of structured data; it also provides features not usually provided by database technology such\u00a0\u2026", "num_citations": "16\n", "authors": ["1881"]}
{"title": "DBWiki: a structured wiki for curated data and collaborative data management\n", "abstract": " Wikis have proved enormously successful as a means to collaborate in the creation and publication of textual information. At the same time, a large number of curated databases have been developed through collaboration for the dissemination of structured data in specific domains, particularly bioinformatics. We demonstrate a general-purpose platform for collaborative data management, DBWiki, designed to achieve the best of both worlds. Our system not only facilitates the collaborative creation of a database; it also provides features not usually provided by database technology such as versioning, provenance tracking, citability, and annotation. In our demonstration we will show how DBWiki makes it easy to create, correct, discuss and query structured data, placing more power in the hands of users while managing tedious details of data curation automatically.", "num_citations": "16\n", "authors": ["1881"]}
{"title": "Incremental relational lenses\n", "abstract": " Lenses are a popular approach to bidirectional transformations, a generalisation of the view update problem in databases, in which we wish to make changes to source tables to effect a desired change on a view. However, perhaps surprisingly, lenses have seldom actually been used to implement updatable views in databases. Bohannon, Pierce and Vaughan proposed an approach to updatable views called relational lenses, but to the best of our knowledge this proposal has not been implemented or evaluated to date. We propose incremental relational lenses, that equip relational lenses with change-propagating semantics that map small changes to the view to (potentially) small changes to the source tables. We also present a language-integrated implementation of relational lenses and a detailed experimental evaluation, showing orders of magnitude improvement over the non-incremental approach. Our work\u00a0\u2026", "num_citations": "15\n", "authors": ["1881"]}
{"title": "Imperative functional programs that explain their work\n", "abstract": " Program slicing provides explanations that illustrate how program outputs were produced from inputs. We build on an approach introduced in prior work, where dynamic slicing was defined for pure higher-order functional programs as a Galois connection between lattices of partial inputs and partial outputs. We extend this approach to imperative functional programs that combine higher-order programming with references and exceptions. We present proofs of correctness and optimality of our approach and a proof-of-concept implementation and experimental evaluation.", "num_citations": "15\n", "authors": ["1881"]}
{"title": "Lenses for web data\n", "abstract": " Putting data on the web typically involves implementing two transformations: one to convert the data into HTML, and another to parse modifications out of interactions with clients. Unfortunately, in current systems, these transformations are usually implemented using two separate functions\u2014an approach that replicates functionality across multiple pieces of code, and makes programs difficult to write, reason about, and maintain. This paper presents a different approach: an abstraction based on formlets that makes it easy to bridge the gap between data stored on a server and values embedded in HTML forms. We introduce formlenses, which combine the advantages of formlets with those of lenses to provide compositional, bidirectional form-based views of Web data. We show that formlenses can be viewed as monoidal functors over lenses, analogously to formlets, which are applicative functors. Finally, we investigate the connection between linearity and bidirectional transformations and describe a translation from a linear pattern syntax into formlens combinators.", "num_citations": "15\n", "authors": ["1881"]}
{"title": "Notions of bidirectional computation and entangled state monads\n", "abstract": " Bidirectional transformations (bx) support principled consistency maintenance between data sources. Each data source corresponds to one perspective on a composite system, manifested by operations to \u2018get\u2019 and \u2018set\u2019 a view of the whole from that particular perspective. Bx are important in a wide range of settings, including databases, interactive applications, and model-driven development. We show that bx are naturally modelled in terms of mutable state; in particular, the \u2018set\u2019 operations are stateful functions. This leads naturally to considering bx that exploit other computational effects too, such as I/O, nondeterminism, and failure, all largely ignored in the bx literature to date. We present a semantic foundation for symmetric bidirectional transformations with effects. We build on the mature theory of monadic encapsulation of effects in functional programming, develop the equational theory and important\u00a0\u2026", "num_citations": "14\n", "authors": ["1881"]}
{"title": "Dynamic Provenance for {SPARQL} Updates Using Named Graphs\n", "abstract": " The (Semantic) Web currently does not have an official or de facto standard way exhibit provenance information. While some provenance models and annotation techniques originally developed with databases or workflows in mind transfer readily to RDF, RDFS and SPARQL, these techniques do not readily adapt to describing changes in dynamic RDF datasets over time. Named graphs have been introduced to RDF motivated as a way of grouping triples in order to facilitate annotation, provenance and other descriptive metadata. Although their semantics is not yet officially part of RDF, there appears to be a consensus based on their syntax and semantics in SPARQL queries. Meanwhile, updates are being introduced as part of the next version of SPARQL. In this paper we explore how to adapt the dynamic copy-paste provenance model of Buneman et al.[2] to RDF datasets that change over time in response to SPARQL updates, how to represent the resulting provenance records themselves as RDF using named graphs, and how the provenance information can be provided as a SPARQL end-point.", "num_citations": "14\n", "authors": ["1881"]}
{"title": "Provenance, XML and the scientific web\n", "abstract": " Science is now being revolutionized by the capabilities of distributing computation and human effort over the World Wide Web. This revolution offers dramatic benefits but also poses serious risks due to the fluid nature of digital information. The Web today does not provide adequate repeatability, reliability, accountability and trust guarantees for scientific applications. One important part of this problem is tracking and managing provenance information, or metadata about sources, authorship, derivation, or other historical aspects of data. In this paper, we discuss motivating examples for provenance in science and computer security, introduce four models of provenance for XML queries, and discuss their properties and prospects for further research.", "num_citations": "14\n", "authors": ["1881"]}
{"title": "Semantics of the PROV data model\n", "abstract": " Provenance is information about entities, activities, and people involved in producing a piece of data or thing, which can be used to form assessments about its quality, reliability or trustworthiness. PROV-DM is the conceptual data model that forms a basis for the W3C provenance (PROV) family of specifications.This document presents a model-theoretic semantics for the PROV data model, viewing PROV-DM statements as atomic formulas in the sense of first-order logic, and viewing the constraints and inferences specified in PROV-CONSTRAINTS as a first-order theory. It is shown that valid PROV instances (in the sense of PROV-CONSTRAINTS) correspond to satisfiable theories. This information may be useful to researchers or users of PROV to understand the intended meaning and use of PROV for modeling information about the actual history, derivation or evolution of Web resources. It may also be useful for development of additional constraints or inferences for reasoning about PROV or integration of PROV with other Semantic Web vocabularies. It is not proposed as a canonical or required semantics of PROV and does not place any constraints on the use of PROV.", "num_citations": "13\n", "authors": ["1881"]}
{"title": "Report on the principles of provenance workshop\n", "abstract": " Provenance, or records of the origin, context, custody, derivation or other historical information about a (digital) object, has recently become an important research topic in a number of areas, particularly databases. However, there has been little interaction between researchers across subdisciplines of computer science working on related problems. This article reports on a workshop on Principles of Provenance held in Edinburgh, Scotland in November 2007, which facilitated interaction among researchers working on provenance in databases, security, information retrieval, Semantic Web, and software engineering settings, as well as developers and database administrators who are currently working with provenance in practice, or foresee the need to do so in the near future.", "num_citations": "13\n", "authors": ["1881"]}
{"title": "Logic column 14: Nominal logic and abstract syntax\n", "abstract": " Formalizing syntactic proofs of properties of logics, programming languages, security protocols, and other formal systems is a significant challenge, in large part because of the obligation to handle name-binding correctly. We present an approach called nominal abstract syntax that has attracted considerable interest since its introduction approximately six years ago. After an overview of other approaches, we describe nominal abstract syntax and nominal logic, a logic for reasoning about nominal abstract syntax. We also discuss applications of nominal techniques to programming, automated reasoning, and identify some future directions.", "num_citations": "13\n", "authors": ["1881"]}
{"title": "Statistical Models for Term Compression.\n", "abstract": " Symbolic tree data structures, or terms, are used in many computing systems. Although terms can be compressed by hand, using specialized algorithms, or using universal compression utilities, all of these approaches have drawbacks. We propose an approach which avoids these problems by using knowledge of term structure to obtain more accurate predictive models for term compression. We describe two models that predict child symbols based on their parents and locations. Our experiments compared these models with first-order Markov sequence models using Huffman coding and found that one model can obtain 20% better compression in similar time, and the other, simpler model can obtain similar compression 40% faster. These compression models also approach, but do not exceed, the performance of gzip.", "num_citations": "12\n", "authors": ["1881"]}
{"title": "\u00b5Puppet: A Declarative Subset of the Puppet Configuration Language\n", "abstract": " Puppet is a popular declarative framework for specifying and managing complex system configurations. The Puppet framework includes a domain-specific language with several advanced features inspired by object-oriented programming, including user-defined resource types, 'classes' with a form of inheritance, and dependency management. Like most real-world languages, the language has evolved in an ad hoc fashion, resulting in a design with numerous features, some of which are complex, hard to understand, and difficult to use correctly. We present an operational semantics for Puppet, a representative subset of the Puppet language that covers the distinctive features of Puppet, while excluding features that are either deprecated or work-in-progress. Formalising the semantics sheds light on difficult parts of the language, identifies opportunities for future improvements, and provides a foundation for future analysis or debugging techniques, such as static typechecking or provenance tracking. Our semantics leads straightforwardly to a reference implementation in Haskell. We also discuss some of Puppet's idiosyncrasies, particularly its handling of classes and scope, and present an initial corpus of test cases supported by our formal semantics.", "num_citations": "11\n", "authors": ["1881"]}
{"title": "Causally consistent dynamic slicing\n", "abstract": " We offer a lattice-theoretic account of dynamic slicing for {\\pi}-calculus, building on prior work in the sequential setting. For any run of a concurrent program, we exhibit a Galois connection relating forward slices of the start configuration to backward slices of the end configuration. We prove that, up to lattice isomorphism, the same Galois connection arises for any causally equivalent execution, allowing an efficient concurrent implementation of slicing via a standard interleaving semantics. Our approach has been formalised in the dependently-typed language Agda.", "num_citations": "11\n", "authors": ["1881"]}
{"title": "Provenance Segmentation.\n", "abstract": " Using pervasive provenance to secure mainstream systems has recently attracted interest from industry and government. Recording, storing and managing all of the provenance associated with a system is a considerable challenge. Analyzing the resulting noisy, heterogeneous, continuously-growing provenance graph adds to this challenge, and apparently necessitates segmentation, that is, approximating, compressing or summarizing part or all of the graph in order to identify patterns or features. In this paper, we describe this new problem space for provenance data management, contrast it with related problem spaces addressed by prior work on provenance abstraction and sanitization, and highlight challenges and future directions toward solutions to the provenance segmentation problem.", "num_citations": "11\n", "authors": ["1881"]}
{"title": "Repairing inconsistent XML write-access control policies\n", "abstract": " XML access control policies involving updates may contain security flaws, here called inconsistencies, in which a forbidden operation may be simulated by performing a sequence of allowed operations. This paper investigates the problem of deciding whether a policy is consistent, and if not, how its inconsistencies can be repaired. We consider policies expressed in terms of annotated DTDs defining which operations are allowed or denied for the XML trees that are instances of the DTD. We show that consistency is decidable in ptime for such policies and that consistent partial policies can be extended to unique \u201cleast-privilege\u201d consistent total policies. We also consider repair problems based on deleting privileges to restore consistency, show that finding minimal repairs is np-complete, and give heuristics for finding repairs.", "num_citations": "11\n", "authors": ["1881"]}
{"title": "Lux: A Lightweight, Statically Typed XML Update Language.\n", "abstract": " Several proposals for updating XML have been introduced. Many of them have a rather complicated semantics due to the interaction of side-effects and updates, and some proposals also complicate the semantics of XQuery because arbitrary side-effecting update statements are allowed inside queries. Moreover, static typechecking has not been studied for any proposed XML update language. In this paper, we survey prior work on XML update languages and motivate an alternative approach to updating XML. We introduce an update language called Lux, which stands for Lightweight Updates for XML. Lux can perform relational database-style updates, has a simple, deterministic operational semantics, and has a sound static type system based on regular expression types and structural subtyping.", "num_citations": "11\n", "authors": ["1881"]}
{"title": "Proof-relevant \u03c0-calculus: a constructive account of concurrency and causality\n", "abstract": " We present a formalisation in Agda of the theory of concurrent transitions, residuation and causal equivalence of traces for the \u03c0-calculus. Our formalisation employs de Bruijn indices and dependently typed syntax, and aligns the \u2018proved transitions\u2019 proposed by Boudol and Castellani in the context of CCS with the proof terms naturally present in Agda's representation of the labelled transition relation. Our main contributions are proofs of the \u2018diamond lemma\u2019 for the residuals of concurrent transitions and a formal definition of equivalence of traces up to permutation of transitions.In the \u03c0-calculus, transitions represent propagating binders whenever their actions involve bound names. To accommodate these cases, we require a more general diamond lemma where the target states of equivalent traces are no longer identical, but are related by a braiding that rewires the bound and free names to reflect the particular\u00a0\u2026", "num_citations": "10\n", "authors": ["1881"]}
{"title": "Effective quotation: relating approaches to language-integrated query\n", "abstract": " Language-integrated query techniques have been explored in a number of different language designs. We consider two different, type-safe approaches employed by Links and F#. Both approaches provide rich dynamic query generation capabilities, and thus amount to a form of heterogeneous staged computation, but to date there has been no formal investigation of their relative expressiveness. We present two core calculi Eff and Quot, respectively capturing the essential aspects of language-integrated querying using effects in Links and quotation in LINQ. We show via translations from Eff to Quot and back that the two approaches are equivalent in expressiveness. Based on the translation from Eff to Quot, we extend a simple Links compiler to handle queries.", "num_citations": "10\n", "authors": ["1881"]}
{"title": "Toward Provenance-Based Security for Configuration Languages.\n", "abstract": " Large system installations are increasingly configured using high-level, mostly-declarative languages. Often, different users contribute data that is compiled centrally and distributed to individual systems. Although the systems themselves have been developed with reliability and availability in mind, the configuration compilation process can lead to unforeseen vulnerabilities because of the lack of access control on the different components combined to build the final configuration. Even if simple change-based access controls are applied to validate changes to the final version, changes can be lost or incorrectly attributed. Based on the growing literature on provenance for database queries and other models of computation, we identify a potential application area for provenance to securing configuration languages.", "num_citations": "10\n", "authors": ["1881"]}
{"title": "Hierarchical Models of Provenance.\n", "abstract": " There is general agreement that we need to understand provenance at various levels of granularity; however, there appears, as yet, to be no general agreement on what granularity means. It can refer both to the detail with which we can view a process or the detail with which we view the data. We describe a simple and straightforward method for imposing a hierarchical structure on a provenance graph and show how it can, if we want, be derived from the program whose execution created that graph.", "num_citations": "10\n", "authors": ["1881"]}
{"title": "Expressiveness Benchmarking for System-Level Provenance\n", "abstract": " Provenance is increasingly being used as a foundation for security analysis and forensics. System-level provenance can help us trace activities at the level of libraries or system calls, which offers great potential for detecting subtle malicious activities that can otherwise go undetected. However, analysing the raw provenance trace is challenging, due to scale and to differences in data representation among system-level provenance recorders: for example, common queries to identify malicious patterns need to be formulated in different ways on different systems. As a first step toward understanding the similarities and differences among approaches, this paper proposes an expressiveness benchmark consisting of tests intended to capture the provenance of individual system calls. We present work in progress on the benchmark examples for Linux and discuss how they are handled by two different provenance collection tools, SPADE and OPUS.", "num_citations": "9\n", "authors": ["1881"]}
{"title": "Provenance for seismological processing pipelines in a distributed streaming workflow\n", "abstract": " Harvesting provenance for streaming workflows presents challenges related to the high rate of the updates and a large distribution of the execution, which can be spread across several institutional infrastructures. Moreover, the typically large volume of data produced by each transformation step can not be always stored and preserved efficiently. This can represent an obstacle for the evaluation of the results, for instance, in real-time, suggesting the importance of customisable metadata extraction procedures. In this paper we present our approach to the aforementioned provenance challenges within a use-case driven scenario in the field of seismology, which requires the execution of processing pipelines over a large datastream. In particular, we will discuss the current implementation and the upcoming challenges for an in-worfklow programmatic approach to provenance tracing, building on composite functions\u00a0\u2026", "num_citations": "9\n", "authors": ["1881"]}
{"title": "Regular expression subtyping for XML query and update languages\n", "abstract": " XML database query languages such as XQuery employ regular expression types with structural subtyping. Subtyping systems typically have two presentations, which should be equivalent: a declarative version in which the subsumption rule may be used anywhere, and an algorithmic version in which the use of subsumption is limited in order to make typechecking syntax-directed and decidable. However, the XQuery standard type system circumvents this issue by using imprecise typing rules for iteration constructs and defining only algorithmic typechecking, and another extant proposal provides more precise types for iteration constructs but ignores subtyping. In this paper, we consider a core XQuery-like language with a subsumption rule and prove the completeness of algorithmic typechecking; this is straightforward for XQuery proper but requires some care in the presence of more precise iteration typing\u00a0\u2026", "num_citations": "9\n", "authors": ["1881"]}
{"title": "Formal type soundness for Cyclone's region system\n", "abstract": " Cyclone is a polymorphic, type-safe programming language derived   from C\\@.  The primary design goals of Cyclone are to let   programmers control data representations and memory management   without sacrificing type-safety.  In this paper, we focus on the   region-based memory management of Cyclone and its static typing   discipline.  The design incorporates several advancements, including   support for region subtyping and a coherent integration with stack  allocation and a garbage collector.  To support separate   compilation, Cyclone requires programmers to write some explicit   region annotations, but uses a combination of default annotations,   local type inference, and a novel treatment of region effects to   reduce this burden.  As a result, we integrate C idioms in a   region-based framework.  In our experience, porting legacy C to Cyclone has required altering about 8\\% of the code; of the   changes, only 6\\% (of the 8\\%) were region annotations.      This technical report is really two documents in one: The first part   is a paper submitted for publication in November,   2001.  The second part is the full formal language and type-safety  proof mentioned briefly in the first part.  If you have already read   a version of, ``Region-Based Memory Management in Cyclone'', then   you should proceed directly to Section 9.", "num_citations": "9\n", "authors": ["1881"]}