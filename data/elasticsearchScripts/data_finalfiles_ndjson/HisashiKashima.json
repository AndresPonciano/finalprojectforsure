{"title": "Marginalized kernels between labeled graphs\n", "abstract": " A new kernel function between two labeled graphs is presented. Feature vectors are defined as the counts of label paths produced by random walks on graphs. The kernel computation finally boils down to obtaining the stationary state of a discrete-time linear system, thus is efficiently performed by solving simultaneous linear equations. Our kernel is based on an infinite dimensional feature space, so it is fundamentally different from other string or tree kernels based on dynamic programming. We will present promising empirical results in classification of chemical compounds. 1", "num_citations": "1028\n", "authors": ["2096"]}
{"title": "Direct importance estimation with model selection and its application to covariate shift adaptation.\n", "abstract": " When training and test samples follow different input distributions (ie, the situation called covariate shift), the maximum likelihood estimator is known to lose its consistency. For regaining consistency, the log-likelihood terms need to be weighted according to the importance (ie, the ratio of test and training input densities). Thus, accurately estimating the importance is one of the key tasks in covariate shift adaptation. A naive approach is to first estimate training and test input densities and then estimate the importance by the ratio of the density estimates. However, since density estimation is a hard problem, this approach tends to perform poorly especially in high dimensional cases. In this paper, we propose a direct importance estimation method that does not require the input density estimates. Our method is equipped with a natural model selection procedure so tuning parameters such as the kernel width can be objectively optimized. This is an advantage over a recently developed method of direct importance estimation. Simulations illustrate the usefulness of our approach.", "num_citations": "795\n", "authors": ["2096"]}
{"title": "Direct importance estimation for covariate shift adaptation\n", "abstract": " A situation where training and test samples follow different input distributions is called covariate shift. Under covariate shift, standard learning methods such as maximum likelihood estimation are no longer consistent\u2014weighted variants according to the ratio of test and training input densities are consistent. Therefore, accurately estimating the density ratio, called the importance, is one of the key issues in covariate shift adaptation. A naive approach to this task is to first estimate training and test input densities separately and then estimate the importance by taking the ratio of the estimated densities. However, this naive approach tends to perform poorly since density estimation is a hard task particularly in high dimensional cases. In this paper, we propose a direct importance estimation method that does not involve density estimation. Our method is equipped with a natural cross validation procedure and\u00a0\u2026", "num_citations": "334\n", "authors": ["2096"]}
{"title": "Eigenspace-based anomaly detection in computer systems\n", "abstract": " We report on an automated runtime anomaly detection method at the application layer of multi-node computer systems. Although several network management systems are available in the market, none of them have sufficient capabilities to detect faults in multi-tier Web-based systems with redundancy. We model a Web-based system as a weighted graph, where each node represents a\" service\" and each edge represents a dependency between services. Since the edge weights vary greatly over time, the problem we address is that of anomaly detection from a time sequence of graphs. In our method, we first extract a feature vector from the adjacency matrix that represents the activities of all of the services. The heart of our method is to use the principal eigenvector of the eigenclusters of the graph. Then we derive a probability distribution for an anomaly measure defined for a time-series of directional data derived\u00a0\u2026", "num_citations": "315\n", "authors": ["2096"]}
{"title": "Roughly balanced bagging for imbalanced data\n", "abstract": " The class imbalance problem appears in many real\u2010world applications of classification learning. We propose an ensemble algorithm \u201cRoughly Balanced (RB) Bagging\u201d using a novel sampling technique to improve the original bagging algorithm for data sets with skewed class distributions. For this sampling method, the number of samples in the largest and smallest classes are different, but they are effectively balanced when averaged over all of the subsets, which supports the approach of bagging in a more appropriate way. Individual models in RB Bagging tend to show larger diversity, which is one of the keys of ensemble models, compared with existing bagging\u2010based methods for imbalanced data that use exactly the same number of majority and minority examples for every training subset. In addition, the proposed method makes full use of all of the minority examples by under\u2010sampling, which is efficiently\u00a0\u2026", "num_citations": "241\n", "authors": ["2096"]}
{"title": "Statistical outlier detection using direct density ratio estimation\n", "abstract": " We propose a new statistical approach to the problem of inlier-based outlier detection, i.e., finding outliers in the test set based on the training set consisting only of inliers. Our key idea is to use the ratio of training and test data densities as an outlier score. This approach is expected to have better performance even in high-dimensional problems since methods for directly estimating the density ratio without going through density estimation are available. Among various density ratio estimation methods, we employ the method called unconstrained least-squares importance fitting (uLSIF) since it is equipped with natural cross-validation procedures, allowing us to objectively optimize the value of tuning parameters such as the regularization parameter and the kernel width. Furthermore, uLSIF offers a closed-form solution as well as a closed-form formula for the leave-one-out error, so it is computationally very\u00a0\u2026", "num_citations": "208\n", "authors": ["2096"]}
{"title": "Estimation of low-rank tensors via convex optimization\n", "abstract": " In this paper, we propose three approaches for the estimation of the Tucker decomposition of multi-way arrays (tensors) from partial observations. All approaches are formulated as convex minimization problems. Therefore, the minimum is guaranteed to be unique. The proposed approaches can automatically estimate the number of factors (rank) through the optimization. Thus, there is no need to specify the rank beforehand. The key technique we employ is the trace norm regularization, which is a popular approach for the estimation of low-rank matrices. In addition, we propose a simple heuristic to improve the interpretability of the obtained factorization. The advantages and disadvantages of three proposed approaches are demonstrated through numerical experiments on both synthetic and real world datasets. We show that the proposed convex optimization based approaches are more accurate in predictive performance, faster, and more reliable in recovering a known multilinear structure than conventional approaches.", "num_citations": "202\n", "authors": ["2096"]}
{"title": "A parameterized probabilistic model of network evolution for supervised link prediction\n", "abstract": " We introduce a new approach to the problem of link prediction for network structured domains, such as the Web, social networks, and biological networks. Our approach is based on the topological features of network structures, not on the node features. We present a novel parameterized probabilistic model of network evolution and derive an efficient incremental learning algorithm for such models, which is then used to predict links among the nodes. We show some promising experimental results using biological network data sets.", "num_citations": "184\n", "authors": ["2096"]}
{"title": "Kernels for semi-structured data\n", "abstract": " Semi-structured data such as XML and HTML is attracting considerable attention. It is important to develop various kinds of data mining techniques that can handle semistructured data. In this paper, we discuss applications of kernel methods for semistructured data. We model semi-structured data by labeled ordered trees, and present kernels for classifying labeled ordered trees based on their tag structures by generalizing the convolution kernel for parse trees introduced by Collins and Duffy (2001). We give algorithms to efficiently compute the kernels for labeled ordered trees. We also apply our kernels to node marking problems that are special cases of information extraction from trees. Preliminary experiments using artificial data and real HTML documents show encouraging results.", "num_citations": "179\n", "authors": ["2096"]}
{"title": "Link propagation: A fast semi-supervised learning algorithm for link prediction\n", "abstract": " We propose Link Propagation as a new semi-supervised learning method for link prediction problems, where the task is to predict unknown parts of the network structure by using auxiliary information such as node similarities. Since the proposed method can fill in missing parts of tensors, it is applicable to multi-relational domains, allowing us to handle multiple types of links simultaneously. We also give a novel efficient algorithm for Link Propagation based on an accelerated conjugate gradient method.", "num_citations": "173\n", "authors": ["2096"]}
{"title": "Statistical performance of convex tensor decomposition\n", "abstract": " We analyze the statistical performance of a recently proposed convex tensor decomposition algorithm. Conventionally tensor decomposition has been formulated as non-convex optimization problems, which hindered the analysis of their performance. We show under some conditions that the mean squared error of the convex method scales linearly with the quantity we call the normalized rank of the true tensor. The current analysis naturally extends the analysis of convex low-rank matrix estimation to tensors. Furthermore, we show through numerical experiments that our theory can precisely predict the scaling behaviour in practice.", "num_citations": "168\n", "authors": ["2096"]}
{"title": "Kernels for graphs\n", "abstract": " This chapter discusses the construction of kernel functions between labeled graphs. We provide a unified account of a family of kernels called label sequence kernels that are defined via label sequences generated by graph traversal. For cyclic graphs, dynamic programming techniques cannot simply be applied, because the kernel is based on an infinite dimensional feature space. We show that the kernel computation boils down to obtaining the stationary state of a discrete-time linear system, which is efficiently performed by solving simultaneous linear equations. Promising empirical results are presented in classification of chemical compounds.", "num_citations": "168\n", "authors": ["2096"]}
{"title": "Tensor factorization using auxiliary information\n", "abstract": " Most of the existing analysis methods for tensors (or multi-way arrays) only assume that tensors to be completed are of low rank. However, for example, when they are applied to tensor completion problems, their prediction accuracy tends to be significantly worse when only a limited number of entries are observed. In this paper, we propose to use relationships among data as auxiliary information in addition to the low-rank assumption to improve the quality of tensor decomposition. We introduce two regularization approaches using graph Laplacians induced from the relationships, one for moderately sparse cases and the other for extremely sparse cases. We also give present two kinds of iterative algorithms for approximate solutions: one based on an EM-like algorithms which is stable but not so scalable, and the other based on gradient-based optimization which is applicable to large scale datasets\u00a0\u2026", "num_citations": "153\n", "authors": ["2096"]}
{"title": "Direct density ratio estimation for large-scale covariate shift adaptation\n", "abstract": " Covariate shift is a situation in supervised learning where training and test inputs follow different distributions even though the functional relation remains unchanged. A common approach to compensating for the bias caused by covariate shift is to reweight the loss function according to the importance, which is the ratio of test and training densities. We propose a novel method that allows us to directly estimate the importance from samples without going through the hard task of density estimation. An advantage of the proposed method is that the computation time is nearly independent of the number of test input samples, which is highly beneficial in recent applications with large numbers of unlabeled samples. We demonstrate through experiments that the proposed method is computationally more efficient than existing approaches with comparable accuracy. We also describe a promising result for large-scale covariate shift adaptation in a natural language processing task.", "num_citations": "144\n", "authors": ["2096"]}
{"title": "Kernels for graph classification\n", "abstract": " In this paper, we apply kernel methods to graph classification problems. To achieve the goal, we have to design an appropriate kernel for computing inner products for pairs of graphs represented in a feature space. We define a graph kernel by a random walk on a vertex product graph of two graphs. Some experiments on predicting properties of chemical compounds show encouraging results.", "num_citations": "136\n", "authors": ["2096"]}
{"title": "A linear-time graph kernel\n", "abstract": " The design of a good kernel is fundamental for knowledge discovery from graph-structured data. Existing graph kernels exploit only limited information about the graph structures but are still computationally expensive. We propose a novel graph kernel based on the structural characteristics of graphs. The key is to represent node labels as binary arrays and characterize each node using logical operations on the label set of the connected nodes. Our kernel has a linear time complexity with respect to the number of nodes times the average number of neighboring nodes in the given graphs. The experimental result shows that the proposed kernel performs comparable and much faster than a state-of-the-art graph kernel for benchmark data sets and shows high scalability for new applications with large graphs.", "num_citations": "125\n", "authors": ["2096"]}
{"title": "Steered crowdsensing: Incentive design towards quality-oriented place-centric crowdsensing\n", "abstract": " Crowdsensing technologies are rapidly evolving and are expected to be utilized on commercial applications such as location-based services. Crowdsensing collects sensory data from daily activities of users without burdening users, and the data size is expected to grow into a population scale. However, quality of service is difficult to ensure for commercial use. Incentive design in crowdsensing with monetary rewards or gamifications is, therefore, attracting attention for motivating participants to collect data to increase data quantity. In contrast, we propose Steered Crowdsensing, which controls the incentives of users by using the game elements on location-based services for directly improving the quality of service rather than data size. For a feasibility study of steered crowdsensing, we deployed a crowdsensing system focusing on application scenarios of building processes on wireless indoor localization systems\u00a0\u2026", "num_citations": "119\n", "authors": ["2096"]}
{"title": "Statistical quality estimation for general crowdsourcing tasks\n", "abstract": " One of the biggest challenges for requesters and platform providers of crowdsourcing is quality control, which is to expect high-quality results from crowd workers who are neither necessarily very capable nor motivated. A common approach to tackle this problem is to introduce redundancy, that is, to request multiple workers to work on the same tasks. For simple multiple-choice tasks, several statistical methods to aggregate the multiple answers have been proposed. However, these methods cannot always be applied to more general tasks with unstructured response formats such as article writing, program coding, and logo designing, which occupy the majority on most crowdsourcing marketplaces. In this paper, we propose an unsupervised statistical quality estimation method for such general crowdsourcing tasks. Our method is based on the two-stage procedure; multiple workers are first requested to work on the\u00a0\u2026", "num_citations": "114\n", "authors": ["2096"]}
{"title": "Multi-task learning via conic programming\n", "abstract": " When we have several related tasks, solving them simultaneously is shown to be more effective than solving them individually. This approach is called multi-task learning (MTL) and has been studied extensively. Existing approaches to MTL often treat all the tasks as uniformly related to each other and the relatedness of the tasks is controlled globally. For this reason, the existing methods can lead to undesired solutions when some tasks are not highly related to each other, and some pairs of related tasks can have significantly different solutions. In this paper, we propose a novel MTL algorithm that can overcome these problems. Our method makes use of a task network, which describes the relation structure among tasks. This allows us to deal with intricate relation structures in a systematic way. Furthermore, we control the relatedness of the tasks locally, so all pairs of related tasks are guaranteed to have similar solutions. We apply the above idea to support vector machines (SVMs) and show that the optimization problem can be cast as a second order cone program, which is convex and can be solved efficiently. The usefulness of our approach is demonstrated through simulations with protein super-family classification and ordinal regression problems.", "num_citations": "103\n", "authors": ["2096"]}
{"title": "Training conditional random fields using incomplete annotations\n", "abstract": " We address corpus building situations, where complete annotations to the whole corpus is time consuming and unrealistic. Thus, annotation is done only on crucial part of sentences, or contains unresolved label ambiguities. We propose a parameter estimation method for Conditional Random Fields (CRFs), which enables us to use such incomplete annotations. We show promising results of our method as applied to two types of NLP tasks: a domain adaptation task of a Japanese word segmentation using partial annotations, and a partof-speech tagging task using ambiguous tags in the Penn treebank corpus.", "num_citations": "100\n", "authors": ["2096"]}
{"title": "A Convex Formulation for Learning from Crowds.\n", "abstract": " Recently crowdsourcing services are often used to collect a large amount of labeled data for machine learning, since they provide us an easy way to get labels at very low cost and in a short period. The use of crowdsourcing has introduced a new challenge in machine learning, that is, coping with the variable quality of crowd-generated data. Although there have been many recent attempts to address the quality problem of multiple workers, only a few of the existing methods consider the problem of learning classifiers directly from such noisy data. All these methods modeled the true labels as latent variables, which resulted in non-convex optimization problems. In this paper, we propose a convex optimization formulation for learning from crowds without estimating the true labels by introducing personal models of the individual crowd workers. We also devise an efficient iterative method for solving the convex optimization problems by exploiting conditional independence structures in multiple classifiers. We evaluate the proposed method against three competing methods on synthetic data sets and a real crowdsourced data set and demonstrate that the proposed method outperforms the other three methods.", "num_citations": "99\n", "authors": ["2096"]}
{"title": "Nonparametric return distribution approximation for reinforcement learning\n", "abstract": " Standard Reinforcement Learning (RL) aims to optimize decision-making rules in terms of the expected return. However, especially for risk-management purposes, other criteria such as the expected shortfall are sometimes preferred. Here, we describe a method of approximating the distribution of returns, which allows us to derive various kinds of information about the returns. We first show that the Bellman equation, which is a recursive formula for the expected return, can be extended to the cumulative return distribution. Then we derive a nonparametric return distribution estimator with particle smoothing based on this extended Bellman equation. A key aspect of the proposed algorithm is to represent the recursion relation in the extended Bellman equation by a simple replacement procedure of particles associated with a state by using those of the successor state. We show that our algorithm leads to a risk-sensitive RL paradigm. The usefulness of the proposed approach is demonstrated through numerical experiments.", "num_citations": "92\n", "authors": ["2096"]}
{"title": "Parametric return density estimation for reinforcement learning\n", "abstract": " Most conventional Reinforcement Learning (RL) algorithms aim to optimize decision-making rules in terms of the expected returns. However, especially for risk management purposes, other risk-sensitive criteria such as the value-at-risk or the expected shortfall are sometimes preferred in real applications. Here, we describe a parametric method for estimating density of the returns, which allows us to handle various criteria in a unified manner. We first extend the Bellman equation for the conditional expected return to cover a conditional probability density of the returns. Then we derive an extension of the TD-learning algorithm for estimating the return densities in an unknown environment. As test instances, several parametric density estimation algorithms are presented for the Gaussian, Laplace, and skewed Laplace distributions. We show that these algorithms lead to risk-sensitive as well as robust RL paradigms through numerical experiments.", "num_citations": "87\n", "authors": ["2096"]}
{"title": "Inlier-based outlier detection via direct density ratio estimation\n", "abstract": " We propose a new statistical approach to the problem of inlier-based outlier detection, i.e.,finding outliers in the test set based on the training set consisting only of inliers. Our key idea is to use the ratio of training and test data densities as an outlier score; we estimate the ratio directly in a semi-parametric fashion without going through density estimation. Thus our approach is expected to have better performance in high-dimensional problems. Furthermore, the applied algorithm for density ratio estimation is equipped with a natural cross-validation procedure, allowing us to objectively optimize the value of tuning parameters such as the regularization parameter and the kernel width. The algorithm offers a closed-form solution as well as a closed-form formula for the leave-one-out error. Thanks to this, the proposed outlier detection method is computationally very efficient and is scalable to massive datasets\u00a0\u2026", "num_citations": "86\n", "authors": ["2096"]}
{"title": "Unsupervised change analysis using supervised learning\n", "abstract": " We propose a formulation of a new problem, which we call change analysis, and a novel method for solving the problem. In contrast to the existing methods of change (or outlier) detection, the goal of change analysis goes beyond detecting whether or not any changes exist. Its ultimate goal is to find the explanation of the changes. While change analysis falls in the category of unsupervised learning in nature, we propose a novel approach based on supervised learning to achieve the goal. The key idea is to use a supervised classifier for interpreting the changes. A classifier should be able to discriminate between the two data sets if they actually come from two different data sources. In other words, we use a hypothetical label to train the supervised learner, and exploit the learner for interpreting the change. Experimental results using real data show the proposed approach is promising in change analysis as\u00a0\u2026", "num_citations": "82\n", "authors": ["2096"]}
{"title": "Anomaly detection\n", "abstract": " A system such as a Web-based system in which a plurality of computers interact with each other is monitored to detect online an anomaly. Transactions of a service provided by each of a plurality of computers to another computer are collected, a matrix of correlations between nodes in the system is calculated from the transactions, and a feature vector representing a node activity balance is obtained from the matrix. The feature vector is monitored using a probability model to detect a transition to an anomalous state.", "num_citations": "80\n", "authors": ["2096"]}
{"title": "Machine learning approach for finding business partners and building reciprocal relationships\n", "abstract": " Business development is vital for any firms. However, globalization and the rapid development of technologies have made it difficult to find appropriate business partners such as suppliers and customers, and build reciprocal relationships among them, while it simultaneously offers many opportunities. In this contribution, we propose AI-based approach to find plausible candidates of business partners using firm profiles and transactional relationships among them. We employ machine learning techniques to build a prediction model of customer\u2013supplier relationships. We applied our approach to the large amount of actual business data. The results showed that our approach successfully found potential business partners with F-values of about 84% and reciprocity among them with F-values of about 77%. Using our method, we also developed the Web-based system that helps people in actual businesses to find their\u00a0\u2026", "num_citations": "77\n", "authors": ["2096"]}
{"title": "Fast and scalable algorithms for semi-supervised link prediction on static and dynamic graphs\n", "abstract": " Recent years have witnessed a widespread interest on methods using both link structure and node information for link prediction on graphs. One of the state-of-the-art methods is Link Propagation which is a new semi-supervised learning algorithm for link prediction on graphs based on the popularly-studied label propagation by exploiting information on similarities of links and nodes. Despite its efficiency and effectiveness compared to other methods, its applications were still limited due to the computational time and space constraints. In this paper, we propose fast and scalable algorithms for the Link Propagation by introducing efficient procedures to solve large linear equations that appear in the method. In particular, we show how to obtain a compact representation of the solution to the linear equations by using a non-trivial combination of techniques in linear algebra to construct algorithms that are also\u00a0\u2026", "num_citations": "76\n", "authors": ["2096"]}
{"title": "Accurate integration of crowdsourced labels using workers' self-reported confidence scores\n", "abstract": " We have developed a method for using confidence scores to integrate labels provided by crowdsourcing workers. Although confidence scores can be useful information for estimating the quality of the provided labels, a way to effectively incorporate them into the integration process has not been established. Moreover, some workers are overconfident about the quality of their labels while others are underconfident, and some workers are quite accurate in judging the quality of their labels. This differing reliability of the confidence scores among workers means that the probability distributions for the reported confidence scores differ among workers. To address this problem, we extended the Dawid-Skene model and created two probabilistic models in which the values of unobserved true labels are inferred from the observed provided labels and reported confidence scores by using the expectation-maximization algorithm. Results of experiments using actual crowdsourced data for image labeling and binary question answering tasks showed that incorporating workers' confidence scores can improve the accuracy of integrated crowdsourced labels.", "num_citations": "67\n", "authors": ["2096"]}
{"title": "Large-scale personalized human activity recognition using online multitask learning\n", "abstract": " Personalized activity recognition usually has the problem of highly biased activity patterns among different tasks/persons. Traditional methods face problems on dealing with those conflicted activity patterns. We try to effectively model the activity patterns among different persons via casting this personalized activity recognition problem as a multitask learning issue. We propose a novel online multitask learning method for large-scale personalized activity recognition. In contrast with existing work of multitask learning that assumes fixed task relationships, our method can automatically discover task relationships from real-world data. Convergence analysis shows reasonable convergence properties of the proposed method. Experiments on two different activity data sets demonstrate that the proposed method significantly outperforms existing methods in activity recognition.", "num_citations": "65\n", "authors": ["2096"]}
{"title": "Knowledge tracing machines: Factorization machines for knowledge tracing\n", "abstract": " Knowledge tracing is a sequence prediction problem where the goal is to predict the outcomes of students over questions as they are interacting with a learning platform. By tracking the evolution of the knowledge of some student, one can optimize instruction. Existing methods are either based on temporal latent variable models, or factor analysis with temporal features. We here show that factorization machines (FMs), a model for regression or classification, encompasses several existing models in the educational literature as special cases, notably additive factor model, performance factor model, and multidimensional item response theory. We show, using several real datasets of tens of thousands of users and items, that FMs can estimate student knowledge accurately and fast even when student data is sparsely observed, and handle side information such as multiple knowledge components and number of attempts at item or skill level. Our approach allows to fit student models of higher dimension than existing models, and provides a testbed to try new combinations of features in order to improve existing models.", "num_citations": "60\n", "authors": ["2096"]}
{"title": "Generalized expansion dimension\n", "abstract": " In this paper we propose a framework for modeling the intrinsic dimensionality of data sets. The models can be viewed as generalizations of the expansion dimension, which was originally proposed for the analysis of certain similarity search indices using the Euclidean distance metric. Here, we extend the original model to other metric spaces: vector spaces with the L p  or vector angle (cosine similarity) distance measures, as well as product spaces for categorical data. We also provide a practical guide for estimating both local and global intrinsic dimensionality. The estimates of data complexity can subsequently be used in the design and analysis of algorithms for data mining applications such as search, clustering, classification, and outlier detection.", "num_citations": "60\n", "authors": ["2096"]}
{"title": "Regret lower bound and optimal algorithm in dueling bandit problem\n", "abstract": " We study the K-armed dueling bandit problem, a variation of the standard stochastic bandit problem where the feedback is limited to relative comparisons of a pair of arms. We introduce a tight asymptotic regret lower bound that is based on the information divergence. An algorithm that is inspired by the Deterministic Minimum Empirical Divergence algorithm (Honda and Takemura, 2010) is proposed, and its regret is analyzed. The proposed algorithm is found to be the first one with a regret upper bound that matches the lower bound. Experimental comparisons of dueling bandit algorithms show that the proposed algorithm significantly outperforms existing ones.", "num_citations": "57\n", "authors": ["2096"]}
{"title": "Robust label propagation on multiple networks\n", "abstract": " Transductive inference on graphs such as label propagation algorithms is receiving a lot of attention. In this paper, we address a label propagation problem on multiple networks and present a new algorithm that automatically integrates structure information brought in by multiple networks. The proposed method is robust in that irrelevant networks are automatically deemphasized, which is an advantage over Tsuda's approach (2005). We also show that the proposed algorithm can be interpreted as an expectation-maximization (EM) algorithm with a student- t  prior. Finally, we demonstrate the usefulness of our method in protein function prediction and digit classification, and show analytically and experimentally that our algorithm is much more efficient than existing algorithms.", "num_citations": "55\n", "authors": ["2096"]}
{"title": "Protein complex prediction via verifying and reconstructing the topology of domain-domain interactions\n", "abstract": " High-throughput methods for detecting protein-protein interactions enable us to obtain large interaction networks, and also allow us to computationally identify the associations of proteins as protein complexes. Although there are methods to extract protein complexes as sets of proteins from interaction networks, the extracted complexes may include false positives because they do not account for the structural limitations of the proteins and thus do not check that the proteins in the extracted complex can simultaneously bind to each other. In addition, there have been few searches for deeper insights into the protein complexes, such as of the topology of the protein-protein interactions or into the domain-domain interactions that mediate the protein interactions. Here, we introduce a combinatorial approach for prediction of protein complexes focusing not only on determining member proteins in complexes but also on the\u00a0\u2026", "num_citations": "51\n", "authors": ["2096"]}
{"title": "Cross-temporal link prediction\n", "abstract": " The increasing interest in dynamically changing networks has led to growing interest in a more general link prediction problem called temporal link prediction in the data mining and machine learning communities. However, only links in identical time frames are considered in temporal link prediction. We propose a new link prediction problem called cross-temporal link prediction in which the links among nodes in different time frames are inferred. A typical example of cross-temporal link prediction is cross-temporal entity resolution to determine the identity of real entities represented by data objects observed in different time periods. In dynamic environments, the features of data change over time, making it difficult to identify cross-temporal links by directly comparing observed data. Other examples of cross-temporal links are asynchronous communications in social networks such as Face book and Twitter, where a\u00a0\u2026", "num_citations": "50\n", "authors": ["2096"]}
{"title": "A gram distribution kernel applied to glycan classification and motif extraction\n", "abstract": " We propose a novel general-purpose tree kernel and apply it to glycan structure analysis. Our kernel measures the similarity between two labeled trees by counting the number of common qlength substrings (tree q-grams) embedded in the trees for all possible lengths q. We apply our tree kernel using a support vector machine (SVM) to classification and specific feature extraction from glycan structure data. Our results show that our kernel outperforms the layered trimer kernel of Hizukuri et al.[9] which is well tailored to glycan data while we do not adjust our kernel to glycanspecific properties. In addition, we extract specific features from various types of glycan data using our trained SVM. The results show that our kernel is more flexible and capable of finding a wider variety of substructures from glycan data.", "num_citations": "50\n", "authors": ["2096"]}
{"title": "Side effect prediction using cooperative pathways\n", "abstract": " Drugs and biological experiments are designed to affect a particular target gene or pathway. However, they might inadvertently activate other pathways and cause side effects. Because of the existence of complex cellular mechanisms responding to stimuli, it is difficult to detect the presence of such side effects. Therefore, identification of pathways that function together under identical conditions would greatly help in anticipating these side effects before conducting these experiments. We develop a novel method to enumerate \"cooperative pathways\" defined as pathways that function together under identical conditions by combining pathway networks with comprehensive gene expression profiles. For finding cooperative pathways from whole pathways, we propose an efficient algorithm, CoopeRativE Pathway Enumerator (CREPE), which enumerates connected subpathways having common activate conditions and\u00a0\u2026", "num_citations": "48\n", "authors": ["2096"]}
{"title": "A spectrum tree kernel\n", "abstract": " Learning from tree-structured data has received increasing interest with the rapid growth of treeencodable data in the World Wide Web, in biology, and in other areas. Our kernel function measures the similarity between two trees by counting the number of shared sub-patterns called tree q-grams, and runs, in effect, in linear time with respect to the number of tree nodes. We apply our kernel function with a support vector machine (SVM) to classify biological data, the glycans of several blood components. The experimental results show that our kernel function performs as well as one exclusively tailored to glycan properties.", "num_citations": "47\n", "authors": ["2096"]}
{"title": "Simultaneous modeling of multiple diseases for mortality prediction in acute hospital care\n", "abstract": " Acute hospital care as performed in the intensive care unit (ICU) is characterized by its frequent, but short-term interventions for patients who are severely ill. Because clinicians have to attend to more than one patient at a time and make decisions in a limited time in acute hospital care environments, the accurate prediction of the in-hospital mortality risk could assist them to pay more attention to patients with a higher in-hospital mortality risk, thereby improving the quality and efficiency of the care. One of the salient features of ICU is the diversity of patients: clinicians are faced by patients with a wide variety of diseases. However, mortality prediction for ICU patients has typically been conducted by building one common predictive model for all the diseases. In this paper, we incorporate disease-specific contexts into mortality modeling by formulating the mortality prediction problem as a multi-task learning problem in\u00a0\u2026", "num_citations": "46\n", "authors": ["2096"]}
{"title": "K-means clustering of proportional data using L1 distance\n", "abstract": " We present a new L1-distance-based k-means clustering algorithm to address the challenge of clustering high-dimensional proportional vectors. The new algorithm explicitly incorporates proportionality constraints in the computation of the cluster centroids, resulting in reduced L1 error rates. We compare the new method to two competing methods, an approximate L1-distance k-means algorithm, where the centroid is estimated using cluster means, and a median L1 k-means algorithm, where the centroid is estimated using cluster medians, with proportionality constraints imposed by normalization in a second step. Application to clustering of projects based on distribution of labor hours by skill illustrates the advantages of the new algorithm.", "num_citations": "46\n", "authors": ["2096"]}
{"title": "Preserving worker privacy in crowdsourcing\n", "abstract": " This paper proposes a crowdsourcing quality control method with worker-privacy preservation. Crowdsourcing allows us to outsource tasks to a number of workers. The results of tasks obtained in crowdsourcing are often low-quality due to the difference in the degree of skill. Therefore, we need quality control methods to estimate reliable results from low-quality results. In this paper, we point out privacy problems of workers in crowdsourcing. Personal information of workers can be inferred from the results provided by each worker. To formulate and to address the privacy problems, we define a worker-private quality control problem, a variation of the quality control problem that preserves privacy of workers. We propose a worker-private latent class protocol where a requester can estimate the true results with worker privacy preserved. The key ideas are decentralization of computation and introduction of\u00a0\u2026", "num_citations": "44\n", "authors": ["2096"]}
{"title": "Approximation ratios of graph neural networks for combinatorial problems\n", "abstract": " In this paper, from a theoretical perspective, we study how powerful graph neural networks (GNNs) can be for learning approximation algorithms for combinatorial problems. To this end, we first establish a new class of GNNs that can solve a strictly wider variety of problems than existing GNNs. Then, we bridge the gap between GNN theory and the theory of distributed local algorithms. We theoretically demonstrate that the most powerful GNN can learn approximation algorithms for the minimum dominating set problem and the minimum vertex cover problem with some approximation ratios with the aid of the theory of distributed local algorithms. We also show that most of the existing GNNs such as GIN, GAT, GCN, and GraphSAGE cannot perform better than with these ratios. This paper is the first to elucidate approximation ratios of GNNs for combinatorial problems. Furthermore, we prove that adding coloring or weak-coloring to each node feature improves these approximation ratios. This indicates that preprocessing and feature engineering theoretically strengthen model capabilities.", "num_citations": "43\n", "authors": ["2096"]}
{"title": "Learning from crowds and experts\n", "abstract": " Crowdsourcing services are often used to collect a large amount of labeled data for machine learning. Although they provide us an easy way to get labels at very low cost in a short period, they have serious limitations. One of them is the variable quality of the crowd-generated data. There have been many attempts to increase the reliability of crowd-generated data and the quality of classifiers obtained from such data. However, in these problem settings, relatively few researchers have tried using expert-generated data to achieve further improvements. In this paper, we extend three models that deal with the problem of learning from crowds to utilize ground truths: a latent class model, a personal classifier model, and a data-dependent error model. We evaluate the proposed methods against two baseline methods on a real data set to demonstrate the effectiveness of combining crowd-generated data and expert-generated data.", "num_citations": "41\n", "authors": ["2096"]}
{"title": "On the extension of trace norm to tensors\n", "abstract": " In this paper, we propose three extensions of trace norm for the minimization of tensor rank via convex optimization. One of the proposed extensions recovers partially observed tensor almost perfectly from a small fraction of observations.", "num_citations": "39\n", "authors": ["2096"]}
{"title": "Matrix-and tensor-based recommender systems for the discovery of currently unknown inorganic compounds\n", "abstract": " Chemically relevant compositions (CRCs) and atomic arrangements of inorganic compounds have been collected as inorganic crystal structure databases. Machine learning is a unique approach to search for currently unknown CRCs from vast candidates. Herein we propose matrix-and tensor-based recommender system approaches to predict currently unknown CRCs from database entries of CRCs. Firstly, the performance of the recommender system approaches to discover currently unknown CRCs is examined. A Tucker decomposition recommender system shows the best discovery rate of CRCs as the majority of the top 100 recommended ternary and quaternary compositions correspond to CRCs. Secondly, systematic density functional theory (DFT) calculations are performed to investigate the phase stability of the recommended compositions. The phase stability of the 27 compositions reveals that 23\u00a0\u2026", "num_citations": "37\n", "authors": ["2096"]}
{"title": "Theoretical evidence for adversarial robustness through randomization\n", "abstract": " This paper investigates the theory of robustness against adversarial attacks. It focuses on the family of randomization techniques that consist in injecting noise in the network at inference time. These techniques have proven effective in many contexts, but lack theoretical arguments. We close this gap by presenting a theoretical analysis of these approaches, hence explaining why they perform well in practice. More precisely, we make two new contributions. The first one relates the randomization rate to robustness to adversarial attacks. This result applies for the general family of exponential distributions, and thus extends and unifies the previous approaches. The second contribution consists in devising a new upper bound on the adversarial generalization gap of randomized neural networks. We support our theoretical claims with a set of experiments.", "num_citations": "36\n", "authors": ["2096"]}
{"title": "Health checkup and telemedical intervention program for preventive medicine in developing countries: verification study\n", "abstract": " Background: The prevalence of non-communicable diseases is increasing throughout the world, including developing countries.Objective: The intent was to conduct a study of a preventive medical service in a developing country, combining eHealth checkups and teleconsultation as well as assess stratification rules and the short-term effects of intervention.Methods: We developed an eHealth system that comprises a set of sensor devices in an attach\u00e9 case, a data transmission system linked to a mobile network, and a data management application. We provided eHealth checkups for the populations of five villages and the employees of five factories/offices in Bangladesh. Individual health condition was automatically categorized into four grades based on international diagnostic standards: green (healthy), yellow (caution), orange (affected), and red (emergent). We provided teleconsultation for orange-and red-grade subjects and we provided teleprescription for these subjects as required.Results: The first checkup was provided to 16,741 subjects. After one year, 2361 subjects participated in the second checkup and the systolic blood pressure of these subjects was significantly decreased from an average of 121 mmHg to an average of 116 mmHg (P<. 001). Based on these results, we propose a cost-effective method using a machine learning technique (random forest method) using the medical interview, subject profiles, and checkup results as predictor to avoid costly measurements of blood sugar, to ensure sustainability of the program in developing countries.Conclusions: The results of this study demonstrate the benefits of an eHealth checkup\u00a0\u2026", "num_citations": "36\n", "authors": ["2096"]}
{"title": "Clustering crowds\n", "abstract": " We present a clustered personal classifier method (CPC method) that jointly estimates a classifier and clusters of workers in order to address the learning from crowds problem. Crowdsourcing allows us to create a large but low-quality data set at very low cost. The learning from crowds problem is to learn a classifier from such a low-quality data set. From some observations, we notice that workers form clusters according to their abilities. Although such a fact was pointed out several times, no method has applied it to the learning from crowds problem. We propose a CPC method that utilizes the clusters of the workers to improve the performance of the obtained classifier, where both the classifier and the clusters of the workers are estimated. The proposed method has two advantages. One is that it realizes robust estimation of the classifier because it utilizes prior knowledge about the workers that they tend to form clusters. The other is that we can obtain the clusters of the workers, which help us analyze the properties of the workers. Experimental results on synthetic and real data sets indicate that the proposed method can estimate the classifier robustly. In addition, clustering workers is shown to work well. Especially in the real data set, an outlier worker was found by applying the proposed method.", "num_citations": "36\n", "authors": ["2096"]}
{"title": "Leveraging non-expert crowdsourcing workers for improper task detection in crowdsourcing marketplaces\n", "abstract": " Controlling the quality of tasks, ie, propriety of posted jobs, is a major challenge in crowdsourcing marketplaces. Most existing crowdsourcing services prohibit requesters from posting illegal or objectionable tasks. Operators in marketplaces have to monitor tasks continuously to find such improper ones; however, it is very expensive to manually investigate each task. In this paper, we present the results of our trial study on automatic detection of improper tasks to support the monitoring of activities by marketplace operators. We performed experiments using real task data from a commercial crowdsourcing marketplace and showed that the classifier trained by the operators\u2019 judgments achieves a high performance in detecting improper tasks. By analyzing the estimated classifier, we observed several effective features for detecting improper tasks, such as the words appeared in the task information, the amount of money\u00a0\u2026", "num_citations": "35\n", "authors": ["2096"]}
{"title": "Kernel-based discriminative learning algorithms for labeling sequences, trees, and graphs\n", "abstract": " We introduce a new perceptron-based discriminative learning algorithm for labeling structured data such as sequences, trees, and graphs. Since it is fully kernelized and uses pointwise label prediction, large features, including arbitrary number of hidden variables, can be incorporated with polynomial time complexity. This is in contrast to existing labelers that can handle only features of a small number of hidden variables, such as Maximum Entropy Markov Models and Conditional Random Fields. We also introduce several kernel functions for labeling sequences, trees, and graphs and efficient algorithms for them.", "num_citations": "34\n", "authors": ["2096"]}
{"title": "A fast augmented lagrangian algorithm for learning low-rank matrices\n", "abstract": " We propose a general and efficient algorithm for learning low-rank matrices. The proposed algorithm converges super-linearly and can keep the matrix to be learned in a compact factorized representation without the need of specifying the rank beforehand. Moreover, we show that the framework can be easily generalized to the problem of learning multiple matrices and general spectral regularization. Empirically we show that we can recover a 10,000\u00d7 10,000 matrix from 1.2 million observations in about 5 minutes. Furthermore, we show that in a brain-computer interface problem, the proposed method can speed-up the optimization by two orders of magnitude against the conventional projected gradient method and produces more reliable solutions.", "num_citations": "32\n", "authors": ["2096"]}
{"title": "Conic programming for multitask learning\n", "abstract": " When we have several related tasks, solving them simultaneously has been shown to be more effective than solving them individually. This approach is called multitask learning (MTL). In this paper, we propose a novel MTL algorithm. Our method controls the relatedness among the tasks locally, so all pairs of related tasks are guaranteed to have similar solutions. We apply the above idea to support vector machines and show that the optimization problem can be cast as a second-order cone program, which is convex and can be solved efficiently. The usefulness of our approach is demonstrated in ordinal regression, link prediction, and collaborative filtering, each of which can be formulated as a structured multitask problem.", "num_citations": "32\n", "authors": ["2096"]}
{"title": "On pairwise kernels: An efficient alternative and generalization analysis\n", "abstract": " Pairwise classification has many applications including network prediction, entity resolution, and collaborative filtering. The pairwise kernel has been proposed for those purposes by several research groups independently, and become successful in various fields. In this paper, we propose an efficient alternative which we call Cartesian kernel. While the existing pairwise kernel (which we refer to as Kronecker kernel) can be interpreted as the weighted adjacency matrix of the Kronecker product graph of two graphs, the Cartesian kernel can be interpreted as that of the Cartesian graph which is more sparse than the Kronecker product graph. Experimental results show the Cartesian kernel is much faster than the existing pairwise kernel, and at the same time, competitive with the existing pairwise kernel in predictive performance.We discuss the generalization bounds by the two pairwise kernels by using\u00a0\u2026", "num_citations": "32\n", "authors": ["2096"]}
{"title": "Random features strengthen graph neural networks\n", "abstract": " Graph neural networks (GNNs) are powerful machine learning models for various graph learning tasks. Recently, the limitations of the expressive power of various GNN models have been revealed. For example, GNNs cannot distinguish some non-isomorphic graphs and they cannot learn efficient graph algorithms. In this paper, we demonstrate that GNNs become powerful just by adding a random feature to each node. We prove that the random features enable GNNs to learn almost optimal polynomial-time approximation algorithms for the minimum dominating set problem and maximum matching problem in terms of approximation ratios. The main advantage of our method is that it can be combined with off-the-shelf GNN models with slight modifications. Through experiments, we show that the addition of random features enables GNNs to solve various problems that normal GNNs, including the graph\u00a0\u2026", "num_citations": "30\n", "authors": ["2096"]}
{"title": "Budgeted stream-based active learning via adaptive submodular maximization.\n", "abstract": " Budgeted stream-based active learning via adaptive submodular maximization Page 1 1/ 26 Budgeted stream-based active learning via adaptive submodular maximization Kaito Fujii (UTokyo) (joint work with Hisashi Kashima (KyotoU)) ERATO \u611f\u8b1d\u796d Season IV 2017 \u5e74 8 \u6708 3 \u65e5 Page 2 2/ 26 \u76ee\u6b21 1 \u5fdc\u7528\uff1a\u30d7\u30fc\u30eb\u578b\u80fd\u52d5\u5b66\u7fd2\u3068\u30b9\u30c8\u30ea\u30fc\u30e0\u578b\u80fd\u52d5\u5b66\u7fd2 2 \u65e2\u5b58\u7814\u7a76\uff1a\u9069\u5fdc\u7684\u52a3 \u30e2\u30b8\u30e5\u30e9\u6700\u5927\u5316 3 \u65e2\u5b58\u7814\u7a76\uff1a\u52a3\u30e2\u30b8\u30e5\u30e9\u79d8\u66f8\u554f\u984c 4 \u63d0\u6848\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af 5 \u5b9f\u9a13 Page 3 3/ 26 \u6559\u5e2b\u3042\u308a \u5206\u985e \u5165\u529b \u5165\u529b \u30e9\u30d9\u30eb\u3064\u304d\u30b5\u30f3\u30d7\u30eb\u306e\u96c6\u5408 {(xi,yi) \u2208 X \u00d7 Y}i=1,\u00b7\u00b7\u00b7 ,n \u51fa\u529b \u51fa\u529b \u30e9\u30d9\u30eb\u3092\u4e88\u6e2c\u3059\u308b\u95a2\u6570 \u02c6f : X \u2192 Y \u4f8b\uff09 X = R 2, Y = {red,blue} Page 4 3/ 26 \u6559\u5e2b\u3042\u308a\u5206\u985e \u5165\u529b \u5165\u529b \u30e9\u30d9\u30eb\u3064\u304d\u30b5\u30f3\u30d7\u30eb\u306e \u96c6\u5408 {(xi,yi) \u2208 X \u00d7 Y}i=1,\u00b7\u00b7\u00b7 ,n \u51fa\u529b \u51fa\u529b \u30e9\u30d9\u30eb\u3092\u4e88\u6e2c\u3059\u308b\u95a2\u6570 \u02c6f : X \u2192 Y \u4f8b\uff09 X = R 2, Y = {red,blue} Page 5 4/ 26 \u80fd\u52d5\u5b66\u7fd2\u306e\u52d5\u6a5f \u73fe\u5b9f\u306b\u306f \u30e9\u30d9\u30eb\u306a\u3057\u30b5\u30f3\u30d7\u30eb\u306f\u305f\u304f\u3055\u3093\u624b\u306b\u5165\u308b\u304c\uff0c \u30e9\u30d9\u30eb \u4ed8\u4e0e\u306b\u306f\u304a\uf90a\u3084\u6642\u9593\u304c\u304b\u304b\u308b \u3068\u3044\u3046\u72b6\u6cc1\u304c\u591a\u3044 \u80fd\u52d5\u5b66\u7fd2 \u80fd\u52d5\u5b66\u7fd2 \u30e9\u30d9\u30eb\u3092\u4ed8\u4e0e\u3059\u308b\u30b5\u30f3\u30d7\u30eb\u3092\u2026", "num_citations": "29\n", "authors": ["2096"]}
{"title": "Autoregressive tensor factorization for spatio-temporal predictions\n", "abstract": " Analysis of spatio-temporal data is a common research topic that requires the interpolations of unknown locations and the predictions of feature observations by utilizing information about where and when the data were observed. One of the most difficult problems is to make predictions of unknown locations. Tensor factorization methods are popular in this field because of their capability of handling multiple types of spatio-temporal data, dealing with missing values, and providing computationally efficient parameter estimation procedures. However, unlike traditional approaches such as spatial autoregressive models, the existing tensor factorization methods have not tried to learn spatial autocorrelations. These methods employ previously inferred spatial dependencies, often resulting in poor performances on the problem of making interpolations and predictions of unknown locations. In this paper, we propose a new\u00a0\u2026", "num_citations": "28\n", "authors": ["2096"]}
{"title": "A subpath kernel for rooted unordered trees\n", "abstract": " Kernel method is one of the promising approaches to learning with tree-structured data, and various efficient tree kernels have been proposed to capture informative structures in trees. In this paper, we propose a new tree kernel function based on \u201csubpath sets\u201d to capture vertical structures in rooted unordered trees, since such tree-structures are often used to code hierarchical information in data. We also propose a simple and efficient algorithm for computing the kernel by extending the multikey quicksort algorithm used for sorting strings. The time complexity of the algorithm is O((|T                 1|\u2009+\u2009|T                 2|)log(|T                 1|\u2009+\u2009|T                 2|)) time on average, and the space complexity is O(|T                 1|\u2009+\u2009|T                 2|), where |T                 1| and |T                 2| are the numbers of nodes in two trees T                 1 and T                 2. We apply the proposed kernel to two supervised classification tasks, XML\u00a0\u2026", "num_citations": "27\n", "authors": ["2096"]}
{"title": "Supervised and unsupervised intrusion detection based on CAN message frequencies for in-vehicle network\n", "abstract": " Modern vehicles are equipped with Electronic Control Units (ECUs) and external communication devices. The Controller Area Network (CAN), a widely used communication protocol for ECUs, does not have a security mechanism to detect improper packets; if attackers exploit the vulnerability of an ECU and manage to inject a malicious message, they are able to control other ECUs to cause improper operation of the vehicle. With the increasing popularity of connected cars, it has become an urgent matter to protect in-vehicle networks against security threats. In this paper, we study the applicability of statistical anomaly detection methods for identifying malicious CAN messages in invehicle networks. We focus on intrusion attacks of malicious messages. Because the occurrence of an intrusion attack certainly influences the message traffic, we focus on the number of messages observed in a fixed time window to detect intrusion attacks. We formalize features to represent a message sequence that incorporates the number of messages associated with each receiver ID. We collected CAN message data from an actual vehicle and conducted a quantitative analysis of the methods and the features in practical situations. The results of our experiments demonstrated our proposed methods provide fast and accurate detection in various cases.", "num_citations": "26\n", "authors": ["2096"]}
{"title": "Simultaneous inference of biological networks of multiple species from genome-wide data and evolutionary information: a semi-supervised approach\n", "abstract": " Motivation: The existing supervised methods for biological network inference work on each of the networks individually based only on intra-species information such as gene expression data. We believe that it will be more effective to use genomic data and cross-species evolutionary information from different species simultaneously, rather than to use the genomic data alone.                    Results: We created a new semi-supervised learning method called Link Propagation for inferring biological networks of multiple species based on genome-wide data and evolutionary information. The new method was applied to simultaneous reconstruction of three metabolic networks of Caenorhabditis elegans, Helicobacter pylori and Saccharomyces cerevisiae, based on gene expression similarities and amino acid sequence similarities. The experimental results proved that the new simultaneous network inference\u00a0\u2026", "num_citations": "26\n", "authors": ["2096"]}
{"title": "Flexible tree kernels based on counting the number of tree mappings\n", "abstract": " Functions counting the number of common sub-patterns between trees have been promising candidates for kernel functions for trees in learning systems. There are several viewpoints of how two patterns between two trees can be regarded as the same. In the tree edit distance, these viewpoints have been well formalized as the class of tree mappings, and several distance measures have been proposed according to the classes of tree mappings. In this paper, we address the design problem of new flexible tree kernels corresponding to four representative classes of tree mappings. Therefore, first, we develop four counting functions for tree mappings according to the four classes. Secondly, we prove that three of the four counting functions are kernel functions, and the other is not.", "num_citations": "26\n", "authors": ["2096"]}
{"title": "Pairwise HITS: Quality estimation from pairwise comparisons in creator-evaluator crowdsourcing process\n", "abstract": " A common technique for improving the quality of crowdsourcing results is to assign a same task to multiple workers redundantly, and then to aggregate the results to obtain a higher-quality result; however, this technique is not applicable to complex tasks such as article writing since there is no obvious way to aggregate the results. Instead, we can use a two-stage procedure consisting of a creation stage and an evaluation stage, where we first ask workers to create artifacts, and then ask other workers to evaluate the artifacts to estimate their quality. In this study, we propose a novel quality estimation method for the two-stage procedure where pairwise comparison results for pairs of artifacts are collected at the evaluation stage. Our method is based on an extension of Kleinberg's HITS algorithm to pairwise comparison, which takes into account the ability of evaluators as well as the ability of creators. Experiments using actual crowdsourcing tasks show that our methods outperform baseline methods especially when the number of evaluators per artifact is small.", "num_citations": "24\n", "authors": ["2096"]}
{"title": "Self-measuring similarity for multi-task gaussian process\n", "abstract": " Multi-task learning aims at transferring knowledge between similar tasks. The multi-task Gaussian process framework of Bonilla et al. models (incomplete) responses of  data points for  tasks (eg, the responses are given by an  matrix) by using a Gaussian process; the covariance function takes its form as the product of a covariance function defined on input-specific features and an inter-task covariance matrix (which is empirically estimated as a model parameter). We extend this framework by incorporating a novel similarity measurement, which allows for the representation of much more complex data structures. The proposed framework also enables us to exploit additional information (eg, the input-specific features) when constructing the covariance matrices by combining additional information with the covariance function. We also derive an efficient learning algorithm which uses an iterative method to make predictions. Finally, we apply our model to a real data set of recommender systems and show that the proposed method achieves the best prediction accuracy on the data set.", "num_citations": "23\n", "authors": ["2096"]}
{"title": "Finding itemset-sharing patterns in a large itemset-associated graph\n", "abstract": " Itemset mining and graph mining have attracted considerable attention in the field of data mining, since they have many important applications in various areas such as biology, marketing, and social network analysis. However, most existing studies focus only on either itemset mining or graph mining, and only a few studies have addressed a combination of both. In this paper, we introduce a new problem which we call itemset-sharing subgraph (ISS) set enumeration, where the task is to find sets of subgraphs with common itemsets in a large graph in which each vertex has an associated itemset. The problem has various interesting potential applications such as in side-effect analysis in drug discovery and the analysis of the influence of word-of-mouth communication in marketing in social networks. We propose an efficient algorithm ROBIN for finding ISS sets in such graph; this algorithm enumerates\u00a0\u2026", "num_citations": "23\n", "authors": ["2096"]}
{"title": "Recent advances and trends in large-scale kernel methods\n", "abstract": " Kernel methods such as the support vector machine are one of the most successful algorithms in modern machine learning. Their advantage is that linear algorithms are extended to non-linear scenarios in a straightforward way by the use of the kernel trick. However, naive use of kernel methods is computationally expensive since the computational complexity typically scales cubically with respect to the number of training samples. In this article, we review recent advances in the kernel methods, with emphasis on scalability for massive problems.", "num_citations": "23\n", "authors": ["2096"]}
{"title": "Classification method of labeled ordered trees using support vector machines\n", "abstract": " To achieve classification of semistructured data with a Kernel method for labeled ordered trees, instances having a labeled ordered tree structure are input and their inner product is computed, the result of which is used for classification learning of the instances. In the inner product computation, a sum of matches is computed for descendant nodes of non-leaf nodes of the labeled ordered trees by applying dynamic programming based on correspondence in which order of the nodes is maintained.", "num_citations": "22\n", "authors": ["2096"]}
{"title": "Prediction of protein\u2013ligand binding affinities using multiple instance learning\n", "abstract": " Accurate prediction of protein\u2013ligand binding affinities for lead optimization in drug discovery remains an important and challenging problem on scoring functions for docking simulation. In this paper, we propose a data-driven approach that integrates multiple scoring functions to predict protein\u2013ligand binding affinity directly. We then propose a new method called multiple instance regression based scoring (MIRS) that incorporates unbound ligand conformations using multiple scoring functions. We evaluated the predictive performance of MIRS using 100 protein\u2013ligand complexes and their binding affinities. The experimental results showed that MIRS outperformed the 11 conventional scoring functions including LigScore, PLP, AutoDock, G-Score, D-Score, LUDI, F-Score, ChemScore, X-Score, PMF, and DrugScore. In addition, we confirmed that MIRS performed well on binding pose prediction. Our results reveal\u00a0\u2026", "num_citations": "21\n", "authors": ["2096"]}
{"title": "Predicting fuel consumption and flight delays for low-cost airlines\n", "abstract": " Low-cost airlines (LCAs) represent a new category of airlines that provides low-fare flights. The rise and growth of LCAs has intensified the price competition among airlines, and LCAs require continuous efforts to reduce their operating costs to lower flight prices; however, LCA passengers still demand high-quality services. A common measure of airline service quality is on-time departure performance. Be-cause LCAs apply efficient aircraft utilization and the time between flights is likely to be small, additional effort is required to avoid flight delays and improve their service quality. In this paper, we apply state-of-the-art predictive modeling approaches to real airline datasets and investigate the feasibility of machine learning methods for cost reduction and service quality improvement in LCAs. We address two prediction problems: fuel consumption prediction and flight delay prediction. We train predictive models using flight and passenger information, and our experiment results show that our regression model predicts the amount of fuel consumption more accurately than flight dispatchers, and our binary classifier achieves an area under the ROC curve (AUC) of 0.75 for predicting a delay of a specific flight route.", "num_citations": "20\n", "authors": ["2096"]}
{"title": "Instance-privacy preserving crowdsourcing\n", "abstract": " Crowdsourcing is a technique to outsource tasks to a number of workers. Although crowdsourcing has many advantages, it gives rise to the risk that sensitive information may be leaked, which has limited the spread of its popularity. Task instances (data workers receive to process tasks) often contain sensitive information, which can be extracted by workers. For example, in an audio transcription task, an audio file corresponds to an instance, and the content of the audio (eg, the abstract of a meeting) can be sensitive information. In this paper, we propose a quantitative analysis framework for the instance privacy problem. The proposed framework supplies us performance measures of instance privacy preserving protocols. As a case study, we apply the proposed framework to an instance clipping protocol and analyze the properties of the protocol. The protocol preserves privacy by clipping instances to limit the amount of information workers obtain. The results show that the protocol can balance task performance and instance privacy preservation. They also show that the proposed measure is consistent with standard measures, which validates the proposed measure.", "num_citations": "20\n", "authors": ["2096"]}
{"title": "Multinomial relation prediction in social data: A dimension reduction approach\n", "abstract": " The recent popularization of social web services has made them one of the primary uses of the World Wide Web. An important concept in social web services is social actions such as making connections and communicating with others and adding annotations to web resources. Predicting social actions would improve many fundamental web applications, such as recommendations and web searches. One remarkable characteristic of social actions is that they involve multiple and heterogeneous objects such as users, documents, keywords, and locations. However, the high-dimensional property of such multinomial relations poses one fundamental challenge, that is, predicting multinomial relations with only a limited amount of data. In this paper, we propose a new multinomial relation prediction method, which is robust to data sparsity. We transform each instance of a multinomial relation into a set of binomial relations between the objects and the multinomial relation of the involved objects. We then apply an extension of a low-dimensional embedding technique to these binomial relations, which results in a generalized eigenvalue problem guaranteeing global optimal solutions. We also incorporate attribute information as side information to address the \u201ccold start\u201d problem in multinomial relation prediction. Experiments with various real-world social web service datasets demonstrate that the proposed method is more robust against data sparseness as compared to several existing methods, which can only find sub-optimal solutions.", "num_citations": "20\n", "authors": ["2096"]}
{"title": "Risk-sensitive learning via minimization of empirical conditional value-at-risk\n", "abstract": " We extend the framework of cost-sensitive classification to mitigate risks of huge costs occurring with low probabilities, and propose an algorithm that achieves this goal. Instead of minimizing the expected cost commonly used in cost-sensitive learning, our algorithm minimizes conditional value-at-risk, also known as expected shortfall, which is considered a good risk metric in the area of financial engineering. The proposed algorithm is a general meta-learning algorithm that can exploit existing example-dependent cost-sensitive learning algorithms, and is capable of dealing with not only alternative actions in ordinary classification tasks, but also allocative actions in resource-allocation type tasks. Experiments on tasks with example-dependent costs show promising results.", "num_citations": "19\n", "authors": ["2096"]}
{"title": "Predicting disease progression from short biomarker series using expert advice algorithm\n", "abstract": " Well-trained clinicians may be able to provide diagnosis and prognosis from very short biomarker series using information and experience gained from previous patients. Although mathematical methods can potentially help clinicians to predict the progression of diseases, there is no method so far that estimates the patient state from very short time-series of a biomarker for making diagnosis and/or prognosis by employing the information of previous patients. Here, we propose a mathematical framework for integrating other patients' datasets to infer and predict the state of the disease in the current patient based on their short history. We extend a machine-learning framework of \u201cprediction with expert advice\u201d to deal with unstable dynamics. We construct this mathematical framework by combining expert advice with a mathematical model of prostate cancer. Our model predicted well the individual biomarker series of\u00a0\u2026", "num_citations": "18\n", "authors": ["2096"]}
{"title": "Crowdordering\n", "abstract": " Crowdsourcing is a promising solution to problems that are difficult for computers, but relatively easy for humans. One of the biggest challenges in crowdsourcing is quality control, since high quality results cannot be expected from crowdworkers who are not necessarily very capable or motivated. Several statistical crowdsourcing quality control methods for binary and multinomial questions have been proposed. In this paper, we consider tasks where crowdworkers are asked to arrange multiple items in the correct order. We propose a probabilistic generative model of crowd answers by extending a distance-based order model to incorporate worker ability, and propose an efficient estimation algorithm. Experiments using real crowdsourced datasets show the advantage of the proposed method over a baseline method.", "num_citations": "18\n", "authors": ["2096"]}
{"title": "Participation recommendation system for crowdsourcing contests\n", "abstract": " We propose a novel participation recommendation approach for crowdsourcing contests including probabilistic modeling of contest participation and winner determination. Our method estimates the winning and participation probability of each worker and offers ranked lists of recommended contests. Since there is only one winner in most contests, standard recommendation techniques fail to estimate the accurate winning probability using only the extremely sparse winning information of completed contests. Our solution is to utilize contest participation information and features of workers and contests as auxiliary information. We use the concept of a transfer learning method for matrices and a feature-based matrix factorization method. Experiments conducted using real crowdsourcing contest datasets show that the use of auxiliary information is crucial for improving the performance of contest recommendation, and\u00a0\u2026", "num_citations": "17\n", "authors": ["2096"]}
{"title": "Database search device, database search system, database search method, program and storage medium\n", "abstract": " A method is provided for allowing a database search that prevents an owner of the database or someone intercepting the database search in the network from knowing the contents of the query, without requiring any particular processing on the part of the database. Client 20 accesses database server 10 which stores sequence patterns and inquires whether the sequence patterns stored in the database server 10 exist in a predetermined sequence. Client 20 comprises sequence processing section 21 for dividing a search sequence subjected to search to create multiple subsequences; and query issuing section 22 for issuing a query to database server 10 using the created multiple subsequences individually as a query sequence.", "num_citations": "17\n", "authors": ["2096"]}
{"title": "Fast computation of subpath kernel for trees\n", "abstract": " The kernel method is a potential approach to analyzing structured data such as sequences, trees, and graphs; however, unordered trees have not been investigated extensively. Kimura et al. (2011) proposed a kernel function for unordered trees on the basis of their subpaths, which are vertical substructures of trees responsible for hierarchical information in them. Their kernel exhibits practically good performance in terms of accuracy and speed; however, linear-time computation is not guaranteed theoretically, unlike the case of the other unordered tree kernel proposed by Vishwanathan and Smola (2003). In this paper, we propose a theoretically guaranteed linear-time kernel computation algorithm that is practically fast, and we present an efficient prediction algorithm whose running time depends only on the size of the input tree. Experimental results show that the proposed algorithms are quite efficient in practice.", "num_citations": "16\n", "authors": ["2096"]}
{"title": "Graph kernels for chemoinformatics\n", "abstract": " The authors review graph kernels which is one of the state-of-the-art approaches using machine learning techniques for computational predictive modeling in chemoinformatics. The authors introduce a random walk graph kernel that defines a similarity between arbitrary two labeled graphs based on label sequences generated by random walks on the graphs. They introduce two applications of the graph kernels, the prediction of properties of chemical compounds and prediction of missing enzymes in metabolic networks. In the latter application, the authors propose to use the random walk graph kernel to compare arbitrary two chemical reactions, and apply it to plant secondary metabolism.", "num_citations": "16\n", "authors": ["2096"]}
{"title": "Integration of multiple networks for robust label propagation\n", "abstract": " Transductive inference on graphs such as label propagation algorithms is receiving a lot of attention. In this paper, we address a label propagation problem on multiple networks and present a new algorithm that automatically integrates structure information brought in by multiple networks. The proposed method is robust in that irrelevant networks are automatically deemphasized, which is an advantage over Tsuda et al.'s approach [14]. We also show that the proposed algorithm can be interpreted as an EM algorithm with a Student-t prior. Finally, we demonstrate the usefulness of our method in protein function prediction.", "num_citations": "16\n", "authors": ["2096"]}
{"title": "Electronic contents proving method and system, and storage medium for storing program therefor\n", "abstract": " The fact that electronic content on a network has been opened for perusal by the public is proven, and the probative force required to demonstrate the openness or the lack of alteration of electronic content can be increased. Upon the receipt of a service request from a user who desires to prove that electronic content has been opened for perusal by the public, a service provider preferably selects, from a registered member group, multiple witnesses or certificate generators, and issues certificate generation requests to the selected witnesses or certificate generators. Electronic signatures of the witnesses or the certificate generators are provided for the certificates, to each of which the service provider adds his or her electronic signature, and the certificates are transmitted to the user.", "num_citations": "16\n", "authors": ["2096"]}
{"title": "Location estimation system, method and program\n", "abstract": " A location estimation method using label propagation. The achieved location estimation method is robust to variations in radio signal strengths and is highly accurate by using the q-norm (0< q< 1), especially, for calculating the similarities among radio signal strength vectors. The accuracy in location estimation is further improved by putting more importance on the time-series similarities. Specifically, the time-series similarity is calculated by using time-series values indicating the temporal order of radio signal strengths during the measurement. If the time-series similarity is larger than the similarity between the radio signal strength vectors, the time-series similarity is preferentially used. The exponential attenuation function can also be used for calculating the similarities, instead of the q norm (0< q< 1).", "num_citations": "15\n", "authors": ["2096"]}
{"title": "Averaged stochastic gradient descent with feedback: An accurate, robust, and fast training method\n", "abstract": " On large datasets, the popular training approach has been stochastic gradient descent (SGD). This paper proposes a modification of SGD, called averaged SGD with feedback (ASF), that significantly improves the performance (robustness, accuracy, and training speed) over the traditional SGD. The proposal is based on three simple ideas: averaging the weight vectors across SGD iterations, feeding the averaged weights back into the SGD update process, and deciding when to perform the feedback (linearly slowing down feedback). Theoretically, we demonstrate the reasonable convergence properties of the ASF. Empirically, the ASF outperforms several strong baselines in terms of accuracy, robustness over the noise, and the training speed. To our knowledge, this is the first study of ``feedback'' in stochastic gradient learning. Although we choose latent conditional models for verifying the ASF in this paper, the\u00a0\u2026", "num_citations": "15\n", "authors": ["2096"]}
{"title": "Quality control of crowdsourced classification using hierarchical class structures\n", "abstract": " Crowdsourcing is an emerging approach to utilize a large pool of human workers and execute various intelligent tasks. Repeated labeling is a widely adopted quality control method in crowdsourcing. This method is based on selecting one reliable label from multiple labels collected by workers because a single label from only one worker has a wide variance of accuracy. Hierarchical classification, where each class has a hierarchical relationship, is a typical task in crowdsourcing and used to organize information in many knowledge systems. However, direct applications of existing methods designed for multi-class classification have the disadvantage of discriminating among a large number of classes. In this paper, we propose a label aggregation method for hierarchical classification tasks. Our method takes the hierarchical structure into account to handle a large number of classes and estimate worker abilities\u00a0\u2026", "num_citations": "14\n", "authors": ["2096"]}
{"title": "Reaction graph kernels predict EC numbers of unknown enzymatic reactions in plant secondary metabolism\n", "abstract": " Understanding of secondary metabolic pathway in plant is essential for finding druggable candidate enzymes. However, there are many enzymes whose functions are not yet discovered in organism-specific metabolic pathways. Towards identifying the functions of those enzymes, assignment of EC numbers to the enzymatic reactions they catalyze plays a key role, since EC numbers represent the categorization of enzymes on one hand, and the categorization of enzymatic reactions on the other hand. We propose reaction graph kernels for automatically assigning EC numbers to unknown enzymatic reactions in a metabolic network. Reaction graph kernels compute similarity between two chemical reactions considering the similarity of chemical compounds in reaction and their relationships. In computational experiments based on the KEGG/REACTION database, our method successfully predicted the first three digits of the EC number with 83% accuracy. We also exhaustively predicted missing EC numbers in plant's secondary metabolism pathway. The prediction results of reaction graph kernels on 36 unknown enzymatic reactions are compared with an expert's knowledge. Using the same data for evaluation, we compared our method with E-zyme, and showed its ability to assign more number of accurate EC numbers. Reaction graph kernels are a new metric for comparing enzymatic reactions.", "num_citations": "14\n", "authors": ["2096"]}
{"title": "Mining significant pairs of patterns from graph structures with class labels\n", "abstract": " In recent years, the problem of mining association rules over frequent itemsets in transactional data has been frequently studied and yielded several algorithms that can find association rules within a limited amount of time. Also more complex patterns have been considered such as ordered trees, unordered trees, or labeled graphs. Although some approaches can efficiently derive all frequent subgraphs from a massive dataset of graphs, a subgraph or subtree that is mathematically defined is not necessarily a better knowledge representation. We propose an efficient approach to discover significant rules to classify positive and negative graph examples by estimating a tight upper bound on the statistical metric. This approach abandons unimportant rules earlier in the computations, and thereby accelerates the overall performance. The performance has been evaluated using real world datasets, and the efficiency\u00a0\u2026", "num_citations": "13\n", "authors": ["2096"]}
{"title": "Fast sparse group lasso\n", "abstract": " Sparse Group Lasso is a method of linear regression analysis that finds sparse parameters in terms of both feature groups and individual features. Block Coordinate Descent is a standard approach to obtain the parameters of Sparse Group Lasso, and iteratively updates the parameters for each parameter group. However, as an update of only one parameter group depends on all the parameter groups or data points, the computation cost is high when the number of the parameters or data points is large. This paper proposes a fast Block Coordinate Descent for Sparse Group Lasso. It efficiently skips the updates of the groups whose parameters must be zeros by using the parameters in one group. In addition, it preferentially updates parameters in a candidate group set, which contains groups whose parameters must not be zeros. Theoretically, our approach guarantees the same results as the original Block Coordinate Descent. Experiments show that our method reduces the processing time by up to 97 percent from that of the original method and achieves the same prediction error.", "num_citations": "12\n", "authors": ["2096"]}
{"title": "Hyper questions: Unsupervised targeting of a few experts in crowdsourcing\n", "abstract": " Quality control is one of the major problems in crowdsourcing. One of the primary approaches to rectify this issue is to assign the same task to different workers and then aggregate their answers to obtain a reliable answer. In addition to simple aggregation approaches such as majority voting, various sophisticated probabilistic models have been proposed. However, given that most of the existing methods operate by strengthening the opinions of the majority, these models often fail when the tasks require highly specialized knowledge and the ability of a large majority of the workers is inadequate. In this paper, we focus on an important class of answer aggregation problems in which majority voting fails and propose the concept of hyper questions to devise effective aggregation methods. A hyper question is a set of single questions, and our key idea is that experts are more likely to provide correct answers to all of the\u00a0\u2026", "num_citations": "12\n", "authors": ["2096"]}
{"title": "Learning implicit tasks for patient-specific risk modeling in ICU\n", "abstract": " Accurate assessment of the severity of a patient\u2019s condition plays a fundamental role in acute hospital care such as that provided in an intensive care unit (ICU). ICU clinicians are required to make sense of a large amount of clinical data in a limited time to estimate the severity of a patient\u2019s condition, which ultimately leads to the planning of appropriate care. The ICU is an especially demanding environment for clinicians because of the diversity of patients who mostly suffer from multiple diseases of various types. In this paper, we propose a mortality risk prediction method for ICU patients. The method is intended to enhance the severity assessment by considering the diversity of patients. Our method produces patient-specific risk models that reflect the collection of diseases associated with the patient. Specifically, we assume a small number of latent basis tasks, where each latent task is associated with its own parameter vector; a parameter vector for a specific patient is constructed as a linear combination of these. The latent representation of a patient, namely, the coefficients of the combination, is learned based on the collection of diseases associated with the patient. Our method could be considered a multi-task learning method where latent tasks are learned based on the collection of diseases. We demonstrate the effectiveness of our proposed method using a dataset collected from a hospital. Our method achieved higher predictive performance compared with a single-task learning method, the \u201cde facto standard,\u201d and several multi-task learning methods including a recently proposed method for ICU mortality risk prediction. Furthermore, our\u00a0\u2026", "num_citations": "12\n", "authors": ["2096"]}
{"title": "Network-based problem detection for distributed systems\n", "abstract": " We introduce a network-based problem detection framework for distributed systems, which includes a data-mining method for discovering dynamic dependencies among distributed services from transaction data collected from network, and a novel problem detection method based on the discovered dependencies. From observed containments of transaction execution time periods, we estimate the probabilities of accidental and non-accidental containments, and build a competitive model for discovering direct dependencies by using a model estimation method based on the online EM algorithm. Utilizing the discovered dependency information, we also propose a hierarchical problem detection framework, where microscopic dependency information is incorporated with a macroscopic anomaly metric that monitors the behavior of the system as a whole. This feature is made possible by employing a network-based\u00a0\u2026", "num_citations": "12\n", "authors": ["2096"]}
{"title": "Dual graph convolutional neural network for predicting chemical networks\n", "abstract": " Predicting of chemical compounds is one of the fundamental tasks in bioinformatics and chemoinformatics, because it contributes to various applications in metabolic engineering and drug discovery. The recent rapid growth of the amount of available data has enabled applications of computational approaches such as statistical modeling and machine learning method. Both a set of chemical interactions and chemical compound structures are represented as graphs, and various graph-based approaches including graph convolutional neural networks have been successfully applied to chemical network prediction. However, there was no efficient method that can consider the two different types of graphs in an end-to-end manner. We give a new formulation of the chemical network prediction problem as a link prediction problem in a graph of graphs (GoG) which can represent the hierarchical structure consisting of\u00a0\u2026", "num_citations": "11\n", "authors": ["2096"]}
{"title": "Short-term precipitation prediction with skip-connected prednet\n", "abstract": " Short-term forecasting of rainfall in a local area is called precipitation nowcasting, and it has been traditionally addressed using rule-based or numerical approaches. Recently, deep neural network models have started to be used for precipitation nowcasting; however, their utility has not been extensively explored yet. Especially, the existing efforts focus only on the choice of their building blocks and pay little attention to the design of the whole network structure. In this paper, we propose a new precipitation nowcasting model based on the PredNet network architecture, which was originally proposed for short-term video prediction tasks. The proposed model outperforms the state-of-the-art models in the MovingMNIST++\u2009dataset in terms of MSE, and it also shows a good predictive performance on a real dataset of precipitation in Kyoto City.", "num_citations": "11\n", "authors": ["2096"]}
{"title": "Incorporating worker similarity for label aggregation in crowdsourcing\n", "abstract": " For the quality control in the crowdsourcing tasks, requesters usually assign a task to multiple workers to obtain redundant answers and then aggregate them to obtain the more reliable answer. Because of the existence of the non-experts in the crowds, one of the problems in the label aggregation is how to differ experts with higher ability from non-experts with lower ability and strengthen the influences of these experts. Most of the existing label aggregation approaches tend to strengthen the workers who provide majority answers and regard them with high ability. In addition, we find that the similarity among worker labels is possible to be effective for this issue because two experts are more probable to reach consensus than two non-experts. We thus propose a novel probabilistic model which can incorporate the similarity information of workers. The experimental results on a number of real datasets show\u00a0\u2026", "num_citations": "11\n", "authors": ["2096"]}
{"title": "Using posters to recommend anime and mangas in a cold-start scenario\n", "abstract": " Item cold-start is a classical issue in recommender systems that affects anime and manga recommendations as well. This problem can be framed as follows: how to predict whether a user will like a manga that received few ratings from the community? Content-based techniques can alleviate this issue but require extra information, that is usually expensive to gather. In this paper, we use a deep learning technique, Illustration2Vec, to easily extract tag information from the manga and anime posters (e.g., sword, or ponytail). We propose BALSE (Blended Alternate Least Squares with Explanation), a new model for collaborative filtering, that benefits from this extra information to recommend mangas. We show, using real data from an online manga recommender system called Mangaki, that our model improves substantially the quality of recommendations, especially for less-known manga, and is able to provide an\u00a0\u2026", "num_citations": "11\n", "authors": ["2096"]}
{"title": "A new multi-task learning method for personalized activity recognition\n", "abstract": " Personalized activity recognition usually faces the problem of data sparseness. We aim at improving accuracy of personalized activity recognition by incorporating the information from other persons. We propose a new online multi-task learning method for personalized activity recognition. The proposed online multi-task learning method automatically learns the ``transfer-factors\" (similarities) among different tasks (i.e., among different persons in our case). Experiments demonstrate that the proposed method significantly outperforms existing methods. The novelty of this paper is twofold: (1) A new multi-task learning framework, which can naturally learn similarities among tasks, (2) To our knowledge, this is the first study of large-scale personalized activity recognition.", "num_citations": "11\n", "authors": ["2096"]}
{"title": "A transfer learning approach and selective integration of multiple types of assays for biological network inference\n", "abstract": " Inferring the relationship among proteins is a central issue of computational biology and a diversity of biological assays are utilized to predict the relationship. However, as experiments are usually expensive to perform, automatic data selection is employed to reduce the data collection cost. Although data useful for link prediction are different in each local sub-network, existing methods cannot select different data for different processes. This article presents a new algorithm for inferring biological networks from multiple types of assays. The proposed algorithm is based on transfer learning and can exploit local information effectively. Each assay is automatically weighted through learning and the weights can be adaptively different in each local part. The authors\u2019 algorithm was favorably examined on two kinds of biological networks: a metabolic network and a protein interaction network. A statistical test confirmed that\u00a0\u2026", "num_citations": "11\n", "authors": ["2096"]}
{"title": "Fast Unbalanced Optimal Transport on a Tree\n", "abstract": " This study examines the time complexities of the unbalanced optimal transport problems from an algorithmic perspective for the first time. We reveal which problems in unbalanced optimal transport can/cannot be solved efficiently. Specifically, we prove that the Kantorovich Rubinstein distance and optimal partial transport in the Euclidean metric cannot be computed in strongly subquadratic time under the strong exponential time hypothesis. Then, we propose an algorithm that solves a more general unbalanced optimal transport problem exactly in quasi-linear time on a tree metric. The proposed algorithm processes a tree with one million nodes in less than one second. Our analysis forms a foundation for the theoretical study of unbalanced optimal transport algorithms and opens the door to the applications of unbalanced optimal transport to million-scale datasets.", "num_citations": "10\n", "authors": ["2096"]}
{"title": "Simultaneous Clustering and Ranking from Pairwise Comparisons.\n", "abstract": " When people make decisions with a number of ideas, designs, or other kinds of objects, one attempt is probably to organize them into several groups of objects and to prioritize them according to some preference. The grouping task is referred to as clustering and the prioritizing task is called as ranking. These tasks are often outsourced with the help of human judgments in the form of pairwise comparisons. Two objects are compared on whether they are similar in the clustering problem, while the object of higher priority is determined in the ranking problem. Our research question in this paper is whether the pairwise comparisons for clustering also help ranking (and vice versa). Instead of solving the two tasks separately, we propose a unified formulation to bridge the two types of pairwise comparisons. Our formulation simultaneously estimates the object embeddings and the preference criterion vector. The experiments using real datasets support our hypothesis; our approach can generate better neighbor and preference estimation results than the approaches that only focus on a single type of pairwise comparisons.", "num_citations": "10\n", "authors": ["2096"]}
{"title": "Cartesian kernel: an efficient alternative to the pairwise kernel\n", "abstract": " Pairwise classification has many applications including network prediction, entity resolution, and collaborative filtering. The pairwise kernel has been proposed for those purposes by several research groups independently, and has been used successfully in several fields. In this paper, we propose an efficient alternative which we call a Cartesian kernel. While the existing pairwise kernel (which we refer to as the Kronecker kernel) can be interpreted as the weighted adjacency matrix of the Kronecker product graph of two graphs, the Cartesian kernel can be interpreted as that of the Cartesian graph, which is more sparse than the Kronecker product graph. We discuss the generalization bounds of the two pairwise kernels by using eigenvalue analysis of the kernel matrices. Also, we consider the N-wise extensions of the two pairwise kernels. Experimental results show the Cartesian kernel is much faster than the Kronecker kernel, and at the same time, competitive with the Kronecker kernel in predictive performance.", "num_citations": "10\n", "authors": ["2096"]}
{"title": "Classification factor detection\n", "abstract": " Detects a condition for classification of data. Apparatus detects a set of some constituents as a factor of the classification. Apparatus has means for selecting a pattern which is a set of constituents; means of selecting a second pattern formed of the first pattern and at least one constituent added to the first pattern; means of generating an evaluation value for a measure of classification of the plurality of objects under a condition including the first pattern but not the second pattern on the basis of the number of objects satisfying the classification condition in the plurality of objects classified into the first group and the number of objects satisfying the classification condition in the objects classified into the second group; and means of outputting the first and second patterns as a factor of classification when the measure indicated by the evaluation value exceeds a reference measure.", "num_citations": "10\n", "authors": ["2096"]}
{"title": "Machine learning approaches for structured data\n", "abstract": " \u535a \u58eb (\u60c5\u5831\u5b66) \u60c5 \u535a \u7b2c 240 \u53f7\u5e73\u6210 19 \u5e74 3 \u6708 23 \u65e5\u5b66\u4f4d\u898f\u5247\u7b2c 4 \u6761\u7b2c 1 \u9805\u8a72\u5f53\u60c5\u5831\u5b66\u7814\u7a76\u79d1\u77e5\u80fd\u60c5\u5831\u5b66\u5c02\u653b Machine Learning Approaches for Structured Data (\u69cb\u9020\u30c7\u30fc\u30bf\u89e3\u6790\u306e\u305f\u3081\u306e\u6a5f\u68b0\u5b66\u7fd2\u6cd5)", "num_citations": "10\n", "authors": ["2096"]}
{"title": "Dual convolutional neural network for graph of graphs link prediction\n", "abstract": " Graphs are general and powerful data representations which can model complex real-world phenomena, ranging from chemical compounds to social networks; however, effective feature extraction from graphs is not a trivial task, and much work has been done in the field of machine learning and data mining. The recent advances in graph neural networks have made automatic and flexible feature extraction from graphs possible and have improved the predictive performance significantly. In this paper, we go further with this line of research and address a more general problem of learning with a graph of graphs (GoG) consisting of an external graph and internal graphs, where each node in the external graph has an internal graph structure. We propose a dual convolutional neural network that extracts node representations by combining the external and internal graph structures in an end-to-end manner. Experiments on link prediction tasks using several chemical network datasets demonstrate the effectiveness of the proposed method.", "num_citations": "9\n", "authors": ["2096"]}
{"title": "Quality control for crowdsourced hierarchical classification\n", "abstract": " Repeated labeling is a widely adopted quality control method in crowdsourcing. This method is based on selecting one reliable label from multiple labels collected by workers because a single label from only one worker has a wide variance of accuracy. Hierarchical classification, where each class has a hierarchical relationship, is a typical task in crowdsourcing. However, direct applications of existing methods designed for multi-class classification have the disadvantage of discriminating among a large number of classes. In this paper, we propose a label aggregation method for hierarchical classification tasks. Our method takes the hierarchical structure into account to handle a large number of classes and estimate worker abilities more precisely. Our method is inspired by the steps model based on item response theory, which models responses of examinees to sequentially dependent questions. We considered\u00a0\u2026", "num_citations": "9\n", "authors": ["2096"]}
{"title": "Learning an accurate entity resolution model from crowdsourced labels\n", "abstract": " We investigated the use of supervised learning methods that use labels from crowd workers to resolve entities. Although obtaining labeled data by crowdsourcing can reduce time and cost, it also brings challenges (eg, coping with the variable quality of crowd-generated data). First, we evaluated the quality of crowd-generated labels for actual entity resolution data sets. Then, we evaluated the prediction accuracy of two machine learning methods that use labels from crowd workers: a conventional LPP method using consensus labels obtained by majority voting and our proposed method that combines multiple Laplacians directly by using crowdsourced data. We discussed the relationship between the accuracy of workers' labels and the prediction accuracy of the two methods.", "num_citations": "9\n", "authors": ["2096"]}
{"title": "Auction method and auction system, and storage medium therefor\n", "abstract": " A solution is provided for the problem of determining a successful bidder in an auction wherein bids are submitted for a great number of trading targets (products). Two restrictions are imposed on the combinations of products that are permitted: 1. Bids may be submitted for only one product type; and 2. The relationship existing among available combinations of products for which bids may be submitted must be based on a hierarchical structure. From a set of bids submitted under the above restrictions, a subset of optimum bids are selected by using dynamic programming.", "num_citations": "9\n", "authors": ["2096"]}
{"title": "Distributed multi-task learning for sensor network\n", "abstract": " A sensor in a sensor network is expected to be able to make prediction or decision utilizing the models learned from the data observed on this sensor. However, in the early stage of using a sensor, there may be not a lot of data available to train the model for this sensor. A solution is to leverage the observation data from other sensors which have similar conditions and models with the given sensor. We thus propose a novel distributed multi-task learning approach which incorporates neighborhood relations among sensors to learn multiple models simultaneously in which each sensor corresponds to one task. It may be not cheap for each sensor to transfer the observation data from other sensors; broadcasting the observation data of a sensor in the entire network is not satisfied for the reason of privacy protection; each sensor is expected to make real-time prediction independently from neighbor sensors\u00a0\u2026", "num_citations": "8\n", "authors": ["2096"]}
{"title": "Predictive approaches for low-cost preventive medicine program in developing countries\n", "abstract": " Non-communicable diseases (NCDs) are no longer just a problem for high-income countries, but they are also a problem that affects developing countries. Preventive medicine is definitely the key to combat NCDs; however, the cost of preventive programs is a critical issue affecting the popularization of these medicine programs in developing countries. In this study, we investigate predictive modeling for providing a low-cost preventive medicine program. In our two-year-long field study in Bangladesh, we collected the health checkup results of 15,075 subjects, the data of 6,607 prescriptions, and the follow-up examination results of 2,109 subjects. We address three prediction problems, namely subject risk prediction, drug recommendation, and future risk prediction, by using machine learning techniques; our multiple-classifier approach successfully reduced the costs of health checkups, a multi-task learning method\u00a0\u2026", "num_citations": "8\n", "authors": ["2096"]}
{"title": "Leveraging crowdsourcing to detect improper tasks in crowdsourcing marketplaces\n", "abstract": " Controlling the quality of tasks is a major challenge in crowdsourcing marketplaces. Most of the existing crowdsourcing services prohibit requesters from posting illegal or objectionable tasks. Operators in the marketplaces have to monitor the tasks continuously to find such improper tasks; however, it is too expensive to manually investigate each task. In this paper, we present the reports of our trial study on automatic detection of improper tasks to support the monitoring of activities by marketplace operators. We perform experiments using real task data from a commercial crowdsourcing marketplace and show that the classifier trained by the operator judgments achieves high accuracy in detecting improper tasks. In addition, to reduce the annotation costs of the operator and improve the classification accuracy, we consider the use of crowdsourcing for task annotation. We hire a group of crowdsourcing (non-expert) workers to monitor posted tasks, and incorporate their judgments into the training data of the classifier. By applying quality control techniques to handle the variability in worker reliability, our results show that the use of non-expert judgments by crowdsourcing workers in combination with expert judgments improves the accuracy of detecting improper crowdsourcing tasks.", "num_citations": "8\n", "authors": ["2096"]}
{"title": "Computer operation analysis\n", "abstract": " Provides an analysis system for analyzing dependencies among server programs in a computer system, comprising: a transaction detection unit for detecting transactions that is processing of a service which each of the plurality of server programs performs by being called from any other server program; a child transaction candidate detection unit for detecting candidates for a child transaction of each detected transaction, the child transaction being called in the transaction concerned; and a calling frequency calculation unit for outputting service calling frequencies obtained by estimating a frequency at which each server program allows each of the other server programs to perform a service by calling the other program, the estimation being made based on the service detected in association with each transaction and on a service detected in association with the candidate for the child transaction of the transaction.", "num_citations": "8\n", "authors": ["2096"]}
{"title": "Quality control for crowdsourced POI collection\n", "abstract": " Crowdsourcing allows human intelligence tasks to be outsourced to a large number of unspecified people at low costs. However, because of the uneven ability and diligence of crowd workers, the quality of their submitted work is also uneven and sometimes quite low. Therefore, quality control is one of the central issues in crowdsourcing research. In this paper, we consider a quality control problem of POI (points of interest) collection tasks, in which workers are asked to enumerate location information of POIs. Since workers neither necessarily provide correct answers nor provide exactly the same answers even if the answers indicate the same place, we propose a two-stage quality control method consisting of an answer clustering stage and a reliability estimation stage. Implemented with a new constrained exemplar clustering and a modified HITS algorithm, the effectiveness of our method is demonstrated\u00a0\u2026", "num_citations": "7\n", "authors": ["2096"]}
{"title": "Large scale real-life action recognition using conditional random fields with stochastic training\n", "abstract": " Action recognition is usually studied with limited lab settings and a small data set. Traditional lab settings assume that the start and the end of each action are known. However, this is not true for the real-life activity recognition, where different actions are present in a continuous temporal sequence, with their boundaries unknown to the recognizer. Also, unlike previous attempts, our study is based on a large-scale data set collected from real world activities. The novelty of this paper is twofold: (1) Large-scale non-boundary action recognition; (2) The first application of the averaged stochastic gradient training with feedback (ASF) to conditional random fields. We find the ASF training method outperforms a variety of traditional training methods in this task.", "num_citations": "7\n", "authors": ["2096"]}
{"title": "Least absolute policy iteration for robust value function approximation\n", "abstract": " Least-squares policy iteration is a useful reinforcement learning method in robotics due to its computational efficiency. However, it tends to be sensitive to outliers in observed rewards. In this paper, we propose an alternative method that employs the absolute loss for enhancing robustness and reliability. The proposed method is formulated as a linear programming problem which can be solved efficiently by standard optimization software, so the computational advantage is not sacrificed for gaining robustness and reliability. We demonstrate the usefulness of the proposed approach through simulated robot-control tasks.", "num_citations": "7\n", "authors": ["2096"]}
{"title": "Probabilistic Modeling of Peer Correction and Peer Assessment.\n", "abstract": " Peer assessment is a promising solution for scaling up the grading of a large number of submissions. The reliability of evaluations is one of the critical issues in peer assessment; several probabilistic models have been proposed for obtaining reliable grades from peers. Peer correction is a similar framework, in which students are instructed to correct the errors in submissions from other students. Peer correction is typically performed simultaneously with peer assessment; a reviewer is instructed to correct the errors in a submission and to provide a grade to it. We observe the occasional inconsistency between a grade and the correction; for example, a reviewer provides a high grade for a submission but she corrects many errors in it. Such inconsistencies can point to unreliable reviewers. In this paper, we propose probabilistic models for peer correction, and the combination of the peer correction models and the existing peer assessment models for capturing the inconsistency to accurately estimate the reviewer reliability and the student ability. We conduct experiments using the dataset of an actual peer correction platform for language translation, and the results demonstrate that the combination of peer correction models and peer assessment models improves the accuracy of the student ability estimation.", "num_citations": "6\n", "authors": ["2096"]}
{"title": "BayesGrad: Explaining predictions of graph convolutional networks\n", "abstract": " Recent advances in graph convolutional networks have significantly improved the performance of chemical predictions, raising a new research question: \u201chow do we explain the predictions of graph convolutional networks?\u201d A possible approach to answer this question is to visualize evidence substructures responsible for the predictions. For chemical property prediction tasks, the sample size of the training data is often small and/or a label imbalance problem occurs, where a few samples belong to a single class and the majority of samples belong to the other classes. This can lead to uncertainty related to the learned parameters of the machine learning model. To address this uncertainty, we propose BayesGrad, utilizing the Bayesian predictive distribution, to define the importance of each node in an input graph, which is computed efficiently using the dropout technique. We demonstrate that BayesGrad\u00a0\u2026", "num_citations": "6\n", "authors": ["2096"]}
{"title": "Predictive modeling of learning continuation in preschool education using temporal patterns of development tests\n", "abstract": " Learning analytics applies data analysis techniques to learning data in order to support students\u2019 learning processes and to improve the quality of education. Despite the increasing attention to learning analytics for higher education, it has not been fully addressed in primary and preschool education. In this research, we apply learning analytics to preschool education to predict the continuation of learning of preschool children. Based on our hypothesis that temporal patterns in the assessment scores of development tests are effective features for prediction, we extract the temporal patterns using time-series clustering, and use them as the features of prediction models. The experimental results using a real preschool education dataset show that the use of the temporal patterns improves the predictive accuracy of future continuation of study.", "num_citations": "6\n", "authors": ["2096"]}
{"title": "Adaflock: Adaptive feature discovery for human-in-the-loop predictive modeling\n", "abstract": " Feature engineering is the key to successful application of machine learning algorithms to real-world data. The discovery of informative features often requires domain knowledge or human inspiration, and data scientists expend a certain amount of effort into exploring feature spaces. Crowdsourcing is considered a promising approach for allowing many people to be involved in feature engineering; however, there is a demand for a sophisticated strategy that enables us to acquire good features at a reasonable crowdsourcing cost. In this paper, we present a novel algorithm called AdaFlock to efficiently obtain informative features through crowdsourcing. AdaFlock is inspired by AdaBoost, which iteratively trains classifiers by increasing the weights of samples misclassified by previous classifiers. AdaFlock iteratively generates informative features; at each iteration of AdaFlock, crowdsourcing workers are shown samples selected according to the classification errors of the current classifiers and are asked to generate new features that are helpful for correctly classifying the given examples. The results of our experiments conducted using real datasets indicate that AdaFlock successfully discovers informative features with fewer iterations and achieves high classification accuracy.", "num_citations": "6\n", "authors": ["2096"]}
{"title": "Crowdsourcing chart digitizer: task design and quality control for making legacy open data machine-readable\n", "abstract": " Despite recent open data initiatives in many countries, a significant percentage of the data provided is in non-machine-readable formats like image format rather than in a machine-readable electronic format, thereby restricting their usability. Various types of software for digitizing data chart images have been developed. However, such software is designed for manual use and thus requires human intervention, making it unsuitable for automatically extracting data from a large number of chart images. This paper describes the first unified framework for converting legacy open data in chart images into a machine-readable and reusable format by using crowdsourcing. Crowd workers are asked not only to extract data from an image of a chart but also to reproduce the chart objects in a spreadsheet. The properties of the reproduced chart objects give their data structures, including series names and values, which\u00a0\u2026", "num_citations": "6\n", "authors": ["2096"]}
{"title": "Crowdsourced data analytics: A case study of a predictive modeling competition\n", "abstract": " Predictive modeling competitions provide a new data mining approach that leverages crowds of data scientists to examine a wide variety of predictive models and build the best performance model. In this paper, we report the results of a study conducted on CrowdSolving, a platform for predictive modeling competitions in Japan. We hosted a competition on a link prediction task and observed that (i) the prediction performance of the winner significantly outperformed that of a state-of-the-art method,(ii) the aggregated model constructed from all submitted models further improved the final performance, and (iii) the performance of the aggregated model built only from early submissions nevertheless overtook the final performance of the winner.", "num_citations": "6\n", "authors": ["2096"]}
{"title": "Link prediction across time via cross-temporal locality preserving projections\n", "abstract": " Link prediction is the task of inferring the existence or absence of certain relationships among data objects such as identity, interaction, and collaboration. Link prediction is found in various applications in the fields of information integration, recommender systems, bioinformatics, and social network analysis. The increasing interest in dynamically changing networks has led to growing interest in a more general link prediction problem called temporal link prediction in the data mining and machine learning communities. However, only links among nodes at the same time point are considered in temporal link prediction. We propose a new link prediction problem called cross-temporal link prediction in which the links among nodes at different time points are inferred. A typical example of cross-temporal link prediction is cross-temporal entity resolution to determine the identity of real entities represented by data objects\u00a0\u2026", "num_citations": "6\n", "authors": ["2096"]}
{"title": "Fast Newton-CG method for batch learning of conditional random fields\n", "abstract": " We propose a fast batch learning method for linear-chain Conditional Random Fields (CRFs) based on Newton-CG methods. Newton-CG methods are a variant of Newton method for high-dimensional problems. They only require the Hessian-vector products instead of the full Hessian matrices. To speed up Newton-CG methods for the CRF learning, we derive a novel dynamic programming procedure for the Hessian-vector products of the CRF objective function. The proposed procedure can reuse the byproducts of the time-consuming gradient computation for the Hessian-vector products to drastically reduce the total computation time of the Newton-CG methods. In experiments with tasks in natural language processing, the proposed method outperforms a conventional quasi-Newton method. Remarkably, the proposed method is competitive with online learning algorithms that are fast but unstable.", "num_citations": "6\n", "authors": ["2096"]}
{"title": "CrowNN: Human-in-the-loop Network with Crowd-generated Inputs\n", "abstract": " Input features are indispensable for almost all machine learning methods; however, their definitions themselves are sometimes too abstract to extract automatically. Human-in-the-loop machine learning is a promising solution to such cases where humans extract the feature values for machine learning models. We use crowdsourcing for feature value extraction and consider a problem to aggregate the feature values to improve machine learning classifiers. We propose a novel neural network model called CROWNN, a neural network with crowd-generated inputs with the worker convolution layer, that learns both the capabilities of human feature extractors and the weights of a neural network classifier by applying the idea of the convolution neural network to feature aggregation. Our experiments using four datasets show the proposed method outperforms the baseline method using unsupervised aggregation\u00a0\u2026", "num_citations": "5\n", "authors": ["2096"]}
{"title": "Wisdom of crowds for synthetic accessibility evaluation\n", "abstract": " Synthetic accessibility evaluation is a process to assess the ease of synthesis of compounds. A rapid method for the assessment of synthetic accessibility for a vast number of chemical compounds is expected to bring about a breakthrough in the drug discovery. Although several computational methods have been proposed, the compound evaluation has still been processed by medicinal chemists; however, the low throughput of the human evaluation due to the lack of chemists is a critical issue for handling a large number of compounds. We propose the use of crowdsourcing for addressing this problem, and we conducted experiments to investigate the feasibility of incorporating semi-experts and a statistical aggregation method into the synthetic accessibility evaluation. Our experimental results show that we can obtain accurate synthetic accessibility scores through the statistical aggregation of judgments from semi\u00a0\u2026", "num_citations": "5\n", "authors": ["2096"]}
{"title": "A generalized model for multidimensional intransitivity\n", "abstract": " Intransitivity is a critical issue in pairwise preference modeling. It refers to the intransitive pairwise preferences between a group of players or objects that potentially form a cyclic preference chain, and has been long discussed in social choice theory in the context of the dominance relationship. However, such multifaceted intransitivity between players and the corresponding player representations in high dimension are difficult to capture. In this paper, we propose a probabilistic model that joint learns the d-dimensional representation () for each player and a dataset-specific metric space that systematically captures the distance metric in  over the embedding space. Interestingly, by imposing additional constraints in the metric space, our proposed model degenerates to former models used in intransitive representation learning. Moreover, we present an extensive quantitative investigation of the wide\u00a0\u2026", "num_citations": "5\n", "authors": ["2096"]}
{"title": "Progressive Comparison for Ranking Estimation.\n", "abstract": " Object ranking is a problem that involves ordering given objects by aggregating pairwise comparison data collected from one or more evaluators; however, the cost of object evaluations is high in some applications. In this paper, we propose an efficient data collection method called progressive comparison, whose objective is to collect many pairwise comparison data while reducing the number of evaluations. We also propose active learning methods to determine which object should be evaluated next in the progressive comparison; we propose two measures of expected model changes, one considering the changes in the evaluation score distributions and the other considering the changes in the winning probabilities. The results of experiments using a synthetic dataset and two real datasets demonstrate that the progressive comparison method achieves high estimation accuracy with a smaller number of evaluations than the standard pairwise comparison method, and that the active learning methods further reduce the number of evaluations as compared with a random sampling method.", "num_citations": "5\n", "authors": ["2096"]}
{"title": "From one star to three stars: Upgrading legacy open data using crowdsourcing\n", "abstract": " Despite recent open data initiatives in many countries, a significant percentage of the data provided is in non-machine-readable formats like image format rather than in a machine-readable electronic format, thereby restricting their usability. This paper describes the first unified framework for converting legacy open data in image format into a machine-readable and reusable format by using crowdsourcing. Crowd workers are asked not only to extract data from an image of a chart but also to reproduce the chart objects in spreadsheets. The properties of the reconstructed chart objects give their data structures including series names and values, which are useful for automatic processing of data by computer. Since results produced by crowdsourcing inherently contain errors, a quality control mechanism was developed that improves the accuracy of extracted tables by aggregating tables created by different workers for\u00a0\u2026", "num_citations": "5\n", "authors": ["2096"]}
{"title": "Crowdsourcing quality control for item ordering tasks\n", "abstract": " One of the biggest challenges in crowdsourcing is quality control which is to expect high quality results from crowd workers who are not necessarily very capable nor motivated. In this paper, we consider item ordering questions, where workersare asked to arrange multiple items in the correct order. We propose a probabilistic generative model of crowd answers by extending a distance-based order model to incorporate worker ability, and give an efficient estimation algorithm.", "num_citations": "5\n", "authors": ["2096"]}
{"title": "Least absolute policy iteration--a robust approach to value function approximation\n", "abstract": " Least-squares policy iteration is a useful reinforcement learning method in robotics due to its computational efficiency. However, it tends to be sensitive to outliers in observed rewards. In this paper, we propose an alternative method that employs the absolute loss for enhancing robustness and reliability. The proposed method is formulated as a linear programming problem which can be solved efficiently by standard optimization software, so the computational advantage is not sacrificed for gaining robustness and reliability. We demonstrate the usefulness of the proposed approach through a simulated robot-control task.", "num_citations": "5\n", "authors": ["2096"]}
{"title": "Predictive modeling of patent quality by using text mining\n", "abstract": " Background: Patent value is important for companies\u2026 but this is not always true for entire society\u25aa It is important to evaluate the value of each patent to one\u2019s own business:", "num_citations": "5\n", "authors": ["2096"]}
{"title": "Regression with interval output values\n", "abstract": " We consider a regression problem where target values are given as intervals, and propose a statistical approach to it. Although it is hard to solve the optimization problem directly, we propose an approximation method based on the EM algorithm. Experiments using the benchmark datasets show effectiveness of our approach.", "num_citations": "5\n", "authors": ["2096"]}
{"title": "Design and Analysis of Convolution Kernels for Tree-Structured Data\n", "abstract": " We introduce a new convolution kernel for labeled ordered trees with arbitrary subgraph features, and an efficient algorithm for computing the kernel with the same time complexity as that of the parse tree kernel. The proposed kernel is extended to allow mutations of labels and structures without increasing the order of computation time. Moreover, as a limit of generalization of the tree kernels, we show a hardness result in computing kernels for unordered rooted labeled trees with arbitrary subgraph features.", "num_citations": "5\n", "authors": ["2096"]}
{"title": "Fast Deterministic CUR Matrix Decomposition with Accuracy Assurance\n", "abstract": " The deterministic CUR matrix decomposition is a low-rank approximation method to analyze a data matrix. It has attracted considerable attention due to its high interpretability, which results from the fact that the decomposed matrices consist of subsets of the original columns and rows of the data matrix. The subset is obtained by optimizing an objective function with sparsity-inducing norms via coordinate descent. However, the existing algorithms for optimization incur high computation costs. This is because coordinate descent iteratively updates all the parameters in the objective until convergence. This paper proposes a fast deterministic CUR matrix decomposition. Our algorithm safely skips unnecessary updates by efficiently evaluating the optimality conditions for the parameters to be zeros. In addition, we preferentially update the parameters that must be nonzeros. Theoretically, our approach guarantees the same result as the original approach. Experiments demonstrate that our algorithm speeds up the deterministic CUR while achieving the same accuracy.", "num_citations": "4\n", "authors": ["2096"]}
{"title": "Fast and robust comparison of probability measures in heterogeneous spaces\n", "abstract": " Comparing two probability measures supported on heterogeneous spaces is an increasingly important problem in machine learning. Such problems arise when comparing for instance two populations of biological cells, each described with its own set of features, or when looking at families of word embeddings trained across different corpora/languages. For such settings, the Gromov Wasserstein (GW) distance is often presented as the gold standard. GW is intuitive, as it quantifies whether one measure can be isomorphically mapped to the other. However, its exact computation is intractable, and most algorithms that claim to approximate it remain expensive. Building on \\cite{memoli-2011}, who proposed to represent each point in each distribution as the 1D distribution of its distances to all other points, we introduce in this paper the Anchor Energy (AE) and Anchor Wasserstein (AW) distances, which are respectively the energy and Wasserstein distances instantiated on such representations. Our main contribution is to propose a sweep line algorithm to compute AE \\emph{exactly} in log-quadratic time, where a naive implementation would be cubic. This is quasi-linear w.r.t. the description of the problem itself. Our second contribution is the proposal of robust variants of AE and AW that uses rank statistics rather than the original distances. We show that AE and AW perform well in various experimental settings at a fraction of the computational cost of popular GW approximations. Code is available at \\url{https://github.com/joisino/anchor-energy}.", "num_citations": "4\n", "authors": ["2096"]}
{"title": "Active change-point detection\n", "abstract": " We introduce Active Change-Point Detection (ACPD), a novel active learning problem for efficient change-point detection in situations where the cost of data acquisition is expensive. At each round of ACPD, the task is to adaptively determine the next input, in order to detect the change-point in a black-box expensive-to-evaluate function, with as few evaluations as possible. We propose a novel framework that can be generalized for different types of data and change-points, by utilizing an existing change-point detection method to compute change scores and a Bayesian optimization method to determine the next input. We demonstrate the efficiency of our proposed framework in different settings of datasets and change-points, using synthetic data and real-world data, such as material science data and seafloor depth data.", "num_citations": "4\n", "authors": ["2096"]}
{"title": "Long-term prediction of small time-series data using generalized distillation\n", "abstract": " The recent increase of \"big data\" in our society has led to major impacts of machine learning and data mining technologies in various fields ranging from marketing to science. On the other hand, there still exist areas where only small-sized data are available for various reasons, for example, high data acquisition costs or the rarity of targets events. Machine learning tasks using such small data are usually difficult because of the lack of information available for training accurate prediction models. In particular, for long-term time-series prediction, the data size tends to be small because of the unavailability of the data between input and output times in training. Such limitations on the size of time-series data further make long-term prediction tasks quite difficult; in addition, the difficulty that the far future is more uncertain than the near future.In this paper, we propose a novel method for long-term prediction of small time\u00a0\u2026", "num_citations": "4\n", "authors": ["2096"]}
{"title": "Data analysis competition platform for educational purposes: lessons learned and future challenges\n", "abstract": " Data analysis education plays an important role in accelerating the efficient use of data analysis technologies in various domains. Not only the knowledge of statistics and machine learning, but also practical skills of deploying machine learning and data analysis techniques, are required for conducting data analysis projects in the real world. Data analysis competitions, such as Kaggle, have been considered as an efficient system for learning such skills by addressing real data analysis problems. However, current data analysis competitions are not designed for educational purposes and it is not well studied how data analysis competition platforms should be designed for enhancing educational effectiveness. To answer this research question, we built, and subsequently operated an educational data analysis competition platform called University of Big Data for several years. In this paper, we present our approaches for supporting and motivating learners and the results of our case studies. We found that providing a tutorial article is beneficial for encouraging active participation of learners, and a leaderboard system allowing an unlimited number of submissions can motivate the efforts of learners. We further discuss future directions of educational data analysis competitions.", "num_citations": "4\n", "authors": ["2096"]}
{"title": "Link prediction in sparse networks by incidence matrix factorization\n", "abstract": " Link prediction plays an important role in multiple areas of artificial intelligence, including social network analysis and bioinformatics; however, it is often negatively affected by the data sparsity problem. In this paper, we present and validate our hypothesis, ie, for sparse networks, incidence matrix factorization (IMF) could perform better than adjacency matrix factorization (AMF), the latter used in many previous studies. A key observation supporting our hypothesis here is that IMF models a partially observed graph more accurately than AMF. Unfortunately, a technical challenge we face in validating our hypothesis is that there is not an obvious method for making link prediction using a factorized incidence matrix, unlike the AMF approach. To this end, we developed an optimization-based link prediction method. Then we have conducted thorough experiments using both synthetic and real-world datasets to investigate the relationship between the sparsity of a network and the predictive performance of the aforementioned two factorization approaches. Our experimental results show that IMF performed better than AMF as networks became sparser, which validates our hypothesis.", "num_citations": "4\n", "authors": ["2096"]}
{"title": "Graph similarity calculation system, method and program\n", "abstract": " A computer implemented method and system for calculating a degree of similarity between two graphs whose nodes are respectively given discrete labels include providing, for each of the two graphs, label values respectively to a given node and nodes adjacent thereto so that different ones of the discrete labels correspond to different ones of the label values. The nodes are sequentially tracing for each of the two graphs, and, during the tracing of the nodes, a new label value is calculated through a hash calculation using a label value of a currently visited node and also using label values of nodes adjacent to the currently visited node to update the label value to the currently visited node. The degree of similarity between the two graphs is calculated on the basis of the number of the label values having been given to nodes of the two graphs and agreeing between the two graphs.", "num_citations": "4\n", "authors": ["2096"]}
{"title": "Method and system for L1-based robust distribution clustering of multinomial distributions\n", "abstract": " A workforce analysis method for solving L 1-based clustering problem of multinomial distributions of workforce data includes acquiring workforce allocation data, arranging the workforce allocation data in sets of fraction data with respect to the L 1 distances, clustering the sets of fraction data t corresponding set of cluster centers, or L 1 distances for each set, minimizing the sets of fraction data based on the cluster centers or L 1 distances and outputting analysis results of the clustering problem.", "num_citations": "4\n", "authors": ["2096"]}
{"title": "Finding business partners and building reciprocal relationships-A machine learning approach\n", "abstract": " Business development is vital for any firms. However, globalization and the rapid development of technologies have made it difficult to find appropriate business partners such as suppliers, customers and outsources and build reciprocal relationships among them, while it simultaneously offers many opportunities. In this contribution, we propose a new computational approach to find business partner candidates based on firm profiles and transactional relationships among them. We employ machine learning techniques to build prediction models of customer-supplier relationships. We applied our approach to Japanese firms and compared our prediction results with the actual business data. The results showed that our approach successfully found plausible candidates and reciprocity among them whose accuracy is about 80%. Using machine learning approach, we have the accuracy of predicting a customer\u00a0\u2026", "num_citations": "4\n", "authors": ["2096"]}
{"title": "Abnormality detection in an on-board network system\n", "abstract": " An abnormality detection method is provided. The abnormality detection method is for detecting an abnormality that may be transmitted to a bus in an on-board network system. The on-board network system includes a plurality of electronic controllers that transmit and receive messages via the bus in a vehicle according to a CAN protocol. In the abnormality detection method, for example, a gateway transmits vehicle identification information to a server and receives a response determining a unit time. An operation process is performed using feature information based on a number of messages received from the bus per the determined unit time and using a model indicating a criterion in terms of a message occurrence frequency. A judgment is made as to an abnormality according to a result of the operation process.", "num_citations": "3\n", "authors": ["2096"]}
{"title": "Regret minimization for causal inference on large treatment space\n", "abstract": " Predicting which action (treatment) will lead to a better outcome is a central task in decision support systems. To build a prediction model in real situations, learning from observational data with a sampling bias is a critical issue due to the lack of randomized controlled trial (RCT) data. To handle such biased observational data, recent efforts in causal inference and counterfactual machine learning have focused on debiased estimation of the potential outcomes on a binary action space and the difference between them, namely, the individual treatment effect. When it comes to a large action space (eg, selecting an appropriate combination of medicines for a patient), however, the regression accuracy of the potential outcomes is no longer sufficient in practical terms to achieve a good decision-making performance. This is because a high mean accuracy on the large action space does not guarantee the nonexistence of a single potential outcome misestimation that misleads the whole decision. Our proposed loss minimizes the classification error of whether or not the action is relatively good for the individual target among all feasible actions, which further improves the decision-making performance, as we demonstrate. We also propose a network architecture and a regularizer that extracts a debiased representation not only from the individual feature but also from the biased action for better generalization in large action spaces. Extensive experiments on synthetic and semi-synthetic datasets demonstrate the superiority of our method for large combinatorial action spaces.", "num_citations": "3\n", "authors": ["2096"]}
{"title": "Stress Prediction from Head Motion\n", "abstract": " The measurement of cognitive stress has huge potential for advertising optimization (e.g., neuromarketing), optimization of recommendation systems, and applications in the fields of human-computer interaction and affective computing. Many studies have addressed stress prediction based on machine learning from the features measured by sensors attached to a subject\u2019s body. Meanwhile, as virtual reality (VR) and augmented reality (AR) have increased in popularity, head motion data from users watching VR/AR contents have become ubiquitous. In addition, stress prediction from head motion can be valuable because it does not rely on skin condition (e.g., sweat, tattoo, and cosmetics) and is not detrimental to usability. However, the effectiveness of stress prediction based on head motion data is not well understood. In this study, we propose a method to predict stress from head motion and verify the performance\u00a0\u2026", "num_citations": "3\n", "authors": ["2096"]}
{"title": "In-vehicle network intrusion detection and explanation using density ratio estimation\n", "abstract": " Most modern vehicles are equipped with Electronic Control Units (ECUs) and they are interconnected by Controller Area Network (CAN), a widely used communication protocol for in-vehicle networks. Recent studies have demonstrated that injecting malicious packets into in-vehicle networks can cause unintended behaviors of vehicles, and such a security threat is considered as an urgent issue to address in the industry. In this paper, we tackle with the task of detecting injected malicious packets as well as the task of explaining these injected packets, that is, to find which parts of the injected packets are essential in the attack. In contrast with the previous approaches using statistical anomaly detection techniques to find anomalous contents, our approach employs change detection which can detect small changes of frequencies of non-anomalous contents. Especially, our change detection approach is based on a\u00a0\u2026", "num_citations": "3\n", "authors": ["2096"]}
{"title": "Large-scale Driver Identification Using Automobile Driving Data\n", "abstract": " We address a large-scale driver identification problem, which aims to predict the driver of a vehicle from various types of data, such as speed and acceleration information, that are collected during driving by using GPS sensors equipped with smart phones. While existing studies consider at most a few hundreds of drivers, we target a huge number of drivers up to 10,000 drivers. The results of our experiments show that our method identifies drivers more precisely than baseline methods. We also show that location features are quite effective in the large scale driver identification, and speed and acceleration features also contribute to driver identification.", "num_citations": "3\n", "authors": ["2096"]}
{"title": "Quality Control for Crowdsourced Multi-label Classification Using RAkEL\n", "abstract": " The quality of labels is one of the major issues in crowdsourced labeling tasks. A convenient method for ensuring the quality of labels is to assign the same labeling task to multiple workers and aggregate the labels. Several statistical aggregation methods for single-label classification tasks have been proposed; however, for multi-label classification tasks has not been well studied. Although the existing aggregation methods for single-label classification tasks can be applied to the multi-label classification tasks, they are not designed to incorporate relationships among classes, or they require large computation time. To address these issues, we propose to use RAndom k-labELsets (RAkEL). By incorporating an existing aggregation method for single-label classification tasks into RAkEL, we propose a novel quality control method for crowdsourced multi-label classification. We demonstrate that our method\u00a0\u2026", "num_citations": "3\n", "authors": ["2096"]}
{"title": "Crowdsourcing data understanding: a case study using open government data\n", "abstract": " We investigate the feasibility of applying crowdsourcing to the task of data understanding, which aims to explore a set of data and discover insights from it. We consider a two-stage workflow composed of data exploration tasks and review tasks. We conducted experiments using nine datasets provided by the government of Japan and collected 114 findings and charts. The experimental results demonstrate the effectiveness of the power of crowds for data understanding and indicate the usefulness of crowd reviews for controlling the quality of crowdsourced data understanding.", "num_citations": "3\n", "authors": ["2096"]}
{"title": "EM-based inference of true labels using confidence judgments\n", "abstract": " We have developed a method for accurately inferring true labels from labels provided by crowdsourcing workers, with the aid of self-reported confidence judgments in their labels. Although confidence judgments can be useful information for estimating the quality of the provided labels, some workers are overconfident about the quality of their labels while others are underconfident. To address this problem, we extended the Dawid-Skene model and created a probabilistic model that considers the differences among workers in their accuracy of confidence judgments. Results of experiments using actual crowdsourced data showed that incorporating workers' confidence judgments can improve the accuracy of inferred labels.", "num_citations": "3\n", "authors": ["2096"]}
{"title": "Matrix factorization with aggregated observations\n", "abstract": " Missing value estimation is a fundamental task in machine learning and data mining. It is not only used as a preprocessing step in data analysis, but also serves important purposes such as recommendation. Matrix factorization with low-rank assumption is a basic tool for missing value estimation. However, existing matrix factorization methods cannot be applied directly to such cases where some parts of the data are observed as aggregated values of several features in high-level categories. In this paper, we propose a new problem of restoring original micro observations from aggregated observations, and we give formulations and efficient solutions to the problem by extending the ordinary matrix factorization model. Experiments using synthetic and real data sets show that the proposed method outperforms several baseline methods.", "num_citations": "3\n", "authors": ["2096"]}
{"title": "\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u306b\u3088\u308b\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30af\u30e9\u30a6\u30c9\u30bd\u30fc\u30b7\u30f3\u30b0\u3067\u4f5c\u6210\u3057\u305f\u8a13\u7df4\u30c7\u30fc\u30bf\u304b\u3089\u306e\u6559\u5e2b\u4ed8\u304d\u5b66\u7fd2\n", "abstract": " Crowdsourcing services are often used to collect a large amount of labeled data for machine learning. Although they provide us an easy way to get labels at very low cost in a short period, they have serious limitations. One of them is the variable quality of the crowd-generated data. There have been many attempts to increase the reliability of crowd-generated data and the quality of classifiers obtained from such data. However, in these problem settings, relatively few researchers have tried using expert-generated data to achieve further improvements. In this paper, we apply three models that deal with the problem of learning from crowds to this problem: a latent class model, a personal classifier model, and a data-dependent error model. We evaluate these methods against two baseline methods on a real data set to demonstrate the effectiveness of combining crowd-generated data and expert-generated data.", "num_citations": "3\n", "authors": ["2096"]}
{"title": "A semi-supervised approach to indoor location estimation\n", "abstract": " \u25aa Given: the i-th data is given as a tuple of (x (i), TID (i), t (i), y (i))\u2013spatial information: x (i) X=< 101 is the received signal strength (RSS) values\u2013temporal information: TID (i)(trace ID) and t (i)(time ID) indicate the time of the data observed\u2013classs label: y (i) Y={1, 2,..., 247} is a location label given only for a small fraction of the data\u2022 Semi-supervised learning problem", "num_citations": "3\n", "authors": ["2096"]}
{"title": "Efficient filtering methods for clustering cDNAs with spliced sequence alignment\n", "abstract": " Motivation: Clustering sequences of a full-length cDNA library into alternative splice form candidates is a very important problem.                    Results: We developed a new efficient algorithm to cluster sequences of a full-length cDNA library into alternative splice form candidates. Current clustering algorithms for cDNAs tend to produce too many clusters containing incorrect splice form candidates. Our algorithm is based on a spliced sequence alignment algorithm that considers splice sites. The spliced sequence alignment algorithm is a variant of an ordinary dynamic programming algorithm, which requires O(nm) time for checking a pair of sequences where n and m are the lengths of the two sequences. Since the time bound is too large to perform all-pair comparison for a large set of sequences, we developed new techniques to reduce the computation time without affecting the accuracy of the output\u00a0\u2026", "num_citations": "3\n", "authors": ["2096"]}
{"title": "A Method for Normalization of Gene Expression Data\n", "abstract": " Advances in microarray technologies have made it possible to comprehensively measure gene expression profiles. However, in these experiments, the efficiency of fluorescent dyes are different between different experiments, and adjustments of these expression values are mandatory before the analysis, It is referred to as normalization. Although the most popular method is total intensity normalization, where each fluorescent intensity value is divided by the sum of all the fluorescent intensities, it is difficult to apply this method in case that the expression of some of the genes changes too much. In this paper, we describe a novel method for normalizing gene expression data of microarays so that we can compare results of microarray experiments correctly even in these difficult cases.", "num_citations": "3\n", "authors": ["2096"]}
{"title": "Learning individually fair classifier with path-specific causal-effect constraint\n", "abstract": " Machine learning is used to make decisions for individuals in various fields, which require us to achieve good prediction accuracy while ensuring fairness with respect to sensitive features (eg, race and gender). This problem, however, remains difficult in complex real-world scenarios. To quantify unfairness under such situations, existing methods utilize {\\it path-specific causal effects}. However, none of them can ensure fairness for each individual without making impractical functional assumptions about the data. In this paper, we propose a far more practical framework for learning an individually fair classifier. To avoid restrictive functional assumptions, we define the {\\it probability of individual unfairness}(PIU) and solve an optimization problem where PIU\u2019s upper bound, which can be estimated from data, is controlled to be close to zero. We elucidate why our method can guarantee fairness for each individual. Experimental results show that our method can learn an individually fair classifier at a slight cost of accuracy.", "num_citations": "2\n", "authors": ["2096"]}
{"title": "Counterfactual Propagation for Semi-Supervised Individual Treatment Effect Estimation\n", "abstract": " Individual treatment effect (ITE) represents the expected improvement in the outcome of taking a particular action to a particular target, and plays important roles in decision making in various domains. However, its estimation problem is difficult because intervention studies to collect information regarding the applied treatments (i.e., actions) and their outcomes are often quite expensive in terms of time and monetary costs. In this study, we consider a semi-supervised ITE estimation problem that exploits more easily-available unlabeled instances to improve the performance of ITE estimation using small labeled data. We combine two ideas from causal inference and semi-supervised learning, namely, matching and label propagation, respectively, to propose counterfactual propagation, which is the first semi-supervised ITE estimation method. Experiments using semi-real datasets demonstrate that the proposed method can successfully mitigate the data scarcity problem in ITE estimation.", "num_citations": "2\n", "authors": ["2096"]}
{"title": "Learning to sample hard instances for graph algorithms\n", "abstract": " \\textit {Hard instances}, which require a long time for a specific algorithm to solve, help (1) analyze the algorithm for accelerating it and (2) build a good benchmark for evaluating the performance of algorithms. There exist several efforts for automatic generation of hard instances. For example, evolutionary algorithms have been utilized to generate hard instances. However, they generate only finite number of hard instances. The merit of such methods is limited because it is difficult to extract meaningful patterns from small number of instances. We seek for a probabilistic generator of hard instances. Once the generative distribution of hard instances is obtained, we can sample a variety of hard instances to build a benchmark, and we can extract meaningful patterns of hard instances from sampled instances. The existing methods for modeling the hard instance distribution rely on parameters or rules that are found by domain experts; however, they are specific to the problem. Hence, it is challenging to model the distribution for general cases. In this paper, we focus on graph problems. We propose\\textsc {HiSampler}, the hard instance sampler, to model the hard instance distribution of graph algorithms.\\textsc {HiSampler} makes it possible to obtain the distribution of hard instances without hand-engineered features. To the best of our knowledge, this is the first method to learn the distribution of hard instances using machine learning. Through experiments, we demonstrate that our proposed method can generate instances that are a few to several orders of magnitude harder than the random-based approach in many settings. In particular, our method\u00a0\u2026", "num_citations": "2\n", "authors": ["2096"]}
{"title": "Topological bayesian optimization with persistence diagrams\n", "abstract": " Finding an optimal parameter of a black-box function is important for searching stable material structures and finding optimal neural network structures, and Bayesian optimization algorithms are widely used for the purpose. However, most of existing Bayesian optimization algorithms can only handle vector data and cannot handle complex structured data. In this paper, we propose the topological Bayesian optimization, which can efficiently find an optimal solution from structured data using \\emph{topological information}. More specifically, in order to apply Bayesian optimization to structured data, we extract useful topological information from a structure and measure the proper similarity between structures. To this end, we utilize persistent homology, which is a topological data analysis method that was recently applied in machine learning. Moreover, we propose the Bayesian optimization algorithm that can handle multiple types of topological information by using a linear combination of kernels for persistence diagrams. Through experiments, we show that topological information extracted by persistent homology contributes to a more efficient search for optimal structures compared to the random search baseline and the graph Bayesian optimization algorithm.", "num_citations": "2\n", "authors": ["2096"]}
{"title": "Robust multi-view topic modeling by incorporating detecting anomalies\n", "abstract": " Multi-view text data consist of texts from different sources. For instance, multilingual Wikipedia corpora contain articles in different languages which are created by different group of users. Because multi-view text data are often created in distributed fashion, information from different sources may not be consistent. Such inconsistency introduce noise to analysis of such kind of data. In this paper, we propose a probabilistic topic model for multi-view data, which is robust against noise. The proposed model can also be used for detecting anomalies. In our experiments on Wikipedia data sets, the proposed model is more robust than existing multi-view topic models in terms of held-out perplexity.", "num_citations": "2\n", "authors": ["2096"]}
{"title": "Learning to Enumerate\n", "abstract": " The Learning to Enumerate problem is a new variant of the typical active learning problem. Our objective is to find data that satisfies arbitrary but fixed conditions, without using any prelabeled training data. The key aspect here is to query as few as possible non-target data. While typical active learning techniques try to keep the number of queried labels low they give no regards to the class these instances belong to. Since the aim of this problem is different from the common active learning problem, we started with applying uncertainty sampling as a base technique and evaluated the performance of three different base learner on 19 public datasets from the UCI Machine Learning Repository.", "num_citations": "2\n", "authors": ["2096"]}
{"title": "Latent confusion analysis by normalized gamma construction\n", "abstract": " We developed a flexible framework for modeling the annotation and judgment processes of humans, which we called \u201cnormalized gamma construction of a confusion matrix.\u201d This framework enabled us to model three properties:(1) the abilities of humans,(2) a confusion matrix with labeling, and (3) the difficulty with which items are correctly annotated. We also provided the concept of \u201clatent confusion analysis (LCA),\u201d whose main purpose was to analyze the principal confusions behind human annotations and judgments. It is assumed in LCA that confusion matrices are shared between persons, which we called \u201clatent confusions\u201d, in tribute to the \u201clatent topics\u201d of topic modeling. We aim at summarizing the workers\u2019 confusion matrices with the small number of latent principal confusion matrices because many personal confusion matrices is difficult to analyze. We used LCA to analyze latent confusions regarding the effects of radioactivity on fish and shellfish following the Fukushima Daiichi nuclear disaster in 2011.", "num_citations": "2\n", "authors": ["2096"]}
{"title": "Change analysis\n", "abstract": " Different virtual labels, for example, like+ 1 and\u2212 1, are assigned to two data sets. A change analysis problem for the two data sets is reduced to a supervised learning problem by using the virtual labels. Specifically, a classifier such as logical regression, decision tree and SVM is prepared and is trained by use of a data set obtained by merging the two data sets assigned the virtual labels. A feature selection function of the resultant classifier is used to rank and output both every attribute contributing to classification and its contribution rate.", "num_citations": "2\n", "authors": ["2096"]}
{"title": "Fast similarity computation in factorized tensors\n", "abstract": " Low-rank factorizations of higher-order tensors have become an invaluable tool for researchers from many scientific disciplines. Tensor factorizations have been successfully applied for moderately sized multimodal data sets involving a small number of modes. However, a significant hindrance to the full realization of the potential of tensor methods is a lack of scalability on the client side: even when low-rank representations are provided by an external agent possessing the necessary computational resources, client applications are quickly rendered infeasible by the space requirements for explicitly storing a (dense) low-rank representation of the input tensor. We consider the problem of efficiently computing common similarity measures between entities expressed by fibers (vectors) or slices (matrices) within a given factorized tensor. We show that after appropriate preprocessing, inner products can be\u00a0\u2026", "num_citations": "2\n", "authors": ["2096"]}
{"title": "A linear time subpath kernel for trees\n", "abstract": " All rights are reserved and no part of this publication may be reproduced or transmitted in any form or by any means, electronic or mechanical, including photocopy, recording, or any information storage and retrieval system, without permission in writing from the publisher. Notwithstanding, instructors are permitted to photocopy isolated articles for noncommercial classroom use without fee.(License No.: 10GA0019/12GB0052/13GB0056/17GB0034/18GB0034)", "num_citations": "2\n", "authors": ["2096"]}
{"title": "A convex formulations of learning from crowds\n", "abstract": " All rights are reserved and no part of this publication may be reproduced or transmitted in any form or by any means, electronic or mechanical, including photocopy, recording, or any information storage and retrieval system, without permission in writing from the publisher. Notwithstanding, instructors are permitted to photocopy isolated articles for noncommercial classroom use without fee.(License No.: 10GA0019/12GB0052/13GB0056/17GB0034/18GB0034)", "num_citations": "2\n", "authors": ["2096"]}
{"title": "Graphite: Estimating individual effects of graph-structured treatments\n", "abstract": " Outcome estimation of treatments for individual targets is a crucial foundation for decision making based on causal relations. Most of the existing outcome estimation methods deal with binary or multiple-choice treatments; however, in some applications, the number of interventions can be very large, while the treatments themselves have rich information. In this study, we consider one important instance of such cases, that is, the outcome estimation problem of graph-structured treatments such as drugs. Due to the large number of possible interventions, the counterfactual nature of observational data, which appears in conventional treatment effect estimation, becomes a more serious issue in this problem. Our proposed method GraphITE (pronounced'graphite') obtains the representations of the graph-structured treatments using graph neural networks, and also mitigates the observation biases by using the HSIC\u00a0\u2026", "num_citations": "1\n", "authors": ["2096"]}
{"title": "Re-evaluating Word Mover's Distance\n", "abstract": " The word mover's distance (WMD) is a fundamental technique for measuring the similarity of two documents. As the crux of WMD, it can take advantage of the underlying geometry of the word space by employing an optimal transport formulation. The original study on WMD reported that WMD outperforms classical baselines such as bag-of-words (BOW) and TF-IDF by significant margins in various datasets. In this paper, we point out that the evaluation in the original study could be misleading. We re-evaluate the performances of WMD and the classical baselines and find that the classical baselines are competitive with WMD if we employ an appropriate preprocessing, i.e., L1 normalization. However, this result is not intuitive. WMD should be superior to BOW because WMD can take the underlying geometry into account, whereas BOW cannot. Our analysis shows that this is due to the high-dimensional nature of the underlying metric. We find that WMD in high-dimensional spaces behaves more similarly to BOW than in low-dimensional spaces due to the curse of dimensionality.", "num_citations": "1\n", "authors": ["2096"]}
{"title": "Learning to Rank for Multi-Step Ahead Time-Series Forecasting\n", "abstract": " Time-series forecasting is a fundamental problem associated with a wide range of engineering, financial, and social applications. The challenge arises from the complexity due to the time-variant property of time series and the inevitable diminishing utility of predictive models. Therefore, it is generally difficult to accurately predict values, especially in a multi-step ahead setting. However, in domains such as financial time series forecasting, an ex-ante prediction of the relative order of values in the near future is sufficient; i.e., the next 100 days can help make meaningful investment decisions. In this paper, we propose a dynamic prediction framework that makes it possible to make an ex-ante forecast of time series with a special focus on the relative ordering of the forecast within a forward-looking time horizon. Through the lens of the concordance index (CI), we compare the proposed method with conventional\u00a0\u2026", "num_citations": "1\n", "authors": ["2096"]}
{"title": "Poincare: Recommending publication venues via treatment effect estimation\n", "abstract": " Choosing a publication venue for an academic paper is a crucial step in the research process. However, in many cases, decisions are based on the experience of researchers, which often leads to suboptimal results. Although some existing methods recommend publication venues, they just recommend venues where a paper is likely to be published. In this study, we aim to recommend publication venues from a different perspective. We estimate the number of citations a paper will receive if the paper is published in each venue and recommend the venue where the paper has the most potential impact. However, there are two challenges to this task. First, a paper is published in only one venue, and thus, we cannot observe the number of citations the paper would receive if the paper were published in another venue. Secondly, the contents of a paper and the publication venue are not statistically independent; that is, there exist selection biases in choosing publication venues. In this paper, we propose to use a causal inference method to estimate the treatment effects of choosing a publication venue effectively and to recommend venues based on the potential influence of papers.", "num_citations": "1\n", "authors": ["2096"]}
{"title": "Simultaneous Link Prediction on Unaligned Networks Using Graph Embedding and Optimal Transport\n", "abstract": " Link prediction is an extensively studied topic and various methods have been proposed to tackle the task in both heuristic and more sophisticated statistical learning approaches. However, most of them focus on the setting of one single graph. Combining information on multiple graphs with similar topological structures can improve the performance and robustness of link prediction; nevertheless, the alignment between nodes of different networks is not always available, or is only partially known. This study considers the link prediction problem on two unaligned networks simultaneously. A new framework is proposed to integrate link prediction using graph embedding and node alignment using optimal transport. The integrated objective is optimized at once via an iterative algorithm. A showcase of the proposed framework using LINE embedding method is discussed with experiments on three real datasets. The\u00a0\u2026", "num_citations": "1\n", "authors": ["2096"]}
{"title": "Synthetic accessibility assessment using auxiliary responses\n", "abstract": " Despite the recent advances in computational approaches to discovering new chemical compounds, accessibility assessment of designed compounds has still been a difficult task to automate because it is a heavily knowledge intensive task. A promising solution to such \u201cAI-hard\u201d tasks is collective intelligence approaches that aggregate opinions of a group of human non-experts or semi-experts. However, the existing aggregation methods rely only on synthetic accessibility evaluation scores given by humans, and they do not exploit auxiliary information obtained as byproducts of human evaluations such as that related to chemical structures. In this paper, we propose to exploit such auxiliary responses to obtain better aggregations. We introduce a new two-stage aggregation method of semi-expert judgments consisting of synthetic accessibility evaluation scores along with auxiliary responses that select\u00a0\u2026", "num_citations": "1\n", "authors": ["2096"]}
{"title": "Link prediction on multiple graphs with graph embedding and optimal transport\n", "abstract": " Link prediction is an extensively studied topic and various methods have been proposed to tackle the task in both heuristic and more sophisticated statistical learning approaches. However, most of them only focus on one single graph. In many scenarios, combining information on multiple graphs with similar topological structures can greatly improve the performance and robustness of link prediction. In this study, we propose a new framework for learning link prediction on two unaligned graphs simultaneously. We use the LINE method, although technically any embedding method is applicable, to embed nodes of each graph into low-dimensional vectors. Optimal Transport is then employed to supervise the node alignment via embedding vectors between the two graphs. The learned embedding vectors are employed for link prediction via a similarity score. Experiments have shown that node alignment using Optimal Transport is beneficial and greatly contributes to the favorable performance of the proposed method over the baseline in many settings.", "num_citations": "1\n", "authors": ["2096"]}
{"title": "In-app purchase prediction using bayesian personalized dwell day ranking\n", "abstract": " In this paper, we consider the in-app purchase prediction problem to effectively show display ads and to promote in-app purchases to users in a personalized manner. Compared to install prediction, purchase prediction is more difficult in the sense that the number of in-app purchases is much fewer than that of installations. To resolve this issue, following the idea of the Bayesian personalized ranking (BPR) framework, we consider the installation of an application as an intermediate feedback between purchase and unobserved feedback and propose install-enhanced BPR. More specifically, in our proposed method, we enhance the quality of the intermediate information by combining the dwell time on the application. Through experiments using a real-world dataset, we show that our proposed method outperforms the baselines and provides several insights on user activity.", "num_citations": "1\n", "authors": ["2096"]}
{"title": "Context-Regularized Neural Collaborative Filtering for Game App Recommendation.\n", "abstract": " People spend a substantial amount of time playing games on their smartphones. Owing to growth in the number of newly released games, it is getting more difficult for people to identify which of the broad selection of games they want to play. In this paper, we introduce context-aware recommendation for game apps that combines neural collaborative filtering and item embedding. We find that some contexts special to games are effective in representing item embeddings in implicit feedback situations. Experimental results show that our proposed method outperforms conventional methods.", "num_citations": "1\n", "authors": ["2096"]}
{"title": "Angle-based convolution networks for extracting local spatial features\n", "abstract": " Convolutional Neural Networks (CNNs) have been a powerful feature extracting method for various machine learning problems, but it is limited to a regular grid shape spatial locations, eg, pixels of images or GPS grids. However, due to physical or privacy preserving problems, spatio-temporal data in the real world often consist of irregular spatial locations. To overcome this limitation, we propose Angle-based Convolutional Networks (ACNs) that leverages the local feature extraction function of CNNs and the graph formulation of Graph Convolutional Neural Networks (GCNs). Our method considers angles among spatial locations and introduces coefficient weight for angle partitions that enables us to extract local features appears in spatio-temporal data without regular grids.", "num_citations": "1\n", "authors": ["2096"]}
{"title": "A dimension reduction approach to multinomial relation prediction\n", "abstract": " Nozomi Nori, Danushka Bollegala & Hisashi Kashima, A Dimension Reduction Approach to Multinomial Relation Prediction - PhilPapers Sign in | Create an account PhilPapers PhilPeople PhilArchive PhilEvents PhilJobs PhilPapers home Syntax Advanced Search Syntax Advanced Search Syntax Advanced Search A Dimension Reduction Approach to Multinomial Relation Prediction Nozomi Nori, Danushka Bollegala & Hisashi Kashima Transactions of the Japanese Society for Artificial Intelligence 29 (1):168-176 (2014) Abstract This article has no associated abstract. (fix it) Keywords No keywords specified (fix it) Categories No categories specified (categorize this paper) DOI 10.1527/tjsai.Options Edit this record Mark as duplicate Export citation Find it on Scholar Request removal from index Revision history Download options PhilArchive copy Upload a copy of this paper Check publisher's policy Papers currently \u2026", "num_citations": "1\n", "authors": ["2096"]}
{"title": "A Simultaneous Completion Method for Multiple Relational Data\n", "abstract": " All rights are reserved and no part of this publication may be reproduced or transmitted in any form or by any means, electronic or mechanical, including photocopy, recording, or any information storage and retrieval system, without permission in writing from the publisher. Notwithstanding, instructors are permitted to photocopy isolated articles for noncommercial classroom use without fee.(License No.: 10GA0019/12GB0052/13GB0056/17GB0034/18GB0034)", "num_citations": "1\n", "authors": ["2096"]}
{"title": "Hash-based structural similarity for semi-supervised Learning on attribute graphs\n", "abstract": " We present an efficient method to compute similarity between graph nodes by comparing their neighborhood structures rather than proximity. The key is to use a hash for avoiding expensive subgraph comparison. Experiments show that the proposed algorithm performs well in semi-supervised node classification.", "num_citations": "1\n", "authors": ["2096"]}
{"title": "System for supporting user's behavior\n", "abstract": " Provided is a system which supports a user's behavior by generating a behavioral decision function indicating behavior to be adopted to a certain target. The system includes: a data acquiring section which acquires a cost caused as a result of adopting each of a plurality of behaviors to a target as training data for generating the behavioral decision function, the plurality of behaviors having already been adopted to the target; and a function generator which generates, based on the training data, the behavioral decision function to minimize the expected shortfall of a cost to be obtained as a result of adopting the behavior to the target.", "num_citations": "1\n", "authors": ["2096"]}
{"title": "Predicting customer-supplier relationships using network-based features\n", "abstract": " Business development is vital for any firms. However, globalization and the rapid development of technologies have made it difficult to find appropriate business partners such as suppliers, customers and outsources. In this contribution, we propose a new computational approach to find business partner candidates based on firm profiles and transactional relationships among them. We employ machine learning techniques to build prediction models of future customer-supplier relationships. We applied our approach to Japanese firms and compared our prediction results with the actual business data. The results showed that our approach successfully found plausible candidates, and can be a new powerful tool to develop one's own business in the complicated, specialized and rapidly changing business environments of recent years.", "num_citations": "1\n", "authors": ["2096"]}
{"title": "System for supporting user's behavior\n", "abstract": " Provided is a system 10 which supports a user's behavior by generating a behavioral decision function indicating behavior to be adopted to a certain target. The system 10 includes: a data acquiring section 110 which acquires a cost caused as a result of adopting each of a plurality of behaviors to a target as training data for generating the behavioral decision function, the plurality of behaviors having already been adopted to the target; and a function generator 120 which generates, based on the training data, the behavioral decision function to minimize the expected shortfall of a cost to be obtained as a result of adopting the behavior to the target.", "num_citations": "1\n", "authors": ["2096"]}
{"title": "Risk-Sensitive Learning via Expected Shortfall Minimization\n", "abstract": " A new approach for cost-sensitive classification is proposed. We extend the framework of cost-sensitive learning to mitigate risks of huge costs occurring with low probabilities, and propose an algorithm that achieves this goal. Instead of minimizing the expected cost commonly used in cost-sensitive learning, our algorithm minimizes expected shortfall, a.k.a. conditional value-at-risk, known as a good risk metric in the area of financial engineering. The proposed algorithm is a general meta-learning algorithm that can utilize existing example-dependent cost-sensitive learning algorithms, and is capable of dealing with not only alternative actions in ordinary classification tasks, but also allocative actions in resource-allocation type tasks.", "num_citations": "1\n", "authors": ["2096"]}
{"title": "\u30ab\u30fc\u30cd\u30eb\u6cd5\u306b\u3088\u308b\u69cb\u9020\u30c7\u30fc\u30bf\u306e\u89e3\u6790\n", "abstract": " \u25aa \u69cb\u9020\u3092\u3082\u3063\u305f\u30c7\u30fc\u30bf\u2013\u5185\u90e8\u69cb\u9020\u3068\u5916\u90e8\u69cb\u9020\u25aa \u69cb\u9020\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u30ab\u30fc\u30cd\u30eb\u2013\u7573\u307f\u8fbc\u307f\u30ab\u30fc\u30cd\u30eb\u306e\u6982\u5ff5\u2013\u30b0\u30e9\u30d5\u30ab\u30fc\u30cd\u30eb\u306e\u8a2d\u8a08\u2013\u69d8\u3005\u306a\u69cb\u9020\u306b\u5bfe\u3059\u308b\u7573\u307f\u8fbc\u307f\u30ab\u30fc\u30cd\u30eb", "num_citations": "1\n", "authors": ["2096"]}
{"title": "Kernel Methods for Analyzing Structured-data\n", "abstract": " ken-system: Kernel Methods for Analyzing Structured-data IEICE Technical Committee Submission System Conference Paper's Information Online Proceedings [Sign in] Tech. Rep. Archives Go Top Page Go Previous [Japanese] / [English] Paper Abstract and Keywords Presentation 2005-02-25 13:45 [Special Talk] Kernel Methods for Analyzing Structured-data Hisashi Kashima (IBM Research) Abstract (in Japanese) (See Japanese page) (in English) We introduce kernel-based approaches for analyzing structured data such as sequences, trees, and graphs. Especially, we introduce the idea of the convolution kernel that is a general framework for designing kernels for structured data, and give some examples of such kernels. Moreover, we introduce the structure mapping problem that is a generalized problem of the supervised classification problem, and kernel-based approaches for the problem. Keyword (in \u2026", "num_citations": "1\n", "authors": ["2096"]}
{"title": "Effective dimension in anomaly detection: its application to computer systems\n", "abstract": " We consider the issue of online anomaly detection from a time sequence of directional data (normalized vectors) in high dimensional systems. In spite of the practical importance, little is known about anomaly detection methods for directional data. Using a novel concept of the effective dimension of the system, we successfully formulated an anomaly detection method which is free from the \u201ccurse of dimensionality.\u201d In our method, we derive a probability distribution function (pdf) for an anomaly metric, and use a novel update algorithm for the parameters in the pdf, where the effective dimension is included as a fitting parameter. For directional data from a computer system, we demonstrate the utility of our algorithm in anomaly detection.", "num_citations": "1\n", "authors": ["2096"]}
{"title": "Method of identifying the source of genetic information in DNA\n", "abstract": " The present invention embeds predetermined information in the nucleotide sequence of DNA to identify the source of the genetic information in DNA. In particular, the pattern of a nucleotide sequence that normally does not appear in DNA is correlated with identification information for identifying a source of predetermined genetic information belonging to the DNA, and, in said portion of the DNA other than the gene, the nucleotide sequence that is correlated with the identification information is embedded in the DNA so as not to affect the genetic information in the DNA.", "num_citations": "1\n", "authors": ["2096"]}