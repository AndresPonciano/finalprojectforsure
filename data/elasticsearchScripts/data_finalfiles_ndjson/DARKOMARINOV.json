{"title": "An empirical analysis of flaky tests\n", "abstract": " Regression testing is a crucial part of software development. It checks that software changes do not break existing functionality. An important assumption of regression testing is that test outcomes are deterministic: an unmodified test is expected to either always pass or always fail for the same code under test. Unfortunately, in practice, some tests often called flaky tests\u2014have non-deterministic outcomes. Such tests undermine the regression testing as they make it difficult to rely on test results. We present the first extensive study of flaky tests. We study in detail a total of 201 commits that likely fix flaky tests in 51 open-source projects. We classify the most common root causes of flaky tests, identify approaches that could manifest flaky behavior, and describe common strategies that developers use to fix flaky tests. We believe that our insights and implications can help guide future research on the important topic of\u00a0\u2026", "num_citations": "301\n", "authors": ["313"]}
{"title": "Practical regression test selection with dynamic file dependencies\n", "abstract": " Regression testing is important but can be time-intensive. One approach to speed it up is regression test selection (RTS), which runs only a subset of tests. RTS was proposed over three decades ago but has not been widely adopted in practice. Meanwhile, testing frameworks, such as JUnit, are widely adopted and well integrated with many popular build systems. Hence, integrating RTS in a testing framework already used by many projects would increase the likelihood that RTS is adopted. We propose a new, lightweight RTS technique, called Ekstazi, that can integrate well with testing frameworks. Ekstazi tracks dynamic dependencies of tests on files, and unlike most prior RTS techniques, Ekstazi requires no integration with version-control systems. We implemented Ekstazi for Java and JUnit, and evaluated it on 615 revisions of 32 open-source projects (totaling almost 5M LOC) with shorter-and longer-running\u00a0\u2026", "num_citations": "173\n", "authors": ["313"]}
{"title": "Comparing non-adequate test suites using coverage criteria\n", "abstract": " A fundamental question in software testing research is how to compare test suites, often as a means for comparing test-generation techniques. Researchers frequently compare test suites by measuring their coverage. A coverage criterion C provides a set of test requirements and measures how many requirements a given suite satisfies. A suite that satisfies 100% of the (feasible) requirements is C-adequate.", "num_citations": "140\n", "authors": ["313"]}
{"title": "Balancing trade-offs in test-suite reduction\n", "abstract": " Regression testing is an important activity but can get expensive for large test suites. Test-suite reduction speeds up regression testing by identifying and removing redundant tests based on a given set of requirements. Traditional research on test-suite reduction is rather diverse but most commonly shares three properties:(1) requirements are defined by a coverage criterion such as statement coverage;(2) the reduced test suite has to satisfy all the requirements as the original test suite; and (3) the quality of the reduced test suites is measured on the software version on which the reduction is performed. These properties make it hard for test engineers to decide how to use reduced test suites. We address all three properties of traditional test-suite reduction:(1) we evaluate test-suite reduction with requirements defined by killed mutants;(2) we evaluate inadequate reduction that does not require reduced test suites to\u00a0\u2026", "num_citations": "105\n", "authors": ["313"]}
{"title": "DeFlaker: Automatically detecting flaky tests\n", "abstract": " Developers often run tests to check that their latest changes to a code repository did not break any previously working functionality. Ideally, any new test failures would indicate regressions caused by the latest changes. However, some test failures may not be due to the latest changes but due to non-determinism in the tests, popularly called flaky tests. The typical way to detect flaky tests is to rerun failing tests repeatedly. Unfortunately, rerunning failing tests can be costly and can slow down the development cycle. We present the first extensive evaluation of rerunning failing tests and propose a new technique, called DeFlaker, that detects if a test failure is due to a flaky test without rerunning and with very low runtime overhead. DeFlaker monitors the coverage of latest code changes and marks as flaky any newly failing test that did not execute any of the changes. We deployed DeFlaker live, in the build process of 96\u00a0\u2026", "num_citations": "104\n", "authors": ["313"]}
{"title": "An extensive study of static regression test selection in modern software evolution\n", "abstract": " Regression test selection (RTS) aims to reduce regression testing time by only re-running the tests affected by code changes. Prior research on RTS can be broadly split into dy namic and static techniques. A recently developed dynamic RTS technique called Ekstazi is gaining some adoption in practice, and its evaluation shows that selecting tests at a coarser, class-level granularity provides better results than selecting tests at a finer, method-level granularity. As dynamic RTS is gaining adoption, it is timely to also evaluate static RTS techniques, some of which were proposed over three decades ago but not extensively evaluated on modern software projects.", "num_citations": "94\n", "authors": ["313"]}
{"title": "Comparing and combining test-suite reduction and regression test selection\n", "abstract": " Regression testing is widely used to check that changes made to software do not break existing functionality, but regression test suites grow, and running them fully can become costly. Researchers have proposed test-suite reduction and regression test selection as two approaches to reduce this cost by not running some of the tests from the test suite. However, previous research has not empirically evaluated how the two approaches compare to each other, and how well a combination of these approaches performs. We present the first extensive study that compares test-suite reduction and regression test selection approaches individually, and also evaluates a combination of the two approaches. We also propose a new criterion to measure the quality of tests with respect to software changes. Our experiments on 4,793 commits from 17 open-source projects show that regression test selection runs on average fewer\u00a0\u2026", "num_citations": "83\n", "authors": ["313"]}
{"title": "On test repair using symbolic execution\n", "abstract": " When developers change a program, regression tests can fail not only due to faults in the program but also due to out-of-date test code that does not reflect the desired behavior of the program. When this occurs, it is necessary to repair test code such that the tests pass. Repairing tests manually is difficult and time consuming. We recently developed ReAssert, a tool that can automatically repair broken unit tests, but only if they lack complex control flow or operations on expected values.", "num_citations": "81\n", "authors": ["313"]}
{"title": "Object equality profiling\n", "abstract": " We present Object Equality Profiling (OEP), a new technique for helping programmers discover optimization opportunities in programs. OEP discovers opportunities for replacing a set of equivalent object instances with a single representative object. Such a set represents an opportunity for automatically or manually applying optimizations such as hash consing, heap compression, lazy allocation, object caching, invariant hoisting, and more. To evaluate OEP, we implemented a tool to help programmers reduce the memory usage of Java programs. Our tool performs a dynamic analysis that records all the objects created during a particular program run. The tool partitions the objects into equivalence classes, and uses collected timing information to determine when elements of an equivalence class could have been safely collapsed into a single representative object without affecting the behavior of that program run\u00a0\u2026", "num_citations": "78\n", "authors": ["313"]}
{"title": "Reliable testing: Detecting state-polluting tests to prevent test dependency\n", "abstract": " Writing reliable test suites for large object-oriented systems is complex and time consuming. One common cause of unreliable test suites are test dependencies that can cause tests to fail unexpectedly, not exposing bugs in the code under test but in the test code itself. Prior research has shown that the main reason for test dependencies is the``pollution''of state shared across tests. We propose a technique, called, for finding tests that pollute the shared state. In a nutshell, finds tests that modify some location on the heap shared across tests or on the file system; a subsequent test could fail if it assumes the shared location to have the initial value before the state was modified. To aid in inspecting the pollutions, provides an access path through the heap that leads to the polluted value or the name of the file that was modified. We implemented a prototype tool for Java and evaluated it on NumOfProjects projects, with a\u00a0\u2026", "num_citations": "77\n", "authors": ["313"]}
{"title": "Ballerina: Automatic generation and clustering of efficient random unit tests for multithreaded code\n", "abstract": " Testing multithreaded code is hard and expensive. A multithreaded unit test creates two or more threads, each executing one or more methods on shared objects of the class under test. Such unit tests can be generated at random, but basic random generation produces tests that are either slow or do not trigger concurrency bugs. Worse, such tests have many false alarms, which require human effort to filter out. We present Ballerina, a novel technique for automated random generation of efficient multithreaded tests that effectively trigger concurrency bugs. Ballerina makes tests efficient by having only two threads, each executing a single, randomly selected method. Ballerina increases chances that such simple parallel code finds bugs by appending it to more complex, randomly generated sequential code. We also propose a clustering technique to reduce the manual effort in inspecting failures of automatically\u00a0\u2026", "num_citations": "73\n", "authors": ["313"]}
{"title": "Automatic testing of software with structurally complex inputs\n", "abstract": " Modern software pervasively uses structurally complex data such as linked data structures. The standard approach to generating test suites for such software, manual generation of the inputs in the suite, is tedious and error-prone. This dissertation proposes a new approach for specifying properties of structurally complex test inputs; presents a technique that automates generation of such inputs; describes the Korat tool that implements this technique for Java; and evaluates the effectiveness of Korat in testing a set of data-structure implementations. Our approach allows the developer to describe the properties of valid test inputs using a familiar implementation language such as Java. Specifically, the user provides an imperative predicate--a piece of code that returns a truth value--that returns true if the input satisfies the required property and false otherwise. Korat implements our technique for solving imperative predicates: given a predicate and a bound on the size of the predicate's inputs, Korat automatically generates the bounded-exhaustive test suite that consists of all inputs, within the given bound, that satisfy the property identified by the predicate. To generate these inputs, Korat systematically searches the bounded input space by executing the predicate on the candidate inputs. Korat does this efficiently by pruning the search based on the predicate's executions and by generating only nonisomorphic inputs. Bounded-exhaustive testing is a methodology for testing the code on all inputs within the given small bound.", "num_citations": "66\n", "authors": ["313"]}
{"title": "Systematic testing of refactoring engines on real software projects\n", "abstract": " Testing refactoring engines is a challenging problem that has gained recent attention in research. Several techniques were proposed to automate generation of programs used as test inputs and to help developers in inspecting test failures. However, these techniques can require substantial effort for writing test generators or finding unique bugs, and do not provide an estimate of how reliable refactoring engines are for refactoring tasks on real software projects.               This paper evaluates an end-to-end approach for testing refactoring engines and estimating their reliability by (1) systematically applying refactorings at a large number of places in well-known, open-source projects and collecting failures during refactoring or while trying to compile the refactored projects, (2) clustering failures into a small, manageable number of failure groups, and (3) inspecting failures to identify non-duplicate bugs. By using\u00a0\u2026", "num_citations": "60\n", "authors": ["313"]}
{"title": "MuTMuT: Efficient exploration for mutation testing of multithreaded code\n", "abstract": " Mutation testing is a method for measuring the quality of test suites. Given a system under test and a test suite, mutations are systematically inserted into the system, and the test suite is executed to determine which mutants it detects. A major cost of mutation testing is the time required to execute the test suite on all the mutants. This cost is even greater when the system under test is multithreaded: not only are test cases from the test suite executed on many mutants, but also each test case is executed for multiple possible thread schedules. We introduce a general framework that can reduce the time for mutation testing of multithreaded code. We present four techniques within the general framework and implement two of them in a tool called MuTMuT. We evaluate MuTMuT on eight multithreaded programs. The results show that MuTMuT reduces the time for mutation testing, substantially over a straightforward mutant\u00a0\u2026", "num_citations": "58\n", "authors": ["313"]}
{"title": "Delta execution for efficient state-space exploration of object-oriented programs\n", "abstract": " We present Delta execution, a technique that speeds up state-space exploration of object-oriented programs. State-space exploration is the essence of model checking and an increasingly popular approach for automating test generation. A key issue in exploration of object-oriented programs is handling the program state, in particular the heap. We exploit the fact that many execution paths in state-space exploration partially overlap. Delta execution simultaneously operates on several states/heaps and shares the common parts across the executions, separately executing only the \"deltas\" where the executions differ. We implemented Delta execution in two model checkers: JPF, a popular general-purpose model checker for Java programs, and BOX, a specialized model checker that we developed for efficient exploration of sequential Java programs. The results for bounded-exhaustive exploration of ten basic\u00a0\u2026", "num_citations": "55\n", "authors": ["313"]}
{"title": "You Want Me to Work with Who? Stakeholder Perceptions of Automated Team Formation in Project-based Courses\n", "abstract": " Instructors are increasingly using algorithmic tools for team formation, yet little is known about how these tools are applied or how students and instructors perceive their use. We studied a representative team formation tool (CATME) in eight project-based courses. An instructor uses the tool to form teams by surveying students' working styles, skills, and demographics; then configuring these criteria as input into an algorithm that assigns teams. We surveyed students (N= 277) in the courses to gauge their perceptions of the strengths and weaknesses of the tool and ideas for improving it. We also interviewed instructors (N= 13) different from those who taught the eight courses to learn about their criteria selections and perceptions of the tool. Students valued the rational basis for forming teams but desired a stronger voice in criteria selection and explanations as to why they were assigned to a particular team. Instructors\u00a0\u2026", "num_citations": "52\n", "authors": ["313"]}
{"title": "Ekstazi: Lightweight test selection\n", "abstract": " Regression testing is a crucial, but potentially time-consuming, part of software development. Regression test selection (RTS), which runs only a subset of tests, was proposed over three decades ago as a promising way to speed up regression testing. However, RTS has not been widely adopted in practice. We propose EKSTAZI , a lightweight RTS tool, that can integrate well with testing frameworks and build systems, increasing the chance for adoption. EKSTAZI tracks dynamic dependencies of tests on files and requires no integration with version-control systems. We implemented EKSTAZI for Java+JUnit and Scala+ScalaTest, and evaluated it on 615 revisions of 32 open-source projects (totaling almost 5M LOC). The results show that EKSTAZI reduced the end-to-end testing time by 32% on average compared to executing all tests. EKSTAZI has been adopted for day-to-day use by several Apache developers\u00a0\u2026", "num_citations": "50\n", "authors": ["313"]}
{"title": "Automated test generation for AspectJ programs\n", "abstract": " Aspect-oriented software development (AOSD) is a new paradigm that improves separation of concerns in software development. AOSD has gained popularity with the adoption of languages such as AspectJ. Automated test generation for AspectJ programs is important for reducing the manual effort in testing AspectJ programs. This position paper proposes Wrasp, a framework for automatic generation of tests for AspectJ programs. In aspect-oriented programs, we define three levels of units: advised methods (methods of base classes), advice, and intertype methods. Wrasp can generate tests to test the integration of these units. Wrasp can also generate tests to test advice as stand-alone units. The main contribution of Wrasp is automatic synthesis of appropriate wrapper classes that enable Wrasp to generate tests for AspectJ programs using the existing tools that generate tests for Java programs.", "num_citations": "47\n", "authors": ["313"]}
{"title": "Change-aware preemption prioritization\n", "abstract": " Successful software evolves as developers add more features, respond to requirements changes, and fix faults. Regression testing is widely used for ensuring the validity of evolving software. As regression test suites grow over time, it becomes expensive to execute them. The problem is exacerbated when test suites contain multithreaded tests. These tests are generally long running as they explore many different thread schedules searching for concurrency faults such as dataraces, atomicity violations, and deadlocks. While many techniques have been proposed for regression test prioritization, selection, and minimization for sequential tests, there is not much work for multithreaded code.", "num_citations": "45\n", "authors": ["313"]}
{"title": "Guidelines for coverage-based comparisons of non-adequate test suites\n", "abstract": " A fundamental question in software testing research is how to compare test suites, often as a means for comparing test-generation techniques that produce those test suites. Researchers frequently compare test suites by measuring their coverage. A coverage criterion C provides a set of test requirements and measures how many requirements a given suite satisfies. A suite that satisfies 100% of the feasible requirements is called C-adequate. Previous rigorous evaluations of coverage criteria mostly focused on such adequate test suites: given two criteria C and C\u2032, are C-adequate suites on average more effective than C\u2032-adequate suites? However, in many realistic cases, producing adequate suites is impractical or even impossible. This article presents the first extensive study that evaluates coverage criteria for the common case of non-adequate test suites: given two criteria C and C\u2032, which one is better to use\u00a0\u2026", "num_citations": "44\n", "authors": ["313"]}
{"title": "Starts: Static regression test selection\n", "abstract": " Regression testing is an important part of software development, but it can be very time consuming. Regression test selection (RTS) aims to speed up regression testing by running only impacted tests-the subset of tests that can change behavior due to code changes. We present STARTS, a tool for STAtic Regression Test Selection. Unlike dynamic RTS, STARTS requires no code instrumentation or runtime information to find impacted tests; instead, STARTS uses only compile-time information. Specifically, STARTS builds a dependency graph of program types and finds, as impacted, tests that can reach some changed type in the transitive closure of the dependency graph. STARTS is a Maven plugin that can be easily integrated into any Maven-based Java project. We find that STARTS selects on average 35.2% of tests, leading to an end-to-end runtime that is on average 81.0% of running all the tests. A video demo\u00a0\u2026", "num_citations": "40\n", "authors": ["313"]}
{"title": "Detecting assumptions on deterministic implementations of non-deterministic specifications\n", "abstract": " Some commonly used methods have nondeterministicspecifications, e.g., iterating through a set canreturn the elements in any order. However, non-deterministicspecifications typically have deterministic implementations, e.g.,iterating through two sets constructed in the same way mayreturn their elements in the same order. We use the termADINS code to refer to code that Assumes a DeterministicImplementation of a method with a Non-deterministic Specification. Such ADINS code can behave unexpectedly whenthe implementation changes, even if the specification remainsthe same. Further, ADINS code can lead to flaky tests -- teststhat pass or fail seemingly non-deterministically. We present a simple technique, called NONDEX, for detectingflaky tests due to ADINS code. We implemented NONDEX forJava: we found 31 methods with non-deterministic specificationsin the Java Standard Library, manually built non\u00a0\u2026", "num_citations": "39\n", "authors": ["313"]}
{"title": "An empirical evaluation and comparison of manual and automated test selection\n", "abstract": " Regression test selection speeds up regression testing by re-running only the tests that can be affected by the most recent code changes. Much progress has been made on research in automated test selection over the last three decades, but it has not translated into practical tools that are widely adopted. Therefore, developers either re-run all tests after each change or perform manual test selection. Re-running all tests is expensive, while manual test selection is tedious and error-prone. Despite such a big trade-off, no study assessed how developers perform manual test selection and compared it to automated test selection.", "num_citations": "36\n", "authors": ["313"]}
{"title": "Counting 1324-avoiding permutations\n", "abstract": " We consider permutations that avoid the pattern 1324. By studying the generating tree for such permutations, we obtain a recurrence formula for their number. A computer program provides data for the number of 1324-avoiding permutations of length up to 20.", "num_citations": "36\n", "authors": ["313"]}
{"title": "Light64: Lightweight hardware support for data race detection during systematic testing of parallel programs\n", "abstract": " Developing and testing parallel code is hard. Even for one given input, a parallel program can have many possible different thread interleavings, which are hard for the programmer to foresee and for a testing tool to cover using stress or random testing. For this reason, a recent trend is to use Systematic Testing, which methodically explores different thread interleavings, while checking for various bugs. Data races are common bugs but, unfortunately, checking for races is often skipped in systematic testers because it introduces substantial runtime overhead if done purely in software. Recently, several techniques for race detection in hardware have been proposed, but they still require significant hardware support.", "num_citations": "35\n", "authors": ["313"]}
{"title": "A large-scale study of test coverage evolution\n", "abstract": " Statement coverage is commonly used as a measure of test suite quality. Coverage is often used as a part of a code review process: if a patch decreases overall coverage, or is itself not covered, then the patch is scrutinized more closely. Traditional studies of how coverage changes with code evolution have examined the overall coverage of the entire program, and more recent work directly examines the coverage of patches (changed statements). We present an evaluation much larger than prior studies and moreover consider a new, important kind of change---coverage changes of unchanged statements. We present a large-scale evaluation of code coverage evolution over 7,816 builds of 47 projects written in popular languages including Java, Python, and Scala. We find that in large, mature projects, simply measuring the change to statement coverage does not capture the nuances of code evolution. Going\u00a0\u2026", "num_citations": "31\n", "authors": ["313"]}
{"title": "Evaluating non-adequate test-case reduction\n", "abstract": " Given two test cases, one larger and one smaller, the smaller test case is preferred for many purposes. A smaller test case usually runs faster, is easier to understand, and is more convenient for debugging. However, smaller test cases also tend to cover less code and detect fewer faults than larger test cases. Whereas traditional research focused on reducing test suites while preserving code coverage, recent work has introduced the idea of reducing individual test cases, rather than test suites, while still preserving code coverage. Other recent work has proposed non-adequately reducing test suites by not even preserving all the code coverage. This paper empirically evaluates a new combination of these two ideas, non-adequate reduction of test cases, which allows for a wide range of trade-offs between test case size and fault detection. Our study introduces and evaluates C%-coverage reduction (where a test case\u00a0\u2026", "num_citations": "28\n", "authors": ["313"]}
{"title": "Jacontebe: A benchmark suite of real-world java concurrency bugs (T)\n", "abstract": " Researchers have proposed various approaches to detect concurrency bugs and improve multi-threaded programs, but performing evaluations of the effectiveness of these approaches still remains a substantial challenge. We survey the existing evaluations and find out that they often use code or bugs not representative of real world. To improve representativeness, we have prepared JaConTeBe, a benchmark suite of 47 confirmed concurrency bugs from 8 popular open-source projects, supplemented with test cases for reproducing buggy behaviors. Running three approaches on JaConTeBe shows that our benchmark suite confirms some limitations of the three approaches. We submitted JaConTeBe to the SIR repository (a software-artifact repository for rigorous controlled experiments), and it was included as a part of SIR.", "num_citations": "27\n", "authors": ["313"]}
{"title": "Mitigating the effects of flaky tests on mutation testing\n", "abstract": " Mutation testing is widely used in research as a metric for evaluating the quality of test suites. Mutation testing runs the test suite on generated mutants (variants of the code under test), where a test suite kills a mutant if any of the tests fail when run on the mutant. Mutation testing implicitly assumes that tests exhibit deterministic behavior, in terms of their coverage and the outcome of a test (not) killing a certain mutant. Such an assumption does not hold in the presence of flaky tests, whose outcomes can non-deterministically differ even when run on the same code under test. Without reliable test outcomes, mutation testing can result in unreliable results, eg, in our experiments, mutation scores vary by four percentage points on average between repeated executions, and 9% of mutant-test pairs have an unknown status. Many modern software projects suffer from flaky tests. We propose techniques that manage flakiness\u00a0\u2026", "num_citations": "24\n", "authors": ["313"]}
{"title": "Mining container image repositories for software configuration and beyond\n", "abstract": " This paper introduces the idea of mining container image repositories for configuration and other deployment information of software systems. Unlike traditional software repositories (eg, source code repositories and app stores), image repositories encapsulate the entire execution ecosystem for running target software, including its configurations, dependent libraries and components, and OS-level utilities, which contributes to a wealth of data and information. We showcase the opportunities based on concrete software engineering tasks that can benefit from mining image repositories. To facilitate future mining efforts, we summarize the challenges of analyzing image repositories and the approaches that can address these challenges. We hope that this paper will stimulate exciting research agenda of mining this emerging type of software repositories.", "num_citations": "24\n", "authors": ["313"]}
{"title": "X10X: Model checking a new programming language with an\" old\" model checker\n", "abstract": " Parallel and distributed computing is becoming a norm with the advent of multi-core, networked, and cloud computing platforms. New programming languages are emerging for these platforms, e.g., the X10 language from IBM. While these languages explicitly support concurrent programming, they cannot eliminate all concurrency related bugs, which are usually hard to find. Finding such bugs is easier using specialized, language-aware model-checking tools. However, such tools are highly complex and developing them from scratch requires large effort. We describe our experience in developing a model-checking tool for a new language, X10, by systematically adapting an existing tool, the JPF model checker for Java. X10 programs can be compiled to Java, but unfortunately checking X10 programs directly with the unmodified JPF and X10 runtime can miss some behaviors and scales very poorly. We present four\u00a0\u2026", "num_citations": "22\n", "authors": ["313"]}
{"title": "CALKAS: A computer architecture learning and knowledge assessment system\n", "abstract": " The paper presents a Computer Architecture Learning and Knowledge Assessment System named the CALKAS. It is a software tool aimed to be used for teaching Computer architecture and organization. It offers the knowledge assessment and self-learning facilities. The knowledge assessment facilities are meant to be used in laboratory for the lab test and at home for the self-test. The self-learning facilities are meant to be used at home in the process of preparation for the work in the laboratory and for the exam. The CALKAS is developed as a WWW application.", "num_citations": "21\n", "authors": ["313"]}
{"title": "Scientific tests and continuous integration strategies to enhance reproducibility in the scientific software context\n", "abstract": " Continuous integration (CI) is a well-established technique in commercial and open-source software projects, although not routinely used in scientific publishing. In the scientific software context, CI can serve two functions to increase reproducibility of scientific results: providing an established platform for testing the reproducibility of these results, and demonstrating to other scientists how the code and data generate the published results. We explore scientific software testing and CI strategies using two articles published in the areas of applied mathematics and computational physics. We discuss lessons learned from reproducing these articles as well as examine and discuss existing tests. We introduce the notion of a\" scientific test\" as one that produces computational results from a published article. We then consider full result reproduction within a CI environment. If authors find their work too time or resource\u00a0\u2026", "num_citations": "19\n", "authors": ["313"]}
{"title": "A comparison of constraint-based and sequence-based generation of complex input data structures\n", "abstract": " Generation of complex input data structures is one of the challenging tasks in testing. Manual generation of such structures is tedious and error-prone. Automated generation approaches include those based on constraints, which generate structures at the concrete representation level, and those based on sequences of operations, which generate structures at the abstract representation level by inserting or removing elements to or from the structure. In this paper, we compare these two approaches for five complex data structures used in previous research studies. Our experiments show several interesting results. First, constraint-based generation can generate more structures than sequence-based generation. Second, the extra structures can lead to false alarms in testing. Third, some concrete representations of structures cannot be generated only with sequences of insert operations. Fourth, slightly different\u00a0\u2026", "num_citations": "19\n", "authors": ["313"]}
{"title": "Evaluating test-suite reduction in real software evolution\n", "abstract": " Test-suite reduction (TSR) speeds up regression testing by removing redundant tests from the test suite, thus running fewer tests in the future builds. To decide whether to use TSR or not, a developer needs some way to predict how well the reduced test suite will detect real faults in the future compared to the original test suite. Prior research evaluated the cost of TSR using only program versions with seeded faults, but such evaluations do not explicitly predict the effectiveness of the reduced test suite in future builds.", "num_citations": "18\n", "authors": ["313"]}
{"title": "Instantcheck: Checking the determinism of parallel programs using on-the-fly incremental hashing\n", "abstract": " Developing multithreaded programs in shared-memory systems is difficult. One key reason is the nondeterminism of thread interaction, which may result in one code input producing different outputs in different runs. Unfortunately, enforcing determinism by construction typically comes at a performance, hardware, or programmability cost. An alternative is to check during testing whether code is deterministic. This paper presents Instant Check, a novel technique that checks determinism with a very small runtime overhead while requiring only a minor hardware extension. During code testing, Instant-Check can check whether the code under test ends up in a deterministic state in various runs. The idea is to compute a 64-bit hash of the memory state and compare the hashes of different test runs that have the same input. If two runs have different hashes, Instant-Check reports state nondeterminism. For efficient operation\u00a0\u2026", "num_citations": "18\n", "authors": ["313"]}
{"title": "An extensible, regular-expression-based tool for multi-language mutant generation\n", "abstract": " Mutation testing is widely used in research (even if not in practice). Mutation testing tools usually target only one programming language and rely on parsing a program to generate mutants, or operate not at the source level but on compiled bytecode. Unfortunately, developing a robust mutation testing tool for a new language in this paradigm is a difficult and time-consuming undertaking. Moreover, bytecode/intermediate language mutants are difficult for programmers to read and understand. This paper presents a simple tool, called universalmutator, based on regular-expression-defined transformations of source code. The primary drawback of such an approach is that our tool can generate invalid mutants that do not compile, and sometimes fails to generate mutants that a parser-based tool would have produced. Additionally, it is incompatible with some approaches to improving the efficiency of mutation testing\u00a0\u2026", "num_citations": "17\n", "authors": ["313"]}
{"title": "Optimized execution of deterministic blocks in Java PathFinder\n", "abstract": " Java PathFinder (JPF) is an explicit-state model checker for Java programs. It explores all executions that a given program can have due to different thread interleavings and nondeterministic choices. JPF implements a backtracking Java Virtual Machine (JVM) that executes Java bytecodes using a special representation of JVM states. This special representation enables JPF to quickly store, restore, and compare states; it is crucial for making the overall state exploration efficient. However, this special representation creates overhead for each execution, even execution of deterministic blocks that have no thread interleavings or nondeterministic choices.               We propose mixed execution, a technique that reduces execution time of deterministic blocks in JPF. JPF is written in Java as a special JVM that runs on top of a regular, host JVM. mixed execution works by translating the state between the special JPF\u00a0\u2026", "num_citations": "17\n", "authors": ["313"]}
{"title": "Improving generation of object-oriented test suites by avoiding redundant tests\n", "abstract": " Object-oriented tests consist of sequences of method invocations. Behavior of an invocation depends on the state of the receiver object and method arguments at the beginning of the invocation. Existing tools for automatic generation of object-oriented test suites, such as Jtest and JCrasher for Java, typically ignore object states. These tools generate redundant tests that exercise the same method behavior, which increases the testing time without increasing the ability to detect faults.We propose a formal framework for detecting redundant tests and present five fully automatic techniques within this framework. Based on these techniques, we have developed a test-minimization tool that removes redundant tests from test suites and a test-generation tool that iteratively augments test suites with non-redundant tests. We evaluate our tools on eight subjects taken from a variety of sources. The experimental results show that our test minimization can remove over 90% of the tests generated by Jtest for most subjects and 30% of the tests generated by JCrasher for half of the subjects, without decreasing the quality of test suites. The results also show that our test generation can effectively generate new tests that increase the quality of test suites generated by Jtest and JCrasher.", "num_citations": "17\n", "authors": ["313"]}
{"title": "gem5-approxilyzer: An open-source tool for application-level soft error analysis\n", "abstract": " Modern systems are increasingly susceptible to soft errors in the field and traditional redundancy-based mitigation techniques are too expensive to protect against all errors. Recent techniques, such as approximate computing and various low-cost resilience mechanisms, intelligently trade off inaccuracy in program output for better energy, performance, and resiliency overhead. A fundamental requirement for realizing the full potential of these techniques is a thorough understanding of how applications react to errors. Approxilyzer is a state-of-the-art tool that enables an accurate, efficient, and comprehensive analysis of how errors in almost all dynamic instructions in a program's execution affect the quality of the final program output. While useful, its adoption is limited by its implementation using the proprietary Simics infrastructure and the SPARC ISA. We present gem5-Approxilyzer, a re-implementation of\u00a0\u2026", "num_citations": "16\n", "authors": ["313"]}
{"title": "Structure or nurture? the effects of team-building activities and team composition on team outcomes\n", "abstract": " How can instructors group students into teams that interact and learn effectively together? One strand of research advocates for grouping students into teams with \"good\" compositions such as skill diversity. Another strand argues for deploying team-building activities to foster interpersonal relations like psychological safety. Our work synthesizes these two strands of research. We describe an experiment (N=249) that compares how team composition vs. team-building activities affect student team outcomes. In two university courses, we composed student teams either randomly or using a criteria-based team formation tool. Teams further performed team-building activities that promoted either team or task outcomes. We collected project scores, and used surveys to measure psychological safety, perceived performance, and team satisfaction. Surprisingly, the criteria-based teams did not statistically differ from the\u00a0\u2026", "num_citations": "16\n", "authors": ["313"]}
{"title": "CoDeSe: fast deserialization via code generation\n", "abstract": " Many tools for automated testing, model checking, and debugging store and restore program states multiple times. Storing/restoring a program state is commonly done with serialization/deserialization. Traditionally, the format for stored states is based on data: serialization generates the data that encodes the state, and deserialization interprets this data to restore the state. We propose a new approach, called CoDeSe, where the format for stored states is based on code: serialization generates code whose execution restores the state, and deserialization simply executes the code. We implemented CoDeSe in Java and performed a number of experiments on deserialization of states. CoDeSe provides on average more than 6X speedup over the highly optimized deserialization from the standard Java library. Our new format also allows simple parallel deserialization that can provide additional speedup on top of the\u00a0\u2026", "num_citations": "16\n", "authors": ["313"]}
{"title": "Minotaur: Adapting software testing techniques for hardware errors\n", "abstract": " With the end of conventional CMOS scaling, efficient resiliency solutions are needed to address the increased likelihood of hardware errors. Silent data corruptions (SDCs) are especially harmful because they can create unacceptable output without the user's knowledge. Several resiliency analysis techniques have been proposed to identify SDC-causing instructions, but they remain too slow for practical use and/or sacrifice accuracy to improve analysis speed. We develop Minotaur, a novel toolkit to improve the speed and accuracy of resiliency analysis. The key insight behind Minotaur is that modern resiliency analysis has many conceptual similarities to software testing; therefore, adapting techniques from the rich software testing literature can lead to principled and significant improvements in resiliency analysis. Minotaur identifies and adapts four concepts from software testing: 1) it introduces the concept of input\u00a0\u2026", "num_citations": "15\n", "authors": ["313"]}
{"title": "Evaluating regression test selection opportunities in a very large open-source ecosystem\n", "abstract": " Regression testing in very large software ecosystems is notoriously costly, requiring computational resources that even large corporations struggle to cope with. Very large ecosystems contain thousands of rapidly evolving, interconnected projects where client projects transitively depend on library projects. Regression test selection (RTS) reduces regression testing costs by rerunning only tests whose pass/fail behavior may flip after code changes. For single projects, researchers showed that class-level RTS is more effective than lower method-or statement-level RTS. Meanwhile, several very large ecosystems in industry, e.g., at Facebook, Google, and Microsoft, perform project-level RTS, rerunning tests in a changed library and in all its transitive clients. However, there was no previous study of the comparative benefits of class-level and project-level RTS in such ecosystems. We evaluate RTS opportunities in the\u00a0\u2026", "num_citations": "15\n", "authors": ["313"]}
{"title": "Efficient mutation testing of multithreaded code\n", "abstract": " Mutation testing is a well\u2010established method for measuring and improving the quality of test suites. A major cost of mutation testing is the time required to execute the test suite on all the mutants. This cost is even greater when the system under test is multithreaded: not only are test cases from the test suite executed on many mutants but also each test case is executed\u2014or more precisely, explored\u2014for multiple possible thread schedules. This paper introduces a general framework for efficient exploration that can reduce the time for mutation testing of multithreaded code. The paper presents five techniques (four optimizations and one heuristic) that are implemented in a tool called MuTMuT within the general framework. Evaluation of MuTMuT on mutation testing of 12 multithreaded programs shows that it can substantially reduce the time required for mutation testing of multithreaded code.Copyright \u00a9 2012 John\u00a0\u2026", "num_citations": "15\n", "authors": ["313"]}
{"title": "Split temporal/spatial cache: A survey and reevaluation of performance\n", "abstract": " The purpose of this paper is to reevaluate the performance of the Split Temporal/Spatial (STS) cache. First we briefly survey the split cache designs found in the open literature. Then we propose quantitative definitions for both temporal and spatial locality. These definitions can be used to represent each split cache design (or any other method for optimized locality exploitation) as a line in a temporal-spatial locality plane. Then we explain the particular process used to evaluate the STS cache design, and finally we present the results of that evaluation. We conclude with possible improvements pointed to by our evaluation results.", "num_citations": "15\n", "authors": ["313"]}
{"title": "Reflection-aware static regression test selection\n", "abstract": " Regression test selection (RTS) aims to speed up regression testing by rerunning only tests that are affected by code changes. RTS can be performed using static or dynamic analysis techniques. Our prior study showed that static and dynamic RTS perform similarly for medium-sized Java projects. However, the results of that prior study also showed that static RTS can be unsafe, missing to select tests that dynamic RTS selects, and that reflection was the only cause of unsafety observed among the evaluated projects.   In this paper, we investigate five techniques\u2014three purely static techniques and two hybrid static-dynamic techniques\u2014that aim to make static RTS safe with respect to reflection. We implement these reflection-aware (RA) techniques by extending the reflection-unaware (RU) class-level static RTS technique in a tool called STARTS. To evaluate these RA techniques, we compare their end-to-end times\u00a0\u2026", "num_citations": "13\n", "authors": ["313"]}
{"title": "Understanding and improving regression test selection in continuous integration\n", "abstract": " Developers rely on regression testing in their continuous integration (CI) environment to find changes that introduce regression faults. While regression testing is widely practiced, it can be costly. Regression test selection (RTS) reduces the cost of regression testing by not running the tests that are unaffected by the changes. Industry has adopted module-level RTS for their CI environment, while researchers have proposed class-level RTS. In this paper, we compare module-and class-level RTS techniques in a cloud-based CI environment, Travis. We also develop and evaluate a hybrid RTS technique that combines aspects of the module-and class-level RTS techniques. We evaluate all the techniques on real Travis builds. We find that the RTS techniques do save testing time compared to running all tests (RetestAll), but the percentage of time for a full build using RTS (76.0%) is not as low as found in previous work\u00a0\u2026", "num_citations": "12\n", "authors": ["313"]}
{"title": "Comparing mutation testing at the levels of source code and compiler intermediate representation\n", "abstract": " Mutation testing is widely used in research for evaluating the effectiveness of test suites. There are multiple mutation tools that perform mutation at different levels, including traditional mutation testing at the level of source code (SRC) and more recent mutation testing at the level of compiler intermediate representation (IR). This paper presents an extensive comparison of mutation testing at the SRC and IR levels, specifically at the C programming language and the LLVM compiler IR levels. We use a mutation testing tool called SRCIROR that implements conceptually the same mutation operators at both levels. We also employ automated techniques to account for equivalent and duplicated mutants, and to determine minimal and surface mutants. We carry out our study on 15 programs from the Coreutils library. Overall, we find mutation testing to be better at the SRC level: the SRC level produces much fewer mutants\u00a0\u2026", "num_citations": "12\n", "authors": ["313"]}
{"title": "Speculation invariance (invarspec): Faster safe execution through program analysis\n", "abstract": " Many hardware-based defense schemes against speculative execution attacks use special mechanisms to protect instructions while speculative, and lift the mechanisms when the instructions turn non-speculative. In this paper, we observe that speculative instructions can sometimes become Speculation Invariant before turning non-speculative. Speculation invariance means that (i) whether the instruction will execute and (ii) the instruction\u2019s operands are not a function of speculative state. Hence, we propose to lift the protection mechanisms on these instructions early, when they become speculation invariant, and issue them without protection. As a result, we improve the performance of the defense schemes without changing their security properties.To exploit speculation invariance, we present the InvarSpec framework. InvarSpec includes a program analysis pass that identifies, for each relevant instruction i, the\u00a0\u2026", "num_citations": "11\n", "authors": ["313"]}
{"title": "Understanding reproducibility and characteristics of flaky tests through test reruns in java projects\n", "abstract": " Flaky tests are tests that can non-deterministically pass and fail. They pose a major impediment to regression testing, because they provide an inconclusive assessment on whether recent code changes contain faults or not. Prior studies of flaky tests have proposed tools to detect flaky tests and identified various sources of flakiness in tests, e.g., order-dependent (OD) tests that deterministically fail for some order of tests in a test suite but deterministically pass for some other orders. Several of these studies have focused on OD tests. We focus on an important and under-explored source of flakiness in tests: non-order-dependent tests that can nondeterministically pass and fail even for the same order of tests. Instead of using specialized tools that aim to detect flaky tests, we run tests using the tool configured by the developers. Specifically, we perform our empirical evaluation on Java projects that rely on the Maven\u00a0\u2026", "num_citations": "11\n", "authors": ["313"]}
{"title": "NonDex: A tool for detecting and debugging wrong assumptions on Java API specifications\n", "abstract": " We present NonDex, a tool for detecting and debugging wrong assumptions on Java APIs. Some APIs have underdetermined specifications to allow implementations to achieve different goals, eg, to optimize performance. When clients of such APIs assume stronger-than-specified guarantees, the resulting client code can fail. For example, HashSet\u2019s iteration order is underdetermined, and code assuming some implementation-specific iteration order can fail. NonDex helps to proactively detect and debug such wrong assumptions. NonDex performs detection by randomly exploring different behaviors of underdetermined APIs during test execution. When a test fails during exploration, NonDex searches for the invocation instance of the API that caused the failure. NonDex is open source, well-integrated with Maven, and also runs from the command line. During our experiments with the NonDex Maven plugin, we\u00a0\u2026", "num_citations": "10\n", "authors": ["313"]}
{"title": "Setac: A framework for phased deterministic testing of Scala actor programs\n", "abstract": " Scala provides an actor library where computation entities, called actors, communicate by exchanging messages. The schedule of message exchanges is in general nondeterministic. Testing non-deterministic programs is hard, because it is necessary to ensure that the system under test has executed all important schedules. Setac is our proposed framework for testing Scala actors that (1) allows programmers to specify constraints on schedules and (2) makes it easy to check test assertions that require actors to be in a stable state. Setac requires little change to the program under test and requires no change to the actor run-time system. In sum, Setac aims to make it much simpler to test nondeterministic actor programs in Scala.", "num_citations": "9\n", "authors": ["313"]}
{"title": "Evaluating machine-independent metrics for state-space exploration\n", "abstract": " Many recent advancements in testing concurrent programs can be described as novel optimization and heuristic techniques for exploring the tests of such programs. To empirically evaluate these techniques, researchers apply them on subject programs and capture a set of metrics that characterize the techniques' effectiveness. From a user's perspective, the most important metric is often the amount of real time required to find a error (if one exists), but using real time for comparison can be misleading because it is necessarily dependent on the machine configuration used for the experiments. On the other hand, using machine-independent metrics can be meaningless if they do not correlate highly with real time. As a result, it can be difficult to select metrics for valid comparisons among exploration techniques. This paper presents a study of the commonly used machine-independent metrics for two different\u00a0\u2026", "num_citations": "6\n", "authors": ["313"]}
{"title": "Scowl: a tool for characterization of parallel workload and its use on SPLASH-2 application suite\n", "abstract": " Concentrates on the problem of defining and measuring parameters that characterize typical behavior of parallel applications targeted to distributed shared memory (DSM) systems and shared-memory multiprocessors (SMPs). These parameters can be used as input to various models for performance evaluation in this research area. Furthermore, typical application behaviors can be recognized, which can help to generate new ideas for improvements to memory consistency protocols, adapting them to specific application characteristics. Our study encompasses a variety of parameters, such as frequencies of operations of various access types (private read/writes, shared read/writes, lock operations, barrier operations), the average number of accessed blocks per interval, the average number of modified words, etc. The results presented in this paper are based on the SPLASH-2 (Stanford Parallel Applications for\u00a0\u2026", "num_citations": "6\n", "authors": ["313"]}
{"title": "Credible compilation\n", "abstract": " This thesis describes a theoretical framework for building compilers that generate formal guarantees that they work correctly. Traditional compilers provide no such guarantees-given an original source program, a traditional compiler generates only a transformed executable program. The only way to investigate the correctness of a compilation is to run the transformed program on some sample inputs. Even if the transformed program generates expected results for these inputs, it does not ensure that the transformed program is indeed equivalent to the original program for all inputs.Most previous research on compiler correctness focused on developing compilers that are guaranteed to correctly translate every original program. It is extremely difficult, however, to verify that a complex code, which implements a compiler, is correct. Therefore, a novel approach was proposed: instead of verifying a compiler, verify the result of each single compilation. We require the compiler to generate a transformed program and some additional information that enables a simple verifier to check the compilation. We call this approach credible compilation. This thesis presents a formal framework for the credible compilation of imperative programming languages. Each transformation generates, in addition to a transformed program, a set of standard invariants and contexts, which the compiler uses to prove that its analysis results are correct, and a set of simulation invariants and contexts, which the compiler uses to prove that the transformed program is equivalent to the original program. The compiler has also to generate a proof for all the invariants and contexts. We\u00a0\u2026", "num_citations": "6\n", "authors": ["313"]}
{"title": "Domain-Specific Fixes for Flaky Tests with Wrong Assumptions on Underdetermined Specifications\n", "abstract": " Library developers can provide classes and methods with underdetermined specifications that allow flexibility in future implementations. Library users may write code that relies on a specific implementation rather than on the specification, e.g., assuming mistakenly that the order of elements cannot change in the future. Prior work proposed the NonDex approach that detects such wrong assumptions. We present a novel approach, called DexFix, to repair wrong assumptions on underdetermined specifications in an automated way. We run the NonDex tool on 200 open-source Java projects and detect 275 tests that fail due to wrong assumptions. The majority of failures are from iterating over HashMap/HashSet collections and the getDeclaredFields method. We provide several new repair strategies that can fix these violations in both the test code and the main code. DexFix proposes fixes for 119 tests from the\u00a0\u2026", "num_citations": "4\n", "authors": ["313"]}
{"title": "Tempura: Temporal dimension for ides\n", "abstract": " Modern integrated development environments (IDEs) make many software engineering tasks easier by providing automated programming support such as code completion and navigation. However, such support -- and therefore IDEs as awhole -- operate on one revision of the code at a time, and leave handling of code history to external tools or plugins, such asEGit for Eclipse. For example, when a method is removed froma class, developers can no longer find the method through code completion. This forces developers to manually switch across different revisions or resort to using external tools when they need to learn about previous code revisions.We propose a novel approach of adding a temporal dimensionto IDEs, enabling code completion and navigation to operate on multiple revisions of code at a time. We previously introduced the idea of temporal code completion and navigation,and presented a vision\u00a0\u2026", "num_citations": "4\n", "authors": ["313"]}
{"title": "Can We Trust Test Outcomes?\n", "abstract": " Software development is an on-going process. To ensure that the changes made do not break previous functionality, a regression test suite is maintained along with the source code. The correctness and the side effects of code changes are judged based on the output of the tests. However, are these tests\u2019 outcomes always correct? Does a test failure imply a bug and do all tests passing imply a bug free code? The answer is no. Experienced researchers and practitioners know that there is an inherent non-determinism in testing. This paper contributes the first comprehensive study on test outcome non-determinism. We target 153 open-source projects with a novel methodology starting from the commit logs, which enabled us to identify many unreported cases of tests with nondeterministic outcomes (also called flaky tests). We were able to detect 1129 commits from 51 projects that are related to flaky tests, out of which we study 486 commits which are about distinct flaky tests. 61% of these flaky tests do not have associated bug reports. In our study, we focus on 81 commits to analyze in depth. We identify 11 causes of flakiness, provide examples for each, and give recommendations on how to fix them. We also discuss other interesting findings such as the effectiveness of common fixes, the evolution of flaky tests, how hard it is to reproduce a flaky test, and how hard it is to patch it.", "num_citations": "4\n", "authors": ["313"]}
{"title": "Automatic detection of duplicate bug reports using word semantics\n", "abstract": " We present in this paper a novel approach to the task of duplicate bug report identification. We map the task onto a text-to-text similarity problem where the challenge is to quantify the semantic similarity of two documents, ie bug reports. If the reports are similar enough they are deemed duplicates, otherwise they are not. The proposed approach and evaluation framework has two novel features:(1) uses a balanced data set, containing same number of positive and negative instances, which allows for a more fair comparison of different methods, and (2) relies on word-to-word similarity measures to detect the semantic similarity of reports as opposed to the vector space model, the method of choice in previous attempts to the task of duplicate report identification. We use 10 knowledge-based similarity measures in our experiments on a balanced dataset of 2000 bug reports (1000 duplicates and 1000 non-duplicates\u00a0\u2026", "num_citations": "4\n", "authors": ["313"]}
{"title": "Probabilistic and Systematic Coverage of Consecutive Test-Method Pairs for Detecting Order-Dependent Flaky Tests.\n", "abstract": " Software developers frequently check their code changes by running a set of tests against their code. Tests that can nondeterministically pass or fail when run on the same code version are called flaky tests. These tests are a major problem because they can mislead developers to debug their recent code changes when the failures are unrelated to these changes. One prominent category of flaky tests is order-dependent (OD) tests, which can deterministically pass or fail depending on the order in which the set of tests are run. By detecting OD tests in advance, developers can fix these tests before they change their code. Due to the high cost required to explore all possible orders (n! permutations for n tests), prior work has developed tools that randomize orders to detect OD tests. Experiments have shown that randomization can detect many OD tests, and that most OD tests depend on just one other test to fail. However, there was no analysis of the probability that randomized orders detect OD tests. In this paper, we present the first such analysis and also present a simple change for sampling random test orders to increase the probability. We finally present a novel algorithm to systematically explore all consecutive pairs of tests, guaranteeing to detect all OD tests that depend on one other test, while running substantially fewer orders and tests than simply running all test pairs.", "num_citations": "3\n", "authors": ["313"]}
{"title": "Using coverage criteria on repOK to reduce bounded-exhaustive test suites\n", "abstract": " Bounded-exhaustive exploration of test case candidates is a commonly employed approach for test generation in some contexts. Even when small bounds are used for test generation, executing the obtained tests may become prohibitive, despite the time for test generation not being prohibitive. In this paper, we propose a technique for reducing the size of bounded-exhaustive test suites. This technique is based on the application of coverage criteria on the representation invariant of the structure for which the suite was produced. More precisely, the representation invariant (which is often implemented as a repOK routine) is executed to determine how its code is exercised by (valid) test inputs. Different valid test inputs are deemed equivalent if they exercise the repOK code in a similar way according to a white-box testing criterion. These equivalences between test cases are exploited for reducing test suites\u00a0\u2026", "num_citations": "3\n", "authors": ["313"]}
{"title": "Performance evaluation of split temporal/spatial caches: Paving the way to new solutions\n", "abstract": " The purpose of this paper is to describe methods used to evaluate performance of split temporal/spatial caches. Previous work in this area mostly describes the architecture of the caches themselves and presents only the performance results, without saying much to explain the method used to obtain those results. Therefore, we first briefly survey the methods used in previous papers on split caches, then explain the particular process used to evaluate one of the designs, and finally present the results of that evaluation. We conclude with possible improvements to that cache design pointed to by our evaluation results.", "num_citations": "3\n", "authors": ["313"]}
{"title": "An approach to characterization of parallel applications for dsm systems\n", "abstract": " The paper concentrates on the problem of defining and measuring parameters that characterize behavior of parallel applications targeted to DSM (distributed shared memory) systems. Results are based on the SPLASH-2 application suite. The developed characterization tool Scopa, along with applied simulation environment Limes, are publicly available, and appropriate for performing measurements on other parallel applications, as well.", "num_citations": "3\n", "authors": ["313"]}
{"title": "Guidelines for Coverage-Based Comparisons of Non-Adequate Test Suites\n", "abstract": " Coverage: Coverage\u2019Coverage-varied Selection Size-varied Selection Java C Java C \u03c4b R2 \u03c4b R2 \u03c4b R2 \u03c4b R2 BC: IMP 11: 4 12: 3 10: 1 11: 0 9: 6 10: 5 8: 3 9: 2 BC: PCT (BB) 12: 3 7: 8 9: 2 6: 5 9: 6 6: 9 6: 5 6: 5 BC: PCT (ST) 12: 3 7: 8 9: 2 6: 5 8: 7 7: 8 5: 6 6: 5 BC: SC 11: 4 13: 2 4: 7 11: 0 10: 5 11: 4 5: 6 9: 2 BC: AIMP 10: 5 5: 10 8: 3 5: 6 5: 10 5: 10 5: 6 2: 9 IMP: PCT (BB) 4: 11 3: 12 4: 7 0: 11 6: 9 3: 12 5: 6 1: 10 IMP: PCT (ST) 6: 9 2: 13 5: 6 0: 11 6: 9 2: 13 5: 6 1: 10 IMP: SC 8: 7 11: 4 3: 8 0: 11 11: 4 12: 3 3: 8 2: 9 IMP: AIMP 3: 12 2: 13 1: 10 1: 10 2: 13 1: 14 3: 8 1: 10 PCT (BB): PCT (ST) 8: 7 7: 8 5: 6 6: 5 10: 5 9: 6 5: 6 6: 5 PCT (BB): SC 4: 11 12: 3 2: 9 6: 5 10: 5 12: 3 4: 7 6: 5 PCT (BB): AIMP 6: 9 7: 8 3: 8 4: 7 3: 12 6: 9 4: 7 5: 6 PCT (ST): SC 7: 8 9: 6 3: 8 5: 6 9: 6 11: 4 6: 5 7: 4 PCT (ST): AIMP 7: 8 7: 8 4: 7 4: 7 2: 13 4: 11 5: 6 4: 7 SC: AIMP 7: 8 4: 11 7: 4 4: 7 4: 11 3: 12 5: 6 2: 9", "num_citations": "2\n", "authors": ["313"]}
{"title": "@ tcomment: Testing javadoc comments to detect comment-code inconsistencies\n", "abstract": " @tComment: Testing Javadoc Comments to Detect Comment-Code Inconsistencies - NASA/ADS Now on home page ads icon ads Enable full ADS view NASA/ADS @tComment: Testing Javadoc Comments to Detect Comment-Code Inconsistencies Hwei Tan, Shin ; Marinov, Darko ; Tan, Lin ; Leavens, Gary T. Abstract This paper has been withdrawn by the author. Publication: arXiv e-prints Pub Date: January 2012 arXiv: arXiv:1201.6078 Bibcode: 2012arXiv1201.6078H Keywords: Computer Science - Software Engineering E-Print: This paper has been withdrawn by the author full text sources arXiv | \u00a9 The SAO/NASA Astrophysics Data System adshelp[at]cfa.harvard.edu The ADS is operated by the Smithsonian Astrophysical Observatory under NASA Cooperative Agreement NNX16AC86A NASA logo Smithsonian logo Resources About ADS ADS Help What's New Careers@ADS Social @adsabs ADS Blog Project (\u2026", "num_citations": "2\n", "authors": ["313"]}
{"title": "Generating trees of (reducible) 1324-avoiding permutations\n", "abstract": " We consider permutations that avoid the pattern 1324. We give exact formulas for thenumber of reducible 1324-avoiding permutations and the number of {1324, 4132, 2413, 3241}-avoiding permutations. By studying the generating tree for all 1324-avoiding permutations,we obtain a recurrence formula for their number. A computer program provides data for thenumber of 1324-avoiding permutations of length up to 20.", "num_citations": "2\n", "authors": ["313"]}
{"title": "Test-case prioritization for configuration testing\n", "abstract": " Configuration changes are among the dominant causes of failures of large-scale software system deployment. Given the velocity of configuration changes, typically at the scale of hundreds to thousands of times daily in modern cloud systems, checking these configuration changes is critical to prevent failures due to misconfigurations. Recent work has proposed configuration testing, Ctest, a technique that tests configuration changes together with the code that uses the changed configurations. Ctest can automatically generate a large number of ctests that can effectively detect misconfigurations, including those that are hard to detect by traditional techniques. However, running ctests can take a long time to detect misconfigurations. Inspired by traditional test-case prioritization (TCP) that aims to reorder test executions to speed up detection of regression code faults, we propose to apply TCP to reorder ctests to speed\u00a0\u2026", "num_citations": "1\n", "authors": ["313"]}
{"title": "Learning from reproducing computational results: introducing three principles and the Reproduction Package\n", "abstract": " We carry out efforts to reproduce computational results for seven published articles and identify barriers to computational reproducibility. We then derive three principles to guide the practice and dissemination of reproducible computational research: (i) Provide transparency regarding how computational results are produced; (ii) When writing and releasing research software, aim for ease of (re-)executability; (iii) Make any code upon which the results rely as deterministic as possible. We then exemplify these three principles with 12 specific guidelines for their implementation in practice. We illustrate the three principles of reproducible research with a series of vignettes from our experimental reproducibility work. We define a novel Reproduction Package, a formalism that specifies a structured way to share computational research artifacts that implements the guidelines generated from our reproduction efforts to allow\u00a0\u2026", "num_citations": "1\n", "authors": ["313"]}
{"title": "LIFT: integrating stakeholder voices into algorithmic team formation\n", "abstract": " Team formation tools assume instructors should configure the criteria for creating teams, precluding students from participating in a process affecting their learning experience. We propose LIFT, a novel learner-centered workflow where students propose, vote for, and weigh the criteria used as inputs to the team formation algorithm. We conducted an experiment (N= 289) comparing LIFT to the usual instructor-led process, and interviewed participants to evaluate their perceptions of LIFT and its outcomes. Learners proposed novel criteria not included in existing algorithmic tools, such as organizational style. They avoided criteria like gender and GPA that instructors frequently select, and preferred those promoting efficient collaboration. LIFT led to team outcomes comparable to those achieved by the instructor-led approach, and teams valued having control of the team formation process. We provide instructors and\u00a0\u2026", "num_citations": "1\n", "authors": ["313"]}
{"title": "A case study on executing instrumented code in Java PathFinder\n", "abstract": " Dynamic program analysis is widely used for detecting faults in software systems. Dynamic analysis tools conceptually either use a modified execution environment or inject instrumentation code into the system under test (SUT). Tools based on a modified environment, such as Java PathFinder (JPF), have full access to the state of the SUT but at the cost of a higher runtime overhead. Instrumentation-based tools have lower overhead but at the cost of less convenient control of runtime such as thread schedules. Combinations of these two approaches are largely unexplored, and to our knowledge, have never been done in JPF. We present a case study of adapting an existing instrumentation-based tool to run inside JPF. To keep the instrumentation unchanged, we limited our changes to the code invoked by the instrumentation. Ultimately, the required changes were few and essentially reduce to properly dividing the\u00a0\u2026", "num_citations": "1\n", "authors": ["313"]}
{"title": "RepOK\u2010based reduction of bounded exhaustive testing\n", "abstract": " While the effectiveness of bounded exhaustive test suites increases as one increases the scope for the bounded exhaustive generation, both the time for test generation and the time for test execution grow exponentially with respect to the scope. In this article, a set of techniques for reducing the time for bounded exhaustive testing, by either reducing the generation time or reducing the obtained bounded exhaustive suites, is proposed. The representation invariant of the software under test's input, implemented as a repOK routine, is exploited for these reductions in two ways: (i) to factor out separate representation invariants for disjoint structures of the inputs; and (ii) to partition valid inputs into equivalence classes, according to how these exercise the repOK code. The first is used in order to split the test input generation process, as disjoint substructures can be independently generated. The second is used in order to\u00a0\u2026", "num_citations": "1\n", "authors": ["313"]}
{"title": "Parallel Computing Research at Illinois The UPCRC Agenda\n", "abstract": " For many decades, Moore\u2019s law has bestowed a wealth of transistors that hardware designers and compiler writers have converted to usable performance, without changing the sequential programming interface. The main techniques for these performance benefits\u2014increased clock frequency and smarter but increasingly complex architectures\u2014are now hitting the so-called power wall. The computer industry has accepted that future performance increases must largely come from increasing the number of processors (or cores) on a die, rather than making a single core go faster. This historic shift to multicore processors changes the programming interface by exposing parallelism to the programmer, after decades of sequential computing.Parallelism has been successfully used in many domains such as high performance computing (HPC), servers, graphics accelerators, and many embedded systems. The multicore inflection point, however, affects the entire market, particularly the client space, where parallelism has not been previously widespread. Programs with millions of lines of code must be converted or rewritten to take advantage of parallelism; yet, as practiced today, parallel programming for the client is a difficult task performed by few programmers. Commonly used programming models are prone to subtle, hard to reproduce bugs, and parallel programs are notoriously hard to test due to data races, non-deterministic interleavings, and complex memory models. Mapping a parallel application to parallel hardware is also difficult given the large number of degrees of freedom (how many cores to use, whether to use special instructions or\u00a0\u2026", "num_citations": "1\n", "authors": ["313"]}
{"title": "The Split Spatial/Non-Spatial Cache: A Performance and Complexity Evaluation\n", "abstract": " A simple new method of detecting useful spatial locality is proposed in this paper. The new method is tested by incorporating it into a new split cache design. Complexity estimation and performance evaluation of the new split cache design is done in order to compare it to the conventional cache architecture and the split temporal/spatial cache design.", "num_citations": "1\n", "authors": ["313"]}