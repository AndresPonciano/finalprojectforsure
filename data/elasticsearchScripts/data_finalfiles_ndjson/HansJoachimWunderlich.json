{"title": "Minimized Power Consumption for Scan-Based BIST\n", "abstract": " Power consumption of digital systems may increase significantly during testing. In this paper, systems equipped with a scan-based built-in self-test like the STUMPS architecture are analyzed, the modules and modes with the highest power consumption are identified, and design modifications to reduce power consumption are proposed. The design modifications include some gating logic for masking the scan path activity during shifting, and the synthesis of additional logic for suppressing random patterns which do not contribute to increase the fault coverage. These design changes reduce power consumption during BIST by several orders of magnitude, at very low cost in terms of area and performance.", "num_citations": "501\n", "authors": ["507"]}
{"title": "Bit-flipping BIST\n", "abstract": " A scan-based BIST scheme is presented which guarantees complete fault coverage with very low hardware overhead. A probabilistic analysis shows that the output of an LFSR which feeds a scan path has to be modified only at a few bits in order to transform the random patterns into a complete test set. These modifications may be implemented by a bit-flipping function which has the LFSR-state as an input, and flips the value shifted into the scan path at certain times. A procedure is described for synthesizing the additional bit-flipping circuitry, and the experimental results indicate that this mixed-mode BIST scheme requires less hardware for complete fault coverage than all the other scan-based BIST approaches published so far.", "num_citations": "299\n", "authors": ["507"]}
{"title": "Multiple distributions for biased random test patterns\n", "abstract": " The test of integrated circuits by random patterns is very attractive, since no expensive test pattern generation is necessary and tests can be applied with a self-test technique or externally using linear feedback shift registers. Unfortunately, not all circuits are random-testable, because either the fault coverage is too low or the required test length too large. In many cases the random test lengths can be reduced by orders of magnitude using weighted random patterns. However, there are also some circuits for which no single optimal set of weights exists. A set of weights defines a distribution of the random patterns. It is shown that the problem can be solved using several distributions instead of a single one, and an efficient procedure for computing the optimized input probabilities is presented. If a sufficient number of distributions is applied, then all combinational circuits can be tested randomly with moderate test\u00a0\u2026", "num_citations": "296\n", "authors": ["507"]}
{"title": "Power-aware design-for-test\n", "abstract": " This chapter describes Design-for-Test (DfT) techniques that allow for controlling the power consumption and reduce the overall energy consumed during a test. While some of the techniques described elsewhere in this book may also involve special DfT, the topics discussed here are orthogonal to those techniques and may be implemented independently.", "num_citations": "269\n", "authors": ["507"]}
{"title": "A modified clock scheme for a low power BIST test pattern generator\n", "abstract": " In this paper, we present a new low power test-per-clock BIST test pattern generator that provides test vectors which can reduce the switching activity during test operation. The proposed low power/energy BIST technique is based on a modified clock scheme for the TPG and the clock tree feeding the TPG. Numerous advantages can be found in applying such a technique during BIST.", "num_citations": "207\n", "authors": ["507"]}
{"title": "Pattern generation for a deterministic BIST scheme\n", "abstract": " Recently a deterministic built-in self-test scheme has been presented based on reseeding of multiple-polynomial linear feedback shift registers. This scheme encodes deterministic test sets at distinctly lower costs than previously known approaches. In this paper it is shown how this scheme can be supported during test pattern generation. The presented ATPG algorithm generates test sets which can be encoded very efficiently. Experiments show that the area required for synthesizing a BIST scheme that encodes these patterns is significantly less than the area needed for storing a compact test set. Furthermore, it is demonstrated that the proposed approach of combining ATPG and BIST synthesis leads to a considerably reduced hardware overhead compared to encoding a conventionally generated test set.", "num_citations": "198\n", "authors": ["507"]}
{"title": "A mixed mode BIST scheme based on reseeding of folding counters\n", "abstract": " In this paper a new scheme for deterministic and mixed mode scan-based BIST is presented. It relies on a new type of test pattern generator which resembles a programmable Johnson counter and is called folding counter. Both the theoretical background and practical algorithms are presented to characterize a set of deterministic test cubes by a reasonably small number of seeds for a folding counter. Combined with classical approaches for test width compression and with pseudo-random pattern generation these new techniques provide an efficient and flexible solution for scan-based BIST. Experimental results show that the proposed scheme outperforms previously published approaches based on the reseeding of LFSRs or Johnson counters.", "num_citations": "189\n", "authors": ["507"]}
{"title": "Low power serial built-in self-test\n", "abstract": " CiNii \u8ad6\u6587 - Low power serial built-in self-test CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3 ] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7 \u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 Low power serial built-in self-test HERTWIG A. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 HERTWIG A. \u53ce\u9332\u520a\u884c\u7269 Proceedings of IEEE European Test Workshop, 1998 Proceedings of IEEE European Test Workshop, 1998, 1998 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u30b9\u30ad\u30e3\u30f3\u30c6\u30b9\u30c8\u306b\u304a\u3051\u308b\u30ad\u30e3\u30d7\u30c1\u30e3\u6642\u306e\u4f4e \u6d88\u8cbb\u96fb\u529b\u5316\u306b\u52b9\u679c\u7684\u306a\u30c6\u30b9\u30c8\u96c6\u5408\u5909\u66f4\u306b\u3064\u3044\u3066 \u9234\u6728 \u9054\u4e5f , \u6e29 \u6681\u9752 , \u68b6\u539f \u8aa0\u53f8 , \u5bae\u702c \u7d18\u5e73 , \u7686\u672c \u7fa9\u5f18 \u60c5\u5831\u51e6\u7406\u5b66\u4f1a\u7814\u7a76\u5831\u544a. SLDM, [\u30b7\u30b9\u30c6\u30e0LSI\u8a2d\u8a08\u6280\u8853] 122, 133-138, 2005-11-30 \u53c2\u8003 \u6587\u732e26\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10016951610 \u8cc7\u6599\u7a2e\u5225 \u4f1a\u8b70\u8cc7\u6599 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 \u2026", "num_citations": "171\n", "authors": ["507"]}
{"title": "Two-dimensional test data compression for scan-based deterministic BIST\n", "abstract": " In this paper a novel architecture for scan-based mixed mode BIST is presented. To reduce the storage requirements for the deterministic patterns it relies on a two-dimensional compression scheme, which combines the advantages of known vertical and horizontal compression techniques. To reduce both the number of patterns to be stored and the number of bits to be stored for each pattern, deterministic test cubes are encoded as seeds of an LFSR (horizontal compression), and the seeds are again compressed into seeds of a folding counter sequence (vertical compression). The proposed BIST architecture is fully compatible with standard scan design, simple and flexible, so that sharing between several logic cores is possible. Experimental results show that the proposed scheme requires less test data storage than previously published approaches providing the same flexibility and scan compatibility.", "num_citations": "155\n", "authors": ["507"]}
{"title": "PROTEST: A tool for probabilistic testability analysis\n", "abstract": " The CAD-tool PROTEST (Probabilistic Testability Analysis) is presented. PROTEST estimates for each fault of a combinational circuit its detection probability which can be used as a testability measure. Moreover it calculates the number of random test patterns which must be generated in order to achieve the required fault coverage. It is also demonstrated that the fault coverage will increase and the necessary number of random patterns will drastically decrease, if each primary input is stimulated by test patterns having specific probabilities of being logical \"1\". PROTEST uses this fact and determines for each input the optimal signal probability for a randomly generated pattern.", "num_citations": "144\n", "authors": ["507"]}
{"title": "Adaptive Debug and Diagnosis Without Fault Dictionaries\n", "abstract": " Diagnosis is essential in modern chip production to increase yield, and debug constitutes a major part in the pre-silicon development process. For recent process technologies, defect mechanisms are increasingly complex, and continuous efforts are made to model these defects by using sophisticated fault models. Traditional static approaches for debug and diagnosis with a simplified fault model are more and more limited. In this paper, a method is presented, which identifies possible faulty regions in a combinational circuit, based on its input/output behavior and independent of a fault model. The new adaptive, statistical approach is named POINTER for \u2018Partially Overlapping Impact couNTER\u2019 and combines a flexible and powerful effect-cause pattern analysis algorithm with high-resolution ATPG. We show the effectiveness of the approach through experiments with benchmark and industrial circuits. In\u00a0\u2026", "num_citations": "137\n", "authors": ["507"]}
{"title": "An analytical approach to the partial scan problem\n", "abstract": " The scan design is the most widely used technique used to ensure the testability of sequential circuits. In this article it is shown that testability is still guaranteed, even if only a small part of the flipflops is integrated into a scan path. An algorithm is presented for selecting a minimal number of flipflops, which must be directly accessible. The direct accessibility ensures that, for each fault, the necessary test sequence is bounded linearly in the circuit size. Since the underlying problem is NP-complete, efficient heuristics are implemented to compute suboptimal solutions. Moreover, a new algorithm is presented to map a sequential circuit into a minimal combinational one, such that test pattern generation for both circuit representations is equivalent and the fast combinational ATPG methods can be applied. For all benchmark circuits investigated, this approach results in a significant reduction of the hardware\u00a0\u2026", "num_citations": "136\n", "authors": ["507"]}
{"title": "Self test using unequiprobable random patterns\n", "abstract": " In this paper we present a module generating unequiprobable random patterns, which can also perform signature analysis and work like a normal register similar to the well known BILBO. The hardware overhead of this module has the same magnitude as a conventional BILBO. Thus the class of self testable circuits is enlarged without additional costs.", "num_citations": "130\n", "authors": ["507"]}
{"title": "An integrated built-in test and repair approach for memories with 2D redundancy\n", "abstract": " An efficient on-chip infrastructure for memory test and repair is crucial to enhance yield and availability of SoCs. Therefore embedded memories are commonly equipped with spare rows and columns (2D redundancy). To avoid the storage of large failure bitmaps needed by classical algorithms for offline repair analysis, existing heuristics for built-in repair analysis (BIRA) either follow very simple search strategies or restrict the search to smaller local bitmaps. Exact BIRA algorithms work with sub analyzers for each possible repair combination. While a parallel implementation suffers from a high hardware overhead, a serial implementation leads to high test times. The integrated built-in test and repair approach proposed in this paper interleaves test and repair analysis and supports an exact solution without failure bitmap. The basic search procedure is combined with an efficient technique to continuously reduce the\u00a0\u2026", "num_citations": "118\n", "authors": ["507"]}
{"title": "Application of deterministic logic BIST on industrial circuits\n", "abstract": " We present the application of a deterministic logic BIST scheme based on bit-flipping on state-of-the-art industrial circuits. Experimental results show that complete fault coverage can be achieved for industrial circuits up to 100 K gates with 10,000 test patterns, at a total area cost for BIST hardware of typically 5\u201315%. It is demonstrated that a trade-off is possible between test quality, test time, and silicon area. In contrast to BIST schemes based on test point insertion no modifications of the circuit under test are required, complete fault efficiency is guaranteed, and the impact on the design process is minimized.", "num_citations": "115\n", "authors": ["507"]}
{"title": "Design and architectures for dependable embedded systems\n", "abstract": " The paper presents an overview of a major research project on dependable embedded systems that has started in Fall 2010 and is running for a projected duration of six years. Aim is a'dependability co-design'that spans various levels of abstraction in the design process of embedded systems starting from gate level through operating system, applications software to system architecture. In addition, we present a new classification on faults, errors, and failures.", "num_citations": "108\n", "authors": ["507"]}
{"title": "Accumulator based deterministic BIST\n", "abstract": " Most built-in self test (BIST) solutions require specialized test pattern generation hardware which may introduce significant area overhead and performance degradation. Recently, some authors proposed test pattern generation on chip by means of functional units also used in system mode like adders or multipliers. These schemes generate pseudo-random or pseudo-exhaustive patterns for serial or parallel BIST. If the circuit under test contains random pattern resistant faults a deterministic test pattern generator is necessary to obtain complete fault coverage. In this paper it is shown that a deterministic test set can be encoded as initial values of an accumulator based structure, and all testable faults can be detected within a given test length by carefully selecting the seeds of the accumulator. A ROM is added for storing the seeds, and the control logic of the accumulator is modified. In most cases the size of the ROM\u00a0\u2026", "num_citations": "107\n", "authors": ["507"]}
{"title": "Mixed-Mode BIST Using Embedded Processors\n", "abstract": " In complex systems, embedded processors may be used to run software routines for test pattern generation and response evaluation. For system components which are not completely random pattern testable, the test programs have to generate deterministic patterns after random testing. Usually the random test part of the program requires long run times whereas the part for deterministic testing has high memory requirements.               In this paper it is shown that an appropriate selection of the random pattern test method can significantly reduce the memory requirements of the deterministic part. A new, highly efficient scheme for software-based random pattern testing is proposed, and it is shown how to extend the scheme for deterministic test pattern generation. The entire test scheme may also be used for implementing a scan based BIST in hardware.", "num_citations": "105\n", "authors": ["507"]}
{"title": "Tailoring ATPG for embedded testing\n", "abstract": " An automatic test pattern generation (ATPG) method is presented for a scan-based test architecture which minimizes ATE storage requirements and reduces the bandwidth between the automatic test equipment (ATE) and the chip under test. To generate tailored deterministic test patterns, a standard ATPG tool performing dynamic compaction and allowing constraints on circuit inputs is used. The combination of an appropriate test architecture and the tailored test patterns reduces the test data volume up to two orders of magnitude compared with standard compacted test sets.", "num_citations": "101\n", "authors": ["507"]}
{"title": "Deterministic BIST with multiple scan chains\n", "abstract": " A deterministic BIST scheme for circuits with multiple scan paths is presented. A procedure is described for synthesizing a pattern generator which stimulates all scan chains simultaneously and guarantees complete fault coverage. The new scheme may require less chip area than a classical LFSR-based approach while better or even complete fault coverage is obtained at the same time.", "num_citations": "99\n", "authors": ["507"]}
{"title": "Zuverl\u00e4ssigkeit mechatronischer Systeme: Grundlagen und Bewertung in fr\u00fchen Entwicklungsphasen\n", "abstract": " Dieses Buch thematisiert die Zuverl\u00e4ssigkeitsbewertung mechatronischer Systeme, speziell in fr\u00fchen Entwicklungsphasen. Herausforderungen hierbei sind vor allem die ganzheitliche Betrachtung \u00fcber die Dom\u00e4nen Mechanik, Elektronik und Software sowie unsichere bzw. unvollst\u00e4ndige Daten. Neben der dom\u00e4nen\u00fcbergreifenden Betrachtungsweise werden zudem Themenaspekte in den einzelnen Dom\u00e4nen vertieft, die zur Zuverl\u00e4ssigkeitsbewertung in fr\u00fchen Entwicklungsphasen dienen.", "num_citations": "92\n", "authors": ["507"]}
{"title": "Programmable deterministic built-in self-test\n", "abstract": " In this paper, we propose a new programmable deterministic built-in self-Test (BIST) method that requires significantly lower storage for deterministic patterns than existing programmable methods and provides high flexibility for test engineering in both internal and external test. Theoretical analysis suggests that significantly more care bits can be encoded in the seed of a linear feedback shift register (LFSR), if a limited number of conflicting equations is ignored in the employed linear equation system. The ignored care bits are separately embedded into the LFSR pattern. In contrast to known deterministic BIST schemes based on test set embedding, the embedding logic function is not hardwired. Instead, this information is stored in memory using a special compression and decompression method. Experiments for benchmark circuits and industrial designs demonstrate that the approach has considerably higher\u00a0\u2026", "num_citations": "79\n", "authors": ["507"]}
{"title": "BIST for systems-on-a-chip\n", "abstract": " An increasing part of microelectronic systems is implemented on the basis of predesigned and preverified modules, so-called cores, which are reused in many instances. Core-providers offer RISC-kernels, embedded memories, DSPs, and many other functions, and built-in self-test is the appropriate method for testing complex systems composed of different cores. In this paper, we overview BIST methods for different types of cores and present advanced BIST solutions. Special emphasis is put on deterministic BIST methods as they do not require any modifications of the core under test and help to protect intellectual property (IP).", "num_citations": "77\n", "authors": ["507"]}
{"title": "On computing optimized input probabilities for random tests\n", "abstract": " Self testing of integrated circuits by random patterns has several technical and economical advantages. But there exists a large number of circuits which cannot be randomly tested, since the fault coverage achieved that way would be too low. In this paper we show that this problem can be solved by unequiprobable random patterns, and an efficient procedure is presented computing the specific optimal probability for each primary input of a combinational network. Those optimized random patterns can be produced on the chip during self test or off the chip in order to accelerate fault simulation and test pattern generation.", "num_citations": "71\n", "authors": ["507"]}
{"title": "Efficient pattern mapping for deterministic logic BIST\n", "abstract": " Deterministic logic BIST (DLBIST) is an attractive test strategy, since it combines advantages of deterministic external testing and pseudo-random LBIST. Unfortunately, previously published DLBIST methods are unsuited for large ICs, since computing time and memory consumption of the DLBIST synthesis algorithms increase exponentially, or at least cubically, with the circuit size. In this paper, we propose a novel DLBIST synthesis procedure that has nearly linear complexity in terms of both computing time and memory consumption. The new algorithms are based on binary decision diagrams (BDDs). We demonstrate the efficiency of the new algorithms for industrial designs up to 2M gates.", "num_citations": "70\n", "authors": ["507"]}
{"title": "Using BIST control for pattern generation\n", "abstract": " A deterministic BIST scheme is presented which requires less hardware overhead than pseudo-random BIST but obtains better or even complete fault coverage at the same time. It takes advantage of the fact that any autonomous BIST scheme needs a BIST control unit for indicating the completion of the self-test at least. Hence, pattern counters and bit counters are always available, and they provide information to be used for deterministic pattern generation by some additional circuitry. This paper presents a systematic way for synthesizing a pattern generator which needs less area than a 32-bit LFSR for random pattern generation for all the benchmark circuits.", "num_citations": "70\n", "authors": ["507"]}
{"title": "The pseudoexhaustive test of sequential circuits\n", "abstract": " The concept of a pseudoexhaustive test for sequential circuits is introduced in a way similar to that which is used for combinational networks. Using partial scan all cycles in the data flow of a sequential circuit are removed, such that a compact combinational model can be constructed. Pseudoexhaustive test sequences for the original circuit are constructed from a pseudoexhaustive test set for this model. To make this concept feasible for arbitrary circuits a technique for circuit segmentation is presented which provides special segmentation cells as well as the corresponding algorithms for the automatic placement of the cells. Example circuits show that the test strategy requires less additional silicon area than a complete scan path. Thus the advantages of a partial scan path are combined with the well-known benefits of a pseudoexhaustive test, such as high fault coverage and simplified test generation.< >", "num_citations": "69\n", "authors": ["507"]}
{"title": "Symmetric Transparent BIST for RAMs\n", "abstract": " The paper introduces the new concept of symmetric transparent BIST for RAMs. This concept allows one to skip the signature prediction phase of conventional transparent BIST approaches and therefore yields a significant reduction of test time. The hardware cost and the fault coverage of the new scheme remain comparable to that of a traditional transparent BIST scheme. In many cases, experimental studies even show a higher fault coverage obtained in shorter test time.", "num_citations": "68\n", "authors": ["507"]}
{"title": "Efficient fault simulation on many-core processors\n", "abstract": " Fault simulation is essential in test generation, design for test and reliability assessment of integrated circuits. Reliability analysis and the simulation of self-test structures are particularly computationally expensive as a large number of patterns has to be evaluated. In this work, we propose to map a fault simulation algorithm based on the parallel-pattern single-fault propagation (PPSFP) paradigm to many-core architectures and describe the involved algorithmic optimizations. Many-core architectures are characterized by a high number of simple execution units with small local memory. The proposed fault simulation algorithm exploits the parallelism of these architectures by use of parallel data structures. The algorithm is implemented for the NVIDIA GT200 Graphics Processing Unit (GPU) architecture and achieves a speed-up of up to 17x compared to an existing GPU fault-simulation algorithm and up to 16x\u00a0\u2026", "num_citations": "67\n", "authors": ["507"]}
{"title": "BIST power reduction using scan-chain disable in the Cell processor\n", "abstract": " Built-in self test is a major part of the manufacturing test procedure for the cell processor. However, pseudo random patterns cause a high switching activity which is not effectively reduced by standard low power design techniques. If special care is not taken, the scan-speed may have to be reduced significantly, thus extending test time and costs. In this paper, we describe a test power reduction method for logic BIST which uses test scheduling, planning and scan-gating. In LBIST, effective patterns that detect additional faults are very scarce after a few dozens of scan cycles and often less than one pattern in a hundred detects new faults. In most cases, such an effective pattern requires only a reduced set of the available scan chains to detect the fault and all don't-care scan chains can be disabled, therefore significantly reducing test power", "num_citations": "66\n", "authors": ["507"]}
{"title": "RESPIN++-deterministic embedded test\n", "abstract": " RESPIN++ is a deterministic embedded test method tailored to system chips, which implement scan test at core level. The scan chains of one core of the system-on-a-chip are reused to decompress the patterns for another core. To implement the RESPIN++ test architecture only a few gates need to be added to the test wrapper. This will not affect the critical paths of the system. The RESPIN++ method reduces both test data volume and test application time up to one order of magnitude per core compared to storing compacted test patterns on the ATE. If several cores may be tested concurrently, test data volume and test application time for the complete system test may be reduced even further. This paper presents the RESPIN++ test architecture and a compression algorithm for the architecture.", "num_citations": "65\n", "authors": ["507"]}
{"title": "Minimizing peak power consumption during scan testing: Test pattern modification with X filling heuristics\n", "abstract": " Scan architectures, though widely used in modern designs, are expensive in power consumption. In this paper, we discuss the issues of excessive peak power consumption during scan testing. We show that taking care of high current levels during the test cycle (i.e. between launch and capture) is highly relevant to avoid noise phenomena such as IR-drop or ground bounce. We propose a solution based on power-aware assignment of don't care bits in deterministic test patterns. For ISCAS'89 and ITC'99 benchmark circuits, this approach reduces peak power during the test cycle up to 89% compared to a random filling solution", "num_citations": "62\n", "authors": ["507"]}
{"title": "Fine-grained access management in reconfigurable scan networks\n", "abstract": " Modern very large scale integration designs incorporate a high amount of instrumentation that supports post-silicon validation and debug, volume test and diagnosis, as well as in-field system monitoring and maintenance. Reconfigurable scan architectures, as allowed by the novel IEEE Std 1149.1-2013 (JTAG) and IEEE Std 1687-2014 [Internal JTAG (IJTAG)], emerge as a scalable mechanism for access to such on-chip instruments. While the on-chip instrumentation is crucial for meeting quality, dependability, and time-to-market goals, it is prone to abuse and threatens system safety and security. A secure access management method is mandatory to assure that critical instruments be accessible to authorized entities only. This paper presents a novel protection method for fine-grained access management in complex reconfigurable scan networks based on a challenge-response authentication protocol. The target\u00a0\u2026", "num_citations": "61\n", "authors": ["507"]}
{"title": "Deterministic pattern generation for weighted random pattern testing\n", "abstract": " Weighted random pattern testing is now widely accepted as a very economic way for external testing as well as for implementing a built-in self-test (BIST) scheme. The weights may be computed either by structural analysis or by extracting the required information from a precomputed deterministic test set. In this paper, we present a method for generating deterministic test patterns which can easily be transformed into weight sets. These test patterns contain only minimal redundant information such that the weight generation process is not biased, and the patterns are grouped such that the conflicts within a group are minimized. The quality of the weight sets obtained this way is superior to the approaches published so far with respect to a small number of weights and weighted patterns, and a complete fault coverage for all the ISCAS-85 and ISCAS-89 benchmark circuits.", "num_citations": "61\n", "authors": ["507"]}
{"title": "Reusing scan chains for test pattern decompression\n", "abstract": " The paper presents a method for testing a system-on-a-chip by using a compressed representation of the patterns on an external tester. The patterns for a certain core under test are decompressed by reusing scan chains of cores idle during that time. The method only requires a few additional gates in the wrapper, while the mission logic is untouched. Storage and bandwidth requirements for the ATE are reduced significantly.", "num_citations": "60\n", "authors": ["507"]}
{"title": "Restrict encoding for mixed-mode BIST\n", "abstract": " Programmable mixed-mode BIST schemes combine pseudo-random pattern testing and deterministic test. This paper presents a synthesis technique for a mixed-mode BIST scheme which is able to exploit the regularities of a deterministic test pattern set for minimizing the hardware overhead and memory requirements. The scheme saves more than 50% hardware costs compared with the best schemes known so far while complete programmability is still preserved.", "num_citations": "55\n", "authors": ["507"]}
{"title": "Hardware-optimal test register insertion\n", "abstract": " Implementing a built-in self-test by a \"test per clock\" scheme offers advantages concerning fault coverage, detection of delay faults, and test application time. Such a scheme is implemented by test registers, for instance built-in logic block observers (BILBO's) and concurrent BILBO's (CBILBO's), which are inserted into the circuit structure at appropriate places. An algorithm is presented which is able to find the cost optimal placement of test registers for nearly all the ISCAS'89 sequential benchmark circuits, and a suboptimal solution with slightly higher costs is obtained for all the circuits within a few minutes of computing time. The algorithm can also be applied to the Minimum Feedback Vertex Set problem in partial scan design, and an optimal solution is found for all the benchmark circuits. The provably optimal solutions for the benchmark circuits mainly use CBILBO's which can simultaneously generate test patterns\u00a0\u2026", "num_citations": "53\n", "authors": ["507"]}
{"title": "The design of random-testable sequential circuits\n", "abstract": " A method is described for selecting a minimal set of directly accessible flip-flops. Since this problem turns out to be NP-complete, suboptimal solutions can be derived using some heuristics. An algorithm is presented to compute the corresponding weights of the patterns, which are time-dependent in some cases. The entire approach is validated with the help of examples. Only 10-40% of the flip-flops have to be integrated into a partial scan path or into a built-in self-test register to obtain nearly complete fault coverage by weighted random patterns.", "num_citations": "51\n", "authors": ["507"]}
{"title": "High defect coverage with low-power test sequences in a BIST environment\n", "abstract": " A new technique, random single-input change (RSIC) test generation, generates low-power test patterns that provide a high level of defect coverage during low-power BIST of digital circuits. The authors propose a parallel BIST implementation of the RSIC generator and analyze its area-overhead impact.", "num_citations": "48\n", "authors": ["507"]}
{"title": "TESTCHIP: A chip for weighted random pattern generation, evaluation, and test control\n", "abstract": " In self-testable circuits, additional hardware is incorporated for generating test patterns and evaluating test responses. A built-off test strategy is presented which moves the additional hardware to a programmable extra chip. This is a low-cost test strategy in three ways: (1) the use of random patterns eliminates the expensive test-pattern computation; (2) a microcomputer and an ASIC (application-specific IC) replace the expensive automatic test equipment; and (3) the design for testability overhead is minimized. The presented ASIC generates random patterns, applies them to a circuit under test, and evaluates the test responses by signature analysis. It contains a hardware structure that can produce weighted random patterns corresponding to multiple programmable distributions. These patterns give a high fault coverage and allow short test lengths. A wide range of circuits can be tested as the only requirement is a\u00a0\u2026", "num_citations": "48\n", "authors": ["507"]}
{"title": "Modeling, verification and pattern generation for reconfigurable scan networks\n", "abstract": " Reconfigurable scan architectures allow flexible integration and efficient access to infrastructure in SoCs, e.g. for test, diagnosis, repair or debug. Such scan networks are often hierarchical and have complex structural and functional dependencies. For instance, the IEEE P1687 proposal, known as IJTAG, allows integration of multiplexed scan networks with arbitrary internal control signals. Common approaches for scan verification based on static structural analysis and functional simulation are not sufficient to ensure correct operation of these types of architectures. Hierarchy and flexibility may result in complex or even contradicting configuration requirements to access single elements. Sequential logic justification is therefore mandatory both to verify the validity of a scan network, and to generate the required access sequences. This work presents a formal method for verification of reconfigurable scan architectures\u00a0\u2026", "num_citations": "47\n", "authors": ["507"]}
{"title": "Models in hardware testing: lecture notes of the forum in honor of Christian Landrault\n", "abstract": " Model based testing is the most powerful technique for testing hardware and software systems. Models in Hardware Testing describes the use of models at all the levels of hardware testing. The relevant fault models for nanoscaled CMOS technology are introduced, and their implications on fault simulation, automatic test pattern generation, fault diagnosis, memory testing and power aware testing are discussed. Models and the corresponding algorithms are considered with respect to the most recent state of the art, and they are put into a historical context by a concluding chapter on the use of physical fault models in fault tolerance.", "num_citations": "47\n", "authors": ["507"]}
{"title": "Module diversification: Fault tolerance and aging mitigation for runtime reconfigurable architectures\n", "abstract": " Runtime reconfigurable architectures based on Field-Programmable Gate Arrays (FPGAs) are attractive for realizing complex applications. However, being manufactured in latest semiconductor process technologies, FPGAs are increasingly prone to aging effects, which reduce the reliability of such systems and must be tackled by aging mitigation and application of fault tolerance techniques. This paper presents module diversification, a novel design method that creates different configurations for runtime reconfigurable modules. Our method provides fault tolerance by creating the minimal number of configurations such that for any faulty Configurable Logic Block (CLB) there is at least one configuration that does not use that CLB. Additionally, we determine the fraction of time that each configuration should be used to balance the stress and to mitigate the aging process in FPGA-based runtime reconfigurable\u00a0\u2026", "num_citations": "45\n", "authors": ["507"]}
{"title": "Efficient online and offline testing of embedded DRAMs\n", "abstract": " This paper presents an integrated approach for both built-in online and off-line testing of embedded DRAMs. It is based on a new technique for output data compression which offers the same benefits as signature analysis during off-line test, but also supports efficient online consistency checking. The initial fault-free memory contents are compressed to a reference characteristic and compared to test characteristics periodically. The reference characteristic depends on the memory contents, but unlike similar characteristics based on signature analysis, it can be easily updated concurrently with WRITE operations. This way, changes in memory do not require a time consuming recomputation. The respective test characteristics can be efficiently computed during the periodic refresh operations of the dynamic RAM. Experiments show that the proposed technique significantly reduces the time between the occurrence of an\u00a0\u2026", "num_citations": "45\n", "authors": ["507"]}
{"title": "Hochintegrierte Schaltungen: Pr\u00fcfgerechter Entwurf und Test\n", "abstract": " Mit dem vorliegenden Lehrbuch werden erstmalig in deutscher Sprache der pr\u00fcfgerechte Entwurf und der Test hochintegrierter Schaltungen in umfassender Weise behandelt. Das Werk wendet sich an alle, die in Studium, Lehre, Forschung und Entwicklung Kenntnisse auf diesem Gebiet erwerben oder vertiefen m\u00f6chten. Insbesondere erh\u00e4lt der Schaltungsentwickler einen \u00dcberblick \u00fcber die Ma\u00dfnahmen, die zur Verbesserung der Testbarkeit seiner Schaltung beitragen k\u00f6nnen; der Entwickler von Entwurfssystemen findet zahlreiche, teilweise neue Algorithmen und Verfahren f\u00fcr den automatisierten Entwurf testbarer Schaltungen und f\u00fcr die zugeh\u00f6rige Testerzeugung. Die Verfahren werden in einem einheitlichen theoretischen Rahmen vorgestellt. Bei dieser einheitlichen Modellierung der Schaltung und der Algorithmen bleibt jedoch stets die Verbindung zur konkreten technischen Realisierung gewahrt.", "num_citations": "42\n", "authors": ["507"]}
{"title": "Concurrent self-test with partially specified patterns for low test latency and overhead\n", "abstract": " Structural on-line self-test may be performed to detect permanent faults and avoid their accumulation. This paper improves concurrent BIST techniques based on a deterministic test set. Here, the test patterns are specially generated with a small number of specified bits. This results in very low test latency, which reduces the likelihood of fault accumulation. Experiments with a large number of circuits show that the hardware overhead is significantly lower than the overhead for previously published methods. Furthermore, the method allows to tradeoff fault coverage, test latency and hardware overhead.", "num_citations": "42\n", "authors": ["507"]}
{"title": "Self-adjusting output data compression: An efficient BIST technique for RAMs\n", "abstract": " After write operations, BIST schemes for RAMs relying on signature analysis must compress the entire memory contents to update the reference signature. This paper introduces a new scheme for output data compression which avoids this overhead while retaining the benefits of signature analysis. The proposed technique is based on a new memory characteristic derived as the module-2 sum of all addresses pointing to non-zero cells. This characteristic can be adjusted concurrently with write operations by simple EXOR-operations on the initial characteristic and on the addresses affected by the change.", "num_citations": "41\n", "authors": ["507"]}
{"title": "Software-based self-test of processors under power constraints\n", "abstract": " Software-based self-test (SBST) of processors offers many benefits, such as dispense with expensive test equipments, test execution during maintenance and in the field or initialization tests for the whole system. In this paper, for the first time a structural SBST methodology is proposed which optimizes energy, average power consumption, test length and fault coverage at the same time", "num_citations": "39\n", "authors": ["507"]}
{"title": "On the reliability evaluation of SRAM-based FPGA designs\n", "abstract": " Benefits of field programmable gate arrays (FPGAs) have lead to a spectrum of use ranging from consumer products to astronautics. This diversity necessitates the need to evaluate the reliability of the FPGA, because of their high susceptibility to soft errors, which are due to the high density of embedded SRAM cells. Reliability evaluation is an important step in designing highly reliable systems, which results in a strong competitive advantage in today's marketplace. This paper proposes a mathematical model able to evaluate and therefore help to improve the reliability of SRAM-based FPGAs.", "num_citations": "39\n", "authors": ["507"]}
{"title": "Non-intrusive BIST for systems-on-a-chip\n", "abstract": " The term \"functional BIST\" describes a test method to control functional modules so that they generate a deterministic test set, which targets structural faults within other parts of the system. It is a promising solution for self-testing complex digital systems at reduced costs in terms of area overhead and performance degradation. While previous work mainly investigated the use of functional modules for generating pseudo-random and pseudo-exhaustive test patterns, the present paper shows that a variety of modules can also be used as a deterministic test pattern generator via an appropriate reseeding strategy. This method enables a BIST technique that does not introduce additional hardware like test points and test registers into combinational and pipelined modules under test. The experimental results prove that the reseeding method works for accumulator based structures, multipliers, or encryption modules as\u00a0\u2026", "num_citations": "39\n", "authors": ["507"]}
{"title": "Reconfigurable scan networks: Modeling, verification, and optimal pattern generation\n", "abstract": " Efficient access to on-chip instrumentation is a key requirement for post-silicon validation, test, debug, bringup, and diagnosis. Reconfigurable scan networks, as proposed by, for example, IEEE Std 1687-2014 and IEEE Std 1149.1-2013, emerge as an effective and affordable means to cope with the increasing complexity of on-chip infrastructure. Reconfigurable scan networks are often hierarchical and may have complex structural and functional dependencies. Common approaches for scan verification based on static structural analysis and functional simulation are not sufficient to ensure correct operation of these types of architectures. To access an instrument in a reconfigurable scan network, a scan-in bit sequence must be generated according to the current state and structure of the network. Due to sequential and combinational dependencies, the access pattern generation process (pattern retargeting) poses a\u00a0\u2026", "num_citations": "38\n", "authors": ["507"]}
{"title": "BISD: Scan-based built-in self-diagnosis\n", "abstract": " Built-In Self-Test (BIST) is less often applied to random logic than to embedded memories due to the following reasons: Firstly, for a satisfiable fault coverage it may be necessary to apply additional deterministic patterns, which cause additional hardware costs. Secondly, the BIST-signature reveals only poor diagnostic information. Recently, the first issue has been addressed successfully. The paper at hand proposes a viable, effective and cost efficient solution for the second problem. The paper presents a new method for Built-in Self-Diagnosis (BISD). The core of the method is an extreme response compaction architecture, which for the first time enables an autonomous on-chip evaluation of test responses with negligible hardware overhead. The key advantage of this architecture is that all data, which is relevant for a subsequent diagnosis, is gathered during just one test session. The BISD method comprises a\u00a0\u2026", "num_citations": "38\n", "authors": ["507"]}
{"title": "Tools and devices supporting the pseudo-exhaustive test\n", "abstract": " In the paper logical cells and algorithms are presented supporting the design of pseudo-exhaustively testable circuits. The approach is based on real hardware segmentation, instead of path-sensitizing. The developed cells segment the entire circuits into exhaustively testable parts, and the presented algorithms place these cells, under the objective to minimize the hardware overhead. The approach is completely compatible with the usual LSSD-rules. The analysis of the well-known benchmark circuits shows only little additional hardware costs.", "num_citations": "38\n", "authors": ["507"]}
{"title": "Resilience Articulation Point (RAP): Cross-layer dependability modeling for nanometer system-on-chip resilience\n", "abstract": " The Resilience Articulation Point (RAP) model aims at provisioning researchers and developers with a probabilistic fault abstraction and error propagation framework covering all hardware/software layers of a System on Chip. RAP assumes that physically induced faults at the technology or CMOS device layer will eventually manifest themselves as a single or multiple bit flip(s). When probabilistic error functions for specific fault origins are known at the bit or signal level, knowledge about the unit of design and its environment allow the transformation of the bit-related error functions into characteristic higher layer representations, such as error functions for data words, Finite State Machine (FSM) state, macro-interfaces or software variables. Thus, design concerns at higher abstraction layers can be investigated without the necessity to further consider the full details of lower levels of design. This paper introduces the\u00a0\u2026", "num_citations": "37\n", "authors": ["507"]}
{"title": "A-abft: Autonomous algorithm-based fault tolerance for matrix multiplications on graphics processing units\n", "abstract": " Graphics processing units (GPUs) enable large-scale scientific applications and simulations on the desktop. To allow scientific computing on GPUs with high performance and reliability requirements, the application of software-based fault tolerance is attractive. Algorithm-Based Fault Tolerance (ABFT) protects important scientific operations like matrix multiplications. However, the application to floating-point operations necessitates the runtime classification of errors into inevitable rounding errors, allowed compute errors in the magnitude of such rounding errors, and into critical errors that are larger than those and not tolerable. Hence, an ABFT scheme needs suitable rounding error bounds to detect errors reliably. The determination of such error bounds is a highly challenging task, especially since it has to be integrated tightly into the algorithm and executed autonomously with low performance overhead. In this\u00a0\u2026", "num_citations": "36\n", "authors": ["507"]}
{"title": "FAST-BIST: Faster-than-at-Speed BIST targeting hidden delay defects\n", "abstract": " Small delay faults may be an indicator of a reliability threat, even if they do not affect the system functionality yet. In recent years, Faster-than-at-Speed-Test (FAST) has become a feasible method to detect faults, which are hidden by the timing slack or by long critical paths in the combinational logic. FAST poses severe challenges to the automatic test equipment with respect to timing, performance, and resolution. In this paper, it is shown how logic built-in self-test (BIST) or embedded deterministic test can be used for an efficient FAST application. Running BIST just at a higher frequency is not an option, as outputs of long paths will receive undefined values due to set time violations and destroy the content of the signature registers. Instead, for a given test pattern sequence, faults are classified according to the optimal detection frequency. For each class, a MISR-based compaction scheme is adapted, such that the\u00a0\u2026", "num_citations": "34\n", "authors": ["507"]}
{"title": "Test strategies for reliable runtime reconfigurable architectures\n", "abstract": " Field-programmable gate array (FPGA)-based reconfigurable systems allow the online adaptation to dynamically changing runtime requirements. The reliability of FPGAs, being manufactured in latest technologies, is threatened by soft errors, as well as aging effects and latent defects. To ensure reliable reconfiguration, it is mandatory to guarantee the correct operation of the reconfigurable fabric. This can be achieved by periodic or on-demand online testing. This paper presents a reliable system architecture for runtime-reconfigurable systems, which integrates two nonconcurrent online test strategies: preconfiguration online tests (PRET) and postconfiguration online tests (PORT). The PRET checks that the reconfigurable hardware is free of faults by periodic or on-demand tests. The PORT has two objectives: It tests reconfigured hardware units after reconfiguration to check that the configuration process completed\u00a0\u2026", "num_citations": "33\n", "authors": ["507"]}
{"title": "Scan chain clustering for test power reduction\n", "abstract": " An effective technique to save power during scan based test is to switch off unused scan chains. The results obtained with this method strongly depend on the mapping of scan flip-flops into scan chains, which determines how many chains can be deactivated per pattern.", "num_citations": "33\n", "authors": ["507"]}
{"title": "Adapting an SoC to ATE concurrent test capabilities\n", "abstract": " Concurrent test features are available in SoC testers to increase ATE throughput. To exploit these new features, design modifications are necessary. In a case study, these modifications were applied to the open source LEON SoC platform containing an embedded 32 bit CPU, an AMBA bus, and several embedded cores. The concurrent test of LEON was performed on an SoC tester. The gain in test application time and area costs are quantified and obstacles in the design flow for concurrent test are discussed.", "num_citations": "33\n", "authors": ["507"]}
{"title": "Scan pattern retargeting and merging with reduced access time\n", "abstract": " Efficient access to on-chip instrumentation is a key enabler for post-silicon validation, debug, bringup or diagnosis. Reconfigurable scan networks, as proposed by e.g. the IEEE Std. P1687, emerge as an effective and affordable means to cope with the increasing complexity of on-chip infrastructure. To access an element in a reconfigurable scan network, a scan-in bit sequence must be generated according to the current state and structure of the network. Due to sequential and combinational dependencies, the scan pattern generation process (pattern retargeting) poses a complex decision and optimization problem. This work presents a method for scan pattern generation with reduced access time. We map the access time reduction to a pseudo-Boolean optimization problem, which enables the use of efficient solvers to exhaustively explore the search space of valid scan-in sequences. This is the first automated\u00a0\u2026", "num_citations": "32\n", "authors": ["507"]}
{"title": "GPU-accelerated simulation of small delay faults\n", "abstract": " Delay fault simulation is an essential task during test pattern generation and reliability assessment of electronic circuits. With the high sensitivity of current nano-scale designs toward even smallest delay deviations, the simulation of small gate delay faults has become extremely important. Since these faults have a subtle impact on the timing behavior, traditional fault simulation approaches based on abstract timing models are not sufficient. Furthermore, the detection of these faults is compromised by the ubiquitous variations in the manufacturing processes, which causes the actual fault coverage to vary from circuit instance to circuit instance, and makes the use of timing accurate methods mandatory. However, the application of timing accurate techniques quickly becomes infeasible for larger designs due to excessive computational requirements. In this paper, we present a method for fast and waveform-accurate\u00a0\u2026", "num_citations": "30\n", "authors": ["507"]}
{"title": "Advanced diagnosis: SBST and BIST integration in automotive E/E architectures\n", "abstract": " The constantly growing amount of semiconductors in automotive systems increases the number of possible defect mechanisms, and therefore raises also the effort to maintain a sufficient level of quality and reliability. A promising solution to this problem is the on-line application of structural tests in key components, typically ECUs. In this work, an approach for the optimized integration of both Software-Based Self-Tests (SBST) and Built-In Self-Tests (BIST) into E/E architectures is presented. The approach integrates the execution of the tests non-intrusively, ie, it (a) does not affect functional applications and (b) does not require costly changes in the communication schedules or additional communication overhead. Via design space exploration, optimized implementations with respect to multiple conflicting objectives, ie, monetary costs, safety, test quality, and required execution time are derived.", "num_citations": "30\n", "authors": ["507"]}
{"title": "Generating pseudo-exhaustive vectors for external testing\n", "abstract": " Over the past years special chips for external tests have been successfully used for random pattern testing. The authors present a technique for combining the advantages of such a low-cost test with the advantages of pseudoexhaustive testing, which are enhanced fault coverage and simplified test pattern generation. To achieve this goal, two tasks are accomplished. First, an algorithm is developed for pseudoexhaustive test pattern generation, which ensures a feasible test length. Second, a chip design for applying these test patterns to a device under test is presented. The chip is programmed by the output of the proposed algorithm and controls the entire test. The technique is first applied to devices with a scan path and then extended to sequential circuits. A large number of benchmark circuits have been investigated, and the results are presented.< >", "num_citations": "30\n", "authors": ["507"]}
{"title": "A diagnosis algorithm for extreme space compaction\n", "abstract": " During volume testing, test application time, test data volume and high performance automatic test equipment (ATE) are the major cost factors. Embedded testing including built-in self-test (BIST) and multi-site testing are quite effective cost reduction techniques which may make diagnosis more complex. This paper presents a test response compaction scheme and a corresponding diagnosis algorithm which are especially suited for BIST and multi-site testing. The experimental results on industrial designs show, that test time and response data volume reduces significantly and the diagnostic resolution even improves with this scheme. A comparison with X-Compact indicates, that simple parity information provides higher diagnostic resolution per response data bit than more complex signatures.", "num_citations": "29\n", "authors": ["507"]}
{"title": "Efficacy and efficiency of algorithm-based fault-tolerance on GPUs\n", "abstract": " Computer simulations drive innovations in science and industry, and they are gaining more and more importance. However, their high computational demand generates extraordinary challenges for computing systems. Typical high-performance computing systems, which provide sufficient performance and high reliability, are extremely expensive. Modern GPUs offer high performance at very low costs, and they enable simulation applications on the desktop. However, they are increasingly prone to transient effects and other reliability threats. To fulfill the strict reliability requirements in scientific computing and simulation technology, appropriate fault tolerance measures have to be integrated into simulation applications for GPUs. Algorithm-Based Fault Tolerance on GPUs has the potential to meet these requirements. In this work we investigate the efficiency and the efficacy of ABFT for matrix operations on GPUs. We\u00a0\u2026", "num_citations": "28\n", "authors": ["507"]}
{"title": "Optimal hardware pattern generation for functional BIST\n", "abstract": " Abstract\u2217Functional BIST is a promising solution for self-testing complex digital systems at reduced costs in terms of area and performance degradation. The present paper addresses the computation of optimal seeds for an arbitrary sequential module to be used as hardware test pattern generator. Up to now, only linear feedback shift registers and accumulator based structures have been used for deterministic test pattern generation by reseeding. In this paper, a method is proposed which can be applied to general finite state machines. Nevertheless the method is absolutely general, for sake of comparison with previous approaches, in this paper an accumulator based unit is assumed as pattern generator module. Experiments prove the effectiveness of the approach which outperforms previous results for accumulators, in terms of test size and test time, without sacrifying the fault detection capability.", "num_citations": "28\n", "authors": ["507"]}
{"title": "Aging resilience and fault tolerance in runtime reconfigurable architectures\n", "abstract": " Runtime reconfigurable architectures based on Field-Programmable Gate Arrays (FPGAs) allow areaand power-efficient acceleration of complex applications. However, being manufactured in latest semiconductor process technologies, FPGAs are increasingly prone to aging effects, which reduce the reliability and lifetime of such systems. Aging mitigation and fault tolerance techniques for the reconfigurable fabric become essential to realize dependable reconfigurable architectures. This article presents an accelerator diversification method that creates multiple configurations for runtime reconfigurable accelerators that are diversified in their usage of Configurable Logic Blocks (CLBs). In particular, it creates a minimal number of configurations such that all single-CLB and some multi-CLB faults can be tolerated. For each fault we ensure that there is at least one configuration that does not use that CLB. Second, a\u00a0\u2026", "num_citations": "27\n", "authors": ["507"]}
{"title": "STRAP: Stress-aware placement for aging mitigation in runtime reconfigurable architectures\n", "abstract": " Aging effects in nano-scale CMOS circuits impair the reliability and Mean Time to Failure (MTTF) of embedded systems. Especially for FPGAs that are manufactured in the latest technology node, aging is amajor concern. We introduce the first cross-layer aging-aware placement method for accelerators in FPGA-based runtime reconfigurable architectures. It optimizes stress distribution by accelerator placement at runtime, i.e. to which reconfigurable region an accelerator shall be reconfigured. Additionally, it optimizes logic placement at synthesis time to diversify the resource usage of individual accelerators, i.e. which CLBs of a reconfigurable region shall be used by an accelerator. Both layers together balance the intra- and inter-region stress induced by the application workload at negligible performance cost. Experimental results show significant reduction of maximum stress of up to 64% and 35%, which leads to\u00a0\u2026", "num_citations": "27\n", "authors": ["507"]}
{"title": "Deterministic Logic BIST for Transition Fault Testing\n", "abstract": " Built-in self-test (BIST) is an attractive approach to detect delay faults because of its inherent support for at-speed test. Deterministic logic BIST (DLBIST) is a technique that has been successfully applied to stuck-at fault testing. As delay faults have lower random pattern testability than stuck-at faults, the need for DLBIST schemes has increased. However, an extension to delay fault testing is not trivial as this necessitates the application of pattern pairs. As a consequence, delay fault testing is expected to require a larger mapping effort and logic overhead than stuck-at fault testing. With this in mind, the authors consider the so-called transition fault model, which is widely used for complexity reasons, and an extension of a DLBIST scheme for transition fault testing is presented. Functional justification is used to generate the required pattern pairs. The efficiency of the extended scheme is investigated using difficult-to-test\u00a0\u2026", "num_citations": "27\n", "authors": ["507"]}
{"title": "Structural in-field diagnosis for random logic circuits\n", "abstract": " In-field diagnosability of electronic components in larger systems such as automobiles becomes a necessity for both customers and system integrators. Traditionally, functional diagnosis is applied during integration and in workshops for in-field failures or break-downs. However, functional diagnosis does not yield sufficient coverage to allow for short repair times and fast reaction on systematic failures in the production. Structural diagnosis could yield the desired coverage, yet recent built-in architectures which could be reused in the field either do not reveal diagnostic information or necessitate dedicated test schemes. The paper at hand closes this gap with a new built-in test method for autonomous in-field testing and in-field diagnostic data collection. The proposed Built-In Self-Diagnosis method (BISD) is based on the standard BIST architecture and can seamlessly be integrated with recent, commercial DfT\u00a0\u2026", "num_citations": "26\n", "authors": ["507"]}
{"title": "On applying the set covering model to reseeding\n", "abstract": " The Functional BIST approach is a rather new BIST technique based on exploiting embedded system functionality to generate deterministic test patterns during BIST. The approach takes advantages of two well-known testing techniques, the arithmetic BIST approach and the reseeding method. The main contribution of the present paper consists in formulating the problem of an optimal reseeding computation as an instance of the set covering problem. The proposed approach guarantees high flexibility, is applicable to different functional modules, and, in general, provides a more efficient test set encoding then previous techniques. In addition, the approach shorts the computation time and allows to better exploiting the tradeoff between area overhead and global test length as well as to deal with larger circuits.", "num_citations": "26\n", "authors": ["507"]}
{"title": "Error detecting refreshment for embedded DRAMs\n", "abstract": " This paper presents a new technique for on-line consistency checking of embedded DRAMs. The basic idea is to use the periodic refresh operation for concurrently computing a test characteristic of the memory contents and compare it to a precomputed reference characteristic. Experiments show that the proposed technique significantly reduces the time between the occurrence of an error and its detection (error detection latency). It also achieves a very high error coverage at low hardware costs. Therefore it perfectly complements standard on-line checking approaches relying on error detecting codes, where the detection of certain types of errors is guaranteed, but only during READ operations accessing the erroneous data.", "num_citations": "26\n", "authors": ["507"]}
{"title": "GPU-accelerated small delay fault simulation\n", "abstract": " The simulation of delay faults is an essential task in design validation and reliability assessment of circuits. Due to the high sensitivity of current nano-scale designs against smallest delay deviations, small delay faults recently became the focus of test research. Because of the subtle delay impact, traditional fault simulation approaches based on abstract timing models are not sufficient for representing small delay faults. Hence, timing accurate simulation approaches have to be utilized, which quickly become inapplicable for larger designs due to high computational requirements. In this work we present a waveform-accurate approach for fast high-throughput small delay fault simulation on Graphics Processing Units (GPUs). By exploiting parallelism from gates, faults and patterns, the proposed approach enables accurate exhaustive small delay fault simulation even for multimillion gate designs without fault dropping\u00a0\u2026", "num_citations": "25\n", "authors": ["507"]}
{"title": "Securing access to reconfigurable scan networks\n", "abstract": " The accessibility of on-chip embedded infrastructure for test, reconfiguration, and debug poses a serious safety and security problem. Special care is required in the design and development of scan architectures based on IEEE Std. 1149.1 (JTAG), IEEE Std. 1500, and especially reconfigurable scan networks, as allowed by the upcoming IEEE P1687 (IJTAG). Traditionally, the scan infrastructure is secured after manufacturing test using fuses that disable the test access port (TAP) completely or partially. The fuse-based approach is efficient if some scan chains or instructions of the TAP controller are to be permanently blocked. However, this approach becomes costly if fine-grained access management is required, and it faces scalability issues in reconfigurable scan networks. In this paper, we propose a scalable solution for multi-level access management in reconfigurable scan networks. The access to protected\u00a0\u2026", "num_citations": "25\n", "authors": ["507"]}
{"title": "Scan test planning for power reduction\n", "abstract": " Many STUMPS architectures found in current chip designs allow disabling of individual scan chains for debug and diagnosis. In a recent paper it has been shown that this feature can be used for reducing the power consumption during test. Here, we present an efficient algorithm for the automated generation of a test plan that keeps fault coverage as well as test time, while significantly reducing the amount of wasted energy. A fault isolation table, which is usually used for diagnosis and debug, is employed to accurately determine scan chains that can be disabled. The algorithm was successfully applied to large industrial circuits and identifies a very large amount of excess pattern shift activity.", "num_citations": "25\n", "authors": ["507"]}
{"title": "Synthesis of I/sub DDQ/-testable circuits: integrating built-in current sensors\n", "abstract": " \"On-Chip\" I/sub DDQ/ testing by the incorporation of Built-In Current (BIC) sensors has some advantages over \"off-chip\" techniques. However, the integration of sensors poses analog design problems which are hard to solve for a digital designer. The automatic incorporation of the sensors using parameterized BIC cells could be a promising alternative. The work reported here identifies partitioning criteria to guide the synthesis of I/sub DDQ/-testable circuits. The circuit must be partitioned, such that the defective I/sub DDQ/ is observable, and the power supply voltage perturbation is within specified limits. In addition to these constraints, cost criteria are considered: circuit extra delay, area overhead of the BIC sensors, connectivity costs of the test circuitry, and the test application time. The parameters are estimated based on logical as well as electrical level information of the target cell library to be used in the\u00a0\u2026", "num_citations": "25\n", "authors": ["507"]}
{"title": "High-throughput logic timing simulation on GPGPUs\n", "abstract": " Many EDA tasks such as test set characterization or the precise estimation of power consumption, power droop and temperature development, require a very large number of time-aware gate-level logic simulations. Until now, such characterizations have been feasible only for rather small designs or with reduced precision due to the high computational demands. The new simulation system presented here is able to accelerate such tasks by more than two orders of magnitude and provides for the first time fast and comprehensive timing simulations for industrial-sized designs. Hazards, pulse-filtering, and pin-to-pin delay are supported for the first time in a GPGPU accelerated simulator, and the system can easily be extended to even more realistic delay models and further applications. A sophisticated mapping with efficient memory utilization and access patterns as well as minimal synchronizations and control flow\u00a0\u2026", "num_citations": "24\n", "authors": ["507"]}
{"title": "On-line prediction of NBTI-induced aging rates\n", "abstract": " Nanoscale technologies are increasingly susceptible to aging processes such as Negative-Bias Temperature Instability (NBTI) which undermine the reliability of VLSI systems. Existing monitoring techniques can detect the violation of safety margins and hence make the prediction of an imminent failure possible. However, since such techniques can only detect measurable degradation effects which appear after a relatively long period of system operation, they are not well suited to early aging prediction and proactive aging alleviation. This work presents a novel method for the monitoring of NBTI-induced degradation rate in digital circuits. It enables the timely adoption of proper mitigation techniques that reduce the impact of aging. The developed method employs machine learning techniques to find a small set of so called Representative Critical Gates (RCG), the workload of which is correlated with the degradation\u00a0\u2026", "num_citations": "24\n", "authors": ["507"]}
{"title": "Access port protection for reconfigurable scan networks\n", "abstract": " Scan infrastructures based on IEEE Std. 1149.1 (JTAG), 1500 (SECT), and P1687 (IJTAG) provide a cost-effective access mechanism for test, reconfiguration, and debugging purposes. The improved accessibility of on-chip instruments, however, poses a serious threat to system safety and security. While state-of-the-art protection methods for scan architectures compliant with JTAG and SECT are very effective, most of these techniques face scalability issues in reconfigurable scan networks allowed by the upcoming IJTAG standard. This paper describes a scalable solution for multi-level access management in reconfigurable scan networks. The access to protected instruments is restricted locally at the interface to the network. The access restriction is realized by a sequence filter that allows only a precomputed set of scan-in access sequences. This approach does not require any modification of the scan\u00a0\u2026", "num_citations": "24\n", "authors": ["507"]}
{"title": "GUARD: Guaranteed reliability in dynamically reconfigurable systems\n", "abstract": " Soft errors are a reliability threat for reconfigurable systems implemented with SRAM-based FPGAs. They can be handled through fault tolerance techniques like scrubbing and modular redundancy. However, selecting these techniques statically at design or compile time tends to be pessimistic and prohibits optimal adaptation to changing soft error rate at runtime. We present the GUARD method which allows for autonomous runtime reliability management in reconfigurable architectures: Based on the error rate observed during runtime, the runtime system dynamically determines whether a computation should be executed by a hardened processor, or whether it should be accelerated by inherently less reliable reconfigurable hardware which can trade-off performance and reliability. GUARD is the first runtime system for reconfigurable architectures that guarantees a target reliability while optimizing the performance\u00a0\u2026", "num_citations": "22\n", "authors": ["507"]}
{"title": "A common approach to test generation and hardware verification based on temporal logic\n", "abstract": " Hardware verifrcation and sequential test generation are aspects of the same problem, namely to prove the equal behavior determined by two circuit descriptions. During test generation, this attempt succeeds for the faulty and fault free circuit if redundancy exists, and during verifrcation it succeeds, if the implementation is correct with regard to its specification. This observation can be used to cross-fertilize both areas, which have been treated separately up to now. In this work, a common formal pamework for hardware verification and sequential test pattern generation is presented, which is based on modeling the circuit behavior with temporal logic. In addition, a new approach to cope with non resetable flipfiops in sequential test generation is proposed, which is not restricted to stuck-at faults. Based on this verification view, it is possible to provide the designer with one tool for checking circuit correctness and\u00a0\u2026", "num_citations": "22\n", "authors": ["507"]}
{"title": "A neural-network-based fault classifier\n", "abstract": " In order to reduce the number of defective parts and increase yield, especially in early stages of production, systematic defects must be identified and corrected as soon as possible. This paper presents a technique to move defect classification to the earliest phase of volume testing without any special diagnostic test patterns. A neural-network-based fault classifier is described, which is able to raise a warning, if the frequency of certain defect mechanisms increases. Only in this case more sophisticated diagnostic patterns or the even more expensive physical failure analysis have to be applied. The fault classification method presented here is able to extract underlying fault types with high confidence by identifying relevant features from the circuit topology and from logic simulation.", "num_citations": "21\n", "authors": ["507"]}
{"title": "Scan test power simulation on GPGPUs\n", "abstract": " The precise estimation of dynamic power consumption, power droop and temperature development during scan test require a very large number of time-aware gate-level logic simulations. Until now, such characterizations have been feasible only for rather small designs or with reduced precision due to the high computational demands. We propose a new, throughput-optimized timing simulator on running on GPGPUs to accelerate these tasks by more than two orders of magnitude and thus providing for the first time precise and comprehensive toggle data for industrial-sized designs and over long scan test operations. Hazards and pulse-filtering are supported for the first time in a GPGPU accelerated simulator, and the system can easily be extended to even more sophisticated delay and power models.", "num_citations": "20\n", "authors": ["507"]}
{"title": "Impact of test point insertion on silicon area and timing during layout\n", "abstract": " This paper presents an experimental investigation on the impact of test point insertion on circuit size and performance. Often test points are inserted into a circuit in order to improve the circuit's testability, which results in smaller test data volume, shorter test time, and higher fault coverage. Inserting test points however requires additional silicon area and influences the timing of a circuit. The paper shows how placement and routing is affected by test point insertion during layout generation. Experimental data for industrial circuits show that inserting 1% test points in general increases the silicon area after layout by less than 0.5% while the performance of the circuit may be reduced by 5% or more.", "num_citations": "20\n", "authors": ["507"]}
{"title": "Deterministic BIST with partial scan\n", "abstract": " An efficient deterministic BIST scheme based on partial scan chains together with a scan selection algorithm tailored for BIST is presented. The algorithm determines a minimum number of flipflops to be scannable so that the remaining circuit has a pipeline-like structure. Experiments show that scanning less flipflops may even decrease the hardware overhead for the on-chip pattern generator besides the classical advantages of partial scan such as less impact on the system performance and less hardware overhead.", "num_citations": "20\n", "authors": ["507"]}
{"title": "Test and testable design\n", "abstract": " Defects may occur during the fabrication process and during the lifetime of integrated circuits. Integrating a faulty device into systems will result in expensive repairs or even in unsafe situations and should be avoided by testing the chips           This section explains defect mechanisms and their consequences for the product quality. Methods for test pattern generation are discussed, and it is shown how these methods can already be supported in the design phase. Modern systems-on-chip often have the capabilities of testing themselves, and recent built-in self-test techniques (BIST) are presented.", "num_citations": "19\n", "authors": ["507"]}
{"title": "Accurate X-propagation for test applications by SAT-based reasoning\n", "abstract": " Unknown or X-values during test applications may originate from uncontrolled sequential cells or macros, from clock or A/D boundaries, or from tristate logic. The exact identification of X-value propagation paths in logic circuits is crucial in logic simulation and fault simulation. In the first case, it enables the proper assessment of expected responses and the effective and efficient handling of X-values during test response compaction. In the second case, it is important for a proper assessment of fault coverage of a given test set and consequently influences the efficiency of test pattern generation. The commonly employed  n -valued logic simulation evaluates the propagation of X-values only pessimistically, i.e., the X-propagation paths found by  n  -valued logic simulation are a superset of the actual propagation paths. This paper presents an efficient method for overcoming this pessimism and for determining accurately\u00a0\u2026", "num_citations": "18\n", "authors": ["507"]}
{"title": "A hybrid fault tolerant architecture for robustness improvement of digital circuits\n", "abstract": " In this paper, a novel hybrid fault tolerant architecture for digital circuits is proposed in order to enable the use of future CMOS technology nodes. This architecture targets robustness, power consumption and yield at the same time, at area costs comparable to standard fault tolerance schemes. The architecture increases circuit robustness by tolerating both transient and permanent online faults. It consumes less power than the classical Triple Modular Redundancy (TMR) approach while utilizing comparable silicon area. It overcomes many permanent faults occurring throughout manufacturing while still tolerating soft errors introduced by particle strikes. These can be done by using scalable redundancy resources, while keeping the hardened combinational logic circuits intact. The technique combines different types of redundancy: information redundancy for error detection, temporal redundancy for soft error\u00a0\u2026", "num_citations": "18\n", "authors": ["507"]}
{"title": "Structural test for graceful degradation of NoC switches\n", "abstract": " Networks-on-Chip (NoCs) are implicitly fault tolerant due to their inherent redundancy. They can overcome defective cores, links and switches. As a side effect, yield is increased at the cost of reduced performability. In this paper, a new diagnosis method based on the standard flow of industrial volume testing is presented, which is able to identify the intact functions rather than providing only a pass/fail result for the complete switch. The new method combines for the first time the precision of structural testing with information on the functional behavior in the presence of defects to determine the unaffected switch functions and use partially defective NoC switches. According to the experimental results, this improves the performability of NoCs as more than 61\\% of defects only impair one switch port. Unlike previous methods for implementing fault tolerant switches, the developed technique does not impose any\u00a0\u2026", "num_citations": "18\n", "authors": ["507"]}
{"title": "Structural-based power-aware assignment of don't cares for peak power reduction during scan testing\n", "abstract": " Scan architectures, though widely used in modern designs for testing purpose, are expensive in power consumption. In this paper, we first discuss the issues of excessive peak power consumption during scan testing. We next show that taking care of high current levels during the test cycle (i.e. between launch and capture) is highly relevant so as to avoid noise phenomena such as IR-drop or Ground Bounce. Then, we propose a solution based on power-aware assignment of don't care bits in deterministic test patterns that considers structural information of the circuit under test. Experiments have been performed on ISCAS'89 and ITC'99 benchmark circuits with the proposed structural-based power-aware X-filling technique. These results show that the proposed technique provides the best tradeoff between peak power reduction and increase of test sequence length", "num_citations": "18\n", "authors": ["507"]}
{"title": "Circuit partitioning for efficient logic BIST synthesis\n", "abstract": " A divide-and-conquer approach using circuit partitioning is presented, which can be used to accelerate logic BIST synthesis procedures. Many BIST synthesis algorithms contain steps with a time complexity which increases more than linearly with the circuit size. By extracting sub-circuits which are almost constant in size, BIST synthesis for very large designs may be possible within linear time. The partitioning approach does nor require any physical modifications of the circuit under test. Experiments show that significant performance improvements can be obtained at the cost of a longer test application time or a slight increase in silicon area for the BIST hardware.", "num_citations": "18\n", "authors": ["507"]}
{"title": "Optimized synthesis techniques for testable sequential circuits\n", "abstract": " The authors describe a synthesis approach that maps a behavioral finite state machine (FSM) description into a testable gate-level structure. The term testable, besides implying the existence of tests, also means that the application of test patterns is facilitated. Depending on the test strategy, the state registers of the FSM are modified, e.g. as scan path or self-test registers. The additional functionality of these state registers is utilized in system mode by interpreting them as smart state registers, capable of producing certain state transitions on their own. To make the best use of such registers, the authors propose a novel state encoding strategy based on an analytic formulation of the coding constraint satisfaction problem as a quadratic assignment problem. An additional minimization potential can be exploited by appropriately choosing the pattern generator for self-testable designs. Experimental results indicate that\u00a0\u2026", "num_citations": "18\n", "authors": ["507"]}
{"title": "Parallel self-test and the synthesis of control units\n", "abstract": " Most self-test techniques are implemented with so-called multifunctional test registers at any specific time either used for pattern generation or for response analysis. In a parallel self-test, however, test registers are used for pattern generation and response analysis simultaneously. In this paper a novel circuit structure for controllers with parallel self-test is presented, which does not result in a loss of fault coverage. By using a dedicated synthesis procedure, which considers the self-test hardware while generating the circuit structure instead of adding it after the design is completed (\"synthesis for testability\"), the self-test overhead can be kept low. The structure also facilitates realistic dynamic tests. As an example to illustrate the approach, the IEEE boundary scan controller is used.", "num_citations": "18\n", "authors": ["507"]}
{"title": "On fault modeling for dynamic MOS circuits\n", "abstract": " Static nMOS and static CMOS circuits show some serious problems for fault modeling and testing. In this paper we point out, that most of these problems are avoided by using dynamic nMOS or dynamic CMOS circuits. Stuck-open faults in this case do not result in sequential behaviour. A logical fault model is presented, where a fault of a logic gate will cause either a faulty combinational function or a degradation of the performance. Integrated test tools for technology dependent logical fault models based on random self test techniques are presented.", "num_citations": "18\n", "authors": ["507"]}
{"title": "Efficient algorithm-based fault tolerance for sparse matrix operations\n", "abstract": " We propose a fault tolerance approach for sparse matrix operations that detects and implicitly locates errors in the results for efficient local correction. This approach reduces the runtime overhead for fault tolerance and provides high error coverage. Existing algorithm-based fault tolerance approaches for sparse matrix operations detect and correct errors, but they often rely on expensive error localization steps. General checkpointing schemes can induce large recovery cost for high error rates. For sparse matrix-vector multiplications, experimental results show an average reduction in runtime overhead of 43.8%, while the error coverage is on average improved by 52.2% compared to related work. The practical applicability is demonstrated in a case study using the iterative Preconditioned Conjugate Gradient solver. When scaling the error rate by four orders of magnitude, the average runtime overhead increases only\u00a0\u2026", "num_citations": "17\n", "authors": ["507"]}
{"title": "Optimized selection of frequencies for faster-than-at-speed test\n", "abstract": " Small gate delay faults (SDFs) are not detectable at-speed, if they can only be propagated along short paths. These hidden delay faults (HDFs) do not influence the circuit's behavior initially, but they may indicate design marginalities leading to early-life failures, and therefore they cannot be neglected. HDFs can be detected by faster-than-at-speed test (FAST), where typically several different frequencies are used to maximize the coverage. A given set of test patterns P potentially detects a HDF if it contains a test pattern sensitizing a path through the fault site, and the efficiency of FAST can be measured as the ratio of actually detected HDFs to potentially detected HDFs. The paper at hand targets maximum test efficiency with a minimum number of frequencies. The procedure starts with a test set for transition delay faults and a set of preselected equidistant frequencies. Timing-accurate simulation of this initial setup\u00a0\u2026", "num_citations": "17\n", "authors": ["507"]}
{"title": "Scan chain organization for embedded diagnosis\n", "abstract": " Keeping diagnostic resolution as high as possible while maximizing the compaction ratio is subject to research since the advent of embedded test. In this paper, we present a novel scan design methodology to maximize diagnostic resolution when compaction is employed. The essential idea is to consider the diagnostic resolution during the clustering of scan elements to scan chains. Our methodology does not depend on a fault model and is helpful with any type of compactor. A linear time heuristic is presented to solve the scan chain clustering problem. We evaluate our approach for industrial and academic benchmark circuits. It turns out to be superior to both random and to layout driven scan chain clustering. The methodology is applicable to any gate-level design and fits smoothly into an industrial design flow.", "num_citations": "17\n", "authors": ["507"]}
{"title": "An efficient procedure for the synthesis of fast self-testable controller structures\n", "abstract": " The BIST implementation of a conventionally synthesized controller in most cases requires the integration of an additional register only for rest purposes. This leads to some serious drawbacks concerning the fault coverage, the system speed and the area overhead. A synthesis technique is presented which uses the additional test register also to implement the system function by supporting self-testable pipeline-like controller structures. It will be shown, that if the need of two different registers in the final structure is already taken into account during synthesis, then the overall number of flipflops can be reduced, and the fault coverage and system speed call be enhanced. The presented algorithm constructs realizations of a given finite state machine a self-testable structure. The efficiency of the procedure is ensured by a very precise characterization of the space of suitable realizations, which avoids the computational overhead of previously published algorithms.", "num_citations": "17\n", "authors": ["507"]}
{"title": "Pushing the limits: How fault tolerance extends the scope of approximate computing\n", "abstract": " Approximate computing in hardware and software promises significantly improved computational performance combined with very low power and energy consumption. This goal is achieved by both relaxing strict requirements on accuracy and precision, and by allowing a deviating behavior from exact Boolean specifications to a certain extent. Today, approximate computing is often limited to applications with a certain degree of inherent error tolerance, where perfect computational results are not always required. However, in order to fully utilize its benefits, the scope of applications has to be significantly extended to other compute-intensive domains including science and engineering. To meet the often rather strict quality and reliability requirements for computational results in these domains, the use of appropriate characterization and fault tolerance measures is highly required. In this paper, we evaluate some of\u00a0\u2026", "num_citations": "16\n", "authors": ["507"]}
{"title": "Non-intrusive integration of advanced diagnosis features in automotive E/E-architectures\n", "abstract": " With ever more complex automotive systems, the current approach of using functional tests to locate faulty components results in very long analysis procedures and poor diagnostic accuracy. Built-In Self-Test (BIST) offers a promising alternative to collect structural diagnostic information during E/E-architecture test. However, as the automotive industry is quite cost-driven, structural diagnosis shall not deteriorate traditional design objectives. With this goal in mind, the work at hand proposes a design space exploration to integrate structural diagnostic capabilities into an E/E-architecture design. The proposed integration is performed non-intrusively, i. e., the addition and execution of tests (a) does not affect any functional applications and (b) does not require any costly changes in the communication schedules.", "num_citations": "16\n", "authors": ["507"]}
{"title": "Cross-layer dependability modeling and abstraction in system on chip\n", "abstract": " The Resilience Articulation Point (RAP) model aims at provisioning researchers and developers with a probabilistic fault abstraction and error propagation framework covering all hardware/software layers of a System on Chip. RAP assumes that physically induced faults at the technology or CMOS device layer will eventually manifest themselves as a single or multiple bit flip (s). When probabilistic error functions for specific fault origins are known at the bit or signal level, knowledge about the unit of design and its environment allow the transformation of the bit-related error functions into characteristic higher layer representations, such as error functions for data words, Finite State Machine (FSM) state, macro interfaces or software variables. Thus, design concerns at higher abstraction layers can be investigated without the necessity to further consider the full details of lower levels of design. This paper introduces the ideas of RAP based on examples of radiation induced soft errors in SRAM cells and sequential CMOS logic. It shows by example how probabilistic bit flips are systematically abstracted and propagated towards higher abstraction levels up to the application software layer, and how RAP can be used to parameterize architecture level resilience methods.", "num_citations": "16\n", "authors": ["507"]}
{"title": "Transparent structural online test for reconfigurable systems\n", "abstract": " FPGA-based reconfigurable systems allow the online adaptation to dynamically changing runtime requirements. However, the reliability of modern FPGAs is threatened by latent defects and aging effects. Hence, it is mandatory to ensure the reliable operation of the FPGA's reconfigurable fabric. This can be achieved by periodic or on-demand online testing. In this paper, a system-integrated, transparent structural online test method for runtime reconfigurable systems is proposed. The required tests are scheduled like functional workloads, and thorough optimizations of the test overhead reduce the performance impact. The proposed scheme has been implemented on a reconfigurable system. The results demonstrate that thorough testing of the reconfigurable fabric can be achieved at negligible performance impact on the application.", "num_citations": "16\n", "authors": ["507"]}
{"title": "Fast controllers for data dominated applications\n", "abstract": " A target structure for implementing fast edge-triggered control units is presented. In many cases, the proposed controller is faster than a one-hot encoded structure as its correct timing does not require master-slave flip-flops even in the presence of unpredictable clocking skews. A synthesis procedure is proposed which leads to a performance improvement of 40% on average for the standard benchmark set whereas the additional area is less than 25% compared with conventional finite state machine (FSM) synthesis. The proposed approach is compatible with the state-of-the-art methods for FSM decomposition, state encoding and logic synthesis.", "num_citations": "16\n", "authors": ["507"]}
{"title": "Simulation results of an efficient defect analysis procedure\n", "abstract": " For obtaining a zero defect level, a high fault coverage with respect to the stuck-at fault model is often not sufficient as there are many defects that show a more complex behavior. In this paper, a method is presented for computing the occurrence probabilities of certain defects and the realistic fault coverage for test sets. The method is highly efficient as a pre-processing step is used for partitioning the layout and extracting the defects ranked in the order of their occurrence probabilities. The method was applied to a public domain library where defects causing a complex faulty behavior are possible. The occurrence probability of these faults was computed, and the defect coverage for different test sets was determined.", "num_citations": "16\n", "authors": ["507"]}
{"title": "From embedded test to embedded diagnosis\n", "abstract": " Testing integrated circuits with millions of transistors puts strong requirements on test volume, test application time, test speed, and test resolution. To overcome these challenges, it is widely accepted to partition test resources between the automatic test equipment (ATE) and the circuit under test (CUT). These strategies may reach from simple test data compression/decompression schemes to implementing a complete built-in self-test. Very often these schemes come with reduced diagnostic resolution. In this paper, an overview is given on techniques for embedding test into a circuit while still keeping diagnostic capabilities. Built-in diagnosis techniques may be used after manufacturing, for chip characterization and field return analysis, and even for rapid prototyping.", "num_citations": "15\n", "authors": ["507"]}
{"title": "Combining deterministic logic bist with test point insertion\n", "abstract": " This paper presents a logic BIST approach which combines deterministic logic BIST with test point insertion. Test points are inserted to obtain a first testability improvement, and next a deterministic pattern generator is added to increase the fault efficiency up to 100%. The silicon cell area for the combined approach is smaller than for approaches that apply a deterministic pattern generator or test points only. The combined approach also removes the classical limitations and drawbacks of test point insertion, such as failing to achieve complete fault coverage and a complicated design flow. The benefits of the combined approach are demonstrated in experimental results on a large number of ISCAS and industrial circuits.", "num_citations": "15\n", "authors": ["507"]}
{"title": "Adaptive Bayesian diagnosis of intermittent faults\n", "abstract": " With increasing transient error rates, distinguishing intermittent and transient faults is especially challenging. In addition to particle strikes relatively high transient error rates are observed in architectures for opportunistic computing and in technologies under high variations. This paper presents a method to classify faults into permanent, intermittent and transient faults based on some intermediate signatures during embedded test or built-in self-test.               Permanent faults are easily determined by repeating test sessions. Intermittent and transient faults can be identified by the amount of failing test sessions in many cases. For the remaining faults, a Bayesian classification technique has been developed which is applicable to large digital circuits. The combination of these methods is able to identify intermittent faults with a probability of more than 98\u00a0%.", "num_citations": "14\n", "authors": ["507"]}
{"title": "Synthesis of workload monitors for on-line stress prediction\n", "abstract": " Stringent reliability requirements call for monitoring mechanisms to account for circuit degradation throughout the complete system lifetime. In this work, we efficiently monitor the stress experienced by the system as a result of its current workload. To achieve this goal, we construct workload monitors that observe the most relevant subset of the circuit's primary and pseudo-primary inputs and produce an accurate stress approximation. The proposed approach enables the timely adoption of suitable countermeasures to reduce or prevent any deviation from the intended circuit behavior. The relation between monitoring accuracy and hardware cost can be adjusted according to design requirements. Experimental results show the efficiency of the proposed approach for the prediction of stress induced by Negative Bias Temperature Instability (NBTI) in critical and near-critical paths of a digital circuit.", "num_citations": "14\n", "authors": ["507"]}
{"title": "Algorithm-based fault tolerance for many-core architectures\n", "abstract": " Modern many-core architectures with hundreds of cores provide a high computational potential. This makes them particularly interesting for scientific high-performance computing and simulation technology. Like all nano scaled semiconductor devices, many-core processors are prone to reliability harming factors like variations and soft errors. One way to improve the reliability of such systems is software-based hardware fault tolerance. Here, the software is able to detect and correct errors introduced by the hardware. In this work, we propose a software-based approach to improve the reliability of matrix operations on many-core processors. These operations are key components in many scientific applications.", "num_citations": "14\n", "authors": ["507"]}
{"title": "Signature rollback-a technique for testing robust circuits\n", "abstract": " Dealing with static and dynamic parameter variations has become a major challenge for design and test. To avoid unnecessary yield loss and to ensure reliable system operation a robust design has become mandatory. However, standard structural test procedures still address classical fault models and cannot deal with the non-deterministic behavior caused by parameter variations and other reasons. Chips may be rejected, even if the test reveals only non-critical failures that could be compensated during system operation. This paper introduces a scheme for embedded test, which can distinguish critical permanent and non-critical transient failures for circuits with time redundancy. To minimize both yield loss and the overall test time, the scheme relies on partitioning the test into shorter sessions. If a faulty signature is observed at the end of a session, a rollback is triggered, and this particular session is repeated. An\u00a0\u2026", "num_citations": "14\n", "authors": ["507"]}
{"title": "A refined electrical model for particle strikes and its impact on SEU prediction\n", "abstract": " Decreasing feature sizes have led to an increased vulnerability of random logic to soft errors. A particle strike may cause a glitch or single event transient (SET) at the output of a gate, which in turn can propagate to a register and cause a single event upset (SEU) there. Circuit level modeling and analysis of SETs provides an attractive compromise between computationally expensive simulations at device level and less accurate techniques at higher levels. At the circuit level particle strikes crossing a pn-junction are traditionally modeled with the help of a transient current source. However, the common models assume a constant voltage across the pn-junction, which may lead to inaccurate predictions concerning the shape of expected glitches. To overcome this problem, a refined circuit level model for strikes through pn-junctions is investigated and validated in this paper. The refined model yields significantly\u00a0\u2026", "num_citations": "14\n", "authors": ["507"]}
{"title": "Analyzing test and repair times for 2D integrated memory built-in test and repair\n", "abstract": " An efficient on-chip infrastructure for memory test and repair is crucial to enhance yield and availability of SoCs. A commonly used repair strategy is to equip memories with spare rows and columns (2D redundancy). To avoid the prohibitive storage requirements for failure bitmaps and the complex data structures inherent in most algorithms for offline repair analysis, existing heuristics for built-in repair analysis (BIRA) either use very simple search strategies or restrict the search to smaller local bitmaps. Exact BIRA algorithms work with sub analyzers for each possible repair combination. While a parallel implementation suffers from a high hardware overhead, a serial implementation leads to increased test times. Recently an integrated built-in test and repair approach has been proposed which interleaves test and repair analysis and supports an exact solution with moderate hardware overhead and reasonable test\u00a0\u2026", "num_citations": "14\n", "authors": ["507"]}
{"title": "Trustworthy reconfigurable access to on-chip infrastructure\n", "abstract": " The accessibility of on-chip embedded infrastructure for test, reconfiguration, or debug poses a serious security problem. Access mechanisms based on IEEE Std 1149.1 (JTAG), and especially reconfigurable scan networks (RSNs), as allowed by IEEE Std 1500, IEEE Std 1149.1-2013, and IEEE Std 1687 (IJTAG), require special care in the design and development. This work studies the threats to trustworthy data transmission in RSNs posed by untrusted components within the RSN and external interfaces. We propose a novel scan pattern generation method that finds trustworthy access sequences to prevent sniffing and spoofing of transmitted data in the RSN. For insecure RSNs, for which such accesses do not exist, we present an automated transformation that improves the security and trustworthiness while preserving the accessibility to attached instruments. The area overhead is reduced based on results from\u00a0\u2026", "num_citations": "13\n", "authors": ["507"]}
{"title": "Efficient observation point selection for aging monitoring\n", "abstract": " Circuit aging causes a performance degradation and eventually a functional failure. It depends on the workload and the environmental condition of the system, which are hard to predict in early design phases resulting in pessimistic design. Existing delay monitoring schemes measure the remaining slack of paths in the circuit, but have a high hardware penalty including global wiring. More importantly, a low sensitization ratio of long paths in applications may lead to a very low measurement frequency or even unmonitored timing violations. In this work, we propose a delay monitor placement method by analyzing the topological circuit structure and sensitization of paths. The delay monitors are inserted at meticulously selected positions in the circuit, named observation points (OPs). This OP monitor placement method can reduce the number of inserted monitors by up to 98% compared to a placement at the end of\u00a0\u2026", "num_citations": "13\n", "authors": ["507"]}
{"title": "Structural software-based self-test of network-on-chip\n", "abstract": " Software-Based Self-Test (SBST) is extended to the switches of complex Network-on-Chips (NoC). Test patterns for structural faults are turned into valid packets by using satisfiability (SAT) solvers. The test technique provides a high fault coverage for both manufacturing test and online test.", "num_citations": "13\n", "authors": ["507"]}
{"title": "Efficient concurrent self-test with partially specified patterns\n", "abstract": " Structural on-line self-test may be performed to detect permanent faults and avoid their accumulation in the system. This paper improves existing techniques for concurrent BIST that are based on a deterministic test set. Here, the test patterns are specially generated with a small number of specified bits. This results in very low test length and fault detection latency, which allows to frequently test critical faults. As a consequence, the likelihood of fault accumulation is reduced. Experiments with benchmark circuits show that the hardware overhead is significantly lower than the overhead of the state of the art. Moreover, a case-study on a super-scalar RISC processor demonstrates the feasibility of the method.", "num_citations": "13\n", "authors": ["507"]}
{"title": "Transparent word-oriented memory BIST based on symmetric march algorithms\n", "abstract": " The paper presents a new approach to transparent BIST for wordoriented RAMs which is based on the transformation of March transparent test algorithms to the symmetric versions. This approach allows to skip the signature prediction phase inherent to conventional transparent memory testing and therefore to significantly reduce test time. The hardware overhead and fault coverage of the new BIST scheme are comparable to the conventional transparent BIST structures. Experimental results show that in many cases the proposed test techniques achieve a higher fault coverage in shorter test time.", "num_citations": "13\n", "authors": ["507"]}
{"title": "Synthesis of self-testable controllers\n", "abstract": " The paper presents a synthesis approach for pipeline-like controller structures. These structures allow to implement a built-in self-test in two sessions without any extra test registers. Hence the additional delay imposed by the test circuitry is reduced, the fault coverage is increased, and in many cases the overall area is minimal, too. The self-testable structure for a given finite state machine specification is derived from an appropriate realization of the machine. A theorem is proven that such realizations can be constructed by means of partition pairs. An algorithm to determine optimal realizations is developed and benchmark experiments are presented to demonstrate the applicability of the presented approach.< >", "num_citations": "13\n", "authors": ["507"]}
{"title": "OTERA: Online test strategies for reliable reconfigurable architectures\u2014Invited paper for the AHS-2012 special session \u201cDependability by reconfigurable hardware\u201d\n", "abstract": " FPGA-based reconfigurable systems allow the online adaptation to dynamically changing runtime requirements. However, the reliability of FPGAs, which are manufactured in latest technologies, is threatened not only by soft errors, but also by aging effects and latent defects. To ensure reliable reconfiguration, it is mandatory to guarantee the correct operation of the underlying reconfigurable fabric. This can be achieved by periodic or on-demand online testing. The OTERA project develops and evaluates components and strategies for reconfigurable systems that feature reliable reconfiguration. The research focus ranges from structural online tests for the FPGA infrastructure and functional online tests for the configured functionality up to the resource management and test scheduling. This paper gives an overview of the project tasks and presents first results.", "num_citations": "12\n", "authors": ["507"]}
{"title": "Integrating scan design and soft error correction in low-power applications\n", "abstract": " Error correcting coding is the dominant technique to achieve acceptable soft-error rates in memory arrays. In many modern circuits, the number of memory elements in the random logic is in the order of the number of SRAM cells on chips only a few years ago. Often latches are clock gated and have to retain their states during longer periods. Moreover, miniaturization has led to elevated susceptibility of the memory elements and further increases the need for protection. This paper presents a fault-tolerant register latch organization that is able to detect single-bit errors while it is clock gated. With active clock, single and multiple errors are detected. The registers can be efficiently integrated similar to the scan design flow, and error detecting or locating information can be collected at module level. The resulting structure can be efficiently reused for offline and general online testing.", "num_citations": "12\n", "authors": ["507"]}
{"title": "Synthesizing fast, online-testable control units\n", "abstract": " The authors present the self-checking bypass pipeline, an online-testable controller structure for data-dominated applications. For most circuits in a standard benchmark set, this structure leads to a performance improvement of more than 30% with an area overhead less than 15% that of conventional online-testable finite-state machines.", "num_citations": "12\n", "authors": ["507"]}
{"title": "The synthesis of self-test control logic\n", "abstract": " In recent years, many built-in self-test techniques have been proposed based on feedback shift-registers for pattern generation and signature analysis. But in general, these test-registers cannot test several modules of the chip concurrently, and they have to be controlled by external automatic test equipment. The authors propose a method to integrate additional test-control logic into the chip. On the basis of a register-transfer description of the circuit, the test control is derived, and a corresponding finite automation is synthesized. A hardware implementation is proposed, resulting in circuits where the entire self-test only consists in activating the test mode and clocking and evaluating the overall signature.", "num_citations": "12\n", "authors": ["507"]}
{"title": "Structure-oriented Test of Reconfigurable Scan Networks\n", "abstract": " Design, production and operation of modern system-on-chips rely on integrated instruments, which range from simple sensors to complex debug interfaces and design-for-test (DfT) structures. Reconfigurable scan networks (RSNs) as defined in IEEE Std. 1687-2014 provide an efficient access mechanism to such instruments. It is essential to test the access mechanism itself before it can be used for test, diagnosis, validation, calibration or runtime monitoring. Realistic fault mechanisms in RSNs are hard to test due to their high sequential depth and limited controllability and observability via serial scan ports.We present a novel low-cost DfT modification specifically designed for RSNs that enhances the observability of shadow registers. Furthermore, we present different test methods for stuck-at and more realistic gate-level fault models like flip-flop-internal and bridge faults. Experimental results demonstrate the\u00a0\u2026", "num_citations": "11\n", "authors": ["507"]}
{"title": "Low-overhead fault-tolerance for the preconditioned conjugate gradient solver\n", "abstract": " Linear system solvers are an integral part for many different compute-intensive applications and they benefit from the compute power of heterogeneous computer architectures. However, the growing spectrum of reliability threats for such nano-scaled CMOS devices makes the integration of fault tolerance mandatory. The preconditioned conjugate gradient (PCG) method is one widely used solver as it finds solutions typically faster compared to direct methods. Although this iterative approach is able to tolerate certain errors, latest research shows that the PCG solver is still vulnerable to transient effects. Even single errors, for instance, caused by marginal hardware, harsh environments, or particle radiation, can considerably affect execution times, or lead to silent data corruption. In this work, a novel fault-tolerant PCG solver with extremely low runtime overhead is proposed. Since the error detection method does not\u00a0\u2026", "num_citations": "11\n", "authors": ["507"]}
{"title": "Efficient variation-aware statistical dynamic timing analysis for delay test applications\n", "abstract": " Increasing parameter variations, caused by variations in process, temperature, power supply, and wear-out, have emerged as one of the most important challenges in semiconductor manufacturing and test. As a consequence for gate delay testing, a single test vector pair is no longer sufficient to provide the required low test escape probabilities for a single delay fault. Recently proposed statistical test generation methods are therefore guided by a metric, which defines the probability of detecting a delay fault with a given test set. However, since runtime and accuracy are dominated by the large number of required metric evaluations, more efficient approximation methods are mandatory for any practical application. In this work, a new statistical dynamic timing analysis algorithm is introduced to tackle this problem. The associated approximation error is very small and predominantly caused by the impact of delay\u00a0\u2026", "num_citations": "11\n", "authors": ["507"]}
{"title": "Acceleration of Monte-Carlo molecular simulations on hybrid computing architectures\n", "abstract": " Markov-Chain Monte-Carlo (MCMC) methods are an important class of simulation techniques, which execute a sequence of simulation steps, where each new step depends on the previous ones. Due to this fundamental dependency, MCMC methods are inherently hard to parallelize on any architecture. The upcoming generations of hybrid CPU/GPGPU architectures with their multi-core CPUs and tightly coupled many-core GPGPUs provide new acceleration opportunities especially for MCMC methods, if the new degrees of freedom are exploited correctly. In this paper, the outcomes of an interdisciplinary collaboration are presented, which focused on the parallel mapping of a MCMC molecular simulation from thermodynamics to hybrid CPU/GPGPU computing systems. While the mapping is designed for upcoming hybrid architectures, the implementation of this approach on an NVIDIA Tesla system already leads\u00a0\u2026", "num_citations": "11\n", "authors": ["507"]}
{"title": "Efficient multi-level fault simulation of HW/SW systems for structural faults\n", "abstract": " In recent technology nodes, reliability is increasingly considered a part of the standard design flow to be taken into account at all levels of embedded systems design. While traditional fault simulation techniques based on low-level models at gate- and register transfer-level offer high accuracy, they are too inefficient to properly cope with the complexity of modern embedded systems. Moreover, they do not allow for early exploration of design alternatives when a detailed model of the whole system is not yet available, which is highly required to increase the efficiency and quality of the design flow. Multi-level models that combine the simulation efficiency of high abstraction models with the accuracy of low-level models are therefore essential to efficiently evaluate the impact of physical defects on the system. This paper proposes a methodology to efficiently implement concurrent multi-level fault simulation across\u00a0\u2026", "num_citations": "11\n", "authors": ["507"]}
{"title": "Efficient simulation of structural faults for the reliability evaluation at system-level\n", "abstract": " In recent technology nodes, reliability is considered a part of the standard design \u00bfow at all levels of embedded system design. While techniques that use only low-level models at gate- and register transfer-level offer high accuracy, they are too inefficient to consider the overall application of the embedded system. Multi-level models with high abstraction are essential to efficiently evaluate the impact of physical defects on the system. This paper provides a methodology that leverages state-of-the-art techniques for efficient fault simulation of structural faults together with transaction-level modeling. This way it is possible to accurately evaluate the impact of the faults on the entire hardware/software system. A case study of a system consisting of hardware and software for image compression and data encryption is presented and the method is compared to a standard gate/RT mixed-level approach.", "num_citations": "11\n", "authors": ["507"]}
{"title": "A unified approach for the synthesis of self-testable finite state machines\n", "abstract": " Conventionally self-test hardware is added after synthesis is completed. For highly sequential circuits like controllers this design method either leads to high hardware overheads or compromises fault coverage. In this paper we outline a unified approach for considering self-test hardware like pattern generators and signature registers during synthesis. Three novel target structures are present@ and a method for designing parallel self-testable circuits is discussed in more detail. For a collection of benchmark circuits we show that hardware overheads for self-testable circuits can be significantly reduced this way without sacriilcing testability.", "num_citations": "11\n", "authors": ["507"]}
{"title": "Generating pattern sequences for the pseudo-exhaustive test of MOS-circuits\n", "abstract": " A method based on linear feedback shift registers over finite fields is presented to generate for a natural number n a pattern sequence with minimal length detecting each m-multiple stuck-open faults for M\u2264n. A hardware architecture is discussed generating this sequence, and for n=1 a built-in self-test (BIST) approach is presented that detects all combinations of multiple combinational and single stuck-open faults. The sequences are of minimum length, and can be produced either by software, by an external chip, or be a BIST-structure. Using the latter, the hardware overhead would be of the same magnitude as a conventional pseudorandom architecture.", "num_citations": "11\n", "authors": ["507"]}
{"title": "Time-optimal control policies for cascaded production-inventory systems with control and state constraints\n", "abstract": " In this paper time-optimal control policies are derived for models of production-inventory systems consisting of a cascade of basic production-inventory systems with control and state constraints. The analytic solution is due to a decoupling of the complete system into its subsystems by a recursive definition of the cascaded system. It is shown that there is at least one bang-bang controlled subsystem. For the \u2018 other\u2019 subsystems singular control policies are obtained. Introducing a pseudo-bang-bang control for these systems it is demonstrated that by strengthening the constraints there is a continuous transition from a singular to a bang-bang control.", "num_citations": "11\n", "authors": ["507"]}
{"title": "Test strategies for reconfigurable scan networks\n", "abstract": " On-chip infrastructure is an essential part of today's complex designs and enables their cost-efficient manufacturing and operation. The diversity and high number of infrastructure elements demands flexible and low-latency access mechanisms, such as reconfigurable scan networks (RSNs). The correct operation of the infrastructure access itself is highly important for the test of the system logic, its diagnosis, debug and bring-up, as well as post-silicon validation. Ensuring correct operation requires the thorough testing of the RSN.Because of sequential and combinational dependencies in RSN accesses, test generation for general RSNs is computationally very difficult and requires dedicated test strategies. This paper explores different test strategies for general RSNs and discusses the achieved structural fault coverage. Experimental results show that the combination of functional test heuristics together with a\u00a0\u2026", "num_citations": "10\n", "authors": ["507"]}
{"title": "Data-parallel simulation for fast and accurate timing validation of CMOS circuits\n", "abstract": " Gate-level timing simulation of combinational CMOS circuits is the foundation of a whole array of important EDA tools such as timing analysis and power-estimation, but the demand for higher simulation accuracy drastically increases the runtime complexity of the algorithms. Data-parallel accelerators such as Graphics Processing Units (GPUs) provide vast amounts of computing performance to tackle this problem, but require careful attention to control-flow and memory access patterns. This paper proposes the novel High-Throughput Oriented Parallel Switch-level Simulator (HiTOPS), which is especially designed to take full advantage of GPUs and provides accurate timesimulation for multi-million gate designs at an unprecedented throughput. HiTOPS models timing at transistor granularity and supports all major timing-related effects found in CMOS including pattern-dependent delay, glitch filtering and transition\u00a0\u2026", "num_citations": "10\n", "authors": ["507"]}
{"title": "Reuse of structural volume test methods for in-system testing of automotive ASICs\n", "abstract": " The automotive industry has to deal with an increasing amount of electronics in today's vehicles. This paper describes the advantages of structural tests during in-field system test, reusing existing test data and on-chip structures. Demonstration is the embedded test of an ASIC within an automotive control unit, utilizing manufacturing scan-tests.", "num_citations": "10\n", "authors": ["507"]}
{"title": "Soft error correction in embedded storage elements\n", "abstract": " In this paper a soft error correction scheme for embedded storage elements in level sensitive designs is presented. It employs space redundancy to detect and locate Single Event Upsets (SEUs). It is able to detect SEUs in registers and employ architectural replay to perform correction with low additional hardware overhead. Together with the proposed bit flipping latch an online correction can be implemented on bit level with a minimal loss of clock cycles. A comparison with other detection and correction schemes shows a significantly lower hardware overhead.", "num_citations": "10\n", "authors": ["507"]}
{"title": "Test set stripping limiting the maximum number of specified bits\n", "abstract": " This paper presents a technique that limits the maximum number of specified bits of any pattern in a given test set. The outlined method uses algorithms similar to ATPG, but exploits the information in the test set to quickly find test patterns with the desired properties. The resulting test sets show a significant reduction in the maximum number of specified bits in the test patterns. Furthermore, for commercial ATPG test sets even the overall number of specified bits is reduced substantially.", "num_citations": "10\n", "authors": ["507"]}
{"title": "Minimizing peak power consumption during scan testing: Structural technique for don't care bits assignment\n", "abstract": " Scan architectures, though widely used in modern designs for testing purpose, are expensive in power consumption. In this paper, we first discuss the issues of excessive peak power consumption during scan testing. We next show that taking care of high current levels during the test cycle (i.e. between launch and capture) is highly relevant so as to avoid noise phenomena such as IR-drop or ground bounce. Then, we propose a solution based on power-aware assignment of don't care bits in deterministic test patterns that considers structural information of the circuit under test. Experiments have been performed on ISCAS'89 and ITC'99 benchmark circuits. These results show that the proposed technique provides the best tradeoff between peak power reduction and increase of test sequence length", "num_citations": "10\n", "authors": ["507"]}
{"title": "Test register insertion with minimum hardware cost\n", "abstract": " Implementing a built-in self-test by a test per clock scheme offers advantages concerning fault coverage, detection of delay faults, and test application time. Such a scheme is implemented by test registers, for instance BILBOs and CBILBOs, which are inserted into the circuit structure at appropriate places. An algorithm is presented which is able to find the cost optimal placement of test registers for nearly all the ISCAS'89 sequential benchmark circuits, and a suboptimal solution with slightly higher costs is obtained for all the circuits within a few minutes of computing time. The algorithm can also be applied to the Minimum Feedback Vertex Set problem in partial scan design, and an optimal solution is found for all the benchmark circuits. The resulting self-testable circuits are analyzed. It is found that often CBILBOs lead to a minimum hardware overhead and also simplify test scheduling and test control.", "num_citations": "10\n", "authors": ["507"]}
{"title": "Efficient on-line fault-tolerance for the preconditioned conjugate gradient method\n", "abstract": " Linear system solvers are key components of many scientific applications and they can benefit significantly from modern heterogeneous computer architectures. However, such nano-scaled CMOS devices face an increasing number of reliability threats, which make the integration of fault tolerance mandatory. The preconditioned conjugate gradient method (PCG) is a very popular solver since it typically finds solutions faster than direct methods, and it is less vulnerable to transient effects. However, as latest research shows, the vulnerability is still considerable. Even single errors caused, for instance, by marginal hardware, harsh operating conditions or particle radiation can increase execution times considerably or corrupt solutions without indication. In this work, a novel and highly efficient fault-tolerant PCG method is presented. The method applies only two inner products to reliably detect errors. In case of errors\u00a0\u2026", "num_citations": "9\n", "authors": ["507"]}
{"title": "A pseudo-dynamic comparator for error detection in fault tolerant architectures\n", "abstract": " Although CMOS technology scaling offers many advantages, it suffers from robustness problem caused by hard, soft and timing errors. The robustness of future CMOS technology nodes must be improved and the use of fault tolerant architectures is probably the most viable solution. In this context, Duplication/Comparison scheme is widely used for error detection. Traditionally, this scheme uses a static comparator structure that detects hard error. However, it is not effective for soft and timing errors detection due to the possible masking of glitches by the comparator itself. To solve this problem, we propose a pseudo-dynamic comparator architecture that combines a dynamic CMOS transition detector and a static comparator. Experimental results show that the proposed comparator detects not only hard errors but also small glitches related to soft and timing errors. Moreover, its dynamic characteristics allow reducing\u00a0\u2026", "num_citations": "9\n", "authors": ["507"]}
{"title": "Academic network for microelectronic test education\n", "abstract": " This paper is an overview of the activities performed in the framework of the European IST project EuNICE-Test (European Network for Initial and Continuing Education in VLSI/SOC Testing) using remote automatic test equipment (ATE)), addressing the shortage of skills in the microelectronics industry in the field of electronic testing. The project was based on the experience of the common test resource centre (CRTC) for French universities. In the framework of the EuNICETest project, the existing network expanded to 4 new academic centres: Universitat Polite\u00c1cnica de Catalunya, Spain, Politecnico di Torino, Italy, University of Stuttgart, Germany and Jozef Stefan Institute Ljubljana, Slovenia. Assessments of the results achieved are presented as well as course topics and possible future extensions.", "num_citations": "9\n", "authors": ["507"]}
{"title": "Development of an audio player as system-on-a-chip using an open source platform\n", "abstract": " In this paper, we report on our experience in developing an SoC audio player using various open source components in both hardware and software parts as well as in the development process. The Ogg Vorbis audio decoder targeted for limited computing resource and low power consumption devices was developed on the free LEON SoC platform, which features a SPARC-V8 architecture compatible processor and AMBA bus. The decoder runs on the open source RTEMS operating system making use of the royalty-free open source Vorbis library. We also aim to illustrate the use of hardware/software co-design techniques. Therefore, in order to speed up the decoding process, after an analysis, a computing-intensive part of the decoding algorithm was selected and designed as an AMBA compatible hardware core. The demonstration prototype was built on the XESS XSV-800 prototyping board using GNU/Linux\u00a0\u2026", "num_citations": "9\n", "authors": ["507"]}
{"title": "Configuring flip-flops to BIST registers\n", "abstract": " Built-in self-test test registers must segment a circuit such that there exists a feasible test schedule. If a register transfer description is used for selecting the positions of test registers, the space for optimizations is small. In this paper, 1-bit test cells are inserted at gate level, and an initial test schedule is constructed. Based on the information of this schedule, test cells that can be controlled in the same way are assembled to test registers. Finally, a test schedule at RT level is constructed and a minimal set of test control signals is determined. The presented approach can reduce both BIST hardware overhead and test application time. It is applicable to control units and circuits produced by control oriented synthesis where an RT description is not available. Considerable gains can also be obtained if existing RT structures are reconfigured for self-testing in the described way.", "num_citations": "9\n", "authors": ["507"]}
{"title": "Multi-layer diagnosis for fault-tolerant networks-on-chip\n", "abstract": " In order to tolerate faults that emerge in operating Networks-on-Chip, diagnosis techniques are employed for fault detection and localization. On various network layers, diverse diagnosis methods can be employed which differ in terms of their impact on network performance (e.g., by operating concurrently versus pre-empting regular network operation) and the quality of diagnostic results. In this contribution, we show how diagnosis techniques of different network layers of a Network-on-Chip can be combined into multi-layer solutions. We present the cross-layer information flow used for the interaction between the layers and show the resulting benefit of the combination compared to layer-specific diagnosis. For evaluation, we investigate the diagnosis quality and the impact on system performance to explore the entire design space of layer-specific techniques and their multi-layer combinations. We identify pareto\u00a0\u2026", "num_citations": "8\n", "authors": ["507"]}
{"title": "Dependable on-chip infrastructure for dependable MPSOCs\n", "abstract": " Today's MPSOCs employ complex on-chip infrastructure and instrumentation for efficient test, debug, diagnosis, and post-silicon validation, reliability management and maintenance in the field, or monitoring and calibration during operation. To enable flexible and efficient access to such instrumentation, reconfigurable scan networks (RSNs) as recently standardized by IEEE Std 1687 can be used. Given the importance of infrastructure for the dependability of the whole MPSOC, however, the RSN itself must be highly dependable. This paper addresses dependability issues of RSNs including verification, test, and security, and their importance for dependable MPSOCs. First research results are summarized, and open questions for future work are highlighted.", "num_citations": "8\n", "authors": ["507"]}
{"title": "Multilevel simulation of nonfunctional properties by piecewise evaluation\n", "abstract": " As the technology shrinks, nonfunctional properties (NFPs) such as reliability, vulnerability, power consumption, or heat dissipation become as important as system functionality. As NFPs often influence each other, depend on the application and workload of a system, and exhibit nonlinear behavior, NFP simulation over long periods of system operation is computationally expensive, if feasible at all. This article presents a piecewise evaluation method for efficient NFP simulation. Simulation time is divided into intervals called evaluation windows, within which the NFP models are partially linearized. High-speed functional system simulation is achieved by parallel execution of models at different levels of abstraction. A trade-off between simulation speed and accuracy is met by adjusting the size of the evaluation window. As an example, the piecewise evaluation technique is applied to analyze aging caused by two\u00a0\u2026", "num_citations": "8\n", "authors": ["507"]}
{"title": "Diagnosis of multiple faults with highly compacted test responses\n", "abstract": " Defects cluster, and the probability of a multiple fault is significantly higher than just the product of the single fault probabilities. While this observation is beneficial for high yield, it complicates fault diagnosis. Multiple faults will occur especially often during process learning, yield ramp-up and field return analysis. In this paper, a logic diagnosis algorithm is presented which is robust against multiple faults and which is able to diagnose multiple faults with high accuracy even on compressed test responses as they are produced in embedded test and built-in self-test. The developed solution takes advantage of the linear properties of a MISR compactor to identify a set of faults likely to produce the observed faulty signatures. Experimental results show an improvement in accuracy of up to 22 % over traditional logic diagnosis solutions suitable for comparable compaction ratios.", "num_citations": "8\n", "authors": ["507"]}
{"title": "Built-in self-diagnosis exploiting strong diagnostic windows in mixed-mode test\n", "abstract": " Efficient diagnosis procedures are crucial both for volume and for in-field diagnosis. In either case the underlying test strategy should provide a high coverage of realistic fault mechanisms and support a low-cost implementation. Built-in self-diagnosis (BISD) is a promising solution, if the diagnosis procedure is fully in line with the test flow. However, most known BISD schemes require multiple test runs or modifications of the standard scan-based test infrastructure. Some recent schemes circumvent these problems, but they focus on deterministic patterns to limit the storage requirements for diagnostic data. Thus, they cannot exploit the benefits of a mixed-mode test such as high coverage of non-target faults and reduced test data storage. This paper proposes a BISD scheme using mixed-mode patterns and partitioning the test sequence into \u201cweak\u201d and \u201cstrong\u201d diagnostic windows, which are treated differently during\u00a0\u2026", "num_citations": "8\n", "authors": ["507"]}
{"title": "On determining the real output Xs by SAT-based reasoning\n", "abstract": " Embedded testing, built-in self-test and methods for test compression rely on efficient test response compaction. Often, a circuit under test contains sources of unknown values (X), uninitialized memories for instance. These X values propagate through the circuit and may spoil the response signatures. The standard way to overcome this problem is X-masking. Outputs which carry an X value are usually determined by logic simulation. In this paper, we show that the amount of Xs is significantly overestimated, and in consequence outputs are over masked, too. An efficient way for the exact computation of output Xs is presented for the first time. The resulting X-masking promises significant gains with respect to test time, test volume and fault coverage.", "num_citations": "8\n", "authors": ["507"]}
{"title": "Signature analysis and test scheduling for self-testable circuits\n", "abstract": " In complex circuits the test execution is usually divided into a number of subtasks, each producing a signature in a self-test register. These signatures influence one another. A model that can be used as a basis for test scheduling procedures is presented, and it is shown how test schedules can be constructed, in order to minimize the number of signatures to be evaluated. The error masking probabilities decrease when the subtasks of the test execution are repeated in an appropriate order, and an equilibrium situation is reached where the error masking probabilities are minimal. A method is presented for constructing test schedules so that only the signatures at the primary outputs must be evaluated to get a sufficient fault coverage. Then no internal scan path is required, only a few signatures have to be evaluated at the end of the test execution, and the test control at chip and board level is simplified. The amount of hardware to implement a built-in self-test is reduced significantly.", "num_citations": "8\n", "authors": ["507"]}
{"title": "Logic module for generating unequiprobable/random patterns for integrated circuits\n", "abstract": " For assisting the self-test of circuits with unequiplebable random patterns, a logic module is provided which is composed of two types of basic cells. Each basic cell contains a register cell and a sub-circuit composed of gates. Dependent on two control signals, the basic cells can be operated as a normal register, as a shift register or as a linear feedback shift register. In the operational mode as a linear feedback shift register, the logic module can be used as a random pattern generator. To this end, the logic module is divided into a first module and into a second module. The first module contains an interconnection of two types of basic cells and a combinational logic system which operates the one part of the output signals of the basic cell in accordance with a Boolean function. The operational result is supplied to a second module of identical basic cells which operates as a shift register. When a random bit sequence\u00a0\u2026", "num_citations": "8\n", "authors": ["507"]}
{"title": "Self-test and diagnosis for self-aware systems\n", "abstract": " Self-testing hardware has a long tradition as a complement to manufacturing testing based on test stimuli and response analysis. Today, it is a mature field and many complex SoCs have self-testing structures built-in (BIST). For self-aware SoCs this is a key technology, allowing the system to distinguish between correct and erroneous behavior. This survey article reviews the state of the art and shows how these techniques are to be generalized to facilitate self-awareness.", "num_citations": "7\n", "authors": ["507"]}
{"title": "Aging monitor reuse for small delay fault testing\n", "abstract": " Small delay faults receive more and more attention, since they may indicate a circuit reliability marginality even if they do not violate the timing at the time of production. At-speed test and faster-than-at-speed test (FAST) are rather expensive tasks to test for such faults. The paper at hand avoids complex on-chip structures or expensive high-speed ATE for test response evaluation, if aging monitors which are integrated into the device under test anyway are reused. The main challenge in reusing aging monitors for FAST consists in possible false alerts at higher frequencies. While a certain test vector pair makes a delay fault observable at one monitor, it may also exceed the time slack in the fault free case at a different monitor which has to be masked. Therefore, a multidimensional optimizing problem has to be solved for minimizing the masking overhead and the number of test vectors while maximizing delay fault\u00a0\u2026", "num_citations": "7\n", "authors": ["507"]}
{"title": "Autonomous testing for 3D-ICs with IEEE std. 1687\n", "abstract": " IEEE Std. 1687, or IJTAG, defines flexible serial scan-based architectures for accessing embedded instruments efficiently. In this paper, we present a novel test architecture that employs IEEE Std. 1687 together with an efficient test controller to carry out 3D-IC testing autonomously. The test controller can deliver parallel test data for the IEEE Std. 1687 structures and the cores under test, and provide required control signals to control the whole test procedure. This design can achieve at-speed, autonomous and programmable testing in 3D-ICs. Experimental results show that the additional area and test cycle overhead of this architecture is small considering its autonomous test capability.", "num_citations": "7\n", "authors": ["507"]}
{"title": "Functional diagnosis for graceful degradation of NoC switches\n", "abstract": " Reconfigurable Networks-on-Chip (NoCs) allow discarding the corrupted ports of a defective switch instead of deactivating it entirely, and thus enable fine-grained reconfiguration of the network, making the NoC structures more robust. A prerequisite for such a fine-grained reconfiguration is to identify the corrupted port of a faulty switch.This paper presents a functional diagnosis approach which extracts structural fault information from functional tests and utilizes this information to identify the broken functions/ports of a defective switch. The broken parts are discarded while the remaining functions are used for the normal operation. The non-intrusive method introduced is independent of the switch architecture and the NoC topology and can be applied for any type of structural fault. The diagnostic resolution of the functional test is so high that for nearly 64% of the faults in the example switch only a single port has to be\u00a0\u2026", "num_citations": "7\n", "authors": ["507"]}
{"title": "Fault tolerance of approximate compute algorithms\n", "abstract": " Approximate computing algorithms cover a wide range of different applications and the boundaries to domains like variable-precision computing, where the precision of the computations can be online adapted to the needs of the application [1, 2], as well as probabilistic and stochastic computing [3], which incorporate stochastic processes and probability distributions in the target computations, are sometimes blurred. The central idea of purely algorithm-based approximate computing is to transform algorithms, without necessarily requiring approximate hardware, to trade-off accuracy against energy. Early termination of algorithms that exhibit incremental refinement [4] reduces iterations at the cost of accuracy. Loop perforation [5] approximates iteratively-computed results by identifying and reducing loops that contribute only insignificantly to the solution. Another group of approximate algorithms is represented by\u00a0\u2026", "num_citations": "7\n", "authors": ["507"]}
{"title": "Logic/clock-path-aware at-speed scan test generation for avoiding false capture failures and reducing clock stretch\n", "abstract": " IR-drop induced by launch switching activity (LSA) in capture mode during at-speed scan testing increases delay along not only logic paths (LPs) but also clock paths (Cps). Excessive extra delay along LPs compromises test yields due to false capture failures, while excessive extra delay along CPs compromises test quality due to test clock stretch. This paper is the first to mitigate the impact of LSA on both LPs and CPs with a novel LCPA (Logic/Clock Path-Aware) at-speed scan test generation scheme, featuring (1) a new metric for assessing the risk of false capture failures based on the amount of LSA around both LPs and CPs, (2) a procedure for avoiding false capture failures by reducing LSA around LPs or masking uncertain test responses, and (3) a procedure for reducing test clock stretch by reducing LSA around CPs. Experimental results demonstrate the effectiveness of the LCPA scheme in improving test\u00a0\u2026", "num_citations": "7\n", "authors": ["507"]}
{"title": "Test pattern generation in presence of unknown values based on restricted symbolic logic\n", "abstract": " Test generation algorithms based on standard n-valued logic algebras are pessimistic in presence of unknown (X) values, overestimate the number of signals with X-values and underestimate fault coverage.", "num_citations": "7\n", "authors": ["507"]}
{"title": "A new hybrid fault-tolerant architecture for digital CMOS circuits and systems\n", "abstract": " This paper presents a new hybrid fault-tolerant architecture for robustness improvement of digital CMOS circuits and systems. It targets all kinds of errors in combinational part of logic circuits and thus, can be combined with advanced SEU protection techniques for sequential elements while reducing the power consumption. The proposed architecture combines different types of redundancies: information redundancy for error detection, temporal redundancy for soft error correction and hardware redundancy for hard error correction. Moreover, it uses a pseudo-dynamic comparator for SET and timing errors detection. Besides, the proposed method also aims to reduce power consumption of fault-tolerant architectures while keeping a comparable area overhead compared to existing solutions. Results on the largest ISCAS\u201985 and ITC\u201999 benchmark circuits show that our approach has an area cost of about 3\u00a0\u2026", "num_citations": "7\n", "authors": ["507"]}
{"title": "Bit-Flipping Scan\u2014A unified architecture for fault tolerance and offline test\n", "abstract": " Test is an essential task since the early days of digital circuits. Every produced chip undergoes at least a production test supported by on-chip test infrastructure to reduce test cost. Throughout the technology evolution fault tolerance gained importance and is now necessary in many applications to mitigate soft errors threatening consistent operation. While a variety of effective solutions exists to tackle both areas, test and fault tolerance are often implemented orthogonally, and hence do not exploit the potential synergies of a combined solution. The unified architecture presented here facilitates fault tolerance and test by combining a checksum of the sequential state with the ability to flip arbitrary bits. Experimental results confirm a reduced area overhead compared to a orthogonal combination of classical test and fault tolerance schemes. In combination with heuristically generated test sequences the test application\u00a0\u2026", "num_citations": "7\n", "authors": ["507"]}
{"title": "Efficient BDD-based fault simulation in presence of unknown values\n", "abstract": " Unknown (X) values, originating from memories, clock domain boundaries or A/D interfaces, may compromise test signatures and fault coverage. Classical logic and fault simulation algorithms are pessimistic w.r.t. the propagation of X values in the circuit. This work proposes efficient hybrid logic and stuck-at fault simulation algorithms which combine heuristics and local BDDs to increase simulation accuracy. Experimental results on benchmark and large industrial circuits show significantly increased fault coverage and low runtime. The achieved simulation precision is quantified for the first time.", "num_citations": "7\n", "authors": ["507"]}
{"title": "P-PET: Partial pseudo-exhaustive test for high defect coverage\n", "abstract": " Pattern generation for embedded testing often consists of a phase generating random patterns and a second phase where deterministic patterns are applied. This paper presents a method which optimizes the first phase significantly and increases the defect coverage, while reducing the number of deterministic patterns required in the second phase. The method is based on the concept of pseudo-exhaustive testing (PET), which was proposed as a method for fault model independent testing with high defect coverage. As its test length can grow exponentially with the circuit size, an application to larger circuits is usually impractical. In this paper, partial pseudo-exhaustive testing (P-PET) is presented as a synthesis technique for multiple polynomial feedback shift registers. It scales with actual technology and is comparable with the usual pseudo-random (PR) pattern testing regarding test costs and test application time\u00a0\u2026", "num_citations": "7\n", "authors": ["507"]}
{"title": "Test encoding for extreme response compaction\n", "abstract": " Optimizing bandwidth by compression and compaction always has to solve the trade-off between input bandwidth reduction and output bandwidth reduction. Recently it has been shown that splitting scan chains into shorter segments and compacting the shift data outputs into a singleparity bit reduces the test response data to one bit per cycle without affecting fault coverage and diagnostic resolution if the compactor's structure is included into the ATPG process.This test data reduction at the output side comes with challenges at the input side. The bandwidth requirement grows due to the increased number of chains and due to a drastically decreased amount of don't care values in the test patterns. The paper at hand presents a new iterative approach to test set encoding which optimizes bandwidth on both input and output side while keeping the diagnostic resolution and fault coverage. Experiments with industrial\u00a0\u2026", "num_citations": "7\n", "authors": ["507"]}
{"title": "Test exploration and validation using transaction level models\n", "abstract": " The complexity of the test infrastructure and test strategies in systems-on-chip approaches the complexity of the functional design space. This paper presents test design space exploration and validation of test strategies and schedules using transaction level models (TLMs). Since many aspects of testing involve the transfer of a significant amount of test stimuli and responses, the communication-centric view of TLMs suits this purpose exceptionally well.", "num_citations": "7\n", "authors": ["507"]}
{"title": "Using a hierarchical DfT methodology in high frequency processor designs for improved delay fault testability\n", "abstract": " In this paper a novel hierarchical DfT methodology is presented which is targeted to improve the delay fault testability for external testing and scan based BIST. After the partitioning of the design into high frequency macros, the analysis for delay fault testability already starts in parallel with the implementation at the macro level. A specification is generated for each macro that defines the delay fault testing characteristics at the macro boundaries. This specification is used to analyse and improve the delay fault testability by improving the scan chain ordering at macro-level before the macros are connected together into the total chip network. The hierarchical methodology has been evaluated with the instruction window buffer core of an out-of-order processor. It was shown that for this design practically no extra hardware is required.", "num_citations": "7\n", "authors": ["507"]}
{"title": "Error masking in self-testable circuits\n", "abstract": " The effects of error masking in a number of signature registers are analyzed. It is shown that a self-test can always be scheduled such that evaluating signatures only at the end of the complete test execution is sufficient. A method for computing the probability of a fault leading to at least one faulty signature in a set of self-test registers is presented. This method allows the computation of the fault coverage with respect to the complete test execution. A minimal subset of all self-test registers can be selected so that only the signatures of these self-test registers have to be evaluated and the fault coverage is almost not affected. The benefits of this approach are a smaller number of self-test registers in the scan path, a smaller number of signatures to be evaluated, a simplified test control unit, and hence a significant reduction in tie hardware required for built-in self-test structures. The proposed method is illustrated by an\u00a0\u2026", "num_citations": "7\n", "authors": ["507"]}
{"title": "Extending Aging Monitors for Early Life and Wear-Out Failure Prevention\n", "abstract": " Aging monitors can indicate the wear-out phase of a semi-conductor device before it will actually fail, and allow the use of integrated circuits in applications with high safety and reliability demands. In the early phase of the lifecycle of integrated systems, small delay faults may indicate reliability problems and early life failures, even if they are smaller than the slack of any path and neither alter the functional behavior of a system nor violate any aging guardband. One option to detect this type of hidden delay faults (HDFs) is the application of a faster-than-at-speed-test (FAST). This paper shows that aging monitors can be extended at low cost to achieve high HDF test coverage with a reduction in test time during FAST. The result is a unified strategy to improve the reliability in both early and late phases of the system lifecycle.", "num_citations": "6\n", "authors": ["507"]}
{"title": "Built-in test for hidden delay faults\n", "abstract": " Marginal hardware introduces severe reliability threats throughout the life cycle of a system. Although marginalities may not affect the functionality of a circuit immediately after manufacturing, they can degrade into hard failures and must be screened out during manufacturing test to prevent early life failures. Furthermore, their evolution in the field must be proactively monitored by periodic tests before actual failures occur. In recent years, small delay faults (SDFs) have gained increasing attention as possible indicators of marginal hardware. However, SDFs on short paths may be undetectable even with advanced timing aware ATPG. Faster-than-at-speed test (FAST) can detect such hidden delay faults (HDFs), but so far FAST has mainly been restricted to manufacturing test. This paper presents a fully autonomous built-in self-test approach for FAST, which supports in-field testing by appropriate strategies for test\u00a0\u2026", "num_citations": "6\n", "authors": ["507"]}
{"title": "Device aging: A reliability and security concern\n", "abstract": " Device aging is an important concern in nanoscale designs. Due to aging the electrical behavior of transistors embedded in an integrated circuit deviates from original intended one. This leads to performance degradation in the underlying device, and the ultimate device failure. This effect is exacerbated in emerging technologies. To be able to tailor effective aging mitigation schemes and improve the reliability of devices realized in cutting edge technologies, there is a need to accurately study the effect of aging in high performance industrial applications. According, this paper targets a high performance SRAM memory realized in 14nm FinFET technology and depicts how aging degrades the individual components of this memory as well as the interaction between them. Aging mitigation is critical not only from device reliability point of view but also regarding device security perspectives. It is essential to assure the\u00a0\u2026", "num_citations": "6\n", "authors": ["507"]}
{"title": "Mixed 01X-RSL-Encoding for fast and accurate ATPG with unknowns\n", "abstract": " Unknown (X) values in a design introduce pessimism in conventional test generation algorithms, which results in a loss of fault coverage. This pessimism is reduced by a more accurate modeling and analysis. Unfortunately, accurate analysis techniques highly increase runtime and limit scalability. One promising technique to prevent high runtimes while still providing high accuracy is the use of restricted symbolic logic (RSL). However, also pure RSL-based algorithms reach their limits as soon as millon gate circuits need to be processed. In this paper, we propose new ATPG techniques to overcome such limitations. An efficient hybrid encoding combines the accuracy of RSL-based modeling with the compactness of conventional threevalued encoding. A low-cost two-valued SAT-based untestability check is able to classify most untestable faults with low runtime. An incremental and event-based accurate fault\u00a0\u2026", "num_citations": "6\n", "authors": ["507"]}
{"title": "Adaptive multi-layer techniques for increased system dependability\n", "abstract": " Achieving system-level dependability is a demanding task.  The manifold requirements and dependability threats can no longer be statically addressed at individual abstraction layers.  Instead, all components of future multi-processor systems-on-chip (MPSoCs) have to contribute to this common goal in an adaptive manner.In this paper we target a generic heterogeneous MPSoC that combines general purpose processors along with dedicated application-specific hard-wired accelerators, fine-grained reconfigurable processors, and coarse-grained reconfigurable architectures.  We present different reactive and proactive measures at the layers of the runtime system (online resource management), system architecture (global communication), micro architecture (individual tiles), and gate netlist (tile-internal circuits) to address dependability threats.", "num_citations": "6\n", "authors": ["507"]}
{"title": "Area-efficient synthesis of fault-secure NoC switches\n", "abstract": " This paper introduces a hybrid method to synthesize area-efficient fault-secure NoC switches to detect all errors resulting from any single-point combinational or transition fault in switches and interconnect links. Firstly, the structural faults that are always detectable by data encoding at flit-level are identified. Next, the fault-secure structure is constructed with minimized area such that errors caused by the remaining faults are detected under any given input vector. The experimental evaluation shows significant area savings compared to conventional fault-secure schemes. In addition, the resulting structure can be reused for test compaction. This reduces the amount of test response data and test time without loss of fault coverage or diagnostic resolution.", "num_citations": "6\n", "authors": ["507"]}
{"title": "Efficient system-level aging prediction\n", "abstract": " Non-functional properties (NFPs) of integrated circuits include reliability, vulnerability, power consumption or heat dissipation. Accurate NFP prediction over long periods of system operation poses a great challenge due to prohibitive simulation costs. For instance, in case of aging estimation, the existing low-level models are accurate but not efficient enough for simulation of complex designs. On the other hand, existing techniques for fast high-level simulation do not provide enough details for NFP analysis. The goal of this paper is to bridge this gap by combining the accuracy of low-level models with high-level simulation speed. We introduce an efficient mixed-level NFP prediction methodology that considers both the structure and application of a system. The system is modeled at transaction-level to enable high simulation speed. To maintain accuracy, NFP assessment for cores under analysis is conducted at gate\u00a0\u2026", "num_citations": "6\n", "authors": ["507"]}
{"title": "SAT-based fault coverage evaluation in the presence of unknown values\n", "abstract": " Fault simulation of digital circuits must correctly compute fault coverage to assess test and product quality. In case of unknown values (X-values), fault simulation is pessimistic and underestimates actual fault coverage, resulting in increased test time and data volume, as well as higher overhead for design-for-test. This work proposes a novel algorithm to determine fault coverage with significantly increased accuracy, offering increased fault coverage at no cost, or the reduction of test costs for the targeted coverage. The algorithm is compared to related work and evaluated on benchmark and industrial circuits.", "num_citations": "6\n", "authors": ["507"]}
{"title": "Low-power test planning for arbitrary at-speed delay-test clock schemes\n", "abstract": " High delay-fault coverage requires rather sophisticated clocking schemes in test mode, which usually combine launch-on-shift and launch-on-capture strategies. These complex clocking schemes make low power test planning more difficult as initialization, justification and propagation require multiple clock cycles. This paper describes a unified method to map the sequential test planning problem to a combinational circuit representation. The combinational representation is subject to known algorithms for efficient low power built-in self-test planning. Experimental results for a set of industrial circuits show that even rather complex test clocking schemes lead to an efficient low power test plan.", "num_citations": "6\n", "authors": ["507"]}
{"title": "Implementing a scheme for external deterministic self-test\n", "abstract": " A method for test resource partitioning is introduced which keeps the design-for-test logic test set independent and moves the test pattern dependent information to an external, programmable chip. The scheme includes a new decompression scheme for a fast and efficient communication between the external test chip and the circuit under test. The hardware costs on chip are significantly lower compared with a deterministic BIST scheme while the test application time is still in the same range. The proposed scheme is fully programmable, flexible and can be reused at board level for testing in the field.", "num_citations": "6\n", "authors": ["507"]}
{"title": "Security compliance analysis of reconfigurable scan networks\n", "abstract": " Hardware security adds another dimension to the design space, and more and more attention is paid to protect a circuit against various types of attacks like sniffing, spoofing or IP theft. However, all the efforts for security taken by a designer might be sacrificed by afterwards integrating infrastructure for test, diagnosis and reliability management. Especially, access mechanisms like reconfigurable scan networks (RSNs) may open options for side-channel attacks. Using the presented approach an accurate estimation of reachability properties of all considered benchmarks is provided. The method uses a matrix-based reachability analysis of the original design and the augmented design. The reachability analysis covers complex functional dependencies, caused by configuring a single scan path as well as multiple sequentially activated scan paths through the RSN. This approach adds acceptable runtime to the\u00a0\u2026", "num_citations": "5\n", "authors": ["507"]}
{"title": "Analysis and Mitigation of IR-Drop Induced Scan Shift-Errors\n", "abstract": " Excessive IR-drop during scan shift can cause localized IR-drop around clock buffers and introduce dynamic clock skew. Excessive clock skew at neighboring scan flip-flops results in hold or setup timing violations corrupting test stimuli or test responses during shifting. We introduce a new method to assess the risk of such test data corruption at each scan cycle and flip-flop. The most likely cases of test data corruption are mitigated in a non-intrusive way by selective test data manipulation and masking of affected responses. Evaluation results show the computational feasibility of our method for large benchmark circuits, and demonstrate that a few targeted pattern changes provide large potential gains in shift safety and test time with negligible cost in fault coverage.", "num_citations": "5\n", "authors": ["507"]}
{"title": "High-throughput transistor-level fault simulation on GPUs\n", "abstract": " Deviations in the first-order parameters of CMOS cells can lead to severe errors in the functional and time domain. With increasing sensitivity of these parameters to manufacturing defects and variation, parametric and parasitic-aware fault simulation is becoming crucial in order to support test pattern generation. Traditional approaches based on gate-level models are not sufficient to represent and capture the impact of deviations in these parameters in either an efficient or accurate manner. Evaluation at electrical level, on the other hand, severely lacks execution speed and quickly becomes inapplicable to larger designs due to high computational demands.This work presents a novel fault simulation approach considering first-order parameters in CMOS circuits to explicitly capture CMOS-specific behavior in the functional and time domain with transistor granularity. The approach utilizes massive parallelization in\u00a0\u2026", "num_citations": "5\n", "authors": ["507"]}
{"title": "Applying efficient fault tolerance to enable the preconditioned conjugate gradient solver on approximate computing hardware\n", "abstract": " A new technique is presented that allows to execute the preconditioned conjugate gradient (PCG) solver on approximate hardware while ensuring correct solver results. This technique expands the scope of approximate computing to scientific and engineering applications. The changing error resilience of PCG during the solving process is exploited by different levels of approximation which trade off numerical precision and hardware utilization. Such approximation levels are determined at runtime by periodically estimating the error resilience. An efficient fault tolerance technique allows reductions in hardware utilization by ensuring the continued exploitation of suitable energy-precision trade-offs. Experimental results show that the hardware utilization is reduced on average by 14.5% and by up to 41.0% compared to executing PCG on precise hardware.", "num_citations": "5\n", "authors": ["507"]}
{"title": "Multi-layer test and diagnosis for dependable nocs\n", "abstract": " Networks-on-chip are inherently fault tolerant or at least gracefully degradable as both, connectivity and amount of resources, provide some useful redundancy. These properties can only be exploited extensively if test and diagnosis techniques support fault detection and error containment in an optimized way. On the one hand, all faulty components have to be isolated, and on the other hand, remaining fault-free functionalities have to be kept operational.", "num_citations": "5\n", "authors": ["507"]}
{"title": "On covering structural defects in NoCs by functional tests\n", "abstract": " Structural tests provide high defect coverage by considering the low-level circuit details. Functional test provides a faster test with reduced test patterns and does not imply additional hardware overhead. However, it lacks a quantitative measure of structural fault coverage. This paper fills this gap by presenting a satisfiability based method to generate functional test patterns while considering structural faults. The method targets NoC switches and links, and it is independent of the switch structure and the network topology. It can be applied for any structural fault type as it relies on a generalized structural fault model.", "num_citations": "5\n", "authors": ["507"]}
{"title": "Incremental computation of delay fault detection probability for variation-aware test generation\n", "abstract": " Large process variations in recent technology nodes present a major challenge for the timing analysis of digital integrated circuits. The optimization decisions of a statistical delay test generation method must therefore rely on the probability of detecting a target delay fault with the currently chosen test vector pairs. However, the huge number of probability evaluations in practical applications creates a large computational overhead. To address this issue, this paper presents the first incremental delay fault detection probability computation algorithm in the literature, which is suitable for the inner loop of automatic test pattern generation methods. Compared to Monte Carlo simulations of NXP benchmark circuits, the new method consistently shows a very large speedup and only a small approximation error.", "num_citations": "5\n", "authors": ["507"]}
{"title": "DFG-Projekt RealTest\u2013Test und Zuverl\u00e4ssigkeit nanoelektronischer Systeme (DFG-Project\u2013Test and Reliability of Nano-Electronic Systems)\n", "abstract": " The increasing number of fabrication defects, spatial and temporal variability of parameters, as well as the growing impact of soft errors in nanoelectronic systems require a paradigm shift in design, verification and test. A robust design is mandatory to ensure dependable systems and acceptable yields. The quest for design robustness, however, invalidates many traditional approaches for testing and implies enormous challenges. Within the framework of the RealTest project unified design and test strategies are developed to support a robust design and a coordinated quality assurance after the production and during the lifetime of a system.", "num_citations": "5\n", "authors": ["507"]}
{"title": "Some common aspects of design validation, debug and diagnosis\n", "abstract": " Design, verification and test of integrated circuits with millions of gates put strong requirements on design time, test volume, test application time, test speed and diagnostic resolution. In this paper, an overview is given on the common aspects of these tasks and how they interact. Diagnosis techniques may be used after manufacturing, for chip characterization and field return analysis, and even for rapid prototyping", "num_citations": "5\n", "authors": ["507"]}
{"title": "Reliability considerations for mechatronic systems on the basis of a state model\n", "abstract": " The first step in analyzing a problem is to establish a valid model that would represent this problem. The model helps mainly in understanding the problem by depicting it in a visual form. Hence, in order to analyze the reliability of mechatronic systems, we need to understand first how such systems fail and how they behave in the presence of a failure. This understanding would help us later in the analysis and the development of formal solutions to achieve the demanded reliability. This could be achieved using the model that we have developed, which will be presented in this paper.", "num_citations": "5\n", "authors": ["507"]}
{"title": "Fast self-recovering controllers\n", "abstract": " A fast fault-tolerant controller structure is presented which is capable of recovering from transient faults by performing a rollback operation in hardware. The proposed fault-tolerant controller structure utilizes the rollback hardware also for system mode and this way achieves performance improvements of more than 50% compared to controller structures made fault tolerant by conventional techniques, while the hardware overhead is often negligible. The proposed approach is compatible with state-of-the-art methods for FSM decomposition, state encoding and logic synthesis.", "num_citations": "5\n", "authors": ["507"]}
{"title": "The random pattern testability of programmable logic arrays\n", "abstract": " An efficient Monte Carlo Algorithm is presented estimating the detection probability of each stuck at fault of a PLA. Furthermore for each primary input of the PLA the optimal probability is computed to set this input to logical \"1\". Using those unequiprobable input probabilities the necessary test set can be reduced by orders of magnitude. Thus a seIftest by optimized random patterns is possible even if the circuit contains large PLAs preventing a conventional random test.", "num_citations": "5\n", "authors": ["507"]}
{"title": "Variation-aware small delay fault diagnosis on compressed test responses\n", "abstract": " With today's tight timing margins, increasing manufacturing variations, and new defect behaviors in FinFETs, effective yield learning requires detailed information on the population of small delay defects in fabricated chips. Small delay fault diagnosis for yield learning faces two main challenges: (1) production test responses are usually highly compressed reducing the amount of available failure data, and (2) failure signatures not only depend on the actual defect but also on omnipresent and unknown delay variations. This work presents the very first diagnosis algorithm specifically designed to diagnose timing issues on compressed test responses and under process variations. An innovative combination of variation-invariant structural analysis, GPU-accelerated time-simulation, and variation-tolerant syndrome matching for compressed test responses allows the proposed algorithm to cope with both challenges\u00a0\u2026", "num_citations": "4\n", "authors": ["507"]}
{"title": "Multi-level timing simulation on GPUs\n", "abstract": " Timing-accurate simulation of circuits is an important task in design validation of modern nano-scale CMOS circuits. With shrinking technology nodes, detailed simulation models down to transistor level have to be considered. While conventional simulation at logic level lacks the ability to accurately model timing behavior for complex cells, more accurate simulation at lower abstraction levels becomes computationally expensive for larger designs. This work presents the first parallel multi-level waveform-accurate timing simulation approach on graphics processing units (GPUs). The simulation uses logic and switch level abstraction concurrently, thus allowing to combine their advantages by trading off speed and accuracy. The abstraction can be lowered in arbitrary regions of interest to locally increase the accuracy. Waveform transformations allow for transparent switching between the abstraction levels. With the\u00a0\u2026", "num_citations": "4\n", "authors": ["507"]}
{"title": "Timing-accurate estimation of IR-drop impact on logic-and clock-paths during at-speed scan test\n", "abstract": " IR-drop induced false capture failures and test clock stretch are severe problems in at-speed scan testing. We propose a new method to efficiently and accurately identify these problems. For the first time, our approach considers the additional dynamic power caused by glitches, the spatial and temporal distribution of all toggles, and their impact on both logic paths and the clock tree without time-consuming electrical simulations.", "num_citations": "4\n", "authors": ["507"]}
{"title": "SAT-based code synthesis for fault-secure circuits\n", "abstract": " This paper presents a novel method for synthesizing fault-secure circuits based on parity codes over groups of circuit outputs. The fault-secure circuit is able to detect all errors resulting from combinational and transition faults at a single node. The original circuit is not modified. If the original circuit is non-redundant, the result is a totally self-checking circuit. At first, the method creates the minimum number of parity groups such that the effect of each fault is not masked in at least one parity group. To ensure fault-secureness, the obtained groups are split such that no fault leads to silent data corruption. This is performed by a formal Boolean satisfiability (SAT) based analysis. Since the proposed method reduces the number of required parity groups, the number of two-rail checkers and the complexity of the prediction logic required for fault-secureness decreases as well. Experimental results show that the area overhead\u00a0\u2026", "num_citations": "4\n", "authors": ["507"]}
{"title": "Structural test and diagnosis for graceful degradation of NoC switches\n", "abstract": " Networks-on-Chip (NoCs) are implicitly fault tolerant and due to their inherent redundancy they can overcome defective cores, links and switches. This effect can be used to increase yield at the cost of reduced performance. In this paper, a new diagnosis method based on the standard flow of industrial volume testing is presented, which is able to identify the intact functions of a defective network switch rather than providing only a pass/fail result for the complete switch. To achieve this, the new method combines for the first time the precision of structural testing with information on the functional behavior in the presence of defects. This allows to disable defective parts of a switch after production test and use the intact functions. Thereby, only a minimum performance decrease is induced while the yield is increased. According to the experimental results, the method improves the performability of NoCs since 56\u00a0\u2026", "num_citations": "4\n", "authors": ["507"]}
{"title": "Parallel simulation of apoptotic receptor-clustering on GPGPU many-core architectures\n", "abstract": " Apoptosis, the programmed cell death, is a physiological process that handles the removal of unwanted or damaged cells in living organisms. The process itself is initiated by signaling through tumor necrosis factor (TNF) receptors and ligands, which form clusters on the cell membrane. The exact function of this process is not yet fully understood and currently subject of basic research. Different mathematical models have been developed to describe and simulate the apoptotic receptor-clustering. In this interdisciplinary work, a previously introduced model of the apoptotic receptor-clustering has been extended by a new receptor type to allow a more precise description and simulation of the signaling process. Due to the high computational requirements of the model, an efficient algorithmic mapping to a modern many-core GPGPU architecture has been developed. Such architectures enable high-performance\u00a0\u2026", "num_citations": "4\n", "authors": ["507"]}
{"title": "Diagnostic test of robust circuits\n", "abstract": " Robust circuits are able to tolerate certain faults, but also pose additional challenges for test and diagnosis. To improve yield, the test must distinguish between critical faults and such faults, that could be compensated during system operation, in addition, efficient diagnosis procedures are needed to support yield ramp-up in the case of critical faults. Previous work on circuits with time redundancy has shown that \"signature rollback\" can distinguish critical permanent faults from uncritical transient faults. The test is partitioned into shorter sessions, and a rollback is triggered immediately after a faulty session. If the repeated session shows the correct result, then a transient fault is assumed. The reference values for the sessions are represented in a very compact format. Storing only a few bits characterizing the MISR state over time can provide the same quality as storing the complete signature. In this work the signature\u00a0\u2026", "num_citations": "4\n", "authors": ["507"]}
{"title": "SAT-based capture-power reduction for at-speed broadcast-scan-based test compression architectures\n", "abstract": " Excessive power dissipation during VLSI testing results in over-testing, yield loss and heat damage of the device. For low power devices with advanced power management features and more stringent power budgets, power-aware testing is even more mandatory. Effective and efficient test set postprocessing techniques based on X-identification and power-aware X-filling have been proposed for external and embedded deterministic test. This work proposes a novel X-filling algorithm for combinational and broadcast-scan-based test compression schemes which have great practical significance. The algorithm ensures compressibility of test cubes using a SAT-based check. Compared to methods based on topological justification, the solution space of the compressed test vector is not pruned early during the search. Thus, this method allows much more precise low-power X-filling of test vectors. Experiments on\u00a0\u2026", "num_citations": "4\n", "authors": ["507"]}
{"title": "Algorithmen-basierte Fehlertoleranz f\u00fcr Many-Core-Architekturen\n", "abstract": " Modern many-core architectures provide a high computational potential, which makes them particularly interesting for applications from the fields of scientific high-performance computing and simulation technology. The execution paradigm of these architectures is best described as \u201cMany-Threading\u201d. Like all nano-scaled semiconductor devices, many-core processors are prone to transient errors (soft errors) and different kinds of variations that can have severe impact on the reliability of such systems. Therefore, fault-tolerance has to be incorporated at all levels, from the hardware up to the software. On the software side, Algorithm-based Fault Tolerance (ABFT) is a mature technique to improve the reliability. However, significant effort is required to adapt this technique to modern many-threading architectures. In this article, an efficient and fault-tolerant mapping of the matrix multiplication to a modern many-core\u00a0\u2026", "num_citations": "4\n", "authors": ["507"]}
{"title": "Test engineering education in Europe: the EuNICE-Test project\n", "abstract": " The paper deals with a European experience of education in industrial test of ICs and SoCs using remote testing facilities. The project addresses the problem of the shortage in microelectronics engineers aware with the new challenge of testing mixed-signal SoCs far multimedia/telecom market. It aims at providing test training facilities at a European scale in both initial and continuing education contexts. This is done by allowing the academic and industrial partners of the consortium to train engineers using the common test resources center (CRTC) hosted by LIRMM (Laboratoire d'Informatique, de Robotique et de Microelectronique de Montpellier, France). CRTC test tools include up-to-date/high-tech testers that are fully representative of real industrial testers as used on production testfloors. At the end of the project, it is aimed at reaching a cruising speed of about 16 trainees per year per center. Each trainee will\u00a0\u2026", "num_citations": "4\n", "authors": ["507"]}
{"title": "The integration of test and high level synthesis in a general design environment\n", "abstract": " This paper describes the integration of new tools for both test and synthesis of integrated circuits. The presented design system CADDY (Carlsruhe Digital Design System) automatically transforms a functional description into a circuit structure. Besides this logic synthesis the system also automatically integrates a complete or incomplete scan path. The software tool PROTEST (PRObabilistic TESTability analysis tool) determines the random testability of the combinational parts of synthesized circuits and suggests optimized input signal probabilities to minimize the necessary test length. To generate these test patterns on chip a specific test hardware is proposed.", "num_citations": "4\n", "authors": ["507"]}
{"title": "Design automation of random testable circuits\n", "abstract": " This paper describes the integration of a new tool for testability measurement and improvement into a design system for integrated circuits. The involved design system, CADDY (Carlsruhe Digital Design System), uses a functional description of a circuit written in a PASCAL like language and synthesizes a list of nets and real logical components. In this resulting structure all storing elements are configured as a scan path automatically. Therefore testability analysis and test generation may be restricted to pure combinational networks. This is done by the software tool PROTEST (Probabilistic Testability Analysis). PROTEST determines the testability of a combinational circuit by random patterns, it computes the test length necessary to reach a given fault coverage with an also given confidence, and it proposes modifications of the random pattern sets, which leads to decreasing test lengths.", "num_citations": "4\n", "authors": ["507"]}
{"title": "SWIFT: Switch Level Fault Simulation on GPUs\n", "abstract": " Current nanometer CMOS circuits show an increasing sensitivity to deviations in first-order parameters and suffer from process variations during manufacturing. To properly assess and support test validation of digital designs, low-level fault simulation approaches are utilized to accurately capture the behavior of CMOS cells under parametric faults and process variations as early as possible throughout the design phase. However, low-level simulation approaches exhibit a high computational complexity, especially when variation has to be taken into account. In this paper, a high-throughput parallel fault simulation at switch level is presented. First-order electrical parameters are utilized to capture CMOS-specific functional and timing behavior of complex cells allowing to model faults with transistor granularity and without the need of logic abstraction. Furthermore, variation modeling in cells and transistor devices\u00a0\u2026", "num_citations": "3\n", "authors": ["507"]}
{"title": "Intermittent and transient fault diagnosis on sparse code signatures\n", "abstract": " Failure diagnosis of field returns typically requires high quality test stimuli and assumes that tests can be repeated. For intermittent faults with fault activation conditions depending on the physical environment, the repetition of tests cannot ensure that the behavior in the field is also observed during diagnosis, causing field returns diagnosed as no-trouble-found. In safety critical applications, self-checking circuits, which provide concurrent error detection, are frequently used. To diagnose intermittent and transient faulty behavior in such circuits, we use the stored encoded circuit outputs in case of a failure (called signatures) for later analysis in diagnosis. For the first time, a diagnosis algorithm is presented that is capable of performing the classification of intermittent or transient faults using only the very limited amount of functional stimuli and signatures observed during operation and stored on chip. The experimental\u00a0\u2026", "num_citations": "3\n", "authors": ["507"]}
{"title": "Adaptive parallel simulation of a two-timescale model for apoptotic receptor-clustering on GPUs\n", "abstract": " Computational biology contributes important solutions for major biological challenges. Unfortunately, most applications in computational biology are highly compute-intensive and associated with extensive computing times. Biological problems of interest are often not treatable with traditional simulation models on conventional multi-core CPU systems. This interdisciplinary work introduces a new multi-timescale simulation model for apoptotic receptor-clustering and a new parallel evaluation algorithm that exploits the computational performance of heterogeneous CPU-GPU computing systems. For this purpose, the different dynamics involved in receptor-clustering are separated and simulated on two timescales. Additionally, the time step sizes are adaptively refined on each timescale independently. This new approach improves the simulation performance significantly and reduces computing times from months to\u00a0\u2026", "num_citations": "3\n", "authors": ["507"]}
{"title": "Fail-safety in core-based system design\n", "abstract": " As scaling of nanoelectronics may deteriorate dependability, fail-safe design techniques gain attention. We apply the concept of fail-safety to IP core-based system design, making the first step towards dependability-aware reuse methodologies. We introduce a methodology for dependability characterization, which uses informal techniques to identify hazards and employs formal methods to check if the hazards occur. The proposed hazard metrics provide qualitative and quantitative insight into possible core misbehavior. Experimental results on two IP cores show that the approach enables early comparative dependability studies.", "num_citations": "3\n", "authors": ["507"]}
{"title": "Parity prediction synthesis for nano-electronic gate designs\n", "abstract": " In this paper we investigate the possibility of using commercial synthesis tools to build parity predictors for nano-electronic gates designs. They will be used as redundant resources for robustness improvement for future CMOS technology nodes.", "num_citations": "3\n", "authors": ["507"]}
{"title": "Direct UV-nanoimprint of polymer microring resonators as optical transducers\n", "abstract": " This research presents current results of an UV-assisted nanoimprint lithography (UV-NIL) technology which is developed for direct structuring of polymer waveguides and microring resonators (MRRs). MRRs are key elements and transducers for optical sensor systems. Chemically functionalized imprint materials enable directly imprinted, functional polymer structures. A route towards residual layer free direct imprinting of large aspect ratios with a combined nanoimprint and photolithography (CNP) technique is discussed. CNP molds can be easily produced by leaving the antireflective chromium (arCr) hard mask after mold etching on top of the mold surface. The achieved surface free energy (SFE) of antisticking layers (ASLs) on the arCr was higher than for quartz, native SiO2 and thermal SiO2. However, the achieved ASL on the arCr was sufficient for imprinting. UV-NIL is suited to directly imprint and successfully\u00a0\u2026", "num_citations": "3\n", "authors": ["507"]}
{"title": "Zur Zuverl\u00e4ssigkeitsmodellierung von Hardware-Software-Systemen\n", "abstract": " Zur Zuverl\u00e4ssigkeitsmodellierung von Hardware- Software-Systemen - Tagungsbeitr\u00e4ge - VDE VERLAG Top Kontaktinformationen Newsletter Login / Registrieren Warenkorb (0) English Area VDE VERLAG Technik. Wissen. Weiterwissen. Suchen Alle Kategorien Alle Kategorien B\u00fccher VDE-Normen IEC-Normen Seminare VDE VERLAG Technik. Wissen. Weiterwissen. NORMEN VDE-Normen und Entw\u00fcrfe Produkt-\u00dcbersicht VDE-Vorschriftenwerk Auswahlen und Gruppen Entw\u00fcrfe Anwendungsregeln Englische \u00dcbersetzungen Apps zu Normen Informationen zum E-Handwerk Informationen zur Ausbildung Bezugsm\u00f6glichkeiten: Abonnement NormenBibliothek VDE-Normen: Suchen + Bestellen VDE-Neuerscheinungen Weitere Services: Preis-Infos FAQ IEC-Publikationen Produkt-\u00dcbersicht IEC-Normen: Suchen + Bestellen IEC-Datenbanken IEC-Online-Collections Erwerb von Mehrplatzlizenzen IEC-\u2026", "num_citations": "3\n", "authors": ["507"]}
{"title": "Debug and diagnosis: Mastering the life cycle of nano-scale systems on chip\n", "abstract": " Rising design complexity and shrinking structures pose new challenges for debug and diagnosis. Finding bugs and defects quickly during the whole life cycle of a product lS crucial for time to market, time to volume and improved product quality. Debug of design errors and diagnosis of defects have many common aspects. in this paper we give an overview of state of the art algorithms, which tackle both tasks, and present an adaptive approach to design debug and logic diagnosis.", "num_citations": "3\n", "authors": ["507"]}
{"title": "Synthesis of irregular combinational functions with large don't care sets\n", "abstract": " A special logic synthesis problem is considered for Booleanfunctions which have large don't care sets and are irregular. Here, a function is considered as irregular if the input assignmentsmapped to specified values ('1'or'0') are randomly spread overthe definition space. Such functions can be encountered in the field of design for test. The proposed method uses ordered BDDs forlogic manipulations and generates free BDD-like covers. For the considered benchmark functions, implementations were found witha significant reduction of the node/gate count as compared to SISor to methods offered by a state-of-the-art BDD package.", "num_citations": "3\n", "authors": ["507"]}
{"title": "Sequence length, area cost and non-target defect coverage tradeoffs in deterministic logic BIST\n", "abstract": " For the first time, we study the coverage of non-target defects for Deterministic Logic BIST (DLBIST) architecture. We consider several DLBIST implementation options that result in test sequences of different lengths. Resistive bridging faults are used as a surrogate of non-target defects. Experimental data obtained for largest ISCAS benchmarks suggests that, although DLBIST always guarantees complete stuck-at coverage, test sequence length does influence the non-target defect detection capabilities. For circuits with a large fraction of random-pattern resistant faults, the embedded deterministic patterns as well as a sufficient amount of random patterns are both demonstrated to be essential for non-target defect detection. It turns out, moreover, that area cost is lower for DLBIST solutions with longer test sequences, due to additional degrees of freedom for the embedding procedure and a lower number of faults undetected by pseudorandom patterns. This implies that DLBIST is particularly effective in covering non-target defects.", "num_citations": "3\n", "authors": ["507"]}
{"title": "A synthesis approach to reduce scan design overhead\n", "abstract": " Enthalten in den Sammlungen: 15 Fakult\u00e4ts\u00fcbergreifend/Sonstige EinrichtungDateien zu dieser Ressource:DateiBeschreibungGr\u00f6\u00dfeFormatwun29. pdf 134, 74 kB Adobe PDF \u00d6ffnen/Anzeigen", "num_citations": "3\n", "authors": ["507"]}
{"title": "Integrated tools for automatic design for testability\n", "abstract": " An increasing part of the overall costs of custom and semicustom integrated circuits has to be spent for test purposes, and therefore the integration of test and design seems to be a key of cost reduction. At the University of Karlsruhe a program system is currently developed supporting the design of testable circuits. The program system under work essentially solves three tasks: 1.) Selection of an economical test strategy. 2.) Implementation of necessary circuit modifications in order to enhance testability, retaining the circuit function by construction. 3.) Generation of the test program.", "num_citations": "3\n", "authors": ["507"]}
{"title": "Automatisierung des Entwurfs vollst\u00e4ndig testbarer Schaltungen\n", "abstract": " Die Kosten f\u00fcr die Testvorbereitung, Testerzeugung und Testdurchf\u00fchrung wachsen \u00fcberproportional mit der Komplexit\u00e4t anwendungsspezifischer Schaltungen, und die Teststrategie sollte daher bereits in einer sehr fr\u00fchen Phase des Schaltungsentwurfs festgelegt und ber\u00fccksichtig werden. In diesem Artikel werden logische Grundzellen und Algorithmen zur Unterst\u00fctzung des pseudo-ersch\u00f6pfenden Tests vorgestellt. Diese Teststrategie hat den Vorteil, da\u00df die \u00e4u\u00dferst rechenzeitaufwendige Testmustererzeugung entf\u00e4llt und zugleich eine vollst\u00e4ndige Fehlererfassung auf Gatterebene garantiert ist. Die vorgestellten Grundzellen dienen der Zerlegung der Gesamtschaltung in ersch\u00f6pfend testbare Teile, die pr\u00e4sentierten Algorithmen sollen diese Segmentierungszellen so plazieren, da\u00df der Mehraufwand an Silizium gering bleibt. Hierzu wurden Varianten sogenannter \u201cHill-Climbing\u201d und \u201cSimulated\u00a0\u2026", "num_citations": "3\n", "authors": ["507"]}
{"title": "Security Preserving Integration and Resynthesis of Reconfigurable Scan Networks\n", "abstract": " Reliable operation, test, debug and diagnosis of complex integrated systems are ensured by embedded instruments, such as sensors, aging monitors or Built-In Self-Test (BIST) registers. Reconfigurable Scan Networks (RSNs) offer a flexible and efficient way to access such test instruments throughout the whole life-cycle. However, improper RSN integration might introduce additional connectivity properties to the device under test (DUT), which can be exploited to perform unauthorized access or cause information leakage. The existence of such additional connectivity through the RSN can compromise the security of the DUT and is considered as a security threat.In this paper, a method is presented to resolve all such security compliance violations. The problem is formulated in terms of Integer Linear Programming (ILP) as a minimum cut problem in multicommodity flow. An efficient heuristic is presented, which, to\u00a0\u2026", "num_citations": "2\n", "authors": ["507"]}
{"title": "Synthesis of fault-tolerant reconfigurable scan networks\n", "abstract": " On-chip instrumentation is mandatory for efficient bring-up, test and diagnosis, post-silicon validation, as well as in-field calibration, maintenance, and fault tolerance. Reconfigurable scan networks (RSNs) provide a scalable and efficient scan-based access mechanism to such instruments. The correct operation of this access mechanism is crucial for all manufacturing, bring-up and debug tasks as well as for in-field operation, but it can be affected by faults and design errors. This work develops for the first time fault-tolerant RSNs such that the resulting scan network still provides access to as many instruments as possible in presence of a fault. The work contributes a model and an algorithm to compute scan paths in faulty RSNs, a metric to quantify its fault tolerance and a synthesis algorithm that is based on graph connectivity and selective hardening of control logic in the scan network. Experimental results\u00a0\u2026", "num_citations": "2\n", "authors": ["507"]}
{"title": "Clock-skew-aware scan chain grouping for mitigating shift timing failures in low-power scan testing\n", "abstract": " High scan shift power often leads to excessive heat as well as shift timing failures. Partial shift (shifting a subset of scan chains at a time) is a widely adopted approach for avoiding excessive heat by reducing global switching activity, we show for the first time that it may actually cause excessive IR-drop on some clock buffers and worsen shift clock skews, thus increasing the risk of shift timing failures. This paper addresses this problem with an innovative method, namely Clock-Skew-Aware Scan Chain Grouping (CSA-SCG). CSA-SCG properly groups scan chains to be shifted simultaneously so as to reduce the imbalance of switching activity around the clock paths for neighboring scan flip-flops in scan chains. Experiments on large ITC'99 benchmark circuits demonstrate the effectiveness of CSA-SCG for reducing scan shift clock skews to lower the risk of shift timing failures in partial shift.", "num_citations": "2\n", "authors": ["507"]}
{"title": "Energy-efficient and error-resilient iterative solvers for approximate computing\n", "abstract": " Iterative solvers like the Preconditioned Conjugate Gradient (PCG) method are widely-used in compute-intensive domains including science and engineering that often impose tight accuracy demands on computational results. At the same time, the error resilience of such solvers may change in the course of the iterations, which requires careful adaption of the induced approximation errors to reduce the energy demand while avoiding unacceptable results. A novel adaptive method is presented that enables iterative Preconditioned Conjugate Gradient (PCG) solvers on Approximate Computing hardware with high energy efficiency while still providing correct results. The method controls the underlying precision at runtime using a highly efficient fault tolerance technique that monitors the induced error and the quality of intermediate computational results.", "num_citations": "2\n", "authors": ["507"]}
{"title": "Probabilistic sensitization analysis for variation-aware path delay fault test evaluation\n", "abstract": " With the ever increasing process variability in recent technology nodes, path delay fault testing of digital integrated circuits has become a major challenge. A randomly chosen long path often has no robust test and many of the existing non-robust tests are likely invalidated by process variations. To generate path delay fault tests that are more tolerant towards process variations, the delay test generation must evaluate different non-robust tests and only those tests that sensitize the target path with a sufficiently high probability in presence of process variations must be selected. This requires a huge number of probability computations for a large number of target paths and makes the development of very efficient approximation algorithms mandatory for any practical application. In this paper, a novel and efficient probabilistic sensitization analysis is presented which is used to extract a small subcircuit for a given test\u00a0\u2026", "num_citations": "2\n", "authors": ["507"]}
{"title": "Built-in self-diagnosis targeting arbitrary defects with partial pseudo-exhaustive test\n", "abstract": " Pseudo-exhaustive test completely verifies all output functions of a combinational circuit, which provides a high coverage of non-target faults and allows an efficient on-chip implementation. To avoid long test times caused by large output cones, partial pseudo-exhaustive test (P-PET) has been proposed recently. Here only cones with a limited number of inputs are tested exhaustively, and the remaining faults are targeted with deterministic patterns. Using P-PET patterns for built-in diagnosis, however, is challenging because of the large amount of associated response data. This paper presents a built-in diagnosis scheme which only relies on sparsely distributed data in the response sequence, but still preserves the benefits of P-PET.", "num_citations": "2\n", "authors": ["507"]}
{"title": "Digital, memory and mixed-signal test engineering education: five centres of competence in Europe\n", "abstract": " The launching of the EuNICE-Test project was announced two years ago at the first DELTA Conference. This project is now completed and the present paper describes the project actions and outcomes. The original idea was to build a long-lasting European Network for test engineering education using both test resource mutualisation and remote experiments. This objective is fully fulfilled and we have now, in Europe, five centres of competence able to deliver high-level and high-specialized training courses in the field of test engineering using a high-performing industrial ATE. All the centres propose training courses on digital testing, three of them propose mixed-signal trainings and three of them propose memory trainings. Taking into account the demand in test engineering, the network is planned to continue in a stand alone mode after project end. Nevertheless a new European proposal with several new\u00a0\u2026", "num_citations": "2\n", "authors": ["507"]}
{"title": "A mixed-mode BIST scheme based on folding compression\n", "abstract": " In this paper a new scheme for mixed mode scan-based BIST is presented with complete fault coverage, and some new concepts of folding set and computing are introduced. This scheme applies single feedback polynomial of LFSR for generating pseudo-random patterns, as well as for compressing and extending seeds of folding sets and an LFSR, where we encode seed of folding set as an initial seed of LFSR. Moreover these new techniques are 100% compatible with scan design. Experimental results show that the proposed scheme outperforms previously published approaches based on the reseeding of LFSRs.", "num_citations": "2\n", "authors": ["507"]}
{"title": "Using mission logic for embedded testing\n", "abstract": " Testing logic cores of a system-on-a-chip causes a high test data volume which has to be stored on the external automatic test equipment (ATE), a high bandwidth requirement between ATE and the chip under test implying the need for high-speed ATE. This paper reduces these requirements by reusing embedded cores during test mode as embedded testers, Hard, firm, and soft cores may be reused, since only the functionality of the core in system mode is used.", "num_citations": "2\n", "authors": ["507"]}
{"title": "A unified method for assembling global test schedules\n", "abstract": " In order to make a register transfer structure testable, it is usually divided into functional blocks that can be tested independently by various test methods. The test patterns are shifted in or generated autonomously at the inputs of each block. The test responses of a block are compacted or observed at its output register. In this paper a unified method for assembling all the single tests to a global schedule is presented. It is compatible with a variety of different test methods. The described scheduling procedures reduce the overall test time and minimize the number of internal registers that have to be made directly observable.< >", "num_citations": "2\n", "authors": ["507"]}
{"title": "Emulation of scan paths in sequential circuit synthesis\n", "abstract": " Scan paths are generally added to a sequential circuit in a final design for testability step. We present an approach to incorporate the behavior of a scan path during circuit synthesis, thus avoiding to implement the scan path shift register as a separate structural entity. The shift transitions of the scan path are treated as a part of the system functionality. Depending on the minimization strategy for the system logic, either the delay or the area of the circuit can be reduced compared to a conventional scan path, which may be interpreted as a special case of realizing the combinational logic. The approach is also extended to partial scan paths. It is shown that the resulting structure is fully testable and test patterns can be efficiently produced by a combinational test generator. The advantages of the approach are illustrated with a collection of finite state machine examples.", "num_citations": "2\n", "authors": ["507"]}
{"title": "A Hybrid Protection Scheme for Reconfigurable Scan Networks\n", "abstract": " The reliable operation of integrated systems is supported by Reconfigurable Scan Networks (RSNs) which allow to access efficiently the embedded instruments throughout the lifecycle. However, the RSN integration may introduce additional connectivities into a Device-under-Test (DUT), and the RSN might be misused for information leakage. Structural methods resynthesize the RSNs and add hardware components such that certain instruments are physically separated, while functional approaches add filters to prevent certain access patterns. Both methods have certain limitations.This paper presents an effective approach to maximize the benefits and to overcome the limitations of the existing solutions by a hybrid combination of structural and functional protection schemes. A minimized number of structural changes is identified in order to resolve violations which cannot be handled by using sequence filters. The\u00a0\u2026", "num_citations": "1\n", "authors": ["507"]}
{"title": "Online Test Strategies and Optimizations for Reliable Reconfigurable Architectures\n", "abstract": " Runtime/reconfigurable architectures based on Field-Programmable Gate Arrays (FPGAs) are a promising augment to conventional processor architectures such as Central Processing Units (CPUs) and Graphic Processing Units (GPUs). Since the reconfigurable parts are typically manufactured in the latest technology, they may suffer from aging and environmentally induced dependability threats. In this chapter, strategic online test methods for dependable runtime-reconfigurable architectures as well as cross-layer optimizations for high reliability and lifetime are developed. Firstly, two orthogonal online tests are proposed that ensure reliable configuration of the reconfigurable fabric and aid fault detection. Secondly, a novel design method called module diversification is presented that enables self-repair of the system in case of faults caused by degradation effects as well as single-event upsets in the configuration. Thirdly, a novel stress-aware placement method is proposed that aims for slowing down system degradation by aging effects. The combined methods ensure reliable operation across architectural and gate level and allow to prolong the lifetime of dependable runtime-reconfigurable architectures. The dependable operation of VLSI circuits is not only threatened by test escapes, intermittent or transient errors, but also by emerging hardware defects due to aging [11\u201313]. In nano-scale CMOS circuits, aging is related to stress which is defined as the condition under which a circuit structure experiences electrical and physical", "num_citations": "1\n", "authors": ["507"]}
{"title": "Logic Fault Diagnosis of Hidden Delay Defects\n", "abstract": " Hidden delay defects (HDDs) are small delay defects that pass all at-speed tests at nominal capture time. They are an important indicator of latent defects that lead to early-life failures and aging problems that are serious especially in autonomous and medical applications. An effective way to screen out HDDs is to use Faster-than-At-Speed Testing (FAST) to observe outputs of sensitized non-critical paths which are expected to be stable earlier than nominal capture time.To improve the reliability of current and future designs, it is important to learn about the population of HDDs using logic diagnosis. We present the very first logic fault diagnosis technique that is able to identify HDDs by analyzing fail logs produced by FAST.Even with aggressive FAST testing, HDDs generate only very few failing test response bits. To overcome this severe challenge, we propose new backtracing and response matching methods that\u00a0\u2026", "num_citations": "1\n", "authors": ["507"]}
{"title": "Variation-Aware Defect Characterization at Cell Level\n", "abstract": " Small Delay Faults (SDFs) are an indicator of reliability threats even if they do not affect the behavior of a system at nominal speed. Various defects may evolve over time into a complete system failure, and defects have to be distinguished from delays due to process variations which also change the circuit timing but are benign. Based on Monte-Carlo electrical simulation at cell level, in this work it is shown that a few measurements at different operating points of voltage and frequency are sufficient to identify a defect cell even if its behavior is completely within the specification range. The developed classifier is based on statistical learning and can be annotated to each element of a cell library to support manufacturing test, diagnosis and optimizing the burn-in process or yield.", "num_citations": "1\n", "authors": ["507"]}
{"title": "Advances in hardware reliability of reconfigurable many-core embedded systems\n", "abstract": " The continued need for performance increase in face of the end of Dennard scaling made many-core architectures with dozens of cores mainstream, and architectures with hundreds or even thousands of cores have already been investigated [1\u20135]. A typical tiled many-core architecture is shown in Figure 16.1 [6]. It consists of multiple tiles interconnected by a network-on-chip (NoC). Memory tiles and I/O tiles handle storage requests and periphery communications, whereas compute tiles host the actual cores including their caches (see right part of Figure 16.1). In addition to homogeneous processing cores, a tile may also host one or multiple heterogeneous cores. Heterogeneity allows to optimize the system for specific workloads and increases the efficiency over homogeneous cores. As a special case, runtime reconfigurable processors even allow the dynamic optimization of this specialization by customization of the hardware organization for changing workload requirements during runtime. This combines the advantages of high performance and low energy consumption only achievable in hardware, with the flexibility of software to customize the hardware resources at runtime. The i-Core reconfigurable processor [7, 8] is an example of such an architecture. It is sketched in Figure 16.1 and will be used as baseline in this chapter (details in Section 16.2. 1). Runtime reconfigurable processors based on field-programmable gate arrays (FPGAs) are a promising augment to conventional processor architectures and have gained economical relevance [9, 10]. They consist of a general purpose processor and a reconfigurable fabric that are\u00a0\u2026", "num_citations": "1\n", "authors": ["507"]}
{"title": "Multi-level timing and fault simulation on GPUs\n", "abstract": " In CMOS technology first-order parametric faults during manufacturing can exhibit severe changes in the timing as well as in the functional behavior of cells. Since these faults are hard to detect by conventional tests, the accurate simulation of these low-level faults plays an important role for test validation. However, pure low-level fault simulation approaches impose a high computational complexity that can quickly become inapplicable to larger simulation problems due to limitations in scalability.In this paper, the first parallel multi-level fault simulation approach on graphics processing units (GPUs) is presented. The approach utilizes both logic level and switch level descriptions concurrently in a mixed-abstraction timing simulation. The abstraction is lowered in user-defined so-called regions of interest that locally increase the modeling accuracy enabling low-level first-order parametric fault injection. Resulting signal\u00a0\u2026", "num_citations": "1\n", "authors": ["507"]}
{"title": "Online Test and Diagnosis\n", "abstract": " Reconfigurable Hardware, like Field-Programmable Gate Array (FPGA), is getting important in different fields ranging from various embedded systems to high performance computers and large-scale research systems. And due to new technologic advances in this field, like Run Time Reconfiguration (RTR), ie a spare part of the chip can be reconfigured dynamically while the rest is still running and won\u2019t get disturbed, more complex applications can be ported. The typical applications are space missions, continuous running systems, automotive and mobile devices, which benefit from the RTR technique. And they are mission critical or crucial in remote and always require high-reliability and high-availability.However, there are many sources which lead to FPGA faults. They exist during the lifetime of FPGAs. For example, at manufacturing level, the chip may be made with some defects; or during the course of usage\u00a0\u2026", "num_citations": "1\n", "authors": ["507"]}
{"title": "Embedded Test for Highly Accurate Defect Localization\n", "abstract": " Modern diagnosis algorithms are able to identify the defective circuit structure directly from existing fail data without being limited to any specialized fault models. Such algorithms however require test patterns with a high defect coverage, posing a major challenge particularly for embedded testing. In mixed-mode embedded test, a large amount of pseudo-random(PR) patterns are applied prior to deterministic test pattern. Partial Pseudo-Exhaustive Testing (P-PET)replaces these pseudo-random patterns during embedded testing by partial pseudo-exhaustive patterns to test a large portion of a circuit fault-model independently. The overall defect coverage is optimized compared to random testing or deterministic tests using the stuck-at fault model while maintaining a comparable hardware overhead and the same test application time. This work for the first time combines P-PET with a fault model independent diagnosis\u00a0\u2026", "num_citations": "1\n", "authors": ["507"]}
{"title": "Korrektur transienter Fehler in eingebetteten Speicherelementen\n", "abstract": " Kurzfassung/AbstractIn der vorliegenden Arbeit wird ein Schema zur Korrektur von transienten Fehlern in eingebetteten, pegelgesteuerten Speicherelementen vorgestellt. Das Schema verwendet Struktur-und Informationsredundanz, um Single Event Upsets (SEUs) in Registern zu erkennen und zu korrigieren. Mit geringem Mehraufwand kann ein betroffenes Bit lokalisiert und mit einem hier vorgestellten Bit-Flipping-Latch (BFL) r\u00fcckgesetzt werden, so dass die Zahl zus\u00e4tzlicher Taktzyklen im Fehlerfall minimiert wird. Ein Vergleich mit anderen Erkennungsund Korrekturschemata zeigt einen deutlich reduzierten Hardwaremehraufwand.In this paper a soft error correction scheme for embedded level sensitive storage elements is presented. The scheme employs structural-and information-redundancy to detect and correct Single Event Upsets (SEUs) in registers. With low additional hardware overhead the affected bit can be localized and reset with the presented Bit-Flipping-Latch (BFL), thereby minimizing the amount of additional clock cycles in the faulty case. A comparison with other detection and correction schemes shows a significantly lower hardware overhead.", "num_citations": "1\n", "authors": ["507"]}
{"title": "System reliability evaluation using concurrent multi-level simulation of structural faults\n", "abstract": " This paper provides a methodology that leverages state-of-the-art techniques for efficient fault simulation of structural faults together with transaction level modeling. This way it is possible to accurately evaluate the impact of the faults on the entire hardware/software system.", "num_citations": "1\n", "authors": ["507"]}
{"title": "Fault Adaptive Routing\n", "abstract": " Over the past decade technology scaling lead to performance increase, since it allows reducing the gate delay, thus increasing operating frequency and enables increasing the transistor density, but on the other hand, problems like power consumption and reliability arise through scaling. Physical defect occurs during chip manufacturing, the electromigration phenomena in wires and failure of NoC switch due to transistor aging process.Network on chip (NoC) is becoming a popular solution on-chip packet switched interconnection architecture [2][3]. NoC architecture is better in terms of bandwidth, scalability, and design complexity with increase in cores than traditional buses architecture. The packet is divided into smaller pieces called flits. The first flit which is the header flit contain the routing information is transmitted to discover and establish path to destination whereas the remaining flits follow the header. This technique is called wormhole switching and it uses small buffer space than Packet switching [5].", "num_citations": "1\n", "authors": ["507"]}
{"title": "Erkennung von transienten Fehlern in Schaltungen mit reduzierter Verlustleistung}}\n", "abstract": " {An effective technique to save power during scan based test is to switch off", "num_citations": "1\n", "authors": ["507"]}
{"title": "Testing and Monitoring Nanoscale Systems-Challenges and Strategies for Advanced Quality ssurance\n", "abstract": " The increased number of fabrication defects, spatial and temporal variability of parameters, as well as the growing impact of soft errors in nanoelectronic systems require a paradigm shift in design, verification and test. A robust design becomes mandatory to ensure dependable systems and acceptable yields. Design robustness, however, invalidates many traditional approaches for testing and implies enormous challenges. The RealTest Project addresses these problems for nanoscale CMOS and targets unified design and test strategies to support both a robust design and a coordinated quality assurance after manufacturing and during the lifetime of a system. The paper first gives a short overview of the research activities within the project and then focuses on a first result concerning soft errors in combinational logic. It will be shown that common electrical models for particle strikes in random logic have underestimated the effects on the system behavior. The refined model developed within the RealTest Project predicts about twice as many single events upsets (SEUs) caused by particle strikes as traditional models.", "num_citations": "1\n", "authors": ["507"]}
{"title": "Software-basierender Selbsttest von Prozessorkernen unter Verlustleistungsbeschr\u00e4nkung.\n", "abstract": " url={http://www2. informatik. uni-stuttgart. de/cgi-bin/NCSTRL/NCSTRL_view. pl? id= INPROC-2005-128&engl= 1}", "num_citations": "1\n", "authors": ["507"]}
{"title": "DLBIST for Delay Testing\n", "abstract": " Due to its inherent support for at-speed test, BIST is an attractive approach to test delay faults. Deterministic logic BIST (DLBIST) is a technique which was successfully applied to enhance the quality of BIST in the case of stuck-at test. The test of delay faults requires the application of pattern pairs. Consequently, delay faults have a lower random pattern testability than stuck-at faults and this increases the need for a DLBIST scheme. On the other hand, a DLBIST solution is expected to require a larger mapping effort and logic overhead than in the case of stuck-at test.In this paper, we present the extension of a DLBIST scheme which becomes available for the test of both delay (transition) faults and stuck-at faults. Functional justification is used to generate the pattern pairs required by the target delay faults. We investigate the efficiency of the extended scheme in the case of some industrial benchmarks.", "num_citations": "1\n", "authors": ["507"]}
{"title": "Power conscious BIST approaches\n", "abstract": " {The paper presents a method for testing a system-on-a-chip by using a", "num_citations": "1\n", "authors": ["507"]}
{"title": "Erfassung und Modellierung komplexer Funktionsfehler in Mikroelektronik-Bauelementen\n", "abstract": " Es wird ein Verfahren vorgestellt, das f\u00fcr die Grundzellen einer Zellbibliothek layoutabh\u00e4ngig die m\u00f6glichen Fehlfunktionen bestimmt, die durch Fertigungsfehler verursacht werden k\u00f6nnen. Eingabe f\u00fcr das Verfahren sind neben dem Layout einer Zelle die Proze\u00dfparameter und die Defektverteilungen, Ausgabe sind die realistischen Fehlfunktionen mit ihren Auftrittswahrscheinlichkeiten. Damit k\u00f6nnen Testerzeugung und Testablauf beschleunigt, schwer testbare Fehler bestimmt und ihre Ursachen lokalisiert und beseitigt werden.", "num_citations": "1\n", "authors": ["507"]}
{"title": "Pr\u00fcfgerechter Entwurf und Test hochintegrierter Schaltungen\n", "abstract": " Der Beitrag gibt einen \u00dcberblick \u00fcber die wichtigsten praxisrelevanten Teststrategien, wobei unter einer Teststrategie nicht nur die Verfahren zur Testsatzerzeugung und zur eigentlichen Testdurchf\u00fchrung, sondern auch das zugrunde liegende Fehlermodell und die erforderlichen testfreundlichen Entwurfsma\u00dfnahmen, die die Voraussetzung f\u00fcr die Anwendung dieser Verfahren darstellen, zu verstehen sind. Es werden die g\u00e4ngigsten Methoden zum konventionellen externen Test vorgestellt und bewertet sowie das Prinzip der immer breitere Anwendung findenden Selbsttestmethoden und ihre Vorteile erl\u00e4utert. Nach einem kurzen Ausblick auf die Fortschritte, die Verfahren zur automatischen Synthese testbarer Schaltungen erhoffen lassen, werden schlie\u00dflich Aspekte des Systemtests und insbesondere das Boundary-Scan-Prinzip und die damit verbundenen Vorteile diskutiert.", "num_citations": "1\n", "authors": ["507"]}
{"title": "Efficient test set evaluation\n", "abstract": " The fault coverage obtained by a set of test patterns is usually determined by expensive fault simulation. Even when using fault dropping techniques, fault simulation provides more information than actually needed. For each fault, the pattern is determined which detects this fault first. This is mainly redundant information if diagnosis is not required. One can dispense with this high resolution and restrict interest to the set of faults which is detected by a set of patterns. It is shown theoretically and practically that this information is obtainable in an highly efficient way.", "num_citations": "1\n", "authors": ["507"]}
{"title": "Maximizing the fault coverage in complex circuits by minimal number of signatures\n", "abstract": " Methods to minimize the number of evaluated signatures without reducing the fault coverage are presented. This is possible because the signatures can influence one another during the test execution. For a fixed test schedule a minimal subset of signatures can be selected, and for a predetermined minimal subset of signatures the test schedule can be constructed such that the fault coverage is maximum. Both approaches result in significant hardware savings when a self-test is implemented.< >", "num_citations": "1\n", "authors": ["507"]}
{"title": "Methoden der Testvorbereitung zum IC-Entwurf\n", "abstract": " Neben dem eigentlichen Testen umfa\u00dft eine Teststrategie die Auswahl eines geeigneten Fehlermodells, ein Verfahren f\u00fcr den pr\u00fcfgerechten strukturierten Entwurf sowie die Testsatzerzeugung. Ziel dieser Pr\u00fcfvorbereitung ist die Steigerung der Produktqualit\u00e4t sowie die Senkung der Testkosten bei integrierten Schaltungen.", "num_citations": "1\n", "authors": ["507"]}
{"title": "The effectiveness of different test sets for PLAs\n", "abstract": " It has been theoretically demonstrated that the single stuck-at fault model for a PLA does not cover as many faults as the single crosspoint model. What has not been demonstrated is the real relative effectiveness of test sets generated using these models. This paper presents the results of a study involving presenting a number of test sets to fabricated PLAs to determine their effectiveness. The test sets included weighted random patterns, of particular interest owing to PLAs being random resistant. Details are given of a method to generate weights, taking into account a PLA's structure.", "num_citations": "1\n", "authors": ["507"]}
{"title": "Weighted random patterns with multiple distributions\n", "abstract": " It is well known that random test lenths can be reduced by orders of magnitude using biased random patterns. But there are also some circuits resistant to optimising. In this paper it is shown that this problem can be solved using several distributions instead of a single one. Firstly we compute bounds of the error caused by the assumption that fault detection consists of completely independent events. Secondly we prove a sharp estimation of the error caused caused by assuming the random property instead of the pseudo-random property of shift register sequences. Finally a heuristic is presented in order to compute an optimal number of random pattern sets, where each set has its specific distribution and its specific size.", "num_citations": "1\n", "authors": ["507"]}
{"title": "Testing Nanoelectronic Circuits Under Massive Statistical Process Variations\n", "abstract": " The increasing parameter variations in nanoelectronic circuits and systems have led to a paradigm shift towards statistical design. Moreover, variation-tolerant adaptive and self-calibrating systems are steadily gaining importance. Manufacturing test methods have, to this date, failed to adequately incorporate these developments. To keep pace with design trends, efficient test methods are needed coping with the specific challenges due to statistical and variation-tolerant design.The Special Session will discuss how statistical information can be integrated into known test approaches, such as fault simulation and test pattern generation, and propose dedicated approaches for adaptive and self-calibrating systems. The starting point is an accurate statistical characterization of standard library elements relying both on defect-oriented fault modeling and on a Monte Carlo analysis of the effects of varying process parameters. Exemplary data for 45 nm technology are presented, which have been derived by massive parallel electrical simulation on a large compute cluster and based on industrial input and openly available data. Integrating this information into fault simulation and test pattern generation allows to determine for which parameter configurations a fault can be detected as well as to optimize test patterns, such that faults are detected for a maximum set of parameter configurations. To account for current trends in computer design, the methods are optimized for implementation on state-of-the-art parallel processors such as GPGPUs. The presented tool flows leverage recent improvements in basic algorithms, including last-generation SAT solvers\u00a0\u2026", "num_citations": "1\n", "authors": ["507"]}