{"title": "Crowdsourcing systems on the world-wide web\n", "abstract": " The practice of crowdsourcing is transforming the Web and giving rise to a new field.", "num_citations": "1920\n", "authors": ["2072"]}
{"title": "Learning to map between ontologies on the semantic web\n", "abstract": " Ontologies play a prominent role on the Semantic Web. They make possible the widespread publication of machine understandable data, opening myriad opportunities for automated information processing. However, because of the Semantic Web's distributed nature, data on it will inevitably come from many different ontologies. Information processing across ontologies is not possible without knowing the semantic mappings between their elements. Manually finding such mappings is tedious, error-prone, and clearly not possible at the Web scale. Hence, the development of tools to assist in the ontology mapping process is crucial to the success of the Semantic Web. We describe glue, a system that employs machine learning techniques to find such mappings. Given two ontologies, for each concept in one ontology glue finds the most similar concept in the other ontology. We give well-founded probabilistic\u00a0\u2026", "num_citations": "1448\n", "authors": ["2072"]}
{"title": "Reconciling schemas of disparate data sources: A machine-learning approach\n", "abstract": " A data-integration system provides access to a multitude of data sources through a single mediated schema. A key bottleneck in building such systems has been the laborious manual construction of semantic mappings between the source schemas and the mediated schema. We describe LSD, a system that employs and extends current machine-learning techniques to semi-automatically find such mappings. LSD first asks the user to provide the semantic mappings for a small set of data sources, then uses these mappings together with the sources to train a set of learners. Each learner exploits a different type of information either in the source schemas or in their data. Once the learners have been trained, LSD finds semantic mappings for a new data source by applying the learners, then combining their predictions using a meta-learner. To further improve matching accuracy, we extend machine learning techniques\u00a0\u2026", "num_citations": "1117\n", "authors": ["2072"]}
{"title": "Principles of data integration\n", "abstract": " Principles of Data Integration is the first comprehensive textbook of data integration, covering theoretical principles and implementation issues as well as current challenges raised by the semantic web and cloud computing. The book offers a range of data integration solutions enabling you to focus on what is most relevant to the problem at hand. Readers will also learn how to build their own algorithms and implement their own data integration application. Written by three of the most respected experts in the field, this book provides an extensive introduction to the theory and concepts underlying today's data integration techniques, with detailed, instruction for their application using concrete examples throughout to explain the concepts. This text is an ideal resource for database practitioners in industry, including data warehouse engineers, database system designers, data architects/enterprise architects, database researchers, statisticians, and data analysts; students in data analytics and knowledge discovery; and other data professionals working at the R&D and implementation levels. Offers a range of data integration solutions enabling you to focus on what is most relevant to the problem at hand Enables you to build your own algorithms and implement your own data integration applications", "num_citations": "775\n", "authors": ["2072"]}
{"title": "Semantic integration research in the database community: A brief survey\n", "abstract": " Semantic integration has been a long-standing challenge for the database community. It has received steady attention over the past two decades, and has now become a prominent area of database research. In this article, we first review database applications that require semantic integration and discuss the difficulties underlying the integration process. We then describe recent progress and identify open research issues. We focus in particular on schema matching, a topic that has received much attention in the database community, but also discuss data matching (for example, tuple deduplication) and open issues beyond the match discovery context (for example, reasoning with matches, match verification and repair, and reconciling inconsistent data values). For previous surveys of database research on semantic integration, see Rahm and Bernstein (2001); Ouksel and Seth (1999); and Batini, Lenzerini, and Navathe (1986).", "num_citations": "710\n", "authors": ["2072"]}
{"title": "Ontology matching: A machine learning approach\n", "abstract": " This chapter studies ontology matching: the problem of finding the semantic mappings between two given ontologies. This problem lies at the heart of numerous information processing applications. Virtually any application that involves multiple ontologies must establish semantic mappings among them, to ensure interoperability. Examples of such applications arise in myriad domains, including e-commerce, knowledge management, e-learning, information extraction, bio-informatics, web services, and tourism (see Part D of this book on ontology applications).               Despite its pervasiveness, today ontology matching is still largely conducted by hand, in a labor-intensive and error-prone process. The manual matching has now become a key bottleneck in building large-scale information management systems. The advent of technologies such as the WWW, XML, and the emerging Semantic Web will further\u00a0\u2026", "num_citations": "701\n", "authors": ["2072"]}
{"title": "Learning to match ontologies on the semantic web\n", "abstract": " On the Semantic Web, data will inevitably come from many different ontologies, and information processing across ontologies is not possible without knowing the semantic mappings between them. Manually finding such mappings is tedious, error-prone, and clearly not possible on the Web scale. Hence the development of tools to assist in the ontology mapping process is crucial to the success of the Semantic Web. We describe GLUE, a system that employs machine learning techniques to find such mappings. Given two ontologies, for each concept in one ontology GLUE finds the most similar concept in the other ontology. We give well-founded probabilistic definitions to several practical similarity measures and show that GLUE can work with all of them. Another key feature of GLUE is that it uses multiple learning strategies, each of which exploits well a different type of information either in the data\u00a0\u2026", "num_citations": "678\n", "authors": ["2072"]}
{"title": "iMAP: Discovering complex semantic matches between database schemas\n", "abstract": " Creating semantic matches between disparate data sources is fundamental to numerous data sharing efforts. Manually creating matches is extremely tedious and error-prone. Hence many recent works have focused on automating the matching process. To date, however, virtually all of these works deal only with one-to-one (1-1) matches, such as address= location. They do not consider the important class of more complex matches, such as address= concat (city, state) and room-pric= room-rate*(1+ tax-rate). We describe the iMAP system which semi-automatically discovers both 1-1 and complex matches. iMAP reformulates schema matching as a search in an often very large or infinite match space. To search effectively, it employs a set of searchers, each discovering specific types of complex matches. To further improve matching accuracy, iMAP exploits a variety of domain knowledge, including past complex\u00a0\u2026", "num_citations": "581\n", "authors": ["2072"]}
{"title": "Corpus-based schema matching\n", "abstract": " Schema matching is the problem of identifying corresponding elements in different schemas. Discovering these correspondences or matches is inherently difficult to automate. Past solutions have proposed a principled combination of multiple algorithms. However, these solutions sometimes perform rather poorly due to the lack of sufficient evidence in the schemas being matched. In this paper we show how a corpus of schemas and mappings can be used to augment the evidence about the schemas being matched, so they can be matched better. Such a corpus typically contains multiple schemas that model similar concepts and hence enables us to learn variations in the elements and their properties. We exploit such a corpus in two ways. First, we increase the evidence about each element being matched by including evidence from similar elements in the corpus. Second, we learn statistics about elements and\u00a0\u2026", "num_citations": "538\n", "authors": ["2072"]}
{"title": "An interactive clustering-based approach to integrating source query interfaces on the deep web\n", "abstract": " An increasing number of data sources now become available on the Web, but often their contents are only accessible through query interfaces. For a domain of interest, there often exist many such sources with varied coverage or querying capabilities. As an important step to the integration of these sources, we consider the integration of their query interfaces. More specifically, we focus on the crucial step of the integration: accurately matching the interfaces. While the integration of query interfaces has received more attentions recently, current approaches are not sufficiently general:(a) they all model interfaces with flat schemas;(b) most of them only consider 1: 1 mappings of fields over the interfaces;(c) they all perform the integration in a blackbox-like fashion and the whole process has to be restarted from scratch if anything goes wrong; and (d) they often require laborious parameter tuning. In this paper, we\u00a0\u2026", "num_citations": "367\n", "authors": ["2072"]}
{"title": "Learning to match the schemas of data sources: A multistrategy approach\n", "abstract": " The problem of integrating data from multiple data sources\u2014either on the Internet or within enterprises\u2014has received much attention in the database and AI communities. The focus has been on building data integration systems that provide a uniform query interface to the sources. A key bottleneck in building such systems has been the laborious manual construction of semantic mappings between the query interface and the source schemas. Examples of mappings are \u201celement location maps to address\u201d and \u201cprice maps to listed-price\u201d. We propose a multistrategy learning approach to automatically find such mappings. The approach applies multiple learner modules, where each module exploits a different type of information either in the schemas of the sources or in their data, then combines the predictions of the modules using a meta-learner. Learner modules employ a variety of techniques, ranging from\u00a0\u2026", "num_citations": "351\n", "authors": ["2072"]}
{"title": "Tuffy: Scaling up statistical inference in markov logic networks using an rdbms\n", "abstract": " Markov Logic Networks (MLNs) have emerged as a powerful framework that combines statistical and logical reasoning; they have been applied to many data intensive problems including information extraction, entity resolution, and text mining. Current implementations of MLNs do not scale to large real-world data sets, which is preventing their wide-spread adoption. We present Tuffy that achieves scalability via three novel contributions: (1) a bottom-up approach to grounding that allows us to leverage the full power of the relational optimizer, (2) a novel hybrid architecture that allows us to perform AI-style local search efficiently using an RDBMS, and (3) a theoretical insight that shows when one can (exponentially) improve the efficiency of stochastic local search. We leverage (3) to build novel partitioning, loading, and parallel algorithms. We show that our approach outperforms state-of-the-art implementations in both quality and speed on several publicly available datasets.", "num_citations": "343\n", "authors": ["2072"]}
{"title": "Deep learning for entity matching: A design space exploration\n", "abstract": " Entity matching (EM) finds data instances that refer to the same real-world entity. In this paper we examine applying deep learning (DL) to EM, to understand DL's benefits and limitations. We review many DL solutions that have been developed for related matching tasks in text processing (eg, entity linking, textual entailment, etc.). We categorize these solutions and define a space of DL solutions for EM, as embodied by four solutions with varying representational power: SIF, RNN, Attention, and Hybrid. Next, we investigate the types of EM problems for which DL can be helpful. We consider three such problem types, which match structured data instances, textual instances, and dirty instances, respectively. We empirically compare the above four DL solutions with Magellan, a state-of-the-art learning-based EM solution. The results show that DL does not outperform current solutions on structured EM, but it can\u00a0\u2026", "num_citations": "296\n", "authors": ["2072"]}
{"title": "Learning Source Description for Data Integration.\n", "abstract": " To build a data-integration system, the application designer must specify a mediated schema and supply the descriptions of data sources. A source description contains a source schema that describes the content of the source, and a mapping between the corresponding elements of the source schema and the mediated schema. Manually constructing these mappings is both labor-intensive and error-prone, and has proven to be a major bottleneck in deploying large-scale data integration systems in practice. In this paper we report on our initial work toward automatically learning mappings between source schemas and the mediated schema. Specifically, we investigate finding one-to-one mappings for the leaf elements of source schemas. We describe LSD, a system that automatically finds such mappings. LSD consults a set of learner modules\u2013where each module looks at the problem from a different perspective, then combines the predictions of the modules using a meta-learner. We report on experimental results of applying LSD to five sources in the real-estate domain.", "num_citations": "258\n", "authors": ["2072"]}
{"title": "Declarative Information Extraction Using Datalog with Embedded Extraction Predicates.\n", "abstract": " In this paper we argue that developing information extraction (IE) programs using Datalog with embedded procedural extraction predicates is a good way to proceed. First, compared to current ad-hoc composition using, eg, Perl or C++, Datalog provides a cleaner and more powerful way to compose small extraction modules into larger programs. Thus, writing IE programs this way retains and enhances the important advantages of current approaches: programs are easy to understand, debug, and modify. Second, once we write IE programs in this framework, we can apply query optimization techniques to them. This gives programs that, when run over a variety of data sets, are more efficient than any monolithic program because they are optimized based on the statistics of the data on which they are invoked. We show how optimizing such programs raises challenges specific to text data that cannot be accommodated in the current relational optimization framework, then provide initial solutions. Extensive experiments over real-world data demonstrate that optimization is indeed vital for IE programs and that we can effectively optimize IE programs written in this proposed framework.", "num_citations": "248\n", "authors": ["2072"]}
{"title": "Corleone: Hands-off crowdsourcing for entity matching\n", "abstract": " Recent approaches to crowdsourcing entity matching (EM) are limited in that they crowdsource only parts of the EM workflow, requiring a developer to execute the remaining parts. Consequently, these approaches do not scale to the growing EM need at enterprises and crowdsourcing startups, and cannot handle scenarios where ordinary users (ie, the masses) want to leverage crowdsourcing to match entities. In response, we propose the notion of hands-off crowdsourcing (HOC)}, which crowdsources the entire workflow of a task, thus requiring no developers. We show how HOC can represent a next logical direction for crowdsourcing research, scale up EM at enterprises and crowdsourcing startups, and open up crowdsourcing for the masses. We describe Corleone, a HOC solution for EM, which uses the crowd in all major steps of the EM process. Finally, we discuss the implications of our work to executing\u00a0\u2026", "num_citations": "241\n", "authors": ["2072"]}
{"title": "Privacy-preserving data integration and sharing\n", "abstract": " Integrating data from multiple sources has been a longstanding challenge in the database community. Techniques such as privacy-preserving data mining promises privacy, but assume data has integration has been accomplished. Data integration methods are seriously hampered by inability to share the data to be integrated. This paper lays out a privacy framework for data integration. Challenges for data integration in the context of this framework are discussed, in the context of existing accomplishments in data integration. Many of these challenges are opportunities for the data mining community.", "num_citations": "240\n", "authors": ["2072"]}
{"title": "The Claremont report on database research\n", "abstract": " In late May, 2008, a group of database researchers, architects, users and pundits met at the Claremont Resort in Berkeley, California to discuss the state of the research field and its impacts on practice. This was the seventh meeting of this sort in twenty years, and was distinguished by a broad consensus that we are at a turning point in the history of the field, due both to an explosion of data and usage scenarios, and to major shifts in computing hardware and platforms. Given these forces, we are at a time of opportunity for research impact, with an unusually large potential for influential results across computing, the sciences and society. This report details that discussion, and highlights the group's consensus view of new focus areas, including new database engine architectures, declarative programming languages, the interplay of structured and unstructured data, cloud data services, and mobile and virtual worlds\u00a0\u2026", "num_citations": "211\n", "authors": ["2072"]}
{"title": "On the provenance of non-answers to queries over extracted data\n", "abstract": " In information extraction, uncertainty is ubiquitous. For this reason, it is useful to provide users querying extracted data with explanations for the answers they receive. Providing the provenance for tuples in a query result partially addresses this problem, in that provenance can explain why a tuple is in the result of a query. However, in some cases explaining why a tuple is not in the result may be just as helpful. In this work we focus on providing provenance-style explanations for non-answers and develop a mechanism for providing this new type of provenance. Our experience with an information extraction prototype suggests that our approach can provide effective provenance information that can help a user resolve their doubts over non-answers to a query.", "num_citations": "197\n", "authors": ["2072"]}
{"title": "Magellan: toward building entity matching management systems over data science stacks\n", "abstract": " Entity matching (EM) has been a long-standing challenge in data management. Most current EM works, however, focus only on developing matching algorithms. We argue that far more efforts should be devoted to building EM systems. We discuss the limitations of current EM systems, then present Magellan, a new kind of EM systems that addresses these limitations. Magellan is novel in four important aspects. (1) It provides a how-to guide that tells users what to do in each EM scenario, step by step. (2) It provides tools to help users do these steps; the tools seek to cover the entire EM pipeline, not just matching and blocking as current EM systems do. (3) Tools are built on top of the data science stacks in Python, allowing Magellan to borrow a rich set of capabilities in data cleaning, IE, visualization, learning, etc. (4) Magellan provide a powerful scripting environment to facilitate interactive experimentation and allow\u00a0\u2026", "num_citations": "194\n", "authors": ["2072"]}
{"title": "eTuner: tuning schema matching software using synthetic scenarios\n", "abstract": " Most recent schema matching systems assemble multiple components, each employing a particular matching technique. The domain user mustthen tune the system: select the right component to be executed and correctly adjust their numerous \u201cknobs\u201d (e.g., thresholds, formula coefficients). Tuning is skill and time intensive, but (as we show) without it the matching accuracy is significantly inferior. We describe eTuner, an approach to automatically tune schema matching systems. Given a schema S, we match S against synthetic schemas, for which the ground truth mapping is known, and find a tuning that demonstrably improves the performance of matching S against real schemas. To efficiently search the huge space of tuning configurations, eTuner works sequentially, starting with tuning the lowest level components. To increase the applicability of eTuner, we develop methods to tune a broad range of\u00a0\u2026", "num_citations": "193\n", "authors": ["2072"]}
{"title": "Muppet: Mapreduce-style processing of fast data\n", "abstract": " MapReduce has emerged as a popular method to process big data. In the past few years, however, not just big data, but fast data has also exploded in volume and availability. Examples of such data include sensor data streams, the Twitter Firehose, and Facebook updates. Numerous applications must process fast data. Can we provide a MapReduce-style framework so that developers can quickly write such applications and execute them over a cluster of machines, to achieve low latency and high scalability? In this paper we report on our investigation of this question, as carried out at Kosmix and WalmartLabs. We describe MapUpdate, a framework like MapReduce, but specifically developed for fast data. We describe Muppet, our implementation of MapUpdate. Throughout the description we highlight the key challenges, argue why MapReduce is not well suited to address them, and briefly describe our current solutions. Finally, we describe our experience and lessons learned with Muppet, which has been used extensively at Kosmix and WalmartLabs to power a broad range of applications in social media and e-commerce.", "num_citations": "190\n", "authors": ["2072"]}
{"title": "Matching schemas in online communities: A web 2.0 approach\n", "abstract": " When integrating data from multiple sources, a key task that online communities often face is to match the schemas of the data sources. Today, such matching often incurs a huge workload that overwhelms the relatively small set of volunteer integrators. In such cases, community members may not even volunteer to be integrators, due to the high workload, and consequently no integration systems can be built. To address this problem, we propose to enlist the multitude of users in the community to help match the schemas, in a Web 2.0 fashion. We discuss the challenges of this approach and provide initial solutions. Finally, we describe an extensive set of experiments on both real-world and synthetic data that demonstrate the utility of the approach.", "num_citations": "172\n", "authors": ["2072"]}
{"title": "Efficient keyword search across heterogeneous relational databases\n", "abstract": " Keyword search is a familiar and potentially effective way to find information of interest that is \"locked\" inside relational databases. Current work has generally assumed that answers for a keyword query reside within a single database. Many practical settings, however, require that we combine tuples from multiple databases to obtain the desired answers. Such databases are often autonomous and heterogeneous in their schemas and data. This paper describes Kite, a solution to the keyword-search problem over heterogeneous relational databases. Kite combines schema matching and structure discovery techniques to find approximate foreign-key joins across heterogeneous databases. Such joins are critical for producing query results that span multiple databases and relations. Kite then exploits the joins - discovered automatically across the databases - to enable fast and effective querying over the distributed\u00a0\u2026", "num_citations": "157\n", "authors": ["2072"]}
{"title": "Combining keyword search and forms for ad hoc querying of databases\n", "abstract": " A common criticism of database systems is that they are hard to query for users uncomfortable with a formal query language. To address this problem, form-based interfaces and keyword search have been proposed; while both have benefits, both also have limitations. In this paper, we investigate combining the two with the hopes of creating an approach that provides the best of both. Specifically, we propose to take as input a target database and then generate and index a set of query forms offline. At query time, a user with a question to be answered issues standard keyword search queries; but instead of returning tuples, the system returns forms relevant to the question. The user may then build a structured query with one of these forms and submit it back to the system for evaluation. In this paper, we address challenges that arise in form generation, keyword search over forms, and ranking and displaying these\u00a0\u2026", "num_citations": "148\n", "authors": ["2072"]}
{"title": "Entity extraction, linking, classification, and tagging for social media: a wikipedia-based approach\n", "abstract": " Many applications that process social data, such as tweets, must extract entities from tweets (e.g., \"Obama\" and \"Hawaii\" in \"Obama went to Hawaii\"), link them to entities in a knowledge base (e.g., Wikipedia), classify tweets into a set of predefined topics, and assign descriptive tags to tweets. Few solutions exist today to solve these problems for social data, and they are limited in important ways. Further, even though several industrial systems such as OpenCalais have been deployed to solve these problems for text data, little if any has been published about them, and it is unclear if any of the systems has been tailored for social media. In this paper we describe in depth an end-to-end industrial system that solves these problems for social data. The system has been developed and used heavily in the past three years, first at Kosmix, a startup, and later at WalmartLabs. We show how our system uses a Wikipedia\u00a0\u2026", "num_citations": "145\n", "authors": ["2072"]}
{"title": "K-anonymization as spatial indexing: Toward scalable and incremental anonymization\n", "abstract": " In this paper, we introduce a novel approach to k-anonymization by making a new observation of a strikingly similar parallel between database indexing and k-anonymity. In general, however, the table to be published may contain more than one quasi-identifier attribute, so rather than use B + -trees, we suggest multidimensional spatial indexing as the basis for anonymization. We take a brief detour to discuss a measure for the quality of an anonymization algorithm.", "num_citations": "135\n", "authors": ["2072"]}
{"title": "Introduction to the special issue on semantic integration\n", "abstract": " Semantic heterogeneity is one of the key challenges in integrating and sharing data across disparate sources, data exchange and migration, data warehousing, model management, the Semantic Web and peer-to-peer databases. Semantic heterogeneity can arise at the schema level and at the data level. At the schema level, sources can differ in relations, attribute and tag names, data normalization, levels of detail, and the coverage of a particular domain. The problem of reconciling schema-level heterogeneity is often referred to as schema matching or schema mapping. At the data level, we find different representations of the same real-world entities (e.g., people, companies, publications, etc.). Reconciling data-level heterogeneity is referred to as data deduplication, record linkage, and entity/object matching. To exacerbate the heterogeneity challenges, schema elements of one source can be represented as data\u00a0\u2026", "num_citations": "135\n", "authors": ["2072"]}
{"title": "Building, maintaining, and using knowledge bases: a report from the trenches\n", "abstract": " A knowledge base (KB) contains a set of concepts, instances, and relationships. Over the past decade, numerous KBs have been built, and used to power a growing array of applications. Despite this flurry of activities, however, surprisingly little has been published about the end-to-end process of building, maintaining, and using such KBs in industry. In this paper we describe such a process. In particular, we describe how we build, update, and curate a large KB at Kosmix, a Bay Area startup, and later at WalmartLabs, a development and research lab of Walmart. We discuss how we use this KB to power a range of applications, including query understanding, Deep Web search, in-context advertising, event monitoring in social media, product search, social gifting, and social mining. Finally, we discuss how the KB team is organized, and the lessons learned. Our goal with this paper is to provide a real-world case\u00a0\u2026", "num_citations": "131\n", "authors": ["2072"]}
{"title": "Crossing the Structure Chasm.\n", "abstract": " Online information comes in two flavors: unstructured corpora of text on the one hand, and structured data managed by databases and knowledge bases on the other. These two different kinds of data lead to very different authoring, management and search paradigms. In the first, search is based on keywords and answers are ranked according to relevance. In the second, search is based on queries in a formal language (eg, SQL), and all the answers returned for the query are correct according to the underlying semantics of the system. In the u-world of unstructured data, authoring data is straightforward. In contrast, in the s-world of structured data, authoring data is a conceptual effort that requires technical expertise and substantial up front effort; the author is required to provide a comprehensive structure (ie, schema) of the domain before entering data. This paper is focused on the profound difference between the u-world and the s-world: we argue that there is a structure chasm between these two worlds. Crossing this structure chasm means introducing technology that imports some of the attractive properties of the u-world into the s-world, facilitating the creation of large-scale data sharing systems. Our first goal is to try to place the problem of crossing the chasm prominently in the data management community\u2019s agenda. While many research efforts have addressed specific aspects of the problem, we introduce a paradigm that places these efforts in context. Our second goal, which occupies the bulk of the paper, is to provide an example architecture for a system that crosses the chasm: we describe the Revere system, which focuses on the chasm\u00a0\u2026", "num_citations": "130\n", "authors": ["2072"]}
{"title": "Constraint-based entity matching\n", "abstract": " Entity matching is the problem of deciding if two given mentions in the data, such as \u201cHelen Hunt\u201d and \u201cHM Hunt\u201d, refer to the same real-world entity. Numerous solutions have been developed, but they have not considered in depth the problem of exploiting integrity constraints that frequently exist in the domains. Examples of such constraints include \u201ca mention with age two cannot match a mention with salary 200K\u201d and \u201cif two paper citations match, then their authors are likely to match in the same order\u201d. In this paper we describe a probabilistic solution to entity matching that exploits such constraints to improve matching accuracy. At the heart of the solution is a generative model that takes into account the constraints during the generation process, and provides well-defined interpretations of the constraints. We describe a novel combination of EM and relaxation labeling algorithms that efficiently learns the model, thereby matching mentions in an unsupervised way, without the need for annotated training data. Experiments on several real-world domains show that our solution can exploit constraints to significantly improve matching accuracy, by 3-12% F-1, and that the solution scales up to large data sets.", "num_citations": "128\n", "authors": ["2072"]}
{"title": "The clinical potential of influencing Nrf2 signaling in degenerative and immunological disorders\n", "abstract": " Nuclear factor (erythroid-derived 2)-like 2 (Nrf2; encoded in humans by the NFE2L2 gene) is a transcription factor that regulates the gene expression of a wide variety of cytoprotective phase II detoxification and antioxidant enzymes through a promoter sequence known as the antioxidant-responsive element (ARE). The ARE is a promoter element found in many cytoprotective genes; therefore, Nrf2 plays a pivotal role in the ARE-driven cellular defense system against environmental stresses. Agents that target the ARE/Nrf2 pathway have been tested in a wide variety of disorders, with at least one new Nrf2-activating drug now approved by the US Food and Drug Administration. Examination of in vitro and in vivo experimental results, and taking into account recent human clinical trial results, has led to an opinion that Nrf2-activating strategies\u2013which can include drugs, foods, dietary supplements, and exercise\u2013are likely\u00a0\u2026", "num_citations": "123\n", "authors": ["2072"]}
{"title": "The Beckman report on database research\n", "abstract": " Database researchers paint big data as a defining challenge. To make the most of the enormous opportunities at hand will require focusing on five research areas.", "num_citations": "122\n", "authors": ["2072"]}
{"title": "The claremont report on database research\n", "abstract": " Database research is expanding, with major efforts in system architecture, new languages, cloud services, mobile and virtual worlds, and interplay between structure and text.", "num_citations": "122\n", "authors": ["2072"]}
{"title": "Semantic integration\n", "abstract": " Sharing data across disparate sources requires solving many problems of semantic integration, such as matching ontologies or schemas, detecting duplicate tuples, reconciling inconsistent data values, modeling complex relations between concepts in different sources, and reasoning with semantic mappings. This issue of AI Magazine includes papers that discuss various methods on establishing mappings between ontology elements or data fragments. The collection includes papers that discuss semantic-integration issues in such contexts as data integration and web services. The issue also includes a brief survey of semantic-integration research in the database community.", "num_citations": "117\n", "authors": ["2072"]}
{"title": "Chimera: Large-scale classification using machine learning, rules, and crowdsourcing\n", "abstract": " Large-scale classification is an increasingly critical Big Data problem. So far, however, very little has been published on how this is done in practice. In this paper we describe Chimera, our solution to classify tens of millions of products into 5000+ product types at WalmartLabs. We show that at this scale, many conventional assumptions regarding learning and crowdsourcing break down, and that existing solutions cease to work. We describe how Chimera employs a combination of learning, rules (created by in-house analysts), and crowdsourcing to achieve accurate, continuously improving, and cost-effective classification. We discuss a set of lessons learned for other similar Big Data systems. In particular, we argue that at large scales crowdsourcing is critical, but must be used in combination with learning, rules, and in-house analysts. We also argue that using rules (in conjunction with learning) is a must, and that\u00a0\u2026", "num_citations": "115\n", "authors": ["2072"]}
{"title": "Community information management.\n", "abstract": " We introduce CIMple, a joint project between the University of Illinois and the University of Wisconsin. CIMple aims to develop a software platform that can be rapidly deployed and customized to manage data-rich online communities. We first describe the envisioned working of such a software platform and our prototype, DBLife, which is a community portal being developed for the database research community. We then describe the technical challenges in CIMple and our solution approach. Finally, we discuss managing uncertainty and provenance, a crucial task in making our software platform practical.", "num_citations": "114\n", "authors": ["2072"]}
{"title": "Learning to map between structured representations of data\n", "abstract": " This dissertation studies representation matching: the problem of creating semantic mappings between two data representations. Examples of data representations are relational schemas, ontologies, and XML DTDs. Examples of semantic mappings include \u201celement location of one representation maps to element address of the other\u201d,\u201ccontact-phone maps to agent-phone\u201d, and \u201clisted-price maps to price*(1+ tax-rate)\u201d.", "num_citations": "110\n", "authors": ["2072"]}
{"title": "Mapping maintenance for data integration systems\n", "abstract": " \u25ba Problem Definition\u25ba Previous Work in the domain & Background information\u25ba Maveric, automatic mapping verification system\u25ba Sensor Ensemble", "num_citations": "108\n", "authors": ["2072"]}
{"title": "Building structured web community portals: A top-down, compositional, and incremental approach\n", "abstract": " Structured community portals extract and integrate information from raw Web pages to present a unified view of entities and relationships in the community. In this paper we argue that to build such portals, a top-down, compositional, and incremental approach is a good way to proceed. Compared to current approaches that employ complex monolithic techniques, this approach is easier to develop, understand, debug, and optimize. In this approach, we first select a small set of important community sources. Next, we compose plans that extract and integrate data from these sources, using a set of extraction/integration operators. Executing these plans yields an initial structured portal. We then incrementally expand this portal by monitoring the evolution of current data sources, to detect and add new data sources. We describe our initial solutions to the above steps, and a case study of employing these solutions to build DBLife, a portal for the database community. We found that DBLife could be built quickly and achieve high accuracy using simple extraction/integration operators, and that it can be maintained and expanded with little human effort. The initial solutions together with the case study demonstrate the feasibility and potential of our approach.", "num_citations": "107\n", "authors": ["2072"]}
{"title": "DBLife: A community information management platform for the database research community\n", "abstract": " Community Information Management: There are many communities on the Web. Some are based on common interests, such as communities of movie goers, database researchers, and bioinformaticians, while others are based on a shared purpose, such as organization intranets and online technical support groups. Community members often want to discover, monitor, and query entities and relationships in their community. For example, database researchers might want to know if there is a connection between two given researchers, where a given paper has been cited in the past week, or what of interest has happened in the last 24 hours. Answering such questions often requires retrieving raw, largely unstructured data from multiple sources (eg, home pages, DBLP, mailing lists), then inferring and monitoring semantic information. Examples of such inference and monitoring include recognizing entity mentions (eg,\u201cJ. Gray\u201d,\u201cSIGMOD-04\u201d), deciding if two mentions (eg,\u201cJ. Gray\u201d and \u201cJim Gray\u201d) refer to the same real-world entity, recognizing that a relationship (eg, co-authoring, advising, giving a talk) exists between two entities, detecting new entities (eg, new workshops), and inferring that a relationship (eg, affiliation with a university) has ceased to exist. The above inference and monitoring tasks are well known to be difficult [1, 2, 3, 7, 10]. As online communities proliferate, developing effective solutions to support their information needs becomes increasingly important. We call this problem community information management, or CIM for short.The Cimple Project: To address the CIM problem, we have recently started Cimple, a joint project\u00a0\u2026", "num_citations": "107\n", "authors": ["2072"]}
{"title": "Managing information extraction: state of the art and research directions\n", "abstract": " This tutorial makes the case for developing a unified framework that manages information extraction from unstructured data (focusing in particular on text). We first survey research on information extraction in the database, AI, NLP, IR, and Web communities in recent years. Then we discuss why this is the right time for the database community to actively participate and address the problem of managing information extraction (including in particular the challenges of maintaining and querying the extracted information, and accounting for the imprecision and uncertainty inherent in the extraction process). Finally, we show how interested researchers can take the next step, by pointing to open problems, available datasets, applicable standards, and software tools. We do not assume prior knowledge of text management, NLP, extraction techniques, or machine learning.", "num_citations": "103\n", "authors": ["2072"]}
{"title": "Webiq: Learning from the web to match deep-web query interfaces\n", "abstract": " Integrating Deep Web sources requires highly accurate semantic matches between the attributes of the source query interfaces. These matches are usually established by comparing the similarities of the attributes\u2019 labels and instances. However, attributes on query interfaces often have no or very few data instances. The pervasive lack of instances seriously reduces the accuracy of current matching techniques. To address this problem, we describe WebIQ, a solution that learns from both the Surface Web and the Deep Web to automatically discover instances for interface attributes. WebIQ extends question answering techniques commonly used in the AI community for this purpose. We describe how to incorporate WebIQ into current interface matching systems. Extensive experiments over five realworld domains show the utility ofWebIQ. In particular, the results show that acquired instances help improve matching\u00a0\u2026", "num_citations": "96\n", "authors": ["2072"]}
{"title": "Information extraction challenges in managing unstructured data\n", "abstract": " Over the past few years, we have been trying to build an end-to-end system at Wisconsin to manage unstructured data, using extraction, integration, and user interaction. This paper describes the key information extraction (IE) challenges that we have run into, and sketches our solutions. We discuss in particular developing a declarative IE language, optimizing for this language, generating IE provenance, incorporating user feedback into the IE process, developing a novel wiki-based user interface for feedback, best-effort IE, pushing IE into RDBMSs, and more. Our work suggests that IE in managing unstructured data can open up many interesting research challenges, and that these challenges can greatly benefit from the wealth of work on managing structured data that has been carried out by the database community.", "num_citations": "95\n", "authors": ["2072"]}
{"title": "Toward scalable keyword search over relational data\n", "abstract": " Keyword search (KWS) over relational databases has recently received significant attention. Many solutions and many prototypes have been developed. This task requires addressing many issues, including robustness, accuracy, reliability, and privacy. An emerging issue, however, appears to be performance related: current KWS systems have unpredictable running times. In particular, for certain queries it takes too long to produce answers, and for others the system may even fail to return (e.g., after exhausting memory). In this paper we argue that as today's users have been \"spoiled\" by the performance of Internet search engines, KWS systems should return whatever answers they can produce quickly and then provide users with options for exploring any portion of the answer space not covered by these answers. Our basic idea is to produce answers that can be generated quickly as in today's KWS systems, then\u00a0\u2026", "num_citations": "94\n", "authors": ["2072"]}
{"title": "Object matching for information integration: A profiler-based approach\n", "abstract": " Object matching is a fundamental problem that arises in numerous information integration scenarios. Virtually all existing solutions to this problem have assumed that the objects to be matched share the same set of attributes, and that they can be matched by comparing the similarities of the attributes. We consider the more general problem where the objects can also have disjoint attributes, such as matching tuples that come from relational tables with schemas (age, name) and (name, salary), respectively. We describe PROM, a solution that also exploits the disjoint attributes to improve matching accuracy. In the above example, PROM begins by matching any two given tuples based on the shared attribute name. Then it applies a set of profilers, each of which contains some knowledge about what constitutes a typical person. The profilers examine the tuple pair to see if it can plausibly make up a person. For example, a profiler may state that because the age is 9 and the salary is 200K, the tuples do not make up a person and thus do not match. Profilers can be manually specified by domain experts, learned from training data, transferred from other matching tasks, or constructed from external data. Thus, the PROM approach is distinguished in that it not only can exploit disjoint attributes to improve matching accuracy, but can also reuse knowledge from previous object matching tasks.", "num_citations": "93\n", "authors": ["2072"]}
{"title": "Building data integration systems via mass collaboration\n", "abstract": " Building data integration systems today is largely done by hand, in a very labor-intensive and error-prone process. In this paper we describe a conceptually new solution to this problem: that of mass collaboration. The basic idea is to think about a data integration system as having a finite set of parameters whose values must be set. To build such a system the system administrators construct and deploy a system \u201cshell\u201d, then ask the users to help the system \u201cautomatically converge\u201d to the correct parameter values. This way the enormous burden of system development is lifted from the administrators and spread \u201cthinly\u201d over a multitude of users. We describe our current effort in applying this approach to the problem of schema matching in the context of data integration. We present experiments with both real and synthetic users that show the promise of the approach. Finally we discuss the future work, challenges, and the potential applications of the approach beyond the data integration context.", "num_citations": "86\n", "authors": ["2072"]}
{"title": "Measuring security\n", "abstract": " The field of computer and communications security begs for a foundational science to guide system design and to reveal the safety, security, and possible fragility of the complex systems we depend on today. To achieve this goal, we must devise suitable metrics for objectively comparing and evaluating the security of system designs and organizations.", "num_citations": "84\n", "authors": ["2072"]}
{"title": "Falcon: Scaling up hands-off crowdsourced entity matching to build cloud services\n", "abstract": " Many works have applied crowdsourcing to entity matching (EM). While promising, these approaches are limited in that they often require a developer to be in the loop. As such, it is difficult for an organization to deploy multiple crowdsourced EM solutions, because there are simply not enough developers. To address this problem, a recent work has proposed Corleone, a solution that crowdsources the entireEM workflow, requiring no developers. While promising, Corleone is severely limited in that it does not scale to large tables. We propose Falcon, a solution that scales up the hands-off crowdsourced EM approach of Corleone, using RDBMS-style query execution and optimization over a Hadoop cluster. Specifically, we define a set of operators and develop efficient implementations. We translate a hands-off crowdsourced EM workflow into a plan consisting of these operators, optimize, then execute the plan\u00a0\u2026", "num_citations": "81\n", "authors": ["2072"]}
{"title": "A relational approach to incrementally extracting and querying structure in unstructured data\n", "abstract": " There is a growing consensus that it is desirable to query over the structure implicit in unstructured documents, and that ideally this capability should be provided incrementally. However, there is no consensus about what kind of system should be used to support this kind of incremental capability. We explore using a relational system as the basis for a workbench for extracting and querying structure from unstructured data. As a proof of concept, we applied our relational approach to support structured queries over Wikipedia. We show that the data set is always available for some form of querying, and that as it is processed, users can pose a richer set of structured queries. We also provide examples of how we can incrementally evolve our understanding of the data in the context of the relational workbench.", "num_citations": "78\n", "authors": ["2072"]}
{"title": "The beckman report on database research\n", "abstract": " Every few years a group of database researchers meets to discuss the state of database research, its impact on practice, and important new directions. This report summarizes the discussion and conclusions of the eighth such meeting, held October 14- 15, 2013 in Irvine, California. It observes that Big Data has now become a defining challenge of our time, and that the database research community is uniquely positioned to address it, with enormous opportunities to make transformative impact. To do so, the report recommends significantly more attention to five research areas: scalable big/fast data infrastructures; coping with diversity in the data management landscape; end-to-end processing and understanding of data; cloud services; and managing the diverse roles of people in the data life cycle.", "num_citations": "72\n", "authors": ["2072"]}
{"title": "Efficient decision-theoretic planning: Techniques and empirical analysis\n", "abstract": " This paper discusses techniques for performing efficient decision-theoretic planning. We give an overview of the DRIPS decision-theoretic refinement planning system, which uses abstraction to efficiently identify optimal plans. We present techniques for automatically generating search control information, which can significantly improve the planner's performance. We evaluate the efficiency of DRIPS both with and without the search control rules on a complex medical planning problem and compare its performance to that of a branch-and-bound decision tree algorithm.", "num_citations": "72\n", "authors": ["2072"]}
{"title": "Deep entity matching with pre-trained language models\n", "abstract": " We present Ditto, a novel entity matching system based on pre-trained Transformer-based language models. We fine-tune and cast EM as a sequence-pair classification problem to leverage such models with a simple architecture. Our experiments show that a straightforward application of language models such as BERT, DistilBERT, or RoBERTa pre-trained on large text corpora already significantly improves the matching quality and outperforms previous state-of-the-art (SOTA), by up to 29% of F1 score on benchmark datasets. We also developed three optimization techniques to further improve Ditto's matching capability. Ditto allows domain knowledge to be injected by highlighting important pieces of input information that may be of interest when making matching decisions. Ditto also summarizes strings that are too long so that only the essential information is retained and used for EM. Finally, Ditto adapts a SOTA technique on data augmentation for text to EM to augment the training data with (difficult) examples. This way, Ditto is forced to learn \"harder\" to improve the model's matching capability. The optimizations we developed further boost the performance of Ditto by up to 9.8%. Perhaps more surprisingly, we establish that Ditto can achieve the previous SOTA results with at most half the number of labeled data. Finally, we demonstrate Ditto's effectiveness on a real-world large-scale EM task. On matching two company datasets consisting of 789K and 412K records, Ditto achieves a high F1 score of 96.5%.", "num_citations": "69\n", "authors": ["2072"]}
{"title": "Efficiently incorporating user feedback into information extraction and integration programs\n", "abstract": " Many applications increasingly employ information extraction and integration (IE/II) programs to infer structures from unstructured data. Automatic IE/II are inherently imprecise. Hence such programs often make many IE/II mistakes, and thus can significantly benefit from user feedback. Today, however, there is no good way to automatically provide and process such feedback. When finding an IE/II mistake, users often must alert the developer team (eg, via email or Web form) about the mistake, and then wait for the team to manually examine the program internals to locate and fix the mistake, a slow, error-prone, and frustrating process.", "num_citations": "69\n", "authors": ["2072"]}
{"title": "Toward best-effort information extraction\n", "abstract": " Current approaches to develop information extraction (IE) programs have largely focused on producing precise IE results. As such, they suffer from three major limitations. First, it is often difficult to execute partially specified IE programs and obtain meaningful results, thereby producing a long\" debug loop\". Second, it often takes a long time before we can obtain the first meaningful result (by finishing and running a precise IE program), thereby rendering these approaches impractical for time-sensitive IE applications. Finally, by trying to write precise IE programs we may also waste a significant amount of effort, because an approximate result--one that can be produced quickly--may already be satisfactory in many IE settings.", "num_citations": "62\n", "authors": ["2072"]}
{"title": "MetaSRA: normalized human sample-specific metadata for the Sequence Read Archive\n", "abstract": " Motivation           The NCBI\u2019s Sequence Read Archive (SRA) promises great biological insight if one could analyze the data in the aggregate; however, the data remain largely underutilized, in part, due to the poor structure of the metadata associated with each sample. The rules governing submissions to the SRA do not dictate a standardized set of terms that should be used to describe the biological samples from which the sequencing data are derived. As a result, the metadata include many synonyms, spelling variants and references to outside sources of information. Furthermore, manual annotation of the data remains intractable due to the large number of samples in the archive. For these reasons, it has been difficult to perform large-scale analyses that study the relationships between biomolecular processes and phenotype across diverse diseases, tissues and cell types present in the SRA\u00a0\u2026", "num_citations": "61\n", "authors": ["2072"]}
{"title": "Efficiently ordering query plans for data integration\n", "abstract": " The goal of a data integration system is to provide a uniform interface to a multitude of data sources. Given a user query formulated in this interface, the system translates it into a set of query plans. Each plan is a query formulated over the data sources, and specifies a way to access sources and combine data to answer the user query. In practice, when the number of sources is large, a data-integration system must generate and execute many query plans with significantly varying utilities. Hence, it is crucial that the system finds the best plans efficiently and executes them first, to guarantee acceptable time to and the quality of the first answers. We describe efficient solutions to this problem. First, we formally define the problem of ordering query plans. Second, we identify several interesting structural properties of the problem and describe three ordering algorithms that exploit these properties. Finally, we describe\u00a0\u2026", "num_citations": "61\n", "authors": ["2072"]}
{"title": "Optimizing SQL queries over text databases\n", "abstract": " Text documents often embed data that is structured in nature, and we can expose this structured data using information extraction technology. By processing a text database with information extraction systems, we can materialize a variety of structured \"relations,\" over which we can then issue regular SQL queries. A key challenge to process SQL queries in this text-based scenario is efficiency: information extraction is time-consuming, so query processing strategies should minimize the number of documents that they process. Another key challenge is result quality: in the traditional relational world, all correct execution strategies for a SQL query produce the same (correct) result; in contrast, a SQL query execution over a text database might produce answers that are not fully accurate or complete, for a number of reasons. To address these challenges, we study a family of select-project-join SQL queries over text\u00a0\u2026", "num_citations": "60\n", "authors": ["2072"]}
{"title": "Efficient information extraction over evolving text data\n", "abstract": " Most current information extraction (IE) approaches have considered only static text corpora, over which we typically have to apply IE only once. Many real-world text corpora however are dynamic. They evolve over time, and to keep extracted information up to date, we often must apply IE repeatedly, to consecutive corpus snapshots. We describe Cyclex, an approach that efficiently executes such repeated IE, by recycling previous IE efforts. Specifically, given a current corpus snapshot U, Cyclex identifies text portions of U that also appear in the previous corpus snapshot V. Since Cyclex has already executed IE over V, it can now recycle the IE results of these parts, by combining these results with the results of executing IE over the remaining parts of U, to produce the complete IE results for U. Realizing Cyclex raises many challenges, including modeling information extractors, exploring the trade-off between runtime\u00a0\u2026", "num_citations": "58\n", "authors": ["2072"]}
{"title": "Crowdsourcing applications and platforms: A data management perspective\n", "abstract": " Over the past decade, crowdsourcing has emerged as a major problem-solving and data-gathering paradigm on the World-Wide Web. Well-known examples of crowdsourcing include Wikipedia, Linux, Yahoo! Answers, YouTube, Mechanical Turk-based applications, and much effort is being directed toward developing many more.", "num_citations": "52\n", "authors": ["2072"]}
{"title": "Profile-based object matching for information integration\n", "abstract": " Traditional object-matching methods rely on similarities among shared attributes. Profile-based object matching builds on this approach but also correlates disjoint attributes to improve matching accuracy. To illustrate the PROM approach, we use two relational tables: one contains information about movies, the other about movie reviews.", "num_citations": "52\n", "authors": ["2072"]}
{"title": "Olap over imprecise data with domain constraints\n", "abstract": " Several recent works have focused on OLAP over imprecise data, where  each fact can be a region, instead of a point, in a multi-dimensional  space. They have provided a multiple-world semantics for such data,  and developed efficient solutions to answer OLAP aggregation queries  over the imprecise facts. These solutions however assume that the  imprecise facts can be interpreted {\\em independently\\/} of one another, a  key assumption that is often violated in practice. Indeed, imprecise  facts in real-world applications are often correlated, and such  correlations can be captured as domain integrity constraints (e.g.,  repairs with the same customer names and models took place in the same  city, or a text span can refer to a person or a city, but not both).    In this paper we provide a solution to answer OLAP aggregation queries  over imprecise data, in the presence of such domain constraints. We first  describe a relatively simple yet powerful constraint language, and define  what it means to take into account such constraints in query answering.  Next, we prove that OLAP queries can be answered efficiently given a  database  of fact marginals. We then exploit the regularities in the  constraint space (captured in a constraint hypergraph) and the fact space  to efficiently construct D*. Extensive experiments over real-world and  synthetic data demonstrate the effectiveness of our approach.", "num_citations": "49\n", "authors": ["2072"]}
{"title": "Modeling entity evolution for temporal record matching\n", "abstract": " Temporal record matching recognizes that if the entities represented by the records change over time, approaches that use temporal information may do better than approaches that do not. Any such temporal matching method relies at its heart on a temporal model that captures information about how entities evolve. In their pioneering work, Li {\\it et al.} used an efficiently computable model that simply tries to predict if an attribute is expected to change over a given time interval. In our work, we propose and evaluate a more detailed model that focuses on the probability that a given attribute value reappears over time. The intuition here is that an entity might change its attribute value in the way that is dependent on its past values. In addition, our model considers sets of records (rather than simply pairs of records) to improve robustness and accuracy. Experimental results show that the resulting approach improves both\u00a0\u2026", "num_citations": "48\n", "authors": ["2072"]}
{"title": "Source-aware entity matching: A compositional approach\n", "abstract": " Entity matching (a.k.a. record linkage) plays a crucial role in integrating multiple data sources, and numerous matching solutions have been developed. However, the solutions have largely exploited only information available in the mentions and employed a single matching technique. We show how to exploit information about data sources to significantly improve matching accuracy. In particular, we observe that different sources often vary substantially in their level of semantic ambiguity, thus requiring different matching techniques. In addition, it is often beneficial to group and match mentions in related sources first, before considering other sources. These observations lead to a large space of matching strategies, analogous to the space of query evaluation plans considered by a relational optimizer. We propose viewing entity matching as a composition of basic steps into a \"match execution plan\". We analyze\u00a0\u2026", "num_citations": "46\n", "authors": ["2072"]}
{"title": "Geometric foundations for interval-based probabilities\n", "abstract": " The need to reason with imprecise probabilities arises in a wealth of situations ranging from pooling of knowledge from multiple experts to abstraction-based probabilistic planning. Researchers have typically represented imprecise probabilities using intervals and have developed a wide array of different techniques to suit their particular requirements. In this paper we provide an analysis of some of the central issues in representing and reasoning with interval probabilities. At the focus of our analysis is the probability cross-product operator and its interval generalization, the cc-operator. We perform an extensive study of these operators relative to manipulation of sets of probability distributions. This study provides insight into the sources of the strengths and weaknesses of various approaches to handling probability intervals. We demonstrate the application of our results to the problems of inference in interval\u00a0\u2026", "num_citations": "44\n", "authors": ["2072"]}
{"title": "The seattle report on database research\n", "abstract": " Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.", "num_citations": "43\n", "authors": ["2072"]}
{"title": "The case for a structured approach to managing unstructured data\n", "abstract": " The challenge of managing unstructured data represents perhaps the largest data management opportunity for our community since managing relational data. And yet we are risking letting this opportunity go by, ceding the playing field to other players, ranging from communities such as AI, KDD, IR, Web, and Semantic Web, to industrial players such as Google, Yahoo, and Microsoft. In this essay we explore what we can do to improve upon this situation. Drawing on the lessons learned while managing relational data, we outline a structured approach to managing unstructured data. We conclude by discussing the potential implications of this approach to managing other kinds of non-relational data, and to the identify of our field.", "num_citations": "43\n", "authors": ["2072"]}
{"title": "Bootstrapping domain ontology for semantic web services from source web sites\n", "abstract": " The vision of Semantic Web services promises a network of interoperable Web services over different sources. A major challenge to the realization of this vision is the lack of automated means of acquiring domain ontologies necessary for marking up the Web services. In this paper, we propose the DeepMiner system which learns domain ontologies from the source Web sites. Given a set of sources in a domain of interest, DeepMiner first learns a base ontology from their query interfaces. It then grows the current ontology by probing the sources and discovering additional concepts and instances from the data pages retrieved from the sources. We have evaluated DeepMiner in several real-world domains. Preliminary results indicate that DeepMiner discovers concepts and instances with high accuracy.", "num_citations": "43\n", "authors": ["2072"]}
{"title": "Data Curation with Deep Learning.\n", "abstract": " Data curation\u2013the process of discovering, integrating, and cleaning data\u2013is one of the oldest, hardest, yet inevitable data management problems. Despite decades of efforts from both researchers and practitioners, it is still one of the most time consuming and least enjoyable work of data scientists. In most organizations, data curation plays an important role so as to fully unlock the value of big data. Unfortunately, the current solutions are not keeping up with the ever-changing data ecosystem, because they often require substantially high human cost. Meanwhile, deep learning is making strides in achieving remarkable successes in multiple areas, such as image recognition, natural language processing, and speech recognition. In this vision paper, we explore how some of the fundamental innovations in deep learning could be leveraged to improve existing data curation solutions and to help build new ones. We identify interesting research opportunities and dispel common myths. We hope that the synthesis of these important domains will unleash a series of research activities that will lead to significantly improved solutions for many data curation tasks.", "num_citations": "41\n", "authors": ["2072"]}
{"title": "Introduction to the special issue on managing information extraction\n", "abstract": " The field of information extraction (IE) focuses on extracting structured data, such as person names and organizations, from unstructured text. This field has had a long history. It attracted steady attention in the 80s and 90s, largely in the AI community. In the past decade, however, spurred on by the explosion of unstructured data on the World-Wide Web, this attention has turned into a torrent, gathering the efforts of researchers in the AI, DB, WWW, KDD, Semantic Web and IR communities. New IE problems have been identified, new IE techniques developed, many workshops organized, tutorials presented, companies founded, academic and industrial products deployed, and opensource prototypes developed (eg,[5, 4, 3, 1, 2]; see [5] for the latest survey). The next few years are poised to witness even more accelerated activities in these areas. It is against this vibrant backdrop that we assemble this special issue. Our objective is threefold. First, we want to provide a glimpse into the current state of the field, highlighting in particular the wide range of IE problems. Second, we want to show that many IE problems can significantly benefit from the wealth of work on managing structured data in the database community. We believe therefore that our community can make a substantial contribution to the IE field. Finally, we hope that examining IE problems can in turn help us gain valuable insights into managing data in this Internet-centric world, a long-term goal of our community.", "num_citations": "40\n", "authors": ["2072"]}
{"title": "Merging interface schemas on the deep web via clustering aggregation\n", "abstract": " We consider the problem of integrating a large number of interface schemas over the deep Web, The scale of the problem and the diversity of the sources present serious challenges to the conventional manual or rule-based approaches to schema integration. To address these challenges, we propose a novel formulation of schema integration as an optimization problem, with the objective of maximally satisfying the constraints given by individual schemas. Since the optimization problem can be shown to be NP-complete, we develop a novel approximation algorithm LMax, which builds the unified schema via recursive applications of clustering aggregation. We further extend LMax to handle the irregularities frequently occurring among the interface schemas. Extensive evaluation on real-world data sets shows the effectiveness of our approach.", "num_citations": "40\n", "authors": ["2072"]}
{"title": "The magazine archive includes every article published in Communications of the ACM for over the past 50 years.\n", "abstract": " Iincreasing diversity in computing is very important for multiple reasons. First, there is the issue of the work force. According to the US Census, Blacks and Hispanics were approximately 12% and 16% of the US residents in 2010, respectively. According to the 2008 Census Bureau projections, Hispanics, African-Americans, and Native Americans/Alaska Natives are projected to account for 47% of the US population by 2050. Second, there is the issue of having diverse perspectives involved in the design of products thereby having more robust end products on the market. Lastly, there is the issue of inclusionthat the field be representative of society.Given the importance of increasing diversity, it follows that trends about the demographics of students in the computing field are necessary to determine what programs and policies are needed to promote diversity. To this end, we present different sources for data on minorities and discuss the importance of having multiple sources to get a comprehensive view. In addition, we begin a discussion about what the data indicates with respect to minorities and the difficulties in the data collection process for people with disabilities. In particular, the focus is on Blacks/African Americans, Hispanics, Native Americans, and people with disabilities. The graphs shown in the accompanying figures were developed by the Center for Minorities and People with Disabilities in IT (CMD-IT). a", "num_citations": "38\n", "authors": ["2072"]}
{"title": "BigGorilla: An Open-Source Ecosystem for Data Preparation and Integration.\n", "abstract": " We present BIGGORILLA, an open-source resource for data scientists who need data preparation and integration tools, and the vision underlying the project. We then describe four packages that we contributed to BIGGORILLA: KOKO (an information extraction tool), FLEXMATCHER (a schema matching tool), MAGELLAN and DEEPMATCHER (two entity matching tools). We hope that as more software packages are added to BIGGORILLA, it will become a one-stop resource for both researchers and industry practitioners, and will enable our community to advance the state of the art at a faster pace.", "num_citations": "36\n", "authors": ["2072"]}
{"title": "SQL queries over unstructured text databases\n", "abstract": " Text documents often embed data that is structured in nature. By processing a text database with information extraction systems, we can define a variety of structured \"relations\" over which we can then issue SQL queries. Processing SQL queries in this text-based scenario presents multiple challenges. One key challenge is efficiency: information extraction is a time-consuming process, so query processing strategies should pick efficient extraction systems whenever possible, and also minimize the number of documents that they process. Another key challenge is result quality: extraction systems might output erroneous information or miss information that they should capture; also, efficiency-related query processing decisions (e.g., to avoid processing large numbers of useless documents) may compromise result completeness. To address these challenges, we characterize SQL query processing strategies in terms\u00a0\u2026", "num_citations": "36\n", "authors": ["2072"]}
{"title": "Crowds, clouds, and algorithms: exploring the human side of\" big data\" applications\n", "abstract": " The creation, collection, analysis, curation, and dissemination of data have become profoundly democratized. Social networks spanning 100\u2019s of millions of users enable instantaneous discussion, debate, and information sharing [8]. Streams of tweets, blogs, photos, and videos identify breaking events faster and in more detail than ever before [10]. Global, ad hoc collaborations addressing scientific, commercial, political, and even mathematical problems make progress where individual investigators or small groups cannot [6, 11, 12, 13, 14].", "num_citations": "34\n", "authors": ["2072"]}
{"title": "Tracking entities in the dynamic world: A fast algorithm for matching temporal records\n", "abstract": " Identifying records referring to the same real world entity over time enables longitudinal data analysis. However, difficulties arise from the dynamic nature of the world: the entities described by a temporal data set often evolve their states over time. While the state of the art approach to temporal entity matching achieves high accuracy, this approach is computationally expensive and cannot handle large data sets. In this paper, we present an approach that achieves equivalent matching accuracy but takes far less time. Our key insight is \"static first, dynamic second.\" Our approach first runs an evidence-collection pass, grouping records without considering the possibility of entity evolution, as if the world were \"static.\" Then, it merges clusters from the initial grouping by determining whether an entity might evolve from the state described in one cluster to the state described in another cluster. This intuitively reduces a\u00a0\u2026", "num_citations": "33\n", "authors": ["2072"]}
{"title": "Building community wikipedias: A machine-human partnership approach\n", "abstract": " The rapid growth of Web communities has motivated many solutions for building community data portals. These solutions follow roughly two approaches. The first approach (e.g., Libra, Citeseer, Cimple) employs semi-automatic methods to extract and integrate data from a multitude of data sources. The second approach (e.g., Wikipedia, Intellipedia) deploys an initial portal in wiki format, then invites community members to revise and add material. In this paper we consider combining the above two approaches to building community portals. The new hybrid machine-human approach brings significant benefits. It can achieve broader and deeper coverage, provide more incentives for users to contribute, and keep the portal more up-to-date with less user effort. In a sense, it enables building \"community wikipedias\", backed by an underlying structured database that is continuously updated using automatic techniques\u00a0\u2026", "num_citations": "33\n", "authors": ["2072"]}
{"title": "Modeling and extracting deep-web query interfaces\n", "abstract": " Interface modeling & extraction is a fundamental step in building a uniform query interface to a multitude of databases on the Web. Existing solutions are limited in that they assume interfaces are flat and thus ignore the inherent structure of interfaces, which then seriously hampers the effectiveness of interface integration. To address this limitation, in this chapter, we model an interface with a hierarchical schema (e.g., an ordered-tree of attributes). We describe ExQ, a novel schema extraction system with two distinct features. First, ExQ discovers the structure of an interface based on its visual representation via spatial clustering. Second, ExQ annotates the discovered schema with labels from the interface by imitating the human-annotation process. ExQ has been extensively evaluated with real-world query interfaces in five different domains and the results show that ExQ achieves above 90% accuracy rate in\u00a0\u2026", "num_citations": "32\n", "authors": ["2072"]}
{"title": "Abstracting probabilistic actions\n", "abstract": " This paper discusses the problem of abstracting conditional probabilistic actions. We identify two distinct types of abstraction: intra-action abstraction and inter-action abstraction. We define what it means for the abstraction of an action to be correct and then derive two methods of intra-action abstraction and two methods of inter-action abstraction which are correct according to this criterion. We illustrate the developed techniques by applying them to actions described with the temporal action representation used in the DRIPS decision-theoretic planner and we describe how the planner uses abstraction to reduce the complexity of planning.", "num_citations": "30\n", "authors": ["2072"]}
{"title": "Why big data industrial systems need rules and what we can do about it\n", "abstract": " Big Data industrial systems that address problems such as classification, information extraction, and entity matching very commonly use hand-crafted rules. Today, however, little is understood about the usage of such rules. In this paper we explore this issue. We discuss how these systems differ from those considered in academia. We describe default solutions, their limitations, and reasons for using rules. We show examples of extensive rule usage in industry. Contrary to popular perceptions, we show that there is a rich set of research challenges in rule generation, evaluation, execution, optimization, and maintenance. We discuss ongoing work at WalmartLabs and UW-Madison that illustrate these challenges. Our main conclusions are (1) using rules (together with techniques such as learning and crowdsourcing) is fundamental to building semantics-intensive Big Data systems, and (2) it is increasingly critical to\u00a0\u2026", "num_citations": "29\n", "authors": ["2072"]}
{"title": "Decision-theoretic refinement planning in medical decision making: Management of acute deep venous thrombosis\n", "abstract": " Decision-theoretic refinement planning is a new technique for finding optimal courses of action. The authors sought to determine whether this technique could identify optimal strategies for medical diagnosis and therapy. An existing model of acute deep venous thrombosis of the lower extremities was encoded for analysis by the decision-theoretic refinement planning system (DRIPS). The encoding represented 6,206 possible plans. The DRIPS planner used artificial intelligence techniques to elimmate 5,150 plans (83%) from consideration without examining them explicitly. The DRIPS system identified the five strategies that minimized cost and mortality. The authors conclude that decision- theoretic planning is useful for examining large medical-decision problems. Key words: decision-theoretic refinement planning; diagnosis; treatment; DRIPS ; acute deep venous thrombosis ; artificial intelligence. (Med Decis\u00a0\u2026", "num_citations": "28\n", "authors": ["2072"]}
{"title": "Toward entity retrieval over structured and text data\n", "abstract": " Many real-world applications increasingly involve both structured data and text. Hence, managing both in an efficient and integrated manner has received much attention from both the IR and database communities. To date, however, little research has been devoted to semantic issues in the integration of text and data. In this paper we introduced a problem in this realm: entity retrieval. Given data fragments that describe various aspects of a real-world entity, find all other data fragments as well as text documents that describe that same entity. As such, entity retrieval is a novel retrieval problem, which differs from both regular text retrieval and database search in that it explicitly requires matching information at the semantic level; matching syntactically as done in the current search engines and relational databases would be inherently non-optimal. We define entity retrieval and conduct a case study of retrieving information about a researcher from both the Web and a bibliographic database (DBLP). We propose several methods for exploiting the structured information in the database to improve entity retrieval over the text collection. Specifically, we present a query expansion mechanism based on extracted information from structured data. Experiment results show that selectively using more structured information to expand the text query improves entity retrieval performance on text. We conclude the paper with future research directions for entity retrieval.", "num_citations": "27\n", "authors": ["2072"]}
{"title": "Human-in-the-loop challenges for entity matching: A midterm report\n", "abstract": " Entity matching (EM) has been a long-standing challenge in data management. In the past few years we have started two major projects on EM (Magellan and Corleone/Falcon). These projects have raised many human-in-the-loop (HIL) challenges. In this paper we discuss these challenges. In particular, we show how these challenges forced us to revise our solution architecture, from a typical RDBMS-style architecture to a very human-centric one, in which human users are first-class objects driving the EM process, using tools at pain-point places. We discuss how such solution architectures can be viewed as combining\" tools in the loop\" with\" human in the loop\". Finally, we discuss lessons learned which can potentially be applied to other problem settings. We also hope that more researchers will investigate EM, as it can be a rich\" playground\" for HIL research.", "num_citations": "26\n", "authors": ["2072"]}
{"title": "Databases and Web 2.0 panel at VLDB 2007\n", "abstract": " Web 2.0 refers to a set of technologies that enables indviduals to create and share content on the Web. The types of content that are shared on Web 2.0 are quite varied and include photos and videos (e.g., Flickr, YouTube), encyclopedic knowledge (e.g., Wikipedia), the blogosphere, social book-marking and even structured data (e.g., Swivel, Many-eyes). One of the important distinguishing features of Web 2.0 is the creation of communities of users. Online communities such as LinkedIn, Friendster, Facebook, MySpace and Orkut attract millions of users who build networks of their contacts and utilize them for social and professional purposes. In a nutshell, Web 2.0 offers an architecture of participation and democracy that encourages users to add value to the application as they use it.", "num_citations": "26\n", "authors": ["2072"]}
{"title": "Analyzing and revising mediated schemas to improve their matchability\n", "abstract": " Data integration systems often provide a uniform interface, called a mediated schema, to a multitude of disparate data sources. To answer user queries posed over the mediated schema, such systems employ a set of semantic matches be-tween this schema and the local schemas of the data sources. Finding such matches is well known to be difficult. Hence much work has focused on developing semi-automatic techniques to efficiently find the matches. In this paper, how-ever, we consider the complementary problem of improving the mediated schema, to make finding such matches easier. Specifically, a mediated schema S will typically be matched with many source schemas. Thus, can the developer of S analyze and revise S in a way that preserves Ss semantics, and yet makes it easier to match with in the futureDescriptors:", "num_citations": "26\n", "authors": ["2072"]}
{"title": "Optimizing complex extraction programs over evolving text data\n", "abstract": " Most information extraction (IE) approaches have considered only static text corpora, over which we apply IE only once. Many real-world text corpora however are dynamic. They evolve over time, and so to keep extracted information up to date we often must apply IE repeatedly, to consecutive corpus snapshots. Applying IE from scratch to each snapshot can take a lot of time. To avoid doing this, we have recently developed Cyclex, a system that recycles previous IE results to speed up IE over subsequent corpus snapshots. Cyclex clearly demonstrated the promise of the recycling idea. The work itself however is limited in that it considers only IE programs that contain a single IE``blackbox.''In practice, many IE programs are far more complex, containing multiple IE blackboxes connected in a compositional``workflow.''", "num_citations": "22\n", "authors": ["2072"]}
{"title": "Human-in-the-loop data analysis: a personal perspective\n", "abstract": " In the past few years human-in-the-loop data analysis (HILDA) has received significant growing attention. Most HILDA works have focused on concrete problems. In this paper I take a step back and discuss several\" big picture\" questions regarding HILDA. First, I discuss problems that I believe should fall under the scope of the field, including some that have received little attention, such as fostering user communities that develop data repositories and tools. Next, I discuss important aspects in developing HILDA solutions that I believe should receive more attention. These include solving problems that real users care about, developing how-to guides to users, building end-to-end systems (such as extending the\" Pandas system\"), developing challenges and benchmarks, and developing a theory of human data interaction. Finally, I speculate about the future of the field, and discuss the dangers it can face, given that\u00a0\u2026", "num_citations": "19\n", "authors": ["2072"]}
{"title": "Decision-theoretic refinement planning: Principles and application\n", "abstract": " We present a general theory of action abstraction for reducing the complexity of decision-theoretic planning. We develop projection rules for abstract actions and prove our abstraction techniques to be correct. We present a planning algorithm that uses the abstraction theory to e ciently explore the space of possible plans by eliminating suboptimal classes of plans without explicitly examining all plans in those classes. An instance of the algorithm has been implemented as the drips decision-theoretic re nement planning system. We apply the planner to the problem of selecting the optimal test/treat strategy for managing patients suspected of having deep-vein thrombosis of the lower extremities. We show that drips signi cantly outperforms a standard branch-and-bound decision tree evaluation algorithm on this domain.", "num_citations": "19\n", "authors": ["2072"]}
{"title": "Modeling Probabilistic Actions for Practical Decision-Theoretic Planning.\n", "abstract": " Most existing decision-theoretic planners represent uncertainty about the state of the world with a precisely specified probability distribution over world states. This representation is not expressive enough to model many interesting classes of practical planning problems, and renders inapplicable some abstractionbaaed planning approaches. In this paper we propose aa a remedy a more general world and action model with a well-founded semantics based on probability intervals. We introduce the concept of interval mass assigment. Unlike mass assignments, which assign a probability mass to each set of states, interval mass assignments assign a probability interval to each set of states and are more expressive. Interval mass assignments are interpreted aa representing sets of probability distributions and are used in our framework to represent the uncertainty about the state of the world. Within this representation, we present a projection rule and a method for computing a plan\u2019s expected utility. We compare our approach with existing probability-interval approaches. We provide complexity results and empirical evidence which suggest evaluating plans (projecting plans and computing the expected utility) in our framework is efficient, and the action model is applicable in real-world planning do-mains.", "num_citations": "18\n", "authors": ["2072"]}
{"title": "Entity matching meets data science: A progress report from the magellan project\n", "abstract": " Entity matching (EM) finds data instances that refer to the same real-world entity. In 2015, we started the Magellan project at UW-Madison, joint with industrial partners, to build EM systems. Most current EM systems are stand-alone monoliths. In contrast, Magellan borrows ideas from the field of data science (DS), to build a new kind of EM systems, which is an ecosystem of interoperable tools.\\em This paper provides a progress report on the past 3.5 years of Magellan, focusing on the system aspects and on how ideas from the field of data science have been adapted to the EM context. We argue why EM can be viewed as a special class of DS problems, and thus can benefit from system building ideas in DS. We discuss how these ideas have been adapted to build\\pymatcher\\and\\cloudmatcher, EM tools for power users and lay users. These tools have been successfully used in 21 EM tasks at 12 companies and\u00a0\u2026", "num_citations": "15\n", "authors": ["2072"]}
{"title": "Smurf: Self-service string matching using random forests\n", "abstract": " We argue that more attention should be devoted to developing self-service string matching (SM) solutions, which lay users can easily use. We show that Falcon, a self-service entity matching (EM) solution, can be applied to SM and is more accurate than current self-service SM solutions. However, Falcon often asks lay users to label many string pairs (eg, 770-1050 in our experiments). This is expensive, can significantly compound labeling mistakes, and takes a long time. We developed Smurf, a self-service SM solution that reduces the labeling effort by 43-76%, yet achieves comparable F1 accuracy. The key to make Smurf possible is a novel solution to efficiently execute a random forest (that Smurf learns via active learning with the lay user) over two sets of strings. This solution uses RDBMS-style plan optimization to reuse computations across the trees in the forest. As such, Smurf significantly advances self-service SM and raises interesting future directions for self-service EM and scalable random forest execution over structured data.", "num_citations": "15\n", "authors": ["2072"]}
{"title": "CloudMatcher: A hands-off cloud/crowd service for entity matching\n", "abstract": " As data science applications proliferate, more and more lay users must perform data integration (DI) tasks, which used to be done by sophisticated CS developers. Thus, it is increasingly critical that we develop hands-off DI services, which lay users can use to perform such tasks without asking for help from developers. We propose to demonstrate such a service. Specifically, we will demonstrate CloudMatcher, a hands-off cloud/crowd service for entity matching (EM). To use CloudMatcher to match two tables, a lay user only needs to upload them to the CloudMatcher\u2019s Web page then iteratively label a set of tuple pairs as match/no-match. Alternatively, the user can enlist a crowd of workers to label the pairs. In either case, the lay user can easily perform EM end-to-end without having to involve any developers. Cloud-Matcher has been used in several domain science projects at UW-Madison and at several organizations, and is scheduled to be deployed in a large company in Summer 2018. In the demonstration we will show how easy it is for lay users to perform EM (either via interactive labeling or crowdsourcing), how users can easily create and experiment with a range of EM workflows, and how CloudMatcher can scale to many concurrent users and large datasets.", "num_citations": "15\n", "authors": ["2072"]}
{"title": "Toward a system building agenda for data integration\n", "abstract": " In this paper we argue that the data management community should devote far more effort to building data integration (DI) systems, in order to truly advance the field. Toward this goal, we make three contributions. First, we draw on our recent industrial experience to discuss the limitations of current DI systems. Second, we propose an agenda to build a new kind of DI systems to address these limitations. These systems guide users through the DI workflow, step by step. They provide tools to address the \"pain points\" of the steps, and tools are built on top of the Python data science and Big Data ecosystem (PyData). We discuss how to foster an ecosystem of such tools within PyData, then use it to build DI systems for collaborative/cloud/crowd/lay user settings. Finally, we discuss ongoing work at Wisconsin, which suggests that these DI systems are highly promising and building them raises many interesting research challenges.", "num_citations": "15\n", "authors": ["2072"]}
{"title": "Sound abstraction of probabilistic actions in the constraint mass assignment framework\n", "abstract": " This paper provides a formal and practical framework for sound abstraction of probabilistic actions. We start by precisely defining the concept of sound abstraction within the context of finite-horizon planning (where each plan is a finite sequence of actions). Next we show that such abstraction cannot be performed within the traditional probabilistic action representation, which models a world with a single probability distribution over the state space. We then present the constraint mass assignment representation, which models the world with a set of probability distributions and is a generalization of mass assignment representations. Within this framework, we present sound abstraction procedures for three types of action abstraction. We end the paper with discussions and related work on sound and approximate abstraction. We give pointers to papers in which we discuss other sound abstraction-related issues, including applications, estimating loss due to abstraction, and automatically generating abstraction hierarchies.", "num_citations": "15\n", "authors": ["2072"]}
{"title": "bigNN: An open-source big data toolkit focused on biomedical sentence classification\n", "abstract": " Every single day, a massive amount of text data is generated by different medical data sources, such as scientific literature, medical web pages, health-related social media, clinical notes, and drug reviews. Processing this wealth of data is indeed a daunting task, and it forces us to adopt smart and scalable computational strategies, including machine intelligence, big data analytics, and distributed architecture. In this contribution, we designed and developed an open-source big data neural network toolkit, namely bigNN which tackles the problem of large-scale biomedical text classification in an efficient fashion, facilitating fast prototyping and reproducible text analytics researches. bigNN scales up a word2vec-based neural network model over Apache Spark 2.10 and Hadoop Distributed File System (HDFS) 2.7.3, allowing for more efficient big data sentence classification. The toolkit supports big data computing\u00a0\u2026", "num_citations": "14\n", "authors": ["2072"]}
{"title": "On Debugging Non-Answers in Keyword Search Systems.\n", "abstract": " Handling non-answers is desirable in information retrieval systems. Current e-commerce websites usually try to suppress the somewhat dreaded message that no results have been found. Possible solutions include, for example, augmenting the data with synonyms and common misspellings based on query logs. Nonetheless, this is only achievable if we can know the cause of the non-answers. Under the hood, most e-commerce data sits in some structured format. Debugging non-answers in the underlying KWS-S systems is therefore not trivial\u2014non-answers in a KWS-S system could be a problem of the data (eg, absence of some keywords), the schema (eg, missing key-foreign-key joins), or due to empty join results from one of possibly several joins in the generated SQL queries. So far, we are unaware of any previous work that explores how to enable developers to debug non-answers in a KWS-S system. In this paper, we take a first step towards this direction by proposing a KWS-S system that can expose non-answers to the developers. Our system presents the developers with the maximal nonempty sub-queries that represent the frontier cause of the non-answers. We outline the challenges in building such a system and propose a lattice structure for efficient exploration of the non-answer query space. We also evaluate our proposed mechanisms over a real world dataset to demonstrate their feasibility.", "num_citations": "14\n", "authors": ["2072"]}
{"title": "Learning mappings between data schemas\n", "abstract": " To build a data-integration system, the application designer must specify a mediated schema and supply the descriptions of data sources. A source description contains a source schema that describes the content of the source, and a mapping between the corresponding elements of the source schema and the mediated schema. Manually constructing these mappings is both laborintensive and error-prone, and has proven to be a major bottleneck in deploying large-scale data integration systems in practice. In this paper we report on our initial work toward automatically learning mappings between source schemas and the mediated schema. Specifically, we investigate finding one-to-one mappings for the leaf elements of source schemas. We describe LSD, a system that automatically finds such mappings. LSD consults a set of learner modules\u2013where each module looks at the problem from a different perspective, then combines the predictions of the modules using a metalearner. Learner modules draw knowledge from the World-Wide Web, as well as on ideas from machine learning and information retrieval. We report on experimental results of applying LSD to five sources in the real-estate domain.", "num_citations": "13\n", "authors": ["2072"]}
{"title": "Cloudmatcher: A cloud/crowd service for entity matching\n", "abstract": " Entity matching (EM) nds disparate data instances that refer to the same real-world entity. EM is critical in health informatics, and will become even more so in the age of Big Data and data science. Many EM systems have been developed. In this paper, we rst discuss why it is still very di cult for domain scientists to use such EM systems. We then describe CloudMatcher, a cloud/crowd service for EM that we have been building. CloudMatcher aims to be a fast, easy-to-use, scalable, and highly available EM service on the Web. We motivate CloudMatcher then describe its design and implementation. Next, we describe its deployment in the past six months, providing a detailed analysis of its performance over four representative datasets. Finally, we discuss lessons learned.", "num_citations": "12\n", "authors": ["2072"]}