{"title": "INTERNET OF THINGS\u2014 ROBUSTNESS AND RELIABILITY\n", "abstract": " Internet of Things (IoT) has a disruptive impact on variety of application domains like healthcare, logistics, transportation, etc., where network-enabled embedded devices work proactively and collaboratively to solve a complex decision-making problem in real time. Many of these applications are business critical, and require the underlying technology to be dependable, that is, it must deliver its service even in the presence of failures. In this chapter we discuss the notion of reliability and recovery oriented systems in general and then explain why this is important for an IoT-based system. We discuss various failure scenarios and reliability challenges that exist at different layers of an IoT-based system. Next, we discuss various failure-prevention and fault-tolerance approaches to make an IoT-based system robust. Energy consumption by IoT devices plays a major role in the overall system reliability. In this chapter we\u00a0\u2026", "num_citations": "355\n", "authors": ["1213"]}
{"title": "Mining business topics in source code using latent dirichlet allocation\n", "abstract": " One of the difficulties in maintaining a large software system is the absence of documented business domain topics and correlation between these domain topics and source code. Without such a correlation, people without any prior application knowledge would find it hard to comprehend the functionality of the system. Latent Dirichlet Allocation (LDA), a statistical model, has emerged as a popular technique for discovering topics in large text document corpus. But its applicability in extracting business domain topics from source code has not been explored so far. This paper investigates LDA in the context of comprehending large software systems and proposes a human assisted approachbased on LDA for extracting domain topics from source code. This method has been applied on a number of open source and proprietary systems. Preliminary results indicate that LDA is able to identify some of the domain topics\u00a0\u2026", "num_citations": "235\n", "authors": ["1213"]}
{"title": "API-based and information-theoretic metrics for measuring the quality of software modularization\n", "abstract": " We present in this paper a new set of metrics that measure the quality of modularization of a non-object-oriented software system. We have proposed a set of design principles to capture the notion of modularity and defined metrics centered around these principles. These metrics characterize the software from a variety of perspectives: structural, architectural, and notions such as the similarity of purpose and commonality of goals. (By structural, we are referring to intermodule coupling-based notions, and by architectural, we mean the horizontal layering of modules in large software systems.) We employ the notion of API (application programming interface) as the basis for our structural metrics. The rest of the metrics we present are in support of those that are based on API. Some of the important support metrics include those that characterize each module on the basis of the similarity of purpose of the services offered\u00a0\u2026", "num_citations": "145\n", "authors": ["1213"]}
{"title": "Metrics for measuring the quality of modularization of large-scale object-oriented software\n", "abstract": " The metrics formulated to date for characterizing the modularization quality of object-oriented software have considered module and class to be synonymous concepts. But a typical class in object oriented programming exists at too low a level of granularity in large object-oriented software consisting of millions of lines of code. A typical module (sometimes referred to as a superpackage) in a large object-oriented software system will typically consist of a large number of classes. Even when the access discipline encoded in each class makes for \"clean\" class-level partitioning of the code, the intermodule dependencies created by associational, inheritance-based, and method invocations may still make it difficult to maintain and extend the software. The goal of this paper is to provide a set of metrics that characterize large object-oriented software systems with regard to such dependencies. Our metrics characterize the\u00a0\u2026", "num_citations": "98\n", "authors": ["1213"]}
{"title": "Semantic-based query techniques for source code\n", "abstract": " A powerful set of features can assist software developers in searching source code. Source code can be queried to find occurrences of source code elements mapped to domain concepts appearing in an ontology. Queries can specify a search for occurrences of particular operations performed on domain concepts within the source code. Query expansion can be used to locate related domain concepts as indicated in the ontology. Query keyword weighting can be used to emphasize one domain concept over another. Tools can be provided to create the ontology and associate the ontology with the elements appearing in the source code. Results can include user interface features assisting in navigation to a location within the source code associated with the query results.", "num_citations": "90\n", "authors": ["1213"]}
{"title": "Modularization of a large-scale business application: A case study\n", "abstract": " In industries such as banking, retail, transportation, and telecommunications, large software systems support numerous work processes and develop over many years. Throughout their evolution, such systems are subject to repeated debugging and feature enhancements. Consequently, they gradually deviate from the intended architecture and deteriorate into unmanageable monoliths. To contend with this, practitioners often rewrite the entire application in a new technology or invest considerable time in documenting the code and training new engineers to work on it. However, for very large systems, such approaches are typically impossible to carry out. As an alternative, researchers have proposed several tools to automatically modularize software that's grown to be inadequate in both quality and scalability. This case study describes the modularization approach that one company adopted to reengineer a\u00a0\u2026", "num_citations": "70\n", "authors": ["1213"]}
{"title": "Classification framework of MapReduce scheduling algorithms\n", "abstract": " A MapReduce scheduling algorithm plays a critical role in managing large clusters of hardware nodes and meeting multiple quality requirements by controlling the order and distribution of users, jobs, and tasks execution. A comprehensive and structured survey of the scheduling algorithms proposed so far is presented here using a novel multidimensional classification framework. These dimensions are (i) meeting quality requirements, (ii) scheduling entities, and (iii) adapting to dynamic environments; each dimension has its own taxonomy. An empirical evaluation framework for these algorithms is recommended. This survey identifies various open issues and directions for future research.", "num_citations": "58\n", "authors": ["1213"]}
{"title": "Enterprise Architecture: A Governance Framework\n", "abstract": " Enterprise Architecture, the holistic view of an enterprise\u2019s processes, information and information technology assets as a vehicle for aligning business and IT in a structured and therefore more efficient and sustainable way, has attracted significant attention over the last two to three years.Our experience and research shows that enterprise architecture hardly ever fails because of inadequate content. The challenges usually arise around how to link the enterprise architecture efforts into the overall enterprise processes, and how to leverage them as assets used regularly by a variety of stakeholders.", "num_citations": "55\n", "authors": ["1213"]}
{"title": "Impact analysis of software change requests\n", "abstract": " In one example, a system is provided to determine the impact of implementing a change request on a software program. The system may include an architecture model of the software program that includes components. Each of the components may have attributes that may be used by the system to determine a degree of effort to modify each respective one of the components. Components may be associated with keywords. The system may search the change request for the keywords to identify components that may be impacted by the change request. The system may determine the degree of effort to modify any impacted component based on the architecture model. The system may determine the overall impact on the software program based on the degree of effort determined for the impacted components.", "num_citations": "47\n", "authors": ["1213"]}
{"title": "System and method for job scheduling optimization\n", "abstract": " A system and computer-implemented method for generating an optimized allocation of a plurality of tasks across a plurality of processors or slots for processing or execution in a distributed computing environment. In a cloud computing environment implementing a MapReduce framework, the system and computer-implemented method may be used to schedule map or reduce tasks to processors or slots on the network such that the tasks are matched to processors or slots in a data locality aware fashion wherein the suitability of node and the characteristics of the task are accounted for using a minimum cost flow function.", "num_citations": "45\n", "authors": ["1213"]}
{"title": "System for modeling architecture for business systems and methods thereof\n", "abstract": " A system and method for generating architecture for a business system is disclosed. The method comprises, in one embodiment, modeling a plurality of viewpoints adapted for describing the architecture of the business system in form of a collection of views and viewpoints and creating a software organization viewpoint adapted for providing architecture guidelines. An exemplary method further comprises creating a first plurality of notations for each of the plurality of viewpoints for describing the plurality of viewpoints and creating a second plurality of notations for capturing design rationale indicative of the first plurality of notations. Furthermore, an exemplary method comprises generating the architecture of the business system using at least one of the plurality of viewpoints or the software organization viewpoint, or at least the second plurality of notations, or combinations thereof.", "num_citations": "33\n", "authors": ["1213"]}
{"title": "Evaluating and enforcing software design quality\n", "abstract": " Evaluation and enforcement of software design quality, in which a system applies design quality rules to a design of a software application to detect violations and provides output describing one or more violations of the design quality rules detected. Based on the output, the system receives user input to address the one or more violations of the design quality rules and, subsequent to receiving the user input, evaluates code developed for the software application for violations of the design quality rules to assess quality of the code being developed for the software application based on the design of the software application.", "num_citations": "32\n", "authors": ["1213"]}
{"title": "Discovery of architectural layers and measurement of layering violations in source code\n", "abstract": " The layers architectural pattern has been widely adopted by the developer community in order to build large software systems. In reality, as the system evolves over time, rarely does the system remain conformed to the intended layers pattern, causing a significant degradation of the system maintainability. As a part of re-factoring such a system, practitioners often undertake a mostly manual exercise to discover the intended layers and organize the modules into these layers. In this paper, we present a method for semi-automatically detecting layers in the system and propose a quantitative measurement to compute the amount of non-conformance of the system from the set of layered design principles. We have applied the layer detection method and the non-conformance measurement on a set of open source and proprietary enterprise applications.", "num_citations": "31\n", "authors": ["1213"]}
{"title": "A method for detecting and measuring architectural layering violations in source code\n", "abstract": " The layered architecture pattern has been widely adopted by the developer community in order to build large software systems. The layered organization of software modules offers a number of benefits such as reusability, changeability and portability to those who are involved in the development and maintenance of such software systems. But in reality as the system evolves over time, rarely does the actual source code of the system conform to the conceptual horizontal layering of modules. This in turn results in a significant degradation of system maintainability. In order to re-factor such a system to improve its maintainability, it is very important to discover, analyze and measure violations of layered architecture pattern. In this paper we propose a technique to discover such violations in the source code and quantitatively measure the amount of non-conformance to the conceptual layering. The proposed approach\u00a0\u2026", "num_citations": "29\n", "authors": ["1213"]}
{"title": "A collaborative platform for application knowledge management in software maintenance projects\n", "abstract": " In the era of global outsourcing, maintenance and enhancement activities are performed in distributed locations. In most cases, the domain expertise is not available which increases the complexity to manifold. A critical success factor in such a scenario is to have a collaborative platform for managing and sharing the domain specific knowledge across distributed locations. In our ongoing research we have developed a human assisted collaborative knowledge sharing tool called CollabDev. The aim of this tool is to analyze applications in multiple languages and render various structural, architectural, and functional insights to the people involved in maintenance. The novelty of this platform lies in integrating different elements of application knowledge by linking them to source code and allowing multiple developers to collaborate on-line by using annotations for the knowledge elements. The platform also provides\u00a0\u2026", "num_citations": "27\n", "authors": ["1213"]}
{"title": "System and method for improving modularity of large legacy software systems\n", "abstract": " A system and method for improving modularity of a software source code is provided. The system comprises of a user interface for receiving source code; a source code model extractor for parsing and forming a model of the source code; a source code model database for storing the source code model, refactoring operators, and a record of refactoring changes; a modularity improvement analyzer for reading the source code model and modularity problem diagnosis data and generating a set of prescriptions; an optimal improvement suggestion selector for evaluating and selecting prescriptions; and a refactoring engine for receiving selected prescriptions and applying them on the source code.", "num_citations": "25\n", "authors": ["1213"]}
{"title": "Identification of topics in source code\n", "abstract": " Topics in source code can be identified using Latent Dirichlet Allocation (LDA) by receiving source code, identifying domain specific keywords from the source code, generating a keyword matrix, processing the keyword matrix and the source code using LDA, and outputting a list of topics. The list of topics is output as collections of domain specific keywords. Probabilities of domain specific keywords belonging to their respective topics can also be output. The keyword matrix comprises weighted sums of occurrences of domain specific keywords in the source code.", "num_citations": "25\n", "authors": ["1213"]}
{"title": "Metrics for analyzing module interactions in large software systems\n", "abstract": " We present a new set of metrics for analyzing the interaction between the modules of a large software system. We believe that these metrics would be important to any automatic or semi-automatic code modularization algorithm. The metrics are based on the rationale that code partitioning should be based on the principle of similarity of service provided by the different functions encapsulated in a module. Although module interaction metrics are necessary for code modularization, in practice they must be accompanied by metrics that measure other important attributes of how the code is partitioned into modules. These other metrics, dealing with code properties such as the approximate uniformity of module sizes, conformance to any size constraints on the modules, etc., are also included in the work presented here. To give the reader some insight into the workings of our metrics, this paper also includes some results\u00a0\u2026", "num_citations": "24\n", "authors": ["1213"]}
{"title": "An Experiment with Conceptual Clustering for the Analysis of Security Alerts\n", "abstract": " In response to attack against corporative and enterprise networks, administrators deploy intrusion detection systems, monitors, vulnerability scans and log systems. These systems monitor and record host and network device activities searching for signs of anomalies and security incidents. Doing that, these systems generally produce a huge number of alerts that overwhelms security analysts. This paper proposes the application of a conceptual clustering technique for filtering alerts and shows the results obtained for seven months of security alerts generated in a real large scale SaaS Cloud system. The technique has been useful to support manual analysis activities conducted by the operations team of the reference Cloud system.", "num_citations": "22\n", "authors": ["1213"]}
{"title": "An Empirical Study of Hadoop's Energy Efficiency on a HPC Cluster\n", "abstract": " Map-Reduce programming model is commonly used for efficient scientific computations, as it executes tasks in parallel and distributed manner on large data volumes. The HPC infrastructure can effectively increase the parallelism of map-reduce tasks. However such an execution will incur high energy and data transmission costs. Here we empirically study how the energy efficiency of a map-reduce job varies with increase in parallelism and network bandwidth on a HPC cluster. We also investigate the effectiveness of power-aware systems in managing the energy consumption of different types of map-reduce jobs. We comprehend that for some jobs the energy efficiency degrades at high degree of parallelism, and for some it improves at low CPU frequency. Consequently we suggest strategies for configuring the degree of parallelism, network bandwidth and power management features in a HPC cluster for energy\u00a0\u2026", "num_citations": "20\n", "authors": ["1213"]}
{"title": "Towards transforming an industrial automation system from monolithic to microservices\n", "abstract": " Container technology enables designers to build (micro)service-oriented systems with on-demand scalability and availability easily, provided the original system has been well-modularized to begin with. Industry automation applications, built a long time ago, aim to adopt this technology to become more flexible and ready to be a part of the internet of thing based next-generation industrial system. In this paper, we share our work-in-progress experience of transforming a complex, distributed industrial automation system to a microservice based containerized architecture. We propose a containerized architecture of the \u201cto-be\u201d system and observe that despite being distributed, the \u201cas-is\u201d system tend to follow a monolithic architecture with strong coupling among the participating components. Consequently it becomes difficult to achieve the proposed microservice based architecture without a significant change. We also\u00a0\u2026", "num_citations": "19\n", "authors": ["1213"]}
{"title": "EAML-architecture modeling language for enterprise applications\n", "abstract": " A typical enterprise system comprises of a collection of applications that automate a set of business processes developed over time and distributed in distributed locations. Identifying the appropriate set of formal notations to describe the architecture aspects of an enterprise system is a challenge. Existing notations such as UML and popular ADLs serve the purpose partially. This observation has led us to define an architecture modeling language EAML to address the needs of enterprise systems. In this paper we present an overview of EAML notations and propose a set of views (expressed as diagrams) that uses these notations to describe the architecture through an example", "num_citations": "18\n", "authors": ["1213"]}
{"title": "VLODS: A VLSI object oriented database system\n", "abstract": " This paper proposes an efficient object oriented data model for VLSI databases. The salient features of the data model and its advantages have been explained. Based on this data model, ODS, an object oriented database system has been implemented. A query language (VDDL) for VLSI database access and design description has been proposed.", "num_citations": "15\n", "authors": ["1213"]}
{"title": "Reuse and refactoring of GPU kernels to design complex applications\n", "abstract": " Developers of GPU kernels, such as FFT, linear solvers, etc, tune their code extensively in order to obtain optimal performance, making efficient use of different resources available on the GPU. Complex applications are composed of several such kernel components. The software engineering community has performed extensive research on component based design to build generic and flexible components, such that a component can be reused across diverse applications, rather than optimizing its performance. Since a GPU is used primarily to improve performance, application performance becomes a key design issue. The contribution of our work lies in extending component based design research in a new direction, dealing with the performance impact of refactoring an application consisting of the composition of highly tuned kernels. Such refactoring can make the composition more effective with respect to GPU\u00a0\u2026", "num_citations": "12\n", "authors": ["1213"]}
{"title": "Implementation of a Scalable Next Generation Sequencing Business Cloud Platform--An Experience Report\n", "abstract": " Life science industry is looking towards new and cost-effective ways to manage and analyze huge amount of genomic data for faster innovation in drug or biologics discovery. To that effect, various alliances among competitive organizations are getting formed, such as the Pistoia Alliance, to collaborate and share a pool of genomic data and build useful search and analysis techniques for the alliance partners. In order to make the development, and management of data and applications cost-effective, a secure cloud computing based platforms are being considered. In this paper we describe an experience report of building such a collaborative platform on Amazon cloud platform. In order to build a scalable genome sequence alignment solution, we have adopted the well-known BLAST framework on Hadoop platform. A major challenge here is that the BLAST executable requires to be ported as it is, and yet the\u00a0\u2026", "num_citations": "11\n", "authors": ["1213"]}
{"title": "Identification of critical parameters for MapReduce energy efficiency using statistical Design of Experiments\n", "abstract": " Energy efficiency is an important concern for data centers today. Most of these data centers use MapReduce frameworks for big data processing. These frameworks and modern hardware provide the flexibility in form of parameters to manage the performance and energy consumption of system. However tuning these parameters such that it reduces energy consumption without impacting performance is challenging since - 1) there are a large number of parameters across the layers of frameworks, 2) impact of the parameters differ based on the workload characteristics, 3) the same parameter may have conflicting impacts on performance and energy and 4) parameters may have interaction effects. To streamline the parameter tuning, we present the systematic design of experiments to study the effects of different parameters on performance and energy consumption with a view to identify the most influential ones\u00a0\u2026", "num_citations": "10\n", "authors": ["1213"]}
{"title": "Architectural partitioning and deployment modeling on hybrid clouds\n", "abstract": " The hybrid cloud idea is increasingly gaining momentum because it brings distinct advantages as a hosting platform for complex software systems. However, there are several challenges that need to be surmounted before hybrid hosting can become pervasive and penetrative. One main problem is to architecturally partition workloads across permutations of feasible cloud and non\u2010cloud deployment choices to yield the best\u2010fit hosting combination. Another is to predict the effort estimate to deliver such an advantageous hybrid deployment. In this paper, we describe a heuristic solution to address the said obstacles and converge on the ideal hybrid cloud deployment architecture, based on properties and characteristics of workloads that are sought to be hosted. We next propose a model to represent such a hybrid cloud deployment and demonstrate a method to estimate the effort required to implement and sustain\u00a0\u2026", "num_citations": "9\n", "authors": ["1213"]}
{"title": "Process and system for assessing modularity of an object-oriented program\n", "abstract": " The present invention describes a process, system and computer program product for assessing the modularity of an object-oriented program. The process includes calculation of metrics associated with various properties of the object-oriented program. Analysis is performed on the basis of the calculated metrics.", "num_citations": "9\n", "authors": ["1213"]}
{"title": "Synchronization of communicating modules and processes in high level synthesis\n", "abstract": " In ASIC designs, reuse of already available components is often preferred. Synthesis systems catering to this need must ensure proper synchronization among the communicating modules. This paper proposes an object oriented design framework to support reuse. The steps to be taken for synchronization of communicating hardware entities through a non-blocking channel have been analyzed. The synthesis system ensures synchronization among the communicating modules before scheduling. The scheme has been tested on a few real life image processing examples.", "num_citations": "9\n", "authors": ["1213"]}
{"title": "Samatulyata: An efficient path based equivalence checking tool\n", "abstract": " An application program can go through significant optimizing and parallelizing transformations, both automated and human guided, before being mapped to an architecture. Formal verification of these transformations is crucial to ensure that they preserve the original behavioural specification. PRES+ model (Petri net based Representation of Embedded Systems) encompassing data processing is used to model parallel behaviours more vividly. This paper presents a translation validation tool for verifying optimizing and parallelizing code transformations by checking equivalence between two PRES+ models, one representing the source code and the other representing its optimized and (or) parallelized version.", "num_citations": "8\n", "authors": ["1213"]}
{"title": "Accelerating technical design of business applications: a knowledge-based approach\n", "abstract": " Technical design of a business application is an involved process which needs an experienced designer to describe an implementation of the application functionality using a set of hardware and software infrastructure elements such that the non-functional requirements are satisfied. The process is extremely knowledge intensive. For instance, it demands the designer to thoroughly understand various architecture styles, a set of technical capabilities available, how different COTS products can realize these capabilities, and how the functional modules can make use of these capabilities. In reality, such experienced designers are hard to come by. Furthermore, to optimize the budget, the project team often involves inexperienced designers to create the technical design. In order to assist an inexperienced designer to create the technical design, we have built a knowledge-based design assistant tool, called Technical\u00a0\u2026", "num_citations": "8\n", "authors": ["1213"]}
{"title": "Predicting execution time of CUDA kernel using static analysis\n", "abstract": " With the growing demand for performance-oriented problems, programmers routinely execute the embarrassing parallel part of the application (GPU kernels) in a GPU in order to achieve signi?cant speedup. These applications are becoming complex and long-running which makes it energy inef?cient. Anticipating its execution time can help the developers to ?x the inef?cient code before running it. In this paper, we propose an approach to predict the execution time of a GPU kernel without the need of executing it. We build an analytical model to predict the execution time of a GPU kernel by analyzing the intermediate PTX code of a CUDA kernel. Our experimental analysis of a set of benchmarks shows that for 45 applications the estimated execution time has the mean absolute error of 26.86% when compared to the actual execution time. Mean absolute error for benchmarks belonging to Dynamic programming\u00a0\u2026", "num_citations": "7\n", "authors": ["1213"]}
{"title": "Thrust++: Extending thrust framework for better abstraction and performance\n", "abstract": " A good design abstraction framework for high performance computing should provide a higher level programming abstraction that strikes a balance between the abstraction and visibility over the hardware so that the software developer can write a portable software without having to understand the hardware nuances, yet exploit the compute power optimally. In this paper we have analyzed a popular design abstraction framework called \"Thrust\" from NVIDIA, and proposed an extension called Thrust++ that provides abstraction over the memory hierarchy of an NVIDIA GPU. Thrust++ allows developers to make efficient use of shared memory and overall, provides better control over the GPU memory hierarchy while writing applications in Thrust style for the CUDA backend. We have shown that when applications are written for the CUDA backend using Thrust++, they have minimal performance degradation when\u00a0\u2026", "num_citations": "7\n", "authors": ["1213"]}
{"title": "How Long will this Live? Discovering the Lifespans of Software Engineering Ideas\n", "abstract": " We all want to be associated with long lasting ideas; as originators, or at least, expositors. For a tyro researcher or a seasoned veteran, knowing how long an idea will remain interesting in the community is critical in choosing and pursuing research threads. In the physical sciences, the notion of half-life is often evoked to quantify decaying intensity. In this paper, we study a corpus of 19,000+ papers written by 21,000+ authors across 16 software engineering publication venues from 1975 to 2010, to empirically determine the half-life of software engineering research topics. In the absence of any consistent and well-accepted methodology for associating research topics to a publication, we have used natural language processing techniques to semi-automatically identify and associate a set of topics with a paper. We adapted measures of half-life already existing in the bibliometric context for our study, and also defined\u00a0\u2026", "num_citations": "7\n", "authors": ["1213"]}
{"title": "Security issues and challenges in cloud computing\n", "abstract": " Cloud is a windfall to new generation technology. But if it fails to ensure proper security protection, cloud services could ultimately result in higher cost and potential loss of business thus eliminating all the potential benefits of cloud technology. So the aim of the cloud security and its researchers to help enterprise information technology and decision makers to analyze the security implications of cloud computing in their business. The main issues concerned with cloud are security, privacy, reliability, legal issues, open standard, compliance, freedom, long term viability etc. The various security issues associated with cloud computing take in privileged access, regulatory compliance, data location, data segregation, recovery, investigative support, data availability. When a customer moves toward cloud computing, they have a clear understanding of potential security and risk connected with cloud computing. This paper\u00a0\u2026", "num_citations": "7\n", "authors": ["1213"]}
{"title": "The importance of being isolated: An empirical study on chromium reviews\n", "abstract": " As large scale software development has become more collaborative, and software teams more globally distributed, several studies have explored how developer interaction influences software development outcomes. The emphasis so far has been largely on outcomes like defect count, the time to close modification requests etc. In the paper, we examine data from the Chromium project to understand how different aspects of developer discussion relate to the closure time of reviews. On the basis of analyzing reviews discussed by 2000+ developers, our results indicate that quicker closure of reviews owned by a developer relates to higher reception of information and insights from peers. However, we also find evidence that higher engagement in collaboration by a developer is associated with slower closure of the reviews she owns. Within the scope of our study, these results lead us to conclude that peer review of\u00a0\u2026", "num_citations": "7\n", "authors": ["1213"]}
{"title": "A profile guided approach to optimize branch divergence while transforming applications for gpus\n", "abstract": " GPUs offer a powerful bulk synchronous programming model for exploiting data parallelism; however, branch divergence amongst executing warps can lead to serious performance degradation due to execution serialization. We propose a novel profile guided approach to optimize branch divergence while transforming a serial program to a data-parallel program for GPUs. Our approach is based on the observation that branches inside some data parallel loops although divergent, exhibit repetitive regular patterns of outcomes. By exploiting such patterns, loop iterations can be aligned so that the corresponding iterations traverse the same branch path. These aligned iterations when executed as a warp in a GPU, become convergent. We propose a new metric based on the repetitive pattern characteristics that indicates whether a data-parallel loop is worth restructuring. When tested our approach on the well-known\u00a0\u2026", "num_citations": "7\n", "authors": ["1213"]}
{"title": "An empirical evaluation of design abstraction and performance of thrust framework\n", "abstract": " High performance computing applications are far more difficult to write, therefore, practitioners expect a well-tuned software to last long and provide optimized performance even when the hardware is upgraded. It may also be necessary to write software using sufficient abstraction over the hardware so that it is capable of running on heterogeneous architecture. Therefore, it is required to have a proper programming abstraction paradigm that strikes a balance between the abstraction and visibility over the hardware so that the programmer can write a program without having to understand the hardware nuances, yet exploit the compute power optimally. In this paper we have analyzed the power of design abstraction and performance of a popular design abstraction framework called Thrust. We have shown quantitatively that while it is easier to write an application using Thrust compared to writing the same in the native\u00a0\u2026", "num_citations": "6\n", "authors": ["1213"]}
{"title": "CPU frequency tuning to improve energy efficiency of mapreduce systems\n", "abstract": " Energy efficiency is a major concern in today's data centers that house large scale distributed processing systems such as data parallel MapReduce clusters. Modern power aware systems utilize the dynamic voltage and frequency scaling mechanism available in processors to manage the energy consumption. In this paper, we initially characterize the energy efficiency of MapReduce jobs with respect to built-in power governors. Our analysis indicates that while a built-in power governor provides the best energy efficiency for a job that is CPU as well as IO intensive, a common CPU-frequency across the cluster provides best the energy efficiency for other types of jobs. In order to identify this optimal frequency setting, we derive energy and performance models for MapReduce jobs on a HPC cluster and validate these models experimentally on different platforms. We demonstrate how these models can be used to\u00a0\u2026", "num_citations": "5\n", "authors": ["1213"]}
{"title": "Method and system for analyzing an extent of speedup achievable for an application in a heterogeneous system\n", "abstract": " The present disclosure includes, in a heterogeneous system, receiving a desired speedup of an application as input and performing a static analysis and a dynamic analysis of the application. The dynamic analysis of the application comprises, identifying a set of parameters including, an end-to-end execution time of the application, an execution time of data parallel loops in the application, an execution time of non-data parallel loops in the application, and an amount of physical memory used by each data structure in each data parallel loop. Dynamic analysis also includes calculating and providing the feasibility of achieving the desired speedup of the application based on the identified set of parameters, and satisfaction of each of, an initialization invariant, a data-parallel invariant and a data transfer invariant.", "num_citations": "5\n", "authors": ["1213"]}
{"title": "Method, system, and computer-readable medium for providing a scalable bio-informatics sequence search on cloud\n", "abstract": " The present invention relates to a computer-implemented method, system and computer readable medium for providing a scalable bio-informatics sequence search on cloud. The method comprises the steps of partitioning a genome data into a plurality of datasets and storing the plurality of data sets in a database. Receiving at least one sequence search request input and searching for a genome sequence in the database corresponding to the search request input and scaling of the sequence search based on the sequence search request input.", "num_citations": "5\n", "authors": ["1213"]}
{"title": "Response Surface Methodology-based approach to Electrochemical Deburring (ECD) of SS304 stainless steel workpiece\n", "abstract": " Deburring operations with high efficiency and full automation of the conventional burr removal methods are extremely difficult. A promising solution is provided by Electrochemical Deburring (ECD). Successful utilisation of ECD, in present day manufacturing, demands extensive research for economical manufacturing of engineering products. In the present research, Response Surface Methodology (RSM) is used to develop predictive models by studying the influence of main process parameters of ECD process of SS304 stainless steel workpiece on various deburring characteristics. High values for coefficient of determination, obtained through Analysis of Variance (ANOVA), ensure satisfactory fit of the developed models to the experimental data.", "num_citations": "5\n", "authors": ["1213"]}
{"title": "Method for improving energy efficiency of map-reduce system and apparatus thereof\n", "abstract": " This technique improves energy efficiency of MapReduce system by using system performance model without changing any component of the MapReduce system. This involves determining presence of any hardware bottleneck in any node of MapReduce system based on a system performance model and if any hardware bottleneck is present in any node, then the maximum bandwidth value of hardware associated with the bottleneck of each node is determined. Thereafter, an energy efficient value of Central Processing Unit (CPU) frequency of each node having the bottleneck is determined by using the system performance model and the maximum bandwidth value of hardware associated with the bottleneck. Further, the CPU frequency of each node having the bottleneck is set at the energy efficient value determined in the earlier step.", "num_citations": "4\n", "authors": ["1213"]}
{"title": "Improving safety in collaborative robot tasks\n", "abstract": " In recent times, there has been significant interest in collaborative robots where the tasks performed by a robot are non-repetitive and complex, and humans and robots share an overlapping workspace. In such a case, the robot controller must necessarily be safety-aware. In this paper, we propose a mechanism to evaluate a robot program and compute a safety score for each action that the robot is about to perform. To this end, we have implemented a code analyzer that examines the robot's Move instructions and assigns a safety score. A subjective logic based approach is used to compute the safety score for each instruction. We have evaluated the approach through ABB's RobotStudio \u00ae  simulator. We simulate two scenarios: First, where two robots share a workspace and second, where a robot moves along a path with an obstacle. The simulations show that using our code analyzer and safety score formalism; it\u00a0\u2026", "num_citations": "4\n", "authors": ["1213"]}
{"title": "Data-race detection: the missing piece for an end-to-end semantic equivalence checker for parallelizing transformations of array-intensive programs\n", "abstract": " The parallelizing transformation (hand-crafted or compiler-assisted) is error prone as it is often performed without verifying any semantic equivalence with the sequential counterpart. Even when the parallel program can be proved to be semantically equivalent with its corresponding sequential program, detecting data-race conditions in the parallel program remains a challenge. In this paper, we propose a formal verification approach that can detect data-race conditions while verifying the computational semantic equivalence of parallelizing loop transformations. We propose a coloured array data dependence graph (C-ADDG) based modeling of a program for verification of program equivalence as well as data-race condition detection across parallel loops. We have tested our tool on a set of Rodinia and PLuTo+ benchmarks and shown that our method is sound, whereby the method does not produce any false\u00a0\u2026", "num_citations": "4\n", "authors": ["1213"]}
{"title": "The social network of software engineering research\n", "abstract": " The social network perspective has served as a useful framework for studying scientific research collaboration in different disciplines. Although collaboration in computer science research has received some attention, software engineering research collaboration has remained unexplored to a large extent. In this paper, we examine the collaboration networks based on co-authorship information of papers from ten software engineering publication venues over the 1976-2010 time period. We compare time variations of certain parameters of these networks with corresponding parameters of collaboration networks from other disciplines. We also explore whether software engineering collaboration networks manifest symptoms of the small-world phenomenon, conform to the criteria of\" social networks\", and manifest increasing collaboration with time. In the light of these observations, we highlight some general\u00a0\u2026", "num_citations": "4\n", "authors": ["1213"]}
{"title": "Architecture Reconstruction from Code for Business Applications - A Practical Approach\n", "abstract": " During application development or maintenance, the application (or a family of applications) under consideration often co-exists with several existing applications. This obviously requires the team members to be aware of the underlying architecture of these applications. To comprehend a complex code base it is necessarily to create multiple views (such as functional, technical, deployment etc.) of these applications, at multiple levels of abstraction. In absence of any formal documentation, and due to unavailability of any automated tool, the practitioner ends up creating a partially complete, ambiguous design from code and thereafter continuously struggle to keep it up-to-date. Most of the stateof-art tools reconstruct a low-level design from a code base. For a typical business application having a large code base, extracted low-level design is extremely complex to comprehend due to overwhelmingly large number of fine grained entities and their relationships. In this paper, we describe a semi-automated, iterative approach, called the\u201d Design Discovery Method\u201d(DDM), to model the hierarchical functional architecture of a family of applications at three levels of abstraction. We have experimented DDM on a few real life systems with multiple applications written in Java and received encouraging feedback from the practitioners on the overall approach.", "num_citations": "4\n", "authors": ["1213"]}
{"title": "Time-Sensitive Provisioning of Bare Metal Compute as a Cloud Service\n", "abstract": " A modern cloud service that is getting popular is the supply of bare metal servers on demand to consumers who need to run high-performance algorithms for short periods of time. However, provisioning-time latencies are unpredictable in bare metal commerce, primarily because suppliers cannot exploit virtualization levers to adjust and optimize capacity. This impacts bare metal service leverage because, in the face of indeterministic fulfillment times, consumers often pre-provision for peak demand, which goes against the fundamental tenets and advantages of cloud adoption. To address this, we advocate that a cloud service broker offers time-sensitive bare metal provisioning services by building and maintaining an inventory of bare metal servers. This, however, leads to inventory optimization and profit maximization problems that resemble what traditional supply chains face, but with characteristics unique to on\u00a0\u2026", "num_citations": "3\n", "authors": ["1213"]}
{"title": "Fitness-aware containerization service leveraging machine learning\n", "abstract": " Containerized deployment of microservices has gained immense traction across industries. To meet demand, traditional cloud providers offer container-as-a-service, where selection of the container and containerization of workloads remain developer's responsibility. This task is arduous for a developer since the choice of containers across different cloud providers is many. Furthermore, there does not exist any mechanism using which one can compare and contrast the capabilities of containers across different providers. In this scenario, we envisage the need for a smart cloud broker that can automatically deploy a chosen IT service into the best-fit container environment mapped to performance requirements, from among the set of available underpinning brokered container hosting systems spread across multiple cloud providers. We propose a novel fitness-aware containerization-as-a-service to achieve this. We\u00a0\u2026", "num_citations": "3\n", "authors": ["1213"]}
{"title": "Best-Fit Containerization as a Brokered Service\n", "abstract": " IT consumption is rapidly moving to an everything-as-a-Service (XaaS) model wherein a cloud broker, or in general, a digital marketplace, maintains a portfolio of infrastructure, platform and software services. In the meanwhile, containerization of workloads and the immutable microservices paradigm of application software architecture has witnessed a dramatic upswing across industries. In this scenario, we see the need for a smart fulfillment engine within a cloud broker that can automatically deploy a chosen cataloged IT service into the best-performing container environment from among the set of available underpinning brokered container hosting systems. More broadly, the problem statement we address in this paper is to supply best-performing container deployment as a provisioning-time service to applications that are part of a broker's SaaS catalog, in an on-demand and pay-as-you-go manner. We show why\u00a0\u2026", "num_citations": "3\n", "authors": ["1213"]}
{"title": "Improving energy efficiency of io-intensive mapreduce jobs\n", "abstract": " Map-Reduce is a popular data-parallel programming model for varied analysis of huge volumes of data. While a multicore and many CPU HPC infrastructure can be used to improve parallelism of map-reduce tasks, IO-bandwidth limitations may make them ineffective. IO-intensive activities are essential in any MapReduce cluster. In HPC nodes, IO-intensive jobs get queued at the IO-resources while the CPU remain underutilized, resulting in a poor performance, high power consumption and thus, energy inefficiency. In this paper, we investigate which power management setting can be used to improve the energy efficiency of IO-intensive MapReduce jobs by performing a thorough empirical study. Our analysis indicates that a constant CPU frequency can reduce the energy consumption of an IO-intensive job, while improving its performance. Consequently, we build a set of regression models to predict the energy\u00a0\u2026", "num_citations": "3\n", "authors": ["1213"]}
{"title": "DOORS: An object-oriented CAD system for high level synthesis\n", "abstract": " To expedite the design process of a complex system, it is required to view the system as interactions of complex subsystems which are not necessarily RTL components and synthesise the hardware from such a specification. In the paper, an automated design and synthesis environment called DOORS has been proposed that can accept a complex system specification at such a high level of abstraction and synthesises the circuit. DOORS is based on an object-oriented design methodology. Circuit elements are treated as classes. The designer describes the circuit behaviour as a sequence of state transitions in response to external messages. The method executed in response to a message is transparent to the external world. The specification may involve presynthesised components whose functionalities are invoked by sending messages to them. The synthesis system of DOORS accepts the specification and\u00a0\u2026", "num_citations": "3\n", "authors": ["1213"]}
{"title": "Representation and synthesis of interface of a circuit for its reuse [VLSI design]\n", "abstract": " In this paper we propose an object oriented approach for reusing presynthesized components in VLSI design. Each component object has an interface which hides its internal implementation details and yet allows the operations, performed by the component, to be invoked by sending an appropriate message. A graph based representation scheme has been adopted for capturing necessary interface information. The proposed interface graph can deal with circuits of various complexities. Moreover, the utilization of the interface graph of an object, during its reuse in subsequent synthesis of a new circuit, has been described and illustrated with the help of examples. Lastly, a scheme for extracting the interface graph during synthesis of an object has been proposed.", "num_citations": "3\n", "authors": ["1213"]}
{"title": "An object oriented environment for modeling and synthesis of hardware circuits\n", "abstract": " Presents an object oriented design environment for modeling as well as synthesis of circuits. The modeling of circuits as classes and storage of presynthesized modules has been discussed. A design approach is presented which synthesizes both the datapath and control path of a circuit from its behavioral specification provided in object oriented style. The synthesis approach can synthesize circuits which reuse the presynthesized modules from the design library.< >", "num_citations": "3\n", "authors": ["1213"]}
{"title": "Modeling operational fairness of hybrid cloud brokerage\n", "abstract": " Cloud service brokerage is an emerging technology that attempts to simplify the consumption and operation of hybrid clouds. Today's cloud brokers attempt to insulate consumers from the vagaries of multiple clouds. To achieve the insulation, the modern cloud broker needs to disguise itself as the end-provider to consumers by creating and operating a virtual data center construct that we call a \"meta-cloud\", which is assembled on top of a set of participating supplier clouds. It is crucial for such a cloud broker to be considered a trusted partner both by cloud consumers and by the underpinning cloud suppliers. A fundamental tenet of brokerage trust is vendor neutrality. On the one hand, cloud consumers will be comfortable if a cloud broker guarantees that they will not be led through a preferred path. And on the other hand, cloud suppliers would be more interested in partnering with a cloud broker who promises a fair\u00a0\u2026", "num_citations": "2\n", "authors": ["1213"]}
{"title": "Predicting the impact of software engineering topics: An empirical study\n", "abstract": " Predicting the future is hard, more so in active research areas. In this paper, we customize an established model for citation prediction of research papers and apply it on research topics. We argue that research topics, rather than individual publications, have wider relevance in the research ecosystem, for individuals as well as organizations. In this study, topics are extracted from a corpus of software engineering publications covering 55,000+ papers written by more than 70,000 authors across 56 publication venues, over a span of 38 years, using natural language processing techniques. We demonstrate how critical aspects of the original paper-based prediction model are valid for a topic-based approach. Our results indicate the customized model is able to predict citations for many of the topics considered in our study with reasonably high accuracy. Insights from these results indicate the promise of citation of\u00a0\u2026", "num_citations": "2\n", "authors": ["1213"]}
{"title": "Unified power and energy measurement API for HPC co-processors\n", "abstract": " Power and energy optimization of applications running on a High Performance Computing infrastructure is an important research area in today's HPC driven world. Considerable amount of investments are being made for the same, emphasizing on the importance of such a research. The above mentioned optimization is a two step process. First, one must measure and analyze the power consumption accurately and then optimize the application by modifying the relevant sections of the application software responsible for it. In this paper we propose an API, called the Unified Power Profiling API (UPPAPI), which allows a parallel application developer to measure the power and energy consumed by the heterogeneous system during the application execution. The first important feature of this API is the abstraction created over different co-processors. Second, we have incorporated a corrective power model to remove\u00a0\u2026", "num_citations": "2\n", "authors": ["1213"]}
{"title": "Discovering the rise and fall of software engineering ideas from scholarly publication data\n", "abstract": " For researchers and practitioners of a relatively young discipline like software engineering, an enduring concern is to identify the acorns that will grow into oaks--ideas remaining most current in the long run. Additionally, it is interesting to know how the ideas have risen in importance, and fallen, perhaps to rise again. We analyzed a corpus of 19,000+ papers written by 21,000+ authors across 16 software engineering publication venues from 1975 to 2010, to empirically determine the half-life of software engineering research topics. We adapted existing measures of half-life as well as defined a specific measure based on publication and citation counts. The results from this empirical study are a presented in this paper.", "num_citations": "2\n", "authors": ["1213"]}
{"title": "Formal architecture modeling of business application-software maintenance case study\n", "abstract": " Maintenance of complex business applications is challenging for software services industry. The maintenance team inherits the software with little design and implementation knowledge. The client-facing team gathers an ad-hoc architectural description of some sort and communicates the same to the geographically distributed maintenance team through informal box and line diagrams. This information is poorly understood, and the underlying architectural constraints are never enforced. This paper proposes a type system to model the architecture of a complex enterprise IT system using Acme architecture description language and reports a modeling approach to capture various architectural design decisions architects perform as a part of the architecture review. An initial field-study to evaluate the usefulness of such modeling has been encouraging.", "num_citations": "2\n", "authors": ["1213"]}
{"title": "A Study of Existing Architecture Description Approaches from Enterprise System Development Perspective\n", "abstract": " Over the past few decades, enterprises have put massive effort to build software systems to automate their business processes. The magnitude of complexity involved in system development has forced the enterprises to think about architecture centric approach to system design. Research community on the other hand has come up with various approaches to describe the architecture of a software intensive system over the past decade. Even today, there is a little consensus that what all should be considered as a part of software architecture. As a result, the architecture description is still ad-hoc and informal in most of the enterprise business application development. This paper identifies the architectural needs for enterprise system development and analyzes various architecture description approaches from this viewpoint.", "num_citations": "2\n", "authors": ["1213"]}
{"title": "An Object Oriented Approach to Digital Circuit Synthesis\n", "abstract": " DSpace at My University: An Object Oriented Approach to Digital Circuit Synthesis Skip navigation DSpace logo Home Browse Communities & Collections Browse Items by: Issue Date Author Title Subject Sign on to: My DSpace Receive email updates Edit Profile DSpace JSPUI DSpace preserves and enables easy and open access to all types of digital content including text, images, moving images, mpegs and data sets Learn More DSpace logo 1.DSpace at My University 2. Ph.D Theses of IIT Kharagpur 3.Computer Science & Engineering 4.An Object Oriented Approach to Digital Circuit Synthesis Please use this identifier to cite or link to this item: http://www.idr.iitkgp.ac.in/xmlui/handle/123456789/8679 Title: An Object Oriented Approach to Digital Circuit Synthesis Authors: Sarkar, Santonu Keywords: VLSI Circuits Circuit Elements OHDL Computer Aided Design DOORS Issue Date: 1-Oct-1995 Publisher: IIT, \u2026", "num_citations": "2\n", "authors": ["1213"]}
{"title": "A New Paradigm of Cloud Brokerage\n", "abstract": " When the cloud computing industry started its commercial acceleration a few years back, a new player entered the computing supply chain - the cloud broker. With the transformation of core business models through hybrid cloud and multi-cloud environments being witnessed across industries today, next generation cloud brokerage is poised to disrupt cloud adoption. In this paper, we propose a cloud broker service framework that will shield consumers from the vagaries of multiple clouds. To achieve this insulation, cloud brokers will disguise themselves as end-providers by creating and operating a virtual data center construct that we call Meta Cloud, which is assembled on top of participating supplier clouds. The Meta Cloud will bring simplification and unified dependability by creating converged determinism on top of non-uniform and nondeterministic provider clouds. Hand in hand with Meta Clouds, is the\u00a0\u2026", "num_citations": "1\n", "authors": ["1213"]}
{"title": "ThrustHetero: A Framework to Simplify Heterogeneous Computing Platform Programming using Design Abstraction\n", "abstract": " Heterogeneous compute architectures like Multi-Core CPUs, CUDA GPUs, and Intel Xeon Phis have become prevalent over the years. While heterogeneity makes architecture specific features available to the programmer, it also makes application development difficult, as one needs to plan for optimal usage of architectural features, suitable partitioning of the workload, communication and data transfer among the participating devices. A suitable design abstraction that hides such variabilities of the underlying devices and at the same time exploits their computing capabilities, can improve developer productivity. In this work, we present\" ThrustHetero\", a lightweight framework based on NVIDIA's Thrust, that provides an abstraction over several devices such as GPUs, Xeon Phis and multicore, yet allows developers to easily leverage the full compute capability of these devices. We also demonstrate a novel method for\u00a0\u2026", "num_citations": "1\n", "authors": ["1213"]}
{"title": "Understanding the Inter-domain Presence of Research Topics in the Computing Discipline: An Empirical Study\n", "abstract": " The very nature of scientific inquiry encourages flow of ideas across research domains in a discipline. Research topics with higher inter-domain presence tend to attract higher attention at individual and organizational levels. This is more pronounced in a discipline like computing, with its deeply intertwined ideas and strong connections with technology. In this paper, we study corpora of research publications across four domains of the computing discipline, covering more than 150,000 papers, involving more than 200,000 authors over 55 years and 175 publication venues, to examine the influences on inter-domain presence of research topics. We find statistically significant evidence that higher collective eminence of researchers publishing on a topic is related to lower inter-domain presence of that topic, fewer authors publishing on a topic relate to the topic being likely to have higher inter-domain presence, while\u00a0\u2026", "num_citations": "1\n", "authors": ["1213"]}
{"title": "Analysis of GPGPU Programs for Data-race and Barrier Divergence.\n", "abstract": " Todays business and scientific applications have a high computing demand due to the increasing data size and the demand for responsiveness. Many such applications have a high degree of parallelism and GPGPUs emerge as a fit candidate for the demand. GPGPUs can offer an extremely high degree of data parallelism owing to its architecture that has many computing cores. However, unless the programs written to exploit the architecture are correct, the potential gain in performance cannot be achieved. In this paper, we focus on the two important properties of the programs written for GPGPUs, namely i) the data-race conditions and ii) the barrier divergence. We present a technique to identify the existence of these properties in a CUDA program using a static property verification method. The proposed approach can be utilized in tandem with normal application development process to help the programmer to remove the bugs that can have an impact on the performance and improve the safety of a CUDA program.", "num_citations": "1\n", "authors": ["1213"]}
{"title": "How Effective is Design Abstraction in Thrust? An Empirical Evaluation\n", "abstract": " High performance computing applications are far more difficult to write, therefore, practitioners expect a well-tuned software to last long and provide optimized performance even when the hardware is upgraded. It may also be necessary to write software using sufficient abstraction over the hardware so that it is capable of running on heterogeneous architecture. A good design abstraction paradigm strikes a balance between the abstraction and visibility over the hardware. This allows the programmer to write applications without having to understand the hardware nuances while exploiting the computing power optimally. In this paper we have analyzed the power of design abstraction of a popular design abstraction framework called Thrust both from ease of programming and performance perspectives. We have shown that while Thrust framework is good in describing an algorithm compared to the native CUDA or\u00a0\u2026", "num_citations": "1\n", "authors": ["1213"]}
{"title": "Uptime-Optimized Cloud Architecture as a Brokered Service\n", "abstract": " Enterprise workloads usually call for an uptime service level agreement (SLA) at the pain of contractual penalty in the event of slippage. Often, the strategy is to introduce ad-hoc HA (High Availability) mechanisms in response. Implemented solutions that we surveyed do not mathematically map their availability model to the required uptime SLA and to any expected penalty payout. In most client cases that we observed, this either resulted in an over-engineered solution that had more redundancies than was required, or in an inadequate solution that could potentially slip on the system uptime SLA stipulated in the contract. In this paper, we propose a framework backed by a model, to automatically determine the HA-enabled solution with the least TCO (total cost of ownership) for a given uptime SLA and slippage penalty. We attempt to establish that our work is best implemented as a brokered service that recommends\u00a0\u2026", "num_citations": "1\n", "authors": ["1213"]}
{"title": "An End-to-end Formal Verifier for Parallel Programs.\n", "abstract": " Among the various models of computation (MoCs) which have been used to model parallel programs, Petri net has been one of the mostly adopted MoC. The traditional Petri net model is extended into the PRES+ model which is specially equipped to precisely represent parallel programs running on heterogeneous and embedded systems. With the inclusion of multicore and multiprocessor systems in the domain of embedded systems, it has become important to validate the optimizing and parallelizing transformations which system specifications go through before deployment. Although PRES+ model based equivalence checkers for validating such transformations already exist, construction of the PRES+ models from the original and the translated programs was carried out manually in these equivalence checkers, thereby leaving scope for inaccurate representation of the programs due to human intervention. Furthermore, PRES+ model tends to grow more rapidly with the program size when compared to other MoCs, such as FSMD. To alleviate these drawbacks, we propose a method for automated construction of PRES+ models from high-level language programs and use an existing translation scheme to convert PRES+ models to FSMD models to validate the transformations using a state-of-the-art FSMD equivalence checker. Thus, we have composed an end-to-end fully automated equivalence checker for validating optimizing and parallelizing transformations as demonstrated by our experimental results.", "num_citations": "1\n", "authors": ["1213"]}
{"title": "Execution profile driven speedup estimation for porting sequential code to gpu\n", "abstract": " Parallelization of an existing sequential application to achieve a good speed-up on a data-parallel infrastructure is quite difficult and time consuming effort. One of the important steps towards this is to assess whether the existing application in its current form can be parallelized to get the desired speedup. In this paper, we propose a method of analyzing an existing sequential source code that contains data-parallel loops, and give a reasonably accurate prediction of the extent of speedup possible from this algorithm. The proposed method performs static and dynamic analysis of the sequential source code to determine the time required by various portions of the code, including the data-parallel portions. Subsequently, it uses a set of novel invariants to calculate various bottlenecks that exists if the program is to be transferred to a GPGPU platform and predicts the extent of parallelization necessary by the GPU in order\u00a0\u2026", "num_citations": "1\n", "authors": ["1213"]}
{"title": "Point of sale vulnerabilities: Solution approach\n", "abstract": " Point of Sale (PoS) applications run on the sales counters of many retail stores across the world. Such applications often run on Microsoft windows based systems. in recent times these applications are increasingly being targeted by malware to steal customer information. the recent attack on target\u2019s PoS system is a case in point where large number of customer credit card data was compromised. this document attempts to analyze the root cause of such attacks and explores possible solution approaches.", "num_citations": "1\n", "authors": ["1213"]}
{"title": "How many researchers does it take to make impact? mining software engineering publication data for collaboration insights\n", "abstract": " In the three and half decades since the inception of organized research publication in software engineering, the discipline has gained a significant maturity. This journey to maturity has been guided by the synergy of ideas, individuals and interactions. In this journey software engineering has evolved into an increasingly empirical discipline. Empirical sciences involve significant collaboration, leading to large teams working on research problems. In this paper we analyze a corpus of 19,000+ papers, written by 21,000+ authors from 16 publication venues between 1975 to 2010, to understand what is the ideal team size that has produced maximum impact in software engineering research, and whether researchers in software engineering have maintained the same co-authorship relations over long periods of time as a means of achieving research impact.", "num_citations": "1\n", "authors": ["1213"]}
{"title": "Identifying hotspots in a program for data parallel architecture: an early experience\n", "abstract": " In applications that rely on data intensive computation, one can gain significant performance if the source code is suitably transformed for parallel hardware. A common approach is to identify the loops inside the program that consume a significant amount of time, that we call hotspots. One of the impending business need here is to quickly identify such loops for further transformation. However, the exact identification of such hotspots requires an elaborate runtime analysis. When we deal with a third party business application, only a partial version of the source code is available, with limited test inputs, which hinders a correct runtime analysis. Therefore, we resort to static analysis of source code to get a conservative loop iteration count. In this paper we describe our approach to analyze a source code to find hotspots. Our approach is based on estimating the iteration count of a loop using the polytope model for\u00a0\u2026", "num_citations": "1\n", "authors": ["1213"]}
{"title": "SAM: a tool for software architecture modeling & performance analysis\n", "abstract": " This paper presents SAM (software architecture modeling & analysis), a set of tools that capture the workload model, performance objectives, and helps the architect to identify an appropriate architecture of a software intensive system. To begin with, the architect captures the AS-IS architecture with performance constraints. This is assessed by building a layered queuing network (LQN) and solved using discrete event simulation (DES). The tool then allows the architects to apply a set of architectural tactics and reassess the architecture. This helps the architect to perform a series of successive refinements iteratively and eventually move towards an appropriate architecture that meets the performance objectives.", "num_citations": "1\n", "authors": ["1213"]}
{"title": "An Integrated Modeling Approach to Enterprise Systems Architecture\n", "abstract": " The architecture of a system is a specification that captures the structure and functionality of the system in an abstract manner. The specification should help various stakeholders to understand and analyze different functional and quality of service (QoS) aspects of the system before its construction, without providing overwhelming details. In most software projects, the architecture specification of an enterprise system is captured using informal box and line diagrams with textual annotations. The informal architecture specification is transformed to implementation through a manual development process where the specification is hypothetically related to the implementation (through manual verification). Such a description suffers from people-and project-specific idiosyncrasies leading to ambiguity, misinterpretation and often, erroneous implementation. Therefore, it is essential to capture architecture specification using\u00a0\u2026", "num_citations": "1\n", "authors": ["1213"]}