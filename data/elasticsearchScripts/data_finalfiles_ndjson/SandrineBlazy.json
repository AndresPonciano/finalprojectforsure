{"title": "Separation Logic for Small-Step cminor\n", "abstract": " cminor is a mid-level imperative programming language; there are proved-correct optimizing compilers from C to cminor and from cminor to machine language. We have redesigned cminor so that it is suitable for Hoare Logic reasoning and we have designed a Separation Logic for cminor. In this paper, we give a small-step semantics (instead of the big-step of the proved-correct compiler) that is motivated by the need to support future concurrent extensions. We detail a machine-checked proof of soundness of our Separation Logic. This is the first large-scale machine-checked proof of a Separation Logic w.r.t. a small-step semantics. The work presented in this paper has been carried out in the Coq proof assistant. It is a first step towards an environment in which concurrent cminor programs can be verified using Separation Logic and also compiled by a proved-correct compiler with formal end-to-end\u00a0\u2026", "num_citations": "130\n", "authors": ["1761"]}
{"title": "Reuse of specification patterns with the B method\n", "abstract": " This paper describes an approach for reusing specification patterns. Specification patterns are design patterns that are expressed in a formal specification language. Reusing a specification pattern means instantiating it or composing it with other specification patterns. Three levels of composition are defined: juxtaposition, composition with inter-patterns links and unification. This paper shows through examples how to define specification patterns in B, how to reuse them directly in B, and also how to reuse the proofs associated with specification patterns.", "num_citations": "51\n", "authors": ["1761"]}
{"title": "Software maintenance: an analysis of industrial needs and constraints\n", "abstract": " The results are given of a series of case studies conducted at different industrial sites in the framework of the ESF/EPSOM (Eureka Software Factory/European Platform for Software Maintenance) project. The approach taken in the case studies was to directly contact software maintainers and obtain their own view of their activity, mainly through the use of interactive methods based on group work. This approach is intended to complement statistical studies which can be found in the literature, by presenting the perspective of the maintainers based on their experience. The aim of these studies has been to gain a better understanding of maintenance needs and constraints, and to highlight directions which could lead to improvements in the quality and efficiency of maintenance activities. The results obtained tend to conform the main preoccupations of the maintenance community, with an emphasis on two types of needs which appear crucial in the domains of activity of the partners, namely: the transfer, preservation and maintenance of knowledge; and the mastering of the maintenance process", "num_citations": "36\n", "authors": ["1761"]}
{"title": "A precise and abstract memory model for C using symbolic values\n", "abstract": " Real life C programs are often written using C dialects which, for the ISO C standard, have undefined behaviours. In particular, according to the ISO C standard, reading an uninitialised variable has an undefined behaviour and low-level pointer operations are implementation defined. We propose a formal semantics which gives a well-defined meaning to those behaviours for the C dialect of the CompCert compiler. Our semantics builds upon a novel memory model leveraging a notion of symbolic values. Symbolic values are used by the semantics to delay the evaluation of operations and are normalised lazily to genuine values when needed. We show that the most precise normalisation is computable and that a slightly relaxed normalisation can be efficiently implemented using an SMT solver. The semantics is executable and our experiments show that the enhancements of our semantics are mandatory to\u00a0\u2026", "num_citations": "28\n", "authors": ["1761"]}
{"title": "Formal verification of coalescing graph-coloring register allocation\n", "abstract": " Iterated Register Coalescing (IRC) is a widely used heuristic for performing register allocation via graph coloring. Many implementations in existing compilers follow (more or less faithfully) the imperative algorithm published in 1996. Several mistakes have been found in some of these implementations.             In this paper, we present a formal verification (in Coq) of the whole IRC algorithm. We detail a specification that can be used as a reference for IRC. We also define the theory of register-interference graphs; we implement a purely functional version of the IRC algorithm, and we prove the total correctness of our implementation. The automatic extraction of our IRC algorithm into Caml yields a program with competitive performance. This work has been integrated into the CompCert verified compiler.", "num_citations": "26\n", "authors": ["1761"]}
{"title": "Structuring abstract interpreters through state and value abstractions\n", "abstract": " We present a new modular way to structure abstract interpreters. Modular means that new analysis domains may be plugged-in. These abstract domains can communicate through different means to achieve maximal precision. First, all abstractions work cooperatively to emit alarms that exclude the undesirable behaviors of the program. Second, the state abstract domains may exchange information through abstractions of the possible value for expressions. Those value abstractions are themselves extensible, should two domains require a novel form of cooperation. We used this approach to design , an abstract interpreter for C implemented within the  framework. We present the domains that are available so far within , and show that this communication mechanism is able to handle them seamlessly.", "num_citations": "23\n", "authors": ["1761"]}
{"title": "CompCertS: A Memory-Aware Verified C Compiler Using a Pointer as Integer Semantics\n", "abstract": " The CompCert C compiler provides the formal guarantee that the observable behaviour of the compiled code improves on the observable behaviour of the source code. In this paper, we present a formally verified C compiler, CompCertS, which is essentially the CompCert compiler, albeit with a stronger formal guarantee: it gives a semantics to more programs and ensures that the memory consumption is preserved by the compiler. CompCertS is based on an enhanced memory model where, unlike CompCert but like Gcc, the binary representation of pointers can be manipulated much like integers and where, unlike CompCert, allocation may fail if no memory is available. The whole proof of CompCertS is a significant proof-effort and we highlight the crux of the novel proofs of 12 passes of the back-end and a challenging proof of an essential optimising pass of the front-end.", "num_citations": "21\n", "authors": ["1761"]}
{"title": "Partial evaluation for the understanding of Fortran programs\n", "abstract": " This paper describes a technique and a tool that support partial evaluation of FORTRAN programs, i.e., their specialization for specific values of their input variables. The authors\u2019 aim is to understand old programs, which have become very complex due to numerous extensions. From a given FORTRAN program and these values of its input variables, the tool provides a simplified program, which behaves like the initial program for the specific values. This tool mainly uses constant propagation and simplification of alternatives to one of their branches. The tool is specified in terms of inference rules and operates by induction on the FORTRAN abstract syntax. These rules are compiled into Prolog by the Centaur/FORTRAN programming environment. The completeness and soundness of these rules are proven using rule induction.", "num_citations": "21\n", "authors": ["1761"]}
{"title": "A concrete memory model for CompCert\n", "abstract": " Semantics preserving compilation of low-level C programs is challenging because their semantics is implementation defined according to the C standard. This paper presents the proof of an enhanced and more concrete memory model for the CompCert C compiler which assigns a definite meaning to more C programs. In our new formally verified memory model, pointers are still abstract but are nonetheless mapped to concrete 32-bit integers. Hence, the memory is finite and it is possible to reason about the binary encoding of pointers. We prove that the existing memory model is an abstraction of our more concrete model thus validating formally the soundness of CompCert\u2019s abstract semantics of pointers. We also show how to adapt the front-end of CompCert thus demonstrating that it should be feasible to port the whole compiler to our novel memory model.", "num_citations": "20\n", "authors": ["1761"]}
{"title": "Formal verification of control-flow graph flattening\n", "abstract": " Code obfuscation is emerging as a key asset in security by obscurity. It aims at hiding sensitive information in programs so that they become more difficult to understand and reverse engineer. Since the results on the impossibility of perfect and universal obfuscation, many obfuscation techniques have been proposed in the literature, ranging from simple variable encoding to hiding the control-flow of a program. In this paper, we formally verify in Coq an advanced code obfuscation called control-flow graph flattening, that is used in state-of-the-art program obfuscators. Our control-flow graph flattening is a program transformation operating over C programs, that is integrated into the CompCert formally verified compiler. The semantics preservation proof of our program obfuscator relies on a simulation proof performed on a realistic language, the Clight language of CompCert. The automatic extraction of our program\u00a0\u2026", "num_citations": "19\n", "authors": ["1761"]}
{"title": "SFAC, a tool for program comprehension by specialization\n", "abstract": " This paper describes a tool for facilitating the comprehension of general programs using automatic specialization. The goal of this approach was to assist in the maintenance of old programs, which have become very complex due to numerous extensions. This paper explains why this approach was chosen, how the tool's architecture was set up, and how the correctness of the specialization has been proved. Then, it discusses the results obtained by using this tool, and the future evolutions.< >", "num_citations": "14\n", "authors": ["1761"]}
{"title": "Formal specification and prototyping of a program specializer\n", "abstract": " This paper reports on the use of formal specifications in the development of a software maintenance tool for specializing imperative programs, which have become very complex due to extensive modifications. The tool is specified in terms of inference rules and operates by induction on the abstract syntax. The correctness of these rules is proved using rule induction. A Prolog prototype has been derived for Fortran programs, using the Centaur programming environment.", "num_citations": "13\n", "authors": ["1761"]}
{"title": "Partial evaluation as an aid to the comprehension of Fortran programs\n", "abstract": " The authors describe a technique and a tool supporting partial evaluation of Fortran programs, i.e. their specialization for specific values of their input variables. They aim at understanding old programs, which have become very complex due to numerous extensions. From a given Fortran program and these values of its input variables, the tool provides a simplified program, which behaves like the initial one for the specific values. This tool uses mainly constant propagation and simplification of alternatives to one of their branches. The tool is specified in inference rules and operates by induction on the Fortran abstract syntax. These rules are compiled into Prolog by the Centaur/Fortran environment.< >", "num_citations": "13\n", "authors": ["1761"]}
{"title": "A verified CompCert front-end for a memory model supporting pointer arithmetic and uninitialised data\n", "abstract": " The CompCert C compiler guarantees that the target program behaves as the source program. Yet, source programs without a defined semantics do not benefit from this guarantee and could therefore be miscompiled. To reduce the possibility of a miscompilation, we propose a novel memory model for CompCert which gives a defined semantics to challenging features such as bitwise pointer arithmetics and access to uninitialised data. We evaluate our memory model both theoretically and experimentally. In our experiments, we identify pervasive low-level C idioms that require the additional expressiveness provided by our memory model. We also show that our memory model provably subsumes the existing CompCert memory model thus cross-validating both semantics. Our memory model relies on the core concepts of symbolic value and normalisation. A symbolic value models a delayed computation\u00a0\u2026", "num_citations": "12\n", "authors": ["1761"]}
{"title": "Towards a formally verified obfuscating compiler\n", "abstract": " This paper extends the idea of specializing modified interpreters for systematically generating obfuscated code. By using the Coq proof assistant we specify some elementary obfuscations and prove that the resulting distorted interpreter is correct, namely it preserves the intended semantics of programs. The paper shows how the semantic preservation proofs generated and verified in Coq can provide a measure of the quality of the obfuscation. In particular we can observe that there is a precise corresponding between the potency of the obfuscation and the complexity of the proof of semantics preservation. Our obfuscation can be easily integrated into the CompCert C compiler, providing the basis for a formally verified obfuscating compiler which can be applied to any C program.", "num_citations": "11\n", "authors": ["1761"]}
{"title": "Partial evaluation for program comprehension\n", "abstract": " Program comprehension is the most tedious and time consuming task of software maintenance, an important phase of the software life cycle [A. Frazer 1992]. This is particularly true while maintaining scientific application programs that have been written in Fortran for decades and that are still vital in various domains even though more modern languages are used to implement their user interfaces. Very often, programs have evolved as their application domains increase continually and have become very complex due to extensive modifications. This generality in programs is implemented by input variables whose value does not vary in the context of a given application. Thus, it is very interesting for the maintainer to propagate such information, that is to obtain a simplified program, which behaves like the initial one when used according to the restriction. We have adapted partial evaluation for program\u00a0\u2026", "num_citations": "11\n", "authors": ["1761"]}
{"title": "Experiments in validating formal semantics for C\n", "abstract": " This paper reports on the design of adequate on-machine formal semantics for a certified C compiler. This compiler is an optimizing compiler, that targets critical embedded software. It is written and formally verified using the Coq proof assistant. The main structure of the compiler is very strongly conditioned by the choice of the languages of the compiler, and also by the kind of semantics of these languages.", "num_citations": "10\n", "authors": ["1761"]}
{"title": "Specifying and automatically generating a specialization tool for Fortran 90\n", "abstract": " Partial evaluation is an optimization technique traditionally used in compilation. We have adapted this technique to the understanding of scientific application programs during their maintenance. We have implemented a tool that analyzes Fortran 90 application programs and performs an interprocedural pointer analysis. This paper presents a dynamic semantics of Fortran 90 and manually derives a partial evaluator from this semantics. The tool implementing the specifications is also detailed. The partial evaluator has been implemented in a generic programming environment and a graphical interface has been developed to visualize the information computed during the partial evaluation (values of variables, already analyzed procedures, scope of variables, removed statements, etc.).", "num_citations": "10\n", "authors": ["1761"]}
{"title": "Compiling sandboxes: Formally verified software fault isolation\n", "abstract": " Software Fault Isolation (SFI) is a security-enhancing program transformation for instrumenting an untrusted binary module so that it runs inside a dedicated isolated address space, called a sandbox. To ensure that the untrusted module cannot escape its sandbox, existing approaches such as Google\u2019s Native Client rely on a binary verifier to check that all memory accesses are within the sandbox. Instead of relying on a posteriori verification, we design, implement and prove correct a program instrumentation phase as part of the formally verified compiler CompCert that enforces a sandboxing security property a priori. This eliminates the need for a binary verifier and, instead, leverages the soundness proof of the compiler to prove the security of the sandboxing transformation. The technical contributions are a novel sandboxing transformation that has a well-defined C semantics and which supports arbitrary function pointers, and a formally verified C compiler that implements SFI. Experiments show that our formally verified technique is a competitive way of implementing SFI.", "num_citations": "8\n", "authors": ["1761"]}
{"title": "Formal verification of a program obfuscation based on mixed boolean-arithmetic expressions\n", "abstract": " The insertion of expressions mixing arithmetic operators and bitwise boolean operators is a widespread protection of sensitive data in source programs. This recent advanced obfuscation technique is one of the less studied among program obfuscations even if it is commonly found in binary code. In this paper, we formally verify in Coq this data obfuscation. It operates over a generic notion of mixed boolean-arithmetic expressions and on properties of bitwise operators operating over machine integers. Our obfuscation performs two kinds of program transformations: rewriting of expressions and insertion of modular inverses. To facilitate its proof of correctness, we define boolean semantic tables, a data structure inspired from truth tables.", "num_citations": "7\n", "authors": ["1761"]}
{"title": "Live-range unsplitting for faster optimal coalescing\n", "abstract": " Register allocation is often a two-phase approach: spilling of registers to memory, followed by coalescing of registers. Extreme live-range splitting (i.e. live-range splitting after each statement) enables optimal solutions based on ILP, for both spilling and coalescing. However, while the solutions are easily found for spilling, for coalescing they are more elusive. This difficulty stems from the huge size of interference graphs resulting from live-range splitting. This paper focuses on coalescing in the context of extreme live-range splitting. It presents some theoretical properties that give rise to an algorithm for reducing interference graphs. This reduction consists mainly in finding and removing useless splitting points. It is followed by a graph decomposition based on clique separators. The reduction and decomposition are general enough, so that any coalescing algorithm can be applied afterwards. Our strategy for reducing\u00a0\u2026", "num_citations": "7\n", "authors": ["1761"]}
{"title": "An automatic interprocedural analysis for the understanding of scientific application programs\n", "abstract": " This paper reports on an approach for improving the understanding of old programs which have become very complex due to numerous extensions. We have adapted partial evaluation techniques for program understanding. These techniques mainly use propagation through statements and simplifications of statements.             We focus here on the automatic interprocedural analysis and we specify both tasks (propagation and simplification) for call-statements, in terms of inference rules with notations taken from the formal specification languages B and VDM.             We describe how we have implemented that interprocedural analysis in a tool, and how it can be used to improve program understanding. The difficulty of that analysis is due to the lack of well defined interprocedural mechanisms and the complexity of visibility rules in Fortran.", "num_citations": "7\n", "authors": ["1761"]}
{"title": "Proofs you can believe in: proving equivalences between prolog semantics in Coq\n", "abstract": " Basing program analyses on formal semantics has a long and successful tradition in the logic programming paradigm. These analyses rely on results about the relative correctness of mathematically sophisticated semantics, and authors of such analyses often invest considerable effort into establishing these results. The development of interactive theorem provers such as Coq and their recent successes both in the field of program verification as well as in mathematics, poses the question whether these tools can be usefully deployed in logic programming. This paper presents formalisations in Coq of several general results about the correctness of semantics in different styles; forward and backward, top-down and bottom-up. The results chosen are paradigmatic of the kind of correctness theorems that semantic analyses rely on and are therefore well-suited to explore the possibilities afforded by the application of\u00a0\u2026", "num_citations": "6\n", "authors": ["1761"]}
{"title": "Application of formal methods to the development of a software maintenance tool\n", "abstract": " Partial evaluation is an optimization technique traditionally used in compilation. We have adapted this technique to the understanding of scientific application programs during their maintenance, and we have implemented a tool that analyzes Fortran 90 application programs and performs an interprocedural pointer analysis. This paper presents how we have specified this analysis with different formalisms (inference rules with global definitions and set and relational operators). Then we present the tool implementing these specifications. It has been implemented in a generic programming environment and a graphical interface has been developed to visualize the information computed during the partial evaluation (values of variables, already-analyzed procedures, scope of variables, removed statements, etc.).", "num_citations": "6\n", "authors": ["1761"]}
{"title": "S\u00e9mantiques formelles\n", "abstract": " Ce m\u00e9moire pr\u00e9sente plusieurs d\u00e9finitions de s\u00e9mantiques formelles et de transformations de programmes, et expose les choix de conception associ\u00e9s. En particulier, ce m\u00e9moire d\u00e9crit une transformation de programmes inspir\u00e9e de l'\u00e9valuation partielle et d\u00e9di\u00e9e \u00e0 la compr\u00e9hension de programmes scientifiques \u00e9crits en Fortran. Il d\u00e9taille \u00e9galement le front-end d'un compilateur r\u00e9aliste du langage C, ayant \u00e9t\u00e9 formellement v\u00e9rifi\u00e9 en C.", "num_citations": "5\n", "authors": ["1761"]}
{"title": "V\u00e9rification formelle d'un algorithme d'allocation de registres par coloration de graphe\n", "abstract": " Le travail pr\u00e9sent\u00e9 dans cet article est \u00e0 l'interface entre la recherche op\u00e9rationnelle et les m\u00e9thodes formelles. Il s'inscrit dans le cadre du projet CompCert ayant pour but le d\u00e9veloppement et la v\u00e9rification formelle, utilisant l'assistant de preuve Coq, d'un compilateur du langage C potentiellement utilisable pour la production de logiciels embarqu\u00e9s critiques. Nous nous int\u00e9ressons dans cet article \u00e0 l'allocation de registres, qui consiste \u00e0 optimiser l'utilisation des registres du processeur. Nous proposons d'aborder cette optimisation en la mod\u00e9lisant par un probl\u00e8me dit de coloration avec pr\u00e9f\u00e9rences dont nous v\u00e9rifions formellement la r\u00e9solution. Cette v\u00e9rification prend deux formes : preuve de correction de la sp\u00e9cification Coq pour la premi\u00e8re partie de l'algorithme et validation a posteriori pour la seconde.", "num_citations": "5\n", "authors": ["1761"]}
{"title": "Improving static analyses of C programs with conditional predicates\n", "abstract": " Static code analysis is increasingly used to guarantee the absence of undesirable behaviors in industrial programs. Designing sound analyses is a continuing trade-off between precision and complexity. Notably, dataflow analyses often perform overly wide approximations when two control-flow paths meet, by merging states from each path.This paper presents a generic abstract interpretation based framework to enhance the precision of such analyses on join points. It relies on predicated domains, that preserve and reuse information valid only inside some branches of the code. Our predicates are derived from conditional statements, and postpone the loss of information.The work has been integrated into Frama-C, a C source code analysis platform. Experiments on real generated code show that our approach scales, and improves significantly the precision of the existing analyses of Frama-C.", "num_citations": "4\n", "authors": ["1761"]}
{"title": "Data tainting and obfuscation: Improving plausibility of incorrect taint\n", "abstract": " Code obfuscation is designed to impede the reverse engineering of a binary software. Dynamic data tainting is an analysis technique used to identify dependencies between data in a software. Performing dynamic data tainting on obfuscated software usually yields hard to exploit results, due to over-tainted data. Such results are clearly identifiable as useless: an attacker will immediately discard them and opt for an alternative tool. In this paper, we present a code transformation technique meant to prevent the identification of useless results: a few lines of code are inserted in the obfuscated software, so that the results obtained by the dynamic data tainting approach appear acceptable. These results remain however wrong and lead an attacker to waste enough time and resources trying to analyze incorrect data dependencies, so that he will usually decide to use less automated and advanced analysis techniques\u00a0\u2026", "num_citations": "3\n", "authors": ["1761"]}
{"title": "Improving static analyses of C programs with conditional predicates\n", "abstract": " Static code analysis is increasingly used to guarantee the absence of undesirable behaviors in industrial programs. Designing sound analyses is a continuing trade-off between precision and complexity. Notably, dataflow analyses often perform overly wide approximations when two control-flow paths meet, by merging states from each path. This paper presents a generic abstract interpretation based framework to enhance the precision of such analyses on join points. It relies on predicated domains, that preserve and reuse information valid only inside some branches of the code. Our predicates are derived from conditionals statements, and postpone the loss of information. The work has been integrated into Frama-C, a C source code analysis platform. Experiments on real code show that our approach scales, and improves significantly the precision of the existing analyses of Frama-C.", "num_citations": "3\n", "authors": ["1761"]}
{"title": "Teaching Deductive Verification in Why3 to Undergraduate Students\n", "abstract": " We present the contents of a new formal methods course taught to undergraduate students in their third year at the University of Rennes 1 in France. This course aims at initiating students to formal methods, using the Why3 platform for deductive verification. It exposes students to several techniques, ranging from testing specifications, designing loop invariants, building adequate data structures and their type invariants, to the use of ghost code. At the end of the course, most of the students were able to prove correct in an automated way non-trivial sorting algorithms, as well as standard recursive algorithms on binary search trees.", "num_citations": "2\n", "authors": ["1761"]}
{"title": "Measuring the robustness of source program obfuscation: Studying the impact of compiler optimizations on the obfuscation of c programs\n", "abstract": " Obfuscation is a commonly used technique to protect software from the reverse engineering process. Advanced obfuscations usually rely on semantic properties of programs and thus may be performed on source programs. This raises the question of how to be sure that the binary code (that is effectively running) is still obfuscated.", "num_citations": "2\n", "authors": ["1761"]}
{"title": "Separation logic for small-step Cminor (extended version)\n", "abstract": " Cminor is a mid-level imperative programming language (just below C), and there exist proved-correct optimizing compilers from C to Cminor and from Cminor to machine language. We have redesigned Cminor so that it is suitable for Hoare Logic reasoning, we have designed a Separation Logic for Cminor, we have given a small-step operational semantics so that extensions to concurrent Cminor will be possible, and we have a machine-checked proof of soundness of our Separation Logic. This is the first large-scale machine-checked proof of a Separation Logic w.r.t. a small-step semantics, or for a language with nontrivial reducible control-flow constructs. Our sequential soundness proof of the sequential Separation Logic for the sequential language features will be reusable change within a soundness proof of Concurrent Separation Logic w.r.t. Concurrent Cminor. In addition, we have a machine-checked proof of the relation between our small-step semantics and Leroy's original big-step semantics; thus sequential programs can be compiled by Leroy's compiler with formal end-to-end correctness guarantees.", "num_citations": "2\n", "authors": ["1761"]}
{"title": "Interprocedural analysis for program comprehension by specialization\n", "abstract": " We report on an approach for program comprehension during large-scale maintenance of Fortran application programs. We have adapted partial evaluation (or specialization of programs given specific values for their input data) techniques for program comprehension. We focus on the automatic interprocedural analysis and we describe how we have specified, implemented in a tool, and used that analysis to improve program comprehension. The difficulty of that analysis is due to the lack of well defined interprocedural mechanisms and the complexity of visibility rules in Fortran.", "num_citations": "2\n", "authors": ["1761"]}
{"title": "Static Analysis: 22nd International Symposium, SAS 2015, Saint-Malo, France, September 9-11, 2015, Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the 22nd International Static Analysis Symposium, SAS 2015, held in Saint-Malo, France, in September 2015. The 18 papers presented in this volume were carefully reviewed and selected from 44 submissions. All fields of static analysis as a fundamental tool for program verification, bug detection, compiler optimization, program understanding, and software maintenance are addressed, featuring theoretical, practical, and application advances in the area", "num_citations": "1\n", "authors": ["1761"]}
{"title": "Coloration avec pr\u00e9f\u00e9rences: complexit\u00e9, in\u00e9galit\u00e9s valides et v\u00e9rification formelle\n", "abstract": " Archive ouverte HAL - Coloration avec pr\u00e9f\u00e9rences : complexit\u00e9, in\u00e9galit\u00e9s valides et v\u00e9rification formelle Acc\u00e9der directement au contenu Acc\u00e9der directement \u00e0 la navigation Toggle navigation CCSD HAL HAL HALSHS TEL M\u00e9diHAL Liste des portails AUR\u00e9HAL API Data Documentation Episciences.org Episciences.org Revues Documentation Sciencesconf.org Support hal Accueil D\u00e9p\u00f4t Consultation Les derniers d\u00e9p\u00f4ts Par type de publication Par discipline Par ann\u00e9e de publication Par structure de recherche Les portails de l'archive Recherche Documentation hal-01125409, version 1 Communication dans un congr\u00e8s Coloration avec pr\u00e9f\u00e9rences : complexit\u00e9, in\u00e9galit\u00e9s valides et v\u00e9rification formelle Sandrine Blazy 1 Benoit Robillard 1 Eric Soutif 1 D\u00e9tails 1 CEDRIC - Centre d'\u00e9tudes et de recherche en informatique et communications Type de document : Communication dans un congr\u00e8s Domaine : \u2026", "num_citations": "1\n", "authors": ["1761"]}
{"title": "Partial evaluation and symbolic computation for the understanding of Fortran programs\n", "abstract": " We describe a technique and a tool supporting partial evaluation of Fortran programs, i.e. their specialization for specific values of their input variables. We aim at understanding old programs, which have become very complex due to numerous extensions. From a given Fortran program and these values of its input variables, the tool provides a simplified program, which behaves like the initial one for the specific values. This tool uses mainly constant propagation and simplification of alternatives to one of their branches. The tool is specified in inference rules and operates by induction on the Fortran abstract syntax. These rules are compiled into Prolog by the Centaur/Fortran environment.", "num_citations": "1\n", "authors": ["1761"]}
{"title": "La sp\u00e9cialisation de programmes pour l'aide \u00e0 la maintenance du logiciel\n", "abstract": " L'objectif de cette these est d'etudier les modeles d'execution de la maintenance du logiciel, de concevoir une methode et un outil d'aide a la comprehension de programmes utilisant en particulier la simplification par specialisation, et de valider les resultats au travers d'exemples issus de reelles applications scientifiques ecrites en fortran. Nous avons utilise l'evaluation partielle (ou specialisation de programmes en fonction de proprietes de leurs donnees d'entree) pour des programmes difficiles a maintenir car trop generaux. A partir d'un programme fortran initial et de valeurs particulieres de certaines de ces donnees d'entree, l'outil fournit-entierement automatiquement-un programme simplifie qui se comporte comme le programme initial pour les valeurs particulieres. Ces programmes sont obtenus en propageant les valeurs constantes et en simplifiant le code. Cette technique aide le mainteneur a comprendre le comportement d'un programme dans un contexte particulier. Concernant son apport scientifique, la technique employee n'est pas nouvelle, mais la maniere dont nous l'avons utilisee (pour l'aide a la comprehension), specifie (en semantique naturelle), prouvee (par induction) et implantee (dans l'environnement centaur) est a notre connaissance originale", "num_citations": "1\n", "authors": ["1761"]}
{"title": "Measuring the robustness of source program obfuscation\n", "abstract": " Obfuscation is a commonly used technique to protect software from the reverse engineering process. Advanced obfuscations usually rely on semantic properties of programs and thus may be performed on source programs. This raises the question of how to be sure that the binary code (that is effectively running) is still obfuscated. This paper presents a data obfuscation of C programs and a methodology to evaluate how the obfuscation resists to the GCC compiler. Information generated by the compiler (including effects of relevant optimizations that could deobfuscate programs) and a study of the disassembled binary code, as well as a dynamic analysis of the performances of binary code show that our obfuscation is worthwhile.", "num_citations": "1\n", "authors": ["1761"]}