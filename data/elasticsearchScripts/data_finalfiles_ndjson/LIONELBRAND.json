{"title": "A validation of object-oriented design metrics as quality indicators\n", "abstract": " This paper presents the results of a study in which we empirically investigated the suite of object-oriented (OO) design metrics introduced in (Chidamber and Kemerer, 1994). More specifically, our goal is to assess these metrics as predictors of fault-prone classes and, therefore, determine whether they can be used as early quality indicators. This study is complementary to the work described in (Li and Henry, 1993) where the same suite of metrics had been used to assess frequencies of maintenance changes to classes. To perform our validation accurately, we collected data on the development of eight medium-sized information management systems based on identical requirements. All eight projects were developed using a sequential life cycle model, a well-known OO analysis/design method and the C++ programming language. Based on empirical and quantitative analysis, the advantages and drawbacks of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2412\n", "authors": ["24"]}
{"title": "A unified framework for coupling measurement in object-oriented systems\n", "abstract": " The increasing importance being placed on software measurement has led to an increased amount of research developing new software measures. Given the importance of object-oriented development techniques, one specific area where this has occurred is coupling measurement in object-oriented systems. However, despite a very interesting and rich body of work, there is little understanding of the motivation and empirical hypotheses behind many of these new measures. It is often difficult to determine how such measures relate to one another and for which application they can be used. As a consequence, it is very difficult for practitioners and researchers to obtain a clear picture of the state of the art in order to select or define measures for object-oriented systems. This situation is addressed and clarified through several different activities. First, a standardized terminology and formalism for expressing measures\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1177\n", "authors": ["24"]}
{"title": "Property-based software engineering measurement\n", "abstract": " Little theory exists in the field of software system measurement. Concepts such as complexity, coupling, cohesion or even size are very often subject to interpretation and appear to have inconsistent definitions in the literature. As a consequence, there is little guidance provided to the analyst attempting to define proper measures for specific problems. Many controversies in the literature are simply misunderstandings and stem from the fact that some people talk about different measurement concepts under the same label (complexity is the most common case). There is a need to define unambiguously the most important measurement concepts used in the measurement of software products. One way of doing so is to define precisely what mathematical properties characterize these concepts, regardless of the specific software artifacts to which these concepts are applied. Such a mathematical framework could\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1049\n", "authors": ["24"]}
{"title": "Exploring the relationships between design measures and software quality in object-oriented systems\n", "abstract": " One goal of this paper is to empirically explore the relationships between existing object-oriented (OO) coupling, cohesion, and inheritance measures and the probability of fault detection in system classes during testing. In other words, we wish to better understand the relationship between existing design measurement in OO systems and the quality of the software developed. The second goal is to propose an investigation and analysis strategy to make these kind of studies more repeatable and comparable, a problem which is pervasive in the literature on quality measurement. Results show that many of the measures capture similar dimensions in the data set, thus reflecting the fact that many of them are based on similar principles and hypotheses. However, it is shown that by using a subset of measures, accurate models can be built to predict which classes most of the faults are likely to lie in. When predicting fault\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "909\n", "authors": ["24"]}
{"title": "A unified framework for cohesion measurement in object-oriented systems\n", "abstract": " The increasing importance being placed on software measurement has led to an increased amount of research developing new software measures. Given the importance of object-oriented development techniques, one specific area where this has occurred is cohesion measurement in object-oriented systems. However, despite a very interesting body of work, there is little understanding of the motivation and empirical hypotheses behind many of these new measures. It is often difficult to determine how such measures relate to one another and for which application they can be used. As a consequence, it is very difficult for practitioners and researchers to obtain a clear picture of the state-of-the-art in order to select or define cohesion measures for object-oriented systems. This situation is addressed and clarified through several different activities. First, a standardized terminology and formalism for expressing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "825\n", "authors": ["24"]}
{"title": "A practical guide for using statistical tests to assess randomized algorithms in software engineering\n", "abstract": " Randomized algorithms have been used to successfully address many different types of software engineering problems. This type of algorithms employ a degree of randomness as part of their logic. Randomized algorithms are useful for difficult problems where a precise solution cannot be derived in a deterministic way within reasonable time. However, randomized algorithms produce different results on every run when applied to the same problem instance. It is hence important to assess the effectiveness of randomized algorithms by collecting data from a large enough number of runs. The use of rigorous statistical tests is then essential to provide support to the conclusions derived by analyzing such data. In this paper, we provide a systematic review of the use of randomized algorithms in selected software engineering venues in 2009. Its goal is not to perform a complete survey but to get a representative snapshot\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "807\n", "authors": ["24"]}
{"title": "Using mutation analysis for assessing and comparing testing coverage criteria\n", "abstract": " The empirical assessment of test techniques plays an important role in software testing research. One common practice is to seed faults in subject software, either manually or by using a program that generates all possible mutants based on a set of mutation operators. The latter allows the systematic, repeatable seeding of large numbers of faults, thus facilitating the statistical analysis of fault detection effectiveness of test suites; however, we do not know whether empirical results obtained this way lead to valid, representative conclusions. Focusing on four common control and data flow criteria (block, decision, C-use, and P-use), this paper investigates this important issue based on a middle size industrial program with a comprehensive pool of test cases and known faults. Based on the data available thus far, the results are very consistent across the investigated criteria as they show that the use of mutation operators\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "543\n", "authors": ["24"]}
{"title": "A systematic review of the application and empirical investigation of search-based test case generation\n", "abstract": " Metaheuristic search techniques have been extensively used to automate the process of generating test cases, and thus providing solutions for a more cost-effective testing process. This approach to test automation, often coined \u0393\u00c7\u00a3Search-based Software Testing\u0393\u00c7\u00a5 (SBST), has been used for a wide variety of test case generation purposes. Since SBST techniques are heuristic by nature, they must be empirically investigated in terms of how costly and effective they are at reaching their test objectives and whether they scale up to realistic development artifacts. However, approaches to empirically study SBST techniques have shown wide variation in the literature. This paper presents the results of a systematic, comprehensive review that aims at characterizing how empirical studies have been designed to investigate SBST cost-effectiveness and what empirical evidence is available in the literature regarding SBST cost\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "521\n", "authors": ["24"]}
{"title": "An investigation into coupling measures for C++\n", "abstract": " This paper proposes a comprehensive suite of measures to quantify the level of class coupling during the design of object-oriented systems. This suite takes into account the different 00 design mechanisms provided by the C++ language (eg, friendship between classes, specialization, and aggregation) but it can be tailored to other 00 languages. The different measures in our suite thus reflect different hypotheses about the different mechanisms of coupling in 00 systems. Based on actual project defect data, the hypotheses underlying our coupling measures are empirically validated by analyzing their relationship with the probability of fault detection across classes. The results demonstrate that some of these coupling measures may be useful early quality indicators of the design'of 00 systems. These measures are conceptually different from the 00 design measures defined by Chidamber and Kemerer; in addition\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "512\n", "authors": ["24"]}
{"title": "Dynamic coupling measurement for object-oriented software\n", "abstract": " The relationships between coupling and external quality factors of object-oriented software have been studied extensively for the past few years. For example, several studies have identified clear empirical relationships between class-level coupling and class fault-proneness. A common way to define and measure coupling is through structural properties and static code analysis. However, because of polymorphism, dynamic binding, and the common presence of unused (\"dead\") code in commercial software, the resulting coupling measures are imprecise as they do not perfectly reflect the actual coupling taking place among classes at runtime. For example, when using static analysis to measure coupling, it is difficult and sometimes impossible to determine what actual methods can be invoked from a client class if those methods are overridden in the subclasses of the server classes. Coupling measurement has\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "503\n", "authors": ["24"]}
{"title": "A systematic and comprehensive investigation of methods to build and evaluate fault prediction models\n", "abstract": " This paper describes a study performed in an industrial setting that attempts to build predictive models to identify parts of a Java system with a high fault probability. The system under consideration is constantly evolving as several releases a year are shipped to customers. Developers usually have limited resources for their testing and would like to devote extra resources to faulty system parts. The main research focus of this paper is to systematically assess three aspects on how to build and evaluate fault-proneness models in the context of this large Java legacy system development project: (1) compare many data mining and machine learning techniques to build fault-proneness models, (2) assess the impact of using different metric sets such as source code structural measures and change/fault history (process measures), and (3) compare several alternative ways of assessing the performance of the models, in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "466\n", "authors": ["24"]}
{"title": "How reuse influences productivity in object-oriented systems\n", "abstract": " THIS article presents the results of a study conduct-ed at the University of Maryland in which we assessed the impact of reuse on quality and pro-ductivity in object-oriented (OO) systems. Reuse is assumed to be an effective strategy for building high-quality software. However, there is currently little empirical information about what to expect from reuse in terms of productivity and quality gains. The study is one step toward a better understanding of the benefits of reuse in an OO framework in light of currently available technology. Data was collected for four months\u0393\u00c7\u00f6September through December 1994\u0393\u00c7\u00f6on the development of eight small (less than 15,000 source lines of code [KSLOC]) systems with equivalent functional requirements. All eight projects were developed using the Waterfall-style Software Engineering Life Cycle Model, an OO design method, and the C++ programming language. The study found\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "426\n", "authors": ["24"]}
{"title": "A hitchhiker's guide to statistical tests for assessing randomized algorithms in software engineering\n", "abstract": " Randomized algorithms are widely used to address many types of software engineering problems, especially in the area of software verification and validation with a strong emphasis on test automation. However, randomized algorithms are affected by chance and so require the use of appropriate statistical tests to be properly analysed in a sound manner. This paper features a systematic review regarding recent publications in 2009 and 2010 showing that, overall, empirical analyses involving randomized algorithms in software engineering tend to not properly account for the random nature of these algorithms. Many of the novel techniques presented clearly appear promising, but the lack of soundness in their empirical evaluations casts unfortunate doubts on their actual usefulness. In software engineering, although there are guidelines on how to carry out empirical analyses involving human subjects, those\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "419\n", "authors": ["24"]}
{"title": "An assessment and comparison of common software cost estimation modeling techniques\n", "abstract": " This paper investigates two essential questions related to data-driven, software cost modeling: (1) What modeling techniques are likely to yield more accurate results when using typical software development cost data? and (2) What are the benefits and drawbacks of using organization-specific data as compared to multi-organization databases? The former question is important in guiding software cost analysts in their choice of the right type of modeling technique, if at all possible. In order to address this issue, we assess and compare a selection of common cost modeling techniques fulfilling a number of important criteria using a large multi-organizational database in the business application domain. Namely, these are: ordinary least squares regression, stepwise ANOVA, CART, and analogy. The latter question is important in order to assess the feasibility of using multi-organization cost databases to build cost\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "403\n", "authors": ["24"]}
{"title": "Defining and validating measures for object-based high-level design\n", "abstract": " The availability of significant measures in the early phases of the software development life-cycle allows for better management of the later phases, and more effective quality assessment when quality can be more easily affected by preventive or corrective actions. We introduce and compare various high-level design measures for object-based software systems. The measures are derived based on an experimental goal, identifying fault-prone software parts, and several experimental hypotheses arising from the development of Ada systems for Flight Dynamics Software at the NASA Goddard Space Flight Center (NASA/GSFC). Specifically, we define a set of measures for cohesion and coupling, which satisfy a previously published set of mathematical properties that are necessary for any such measures to be valid. We then investigate the measures' relationship to fault-proneness on three large scale projects, to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "400\n", "authors": ["24"]}
{"title": "Investigating quality factors in object-oriented designs: an industrial case study\n", "abstract": " This paper aims at empirically exploring the relationships between most of the existing coupling and cohesion measures for object-oriented (00) systems, and the faultproneness of 00 system classes. The underlying goal of such a study is to better understand the relationship between existing design measurement in 00 systems and the quality of the software developed.The study described here is a replication of an analogous stt. & conducted in an university environment with systems developed by students. In order to draw more general conclusions and to (dis) conJirm the results obtained there, we now replicated the study using data collected on an industrial system developed by professionals.", "num_citations": "394\n", "authors": ["24"]}
{"title": "Assessing the applicability of fault-proneness models across object-oriented software projects\n", "abstract": " A number of papers have investigated the relationships between design metrics and the detection of faults in object-oriented software. Several of these studies have shown that such models can be accurate in predicting faulty classes within one particular software product. In practice, however, prediction models are built on certain products to be used on subsequent software development projects. How accurate can these models be, considering the inevitable differences that may exist across projects and systems? Organizations typically learn and change. From a more general standpoint, can we obtain any evidence that such models are economically viable tools to focus validation and verification effort? This paper attempts to answer these questions by devising a general but tailorable cost-benefit model and by using fault and design data collected on two mid-size Java systems developed in the same\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "389\n", "authors": ["24"]}
{"title": "A replicated assessment and comparison of common software cost modeling techniques\n", "abstract": " Delivering a software product on time, within budget, and to an agreed level of quality is a critical concern for many software organizations. Underestimating software costs can have detrimental effects on the quality of the delivered software and thus on a company's business reputation and competitiveness. On the other hand, overestimation of software cost can result in missed opportunities to funds in other projects. In response to industry demand, a myriad of estimation techniques has been proposed during the last three decades. In order to assess the suitability of a technique from a diverse selection, its performance and relative merits must be compared.", "num_citations": "332\n", "authors": ["24"]}
{"title": "Practical guidelines for measurement\u0393\u00c7\u00c9based process improvement\n", "abstract": " Despite significant progress in the last 15 years, implementing a successful measurement program for software development is still a challenging undertaking. Most problems are not of theoretical but of methodological or practical nature. In this article, we present lessons learned from experiences with goal\u0393\u00c7\u00c9oriented measurement. We structure them into practical guidelines for efficient and useful software measurement aimed at process improvement in industry. Issues related to setting measurement goals, defining explicit measurement models, and implementing data collection procedures are addressed from a practical perspective. In addition, guidelines for using measurement in the context of process improvement are provided. \u252c\u2310 1996 by John Wiley & Sons, Ltd. and Gauthier\u0393\u00c7\u00c9Villars", "num_citations": "313\n", "authors": ["24"]}
{"title": "A pattern recognition approach for software engineering data analysis\n", "abstract": " In order to understand, evaluate, predict, and control the software development process with regard to such perspectives as productivity, quality, and reusability, one needs to collect meaningful data and analyze them in an effective way. However, software engineering data have several inherent problems associated with them and the classical statistical analysis techniques do not address these problems very well. In this paper, we define a specific pattern recognition approach for analyzing software engineering data, called Optimized Set Reduction (OSR), that overcomes many of the problems associated with statistical techniques. OSR provides mechanisms for building models for prediction that provide accuracy estimates, risk management evaluation and quality assesssment. The construction of the models can be automated and evolve with new data over time to provide an evolutionary learning approach (the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "302\n", "authors": ["24"]}
{"title": "Using coupling measurement for impact analysis in object-oriented systems\n", "abstract": " Many coupling measures have been proposed in the context of object oriented (OO) systems. In addition, due to the numerous dependencies present in OO systems, several studies have highlighted the complexity of using dependency analysis to perform impact analysis. An alternative is to investigate the construction of probabilistic decision models based on coupling measurement to support impact analysis. In addition to providing an ordering of classes where ripple effects are more likely, such an approach is simple and can be automated. In our investigation, we perform a thorough analysis on a commercial C++ system where change data has been collected over several years. We identify the coupling dimensions that seem to be significantly related to ripple effects and use these dimensions to rank classes according to their probability of containing ripple effects. We then assess the expected effectiveness of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "299\n", "authors": ["24"]}
{"title": "On the application of measurement theory in software engineering\n", "abstract": " Elements of measurement theory have recently been introduced into the software engineering discipline. It has been suggested that these elements should serve as the basis for developing, reasoning about, and applying measures. For example, it has been suggested that software complexity measures should be additive, that measures fall into a number of distinct types (i.e., levels of measurement: nominal, ordinal, interval, and ratio), that certain statistical techniques are not appropriate for certain types of measures (e.g., parametric statistics for less-than-interval measures), and that certain transformations are not permissible for certain types of measures (e.g., non-linear transformations for interval measures). In this paper we argue that, inspite of the importance of measurement theory, and in the context of software engineering, many of these prescriptions and proscriptions are either premature or, if strictly\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "296\n", "authors": ["24"]}
{"title": "Impact analysis and change management of UML models\n", "abstract": " The use of Unified Modeling Language (UML) analysis/design models on large projects leads to a large number of interdependent UML diagrams. As software systems evolve, those diagrams undergo changes to, for instance, correct errors or address changes in the requirements. Those changes can in turn lead to subsequent changes to other elements in the UML diagrams. Impact analysis is then defined as the process of identifying the potential consequences (side-effects) of a change, and estimating what needs to be modified to accomplish a change. In this article, we propose a UML model-based approach to impact analysis that can be applied before any implementation of the changes, thus allowing an early decision-making and change planning process. We first verify that the UML diagrams are consistent (consistency check). Then changes between two different versions of a UML model are identified\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "281\n", "authors": ["24"]}
{"title": "A UML-based approach to system testing\n", "abstract": " System testing is concerned with testing an entire system based on its specifications. In the context of object-oriented, UML development, this means that system test requirements are derived from UML analysis artifacts such as use cases, their corresponding sequence and collaboration diagrams, class diagrams, and possibly the use of the Object Constraint Language across all these artifacts. Our goal is to support the derivation of test requirements, which will be transformed into test cases, test oracles, and test drivers once we have detailed design information.               Another important issue we address is the one of testability. Testability requirements (or rules) need to be imposed on UML artifacts so as to be able to support system testing efficiently. Those testability requirements result from a trade-off between analysis and design overhead and improved testability. The potential for automation is also an\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "281\n", "authors": ["24"]}
{"title": "Toward the reverse engineering of UML sequence diagrams for distributed Java software\n", "abstract": " This paper proposes a methodology and instrumentation infrastructure toward the reverse engineering of UML (Unified Modeling Language) sequence diagrams from dynamic analysis. One motivation is, of course, to help people understand the behavior of systems with no (complete) documentation. However, such reverse-engineered dynamic models can also be used for quality assurance purposes. They can, for example, be compared with design sequence diagrams and the conformance of the implementation to the design can thus be verified. Furthermore, discrepancies can also suggest failures in meeting the specifications. Due to size constraints, this paper focuses on the distribution aspects of the methodology we propose. We formally define our approach using metamodels and consistency rules. The instrumentation is based on aspect-oriented programming in order to alleviate the effort overhead usually\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "276\n", "authors": ["24"]}
{"title": "A controlled experiment for evaluating quality guidelines on the maintainability of object-oriented designs\n", "abstract": " The paper presents a controlled experiment, focusing on the impact of applying quality design principles such as the ones provided by P. Coad and E. Yourdon (1991) on the maintainability of object oriented designs. Results, which repeat the findings of a previous study, strongly suggest that such design principles have a beneficial effect on the maintainability of object oriented designs. It is argued that object oriented designs are sensitive to poor design practices because the cognitive complexity introduced becomes increasingly unmanageable. However, as our ability to generalize these results is limited, they should be considered as preliminary, i.e., it is very likely that they can only be generalized to programmers with little object oriented training and programming experience. Such programmers can, however, be commonly found on maintenance projects. As well as additional research, external replications of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "274\n", "authors": ["24"]}
{"title": "Developing interpretable models with optimized set reduction for identifying high-risk software components\n", "abstract": " Applying equal testing and verification effort to all parts of a software system is not very efficient, especially when resources are tight. Therefore, one needs to low/high fault frequency components so that testing/verification effort can be concentrated where needed. Such a strategy is expected to detect more faults and thus improve the resulting reliability of the overall system. The authors present the optimized set reduction approach for constructing such models, which is intended to fulfill specific software engineering needs. The approach to classification is to measure the software system and build multivariate stochastic models for predicting high-risk system components. Experimental results obtained by classifying Ada components into two classes (is, or is not likely to generate faults during system and acceptance rest) are presented. The accuracy of the model and the insights it provides into the error-making\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "254\n", "authors": ["24"]}
{"title": "Empirical studies of quality models in object-oriented systems\n", "abstract": " Measuring structural design properties of a software system, such as coupling, cohesion, or complexity, is a promising approach toward early quality assessments. To use such measurement effectively, quality models that quantitatively describe how these internal structural properties relate to relevant external system qualities such as reliability or maintainability are needed. This chapter\u0393\u00c7\u00d6s objective is to summarize, in a structured and detailed fashion, the empirical results reported so far with modeling external system quality based on structural design properties in object-oriented systems. We perform a critical review of existing work in order to identify lessons learned regarding the way these studies are performed and reported. Constructive guidelines for facilitating the work of future studies are also provided, thus facilitating the development of an empirical body of knowledge.", "num_citations": "236\n", "authors": ["24"]}
{"title": "Replicated case studies for investigating quality factors in object-oriented designs\n", "abstract": " Thispaper aims at empirically exploring the relationships betweenmost of the existing design coupling, cohesion, and inheritancemeasures for object-oriented (OO) systems, and the fault-pronenessof OO system classes. The underlying goal of this study is tobetter understand the relationship between existing design measurementin OO systems and the quality of the software developed. in addition,we aim at assessing whether such relationships, once modeled,can be used to effectively drive and focus inspections or testing.The study described here is a replication of an analogous studyconducted in a university environment with systems developedby students. In order to draw more general conclusions and to(dis)confirm the results obtained there, we now replicated thestudy using data collected on an industrial system developedby professionals. Results show that many of our findings areconsistent across\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "235\n", "authors": ["24"]}
{"title": "Predicting fault-prone components in a java legacy system\n", "abstract": " This paper reports on the construction and validation of faultproneness prediction models in the context of an object-oriented, evolving, legacy system. The goal is to help QA engineers focus their limited verification resources on parts of the system likely to contain faults. A number of measures including code quality, class structure, changes in class structure, and the history of class-level changes and faults are included as candidate predictors of class fault-proneness. A cross-validated classification analysis shows that the obtained model has less than 20% of false positives and false negatives, respectively. However, as shown in this paper, statistics regarding the classification accuracy tend to inflate the potential usefulness of the fault-proneness prediction models. We thus propose a simple and pragmatic methodology for assessing the costeffectiveness of the predictions to focus verification effort. On the basis of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "233\n", "authors": ["24"]}
{"title": "The impact of UML documentation on software maintenance: An experimental evaluation\n", "abstract": " The Unified Modeling Language (UML) is becoming the de facto standard for software analysis and design modeling. However, there is still significant resistance to model-driven development in many software organizations because it is perceived to be expensive and not necessarily cost-effective. Hence, it is important to investigate the benefits obtained from modeling. As a first step in this direction, this paper reports on controlled experiments, spanning two locations, that investigate the impact of UML documentation on software maintenance. Results show that, for complex tasks and past a certain learning curve, the availability of UML documentation may result in significant improvements in the functional correctness of changes as well as the quality of their design. However, there does not seem to be any saving of time. For simpler tasks, the time needed to update the UML documentation may be substantial\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "231\n", "authors": ["24"]}
{"title": "COBRA: a hybrid method for software cost estimation, benchmarking, and risk assessment\n", "abstract": " Current cost estimation techniques have a number of drawbacks. For example, developing algorithmic models requires extensive past project data. Also, off-the-shelf models have been found to be difficult to calibrate but inaccurate without calibration. Informal approaches based on experienced estimators depend on estimators' availability and are not easily repeatable, as well as not being much more accurate than algorithmic techniques. We present a method for cost estimation that combines aspects of algorithmic and experiential approaches (referred to as COBRA, COst estimation, Benchmarking, and Risk Assessment). We find through a case study that cost estimates using COBRA show an average ARE of 0.09. Although we do not have the room to describe the benchmarking and risk assessment parts, the reader will find detailed information in (Briand et al., 1997).", "num_citations": "229\n", "authors": ["24"]}
{"title": "Achieving scalable model-based testing through test case diversity\n", "abstract": " The increase in size and complexity of modern software systems requires scalable, systematic, and automated testing approaches. Model-based testing (MBT), as a systematic and automated test case generation technique, is being successfully applied to verify industrial-scale systems and is supported by commercial tools. However, scalability is still an open issue for large systems, as in practice there are limits to the amount of testing that can be performed in industrial contexts. Even with standard coverage criteria, the resulting test suites generated by MBT techniques can be very large and expensive to execute, especially for system level testing on real deployment platforms and network facilities. Therefore, a scalable MBT technique should be flexible regarding the size of the generated test suites and should be easily accommodated to fit resource and time constraints. Our approach is to select a subset of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "213\n", "authors": ["24"]}
{"title": "A comprehensive evaluation of capture-recapture models for estimating software defect content\n", "abstract": " An important requirement to control the inspection of software artifacts is to be able to decide, based on more objective information, whether the inspection can stop or whether it should continue to achieve a suitable level of artifact quality. A prediction of the number of remaining defects in an inspected artifact can be used for decision making. Several studies in software engineering have considered capture-recapture models to make a prediction. However, few studies compare the actual number of remaining defects to the one predicted by a capture-recapture model on real software engineering artifacts. The authors focus on traditional inspections and estimate, based on actual inspections data, the degree of accuracy of relevant state-of-the-art capture-recapture models for which statistical estimators exist. In order to assess their robustness, we look at the impact of the number of inspectors and the number of actual\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "209\n", "authors": ["24"]}
{"title": "Understanding and predicting the process of software maintenance releases\n", "abstract": " One of the major concerns of any maintenance organization is to understand and estimate the cost of maintenance releases of software systems. Planning the next release so as to maximize the increase in functionality and the improvement in quality are vital to successful maintenance management. The objective of the paper is to present the results of a case study in which an incremental approach was used to better understand the effort distribution of releases and build a predictive effort model for software maintenance releases. The study was conducted in the Flight Dynamics Division (FDD) of NASA Goddard Space Flight Center (GSFC). The paper presents three main results: (1) a predictive effort model developed for the FDD's software maintenance release process, (2) measurement-based lessons learned about the maintenance process in the FDD, (3) a set of lessons learned about the establishment of a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "203\n", "authors": ["24"]}
{"title": "A realistic empirical evaluation of the costs and benefits of UML in software maintenance\n", "abstract": " The Unified Modeling Language (UML) is the de facto standard for object-oriented software analysis and design modeling. However, few empirical studies exist that investigate the costs and evaluate the benefits of using UML in realistic contexts. Such studies are needed so that the software industry can make informed decisions regarding the extent to which they should adopt UML in their development practices. This is the first controlled experiment that investigates the costs of maintaining and the benefits of using UML documentation during the maintenance and evolution of a real, non-trivial system, using professional developers as subjects, working with a state-of-the-art UML tool during an extended period of time. The subjects in the control group had no UML documentation. In this experiment, the subjects in the UML group had on average a practically and statistically significant 54% increase in the functional\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "198\n", "authors": ["24"]}
{"title": "A systematic review of transformation approaches between user requirements and analysis models\n", "abstract": " Model transformation is one of the basic principles of Model Driven Architecture. To build a software system, a sequence of transformations is performed, starting from requirements and ending with implementation. However, requirements are mostly in the form of text, but not a model that can be easily understood by computers; therefore, automated transformations from requirements to analysis models are not easy to achieve. The overall objective of this systematic review is to examine existing literature works that transform textual requirements into analysis models, highlight open issues, and provide suggestions on potential directions of future research. The systematic review led to the analysis of 20 primary studies (16 approaches) obtained after a carefully designed procedure for selecting papers published in journals and conferences from 1996 to 2008 and Software Engineering textbooks. A conceptual\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "191\n", "authors": ["24"]}
{"title": "A comprehensive empirical validation of design measures for object-oriented systems\n", "abstract": " This paper aims at empirically exploring the relationships between existing object-oriented coupling, cohesion, and inheritance measures and the probability of fault detection in system classes during testing. The underlying goal of such a study is to better understand the relationship between existing design measurement in OO systems and the quality of the software developed. Results show that many of the measures capture similar dimensions in the data set, thus reflecting the fact that many of them are based on similar principles and hypotheses. Besides the size of classes, the frequency of method invocations and the depth of inheritance hierarchies seem to be the main driving factors of fault-proneness.", "num_citations": "187\n", "authors": ["24"]}
{"title": "Random testing: Theoretical results and practical implications\n", "abstract": " A substantial amount of work has shed light on whether random testing is actually a useful testing technique. Despite its simplicity, several successful real-world applications have been reported in the literature. Although it is not going to solve all possible testing problems, random testing appears to be an essential tool in the hands of software testers. In this paper, we review and analyze the debate about random testing. Its benefits and drawbacks are discussed. Novel results addressing general questions about random testing are also presented, such as how long does random testing need, on average, to achieve testing targets (e.g., coverage), how does it scale, and how likely is it to yield similar results if we rerun it on the same testing problem (predictability). Due to its simplicity that makes the mathematical analysis of random testing tractable, we provide precise and rigorous answers to these questions. Results\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "180\n", "authors": ["24"]}
{"title": "Theoretical and empirical validation of software product measures\n", "abstract": " In this paper we present and discuss a concrete method for validating measures of software product internal attributes and provide guidelines for its application. This method integrates much of the relevant previous work, such as measurement theory, properties of measures, and the Goal/Question/Metric paradigm (GQM). We identify two types of validation: theoretical and empirical. The former addresses the question \u0393\u00c7\u00a3is the measure measuring the attribute it is purporting to measure?\u0393\u00c7\u00a5, and the latter addresses the question \u0393\u00c7\u00a3is the measure useful in the sense that it is related to other variables in expected ways?\u0393\u00c7\u00a5", "num_citations": "176\n", "authors": ["24"]}
{"title": "An investigation of graph-based class integration test order strategies\n", "abstract": " The issue of ordering class integration in the context of integration testing has been discussed by a number of researchers. More specifically, strategies have been proposed to generate a test order while minimizing stubbing. Recent papers have addressed the problem of deriving an integration order in the presence of dependency cycles in the class diagram. Such dependencies represent a practical problem as they make any topological ordering of classes impossible. Three main approaches, aimed at \"breaking\" cycles, have been proposed. The first one was proposed by Tai and Daniels (1999) and is based on assigning a higher-level order according to aggregation and inheritance relationships and a lower-level order according to associations. The second one was proposed by Le Traon et al. (2000) and is based on identifying strongly connected components in the dependency graph. The third one was\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "168\n", "authors": ["24"]}
{"title": "An experimental investigation of formality in UML-based development\n", "abstract": " The object constraint language (OCL) was introduced as part of the Unified Modeling Language (UML). Its main purpose is to make UML models more precise and unambiguous by providing a constraint language describing constraints that the UML diagrams alone do not convey, including class invariants, operation contracts, and statechart guard conditions. There is an ongoing debate regarding the usefulness of using OCL in UML-based development, questioning whether the additional effort and formality is worth the benefit. It is argued that natural language may be sufficient, and using OCL may not bring any tangible benefits. This debate is in fact similar to the discussion about the effectiveness of formal methods in software engineering, but in a much more specific context. This paper presents the results of two controlled experiments that investigate the impact of using OCL on three software engineering\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "165\n", "authors": ["24"]}
{"title": "Assessing and improving state-based class testing: A series of experiments\n", "abstract": " This work describes an empirical investigation of the cost effectiveness of well-known state-based testing techniques for classes or clusters of classes that exhibit a state-dependent behavior. This is practically relevant as many object-oriented methodologies recommend modeling such components with statecharts which can then be used as a basis for testing. Our results, based on a series of three experiments, show that in most cases state-based techniques are not likely to be sufficient by themselves to catch most of the faults present in the code. Though useful, they need to be complemented with black-box, functional testing. We focus here on a particular technique, Category Partition, as this is the most commonly used and referenced black-box, functional testing technique. Two different oracle strategies have been applied for checking the success of test cases. One is a very precise oracle checking the concrete\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "165\n", "authors": ["24"]}
{"title": "Resource estimation in software engineering\n", "abstract": " This paper presents a comprehensive overview of the state of the art in software resource estimation. We describe common estimation methods and also provide an evaluation framework to systematically compare and assess alternative estimation methods. Though we have tried to be as precise and objective as possible, it is inevitable that such a comparison exercise be somewhat subjective. We however, provide as much information as possible, so that the reader can form his or her own opinion on the methods to employ. We also discuss the applications of such estimation methods and provide practical guidelines.Understanding this article does not require any specific expertise in resource estimation or quantitative modeling. However, certain method descriptions are brief and the level of understanding that can be expected from such a text depends, to a certain extent, upon the reader\u0393\u00c7\u00d6s knowledge. Our objective is to provide the reader with a comprehension of existing software resource estimation methods as well as with the tools to reason about estimation methods and how they relate to the reader\u0393\u00c7\u00d6s problems.", "num_citations": "162\n", "authors": ["24"]}
{"title": "Automated impact analysis of UML models\n", "abstract": " The use of Unified Modeling Language (UML) analysis/design models on large projects leads to a large number of interdependent UML diagrams. As software systems evolve, UML diagrams undergo changes that address error corrections and changed requirements. Those changes can in turn lead to subsequent changes to other elements in the UML diagrams. Impact analysis is defined as the process of identifying the potential consequences (side-effects) of a change, and estimating what needs to be modified to accomplish that change. In this article, we propose a UML model-based approach to impact analysis that can be applied before implementation of changes, thus allowing early decision-making and change planning. We first verify that the UML diagrams in a design model are consistent. Then the changes between two different versions of UML models are automatically identified according to a change\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "160\n", "authors": ["24"]}
{"title": "Adaptive random testing: An illusion of effectiveness?\n", "abstract": " Adaptive Random Testing (ART) has been proposed as an enhancement to random testing, based on assumptions on how failing test cases are distributed in the input domain. The main assumption is that failing test cases are usually grouped into contiguous regions. Many papers have been published in which ART has been described as an effective alternative to random testing when using the average number of test case executions needed to find a failure (F-measure). But all the work in the literature is based either on simulations or case studies with unreasonably high failure rates. In this paper, we report on the largest empirical analysis of ART in the literature, in which 3727 mutated programs and nearly ten trillion test cases were used. Results show that ART is highly inefficient even on trivial problems when accounting for distance calculations among test cases, to an extent that probably prevents its practical\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "159\n", "authors": ["24"]}
{"title": "Empirical studies of object-oriented artifacts, methods, and processes: state of the art and future directions\n", "abstract": " Object-Oriented technologies are becoming pervasive in many software development organizations. However, many methods, processes, tools, or notations are being used without thorough evaluation. Empirical studies aim at investigating the performance of such technologies and the quality of the resulting object-oriented (OO) software products. In other words, the goal is to provide a scientific foundation to the engineering of OO software.This paper summarizes the results of a working group at the Empirical Studies of Software Development and Evolution (ESSDE) workshop in Los Angeles in May 1999. The authors of this paper took part in the working group and have all been involved with various aspects of empirical studies of OO software development. We therefore hope to achieve a good coverage of the current state of the art.", "num_citations": "144\n", "authors": ["24"]}
{"title": "Modeling development effort in object-oriented systems using design properties\n", "abstract": " In the context of software cost estimation, system size is widely taken as a main driver of system development effort. However, other structural design properties, such as coupling, cohesion, and complexity, have been suggested as additional cost factors. Using effort data from an object-oriented development project, we empirically investigate the relationship between class size and the development effort for a class and what additional impact structural properties such as class coupling have on effort. The paper proposes a practical, repeatable, and accurate analysis procedure to investigate relationships between structural properties and development effort. Results indicate that fairly accurate predictions of class effort can be made based on simple measures of the class interface size alone (mean MREs below 30 percent). Effort predictions at the system level are even more accurate as, using Bootstrapping, the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "141\n", "authors": ["24"]}
{"title": "A state-based approach to integration testing based on UML models\n", "abstract": " Correct functioning of object-oriented software depends upon the successful integration of classes. While individual classes may function correctly, several new faults can arise when these classes are integrated together. In this paper, we present a technique to enhance testing of interactions among modal classes. The technique combines UML collaboration diagrams and statecharts to automatically generate an intermediate test model, called SCOTEM (State COllaboration TEst Model). The SCOTEM is then used to generate valid test paths. We also define various coverage criteria to generate test paths from the SCOTEM model. In order to assess our technique, we have developed a tool and applied it to a case study to investigate its fault detection capability. The results show that the proposed technique effectively detects all the seeded integration faults when complying with the most demanding adequacy\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "139\n", "authors": ["24"]}
{"title": "Generating test data from OCL constraints with search techniques\n", "abstract": " Model-based testing (MBT) aims at automated, scalable, and systematic testing solutions for complex industrial software systems. To increase chances of adoption in industrial contexts, software systems can be modeled using well-established standards such as the Unified Modeling Language (UML) and the Object Constraint Language (OCL). Given that test data generation is one of the major challenges to automate MBT, we focus on test data generation from OCL constraints in this paper. This endeavor is all the more challenging given the numerous OCL constructs and operations that are designed to facilitate the definition of constraints. Though search-based software testing has been applied to test data generation for white-box testing (e.g., branch coverage), its application to the MBT of industrial software systems has been limited. In this paper, we propose a set of search heuristics targeted to OCL constraints\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "136\n", "authors": ["24"]}
{"title": "A precise method-method interaction-based cohesion metric for object-oriented classes\n", "abstract": " The building of highly cohesive classes is an important objective in object-oriented design. Class cohesion refers to the relatedness of the class members, and it indicates one important aspect of the class design quality. A meaningful class cohesion metric helps object-oriented software developers detect class design weaknesses and refactor classes accordingly. Several class cohesion metrics have been proposed in the literature. Most of these metrics are applicable based on low-level design information such as attribute references in methods. Some of these metrics capture class cohesion by counting the number of method pairs that share common attributes. A few metrics measure cohesion more precisely by considering the degree of interaction, through attribute references, between each pair of methods. However, the formulas applied by these metrics to measure the degree of interaction cause the metrics to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "136\n", "authors": ["24"]}
{"title": "An object-oriented high-level design-based class cohesion metric\n", "abstract": " ContextClass cohesion is an important object-oriented software quality attribute. Assessing class cohesion during the object-oriented design phase is one important way to obtain more comprehensible and maintainable software. In practice, assessing and controlling cohesion in large systems implies measuring it automatically. One issue with the few existing cohesion metrics targeted at the high-level design phase is that they are not based on realistic assumptions and do not fulfill expected mathematical properties.ObjectiveThis paper proposes a High-Level Design (HLD) class cohesion metric, which is based on realistic assumptions, complies with expected mathematical properties, and can be used to automatically assess design quality at early stages using UML diagrams.MethodThe notion of similarity between pairs of methods and pairs of attribute types in a class is introduced and used as a basis to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "136\n", "authors": ["24"]}
{"title": "Data mining techniques for building fault-proneness models in telecom java software\n", "abstract": " This paper describes a study performed in an industrial setting that attempts to build predictive models to identify parts of a Java system with a high fault probability. The system under consideration is constantly evolving as several releases a year are shipped to customers. Developers usually have limited resources for their testing and inspections and would like to be able to devote extra resources to faulty system parts. The main research focus of this paper is two-fold: (1) use and compare many data mining and machine learning techniques to build fault-proneness models based mostly on source code measures and change/fault history data, and (2) demonstrate that the usual classification evaluation criteria based on confusion matrices may not be fully appropriate to compare and evaluate models.", "num_citations": "135\n", "authors": ["24"]}
{"title": "Solving the class responsibility assignment problem in object-oriented analysis with multi-objective genetic algorithms\n", "abstract": " In the context of object-oriented analysis and design (OOAD), class responsibility assignment is not an easy skill to acquire. Though there are many methodologies for assigning responsibilities to classes, they all rely on human judgment and decision making. Our objective is to provide decision-making support to reassign methods and attributes to classes in a class diagram. Our solution is based on a multi-objective genetic algorithm (MOGA) and uses class coupling and cohesion measurement for defining fitness functions. Our MOGA takes as input a class diagram to be optimized and suggests possible improvements to it. The choice of a MOGA stems from the fact that there are typically many evaluation criteria that cannot be easily combined into one objective, and several alternative solutions are acceptable for a given OO domain model. Using a carefully selected case study, this paper investigates the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "134\n", "authors": ["24"]}
{"title": "Facilitating the transition from use case models to analysis models: Approach and experiments\n", "abstract": " Use case modeling, including use case diagrams and use case specifications (UCSs), is commonly applied to structure and document requirements. UCSs are usually structured but unrestricted textual documents complying with a certain use case template. However, because Use Case Models (UCMods) remain essentially textual, ambiguity is inevitably introduced. In this article, we propose a use case modeling approach, called Restricted Use Case Modeling (RUCM), which is composed of a set of well-defined restriction rules and a modified use case template. The goal is two-fold: (1) restrict the way users can document UCSs in order to reduce ambiguity and (2) facilitate the manual derivation of initial analysis models which, when using the Unified Modeling Language (UML), are typically composed of class diagrams, sequence diagrams, and possibly other types of diagrams. Though the proposed restriction\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "132\n", "authors": ["24"]}
{"title": "Using machine learning to support debugging with tarantula\n", "abstract": " Using a specific machine learning technique, this paper proposes a way to identify suspicious statements during debugging. The technique is based on principles similar to Tarantula but addresses its main flaw: its difficulty to deal with the presence of multiple faults as it assumes that failing test cases execute the same fault(s). The improvement we present in this paper results from the use of C4.5 decision trees to identify various failure conditions based on information regarding the test cases' inputs and outputs. Failing test cases executing under similar conditions are then assumed to fail due to the same fault(s). Statements are then considered suspicious if they are covered by a large proportion of failing test cases that execute under similar conditions. We report on a case study that demonstrates improvement over the original Tarantula technique in terms of statement ranking. Another contribution of this paper is to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "128\n", "authors": ["24"]}
{"title": "Automated checking of conformance to requirements templates using natural language processing\n", "abstract": " Templates are effective tools for increasing the precision of natural language requirements and for avoiding ambiguities that may arise from the use of unrestricted natural language. When templates are applied, it is important to verify that the requirements are indeed written according to the templates. If done manually, checking conformance to templates is laborious, presenting a particular challenge when the task has to be repeated multiple times in response to changes in the requirements. In this article, using techniques from natural language processing (NLP), we develop an automated approach for checking conformance to templates. Specifically, we present a generalizable method for casting templates into NLP pattern matchers and reflect on our practical experience implementing automated checkers for two well-known templates in the requirements engineering community. We report on the application of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "126\n", "authors": ["24"]}
{"title": "Black-box system testing of real-time embedded systems using random and search-based testing\n", "abstract": " Testing real-time embedded systems (RTES) is in many ways challenging. Thousands of test cases can be potentially executed on an industrial RTES. Given the magnitude of testing at the system level, only a fully automated approach can really scale up to test industrial RTES. In this paper we take a black-box approach and model the RTES environment using the UML/MARTE international standard. Our main motivation is to provide a more practical approach to the model-based testing of RTES by allowing system testers, who are often not familiar with the system design but know the application domain well-enough, to model the environment to enable test automation. Environment models can support the automation of three tasks: the code generation of an environment simulator, the selection of test cases, and the evaluation of their expected results (oracles). In this paper, we focus on the second task (test\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "124\n", "authors": ["24"]}
{"title": "An extended systematic literature review on provision of evidence for safety certification\n", "abstract": " ContextCritical systems in domains such as aviation, railway, and automotive are often subject to a formal process of safety certification. The goal of this process is to ensure that these systems will operate safely without posing undue risks to the user, the public, or the environment. Safety is typically ensured via complying with safety standards. Demonstrating compliance to these standards involves providing evidence to show that the safety criteria of the standards are met.ObjectiveIn order to cope with the complexity of large critical systems and subsequently the plethora of evidence information required for achieving compliance, safety professionals need in-depth knowledge to assist them in classifying different types of evidence, and in structuring and assessing the evidence. This paper is a step towards developing such a body of knowledge that is derived from a large-scale empirically rigorous literature review\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "119\n", "authors": ["24"]}
{"title": "Mining SQL injection and cross site scripting vulnerabilities using hybrid program analysis\n", "abstract": " In previous work, we proposed a set of static attributes that characterize input validation and input sanitization code patterns. We showed that some of the proposed static attributes are significant predictors of SQL injection and cross site scripting vulnerabilities. Static attributes have the advantage of reflecting general properties of a program. Yet, dynamic attributes collected from execution traces may reflect more specific code characteristics that are complementary to static attributes. Hence, to improve our initial work, in this paper, we propose the use of dynamic attributes to complement static attributes in vulnerability prediction. Furthermore, since existing work relies on supervised learning, it is dependent on the availability of training data labeled with known vulnerabilities. This paper presents prediction models that are based on both classification and clustering in order to predict vulnerabilities, working in the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "118\n", "authors": ["24"]}
{"title": "Automatic generation of system test cases from use case specifications\n", "abstract": " In safety critical domains, system test cases are often derived from functional requirements in natural language (NL) and traceability between requirements and their corresponding test cases is usually mandatory. The definition of test cases is therefore time-consuming and error prone, especially so given the quickly rising complexity of embedded systems in many critical domains. Though considerable research has been devoted to automatic generation of system test cases from NL requirements, most of the proposed approaches re-quire significant manual intervention or additional, complex behavioral modelling. This significantly hinders their applicability in practice. In this paper, we propose Use Case Modelling for System Tests Generation (UMTG), an approach that automatically generates executable system test cases from use case spec-ifications and a domain model, the latter including a class diagram and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "115\n", "authors": ["24"]}
{"title": "An experimental comparison of the maintainability of object-oriented and structured design documents\n", "abstract": " Several important questions still need to be answered regarding the maintainability of object-oriented design documents. This paper focuses on the following issues: are object-oriented design documents easier to understand and modify than structured design documents? Do they need to comply with quality guidelines such as the ones provided by Coad and Yourdon? What is the impact of such quality standards on the understandability and modifiability of design documents? Answers can be based on informed opinion or empirical evidence. Since software technology investments are substantial and contradictory opinions exist regarding design strategies, performing empirical studies on these topics is a relevant research activity.               This paper presents a controlled experiment performed with computer science students as subjects. Results strongly suggest that quality guidelines based on Coad and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "115\n", "authors": ["24"]}
{"title": "Testing advanced driver assistance systems using multi-objective search and neural networks\n", "abstract": " Recent years have seen a proliferation of complex Advanced Driver Assistance Systems (ADAS), in particular, for use in autonomous cars. These systems consist of sensors and cameras as well as image processing and decision support software components. They are meant to help drivers by providing proper warnings or by preventing dangerous situations. In this paper, we focus on the problem of design time testing of ADAS in a simulated environment. We provide a testing approach for ADAS by combining multi-objective search with surrogate models developed based on neural networks. We use multi-objective search to guide testing towards the most critical behaviors of ADAS. Surrogate modeling enables our testing approach to explore a larger part of the input search space within limited computational resources. We characterize the condition under which the multi-objective search algorithm behaves the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "111\n", "authors": ["24"]}
{"title": "Testing vision-based control systems using learnable evolutionary algorithms\n", "abstract": " Vision-based control systems are key enablers of many autonomous vehicular systems, including self-driving cars. Testing such systems is complicated by complex and multidimensional input spaces. We propose an automated testing algorithm that builds on learnable evolutionary algorithms. These algorithms rely on machine learning or a combination of machine learning and Darwinian genetic operators to guide the generation of new solutions (test scenarios in our context). Our approach combines multiobjective population-based search algorithms and decision tree classification models to achieve the following goals: First, classification models guide the search-based generation of tests faster towards critical test scenarios (i.e., test scenarios leading to failures). Second, search algorithms refine classification models so that the models can accurately characterize critical regions (i.e., the regions of a test input\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "106\n", "authors": ["24"]}
{"title": "Automated testing for SQL injection vulnerabilities: an input mutation approach\n", "abstract": " Web services are increasingly adopted in various domains, from finance and e-government to social media. As they are built on top of the web technologies, they suffer also an unprecedented amount of attacks and exploitations like the Web. Among the attacks, those that target SQL injection vulnerabilities have consistently been top-ranked for the last years. Testing to detect such vulnerabilities before making web services public is crucial. We present in this paper an automated testing approach, namely \u256c\u255d4SQLi, and its underpinning set of mutation operators. \u256c\u255d4SQLi can produce effective inputs that lead to executable and harmful SQL statements. Executability is key as otherwise no injection vulnerability can be exploited. Our evaluation demonstrated that the approach is effective to detect SQL injection vulnerabilities and to produce inputs that bypass application firewalls, which is a common configuration in real\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "105\n", "authors": ["24"]}
{"title": "Control flow analysis of UML 2.0 sequence diagrams\n", "abstract": " This article presents a control flow analysis methodology based on UML 2.0 sequence diagrams (SD). In contrast to the conventional code-based control flow analysis techniques, this technique can be used earlier in software development life cycle, when the UML design model of a system becomes available. Among many applications, this technique can be used in SD-based test techniques, model comprehension and model execution in the context of MDA. Based on the well-defined UML 2.0 activity diagrams, we propose an extended activity diagram metamodel, referred to as Concurrent Control Flow Graph (CCFG), to support control flow analysis of UML 2.0 sequence diagrams. Our strategy in this article is to define an OCL-based mapping in a formal and verifiable form as consistency rules between a SD and a CCFG, so as to ensure the completeness of the rules and the CCFG metamodel with\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "104\n", "authors": ["24"]}
{"title": "Software documentation: how much is enough?\n", "abstract": " It is a well-known fact that software documentation is, in practice, poor and incomplete. Though specification, design, and test documents-among other things-are required by standards and capability maturity models (e.g., SEI CMM), such documentation does not exist in a complete and consistent form in most organizations. When documents are produced, they tend to follow no defined standard and lack information that is crucial to make them understandable and usable by developers and maintainers. Then a fundamental practical question, which motivated this keynote address, is to better understand what type of documentation is required, what is needed to support its completeness and consistency, and what is the level of precision required for each type of document. These questions cannot be investigated at that level of generality though. Answers are likely to be very context-dependent if they are to be\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "103\n", "authors": ["24"]}
{"title": "A measurement framework for object-oriented software testability\n", "abstract": " Testing is an expensive activity in the development process of any software system. Measuring and assessing the testability of software would help in planning testing activities and allocating required resources. More importantly, measuring software testability early in the development process, during analysis or design stages, can yield the highest payoff as design refactoring can be used to improve testability before the implementation starts.This paper presents a generic and extensible measurement framework for object-oriented software testability, which is based on a theory expressed as a set of operational hypotheses. We identify design attributes that have an impact on testability directly or indirectly, by having an impact on testing activities and sub-activities. We also describe the cause-effect relationships between these attributes and software testability based on thorough review of the literature and our own\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "102\n", "authors": ["24"]}
{"title": "Assessing, comparing, and combining state machine-based testing and structural testing: A series of experiments\n", "abstract": " A large number of research works have addressed the importance of models in software engineering. However, the adoption of model-based techniques in software organizations is limited since these models are perceived to be expensive and not necessarily cost-effective. Focusing on model-based testing, this paper reports on a series of controlled experiments. It investigates the impact of state machine testing on fault detection in class clusters and its cost when compared with structural testing. Based on previous work showing this is a good compromise in terms of cost and effectiveness, this paper focuses on a specific state-based technique: the round-trip paths coverage criterion. Round-trip paths testing is compared to structural testing, and it is investigated whether they are complementary. Results show that even when a state machine models the behavior of the cluster under test as accurately as possible, no\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "99\n", "authors": ["24"]}
{"title": "Investigating the use of analysis contracts to improve the testability of object\u0393\u00c7\u00c9oriented code\n", "abstract": " A number of activities involved in testing software are known to be difficult and time consuming. Among them is the definition and coding of test oracles and the isolation of faults once failures have been detected. Through a thorough and rigorous empirical study, we investigate how the instrumentation of contracts could address both issues. Contracts are known to be a useful technique in specifying the precondition and postcondition of operations and class invariants, thus making the definition of object\u0393\u00c7\u00c9oriented analysis or design elements more precise. It is one of the reasons the Object Constraint Language (OCL) was made part of the Unified Modeling Language. Our aim in this paper is to reuse and instrument contracts to ease testing. A thorough case study is run where we define OCL contracts, instrument them using a commercial tool and assess the benefits and limitations of doing so to support the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "95\n", "authors": ["24"]}
{"title": "Traffic-aware stress testing of distributed systems based on UML models\n", "abstract": " A stress test methodology aimed at increasing chances of discovering faults related to network traffic in distributed systems is presented. The technique uses the UML 2.0 model of the distributed system under test, augmented with timing information, and is based on an analysis of the control flow in sequence diagrams. It yields stress test requirements that are made of specific control flow paths along with time values indicating when to trigger them. Different variants of our stress testing technique already exist (they stress different aspects of a distributed system) and we focus here on one variant that is designed to identify and to stress test the system at the instant when data traffic on a network is maximal. Using a real-world distributed system specification, we design and implement a prototype distributed system and describe, for that particular system, how the stress test cases are derived and executed using our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "93\n", "authors": ["24"]}
{"title": "A SysML-based approach to traceability management and design slicing in support of safety certification: Framework, tool support, and case studies\n", "abstract": " ContextTraceability is one of the basic tenets of all safety standards and a key prerequisite for software safety certification. In the current state of practice, there is often a significant traceability gap between safety requirements and software design. Poor traceability, in addition to being a non-compliance issue on its own, makes it difficult to determine whether the design fulfills the safety requirements, mainly because the design aspects related to safety cannot be clearly identified.ObjectiveThe goal of this article is to develop a framework for specifying and automatically extracting design aspects relevant to safety requirements. This goal is realized through the combination of two components: (1) A methodology for establishing traceability between safety requirements and design, and (2) an algorithm that can extract for any given safety requirement a minimized fragment (slice) of the design that is sound, and yet easy to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "92\n", "authors": ["24"]}
{"title": "Using multiple adaptive regression splines to support decision making in code inspections\n", "abstract": " Inspections have been shown to be an effective means of detecting defects early on in the software development life cycle. However, they are not always successful or beneficial as they are affected by a number of technical and managerial factors. To make inspections successful, one important aspect is to understand what are the factors that affect inspection effectiveness (the rate of detected defects) in a given environment, based on project data. In this paper we collected data from over 230 code inspections and performed a multivariate statistical analysis in order to look at how management factors, such as the effort assigned and the inspection rate, affect inspection effectiveness. Because the functional form of effectiveness models is a priori unknown, we use a novel exploratory analysis technique: multiple adaptive regression splines (MARS). We compare the MARS model with more classical regression models\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "92\n", "authors": ["24"]}
{"title": "Formal analysis of the effectiveness and predictability of random testing\n", "abstract": " There has been a lot of work to shed light on whether random testing is actually a useful testing technique. Despite its simplicity, several successful real-world applications appear in the literature. Although it is not going to solve all possible testing problems, random testing is an essential tool in the hands of software testers. In this paper, we address general questions about random testing, such as how long random testing needs on average to achieve testing targets (eg, coverage), how does it scale and how likely is it to yield similar results if we re-run random testing on the same testing problem. Due to its simplicity that makes the mathematical analysis of random testing tractable, we provide precise and rigorous answers to these questions. Our formal results can be applied to most types of software and testing criteria. Simulations are carried out to provide further support to our formal results. The obtained results\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "91\n", "authors": ["24"]}
{"title": "Modeling robustness behavior using aspect-oriented modeling to support robustness testing of industrial systems\n", "abstract": " Model-based robustness testing requires precise and complete behavioral, robustness modeling. For example, state machines can be used to model software behavior when hardware (e.g., sensors) breaks down and be fed to a tool to automate test case generation. But robustness behavior is a crosscutting behavior and, if modeled directly, often results in large, complex state machines. These in practice tend to be error prone and difficult to read and understand. As a result, modeling robustness behavior in this way is not scalable for complex industrial systems. To overcome these problems, aspect-oriented modeling (AOM) can be employed to model robustness behavior as aspects in the form of state machines specifically designed to model robustness behavior. In this paper, we present a RobUstness Modeling Methodology (RUMM) that allows modeling robustness behavior as aspects. Our goal is to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "89\n", "authors": ["24"]}
{"title": "Modeling safety and airworthiness (RTCA DO-178B) information: conceptual model and UML profile\n", "abstract": " Several safety-related standards exist for developing and certifying safety-critical systems. System safety assessments are common practice and system certification according to a standard requires submitting relevant system safety information to appropriate authorities. The RTCA DO-178B standard is a software quality assurance, safety-related standard for the development of software aspects of aerospace systems. This research introduces an approach to improve communication and collaboration among safety engineers, software engineers, and certification authorities in the context of RTCA DO-178B. This is achieved by utilizing a Unified Modeling Language (UML) profile that allows software engineers to model safety-related concepts and properties in UML, the de facto software modeling standard. A conceptual meta-model is defined based on RTCA DO-178B, and then a corresponding UML profile\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "89\n", "authors": ["24"]}
{"title": "A use case modeling approach to facilitate the transition towards analysis models: Concepts and empirical evaluation\n", "abstract": " Use case modeling (UCM) is commonly applied to document requirements. Use case specifications (UCSs) are usually structured, unrestricted textual documents complying with a certain template. However, because they remain essentially textual, ambiguities are inevitable. In this paper, we propose a new UCM approach, which is composed of a set of well-defined restriction rules and a new template. The goal is to reduce ambiguity and facilitate automated analysis, though the later point is not addressed in this paper. We also report on a controlled experiment which evaluates our approach in terms of its ease of application and the quality of the analysis models derived by trained individuals. Results show that the restriction rules are overall easy to apply and that our approach results in significant improvements over UCM using a standard template and no restrictions in UCSs, in terms of the correctness of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "89\n", "authors": ["24"]}
{"title": "Web application vulnerability prediction using hybrid program analysis and machine learning\n", "abstract": " Due to limited time and resources, web software engineers need support in identifying vulnerable code. A practical approach to predicting vulnerable code would enable them to prioritize security auditing efforts. In this paper, we propose using a set of hybrid (static+dynamic) code attributes that characterize input validation and input sanitization code patterns and are expected to be significant indicators of web application vulnerabilities. Because static and dynamic program analyses complement each other, both techniques are used to extract the proposed attributes in an accurate and scalable way. Current vulnerability prediction techniques rely on the availability of data labeled with vulnerability information for training. For many real world applications, past vulnerability data is often not available or at least not complete. Hence, to address both situations where labeled past data is fully available or not, we apply\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "88\n", "authors": ["24"]}
{"title": "Coverage\u0393\u00c7\u00c9based regression test case selection, minimization and prioritization: A case study on an industrial system\n", "abstract": " This paper presents a case study of coverage\u0393\u00c7\u00c9based regression testing techniques on a real world industrial system with real regression faults. The study evaluates four common prioritization techniques, a test selection technique, a test suite minimization technique and a hybrid approach that combines selection and minimization. The study also examines the effects of using various coverage criteria on the effectiveness of the studied approaches. The results show that prioritization techniques that are based on additional coverage with finer grained coverage criteria perform significantly better in fault detection rates. The study also reveals that using modification information in prioritization techniques does not significantly enhance fault detection rates. The results show that test selection does not provide significant savings in execution cost (<2%), which might be attributed to the nature of the changes made to the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "85\n", "authors": ["24"]}
{"title": "Using genetic algorithms for early schedulability analysis and stress testing in real-time systems\n", "abstract": " Reactive real-time systems have to react to external events within time constraints: Triggered tasks must execute within deadlines. It is therefore important for the designers of such systems to analyze the schedulability of tasks during the design process, as well as to test the system's response time to events in an effective manner once it is implemented. This article explores the use of genetic algorithms to provide automated support for both tasks. Our main objective is then to automate, based on the system task architecture, the derivation of test cases that maximize the chances of critical deadline misses within the system; we refer to this testing activity as stress testing. A second objective is to enable an early but realistic analysis of tasks' schedulability at design time. We have developed a specific solution based on genetic algorithms and implemented it in a tool. Case studies were run and results show that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "85\n", "authors": ["24"]}
{"title": "Using simulation for assessing the real impact of test coverage on defect coverage\n", "abstract": " The use of test coverage measures (e.g. block coverage) to control the software test process has become an increasingly common practice. This is justified by the assumption that higher test coverage helps achieve higher defect coverage and therefore improves software quality. In practice, data often shows that defect coverage and test coverage grow over time, as additional testing is performed. However, it is unclear whether this phenomenon of concurrent growth can be attributed to a causal dependency or if it is coincidental, simply due to the cumulative nature of both measures. Answering such a question is important as it determines whether a given test coverage measure should be monitored for quality control and used to drive testing. Although this is no general answer to the problem above, we propose a procedure to investigate whether any test coverage criterion has a genuine additional impact on defect\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "82\n", "authors": ["24"]}
{"title": "An automated approach to transform use cases into activity diagrams\n", "abstract": " Use cases are commonly used to structure and document requirements while UML activity diagrams are often used to visualize and formalize use cases, for example to support automated test case generation. Therefore the automated support for the transition from use cases to activity diagrams would provide significant, practical help. Additionally, traceability could be established through automated transformation, which could then be used for instance to relate requirements to design decisions and test cases. In this paper, we propose an approach to automatically generate activity diagrams from use cases while establishing traceability links. Data flow information can also be generated and added to these activity diagrams. Our approach is implemented in a tool, which we used to perform five case studies. The results show that high quality activity diagrams can be generated. Our analysis also shows that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "80\n", "authors": ["24"]}
{"title": "Automated test suite generation for time-continuous simulink models\n", "abstract": " All engineering disciplines are founded and rely on models, although they may differ on purposes and usages of modeling. Interdisciplinary domains such as Cyber Physical Systems (CPSs) seek approaches that incorporate different modeling needs and usages. Specifically, the Simulink modeling platform greatly appeals to CPS engineers due to its seamless support for simulation and code generation. In this paper, we propose a test generation approach that is applicable to Simulink models built for both purposes of simulation and code generation. We define test inputs and outputs as signals that capture evolution of values over time. Our test generation approach is implemented as a meta-heuristic search algorithm and is guided to produce test outputs with diverse shapes according to our proposed notion of diversity. Our evaluation, performed on industrial and public domain models, demonstrates that:(1) In\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "77\n", "authors": ["24"]}
{"title": "aToucan: an automated framework to derive UML analysis models from use case models\n", "abstract": " The transition from an informal requirements specification in natural language to a structured, precise specification is an important challenge in practice. It is particularly so for object-oriented methods, defined in the context of the OMG's Model Driven Architecture (MDA), where a key step is to transition from a use case model to an analysis model. However, providing automated support for this transition is challenging, mostly because, in practice, requirements are expressed in natural language and are much less structured than other kinds of development artifacts. Such an automated transformation would enable at least the generation of an initial, likely incomplete, analysis model and enable automated traceability from requirements to code, through various intermediate models. In this article, we propose a method and a tool called aToucan, building on existing work, to automatically generate a UML analysis model\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "77\n", "authors": ["24"]}
{"title": "A search-based OCL constraint solver for model-based test data generation\n", "abstract": " Model-based testing (MBT) aims at automated, scalable, and systematic testing solutions for complex industrial software systems. To increase chances of adoption in industrial contexts, software systems should be modeled using well-established standards such as the Unified Modeling Language (UML) and Object Constraint Language (OCL). Given that test data generation is one of the major challenges to automate MBT, this is the topic of this paper with a specific focus on test data generation from OCL constraints. Though search-based software testing (SBST) has been applied to test data generation for white-box testing (e.g., branch coverage), its application to the MBT of industrial software systems has been limited. In this paper, we propose a set of search heuristics based on OCL constraints to guide test data generation and automate MBT in industrial applications. These heuristics are used to develop an\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "77\n", "authors": ["24"]}
{"title": "An overview of UML consistency management\n", "abstract": " The ambiguity inherent in UML coupled with its support of multiple viewpoint modeling pose a great risk of inconsistency. Many classifications of UML inconsistencies exist in the literature today. Several proposals are also made for the mitigation of that risk. The essence of these proposals is to clear the existing ambiguity and to seek the formalization of UML. Unfortunately most of these proposals are not implemented in today\u0393\u00c7\u00d6s UML CASE tools. For these tools to fulfill their promises of supporting automation and hiding complexity, they need to employ a consistency management framework with certain characteristics. This paper surveys the state of the art in UML consistency management and proposes a research agenda for the implementation of a successful consistency management framework.", "num_citations": "77\n", "authors": ["24"]}
{"title": "The case for context-driven software engineering research: Generalizability is overrated\n", "abstract": " For software engineering research to increase its impact and steer our community toward a more successful future, it must foster context-driven research. Such research focuses on problems defined in collaboration with industrial partners and is driven by concrete needs in specific domains and development projects.", "num_citations": "76\n", "authors": ["24"]}
{"title": "Embracing the engineering side of software engineering\n", "abstract": " The author provides, based on 20 years of research and industrial experience, his assessment of software engineering research. He then builds on such analysis to provide recommendations on how we need to change as a research community to increase our impact, gain credibility, and ultimately ensure the success and recognition of our young discipline. The gist of the author's message is that we need to become a true engineering discipline.", "num_citations": "76\n", "authors": ["24"]}
{"title": "An industrial investigation of similarity measures for model-based test case selection\n", "abstract": " Applying model-based testing (MBT) in practice requires practical solutions for scaling up to large industrial systems. One challenge that we have faced while applying MBT was the generation of test suites that were too large to be practical, even for simple coverage criteria. The goal of test case selection techniques is to select a subset of the generated test suite that satisfies resource constraints while yielding a maximum fault detection rate. One interesting heuristic is to choose the most diverse test cases based on a pre-defined similarity measure. In this paper, we investigate and compare possible similarity functions to support similarity-based test selection in the context of state machine testing, which is the most common form of MBT. We apply the proposed similarity measures and a selection strategy based on genetic algorithms to an industrial software system. We compare their fault detection rate based on\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "75\n", "authors": ["24"]}
{"title": "A change analysis process to characterize software maintenance projects\n", "abstract": " In order to improve software maintenance processes, we need to be able to first characterize and assess them. This task needs to be performed in depth and with objectivity since the problems are complex. One approach is to set up a measurement program specifically aimed at maintenance. However, establishing a measurement program requires that one understands the issues and is able to characterize the maintenance environment and processes in order to collect suitable and cost-effective data. Also, enacting such a program and getting usable data sets takes time. A short term substitute is needed. We propose a characterization process aimed specifically at maintenance and based on a general qualitative analysis methodology. This process is rigorously defined in order to be repeatable and usable by people who are not acquainted with such analysis procedures. A basic feature of our approach is that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "75\n", "authors": ["24"]}
{"title": "Novel applications of machine learning in software testing\n", "abstract": " Machine learning techniques have long been used for various purposes in software engineering. This paper provides a brief overview of the state of the art and reports on a number of novel applications I was involved with in the area of software testing. Reflecting on this personal experience, I draw lessons learned and argue that more research should be performed in that direction as machine learning has the potential to significantly help in addressing some of the long-standing software testing problems.", "num_citations": "74\n", "authors": ["24"]}
{"title": "Quantitative evaluation of capture-recapture models to control software inspections\n", "abstract": " An important requirement to control the inspection of software artifacts is to be able to decide, based on objective information, whether inspection can stop or whether it should continue to achieve a suitable level of artifact quality. Several studies in software engineering have considered the use of capture-recapture models to predict the number of remaining defects in an inspected document as a decision criterion about reinspection. However, no study on software engineering artifacts compares the actual number of remaining defects to the one predicted by a capture-recapture model. Simulations have been performed but no definite conclusions can be drawn regarding the degree of accuracy of such models under realistic inspection conditions, and the factors affecting this accuracy. Furthermore, none of these studies performed an exhaustive comparison of existing models. In this study, we focus on traditional\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "74\n", "authors": ["24"]}
{"title": "Revisiting strategies for ordering class integration testing in the presence of dependency cycles\n", "abstract": " The issue of ordering class integration in the context of integration testing of object-oriented software has been discussed by a number of researchers. More specifically, strategies have been proposed to generate a test order while minimizing stubbing. Recent papers have addressed the problem of deriving an integration order in the presence of dependency cycles in the class diagram. Such dependencies represent a practical problem as they make any topological ordering of classes impossible. The paper proposes a strategy that integrates two existing methods aimed at \"breaking\" cycles so as to allow a topological order of classes. The first one was proposed by K.-C. Tai and F.J. Daniels (1999) and is based on assigning a higher-level order according to aggregation and inheritance relationships and a lower-level order according to associations. The second one was proposed by Y. Le Traon et al. (2000) and is\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "73\n", "authors": ["24"]}
{"title": "Extracting domain models from natural-language requirements: approach and industrial evaluation\n", "abstract": " Domain modeling is an important step in the transition from natural-language requirements to precise specifications. For large systems, building a domain model manually is a laborious task. Several approaches exist to assist engineers with this task, whereby candidate domain model elements are automatically extracted using Natural Language Processing (NLP). Despite the existing work on domain model extraction, important facets remain under-explored:(1) there is limited empirical evidence about the usefulness of existing extraction rules (heuristics) when applied in industrial settings;(2) existing extraction rules do not adequately exploit the natural-language dependencies detected by modern NLP technologies; and (3) an important class of rules developed by the information retrieval community for information extraction remains unutilized for building domain models.", "num_citations": "72\n", "authors": ["24"]}
{"title": "A critical analysis of empirical research in software testing\n", "abstract": " In the foreseeable future, software testing will remain one of the best tools we have at our disposal to ensure software dependability. Empirical studies are crucial to software testing research in order to compare and improve software testing techniques and practices. In fact, there is no other way to assess the cost-effectiveness of testing techniques, since all of them are, to various extents, based on heuristics and simplifying assumptions. However, when empirically studying the cost and fault- detection rates of a testing technique, a number of validity issues arise. Further, there are many ways in which empirical studies can be performed, ranging from simulations to controlled experiments with human subjects. What are the strengths and drawbacks of the various approaches? What is the best option under which circumstances? This paper presents a critical analysis of empirical research in software testing and will\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "71\n", "authors": ["24"]}
{"title": "Formal analysis of the probability of interaction fault detection using random testing\n", "abstract": " Modern systems are becoming highly configurable to satisfy the varying needs of customers and users. Software product lines are hence becoming a common trend in software development to reduce cost by enabling systematic, large-scale reuse. However, high levels of configurability entail new challenges. Some faults might be revealed only if a particular combination of features is selected in the delivered products. But testing all combinations is usually not feasible in practice, due to their extremely large numbers. Combinatorial testing is a technique to generate smaller test suites for which all combinations of t features are guaranteed to be tested. In this paper, we present several theorems describing the probability of random testing to detect interaction faults and compare the results to combinatorial testing when there are no constraints among the features that can be part of a product. For example, random\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "70\n", "authors": ["24"]}
{"title": "Search-based automated testing of continuous controllers: Framework, tool support, and case studies\n", "abstract": " ContextTesting and verification of automotive embedded software is a major challenge. Software production in automotive domain comprises three stages: Developing automotive functions as Simulink models, generating code from the models, and deploying the resulting code on hardware devices. Automotive software artifacts are subject to three rounds of testing corresponding to the three production stages: Model-in-the-Loop (MiL), Software-in-the-Loop (SiL) and Hardware-in-the-Loop (HiL) testing.ObjectiveWe study testing of continuous controllers at the Model-in-Loop (MiL) level where both the controller and the environment are represented by models and connected in a closed loop system. These controllers make up a large part of automotive functions, and monitor and control the operating conditions of physical devices.MethodWe identify a set of requirements characterizing the behavior of continuous\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "69\n", "authors": ["24"]}
{"title": "A UML profile for developing airworthiness-compliant (RTCA DO-178B), safety-critical software\n", "abstract": " Many safety-related, certification standards exist for developing safety-critical systems. System safety assessments are common practice and system certification according to a standard requires submitting relevant software safety information to appropriate authorities. The airworthiness standard, RTCA DO-178B, is the de-facto standard for certifying aerospace systems containing software. This research introduces an approach to improve communication and collaboration among safety engineers and software engineers by proposing a Unified Modeling Language (UML) profile that allows software engineers to model safety-related concepts and properties in UML, the de-facto software modeling language. Key safety-related concepts are extracted from RTCA DO-178B, and then a UML profile is defined to enable their precise modeling. We show that the profile improves the line of communication between\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "69\n", "authors": ["24"]}
{"title": "Towards a theoretical framework for measuring software attributes\n", "abstract": " Several attributes (e.g., size, complexity, cohesion, coupling) are commonly used in software engineering to refer to software product properties. A large number of measures have been proposed in the literature to measure these attributes. However, since software attributes are often defined in fuzzy and ambiguous ways, it is sometimes unclear whether the proposed measures are adequate for the software attributes they purport to measure (i.e., their construct validity). In recent years, a few approaches have been proposed to lay theoretical foundations for defining measures for software attributes, but no widespread agreement has been reached on a rigorous, unambiguous definition of software attributes. We first extend previous work carried out on axiomatic approaches for the definition of measures for software attributes (E. Weyuker, 1988; K.B. Lakshmanan et al., 1991). Second, we show how a hierarchical\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "69\n", "authors": ["24"]}
{"title": "Traffic-aware stress testing of distributed real-time systems based on UML models using genetic algorithms\n", "abstract": " This paper presents a model-driven, stress test methodology aimed at increasing chances of discovering faults related to network traffic in distributed real-time systems (DRTS). The technique uses the UML 2.0 model of the distributed system under test, augmented with timing information, and is based on an analysis of the control flow in sequence diagrams. It yields stress test requirements that are made of specific control flow paths along with time values indicating when to trigger them. The technique considers different types of arrival patterns (e.g., periodic) for real-time events (common to DRTSs), and generates test requirements which comply with such timing constraints. Though different variants of our stress testing technique already exist (that stress different aspects of a distributed system), they share a large amount of common concepts and we therefore focus here on one variant that is designed to stress test\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "68\n", "authors": ["24"]}
{"title": "The impact of design properties on development cost in object-oriented systems\n", "abstract": " In the context of software cost estimation, system size is widely taken as a main driver of system development effort, but other structural design properties, such as coupling, cohesion and complexity, have been suggested as additional cost factors. In this paper, using effort data from an object-oriented development project, we empirically investigate the relationship between class size and the development effort for a class, and what additional impact structural properties such as class coupling have on effort. We use Poisson regression and regression trees to build cost prediction models from size and design measures, and use these models to predict the system development effort. We also investigate a technique to combine regression trees with regression analysis, which aims at building more accurate models. The results indicate that fairly accurate predictions of class effort can be made based on simple\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "68\n", "authors": ["24"]}
{"title": "Characterizing the chain of evidence for software safety cases: A conceptual model based on the IEC 61508 standard\n", "abstract": " Increasingly, licensing and safety regulatory bodies require the suppliers of software-intensive, safety-critical systems to provide an explicit software safety case - a structured set of arguments based on objective evidence to demonstrate that the software elements of a system are acceptably safe. Existing research on safety cases has mainly focused on how to build the arguments in a safety case based on available evidence; but little has been done to precisely characterize what this evidence should be. As a result, system suppliers are left with practically no guidance on what evidence to collect during software development. This has led to the suppliers having to recover the relevant evidence after the fact - an extremely costly and sometimes impractical task. Although standards such as the IEC 61508 - which is widely viewed as the best available generic standard for managing functional safety in software - provide\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "67\n", "authors": ["24"]}
{"title": "Supporting the verification of compliance to safety standards via model-driven engineering: Approach, tool-support and empirical validation\n", "abstract": " ContextMany safety\u0393\u00c7\u00f4critical systems are subject to safety certification as a way to provide assurance that these systems cannot unduly harm people, property or the environment. Creating the requisite evidence for certification can be a challenging task due to the sheer size of the textual standards based on which certification is performed and the amenability of these standards to subjective interpretation.ObjectiveThis paper proposes a novel approach to aid suppliers in creating the evidence necessary for certification according to standards. The approach is based on Model-Driven Engineering (MDE) and addresses the challenges of using certification standards while providing assistance with compliance.MethodGiven a safety standard, a conceptual model is built that provides a succinct and explicit interpretation of the standard. This model is then used to create a UML profile that helps system suppliers in relating\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "66\n", "authors": ["24"]}
{"title": "Traceability and SysML design slices to support safety inspections: A controlled experiment\n", "abstract": " Certifying safety-critical software and ensuring its safety requires checking the conformance between safety requirements and design. Increasingly, the development of safety-critical software relies on modeling, and the System Modeling Language (SysML) is now commonly used in many industry sectors. Inspecting safety conformance by comparing design models against safety requirements requires safety inspectors to browse through large models and is consequently time consuming and error-prone. To address this, we have devised a mechanism to establish traceability between (functional) safety requirements and SysML design models to extract design slices (model fragments) that filter out irrelevant details but keep enough context information for the slices to be easy to inspect and understand. In this article, we report on a controlled experiment assessing the impact of the traceability and slicing mechanism\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "65\n", "authors": ["24"]}
{"title": "Using simulation to build inspection efficiency benchmarks for development projects\n", "abstract": " It is difficult for organizations introducing and using software inspections to evaluate how efficient they are. However, it is of practical importance to determine whether they have been efficiently implemented or whether further corrective actions are necessary to bring them up to standard. We present in this paper a procedure for building inspection efficiency benchmarks based on simulation and typical inspection data. Based on most of the data published in the literature, we build an industry-wide benchmark which intends to capture the current practice regarding inspection efficiency. This benchmark construction procedure can also be used to build enterprise specific benchmarks. Last, we assess how robust we can expect them to be by distorting their input distributions to reflect violations of the assumptions made.", "num_citations": "65\n", "authors": ["24"]}
{"title": "Automated traceability analysis for UML model refinements\n", "abstract": " During iterative, UML-based software development, various UML diagrams, modeling the same system at different levels of abstraction are developed. These models must remain consistent when changes are performed. In this context, we refine the notion of impact analysis and distinguish horizontal impact analysis\u0393\u00c7\u00f4that focuses on changes and impacts at one level of abstraction\u0393\u00c7\u00f4from vertical impact analysis\u0393\u00c7\u00f4that focuses on changes at one level of abstraction and their impacts on another level. Vertical impact analysis requires that some traceability links be established between model elements at the two levels of abstraction. We propose a traceability analysis approach for UML 2.0 class diagrams which is based on a careful formalization of changes to those models, refinements which are composed of those changes, and traceability links corresponding to refinements. We show how actual refinements and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "63\n", "authors": ["24"]}
{"title": "Predicting fault-prone classes with design measures in object-oriented systems\n", "abstract": " The paper aims at empirically exploring the relationships between existing object oriented coupling, cohesion, and inheritance measures and the probability of fault detection in system classes during testing. The underlying goal of such a study is to better understand the relationship between existing product measurement in OO systems and the quality of the software developed. It is shown that by using a subset of existing measures, accurate models can be built to predict in which classes most of the faults are likely to lie in. By inspecting 48% of the classes, it is possible to find 95% of the faults. Besides the size of classes, the frequency of method invocations and the depth of inheritance hierarchies seem to be the main driving factors of fault proneness.", "num_citations": "63\n", "authors": ["24"]}
{"title": "Environment modeling and simulation for automated testing of soft real-time embedded software\n", "abstract": " Given the challenges of testing at the system level, only a fully automated approach can really scale up to industrial real-time embedded systems (RTES). Our goal is to provide a practical approach to the model-based testing of RTES by allowing system testers, who are often not familiar with the system\u0393\u00c7\u00d6s design but are application domain experts, to model the system environment in such a way as to enable its black-box test automation. Environment models can support the automation of three tasks: the code generation of an environment simulator to enable testing on the development platform or without involving actual hardware, the selection of test cases, and the evaluation of their expected results (oracles). From a practical standpoint\u0393\u00c7\u00f6and such considerations are crucial for industrial adoption\u0393\u00c7\u00f6environment modeling should be based on modeling standards (1) that are at an adequate level of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "62\n", "authors": ["24"]}
{"title": "A comparison and integration of capture-recapture models and the detection profile method\n", "abstract": " In order to control inspections, the number of remaining defects in software artifacts after their inspection should be estimated. This would allow, for example, deciding whether a reinspection of supposedly faulty artifacts is necessary. Several studies in software engineering have considered capture-recapture models for performing such estimations. These models were initially developed for estimating animal abundance in wildlife research. In addition to these models, researchers in software engineering have recently proposed an alternative approach, namely the detection profile method (DPM), that makes less restrictive assumptions than some capture-recapture models and that show promise in terms of estimation accuracy. The authors investigate how to select between these two approaches for defect content estimation. As a result of this investigation they present a selection procedure taking into account the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "62\n", "authors": ["24"]}
{"title": "An enhanced test case selection approach for model-based testing: an industrial case study\n", "abstract": " In recent years, Model-Based Testing (MBT) has attracted an increasingly wide interest from industry and academia. MBT allows automatic generation of a large and comprehensive set of test cases from system models (eg, state machines), which leads to the systematic testing of the system. However, even when using simple test strategies, applying MBT in large industrial systems often leads to generating large sets of test cases that cannot possibly be executed within time and cost constraints. In this situation, test case selection techniques are employed to select a subset from the entire test suite such that the selected subset conforms to available resources while maximizing fault detection. In this paper, we propose a new similarity-based selection technique for state machine-based test case selection, which includes a new similarity function using triggers and guards on transitions of state machines and a genetic\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "61\n", "authors": ["24"]}
{"title": "Automated transition from use cases to UML state machines to support state-based testing\n", "abstract": " Use cases are commonly used to structure and document requirements while UML state machine diagrams often describe the behavior of a system and serve as a basis to automate test case generation in many model-based testing (MBT) tools. Therefore, automated support for the transition from use cases to state machines would provide significant, practical help for testing system requirements. Additionally, traceability could be established through automated transformations, which could then be used for instance to link requirements to design decisions and test cases, and assess the impact of requirements changes. In this paper, we propose an approach to automatically generate state machine diagrams from use cases while establishing traceability links. Our approach is implemented in a tool, which we used to perform three case studies, including an industrial case study. The results show that high\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "60\n", "authors": ["24"]}
{"title": "Automated extraction and clustering of requirements glossary terms\n", "abstract": " A glossary is an important part of any software requirements document. By making explicit the technical terms in a domain and providing definitions for them, a glossary helps mitigate imprecision and ambiguity. A key step in building a glossary is to decide upon the terms to include in the glossary and to find any related terms. Doing so manually is laborious, particularly for large requirements documents. In this article, we develop an automated approach for extracting candidate glossary terms and their related terms from natural language requirements documents. Our approach differs from existing work on term extraction mainly in that it clusters the extracted terms by relevance, instead of providing a flat list of terms. We provide an automated, mathematically-based procedure for selecting the number of clusters. This procedure makes the underlying clustering algorithm transparent to users, thus alleviating the need\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "59\n", "authors": ["24"]}
{"title": "Empirical investigation of search algorithms for environment model-based testing of real-time embedded software\n", "abstract": " System testing of real-time embedded systems (RTES) is a challenging task and only a fully automated testing approach can scale up to the testing requirements of industrial RTES. One such approach, which offers the advantage for testing teams to be black-box, is to use environment models to automatically generate test cases and oracles and an environment simulator to enable earlier and more practical testing. In this paper, we propose novel heuristics for search-based, RTES system testing which are based on these environment models. We evaluate the fault detection effectiveness of two search-based algorithms, ie, Genetic Algorithms and (1+ 1) Evolutionary Algorithm, when using these novel heuristics and their combinations. Preliminary experiments on 13 carefully selected, non-trivial artificial problems, show that, under certain conditions, these novel heuristics are effective at bringing the environment into\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "58\n", "authors": ["24"]}
{"title": "Automated support for deriving test requirements from UML statecharts\n", "abstract": " Many statechart-based testing strategies result in specifying a set of paths to be executed through a (flattened) statechart. These techniques can usually be easily automated so that the tester does not have to go through the tedious procedure of deriving paths manually to comply with a coverage criterion. The next step is then to take each test path individually and derive test requirements leading to fully specified test cases. This requires that we determine the system state required for each event/transition that is part of the path to be tested and the input parameter values for all events and actions associated with the transitions. We propose here a methodology towards the automation of this procedure, which is based on a careful normalization and analysis of operation contracts and transition guards written with the Object Constraint Language (OCL). It is illustrated by one case study that exemplifies the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "58\n", "authors": ["24"]}
{"title": "Towards automated support for deriving test data from UML statecharts\n", "abstract": " Many statechart-based testing strategies result in specifying a set of paths to be executed through a (flattened) statechart. These techniques can usually be easily automated so that the tester does not have to go through the tedious procedure of deriving paths manually to comply with a coverage criterion. The next step is then to take each test path individually and derive test data, i.e., fully specified test cases. This requires that we determine the system state required for each event/transition that is part of the path to be tested and the input parameter values for all events and actions on the transitions. We propose here a methodology to automate this procedure, which is based on a careful normalization and analysis of event/action contracts and transition guards written with the Object Constraint Language (OCL). It is illustrated by a case study that exemplifies the steps and provides an initial validation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "58\n", "authors": ["24"]}
{"title": "A metamodeling approach to pattern specification\n", "abstract": " This paper presents the Pattern Modeling Framework (PMF), a new metamodeling approach to pattern specification for MOF-compliant modeling frameworks and languages. Patterns need to be precisely specified before a tool can manipulate them, and though several approaches to pattern specification have been proposed, they do not provide the scalability and flexibility required in practice. PMF provides a pattern specification language called Epattern, which is capable of precisely specifying patterns in MOF-compliant metamodels. The language is defined as an extension to MOF by adding semantics inspired from the UML composite structure diagram. The language also comes with a graphical notation and a recommended iterative specification process. It also contains features to manage the complexity of specifying patterns and simplify their application and detection in user models. Most importantly\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "57\n", "authors": ["24"]}
{"title": "On the many ways software engineering can benefit from knowledge engineering\n", "abstract": " Software Engineering is not only a technical discipline of its own. It is also a problem domain where technologies coming from other disciplines are relevant and can play an important role. One important example is knowledge engineering, a term that I use in the broad sense to encompass artificial intelligence, computational intelligence, knowledge bases, data mining, and machine learning. I see a number of typical software development issues that can benefit from these disciplines and, for the sake of clarifying the discussion, I have divided them into four categories:(1) Planning, monitoring, and quality control of projects,(2) The quality and process improvement of software organizations,(3) Decision making support,(4) Automation. First, the planning, monitoring, and quality control of software development is typically based, unless it is entirely ad-hoc, on past project data and/or expert opinion. As discussed below\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "57\n", "authors": ["24"]}
{"title": "Automated, contract-based user testing of commercial-off-the-shelf components\n", "abstract": " Commercial-off-the-Shelf (COTS) components provide a means to construct software (component-based) systems in reduced time and cost. In a COTS component software market there exist component vendors (original developers of the component) and component users (developers of the component-based systems). The former provide the component to the user without source code or design documentation, and as a result it is difficult for the latter to adequately test the component when deployed in their system. In this article we propose a framework that clarifies the roles and responsibilities of both parties so that the user can adequately test the component in a deployment environment and the vendor does not need to release proprietary details. Then, based on this framework we combine and adapt two specification-based testing techniques and describe (and implement) a method for the automated\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "56\n", "authors": ["24"]}
{"title": "Testing autonomous cars for feature interaction failures using many-objective search\n", "abstract": " Complex systems such as autonomous cars are typically built as a composition of features that are independent units of functionality. Features tend to interact and impact one another's behavior in unknown ways. A challenge is to detect and manage feature interactions, in particular, those that violate system requirements, hence leading to failures. In this paper, we propose a technique to detect feature interaction failures by casting this problem into a search-based test generation problem. We define a set of hybrid test objectives (distance functions) that combine traditional coverage-based heuristics with new heuristics specifically aimed at revealing feature interaction failures. We develop a new search-based test generation algorithm, called FITEST, that is guided by our hybrid test objectives. FITEST extends recently proposed many-objective evolutionary algorithms to reduce the time required to compute fitness\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "55\n", "authors": ["24"]}
{"title": "SimPL: a product-line modeling methodology for families of integrated control systems\n", "abstract": " ContextIntegrated control systems (ICSs) are heterogeneous systems where software and hardware components are integrated to control and monitor physical devices and processes. A family of ICSs share the same software code base, which is configured differently for each product to form a unique installation. Due to the complexity of ICSs and inadequate automation support, product configuration in this context is typically error-prone and costly.ObjectiveAs a first step to overcome these challenges, we propose a UML-based product-line modeling methodology that provides a foundation for semi-automated product configuration in the specific context of ICSs.MethodWe performed a comprehensive domain analysis to identify characteristics of ICS families, and their configuration challenges. Based on this, we formulated the characteristics of an adequate configuration solution, and derived from them a set of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "55\n", "authors": ["24"]}
{"title": "Determining inspection cost-effectiveness by combining project data and expert opinion\n", "abstract": " There is a general agreement among software engineering practitioners that software inspections are an important technique to achieve high software quality at a reasonable cost. However, there are many ways to perform such inspections and many factors that affect their cost-effectiveness. It is therefore important to be able to estimate this cost-effectiveness in order to monitor it, improve it, and convince developers and management that the technology and related investments are worth while. This work proposes a rigorous but practical way to do so. In particular, a meaningful model to measure cost-effectiveness is proposed and a method to determine cost-effectiveness by combining project data and expert opinion is described. To demonstrate the feasibility of the proposed approach, the results of a large-scale industrial case study are presented and an initial validation is performed.", "num_citations": "55\n", "authors": ["24"]}
{"title": "Characterizing and assessing a large-scale software maintenance organization\n", "abstract": " One important component of a software process is the organizational context in which the process is enacted. This component is ofien missing or incomplete in current process modeling approaches. One technique for modeling this perspective is the Actor-Dependency (AD) Model. This paper reports on a case study which used this approach to analyze and assess a large software maintenance organization. Our goal was to identify the approach's strengths and weaknesses while providing practical recommendations for improvement and research directions. The AD model was found to be very useful in capturing the important properties of the organizational context of the maintenance process, and aided in the understanding of the flaws found in this process. However, a number of opportunities for extending and improving the AD model were identified. Among others, there is a need to incorporate quantitative\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "55\n", "authors": ["24"]}
{"title": "Modeling and managing risk early in software development\n", "abstract": " The authors present an automated modeling technique which can be used as an alternative to regression techniques to improve the quality of the software development process. The modeling process will allow for the reliable detection of potential problem areas and for the interpretation of the cause of the problem so that the most appropriate remedial action can be taken. It is shown that it can be used to facilitate the identification and aid the interpretation of the significant trends which characterize high risk components in several Ada systems. The effectiveness of the technique is evaluated based on a comparison with logistic regression based models.< >", "num_citations": "55\n", "authors": ["24"]}
{"title": "Testing the untestable: model testing of complex software-intensive systems\n", "abstract": " Increasingly, we are faced with systems that are untestable, meaning that traditional testing methods are expensive, time-consuming or infeasible to apply due to factors such as the systems' continuous interactions with the environment and the deep intertwining of software with hardware.", "num_citations": "53\n", "authors": ["24"]}
{"title": "The value of design rationale information\n", "abstract": " A complete and detailed (full) Design Rationale Documentation (DRD) could support many software development activities, such as an impact analysis or a major redesign. However, this is typically too onerous for systematic industrial use as it is not cost effective to write, maintain, or read. The key idea investigated in this article is that DRD should be developed only to the extent required to support activities particularly difficult to execute or in need of significant improvement in a particular context. The aim of this article is to empirically investigate the customization of the DRD by documenting only the information items that will probably be required for executing an activity. This customization strategy relies on the hypothesis that the value of a specific DRD information item depends on its category (e.g., assumptions, related requirements, etc.) and on the activity it is meant to support. We investigate this hypothesis\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "53\n", "authors": ["24"]}
{"title": "Assessing the cost-effectiveness of inspections by combining project data and expert opinion\n", "abstract": " There is a general agreement among software engineering practitioners that sofware inspections are an important technique to achieve high software quality at a reasonable cost. However, there are many ways to perform such inspections and many factors that affect their cost-effectiveness. It is therefore important to be able to estimate this cost-effectiveness in order to monitor it, improve it, and convince developers and management that the technology and related investments are worthwhile. This work proposes a rigorous but practical way to do so. In particular, a meaningful model to measure cost-effectiveness is selected and a method to determine the cost-effectiveness by combining project data and expert opinion is proposed. To demonstrate the feasibility of the proposed approach, the results of a large-scale industrial case study are presented.", "num_citations": "53\n", "authors": ["24"]}
{"title": "Explaining the cost of european space and military projects\n", "abstract": " There has been much controversy in the literature on several issues underlying the construction of parametric software development cost models. For example, it has been argued whether (dis)economies of scale exist in software production, what functional form should be assumed between effort and product size, whether COCOMO factors were useful, and whether the COCOMO factors are independent. Answers to such questions should help software organizations define suitable data collection programs and well-specified cost models. We use a data set collected by the European Space Agency to perform such an investigation. To ensure a certain degree of consistency in our data, we focus our analysis on a set of space and military projects that represent an important application domain and the largest subset in the database. These projects have been performed, however, by a variety of organizations. First\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "53\n", "authors": ["24"]}
{"title": "Change impact analysis for natural language requirements: An NLP approach\n", "abstract": " Requirements are subject to frequent changes as a way to ensure that they reflect the current best understanding of a system, and to respond to factors such as new and evolving needs. Changing one requirement in a requirements specification may warrant further changes to the specification, so that the overall correctness and consistency of the specification can be maintained. A manual analysis of how a change to one requirement impacts other requirements is time-consuming and presents a challenge for large requirements specifications. We propose an approach based on Natural Language Processing (NLP) for analyzing the impact of change in Natural Language (NL) requirements. Our focus on NL requirements is motivated by the prevalent use of these requirements, particularly in industry. Our approach automatically detects and takes into account the phrasal structure of requirements statements. We\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "52\n", "authors": ["24"]}
{"title": "Empirical investigation of the effects of test suite properties on similarity-based test case selection\n", "abstract": " Our experience with applying model-based testing on industrial systems showed that the generated test suites are often too large and costly to execute given project deadlines and the limited resources for system testing on real platforms. In such industrial contexts, it is often the case that only a small subset of test cases can be run. In previous work, we proposed novel test case selection techniques that minimize the similarities among selected test cases and outperforms other selection alternatives. In this paper, our goal is to gain insights into why and under which conditions similarity-based selection techniques, and in particular our approach, can be expected to work. We investigate the properties of test suites with respect to similarities among fault revealing test cases. We thus identify the ideal situation in which a similarity-based selection works best, which is useful for devising more effective similarity functions\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "52\n", "authors": ["24"]}
{"title": "Reducing the cost of model-based testing through test case diversity\n", "abstract": " Model-based testing (MBT) suffers from two main problems which in many real world systems make MBT impractical: scalability and automatic oracle generation. When no automated oracle is available, or when testing must be performed on actual hardware or a restricted-access network, for example, only a small set of test cases can be executed and evaluated. However, MBT techniques usually generate large sets of test cases when applied to real systems, regardless of the coverage criteria. Therefore, one needs to select a small enough subset of these test cases that have the highest possible fault revealing power. In this paper, we investigate and compare various techniques for rewarding diversity in the selected test cases as a way to increase the likelihood of fault detection. We use a similarity measure defined on the representation of the test cases and use it in several algorithms that aim at\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "52\n", "authors": ["24"]}
{"title": "A model-driven engineering approach to support the verification of compliance to safety standards\n", "abstract": " Certification of safety-critical systems according to well-recognised standards is the norm in many industries where the failure of such systems can harm people or the environment. Certification bodies examine such systems, based on evidence that the system suppliers provide, to ensure that the relevant safety risks have been sufficiently mitigated. The evidence is aimed at satisfying the requirements of the standards used for certification, and naturally a key prerequisite for effective collection of evidence, is that the supplier be aware of these requirements and the evidence they require. This often proves to be a very challenging task because of the sheer size of the standards and the fact that the textual standards are amenable to subjective interpretation. In this paper, we propose an approach based on UML profiles and model-driven engineering. It addresses not only the above challenge but also enables the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "48\n", "authors": ["24"]}
{"title": "Environment modeling with UML/MARTE to support black-box system testing for real-time embedded systems: Methodology and industrial case studies\n", "abstract": " The behavior of real-time embedded systems (RTES) is driven by their environment. Independent system test teams normally focus on black-box testing as they have typically no easy access to precise design information. Black-box testing in this context is mostly about selecting test scenarios that are more likely to lead to unsafe situations in the environment. Our Model-Based Testing (MBT) methodology explicitly models key properties of the environment, its interactions with the RTES, and potentially unsafe situations triggered by failures of the RTES under test. Though environment modeling is not new, we propose a precise methodology fitting our specific purpose, based on a language that is familiar to software testers, that is the UML and its extensions, as opposed to technologies geared towards simulating natural phenomena. Furthermore, in our context, simulation should only be concerned with what\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "48\n", "authors": ["24"]}
{"title": "Behind an application firewall, are we safe from SQL injection attacks?\n", "abstract": " Web application firewalls are an indispensable layer to protect online systems from attacks. However, the fast pace at which new kinds of attacks appear and their sophistication require that firewalls be updated and tested regularly as otherwise they will be circumvented. In this paper, we focus our research on web application firewalls and SQL injection attacks. We present a machine learning-based testing approach to detect holes in firewalls that let SQL injection attacks bypass. At the beginning, the approach can automatically generate diverse attack payloads, which can be seeded into inputs of web- based applications, and then submit them to a system that is protected by a firewall. Incrementally learning from the tests that are blocked or passed by the firewall, our approach can then select tests that exhibit characteristics associated with bypassing the firewall and mutate them to efficiently generate new\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "47\n", "authors": ["24"]}
{"title": "Experiences of applying UML/MARTE on three industrial projects\n", "abstract": " MARTE (Modeling and Analysis of Real-Time and Embedded Systems) is a UML profile, which has been developed to model concepts specific to Real-Time and Embedded Systems (RTES). In previous years, we have applied UML/MARTE to three distinct industrial problems in various industry sectors: architecture modeling and configuration of large-scale and highly configurable integrated control systems, model-based robustness testing of communication-intensive systems, and model-based environment simulator generation of large-scale RTES for testing. In this paper, we report on our experiences of solving these problems by applying UML/MARTE on four industrial case studies. Based on our common experiences, we derive a framework to help practitioners for future applications of UML/MARTE. The framework provides a set of detailed guidelines on how to apply MARTE in industrial contexts and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "47\n", "authors": ["24"]}
{"title": "ANSI--An Inductive Method for Software Process Improvement: Concrete Steps and Guidelines\n", "abstract": " Top-down approaches to process improvement based on generic\" best practice\" models (eg, CMM, TRILLIUM, BOOTSTRAP, SPICE) have became popular. Despite the idiosyncrasies of each of these approaches, they share some common characteristics: all of them are based on numerous assumptions about what are best practices, and about the business goals of organizations and the problems they face. Other organizations, like the Software Engineering Laboratory of the NASA Goddard Space Flight Center, HP and CRIM in Canada, have adopted the Quality Improvement Paradigm (QIP). The QIP stipulates a more bottom-up and inductive approach to process improvement. The focus of this paradigm is to first understand what processes exist in the organization and to determine what causes the most significant problems. Based on this, opportunities for improvement are devised, and empirical studies are conducted to evaluate potential solutions. In this paper, we present a method, named AINSI (An INductive Software process Improvment method), which defines general but concrete steps and guidelines for putting in place the QIP. This method is the result of the collective experiences of the authors and integrates many lessons learned from process improvement efforts in different environments. It also integrates many complementary techniques such as qualitative analysis, methods for data collection (eg, the Goal/Question/Metric paradigm), and quantitative evaluation.", "num_citations": "47\n", "authors": ["24"]}
{"title": "Test generation and test prioritization for simulink models with dynamic behavior\n", "abstract": " All engineering disciplines are founded and rely on models, although they may differ on purposes and usages of modeling. Among the different disciplines, the engineering of Cyber Physical Systems (CPSs) particularly relies on models with dynamic behaviors (i.e., models that exhibit time-varying changes). The Simulink modeling platform greatly appeals to CPS engineers since it captures dynamic behavior models. It further provides seamless support for two indispensable engineering activities: (1) automated verification of abstract system models via model simulation, and (2) automated generation of system implementation via code generation. We identify three main challenges in the verification and testing of Simulink models with dynamic behavior, namely incompatibility, oracle and scalability challenges. We propose a Simulink testing approach that attempts to address these challenges. Specifically, we\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "46\n", "authors": ["24"]}
{"title": "A classification procedure for the effective management of changes during the maintenance process.\n", "abstract": " During software operation, maintainers are often faced with numerous change requests. Given available resources such as effort and calendar time, changes, if approved, have to be planned to fit within budget and schedule constraints. In this paper, we address the issue of assessing the difficulty of a change based on known or predictable data. This paper should be considered as a first step towards the construction of customized economic models for maintainers. In it, we propose a modeling approach, based on regular statistical techniques, that can be used in a variety of software maintenance environments. This approach can be easily automated, and is simple for people with limited statistical experience to use. Moreover, it deals effectively with the uncertainty usually associated with both model inputs and outputs. The modeling approach is validated on a data set provided by the NASA Goddard Space Flight\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "44\n", "authors": ["24"]}
{"title": "Lessons learned from developing a dynamic OCL constraint enforcement tool for Java\n", "abstract": " Analysis and design by contract allows the definition of a formal agreement between a class and its clients, expressing each party\u0393\u00c7\u00d6s rights and obligations. Contracts written in the Object Constraint Language (OCL) are known to be a useful technique to specify the precondition and postcondition of operations and class invariants in a UML context, making the definition of object-oriented analysis or design elements more precise while also helping in testing and debugging. In this article, we report on the experiences with the development of ocl2j, a tool that automatically instruments OCL constraints in Java programs using aspect-oriented programming (AOP). The approach strives for automatic and efficient generation of contract code, and a non-intrusive instrumentation technique. A summary of our approach is given along with the results of an initial case study, the discussion of encountered problems, and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "43\n", "authors": ["24"]}
{"title": "A scalable approach for malware detection through bounded feature space behavior modeling\n", "abstract": " In recent years, malware (malicious software) has greatly evolved and has become very sophisticated. The evolution of malware makes it difficult to detect using traditional signature-based malware detectors. Thus, researchers have proposed various behavior-based malware detection techniques to mitigate this problem. However, there are still serious shortcomings, related to scalability and computational complexity, in existing malware behavior modeling techniques. This raises questions about the practical applicability of these techniques. This paper proposes and evaluates a bounded feature space behavior modeling (BOFM) framework for scalable malware detection. BOFM models the interactions between software (which can be malware or benign) and security-critical OS resources in a scalable manner. Information collected at run-time according to this model is then used by machine learning algorithms to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "42\n", "authors": ["24"]}
{"title": "Test case selection for black-box regression testing of database applications\n", "abstract": " ContextThis paper presents an approach for selecting regression test cases in the context of large-scale database applications. We focus on a black-box (specification-based) approach, relying on classification tree models to model the input domain of the system under test (SUT), in order to obtain a more practical and scalable solution. We perform an experiment in an industrial setting where the SUT is a large database application in Norway\u0393\u00c7\u00d6s tax department.ObjectiveWe investigate the use of similarity-based test case selection for supporting black box regression testing of database applications. We have developed a practical approach and tool (DART) for functional black-box regression testing of database applications. In order to make the regression test approach scalable for large database applications, we needed a test case selection strategy that reduces the test execution costs and analysis effort. We used\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "40\n", "authors": ["24"]}
{"title": "A uml/marte model analysis method for uncovering scenarios leading to starvation and deadlocks in concurrent systems\n", "abstract": " Concurrency problems such as starvation and deadlocks should be identified early in the design process. As larger, more complex concurrent systems are being developed, this is made increasingly difficult. We propose here a general approach based on the analysis of specialized design models expressed in the Unified Modeling Language (UML) that uses a specifically designed genetic algorithm to detect concurrency problems. Though the current paper addresses deadlocks and starvation, we will show how the approach can be easily tailored to other concurrency issues. Our main motivations are 1) to devise solutions that are applicable in the context of the UML design of concurrent systems without requiring additional modeling and 2) to use a search technique to achieve scalable automation in terms of concurrency problem detection. To achieve the first objective, we show how all relevant concurrency\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "40\n", "authors": ["24"]}
{"title": "Instrumenting contracts with aspect-oriented programming to increase observability and support debugging\n", "abstract": " In this paper we report on how aspect-oriented programming (AOP), using AspectJ, can be employed to automatically and efficiently instrument contracts and invariants in Java. The paper focuses on the templates to instrument preconditions, postconditions, and class invariants, and the necessary instrumentation for compliance-checking to the Liskov substitution principle.", "num_citations": "40\n", "authors": ["24"]}
{"title": "Modeling security and privacy requirements: a use case-driven approach\n", "abstract": " Context: Modern internet-based services, ranging from food-delivery to home-caring, leverage the availability of multiple programmable devices to provide handy services tailored to end-user needs. These services are delivered through an ecosystem of device-specific software components and interfaces (e.g., mobile and wearable device applications). Since they often handle private information (e.g., location and health status), their security and privacy requirements are of crucial importance. Defining and analyzing those requirements is a significant challenge due to the multiple types of software components and devices integrated into software ecosystems. Each software component presents peculiarities that often depend on the context and the devices the component interact with, and that must be considered when dealing with security and privacy requirements. Objective: In this paper, we propose, apply, and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "39\n", "authors": ["24"]}
{"title": "Extending SysML with AADL concepts for comprehensive system architecture modeling\n", "abstract": " Recent years have seen a proliferation of languages for describing embedded systems. Some of these languages have emerged from domain-specific frameworks, and some are adaptions or extensions of more general-purpose languages. In this paper, we focus on two widely-used standard languages: the Architecture Analysis and Design Language (AADL) and the Systems Modeling Language (SysML). AADL was born as an avionics-focused domain-specific language and later on has been revised to represent and support a more general category of embedded real-time systems. SysML is an extension of the Unified Modeling Language (UML) intended to support system engineering and modeling. We propose the ExSAM profile that extends SysML by adding AADL concepts to it with the goal of exploiting the key advantages of both languages in a seamless way. More precisely, by using ExSAM and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "39\n", "authors": ["24"]}
{"title": "Multi-objective genetic algorithm to support class responsibility assignment\n", "abstract": " Class responsibility assignment is not an easy skill to acquire. Though there are many methodologies for assigning responsibilities to classes, they all rely on human judgment and decision making. Our objective is to provide decision-making help to re-assign methods and attributes to classes in a class diagram. Our solution is based on a multi-objective genetic algorithm (MOGA) and uses class coupling and cohesion measurement. Our MOGA takes as input a class diagram to be optimized and suggests possible improvements to it. The choice of a MOGA stems from the fact that there are typically many evaluation criteria that cannot be easily combined into one objective, and several alternative solutions are acceptable for a given OO domain model. This article presents our approach in detail, our decisions regarding the multi-objective genetic algorithm, and reports on a case study. Our results suggest that the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "39\n", "authors": ["24"]}
{"title": "Improving fault localization for Simulink models using search-based testing and prediction models\n", "abstract": " One promising way to improve the accuracy of fault localization based on statistical debugging is to increase diversity among test cases in the underlying test suite. In many practical situations, adding test cases is not a cost-free option because test oracles are developed manually or running test cases is expensive. Hence, we require to have test suites that are both diverse and small to improve debugging. In this paper, we focus on improving fault localization of Simulink models by generating test cases. We identify three test objectives that aim to increase test suite diversity. We use these objectives in a search-based algorithm to generate diversified but small test suites. To further minimize test suite sizes, we develop a prediction model to stop test generation when adding test cases is unlikely to improve fault localization. We evaluate our approach using three industrial subjects. Our results show (1) the three\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "37\n", "authors": ["24"]}
{"title": "Simulink fault localization: an iterative statistical debugging approach\n", "abstract": " Debugging Simulink models presents a significant challenge in the embedded industry. This paper proposes SimFL, a fault localization approach for Simulink models by combining statistical debugging and dynamic model slicing. Simulink models, being visual and hierarchical, have multiple outputs at different hierarchy levels. Given a set of outputs to observe for localizing faults, we generate test execution slices, for each test case and output, of the Simulink model. In order to further improve fault localization accuracy, we propose iSimFL, an iterative fault localization algorithm. At each iteration, iSimFL increases the set of observable outputs by including outputs at lower hierarchy levels, thus increasing the test oracle cost but offsetting it with significantly more precise fault localization. We utilize a heuristic stopping criterion to avoid unnecessary test oracle extension. We evaluate our work on three industrial\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "37\n", "authors": ["24"]}
{"title": "Automatic checking of conformance to requirement boilerplates via text chunking: An industrial case study\n", "abstract": " Context. Boilerplates have long been used in Requirements Engineering (RE) to increase the precision of natural language requirements and to avoid ambiguity problems caused by unrestricted natural language. When boilerplates are used, an important quality assurance task is to verify that the requirements indeed conform to the boilerplates. Objective. If done manually, checking conformance to boilerplates is laborious, presenting a particular challenge when the task has to be repeated multiple times in response to requirements changes. Our objective is to provide automation for checking conformance to boilerplates using a Natural Language Processing (NLP) technique, called Text Chunking, and to empirically validate the effectiveness of the automation. Method. We use an exploratory case study, conducted in an industrial setting, as the basis for our empirical investigation. Results. We present a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "37\n", "authors": ["24"]}
{"title": "Automating image segmentation verification and validation by learning test oracles\n", "abstract": " An image segmentation algorithm delineates (an) object(s) of interest in an image. Its output is referred to as a segmentation. Developing these algorithms is a manual, iterative process involving repetitive verification and validation tasks. This process is time-consuming and depends on the availability of experts, who may be a scarce resource (e.g., medical experts). We propose a framework referred to as Image Segmentation Automated Oracle (ISAO) that uses machine learning to construct an oracle, which can then be used to automatically verify the correctness of image segmentations, thus saving substantial resources and making the image segmentation verification and validation task significantly more efficient. The framework also gives informative feedback to the developer as the segmentation algorithm evolves and provides a systematic means of testing different parametric configurations of the algorithm\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "37\n", "authors": ["24"]}
{"title": "COTS evaluation and selection\n", "abstract": " COTS-based development raises a number of issues that are usually not encountered in more classical software development settings. For instance, how to select appropriate COTS? (e.g., middleware for distributed applications, GUI builders). How to integrate COTS selection and evaluation in the requirements engineering and design phases? In this position paper, we address a number of questions regarding COTS evaluation and selection. We first try to better state why this problem is an important one, and why it is likely to become pervasive in the near future. Then, we attempt to clearly identify the difficulties associated with evaluating and selecting COTS. A number of solutions that may alleviate these problems are then discussed. Last, we will list a number of open research questions.", "num_citations": "37\n", "authors": ["24"]}
{"title": "Does aspect-oriented modeling help improve the readability of UML state machines?\n", "abstract": " Aspect-oriented modeling (AOM) is a relatively recent and very active field of research, whose application has, however, been limited in practice. AOM is assumed to yield several potential benefits such as enhanced modularization, easier evolution, increased reusability, and improved readability of models, as well as reduced modeling effort. However, credible, solid empirical evidence of such benefits is lacking. We evaluate the \u0393\u00c7\u00a3readability\u0393\u00c7\u00a5 of state machines when modeling crosscutting behavior using AOM and more specifically AspectSM, a recently published UML profile. This profile extends the UML state machine notation with mechanisms to define aspects using state machines. Readability is indirectly measured through defect identification and fixing rates in state machines, and the scores obtained when answering a comprehension questionnaire about the system behavior. With AspectSM\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "35\n", "authors": ["24"]}
{"title": "A machine-learning-driven evolutionary approach for testing web application firewalls\n", "abstract": " Web application firewalls (WAFs) are an essential protection mechanism for online software systems. Because of the relentless flow of new kinds of attacks as well as their increased sophistication, WAFs have to be updated and tested regularly to prevent attackers from easily circumventing them. In this paper, we focus on testing WAFs for SQL injection attacks, but the general principles and strategy we propose can be adapted to other contexts. We present ML-Driven, an approach based on machine learning and an evolutionary algorithm to automatically detect holes in WAFs that let SQL injection attacks bypass them. Initially, ML-Driven automatically generates a diverse set of attacks and submits them to the system being protected by the target WAF. Then, ML-Driven selects attacks that exhibit patterns (substrings) associated with bypassing the WAF and evolves them to generate new successful bypassing attacks\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "34\n", "authors": ["24"]}
{"title": "Using SysML for modeling of safety-critical software-hardware interfaces: Guidelines and industry experience\n", "abstract": " Safety-critical embedded systems often need to undergo a rigorous certification process to ensure that the safety risks associated with the use of the systems are adequately mitigated. Interfaces between software and hardware components (SW/HW interfaces) play a fundamental role in these systems by linking the systems' control software to either the physical hardware components or to a hardware abstraction layer. Subsequently, safety certification of embedded systems necessarily has to cover the SW/HW interfaces used in these systems. In this paper, we describe a Model Driven Engineering (MDE) approach based on the SysML language, targeted at facilitating the certification of SW/HW interfaces in embedded systems. Our work draws on our experience with maritime and energy systems, but the work should also apply to a broader set of domains, e.g., the automotive sector, where similar design principles\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "34\n", "authors": ["24"]}
{"title": "Improving statechart testing criteria using data flow information\n", "abstract": " Empirical studies have shown there is wide variation in cost (e.g., of devising and executing test cases) and effectiveness (at finding faults) across existing state-based coverage criteria. As these criteria can be considered as executing the control flow structure of the statechart, we are attempting to investigate how data flow information can be used to improve their cost-effectiveness. This article presents a comprehensive methodology to perform data flow analysis of UML statecharts, applies it to the round-trip path (transition tree) coverage criterion and reports on two case studies. The results of the case studies show that dataflow information can be used to select the best cost-effective transition tree when more than one satisfies the transition tree criterion. We further propose a more optimal strategy for the transition tree criterion, in terms of cost and effectiveness. The improved tree strategy is evaluated through the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "34\n", "authors": ["24"]}
{"title": "SOFIA: An automated security oracle for black-box testing of SQL-injection vulnerabilities\n", "abstract": " Security testing is a pivotal activity in engineering secure software. It consists of two phases: generating attack inputs to test the system, and assessing whether test executions expose any vulnerabilities. The latter phase is known as the security oracle problem. In this work, we present SOFIA, a Security Oracle for SQL-Injection Vulnerabilities. SOFIA is programming-language and source-code independent, and can be used with various attack generation tools. Moreover, because it does not rely on known attacks for learning, SOFIA is meant to also detect types of SQLi attacks that might be unknown at learning time. The oracle challenge is recast as a one-class classification problem where we learn to characterise legitimate SQL statements to accurately distinguish them from SQLi attack statements. We have carried out an experimental validation on six applications, among which two are large and widely-used\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["24"]}
{"title": "Requirement boilerplates: Transition from manually-enforced to automatically-verifiable natural language patterns\n", "abstract": " By enforcing predefined linguistic patterns on requirements statements, boilerplates serve as an effective tool for mitigating ambiguities and making Natural Language requirements more amenable to automation. For a boilerplate to be effective, one needs to check whether the boilerplate has been properly applied. This should preferably be done automatically, as manual checking of conformance to a boilerplate can be laborious and error prone. In this paper, we present insights into building an automatic solution for checking conformance to requirement boilerplates using Natural Language Processing (NLP). We present a generalizable method for casting requirement boilerplates into automated NLP pattern matchers and reflect on our practical experience implementing automated checkers for two well-known boilerplates in the RE community. We further highlight the use of NLP for identification of several\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["24"]}
{"title": "A comprehensive modeling framework for role-based access control policies\n", "abstract": " Prohibiting unauthorized access to critical resources and data has become a major requirement for enterprises; access control (AC) mechanisms manage requests from users to access system resources. One of the most used AC paradigms is role-based access control (RBAC), in which access rights are determined based on the user\u0393\u00c7\u00d6s role.Many different types of RBAC policies have been proposed in the literature, each one accompanied by the corresponding extension of the original RBAC model. However, there is no unified framework that can be used to define all these types of policies in a coherent way, using a common model.In this paper we propose a model-driven engineering approach, based on UML and the Object Constraint Language (OCL), to enable the precise specification and verification of such policies. More specifically, we first present a taxonomy of the various types of RBAC policies proposed\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "32\n", "authors": ["24"]}
{"title": "Q\u0393\u00c7\u00c9MOPP: qualitative evaluation of maintenance organizations, processes and products\n", "abstract": " In this paper, we propose a qualitative, inductive method for characterizing and evaluating software maintenance processes, thereby identifying their specific problems and needs. This method encompasses a set of procedures which attempt to determine causal links between maintenance problems and flaws in the maintenance organization and process. This allows for a set of concrete steps to be taken for maintenance quality and productivity improvement, based on a tangible understanding of the relevant maintenance issues in a particular maintenance environment. Moreover, this understanding provides a solid basis on which to define relevant software maintenance models and measures. A case study of the application of this method, called Q\u0393\u00c7\u00c9MOPP, is presented to further illustrate its feasibility and benefits. \u252c\u2310 1998 John Wiley & Sons, Ltd.", "num_citations": "32\n", "authors": ["24"]}
{"title": "OCLR: a more expressive, pattern-based temporal extension of OCL\n", "abstract": " Modern enterprise information systems often require to specify their functional and non-functional (e.g., Quality of Service) requirements using expressions that contain temporal constraints. Specification approaches based on temporal logics demand a certain knowledge of mathematical logic, which is difficult to find among practitioners; moreover, tool support for temporal logics is limited. On the other hand, a standard language such as the Object Constraint Language (OCL), which benefits from the availability of several industrial-strength tools, does not support temporal expressions.               In this paper we propose OCLR, an extension of OCL with support for temporal constraints based on well-known property specification patterns. With respect to previous extensions, we add support for referring to a specific occurrence of an event as well as for indicating a time distance between events and/or from\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["24"]}
{"title": "Combining goal models, expert elicitation, and probabilistic simulation for qualification of new technology\n", "abstract": " New technologies typically involve innovative aspects that are not addressed by the existing normative standards and hence are not assessable through common certification procedures. To ensure that new technologies can be implemented in a safe and reliable manner, a specific kind of assessment is performed, which in many industries, e.g., the energy sector, is known as Technology Qualification (TQ). TQ aims at demonstrating with an acceptable level of confidence that a new technology will function within specified limits. Expert opinion plays an important role in TQ, both to identify the safety and reliability evidence that needs to be developed, and to interpret the evidence provided. Hence, it is crucial to apply a systematic process for eliciting expert opinions, and to use the opinions for measuring the satisfaction of a technology's safety and reliability objectives. In this paper, drawing on the concept of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["24"]}
{"title": "Improving the coverage criteria of UML state machines using data flow analysis\n", "abstract": " A number of coverage criteria have been proposed for testing classes and class clusters modeled with state machines. Previous research has revealed their limitations in terms of their capability to detect faults. As these criteria can be considered to execute the control flow structure of the state machine, we are investigating how data flow information can be used to improve them in the context of UML state machines. More specifically, we investigate how such data flow analysis can be used to further refine the selection of a cost\u0393\u00c7\u00c9effective test suite among alternative, adequate test suites for a given state machine criterion. This paper presents a comprehensive methodology to perform data flow analysis of UML state machines\u0393\u00c7\u00f6with a specific focus on identifying the data flow from OCL guard conditions and operation contracts\u0393\u00c7\u00f6and applies it to a widely referenced coverage criterion, the round\u0393\u00c7\u00c9trip path (transition tree\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["24"]}
{"title": "Interrater agreement in SPICE-based assessments: Some preliminary results\n", "abstract": " The international SPICE Project intends to deliver an ISO standard on software process assessment. This project is unique in software engineering standards in that there is a set of empirical trials, the objectives of which are to evaluate the prospective standard and provide feedback before standardization. One of the enduring issues being evaluated during the trials is the reliability of assessments based on SPICE. One element of reliability is the extent to which different teams assessing the same processes produce similar ratings when presented with the same evidence. We present some preliminary results from two assessments conducted during the SPICE trials. In each of these assessments two independent teams performed the same ratings. The results indicate that in general there is at least moderate agreement between the two teams in both cases. When we take into account the severity of disagreement\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["24"]}
{"title": "Automated extraction of semantic legal metadata using natural language processing\n", "abstract": " [Context] Semantic legal metadata provides information that helps with understanding and interpreting the meaning of legal provisions. Such metadata is important for the systematic analysis of legal requirements. [Objectives] Our work is motivated by two observations: (1) The existing requirements engineering (RE) literature does not provide a harmonized view on the semantic metadata types that are useful for legal requirements analysis. (2) Automated support for the extraction of semantic legal metadata is scarce, and further does not exploit the full potential of natural language processing (NLP). Our objective is to take steps toward addressing these limitations. [Methods] We review and reconcile the semantic legal metadata types proposed in RE. Subsequently, we conduct a qualitative study aimed at investigating how the identified metadata types can be extracted automatically. [Results and Conclusions] We\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "30\n", "authors": ["24"]}
{"title": "Empirical evaluations on the cost-effectiveness of state-based testing: An industrial case study\n", "abstract": " ContextTest models describe the expected behavior of the software under test and provide the basis for test case and oracle generation. When test models are expressed as UML state machines, this is typically referred to as state-based testing (SBT). Despite the importance of being systematic while testing, all testing activities are limited by resource constraints. Thus, reducing the cost of testing while ensuring sufficient fault detection is a common goal in software development. No rigorous industrial case studies of SBT have yet been published.ObjectiveIn this paper, we evaluate the cost-effectiveness of SBT on actual control software by studying the combined influence of four testing aspects: coverage criterion, test oracle, test model and unspecified behavior (sneak paths).MethodAn industrial case study was used to investigate the cost-effectiveness of SBT. To enable the evaluation of SBT techniques, a model\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "30\n", "authors": ["24"]}
{"title": "Classification, structuring, and assessment of evidence for safety--a systematic literature review\n", "abstract": " Safety assurance and certification are amongst the most expensive and time-consuming tasks in the development of safety-critical systems. Demonstration of compliance with safety standards involves providing evidence that the standards' safety criteria are met. To handle large collections of evidence effectively, safety professionals need knowledge of how to classify different types of evidence, how to structure the evidence, and how to assess it. This paper takes a step towards developing such a body of knowledge by conducting a Systematic Literature Review (SLR). Specifically, the SLR identifies and classifies the information and artefacts considered as evidence for safety, examines existing techniques for evidence structuring and assessment, and summarizes the challenges noted in the literature in relation to safety evidence. The paper, to our knowledge, is the first systematic review on the topic of safety\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "30\n", "authors": ["24"]}
{"title": "Modeling and analysis of CPU usage in safety-critical embedded systems to support stress testing\n", "abstract": " Software safety certification needs to address non-functional constraints with safety implications, e.g., deadlines, throughput, and CPU and memory usage. In this paper, we focus on CPU usage constraints and provide a framework to support the derivation of test cases that maximize the chances of violating CPU usage requirements. We develop a conceptual model specifying the generic abstractions required for analyzing CPU usage and provide a mapping between these abstractions and UML/MARTE. Using this model, we formulate CPU usage analysis as a constraint optimization problem and provide an implementation of our approach in a state-of-the-art optimization tool. We report an application of our approach to a case study from the maritime and energy domain. Through this case study, we argue that our approach (1) can be applied with a practically reasonable overhead in an industrial setting\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "30\n", "authors": ["24"]}
{"title": "Assessor agreement in rating SPICE processes\n", "abstract": " One of the enduring issues being evaluated during the SPICE trials is the reliability of assessments. One type of reliability is the extent to which different assessors produce similar ratings when assessing the same organization and presented with the same evidence. In this paper we report on a study that was conducted to start answering this question. Data was collected from an assessment of 21 process instances covering 15 processes. In each of these assessments two independent assessors performed the ratings. We found that six of the fifteen processes do not meet our minimal benchmark for interrater agreement. Three of these were due to systematic biases by either an internal or external assessor. Furthermore, for eight processes specific rating scale adjustments were identified that could improve its reliability. The findings reported in this paper provide guidance for assessors using the SPICE framework\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "30\n", "authors": ["24"]}
{"title": "Using models to enable compliance checking against the GDPR: an experience report\n", "abstract": " The General Data Protection Regulation (GDPR) harmonizes data privacy laws and regulations across Europe. Through the GDPR, individuals are able to better control their personal data in the face of new technological developments. While the GDPR is highly advantageous to individuals, complying with it poses major challenges for organizations that control or process personal data. Since no automated solution with broad industrial applicability currently exists for GDPR compliance checking, organizations have no choice but to perform costly manual audits to ensure compliance. In this paper, we share our experience building a UML representation of the GDPR as a first step towards the development of future automated methods for assessing compliance with the GDPR. Given that a concrete implementation of the GDPR is affected by the national laws of the EU member states, GDPR's expanding body of case\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["24"]}
{"title": "An automated framework for detection and resolution of cross references in legal texts\n", "abstract": " When identifying and elaborating compliance requirements, analysts need to follow the cross references in legal texts and consider the additional information in the cited provisions. Enabling easier navigation and handling of cross references requires automated support for the detection of the natural language expressions used in cross references, the interpretation of cross references in their context, and the linkage of cross references to the targeted provisions. In this article, we propose an approach and tool support for automated detection and resolution of cross references. The approach leverages the structure of legal texts, formalized into a schema, and a set of natural language patterns for legal cross reference expressions. These patterns were developed based on an investigation of Luxembourg\u0393\u00c7\u00d6s legislation, written in French. To build confidence about their applicability beyond the context where\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["24"]}
{"title": "Effective test suites for mixed discrete-continuous stateflow controllers\n", "abstract": " Modeling mixed discrete-continuous controllers using Stateflow is common practice and has a long tradition in the embedded software system industry. Testing Stateflow models is complicated by expensive and manual test oracles that are not amenable to full automation due to the complex continuous behaviors of such models. In this paper, we reduce the cost of manual test oracles by providing test case selection algorithms that help engineers develop small test suites with high fault revealing power for Stateflow models. We present six test selection algorithms for discrete-continuous Stateflows: An adaptive random test selection algorithm that diversifies test inputs, two white-box coverage-based algorithms, a black-box algorithm that diversifies test outputs, and two search-based black-box algorithms that aim to maximize the likelihood of presence of continuous output failure patterns. We evaluate and compare\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["24"]}
{"title": "SafeSlice: a model slicing and design safety inspection tool for SysML\n", "abstract": " Software safety certification involves checking that the software design meets the (software) safety requirements. In practice, inspections are one of the primary vehicles for ensuring that safety requirements are satisfied by the design. Unless the safety-related aspects of the design are clearly delineated, the inspections conducted by safety assessors would have to consider the entire design, although only small fragments of the design may be related to safety. In a model-driven development context, this means that the assessors have to browse through large models, understand them, and identify the safety-related fragments. This is time-consuming and error-prone, specially noting that the assessors are often third-party regulatory bodies who were not involved in the design. To address this problem, we describe in this paper a prototype tool called, SafeSlice, that enables one to automatically extract the safety\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["24"]}
{"title": "Domain-specific model verification with QVT\n", "abstract": " Model verification is the process of checking models for known problems (or anti-patterns). We propose a new approach to declaratively specify and automatically detect problems in domain-specific models using QVT (Query/View/Transformation). Problems are specified with QVT-Relations transformations from models where elements involved in problems are identified, to result models where problem occurrences are reported in a structured and concise manner. The approach uses a standard formalism, applies generically to any MOF-based modeling language and has well-defined detection semantics. We apply the approach by defining a catalog of problems for a particular but important kind of models, namely metamodels. We report on a case study where we used the catalog to verify recent revisions of the UML metamodel. We detected many problem occurrences that we analyzed and helped\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["24"]}
{"title": "Search-driven string constraint solving for vulnerability detection\n", "abstract": " Constraint solving is an essential technique for detecting vulnerabilities in programs, since it can reason about input sanitization and validation operations performed on user inputs. However, real-world programs typically contain complex string operations that challenge vulnerability detection. State-of-the-art string constraint solvers support only a limited set of string operations and fail when they encounter an unsupported one, this leads to limited effectiveness in finding vulnerabilities. In this paper we propose a search-driven constraint solving technique that complements the support for complex string operations provided by any existing string constraint solver. Our technique uses a hybrid constraint solving procedure based on the Ant Colony Optimization meta-heuristic. The idea is to execute it as a fallback mechanism, only when a solver encounters a constraint containing an operation that it does not support\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "28\n", "authors": ["24"]}
{"title": "An industrial application of robustness testing using aspect-oriented modeling, UML/MARTE, and search algorithms\n", "abstract": " Systematic and rigorous robustness testing is very critical for embedded systems, as for example communication and control systems. Robustness testing aims at testing the behavior of a system in the presence of faulty situations in its operating environment (e.g., sensors and actuators). In such situations, the system should gracefully degrade its performance instead of abruptly stopping execution. To systematically perform robustness testing, one option is to resort to model-based robustness testing (MBRT), based for example on UML/MARTE models. However, to successfully apply MBRT in industrial contexts, new technology needs to be developed to scale to the complexity of real industrial systems. In this paper, we report on our experience of performing MBRT on video conferencing systems developed by Cisco Systems, Norway. We discuss how we developed and integrated various techniques and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "28\n", "authors": ["24"]}
{"title": "Industrial experiences with automated regression testing of a legacy database application\n", "abstract": " This paper presents a practical approach and tool (DART) for functional black-box regression testing of complex legacy database applications. Such applications are important to many organizations, but are often difficult to change and consequently prone to regression faults during maintenance. They also tend to be built without particular considerations for testability and can be hard to control and observe. We have therefore devised a practical solution for functional regression testing that captures the changes in database state (due to data manipulations) during the execution of a system under test. The differences in changed database states between consecutive executions of the system under test, on different system versions, can help identify potential regression faults. In order to make the regression test approach scalable for large, complex database applications, classification tree models are used to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "28\n", "authors": ["24"]}
{"title": "Using aspect-oriented programming to instrument ocl contracts in java\n", "abstract": " Analysis and design by contract allows the definitions of a formal agreement between a class and its clients, expressing each party\u0393\u00c7\u00d6s rights and obligations. Contracts written in the Object-Constraint Language (OCL) are known to be a useful technique to specify the precondition and postcondition of operations and class invariants in a UML context, making the definition of object-oriented analysis or design elements more precise. In this report, we introduce the ocl2j approach to automatically instrument OCL constraints in Java programs using aspect-oriented programming (AOP). One of the many possible applications of checking contract assertions at run time is to help testing and debugging. The approach strives for automatic, efficient generation of contract code and a nonintrusive instrumentation technique. It is assessed on a case study and initial results show that the approach is viable. We conclude by discussing strategies to optimize instrumentation so as to further decrease overhead.", "num_citations": "28\n", "authors": ["24"]}
{"title": "Software engineering research and industry: a symbiotic relationship to foster impact\n", "abstract": " Software engineering is not only an increasingly challenging endeavor that goes beyond the intellectual capabilities of any single individual engineer but also an intensely human one. Tools and methods to develop software are employed by engineers of varied backgrounds within a large variety of organizations and application domains. As a result, the variation in challenges and practices in system requirements, architecture, and quality assurance is staggering. Human, domain, and organizational factors define the context within which software engineering methodologies and technologies are to be applied and therefore the context that research needs to account for, if it is to be impactful. This article provides an assessment of the current challenges faced by software engineering research in achieving its potential, a description of the root causes of such challenges, and a proposal for the field to move forward\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["24"]}
{"title": "Automated change impact analysis between SysML models of requirements and design\n", "abstract": " An important activity in systems engineering is analyzing how a change in requirements will impact the design of a system. Performing this analysis manually is expensive, particularly for complex systems. In this paper, we propose an approach to automatically identify the impact of requirements changes on system design, when the requirements and design elements are expressed using models. We ground our approach on the Systems Modeling Language (SysML) due to SysML's increasing use in industrial applications.", "num_citations": "27\n", "authors": ["24"]}
{"title": "Applying UML/MARTE on industrial projects: challenges, experiences, and guidelines\n", "abstract": " Modeling and Analysis of Real-Time and Embedded Systems (MARTE) is a Unified Modeling Language (UML) profile, which has been developed to model concepts specific to Real-Time and Embedded Systems (RTES). In the last 5\u252c\u00e1years, we have applied UML/MARTE to three distinct industrial problems in three industry sectors: architecture modeling and configuration of large-scale and highly configurable integrated control systems, model-based robustness testing of communication-intensive systems, and model-based environment simulator generation of large-scale RTES for testing. In this paper, we report on our experience of solving these problems by applying UML/MARTE on four industrial case studies. We highlight the challenges we faced with respect to the industrial adoption of MARTE. Based on our combined experience, we derive a framework to guide practitioners for future applications of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["24"]}
{"title": "based innovation: A tale of three projects in model-driven engineering\n", "abstract": " In recent years, we have been exploring ways to foster a closer collaboration between software engineering research and industry both to align our research with practical needs, and to increase awareness about the importance of research for innovation. This paper outlines our experience with three research projects conducted in collaboration with the industry. We examine the way we collaborated with our industry partners and describe the decisions that contributed to the effectiveness of the collaborations. We report on the lessons learned from our experience and illustrate the lessons using examples from the three projects. The lessons focus on the applications of Model-Driven Engineering (MDE), as all the three projects we draw on here were MDE projects. Our goal from structuring and sharing our experience is to contribute to a better understanding of how researchers and practitioners can\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["24"]}
{"title": "Configuring use case models in product families\n", "abstract": " In many domains such as automotive and avionics, the size and complexity of software systems is quickly increasing. At the same time, many stakeholders tend to be involved in the development of such systems, which typically must also be configured for multiple customers with varying needs. Product Line Engineering (PLE) is therefore an inevitable practice for such systems. Furthermore, because in many areas requirements must be explicit and traceability to them is required by standards, use cases and domain models are common practice for requirements elicitation and analysis. In this paper, based on the above observations, we aim at supporting PLE in the context of use case-centric development. Therefore, we propose, apply, and assess a use case-driven configuration approach which interactively receives configuration decisions from the analysts to generate product-specific (PS) use case and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["24"]}
{"title": "Security slicing for auditing common injection vulnerabilities\n", "abstract": " Cross-site scripting and injection vulnerabilities are among the most common and serious security issues for Web applications. Although existing static analysis approaches can detect potential vulnerabilities in source code, they generate many false warnings and source-sink traces with irrelevant information, making their adoption impractical for security auditing.One suitable approach to support security auditing is to compute a program slice for each sink, which contains all the information required for security auditing. However, such slices are likely to contain a large amount of information that is irrelevant to security, thus raising scalability issues for security audits.In this paper, we propose an approach to assist security auditors by defining and experimenting with pruning techniques to reduce original program slices to what we refer to as security slices, which contain sound and precise information.To evaluate the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["24"]}
{"title": "Stress testing of task deadlines: A constraint programming approach\n", "abstract": " Safety-critical Real Time Embedded Systems (RT-ESs) are usually subject to strict timing and performance requirements that must be satisfied for the system to be deemed safe. In this paper, we use effective search strategies whose goal is finding worst case scenarios with respect to deadline misses. Such scenarios can in turn be used to test the target RTES and ensure that it satisfies its timing requirements even under worst case conditions. Specifically, we develop an approach based on Constraint Programming (CP) to automate the generation of test cases that reveal, or are likely to, task deadline misses. We evaluate it through a comparison with a state-of-the-art approach based on Genetic Algorithms (GA). In particular, we compare CP and GA in five case studies for efficiency, effectiveness, and scalability. Our experimental results show that, on the largest and more complex case studies, CP performs\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["24"]}
{"title": "Automated model-in-the-loop testing of continuous controllers using search\n", "abstract": " The number and the complexity of software components embedded in today\u0393\u00c7\u00d6s vehicles is rapidly increasing. A large group of these components monitor and control the operating conditions of physical devices (e.g., components controlling engines, brakes, and airbags). These controllers are known as continuous controllers. In this paper, we study testing of continuous controllers at the Model-in-Loop (MiL) level where both the controller and the environment are represented by models and connected in a closed feedback loop system. We identify a set of common requirements characterizing the desired behavior of continuous controllers, and develop a search-based technique to automatically generate test cases for these requirements. We evaluated our approach by applying it to a real automotive air compressor module. Our experience shows that our approach automatically generates several test cases\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["24"]}
{"title": "Localizing multiple faults in simulink models\n", "abstract": " As Simulink is a widely used language in the embedded industry, there is a growing need to support debugging activities for Simulink models. In this work, we propose an approach to localize multiple faults in Simulink models. Our approach builds on statistical debugging and is iterative. At each iteration, we identify and resolve one fault and re-test models to focus on localizing faults that might have been masked before. We use decision trees to cluster together failures that satisfy similar (logical) conditions on model blocks or inputs. We then present two alternative selection criteria to choose a cluster that is more likely to yield the best fault localization results among the clusters produced by our decision trees. Engineers are expected to inspect the ranked list obtained from the selected cluster to identify faults. We evaluate our approach on 240 multi-fault models obtained from three different industrial subjects. We\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "25\n", "authors": ["24"]}
{"title": "Combining genetic algorithms and constraint programming to support stress testing of task deadlines\n", "abstract": " Tasks in real-time embedded systems (RTES) are often subject to hard deadlines that constrain how quickly the system must react to external inputs. These inputs and their timing vary in a large domain depending on the environment state and can never be fully predicted prior to system execution. Therefore, approaches for stress testing must be developed to uncover possible deadline misses of tasks for different input arrival times. In this article, we describe stress-test case generation as a search problem over the space of task arrival times. Specifically, we search for worst-case scenarios maximizing deadline misses, where each scenario characterizes a test case. In order to scale our search to large industrial-size problems, we combine two state-of-the-art search strategies, namely, genetic algorithms (GA) and constraint programming (CP). Our experimental results show that, in comparison with GA and CP in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "25\n", "authors": ["24"]}
{"title": "Using UML profiles for sector-specific tailoring of safety evidence information\n", "abstract": " Safety-critical systems are often subject to certification as a way to ensure that the safety risks associated with their use are sufficiently mitigated. A key requirement of certification is the provision of evidence that a system complies with the applicable standards. The way this is typically organized is to have a generic standard that sets forth the general evidence requirements across different industry sectors, and then to have a derived standard that specializes the generic standard according to the needs of a specific industry sector. To demonstrate standards compliance, one therefore needs to precisely specify how the evidence requirements of a sector-specific standard map onto those of the generic parent standard. Unfortunately, little research has been done to date on capturing the relationship between generic and sector-specific standards and a large fraction of the issues arising during certification can\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "25\n", "authors": ["24"]}
{"title": "Analysis and visualization of behavioral dependencies among distributed objects based on UML models\n", "abstract": " The development of Behavioral Dependency Analysis (BDA) techniques and the visualization of such dependencies have been identified as a high priority in industrial Distributed Real-Time Systems (DRTS). BDA determines the extent to which the functionality of one system entity (e.g., an object, a node) is dependent on other entities. Among many uses, a BDA is traditionally used to perform risk analysis and assessment, fault tolerance and redundancy provisions (e.g. multiple instances of a system entity) in DRTS. Traditionally, most BDA techniques are based on source code or execution traces of a system. However, as model driven development is gaining more popularity, there is a need for model-based BDA techniques. To address this need, we propose a set of procedures and measures for the BDA of distributed objects based on behavioral models (UML sequence diagrams). In contrast to the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "25\n", "authors": ["24"]}
{"title": "Integrating scenario-based and measurement-based software product assessment\n", "abstract": " The software industry needs means to evaluate software products and compare development and implementation technologies in the context of actual projects. Solutions need to be cost-effective but also technically sound. This paper presents a methodology to combine two software product evaluation techniques: measurement of structural design properties, and evaluation of change scenarios. The goal is to use these two approaches together so that they can address each other's limitations. In a case study in the context of the European aerospace industry, this combined methodology was used to assess the impact of choice of programming language and distribution technology on the maintainability of resulting systems. It encompasses the comparison of C++ and Java, as well as distribution/communication technologies such as IPC via sockets, and CORBA implementation. Lessons learned in terms of benefits\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "25\n", "authors": ["24"]}
{"title": "Approximation-refinement testing of compute-intensive cyber-physical models: An approach based on system identification\n", "abstract": " Black-box testing has been extensively applied to test models of Cyber-Physical systems (CPS) since these models are not often amenable to static and symbolic testing and verification. Black-box testing, however, requires to execute the model under test for a large number of candidate test inputs. This poses a challenge for a large and practically-important category of CPS models, known as compute-intensive CPS (CI-CPS) models, where a single simulation may take hours to complete. We propose a novel approach, namely ARIsTEO, to enable effective and efficient testing of CI-CPS models. Our approach embeds black-box testing into an iterative approximation-refinement loop. At the start, some sampled inputs and outputs of the CI-CPS model under test are used to generate a surrogate model that is faster to execute and can be subjected to black-box testing. Any failure-revealing test identified for the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["24"]}
{"title": "Generating automated and online test oracles for simulink models with continuous and uncertain behaviors\n", "abstract": " Test automation requires automated oracles to assess test outputs. For cyber physical systems (CPS), oracles, in addition to be automated, should ensure some key objectives:(i) they should check test outputs in an online manner to stop expensive test executions as soon as a failure is detected;(ii) they should handle time-and magnitude-continuous CPS behaviors;(iii) they should provide a quantitative degree of satisfaction or failure measure instead of binary pass/fail outputs; and (iv) they should be able to handle uncertainties due to CPS interactions with the environment. We propose an automated approach to translate CPS requirements specified in a logic-based language into test oracles specified in Simulink-a widely-used development and simulation language for CPS. Our approach achieves the objectives noted above through the identification of a fragment of Signal First Order logic (SFOL) to specify\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["24"]}
{"title": "A product line modeling and configuration methodology to support model-based testing: an industrial case study\n", "abstract": " Product Line Engineering (PLE) is expected to enhance quality and productivity, speed up time-to-market and decrease development effort, through reuse\u0393\u00c7\u00f6the key mechanism of PLE. In addition, one can also apply PLE to support systematic testing and more specifically model-based testing (MBT) of product lines\u0393\u00c7\u00f6the original motivation behind this work. MBT has shown to be cost-effective in many industry sectors but at the expense of building models of the system under test (SUT). However, the modeling effort to support MBT can significantly be reduced if an adequate product line modeling and configuration methodology is followed, which is the main motivation of this paper. The initial motivation for this work emerged while working with MBT for a Video Conferencing product line at Cisco Systems, Norway. In this paper, we report on our experience in modeling product family models and various types\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["24"]}
{"title": "Comparing offline and online testing of deep neural networks: An autonomous car case study\n", "abstract": " There is a growing body of research on developing testing techniques for Deep Neural Networks (DNNs). We distinguish two general modes of testing for DNNs: Offline testing where DNNs are tested as individual units based on test datasets obtained independently from the DNNs under test, and online testing where DNNs are embedded into a specific application and tested in a close-loop mode in interaction with the application environment. In addition, we identify two sources for generating test datasets for DNNs: Datasets obtained from real-life and datasets generated by simulators. While offline testing can be used with datasets obtained from either sources, online testing is largely confined to using simulators since online testing within real-life applications can be time consuming, expensive and dangerous. In this paper, we study the following two important questions aiming to compare test datasets and testing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["24"]}
{"title": "Test case prioritization for acceptance testing of cyber physical systems: a multi-objective search-based approach\n", "abstract": " Acceptance testing validates that a system meets its requirements and determines whether it can be sufficiently trusted and put into operation. For cyber physical systems (CPS), acceptance testing is a hardware-in-the-loop process conducted in a (near-) operational environment. Acceptance testing of a CPS often necessitates that the test cases be prioritized, as there are usually too many scenarios to consider given time constraints. CPS acceptance testing is further complicated by the uncertainty in the environment and the impact of testing on hardware. We propose an automated test case prioritization approach for CPS acceptance testing, accounting for time budget constraints, uncertainty, and hardware damage risks. Our approach is based on multi-objective search, combined with a test case minimization algorithm that eliminates redundant operations from an ordered sequence of test cases. We evaluate our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["24"]}
{"title": "Change impact analysis for evolving configuration decisions in product line use case models\n", "abstract": " Product Line Engineering is becoming a key practice in many software development environments where complex systems are developed for multiple customers with varying needs. In many business contexts, use cases are the main artifacts for communicating requirements among stakeholders. In such contexts, Product Line (PL) use cases capture variable and common requirements while use case-driven configuration generates Product Specific (PS) use cases for each new customer in a product family. In this paper, we propose, apply, and assess a change impact analysis approach for evolving configuration decisions in PL use case models. Our approach includes: (1) automated support to identify the impact of decision changes on prior and subsequent decisions in PL use case diagrams and (2) automated incremental regeneration of PS use case models from PL use case models and evolving configuration\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["24"]}
{"title": "Automated and effective testing of web services for XML injection attacks\n", "abstract": " XML is extensively used in web services for integration and data exchange. Its popularity and wide adoption make it an attractive target for attackers and a number of XML-based attack types have been reported recently. This raises the need for cost-effective, automated testing of web services to detect XML-related vulnerabilities, which is the focus of this paper. We discuss a taxonomy of the types of XML injection attacks and use it to derive four different ways to mutate XML messages, turning them into attacks (tests) automatically. Further, we consider domain constraints and attack grammars, and use a constraint solver to generate XML messages that are both malicious and valid, thus making it more difficult for any protection mechanism to recognise them. As a result, such messages have a better chance to detect vulnerabilities. Our evaluation on an industrial case study has shown that a large proportion (78.86\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["24"]}
{"title": "Planning for safety standards compliance: A model-based tool-supported approach\n", "abstract": " Safety-critical software-dependent systems such as those found in the avionics, automotive, maritime, and energy domains often require certification based on one or more safety standards. To demonstrate compliance with software safety standards, such as IEC 61508, suppliers must collect evidence that the certifiers can use. Without an upfront agreement between the system supplier and the certifier about the necessary evidence to collect, omissions invariably occur and must be remedied after the fact and at significant costs. The authors present a flexible approach and a supporting tool for assisting suppliers and certifiers in developing an agreement about the evidence necessary to demonstrate compliance to a safety standard. The approach is model-based-specifically, it expresses the safety standard of interest via an information model. The supporting tool, which is available online, takes this information\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["24"]}
{"title": "Automatically repairing web application firewalls based on successful SQL injection attacks\n", "abstract": " Testing and fixing Web Application Firewalls (WAFs) are two relevant and complementary challenges for security analysts. Automated testing helps to cost-effectively detect vulnerabilities in a WAF by generating effective test cases, i.e., attacks. Once vulnerabilities have been identified, the WAF needs to be fixed by augmenting its rule set to filter attacks without blocking legitimate requests. However, existing research suggests that rule sets are very difficult to understand and too complex to be manually fixed. In this paper, we formalise the problem of fixing vulnerable WAFs as a combinatorial optimisation problem. To solve it, we propose an automated approach that combines machine learning with multi-objective genetic algorithms. Given a set of legitimate requests and bypassing SQL injection attacks, our approach automatically infers regular expressions that, when added to the WAF's rule set, prevent many\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["24"]}
{"title": "Security slicing for auditing XML, XPath, and SQL injection vulnerabilities\n", "abstract": " XML, XPath, and SQL injection vulnerabilities are among the most common and serious security issues for Web applications and Web services. Thus, it is important for security auditors to ensure that the implemented code is, to the extent possible, free from these vulnerabilities before deployment. Although existing taint analysis approaches could automatically detect potential vulnerabilities in source code, they tend to generate many false warnings. Furthermore, the produced traces, i.e. dataflow paths from input sources to security-sensitive operations, tend to be incomplete or to contain a great deal of irrelevant information. Therefore, it is difficult to identify real vulnerabilities and determine their causes. One suitable approach to support security auditing is to compute a program slice for each security-sensitive operation, since it would contain all the information required for performing security audits (Soundness). A\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["24"]}
{"title": "Applying product line use case modeling in an industrial automotive embedded system: Lessons learned and a refined approach\n", "abstract": " In this paper, we propose, apply, and assess Product line Use case modeling Method (PUM), an approach that supports modeling variability at different levels of granularity in use cases and domain models. Our motivation is that, in many software development environments, use case modeling drives interactions among stakeholders and, therefore, use cases and domain models are common practice for requirements elicitation and analysis. In PUM, we integrate and adapt existing product line extensions for use cases and introduce some template extensions for use case specifications. Variability is captured in use case diagrams while it is reflected at a greater level of detail in use case specifications. Variability in domain concepts is captured in domain models. PUM is supported by a tool relying on Natural Language Processing (NLP). We applied PUM to an industrial automotive embedded system and report\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["24"]}
{"title": "MiL testing of highly configurable continuous controllers: scalable search using surrogate models\n", "abstract": " Continuous controllers have been widely used in automotive domain to monitor and control physical components. These controllers are subject to three rounds of testing: Model-in-the-Loop (MiL), Software-in-the-Loop and Hardware-in-the-Loop. In our earlier work, we used meta-heuristic search to automate MiL testing of fixed configurations of continuous controllers. In this paper, we extend our work to support MiL testing of all feasible configurations of continuous controllers. Specifically, we use a combination of dimensionality reduction and surrogate modeling techniques to scale our earlier MiL testing approach to large, multi-dimensional input spaces formed by configuration parameters. We evaluated our approach by applying it to a complex, industrial continuous controller. Our experiment shows that our approach identifies test cases indicating requirements violations. Further, we demonstrate that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["24"]}
{"title": "Automated detection and resolution of legal cross references: Approach and a study of luxembourg's legislation\n", "abstract": " When elaborating compliance requirements, analysts need to follow the cross references in the underlying legal texts and consider the additional information in the cited provisions. To enable easier navigation and handling of cross references, automation is necessary for recognizing the natural language patterns used in cross reference expressions (cross reference detection), and for interpreting these expressions and linking them to the target provisions (cross reference resolution). In this paper, we propose a solution for automated detection and resolution of legal cross references. We ground our work on Luxembourg's legislative texts, both for studying the natural language patterns in cross reference expressions and for evaluating the accuracy and scalability of our solution.", "num_citations": "22\n", "authors": ["24"]}
{"title": "A UML/SPT model analysis methodology for concurrent systems based on genetic algorithms\n", "abstract": " Concurrency problems, such as deadlocks, should be identified early in the design process. This is made increasingly difficult as larger and more complex concurrent systems are being developed. We propose here an approach, based on the analysis of specific models expressed in the Unified Modeling Language (UML) that uses a specifically designed genetic algorithm to detect deadlocks. Our main motivations are (1) to devise practical solutions that are applicable in the context of UML design without requiring additional modeling and (2) to achieve scalable automation. All relevant concurrency information is extracted from systems\u0393\u00c7\u00d6 UML models that comply with the UML Schedulability, Performance and Time profile, a standardized specialization of UML for real-time, concurrent systems. Our genetic algorithm is then used to search for execution sequences exhibiting deadlocks. Results on three case\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["24"]}
{"title": "UMTG: a toolset to automatically generate system test cases from use case specifications\n", "abstract": " We present UMTG, a toolset for automatically generating executable and traceable system test cases from use case specifications. UMTG employs Natural Language Processing (NLP), a restricted form of use case specifications, and constraint solving. Use cases are expected to follow a template with restriction rules that reduce imprecision and enable NLP. NLP is used to capture the control flow implicitly described in use case specifications. Finally, to generate test input, constraint solving is applied to OCL constraints referring to the domain model of the system. UMTG is integrated with two tools that are widely adopted in industry, IBM Doors and Rhapsody. UMTG has been successfully evaluated on an industrial case study.", "num_citations": "21\n", "authors": ["24"]}
{"title": "Generating complex and faulty test data through model-based mutation analysis\n", "abstract": " Testing the correct behaviour of data processing systems in the presence of faulty data is extremely expensive. The data structures processed by these systems are often complex, with many data fields and multiple constraints among them. Software engineers, in charge of testing these systems, have to handcraft complex data files or databases, while ensuring compliance with the multiple constraints to prevent the generation of trivially invalid inputs. In addition, assessing test results often means analysing complex output and log data. Though many techniques have been proposed to automatically test systems based on models, little exists in the literature to support the testing of systems where the complexity is in the data consumed in input or produced in output, with complex constraints between them. In particular, such systems often need to be tested with the presence of faults in the input data, in order to assess\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["24"]}
{"title": "Assessing, comparing, and combining statechart-based testing and structural testing: An experiment\n", "abstract": " Although models have been proven to be helpful in a number of software engineering activities there is still significant resistance to model-driven development. This paper investigates one specific aspect of this larger problem. It addresses the impact of using statecharts for testing class clusters that exhibit a state-dependent behavior. More precisely, it reports on a controlled experiment that investigates their impact on testing fault-detection effectiveness. Code-based, structural testing is compared to statechart-based testing and their combination is investigated to determine whether they are complementary. Results show that there is no significant difference between the fault detection effectiveness of the two test strategies but that they are significantly more effective when combined. This implies that a cost-effective strategy would specify statechart-based test cases early on, execute them once the source code is\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["24"]}
{"title": "Panel: Empirical validation-what, why, when, and how\n", "abstract": " Opinions as to the methodology to apply to SE research appear to be in disagreement, as a large variety of possibilities exist [7]. Tichy promotes quantitative, controlled, statistically-analyzable experimentation [6]. Kitchenham et al. recognize the value of \u0393\u00c7\u00a3observational studies\u0393\u00c7\u00a5 in addition to formal experimentation [3], but emphasize industrial-context, quantitative evaluation, and statistics. Seaman promotes the value of qualitative evaluation [5]. Murphy et al. suggest that a different treatment is needed for emerging technologies than for more mature ones [4]. Briand et al. state that \u0393\u00c7\u00a3each discipline needs to develop its own body of experience and strategies to answer its most pressing research questions\u0393\u00c7\u00a5[1, p. 398]. Without some consensus, an SE researcher is faced with a difficult task of convincing their peers that their selected methodology is appropriate, let alone the details of their validation. This panel session strives to address these issues in order to determine where a consensus does and does not exist. Brief synopses of each panelist\u0393\u00c7\u00d6s thoughts follow.", "num_citations": "21\n", "authors": ["24"]}
{"title": "Enabling model testing of cyber-physical systems\n", "abstract": " Applying traditional testing techniques to Cyber-Physical Systems (CPS) is challenging due to the deep intertwining of software and hardware, and the complex, continuous interactions between the system and its environment. To alleviate these challenges we propose to conduct testing at early stages and over executable models of the system and its environment. Model testing of CPSs is however not without difficulties. The complexity and heterogeneity of CPSs renders necessary the combination of different modeling formalisms to build faithful models of their different components. The execution of CPS models thus requires an execution framework supporting the cosimulation of different types of models, including models of the software (eg, SysML), hardware (eg, SysML or Simulink), and physical environment (eg, Simulink). Furthermore, to enable testing in realistic conditions, the cosimulation process must be\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["24"]}
{"title": "Synthetic data generation for statistical testing\n", "abstract": " Usage-based statistical testing employs knowledge about the actual or anticipated usage profile of the system under test for estimating system reliability. For many systems, usage-based statistical testing involves generating synthetic test data. Such data must possess the same statistical characteristics as the actual data that the system will process during operation. Synthetic test data must further satisfy any logical validity constraints that the actual data is subject to. Targeting data-intensive systems, we propose an approach for generating synthetic test data that is both statistically representative and logically valid. The approach works by first generating a data sample that meets the desired statistical characteristics, without taking into account the logical constraints. Subsequently, the approach tweaks the generated sample to fix any logical constraint violations. The tweaking process is iterative and continuously\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["24"]}
{"title": "SimCoTest: A test suite generation tool for Simulink/Stateflow controllers\n", "abstract": " We present SimCoTest, a tool to generate small test suites with high fault revealing ability for Simulink/Stateflow controllers. SimCoTest uses meta-heuristic search to (1) maximize the likelihood of presence of specific failure patterns in output signals (failure-based test generation), and to (2) maximize diversity of output signal shapes (output diversity test generation). SimCoTest has been evaluated on industrial Simulink models and has been systematically compared with Simuilnk Design Verifier (SLDV), an alternative commercial Simulink testing tool. Our results show that the fault revealing ability of SimCoTest outperforms that of SLDV. Further, in contrast to SLDV, SimCoTest is applicable to Simulink/Stateflow models in their entirety. A video describing the main features of SimCoTest is available at: https://youtu. be/YnXgveiGXEA", "num_citations": "20\n", "authors": ["24"]}
{"title": "Clustering deviations for black box regression testing of database applications\n", "abstract": " Regression tests often result in many deviations (differences between two system versions), either due to changes or regression faults. For the tester to analyze such deviations efficiently, it would be helpful to accurately group them, such that each group contains deviations representing one unique change or regression fault. Because it is unlikely that a general solution to the above problem can be found, we focus our work on a common type of software system: database applications. We investigate the use of clustering, based on database manipulations and test specifications (from test models), to group regression test deviations according to the faults or changes causing them. We also propose assessment criteria based on the concept of entropy to compare alternative clustering strategies. To validate our approach, we ran a large scale industrial case study, and our results show that our clustering approach can\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["24"]}
{"title": "Improving requirements glossary construction via clustering: approach and industrial case studies\n", "abstract": " Context. A glossary is an important part of any software requirements document. By making explicit the technical terms in a domain and providing definitions for them, a glossary serves as a helpful tool for mitigating ambiguities.Goal. A necessary step for building a glossary is to decide upon the glossary terms and to identify their related terms. Doing so manually is a laborious task. Our objective is to provide automated support for identifying candidate glossary terms and their related terms. Our work differs from existing work on term extraction mainly in that, instead of providing a flat list of candidate terms, our approach clusters the terms by relevance.Method. We use case study research as the basis for our empirical investigation.Results. We present an automated approach for identifying and clustering candidate glossary terms. We evaluate the approach through two industrial case studies; one study concerns a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["24"]}
{"title": "Architecture-level configuration of large-scale embedded software systems\n", "abstract": " Configuration in the domain of Integrated Control Systems (ICS) is largely manual, laborious, and error prone. In this article, we propose a model-based configuration approach that provides automation support for reducing configuration effort and the likelihood of configuration errors in the ICS domain. We ground our approach on component-based specifications of ICS families. We then develop a configuration algorithm using constraint satisfaction techniques over finite domains to generate products that are consistent with respect to their ICS family specifications. We reason about the termination and consistency of our configuration algorithm analytically. We evaluate the effectiveness of our configuration approach by applying it to a real subsea oil production system. Specifically, we have rebuilt a number of existing verified product configurations of our industry partner. Our experience shows that our approach can\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["24"]}
{"title": "Using Model-Driven engineering for managing safety evidence: Challenges, vision and experience\n", "abstract": " Certification is a major prerequisite for most safety-critical systems before they can be put into operation. During certification, system suppliers often have to present a coherent body of evidence demonstrating that the developed systems are safe for operation. Regardless of the certification approach taken (process-based or product-based), collection of proper evidence at the proper stage of development is critical for successful certification. Currently, system suppliers and certification bodies alike are facing various challenges in relation to safety evidence collection. Notably, they find it hard to interpret the evidence requirements imposed by the safety standards within the domain of application; little support exists for recording, querying, and reporting evidence in a structured manner; and there is a general absence of guidelines on how the collected evidence supports the safety objectives. This paper states our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["24"]}
{"title": "Evaluating model testing and model checking for finding requirements violations in Simulink models\n", "abstract": " Matlab/Simulink is a development and simulation language that is widely used by the Cyber-Physical System (CPS) industry to model dynamical systems. There are two mainstream approaches to verify CPS Simulink models: model testing that attempts to identify failures in models by executing them for a number of sampled test inputs, and model checking that attempts to exhaustively check the correctness of models against some given formal properties. In this paper, we present an industrial Simulink model benchmark, provide a categorization of different model types in the benchmark, describe the recurring logical patterns in the model requirements, and discuss the results of applying model checking and model testing approaches to identify requirements violations in the benchmarked models. Based on the results, we discuss the strengths and weaknesses of model testing and model checking. Our results\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["24"]}
{"title": "An integrated approach for effective injection vulnerability analysis of web applications through security slicing and hybrid constraint solving\n", "abstract": " Malicious users can attack Web applications by exploiting injection vulnerabilities in the source code. This work addresses the challenge of detecting injection vulnerabilities in the server-side code of Java Web applications in a scalable and effective way. We propose an integrated approach that seamlessly combines security slicing with hybrid constraint solving; the latter orchestrates automata-based solving with meta-heuristic search. We use static analysis to extract minimal program slices relevant to security from Web programs and to generate attack conditions. We then apply hybrid constraint solving to determine the satisfiability of attack conditions and thus detect vulnerabilities. The experimental results, using a benchmark comprising a set of diverse and representative Web applications/services as well as security benchmark applications, show that our approach (implemented in the JOACO tool) is\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["24"]}
{"title": "Automatic generation of tests to exploit XML injection vulnerabilities in web applications\n", "abstract": " Modern enterprise systems can be composed of many web services (e.g., SOAP and RESTful). Users of such systems might not have direct access to those services, and rather interact with them through a single-entry point which provides a GUI (e.g., a web page or a mobile app). Although the interactions with such entry point might be secure, a hacker could trick such systems to send malicious inputs to those internal web services. A typical example is XML injection targeting SOAP communications. Previous work has shown that it is possible to automatically generate such kind of attacks using search-based techniques. In this paper, we improve upon previous results by providing more efficient techniques to generate such attacks. In particular, we investigate four different algorithms and two different fitness functions. A large empirical study, involving also two industrial systems, shows that our technique is effective\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["24"]}
{"title": "Using UML for modeling procedural legal rules: Approach and a study of Luxembourg\u0393\u00c7\u00d6s Tax Law\n", "abstract": " Many laws, e.g., those concerning taxes and social benefits, need to be operationalized and implemented into public administration procedures and eGovernment applications. Where such operationalization is warranted, the legal frameworks that interpret the underlying laws are typically prescriptive, providing procedural rules for ensuring legal compliance. We propose a UML-based approach for modeling procedural legal rules. With help from legal experts, we investigate actual legal texts, identifying both the information needs and sources of complexity in the formalization of procedural legal rules. Building on this study, we develop a UML profile that enables more precise modeling of such legal rules. To be able to use logic-based tools for compliance analysis, we automatically transform models of procedural legal rules into the Object Constraint Language (OCL). We report on an application of our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["24"]}
{"title": "Known xml vulnerabilities are still a threat to popular parsers and open source systems\n", "abstract": " The Extensible Markup Language (XML) is extensively used in software systems and services. Various XML-based attacks, which may result in sensitive information leakage or denial of services, have been discovered and published. However, due to development time pressures and limited security expertise, such attacks are often overlooked in practice. In this paper, following a rigorous and extensive experimental process, we study the presence of two types of XML-based attacks: BIL and XXE in 13 popular XML parsers. Furthermore, we investigate whether open-source systems that adopt a vulnerable XML parser apply any mitigation to prevent such attacks. Our objective is to provide clear and solid scientific evidence about the extent of the threat associated with such XML-based attacks and to discuss the implications of the obtained results. Our conclusion is that most of the studied parsers are vulnerable and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "18\n", "authors": ["24"]}
{"title": "An ai-assisted approach for checking the completeness of privacy policies against gdpr\n", "abstract": " Privacy policies are critical for helping individuals make informed decisions about their personal data. In Europe, privacy policies are subject to compliance with the General Data Protection Regulation (GDPR). If done entirely manually, checking whether a given privacy policy complies with GDPR is both time-consuming and error-prone. Automated support for this task is thus advantageous. At the moment, there is an evident lack of such support on the market. In this paper, we tackle an important dimension of GDPR compliance checking for privacy policies. Specifically, we provide automated support for checking whether the content of a given privacy policy is complete according to the provisions stipulated by GDPR. To do so, we present: (1) a conceptual model to characterize the information content envisaged by GDPR for privacy policies, (2) an AI-assisted approach for classifying the information content in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["24"]}
{"title": "PUMConf: a tool to configure product specific use case and domain models in a product line\n", "abstract": " We present PUMConf, a tool for supporting configuration that currently focuses on requirements and enables effective product line management in the context of use case-driven development. By design, it relies exclusively on variability modeling for artifacts that are commonly used in such contexts (ie, use case diagram, specifications and domain model). For given Product Line (PL) use case and domain models, PUMConf checks the consistency of the models, interactively receives configuration decisions from analysts, automatically checks decision consistency, and generates Product Specific (PS) use case and domain models from the PL models and decisions. It has been evaluated on an industrial case study in the automotive domain.", "num_citations": "17\n", "authors": ["24"]}
{"title": "Planning for safety evidence collection: A tool-supported approach based on modeling of standards compliance information\n", "abstract": " Safety-critical software-dependent systems such as those found in the avionics, automotive, maritime, and energy domains often need to be certified by a licensing or regulatory body based on one or more safety standards. Safety standards do not specify the details of the evidence that needs to be collected for the certification of a particular system because these standards need to be generalizable and applicable to a wide variety of systems. Without an upfront agreement between the system supplier and the certifier about the details of the evidence that needs to be collected, there will invariably be important omissions in the evidence information provided by the supplier, which will need to be remedied after the fact and at significant costs. The contributions of this article are twofold: we present both a flexible approach and a publicly available supporting tool for assisting suppliers and certifiers in developing an agreement about the evidence necessary to demonstrate compliance to a safety standard. The approach is model-based; specifically, the safety standard of interest is expressed via an information model. The supporting tool, which is available online, takes this information model as input and assists system suppliers and the certifiers in reaching a documented and consistent agreement about the safety evidence that needs to be collected.", "num_citations": "17\n", "authors": ["24"]}
{"title": "The experimental paradigm in reverse engineering: Role, challenges, and limitations\n", "abstract": " In many areas of software engineering, empirical studies are playing an increasingly important role. This stems from the fact that software technologies are often based on heuristics and are moreover expected to be used in processes where human intervention is paramount. As a result, not only it is important to assess their cost-effectiveness under conditions that are as realistic and representative as possible, but we must also understand the conditions under which they are more suitable and applicable. There exists a wealth of empirical methods aimed at maximizing the validity of results obtained through empirical studies. However, in the case of reverse engineering, as for other domains of investigation, researchers and practitioners are faced with specific constraints and challenges. This is the focus of this keynote address and what the current paper attempts to clarify", "num_citations": "17\n", "authors": ["24"]}
{"title": "Costs and benefits of software process improvement\n", "abstract": " In recent years a substantial number of organizations have gained experience in software process improvement (SPI). Furthermore, some researchers have studied such organizations by collecting and analyzing costs and benefits data on their SPI efforts. The objective of this report is to review and summarize the empirical evidence thus far on the costs and benefits of SPI. The intention is that this review would be utilized to support the business case for initiating and continuing SPI programs, to aid in the selection amongst the alternative improvement paradigms, to make more accurate estimates of the costs and benefits of such efforts, and to help set and manage the expectations of technical staff and management.", "num_citations": "17\n", "authors": ["24"]}
{"title": "Automated testing of hybrid Simulink/Stateflow controllers: industrial case studies\n", "abstract": " We present the results of applying our approach for testing Simulink controllers to one public and one proprietary model, both industrial. Our approach combines explorative and exploitative search algorithms to visualize the controller behavior over its input space and to identify test scenarios in the controller input space that violate or are likely to violate the controller requirements. The engineers' feedback shows that our approach is easy to use in practice and gives them confidence about the behavior of their models.", "num_citations": "16\n", "authors": ["24"]}
{"title": "Automatically deriving UML sequence diagrams from use cases\n", "abstract": " Use cases are commonly used to structure and document requirements during requirement elicitation while sequence diagrams are often used during the analysis phase to document use cases as objects\u0393\u00c7\u00d6 interactions. Since creating such sequence diagrams is mostly manual, automated support would provide significant, practical help. Additionally, traceability could be easily established through automated transformation, which could then be used for instance to relate requirements to design. In this paper, we propose an approach and a tool to automatically generate sequence diagrams from use cases while establishing traceability links. We validate our approach with six case studies, where we compare sequence diagrams generated by our tool to the ones devised by experts and trained 4th year undergraduate students. Results show that sequence diagrams automatically generated by our tool are highly consistent with the ones devised by experts and are also very complete. Results also show that the automatically generated diagrams are far more complete than the ones manually created by students. These encouraging results suggest that our approach and tool would indeed provide significant, practical help to engineers creating (initial) sequence diagrams from use case descriptions.", "num_citations": "16\n", "authors": ["24"]}
{"title": "Toward automatic generation of intrusion detection verification rules\n", "abstract": " An Intrusion Detection System (IDS) is a crucial element of a network security posture. One class of IDS, called signature-based network IDSs, monitors network traffic, looking for evidence of malicious behavior as specified in attack descriptions (referred to as signatures). Many studies have reported that IDSs can generate thousands of alarms a day, many of which are false alarms. The problem often lies in the low accuracy of IDS signatures. It is therefore important to have more accurate signatures in order to reduce the number of false alarms. One part of the false alarm problem is the inability of IDSs to verify attacks (i.e. distinguish between successful and failed attacks). If IDSs were able to accurately verify attacks, this would reduce the number of false alarms a network administrator has to investigate. In this paper, we demonstrate the feasibility of using a data mining algorithm to automatically generate IDS\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["24"]}
{"title": "Experiences with precise state modeling in an industrial safety critical system\n", "abstract": " The development of safety critical systems is a complex and challenging task. There are many claims regarding how precise modeling, either with the UML or other modeling notations, can yield benefits in terms of more rigorous specifications and designs. This paper reports on experiences from applying statechart-driven UML modeling in the development of a safety-critical system at ABB. The primary lessons learned from the study are related to the impact of precise modeling on the ease of transitioning to design.", "num_citations": "16\n", "authors": ["24"]}
{"title": "An experimental evaluation of quality guidelines on the maintainability of object-oriented design documents\n", "abstract": " This paperpresents a controlled experimentfocusing on the impact of applying quality design principles such as the ones provided by Coad and Yourdon on the maintainability of object-oriented design documents.Results, which repeat thefindings of a previous study; strongly suggest that such design principles have a benejicial eflect on the maintainability of object-oriented design documents. It is argued that object-oriented design documents are sensitive to poor design practices because the cognitive complexity introduced becomes increasingly unmanageable. However; as our ability to generalise these results is limited, they should be considered as preliminaq\\ie, it is very likely that they can only be generalised to programmers with little object-oriented training and programming experience. Such programmers can, however; be commonly found on maintenance projects. As well as additional research, external\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["24"]}
{"title": "On the effectiveness of contracts as test oracles in the detection and diagnosis of functional faults in concurrent object-oriented software\n", "abstract": " Design by contract (DbC) is a software development methodology that focuses on clearly defining the interfaces between components to produce better quality object-oriented software. Though there exists ample support for DbC for sequential programs, applying DbC to concurrent programs presents several challenges. Using Java as the target programming language, we tackle such challenges by augmenting the Java Modelling Language (JML) and modifying the JML compiler (jmlc) to generate runtime assertion checking code to support DbC in concurrent programs. We applied our solution in a carefully designed case study on a highly concurrent industrial software system from the telecommunications domain to assess the effectiveness of contracts as test oracles in detecting and diagnosing functional faults in concurrent software. Based on these results, clear and objective requirements are defined for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["24"]}
{"title": "A machine learning-based approach for demarcating requirements in textual specifications\n", "abstract": " A simple but important task during the analysis of a textual requirements specification is to determine which statements in the specification represent requirements. In principle, by following suitable writing and markup conventions, one can provide an immediate and unequivocal demarcation of requirements at the time a specification is being developed. However, neither the presence nor a fully accurate enforcement of such conventions is guaranteed. The result is that, in many practical situations, analysts end up resorting to after-the-fact reviews for sifting requirements from other material in a requirements specification. This is both tedious and time-consuming. We propose an automated approach for demarcating requirements in free-form requirements specifications. The approach, which is based on machine learning, can be applied to a wide variety of specifications in different domains and with different writing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["24"]}
{"title": "Model-based simulation of legal policies: Framework, tool support, and validation\n", "abstract": " Simulation of legal policies is an important decision-support tool in domains such as taxation. The primary goal of legal policy simulation is predicting how changes in the law affect measures of interest, e.g., revenue. Legal policy simulation is currently implemented using a combination of spreadsheets and software code. Such a direct implementation poses a validation challenge. In particular, legal experts often lack the necessary software background to review complex spreadsheets and code. Consequently, these experts currently have no reliable means to check the correctness of simulations against the requirements envisaged by the law. A further challenge is that representative data for simulation may be unavailable, thus necessitating a data generator. A hard-coded generator is difficult to build and validate. We develop a framework for legal policy simulation that is aimed at addressing the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["24"]}
{"title": "Cost-effective strategies for the regression testing of database applications: Case study and lessons learned\n", "abstract": " Testing and, more specifically, the regression testing of database applications is highly challenging and costly. One can rely on production data or generate synthetic data, for example based on combinatorial techniques or operational profiles. Both approaches have drawbacks and advantages. Automating testing with production data is impractical and combinatorial test suites might not be representative of system operations.In this paper, based on a large scale case study in a representative development environment, we explore the cost and effectiveness of various approaches and their combination for the regression testing of database applications, based on production data and synthetic data generated through classification tree models of the input domain.The results confirm that combinatorial test suite specifications bear little relation to test suite specifications derived from the system operational profile\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["24"]}
{"title": "Assessing the impact of firewalls and database proxies on sql injection testing\n", "abstract": " This paper examines the effects and potential benefits of utilising Web Application Firewalls (WAFs) and database proxies in SQL injection testing of web applications and services. We propose testing the WAF itself to refine and evaluate its security rules and prioritise fixing vulnerabilities that are not protected by the WAF. We also propose using database proxies as oracles for black-box security testing instead of relying only on the output of the application under test. The paper also presents a case study of our proposed approaches on two sets of web services. The results indicate that testing through WAFs can be used to prioritise vulnerabilities and that an oracle that uses a database proxy finds more vulnerabilities with fewer tries than an oracle that relies only on the output of the application.", "num_citations": "14\n", "authors": ["24"]}
{"title": "RUBRIC: A flexible tool for automated checking of conformance to requirement boilerplates\n", "abstract": " Using requirement boilerplates is an effective way to mit-igate many types of ambiguity in Natural Language (NL) requirements and to enable more automated transformation and analysis of these requirements. When requirements are expressed using boilerplates, one must check, as a first qual-ity assurance measure, whether the requirements actually conform to the boilerplates. If done manually, boilerplate conformance checking can be laborious, particularly when requirements change frequently. We present RUBRIC (Re-qUirements BoileRplate sanIty Checker), a flexible tool for automatically checking NL requirements against boilerplates for conformance. RUBRIC further provides a range of di-agnostics to highlight potentially problematic syntactic con-structs in NL requirement statements. RUBRIC is based on a Natural Language Processing (NLP) technique, known as text chunking. A key advantage of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["24"]}
{"title": "Combining UML sequence and state machine diagrams for data-flow based integration testing\n", "abstract": " UML interaction diagrams are used during integration testing. However, this will typically not find all integration faults as some incorrect behaviors are only exhibited in certain states of the collaborating classes during interactions. State machine diagrams are typically used to model the behavior of state-dependent objects. This paper presents a technique to enhance interaction testing by accounting for state-based behavior as well as data-flow information. UML sequence and state machine diagrams are combined into a control-flow graph to then generate integration test cases, adapting well-known coupling-based, data-flow testing criteria. In order to assess our technique, we developed a prototype tool and applied it on a small case study. The results suggest that the proposed technique is more cost-effective than the most closely related approach reported in the literature, which only relies on control flow\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["24"]}
{"title": "Testing deadline misses for real-time systems using constraint optimization techniques\n", "abstract": " Safety-critical real-time applications are typically subject to stringent timing constraints which are dictated by the surrounding physical environments. Specifically, tasks in these applications need to finish their execution before given deadlines, otherwise the system is deemed unsafe. It is therefore important to test real-time systems for deadline misses. In this paper, we present a strategy for testing real-time applications that aim sat finding test scenarios in which deadline misses become more likely. We identify such test scenarios by searching the possible ways that a set of real-time tasks can be executed according to the scheduling policy of the operating system on which they are running. We formulate this search problem using a constraint optimization model that includes (1) a set of constraints capturing how a given set of tasks with real-time constraints are executed according to a particular scheduling policy\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["24"]}
{"title": "Model-based simulation of legal requirements: Experience from tax policy simulation\n", "abstract": " Using models for expressing legal requirements is now commonplace in Requirements Engineering. Models of legal requirements, on the one hand, facilitate communication between software engineers and legal experts, and on the other hand, provide a basis for systematic and automated analysis. The most prevalent application of legal requirements models is for checking the compliance of software systems with laws and regulations. In this experience paper, we explore a complementary application of legal requirements models, namely simulation. We observe that, in domains such as taxation, the same models that underlie legal compliance analysis bring important added value by enabling simulation. Concretely, this paper reports on the model-based simulation of selected legal requirements (policies) derived from Luxembourg's Income Tax Law. The simulation scenario considered in the paper is aimed at\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["24"]}
{"title": "A model-driven approach to representing and checking RBAC contextual policies\n", "abstract": " Among the various types of Role-based access control (RBAC) policies proposed in the literature, contextual policies take into account the user's location and the time at which she requests an access. The precise characterization of the context in such policies and the definition of an access decision procedure for them are non-trivial ntasks, since they have to take into account the various facets of the temporal and spatial expressions occurring in these policies. Existing approaches for modeling contextual policies do not support all the various spatio-temporal concepts and often do not provide an access decision procedure.", "num_citations": "13\n", "authors": ["24"]}
{"title": "NARCIA: an automated tool for change impact analysis in natural language requirements\n", "abstract": " We present NARCIA, a tool for analyzing the impact of change in natural language requirements. For a given change in a requirements document, NARCIA calculates quantitative scores suggesting how likely each requirements statement in the document is to be impacted. These scores, computed using Natural Language Processing (NLP), are used for sorting the requirements statements, enabling the user to focus on statements that are most likely to be impacted. To increase the accuracy of change impact analysis, NARCIA provides a mechanism for making explicit the rationale behind changes. NARCIA has been empirically evaluated on two industrial case studies. The results of this evaluation are briefly highlighted.", "num_citations": "13\n", "authors": ["24"]}
{"title": "VPML: an approach to detect design patterns of MOF-based modeling languages\n", "abstract": " A design pattern is a recurring and well-understood design fragment. In a model-driven engineering methodology, detecting occurrences of design patterns supports the activities of model comprehension and maintenance. With the recent explosion of domain-specific modeling languages, each with its own syntax and semantics, there has been a corresponding explosion in approaches to detecting design patterns that are so much tailored to those many languages that they are difficult to reuse. This makes developing generic analysis tools extremely hard. Such a generic tool is however desirable to reduce the learning curve for pattern designers as they specify patterns for different languages used to model different aspects of a system. In this paper, we propose a unified approach to detecting design patterns of MOF-based modeling languages. MOF is increasingly used to define modeling languages\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["24"]}
{"title": "Worst-case scheduling of software tasks\n", "abstract": " Real-Time Embedded Systems (RTES) in safety-critical domains, such as maritime and energy, must satisfy strict performance requirements to be deemed safe. Therefore, such systems have to be thoroughly tested to ensure their correct behavior even under the worst operating conditions. In this paper, we address the need of deriving worst case scenarios with respect to three common performance requirements, namely task deadlines, response time, and CPU usage. Specifically, we investigate whether this worst-case analysis can be effectively re-expressed as a Constrained Optimization Problem (COP) over the space of possible inputs to the system. Solving this problem means finding the sets of inputs that maximize the chance to violate performance requirements at runtime. Such inputs can in turn be used to test if the target RTES meets the expected performance even in the worst case. We develop\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["24"]}
{"title": "State-based testing: Industrial evaluation of the cost-effectiveness of round-trip path and sneak-path strategies\n", "abstract": " In the context of safety-critical software development, one important step in ensuring safe behavior is conformance testing, i.e., checking compliance between expected behavior and implementation. Round-trip path testing (RTP) is one example of conformance testing. Another essential step, however, is sneak-path testing, that is testing of how software reacts to unexpected events for a particular system state. Despite the importance of being systematic while testing, all testing activities take place, even for safety-critical software, under resource constraints. In this paper, we present an empirical evaluation of the cost-effectiveness of RTP when combined with sneak-path testing in the context of an industrial control system. Results highlight the importance of sneak-path testing since unexpected behavior is shown to be difficult to detect by other common, state-based test strategies. Results also suggest that sneak-path\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["24"]}
{"title": "Automatic selection of test execution plans from a video conferencing system product line\n", "abstract": " The Cisco Video Conferencing Systems (VCS) Product Line is composed of many distinct products that can be configured in many different ways. The validation of this product line is currently performed manually during test plan design and test executions' scheduling. For example, the testing of a specific VCS product leads to the manual selection of a set of test cases to be executed and scheduled, depending on the functionalities that are available on the product. In this paper, we develop an alternative approach where the variability of the VCS Product Line is captured by a feature model, while the variability within the set of test cases is captured by a component family model. Using the well-known pure:: variants tool approach that establishes links between those two models through restrictions, we can obtain relevant test cases automatically for the testing of a new VCS product. The novelty in this paper lies in the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["24"]}
{"title": "An active learning approach for improving the accuracy of automated domain model extraction\n", "abstract": " Domain models are a useful vehicle for making the interpretation and elaboration of natural-language requirements more precise. Advances in natural-language processing (NLP) have made it possible to automatically extract from requirements most of the information that is relevant to domain model construction. However, alongside the relevant information, NLP extracts from requirements a significant amount of information that is superfluous (not relevant to the domain model). Our objective in this article is to develop automated assistance for filtering the superfluous information extracted by NLP during domain model extraction. To this end, we devise an active-learning-based approach that iteratively learns from analysts\u0393\u00c7\u00d6 feedback over the relevance and superfluousness of the extracted domain model elements and uses this feedback to provide recommendations for filtering superfluous elements. We empirically\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["24"]}
{"title": "A model-driven approach to trace checking of pattern-based temporal properties\n", "abstract": " Trace checking is a procedure for evaluating requirements over a log of events produced by a system. This paper deals with the problem of performing trace checking of temporal properties expressed in TemPsy, a pattern-based specification language. The goal of the paper is to present a scalable and practical solution for trace checking, which can be used in contexts where relying on model-driven engineering standards and tools for property checking is a fundamental prerequisite.The main contributions of the paper are: a model-driven trace checking procedure, which relies on the efficient mapping of temporal requirements written in TemPsy into OCL constraints on a conceptual model of execution traces; the implementation of this trace checking procedure in the TEMPSY-CHECK tool; the evaluation of the scalability of TEMPSY-CHECK, applied to the verification of real properties derived from a case study of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["24"]}
{"title": "A search-based testing approach for XML injection vulnerabilities in web applications\n", "abstract": " In most cases, web applications communicate with web services (SOAP and RESTful). The former act as a front-end to the latter, which contain the business logic. A hacker might not have direct access to those web services (e.g., they are not on public networks), but can still provide malicious inputs to the web application, thus potentially compromising related services. Typical examples are XML injection attacks that target SOAP communications. In this paper, we present a novel, search-based approach used to generate test data for a web application in an attempt to deliver malicious XML messages to web services. Our goal is thus to detect XML injection vulnerabilities in web applications. The proposed approach is evaluated on two studies, including an industrial web application with millions of users. Results show that we are able to effectively generate test data (e.g., input values in an HTML form) that detect\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["24"]}
{"title": "Automated classification of legal cross references based on semantic intent\n", "abstract": " [Context and motivation] To elaborate legal compliance requirements, analysts need to read and interpret the relevant legal provisions. An important complexity while performing this task is that the information pertaining to a compliance requirement may be scattered across several provisions that are related via cross references. [Question/Problem] Prior research highlights the importance of determining and accounting for the semantics of cross references in legal texts during requirements elaboration, with taxonomies having been already proposed for this purpose. Little work nevertheless exists on automating the classification of cross references based on their semantic intent. Such automation is beneficial both for handling large and complex legal texts, and also for providing guidance to analysts. [Principal ideas/results] We develop an approach for automated classification of legal cross references\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["24"]}
{"title": "A UML-based quantitative framework for early prediction of resource usage and load in distributed real-time systems\n", "abstract": " This paper presents a quantitative framework for early prediction of resource usage and load in distributed real-time systems (DRTS). The prediction is based on an analysis of UML 2.0 sequence diagrams, augmented with timing information, to extract timed-control flow information. It is aimed at improving the early predictability of a DRTS by offering a systematic approach to predict, at the design phase, system behavior in each time instant during its execution. Since behavioral models such as sequence diagrams are available in early design phases of the software life cycle, the framework enables resource analysis at a stage when design decisions are still easy to change. Though we provide a general framework, we use network traffic as an example resource type to illustrate how the approach is applied. We also indicate how usage and load analysis of other types of resources (e.g., CPU and memory\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["24"]}
{"title": "Future of Software Engineering 2007: FoSE 2007: 23-25 May 2007, Minneapolis, Minnesota\n", "abstract": " Sponsored by IEEE Computer Society Technical Council on Software Engineering, The ACM Special Interest Group on Software Engineering (SIGSOFT)", "num_citations": "12\n", "authors": ["24"]}
{"title": "Software measurement and formal methods: a case study centered on TRIO+ Specifications\n", "abstract": " Presents a case study where product measures are defined for a formal specification language (TRIO+) and are validated as quality indicators. To this end, defect and effort data were collected during the development of a monitoring and control system for a power plant. We show that some of the underlying hypotheses of these measures are supported bp empirical results and that several measures are significant early indicators of specification change and effort. From a more general perspective, this study exemplifies one important advantage of formal specifications: they are measurable and can thus be better controlled, assessed and managed than informal ones.", "num_citations": "12\n", "authors": ["24"]}
{"title": "A natural language programming approach for requirements-based security testing\n", "abstract": " [en] To facilitate communication among stakeholders, software security requirements are typically written in natural language and capture both positive requirements (ie, what the system is supposed to do to ensure security) and negative requirements (ie, undesirable behavior undermining security).In this paper, we tackle the problem of automatically generat-ing executable security test cases from security requirements in natural language (NL). More precisely, since existing approaches for the generation of test cases from NL requirements verify only positive requirements, we focus on the problem of generating test cases from negative requirements.", "num_citations": "11\n", "authors": ["24"]}
{"title": "Automated inference of access control policies for web applications\n", "abstract": " In this paper, we present a novel, semi-automated approach to infer access control policies automatically for web-based applications. Our goal is to support the validation of implemented access control policies, even when they have not been clearly specified or documented. We use role-based access control as a reference model. Built on top of a suite of security tools, our approach automatically exercises a system under test and builds access spaces for a set of known users and roles. Then, we apply a machine learning technique to infer access rules. Inconsistent rules are then analysed and fed back to the process for further testing and improvement. Finally, the inferred rules can be validated based on pre-specified rules if they exist. Otherwise, the inferred rules are presented to human experts for validation and for detecting access control issues. We have evaluated our approach on two applications; one is\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["24"]}
{"title": "Model based test validation and oracles for data acquisition systems\n", "abstract": " This paper presents an automated, model based test validation and oracle approach for systems with complex input and output structures, such as Data Acquisition (DAQ) systems, which are common in many sectors including the satellite communications industry. We present a customised modelling methodology for such systems and a tool that automatically validates test inputs and, after test execution, applies an oracle that is based on mappings between the input and output. We also apply our proposed approach and tool to a complex industrial DAQ system and investigate the scalability and effectiveness of the approach in validating test cases, the DAQ system, or its specifications (captured as models). The results of the case study show that the approach is indeed scalable with respect to two dimensions: (1) model size and (2) test validation and oracle execution time. The size of the model for the DAQ system\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["24"]}
{"title": "A goal-based approach for qualification of new technologies: Foundations, tool support, and industrial validation\n", "abstract": " New technologies typically involve innovative aspects that are not addressed by the existing normative standards and hence are not assessable through common certification procedures. To ensure that new technologies can be implemented in a safe and reliable manner, a specific kind of assessment is performed, which in many industries, e.g., the energy sector, is known as Technology Qualification (TQ). TQ aims at demonstrating with an acceptable level of confidence that a new technology will function within specified limits. Expert opinion plays an important role in TQ, both to identify the safety and reliability evidence that needs to be developed and to interpret the evidence provided. Since there are often multiple experts involved in TQ, it is crucial to apply a structured process for eliciting expert opinions, and to use this information systematically when analyzing the satisfaction of the technology's safety and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["24"]}
{"title": "Model-based automated and guided configuration of embedded software systems\n", "abstract": " Configuring Integrated Control Systems (ICSs) is largely manual, time-consuming and error-prone. In this paper, we propose a model-based configuration approach that interactively guides engineers to configure software embedded in ICSs. Our approach verifies engineers\u0393\u00c7\u00d6 decisions at each configuration iteration, and further, automates some of the decisions. We use a constraint solver, SICStus Prolog, to automatically infer configuration decisions and to ensure the consistency of configuration data. We evaluated our approach by applying it to a real subsea oil production system. Specifically, we rebuilt a number of existing verified product configurations of our industry partner. Our experience shows that our approach successfully enforces consistency of configurations, can automatically infer up to 50% of the configuration decisions, and reduces the complexity of making configuration decisions.", "num_citations": "11\n", "authors": ["24"]}
{"title": "An experimental evaluation of the impact of system sequence diagrams and system operation contracts on the quality of the domain model\n", "abstract": " The Unified Modeling Language (UML) is an object-oriented analysis and design language widely used to created artifacts during the software system lifecycle. UML being a standard notation, without specific guidelines as to how to use it, it must be applied in the context of a specific software development process. The Unified Process (UP) is one such process, extensively used by the object-oriented community, which delivers software best practices via guidelines for all software lifecycle activities. The UP suggests many artifacts to be produced during the software lifecycle. But many practitioners are reluctant to use those artifacts as they question their benefits. System Sequence Diagrams and System Operation Contracts are artifacts, suggested by Larman in his well-known methodology, to complement standard UP artifacts with the intent of better understanding the input and output events related to the system\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["24"]}
{"title": "On the effectiveness of contracts as test oracles in the detection and diagnosis of race conditions and deadlocks in concurrent object-oriented software\n", "abstract": " The idea behind Design by Contract (DbC) is that a method defines a contract stating the requirements a client needs to fulfill to use it, the precondition, and the properties it ensures after its execution, the post condition. Though there exists ample support for DbC for sequential programs, applying DbC to concurrent programs presents several challenges. We have proposed a solution to these challenges in the context of Java as programming language and the Java Modeling language as specification language. This paper presents our findings when applying our DbC technique on an industrial case study to evaluate the ability of contract-based, runtime assertion checking code at detecting and diagnosing race conditions and deadlocks during system testing. The case study is a highly concurrent industrial system from the telecommunications domain, with actual faults. It is the first work to systematically investigate\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["24"]}
{"title": "Software Verification\u0393\u00c7\u00f6A Scalable, Model-Driven, Empirically Grounded Approach\n", "abstract": " Software is present in most systems across all industries, including energy, automotive, health care, maritime, aerospace, and banking, to name just a few. Software systems are increasingly taking on safety- and business-critical roles and growing in complexity. One crucial aspect of software development is therefore to ensure the dependability of such systems, that is, their reliability, safety, and robustness. This is achieved by several complementary means of verification, ranging from early analysis of system specifications and designs to systematic testing of the executable software. Such verification activities are, however, difficult and time-consuming. This stems in part from the sheer complexity of most software systems and because they must accommodate changing requirements from many stakeholders.", "num_citations": "11\n", "authors": ["24"]}
{"title": "A case study in productivity benchmarking: Methods and lessons learned\n", "abstract": " Productivity benchmarking allows software development projects and organizations to compare themselves to the market place in a given sector of industry. However, in practice benchmarking presents many difficulties such as identifying a meaningful basis of comparison. The European Space Agency (ESA) outsources many software projects. They have accumulated a large cost database from these projects. In this paper, we present a method for productivity benchmarking, as well as the productivity benchmarks we derived for one of our customers based on the ESA database. Furthermore, we provide usage scenarios for these models by describing how these models can be practically applied for benchmarking purposes. We developed alternative types of benchmarks using two different modelling techniques, namely least squares regression and regression trees. The most accurate model is obtained using least-squares regression, explains 92% of the variation in project effort, ie, R2= 0.92, corresponding to an average magnitude of relative error (MRE) of 0.34. Nevertheless, regression tree models are more intuitive and easier to apply for benchmarking purposes.", "num_citations": "11\n", "authors": ["24"]}
{"title": "A Validation of Object-Oriented Design Metrics\n", "abstract": " This paper presents the results of a study conducted at the University o-fMaryland in which we experimentally investigated the suite of Object-Oriented (00) design metrics introduced by [Chidamber&Kemerer, 1994]. In order to do this, we assessed these metrics as predictors of fault-prone classes. This study is complementary to [Lie&Henry, 1993] where the same suite of metrics had been used to assess frequencies of maintenance changes to classes. To perform our validation accurately, we collected data on the development of eight medium-sized information management systems based on identical requirements. All eight projects were developed using a sequential life cycle model, a well-known O0 analysis/design method and the C++ programming language. Based on experimental results, the advantages and drawbacks of these 00 metrics are discussed and suggestions for improvement are provided\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["24"]}
{"title": "Metamorphic security testing for web systems\n", "abstract": " Security testing verifies that the data and the resources of software systems are protected from attackers. Unfortunately, it suffers from the oracle problem, which refers to the challenge, given an input for a system, of distinguishing correct from incorrect behavior. In many situations where potential vulnerabilities are tested, a test oracle may not exist, or it might be impractical due to the many inputs for which specific oracles have to be defined. In this paper, we propose a metamorphic testing approach that alleviates the oracle problem in security testing. It enables engineers to specify metamorphic relations (MRs) that capture security properties of the system. Such MRs are then used to automate testing and detect vulnerabilities. We provide a catalog of 22 system-agnostic MRs to automate security testing in Web systems. Our approach targets 39% of the OWASP security testing activities not automated by state-of-the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["24"]}
{"title": "Effective fault localization of automotive Simulink models: achieving the trade-off between test oracle effort and fault localization accuracy\n", "abstract": " One promising way to improve the accuracy of fault localization based on statistical debugging is to increase diversity among test cases in the underlying test suite. In many practical situations, adding test cases is not a cost-free option because test oracles are developed manually or running test cases is expensive. Hence, we require to have test suites that are both diverse and small to improve debugging. In this paper, we focus on improving fault localization of Simulink models by generating test cases. We identify four test objectives that aim to increase test suite diversity. We use four objectives in a search-based algorithm to generate diversified but small test suites. To further minimize test suite sizes, we develop a prediction model to stop test generation when adding test cases is unlikely to improve fault localization. We evaluate our approach using three industrial subjects. Our results show (1) expanding\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["24"]}
{"title": "Automated generation of constraints from use case specifications to support system testing\n", "abstract": " System testing plays a crucial role in safety-critical domains, e.g., automotive, where system test cases are used to demonstrate the compliance of software with its functional and safety requirements. Unfortunately, since requirements are typically written in natural language, significant engineering effort is required to derive test cases from requirements. In such a context, automated support for generating system test cases from requirements specifications written in natural language would be highly beneficial. Unfortunately, existing approaches have limited applicability. For example, some of them require that software engineers provide formal specifications that capture some of the software behavior described using natural language. The effort needed to define such specifications is usually a significant deterrent for software developers. This paper proposes an approach, OCLgen, which largely automates the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["24"]}
{"title": "Incremental reconfiguration of product specific use case models for evolving configuration decisions\n", "abstract": " Context and motivation: Product Line Engineering (PLE) is increasingly common practice in industry to develop complex systems for multiple customers with varying needs. In many business contexts, use cases are central development artifacts for requirements engineering and system testing. In such contexts, use case configurators can play a significant role to capture variable and common requirements in Product Line (PL) use case models and to generate Product Specific (PS) use case models for each new customer in a product family. Question/Problem: Although considerable research has been devoted to use case configurators, little attention has been paid to supporting the incremental reconfiguration of use case models with evolving configuration decisions. Principal ideas/results: We propose, apply, and assess an incremental reconfiguration approach to support evolving configuration\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["24"]}
{"title": "GemRBAC-DSL: a high-level specification language for role-based access control policies\n", "abstract": " A role-based access control (RBAC) policy restricts a user to perform operations based on her role within an organization. Several RBAC models have been proposed to represent different types of RBAC policies. However, the expressiveness of these models has not been matched by specification languages for RBAC policies. Indeed, existing policy specification languages do not support all the types of RBAC policies defined in the literature.", "num_citations": "10\n", "authors": ["24"]}
{"title": "Useful software engineering research-leading a double-agent life\n", "abstract": " Though software engineering is in essence an engineering discipline, that is a discipline whose aim is\" the construction of machinery and other artifacts for use by society\", software engineering research has always been struggling to demonstrate impact. This is reflected in part by the funding challenges that the discipline faces in many countries, the difficulties we have to attract industrial participants to our conferences, and the scarcity of papers reporting industrial case studies.", "num_citations": "10\n", "authors": ["24"]}
{"title": "A UML/MARTE model analysis method for detection of data races in concurrent systems\n", "abstract": " The earlier concurrency problems are identified, the less costly they are to fix. As larger, more complex concurrent systems are developed, early detection of problems is made increasingly difficult. We have developed a general approach meant to be used in the context of Model Driven Development. Our approach is based on the analysis of design models expressed in the Unified Modeling Language (UML) and uses specifically designed genetic algorithms to detect concurrency problems. Our main motivation is to devise practical solutions that are applicable in the context of UML design of concurrent systems without requiring additional modeling. All relevant concurrency information is extracted from UML models that comply with the UML Modeling and Analysis of Real-Time and Embedded Systems (MARTE) profile. Our approach was shown to work for both deadlocks and starvation. The current paper\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["24"]}
{"title": "Building resource and quality management models for software inspections\n", "abstract": " Inspection of software artifacts, eg, code, is a well-accepted approach to improve software quality and to lower software development costs [1][2][18]. However, within a particular organization or across organizations, inspections vary widely with respect to their defect detection effectiveness and efficiency (ie, cost-effectiveness)[3][4][6]. In addition, what is meant by inspection effectiveness and efficiency is often not clearly defined in quantitative terms. As a consequence, because the use of collected inspection data is not specified, project managers only perceive the cost of inspections and not the benefits of achieving higher product quality [18]. In this paper, we describe and use quantitative predictive models of inspection effectiveness and efficiency within a given environment in order to control the quality of inspected products and manage inspection resources. First, we provide an operational definition of inspection\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["24"]}
{"title": "System testing of timing requirements based on use cases and timed automata\n", "abstract": " In the context of use-case centric development and requirements-driven testing, this paper addresses the problem of automatically deriving system test cases to verify timing requirements. Inspired by engineering practice in an automotive software development context, we rely on an analyzable form of use case specifications and augment such functional descriptions with timed automata, capturing timing requirements, following a methodology aiming at minimizing modeling overhead. We automate the generation of executable test cases using a test strategy based on maximizing test suite diversity and building over the UPPAAL model checker. Initial empirical results based on an industrial case study provide evidence of the effectiveness of the approach.", "num_citations": "9\n", "authors": ["24"]}
{"title": "A change management approach in product lines for use case-driven development and testing\n", "abstract": " [en] In this paper, driven by industrial needs, we present a change management approach for product lines within the context of use case-driven development and testing. As part of the approach, we first provide a modeling method to support variability modeling in Product Line (PL) use case diagrams, specifications, and domain models, intentionally avoiding any reliance on feature models and thus avoiding unnecessary modeling and traceability overhead. Then, we introduce a use case-driven configuration approach based on the proposed modelling method to automatically generate Product Specific (PS) use case and domain models from the PL models and configuration decisions. Building on this, we provide a change impact analysis approach for evolving configuration decisions in PL use case models. In addition, we plan to develop a change impact analysis approach for evolving PL use case models and an automated regression test selection technique for evolving configuration decisions and PL models.", "num_citations": "9\n", "authors": ["24"]}
{"title": "A model-based framework for probabilistic simulation of legal policies\n", "abstract": " Legal policy simulation is an important decision-support tool in domains such as taxation. The primary goal of legal policy simulation is predicting how changes in the law affect measures of interest, e.g., revenue. Currently, legal policies are simulated via a combination of spreadsheets and software code. This poses a validation challenge both due to complexity reasons and due to legal experts lacking the expertise to understand software code. A further challenge is that representative data for simulation may be unavailable, thus necessitating a data generator. We develop a framework for legal policy simulation that is aimed at addressing these challenges. The framework uses models for specifying both legal policies and the probabilistic characteristics of the underlying population. We devise an automated algorithm for simulation data generation. We evaluate our framework through a case study on Luxembourg's\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["24"]}
{"title": "Identifying optimal trade-offs between cpu time usage and temporal constraints using search\n", "abstract": " Integration of software from different sources is a critical activity in many embedded systems across most industry sectors. Software integrators are responsible for producing reliable systems that fulfil various functional and performance requirements. In many situations, these requirements inversely impact one another. In particular, embedded system integrators often need to make compromises regarding some of the functional system properties to optimize the use of various resources, such as CPU time. In this paper, motivated by challenges faced by industry, we introduce a multi-objective decision support approach to help balance the minimization of CPU time usage and the satisfaction of temporal constraints in automotive systems. We develop a multi-objective, search-based optimization algorithm, specifically designed to work for large search spaces, to identify optimal trade-off solutions fulfilling these two\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["24"]}
{"title": "Empirically evaluating the impact of applying aspect state machines on modeling quality and effort\n", "abstract": " Aspect-Oriented Modeling (AOM) has been the subject of intense research over the last decade and aims to provide numerous benefits to modeling, such as enhanced modularization, easier evolution, higher applicability as well as reduced modeling effort. However, these benefits can only be obtained at the cost of learning and applying new modeling approaches. Studying their applicability is therefore important to assess whether they are worth using in practice. In this paper, we report the first controlled experiment to assess the applicability of AOM, focusing on a recently published UML profile (AspectSM). This profile was originally designed to support model-based robustness testing in an industrial context but is applicable to the behavioral modeling of other crosscutting concerns. This experiment assesses the applicability of AspectSM from two aspects: the quality of derived state machines and the effort required to build them. With AspectSM, a crosscutting behavior is modeled using socalled \u0393\u00c7\u00a3aspect state machine\u0393\u00c7\u00a5. The applicability of aspect state machines is evaluated by comparing them with standard UML state machines that directly model the entire system behavior, including crosscutting concerns. The quality of both aspect and standard UML state machines derived by subjects is measured by comparing them against their corresponding reference state machines. Results show that aspect state machines derived with AspectSM are significantly more complete and correct though AspectSM took significantly more time than the standard approach, probably due to a lack of familiarity of the subjects.", "num_citations": "9\n", "authors": ["24"]}
{"title": "Requirements for the knowledge-based support of software engineering measurement plans\n", "abstract": " In order to improve the quality of software systems, measurement programs have been implemented in many companies to support process improvement activities. The planning and implementation of a successful measurement program requires, in practice, a significant amount of effect. Cost may be reduced and quality of measurement may be improved by providing knowledge-based support and reusing experiences gathered on past measurement programs. In this article, we state the requirements for the knowledge-based support of planning measurement programs based on the Goal/Question/Metric paradigm. Reuse opportunities are precisely identified, the knowledge to be captured for effective reuse is identified and structured, and reuse scenarios are provided.", "num_citations": "9\n", "authors": ["24"]}
{"title": "Mining assumptions for software components using machine learning\n", "abstract": " Software verification approaches aim to check a software component under analysis for all possible environments. In reality, however, components are expected to operate within a larger system and are required to satisfy their requirements only when their inputs are constrained by environment assumptions. In this paper, we propose EPIcuRus, an approach to automatically synthesize environment assumptions for a component under analysis (ie, conditions on the component inputs under which the component is guaranteed to satisfy its requirements). EPIcuRus combines search-based testing, machine learning and model checking. The core of EPIcuRus is a decision tree algorithm that infers environment assumptions from a set of test results including test cases and their verdicts. The test cases are generated using search-based testing, and the assumptions inferred by decision trees are validated through model\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["24"]}
{"title": "Revisiting model-driven engineering for run-time verification of business processes\n", "abstract": " Run-time verification has been widely advocated in the last decade as a key technique to check whether the execution of a business process and its interactions with partner services comply with the application requirements. Despite the substantial research performed in this area, there are very few approaches that leverage model-driven engineering (MDE) methodologies and integrate them in the development process of applications based on business process descriptions. In this position paper we describe our vision and present the research roadmap for adopting MDE techniques in the context of run-time verification of business processes, based on our early experience with a public service partner in the domain of eGovernment. We maintain that within this context, the adoption of MDE would contribute in three ways: 1)\u252c\u00e1expressing, at a logical level, complex properties to be checked at run time using a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["24"]}
{"title": "Investigating the impact of a measurement program on software quality\n", "abstract": " ContextMeasurement programs have been around for several decades but have been often misused or misunderstood by managers and developers. This misunderstanding prevented their adoption despite their many advantages.ObjectiveIn this paper, we present the results of an empirical study on the impact of a measurement program, MQL (\u0393\u00c7\u00a3Mise en Qualit\u251c\u2310 du Logiciel\u0393\u00c7\u00a5, French for \u0393\u00c7\u00a3Quality Software Development\u0393\u00c7\u00a5), in an industrial context.MethodWe analyzed data collected on 44 industrial systems of different sizes: 22 systems were developed using MQL while the other 22 used ad-hoc approaches to assess and control quality (control group, referred to as \u0393\u00c7\u00a3ad-hoc systems\u0393\u00c7\u00a5). We studied the impact of MQL on a set of nine variables: six quality factors (maintainability, evolvability, reusability, robustness, testability, and architecture quality), corrective-maintenance effort, code complexity, and the presence of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["24"]}
{"title": "Model-driven, network-context sensitive intrusion detection\n", "abstract": " Intrusion Detection Systems (IDSs) have the reputation of generating many false positives. Recent approaches, known as stateful IDSs, take the state of communication sessions into account to address this issue. A substantial reduction of false positives, however, requires some correlation between the state of the session, known vulnerabilities, and the gathering of more network context information by the IDS than what is currently done (e.g., configuration of a node, its operating system, running applications). In this paper we present an IDS approach that attempts to decrease the number of false positives by collecting more network context and combining this information with known vulnerabilities. The approach is model-driven as it relies on the modeling of packet and network information as UML class diagrams, and the definition of intrusion detection rules as OCL expressions constraining these diagrams\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["24"]}
{"title": "Performance stress testing of real-time systems using genetic algorithms\n", "abstract": " Reactive real-time systems must react to external events within time constraints: Triggered tasks must execute within deadlines. Through performance stress testing, the risks of performance failures in real-time systems are reduced. We develop a methodology for the derivation of test cases that aims at maximizing the chances of critical deadline misses within a system. This testing activity is referred to as performance stress testing. Performance stress testing is based on the system task architecture, where a task is a single unit of work carried out by the system. The method developed is based on genetic algorithms and is augmented with a tool, Real Time Test Tool (RTTT). Case studies performed on the tool show that it may actually help testers identify test cases that are likely to exhibit missed deadlines during testing or, even worse, ones that are certain to lead to missed deadlines, despite schedulability analysis assertions.", "num_citations": "8\n", "authors": ["24"]}
{"title": "METRIX: a tool for software-risk analysis and management\n", "abstract": " In order to improve the production process and to guarantee the quality of the manufactured products, more and more sophisticated data is needed. Also mathematical models are needed to exploit them: these models have to be useful for prediction and easy to interpret so that remedial actions may be taken, as early as possible, in order to control and optimize the production process. These models enhance their efficiency when integrated into decision support tools. The previous remarks, applied to the software development process, have led the authors to study an efficient modeling technique to model software quality and to automate it in the METRIX tool. The paper presents together the modeling technique (i.e. optimized set reduction), the tool that supports it (i.e. METRIX), and the global strategy in which the tool is to be used. The efficiency of the modeling technique is demonstrated by results obtained for the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["24"]}
{"title": "Trace-checking signal-based temporal properties: A model-driven approach\n", "abstract": " Signal-based temporal properties (SBTPs) characterize the behavior of a system when its inputs and outputs are signals over time; they are very common for the requirements specification of cyber-physical systems. Although there exist several specification languages for expressing SBTPs, such languages either do not easily allow the specification of important types of properties (such as spike or oscillatory behaviors), or are not supported by (efficient) trace-checking procedures. In this paper, we propose SB-TemPsy, a novel model-driven trace-checking approach for SBTPs. SB-TemPsy provides (i) SB-TemPsy-DSL, a domain-specific language that allows the specification of SBTPs covering the most frequent requirement types in cyber-physical systems, and (ii) SB-TemPsy-Check, an efficient, model-driven trace-checking procedure. This procedure reduces the problem of checking an SB-TemPsy-DSL property\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["24"]}
{"title": "Automatic generation of acceptance test cases from use case specifications: an nlp-based approach\n", "abstract": " Acceptance testing is a validation activity performed to ensure the conformance of software systems with respect to their functional requirements. In safety critical systems, it plays a crucial role since it is enforced by software standards, which mandate that each requirement be validated by such testing in a clearly traceable manner. Test engineers need to identify all the representative test execution scenarios from requirements, determine the runtime conditions that trigger these scenarios, and finally provide the input data that satisfy these conditions. Given that requirements specifications are typically large and often provided in natural language (e.g., use case specifications), the generation of acceptance test cases tends to be expensive and error-prone. In this paper, we present Use Case Modeling for System-level, Acceptance Tests Generation (UMTG), an approach that supports the generation of executable\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["24"]}
{"title": "A query system for extracting requirements-related information from legal texts\n", "abstract": " Searching legal texts for relevant information is a complex and expensive activity. The search solutions offered by present-day legal portals are targeted primarily at legal professionals. These solutions are not adequate for requirements analysts whose objective is to extract domain knowledge including stakeholders, rights and duties, and business processes that are relevant to legal requirements. Semantic Web technologies now enable smart search capabilities and can be exploited to help requirements analysts in elaborating legal requirements. In our previous work, we developed an automated framework for extracting semantic metadata from legal texts. In this paper, we investigate the use of our metadata extraction framework as an enabler for smart legal search with a focus on requirements engineering activities. We report on our industrial experience helping the Government of Luxembourg provide an\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["24"]}
{"title": "MCP: a security testing tool driven by requirements\n", "abstract": " We present MCP, a tool for automatically generating executable security test cases from misuse case specifications in natural language (i.e., use case specifications capturing the behavior of malicious users). MCP relies on Natural Language Processing (NLP), a restricted form of misuse case specifications, and a test driver API implementing basic utility functions for security testing. NLP is used to identify the activities performed by the malicious user and the control flow of misuse case specifications. MCP matches the malicious user's activities to the methods of the provided test driver API in order to generate executable security test cases that perform the activities described in the misuse case specifications. MCP has been successfully evaluated on an industrial case study.", "num_citations": "7\n", "authors": ["24"]}
{"title": "Decision support for security-control identification using machine learning\n", "abstract": " [Context & Motivation] In many domains such as healthcare and banking, IT systems need to fulfill various requirements related to security. The elaboration of security requirements for a given system is in part guided by the controls envisaged by the applicable security standards and best practices. [Problem] An important difficulty that analysts have to contend with during security requirements elaboration is sifting through a large number of security controls and determining which ones have a bearing on the security requirements for a given system. This challenge is often exacerbated by the scarce security expertise available in most organizations. [Principal ideas/results] In this paper, we develop automated decision support for the identification of security controls that are relevant to a specific system in a particular context. Our approach, which is based on machine learning, leverages historical data from security\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["24"]}
{"title": "Practical model-driven data generation for system testing\n", "abstract": " The ability to generate test data is often a necessary prerequisite for automated software testing. For the generated data to be fit for its intended purpose, the data usually has to satisfy various logical constraints. When testing is performed at a system level, these constraints tend to be complex and are typically captured in expressive formalisms based on first-order logic. Motivated by improving the feasibility and scalability of data generation for system testing, we present a novel approach, whereby we employ a combination of metaheuristic search and Satisfiability Modulo Theories (SMT) for constraint solving. Our approach delegates constraint solving tasks to metaheuristic search and SMT in such a way as to take advantage of the complementary strengths of the two techniques. We ground our work on test data models specified in UML, with OCL used as the constraint language. We present tool support and an evaluation of our approach over three industrial case studies. The results indicate that, for complex system test data generation problems, our approach presents substantial benefits over the state of the art in terms of applicability and scalability.", "num_citations": "7\n", "authors": ["24"]}
{"title": "Oracles for testing software timeliness with uncertainty\n", "abstract": " Uncertainty in timing properties (e.g., detection time of external events) is a common occurrence in embedded software systems, since these systems interact with complex physical environments. Such time uncertainty leads to non-determinism. For example, time-triggered operations may either generate different valid outputs across different executions or experience failures (e.g., results not being generated in the expected time window) that occur only occasionally over many executions. For these reasons, time uncertainty makes the generation of effective test oracles for timing requirements a challenging task. To address the above challenge, we propose Stochastic Testing with Unique Input Output Sequences, an approach for the automated generation of stochastic oracles that verify the capability of a software system to fulfill timing constraints in the presence of time uncertainty. Such stochastic oracles entail the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["24"]}
{"title": "Legal markup generation in the large: An experience report\n", "abstract": " Legal markup (metadata) is an important prerequisite for the elaboration of legal requirements. Manually encoding legal texts into a markup representation is laborious, specially for large legal corpora amassed over decades and centuries. At the same time, automating the generation of markup in a fully accurate manner presents a challenge due to the flexibility of the natural-language content in legal texts and variations in how these texts are organized. Following an action research method, we successfully collaborated with the Government of Luxembourg in transitioning five major legislative codes from plain-text to a legal markup format. Our work focused on generating markup for the structural elements of the underlying codes. The technical basis for our work is an adaptation and enhancement of an academic markup generation tool developed in our prior research [1]. We reflect on the experience gained from\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["24"]}
{"title": "Augmenting field data for testing systems subject to incremental requirements changes\n", "abstract": " When testing data processing systems, software engineers often use real-world data to perform system-level testing. However, in the presence of new data requirements, software engineers may no longer benefit from having real-world data with which to perform testing. Typically, new test inputs complying with the new requirements have to be manually written. We propose an automated model-based approach that combines data modelling and constraint solving to modify existing field data to generate test inputs for testing new data requirements. The approach scales in the presence of complex and structured data, thanks to both the reuse of existing field data and the adoption of an innovative input generation algorithm based on slicing the model into parts. We validated the scalability and effectiveness of the proposed approach using an industrial case study. The empirical study shows that the approach scales in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["24"]}
{"title": "Assessing quality and effort of applying aspect state machines for robustness testing: A controlled experiment\n", "abstract": " Aspect-Oriented Modeling (AOM) has been the subject of intense research over the last decade and aims to provide numerous benefits to modeling, such as enhanced modularization, easier evolution, higher quality as well as reduced modeling effort. However, these benefits can only be obtained at the cost of learning and applying new modeling approaches. Studying their applicability is therefore important to assess whether they are worth using in practice. In this paper, we report a controlled experiment to assess the applicability of AOM, focusing on a recently published UML profile (AspectSM). This profile was originally designed to support model-based robustness testing in an industrial context but is applicable to the behavioral modeling of other crosscutting concerns. This experiment assesses the applicability of AspectSM from two aspects: the quality of derived state machines and the effort required to build\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["24"]}
{"title": "A systematic review of the application and empirical investigation of evolutionary testing\n", "abstract": " Metaheuristic search techniques have been extensively used to automate the process of generating test cases and thus providing solutions for a more cost-effective testing process. This approach to test automation, often coined as \u0393\u00c7\u00a3Evolutionary Testing\u0393\u00c7\u00a5(ET), has been used for a wide variety of test case generation purposes, differing in terms of test objectives, test levels, and other characteristics. Since ET techniques are heuristic by nature, they must be empirically investigated in terms of how costly and effective they are at reaching their test objectives and whether they scale up to realistic development artifacts. However, approaches to empirically study ET techniques have shown wide variation in the literature.This paper presents the results of a systematic, comprehensive review that aims at characterizing how empirical studies have been designed to investigate ET cost-effectiveness and what empirical evidence is available in the literature regarding ET cost-effectiveness and scalability. We also provide a framework that drives the data collection process of this systematic review and can be the starting point of guidelines on how ET techniques can be empirically assessed.", "num_citations": "7\n", "authors": ["24"]}
{"title": "Towards the reverse engineering of UML sequence diagrams for distributed real-time Java software\n", "abstract": " This paper proposes a comprehensive methodology and instrumentation infrastructure for the reverse-engineering of UML (Unified Modeling Language) sequence diagrams from dynamic analysis. One motivation is of course to help people understand the behavior of systems with no (complete) documentation. However, such reverse-engineered dynamic models can also be used for quality assurance purposes. They can, for example, be compared with design sequence diagrams and the conformance of the implementation to the design can thus be verified. Furthermore, discrepancies can also suggest failures in meeting the specifications. We formally define our approach using metamodels and consistency rules. The instrumentation is based on Aspect-Oriented Programming in order to alleviate the overhead usually associated with source code instrumentation. A case study is discussed to demonstrate the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["24"]}
{"title": "Using simulation to build inspection efficiency benchmarks for development projects\n", "abstract": " It is difficult for organizations introducing and using software inspections to evaluate how efficient they are. However, it is of practical importance to determine whether they have been effectively implemented or whether corrective actions are necessary to bring them up to standard. We present in this paper a procedure for building inspection efficiency benchmarks based on simulation and typical inspection data. Based on most of the data published in the literature, we build an industry-wide benchmark which intends to capture the current practice regarding inspection efficiency. Moreover, we discuss how this benchmark construction procedure can be used to build enterprise specific benchmarks. Last, we assess how robust we can expect them to be in varying conditions by distorting their input distributions.", "num_citations": "7\n", "authors": ["24"]}
{"title": "Building an experience factory for maintenance\n", "abstract": " This paper reports the preliminary results of a study of the software maintenance process in the Flight Dynamics Division (FDD) of the National Aeronautics and Space Administration/Goddard Space Flight Center (NASA/GSFC). This study is being conducted by the Software Engineering Laboratory (SEL), a research organization sponsored by the Software Engineering Branch of the FDD, which investigates the effectiveness of software engineering technologies when applied to the development of applications software.This software maintenance study began in October 1993 and is being conducted using the Quality Improvement Paradigm (QIP), a process improvement strategy based on three iterative steps: understanding, assessing, and packaging. The preliminary results presented in this paper represent the outcome of the understanding phase, during which SEL researchers characterized the maintenance\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["24"]}
{"title": "Automated repair of feature interaction failures in automated driving systems\n", "abstract": " In the past years, several automated repair strategies have been proposed to fix bugs in individual software programs without any human intervention. There has been, however, little work on how automated repair techniques can resolve failures that arise at the system-level and are caused by undesired interactions among different system components or functions. Feature interaction failures are common in complex systems such as autonomous cars that are typically built as a composition of independent features (ie, units of functionality). In this paper, we propose a repair technique to automatically resolve undesired feature interaction failures in automated driving systems (ADS) that lead to the violation of system safety requirements. Our repair strategy achieves its goal by (1) localizing faults spanning several lines of code,(2) simultaneously resolving multiple interaction failures caused by independent faults,(3\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["24"]}
{"title": "Dynamic adaptation of software-defined networks for IoT systems: A search-based approach\n", "abstract": " The concept of Internet of Things (IoT) has led to the development of many complex and critical systems such as smart emergency management systems. IoT-enabled applications typically depend on a communication network for transmitting large volumes of data in unpredictable and changing environments. These networks are prone to congestion when there is a burst in demand, eg, as an emergency situation is unfolding, and therefore rely on configurable software-defined networks (SDN). In this paper, we propose a dynamic adaptive SDN configuration approach for IoT systems. The approach enables resolving congestion in real time while minimizing network utilization, data transmission delays and adaptation costs. Our approach builds on existing work in dynamic adaptive search-based software engineering (SBSE) to reconfigure an SDN while simultaneously ensuring multiple quality of service criteria. We\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["24"]}
{"title": "Search-based multi-vulnerability testing of XML injections in web applications\n", "abstract": " Modern web applications often interact with internal web services, which are not directly accessible to users. However, malicious user inputs can be used to exploit security vulnerabilities in web services through the application front-ends. Therefore, testing techniques have been proposed to reveal security flaws in the interactions with back-end web services, e.g., XML Injections (XMLi). Given a potentially malicious message between a web application and web services, search-based techniques have been used to find input data to mislead the web application into sending such a message, possibly compromising the target web service. However, state-of-the-art techniques focus on (search for) one single malicious message at a time.               Since, in practice, there can be many different kinds of malicious messages, with only a few of them which can possibly be generated by a given front-end, searching\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["24"]}
{"title": "An empirical study on the potential usefulness of domain models for completeness checking of requirements\n", "abstract": " Domain modeling is a common strategy for mitigating incompleteness in requirements. While the benefits of domain models for checking the completeness of requirements are anecdotally known, these benefits have never been evaluated systematically. We empirically examine the potential usefulness of domain models for detecting incompleteness in natural-language requirements. We focus on requirements written as \u0393\u00c7\u00a3shall\u0393\u00c7\u00a5-style statements and domain models captured using UML class diagrams. Through a randomized simulation process, we analyze the sensitivity of domain models to omissions in requirements. Sensitivity is a measure of whether a domain model contains information that can lead to the discovery of requirements omissions. Our empirical research method is case study research in an industrial setting. We have experts construct domain models in three distinct industry domains. We\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["24"]}
{"title": "Model-driven trace diagnostics for pattern-based temporal specifications\n", "abstract": " Offline trace checking tools check whether a specification holds on a log of events recorded at run time; they yield a verification verdict (typically a boolean value) when the checking process ends. When the verdict is false, a software engineer needs to diagnose the property violations found in the trace in order to understand their cause and, if needed, decide for corrective actions to be performed on the system. However, a boolean verdict may not be informative enough to perform trace diagnostics, since it does not provide any useful information about the cause of the violation and because a property can be violated for multiple reasons.", "num_citations": "6\n", "authors": ["24"]}
{"title": "Joanaudit: A tool for auditing common injection vulnerabilities\n", "abstract": " JoanAudit is a static analysis tool to assist security auditors in auditing Web applications and Web services for common injection vulnerabilities during software development. It automatically identifies parts of the program code that are relevant for security and generates an HTML report to guide security auditors audit the source code in a scalable way. JoanAudit is configured with various security-sensitive input sources and sinks relevant to injection vulnerabilities and standard sanitization procedures that prevent these vulnerabilities. It can also automatically fix some cases of vulnerabilities in source code\u0393\u00c7\u00f6cases where inputs are directly used in sinks without any form of sanitization\u0393\u00c7\u00f6by using standard sanitization procedures. Our evaluation shows that by using JoanAudit, security auditors are required to inspect only 1% of the total code for auditing common injection vulnerabilities. The screen-cast demo is\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["24"]}
{"title": "Minimizing CPU time shortage risks in integrated embedded software\n", "abstract": " A major activity in many industries is to integrate software artifacts such that the functional and performance requirements are properly taken care of. In this paper, we focus on the problem of minimizing the risk of CPU time shortage in integrated embedded systems. In order to minimize this risk, we manipulate the start time (offset) of the software executables such that the system real-time constraints are satisfied, and further, the maximum CPU time usage is minimized. We develop a number of search-based optimization algorithms, specifically designed to work for large search spaces, to compute offsets for concurrent software executables with the objective of minimizing CPU usage. We evaluated and compared our algorithms by applying them to a large automotive software system. Our experience shows that our algorithms can automatically generate offsets such that the maximum CPU usage is very close to the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["24"]}
{"title": "CRESCO: Construction of Evidence Repositories for Managing Standards Compliance\n", "abstract": " We describe CRESCO, a tool for Construction of Evidence REpositories for Managing Standards COmpliance. CRESCO draws on Model Driven Engineering (MDE) technologies to generate a database repository schema from the evidence requirements of a given standard, expressed as a UML class diagram. CRESCO in addition generates a web-based user interface for building and manipulating evidence repositories based on the schema. CRESCO is targeted primarily at addressing the tool infrastructure needs for supporting the collection and management of safety evidence data. A systematic treatment of evidence information is a key prerequisite for demonstration of compliance to safety standards, such as IEC 61508, during the safety certification process.", "num_citations": "6\n", "authors": ["24"]}
{"title": "The impact of automated support for linking equivalent requirements based on similarity measures\n", "abstract": " When developing systems of systems, requirements tend to be redundant especially when running large numbers of projects, with many requirements per project, and diverse sources of requirements. It is therefore necessary to consolidate requirements by identifying the ones that are equivalent in order to avoid redundant work. The aim of this paper is to evaluate requirement similarity measurement to support analysts when linking equivalent requirements. The evaluation is conducted based on the requirements management process of an Italian company in the defense and aerospace domain. Our empirical investigation combines a controlled experiment with graduate students and an industrial case study. Results clearly show that one cannot expect any significant advantage in general. The level of support provided by similarity measures significantly depends on their level of credibility, that is the extent to which similarity measurement reliably indicates the equivalence of requirements. On average, given the credibility distribution observed in our industrial case study, showing similarity measurement to analysts is expected to: 1) improve by 20% the number of equivalence links identified per minute and 2) decrease by 40% the number of incorrect links. Finally, we investigate whether there is an effective way to combine human judgment and similarity measurement to effectively determine equivalence links. Based on machine learning, our approach yielded positive results both in terms of the correctness of the links and the speed at which they are established. Moreover, this hybrid solution is effective even when the credibility of similarity\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["24"]}
{"title": "Toward a comprehensive and systematic methodology for class integration testing\n", "abstract": " This report is a first attempt towards a comprehensive, systematic methodology for class interface testing in the context of client/server relationships. The proposed approach builds on and combines existing techniques. It first consists in selecting a subset of the method sequences defined for the class testing of the client class, based on an analysis of the interactions between the client and the server methods. Coupling information is then used to determine the conditions, ie, values for parameters and data members, under which the selected client method sequences are to be executed so as to exercise the interaction. The approach is illustrated by means of an abstract example and its cost-effectiveness is evaluated through two case studies.", "num_citations": "6\n", "authors": ["24"]}
{"title": "Modelling the Factors Driving the Quality of Meetings in the Software Development Process\n", "abstract": " It is a well-known fact that communication mechanisms within large-scale software development are important to ensure the success of projects. The empirical study described here investigates factors driving the outcome quality of various types of meetings, taking place during software development activities at Bosch Telecom Private Networks. The dependent variable under study is the perceived quality of the results of a meeting; the independent variables investigated are the number of participants, the setting of a meeting (reason, leadership style), the number of organisational roles involved, and its duration. The analysis methods used to explore and confirm the relationships between independent and dependent variables include Classification Trees and Logistic Regression. In addition, a questionnaire was designed and administered to help us interpret the outcome of the statistical analysis. This study is based on a data set of about two hundred data points (ie, meetings) and, therefore, made multivariate analysis possible. The results of the study show that there is supporting evidence that several attributes are significant factors driving the quality of meetings. The questionnaire results helped us gain a better understanding of how various factors influence a meeting\u0393\u00c7\u00d6s outcome. This, in turn, leads to suggestions on the way to improve communication through meetings. The methodology presented here for data collection and analysis is widely reusable in other software development organisations, and for other process improvement issues to be addressed by measurement.", "num_citations": "6\n", "authors": ["24"]}
{"title": "Recognizing patterns for software development prediction and evaluation\n", "abstract": " Managing a large scale software development requires the use of quantitative models to provide insight and support control based upon historical data from similar projects. Basili introduces a paradigm of measurement based, improvement-oriented software development, called the Improvement Paradigm [1]. This paradigm provides an experimental view of the software activities with a focus on learning and improvement. This implies the need for quantitative approaches for the following uses:                                     to build models of the software process, product, and other forms of experience (e.g., effort, schedule, and reliability) for the purpose of prediction.                                                     to recognize and quantify the influential factors (e.g. personnel capability, storage constraints) on various issues of interest (e.g. productivity and quality) for the purpose of understanding and monitoring the development\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["24"]}
{"title": "Uncertainty-aware specification and analysis for hardware-in-the-loop testing of cyber-physical systems\n", "abstract": " Hardware-in-the-loop (HiL) testing is important for developing cyber-physical systems (CPS). HiL test cases manipulate hardware, are time-consuming and their behaviors are impacted by the uncertainties in the CPS environment. To mitigate the risks associated with HiL testing, engineers have to ensure that (1)\u252c\u00e1test cases are well-behaved, e.g., they do not damage hardware, and (2)\u252c\u00e1test cases can execute within a time budget. Leveraging the UML profile mechanism, we develop a domain-specific language, HITECS, for HiL test case specification. Using HITECS, we provide uncertainty-aware analysis methods to check the well-behavedness of HiL test cases. In addition, we provide a method to estimate the execution times of HiL test cases before the actual HiL testing. We apply HITECS to an industrial case study from the satellite domain. Our results show that: (1)\u252c\u00e1HITECS helps engineers define more effective\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["24"]}
{"title": "Practical constraint solving for generating system test data\n", "abstract": " The ability to generate test data is often a necessary prerequisite for automated software testing. For the generated data to be fit for their intended purpose, the data usually have to satisfy various logical constraints. When testing is performed at a system level, these constraints tend to be complex and are typically captured in expressive formalisms based on first-order logic. Motivated by improving the feasibility and scalability of data generation for system testing, we present a novel approach, whereby we employ a combination of metaheuristic search and Satisfiability Modulo Theories (SMT) for constraint solving. Our approach delegates constraint solving tasks to metaheuristic search and SMT in such a way as to take advantage of the complementary strengths of the two techniques. We ground our work on test data models specified in UML, with OCL used as the constraint language. We present tool support and an\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["24"]}
{"title": "A modeling approach to support the similarity-based reuse of configuration data\n", "abstract": " Product configuration in families of Integrated Control Systems (ICSs) involves resolving thousands of configurable parameters and is, therefore, time-consuming and error-prone. Typically, these systems consist of highly similar components that need to be configured similarly. For large-scale systems, a considerable portion of the configuration data can be reused, based on such similarities, during the configuration of each individual product. In this paper, we propose a model-based approach to automate the reuse of configuration data based on the similarities within an ICS product. Our approach enables configuration engineers to manipulate the reuse of configuration data, and ensures the consistency of the reused data. Evaluation of the approach, using a number of configured products from an industry partner, shows that more than 60% of configuration data can be automatically reused using our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["24"]}
{"title": "An AADL-based SysML profile for architecture level systems engineering: approach, metamodels, and experiments\n", "abstract": " Recent years have seen a proliferation of languages for describing embedded control systems. Some of these languages have emerged from domain-specific frameworks, and some are adaptions or extensions of more general-purpose languages. In this paper, we focus on two widely-used standard languages: the Architecture Analysis and Design Language (AADL) and the Systems Modeling Language (SysML). AADL was born as an avionics-focused domain-specific language and later on has been revised to represent and support a more general category of embedded real-time systems. SysML is an extension of the Unified Modeling Language (UML) intended to support modeling system engineering applications. We propose the ExSAM profile that extends SysML by adding AADL concepts to it with the goal of exploiting the key advantages of both languages in a seamless way. We describe this profile through several examples and compare it with existing alternatives. We have implemented ExSAM using IBM Rational Rhapsody and evaluated its completeness and usefulness through two case studies.", "num_citations": "5\n", "authors": ["24"]}
{"title": "Code generation from UML/MARTE/OCL environment models to support automated system testing of real-time embedded software\n", "abstract": " Given the challenges of testing at the system level, only a fully automated approach can really scale up to industrial real-time embedded systems (RTES). Our goal is to provide a practical approach to the model-based testing of RTES by allowing system testers, who are often not familiar with the system\u0393\u00c7\u00d6s design but are application domain experts, to model the system environment in such a way as to enable its black-box test automation. Environment models can support the automation of three tasks: the code generation of an environment simulator to enable testing on the development platform or without involving actual hardware, the selection of test cases, and the evaluation of their expected results (oracles). From a practical standpoint\u0393\u00c7\u00f2and such considerations are crucial for industrial adoption\u0393\u00c7\u00f2environment modeling should be based on modeling standards (1) that are at an adequate level of abstraction,(2) that software engineers are familiar with, and (3) that are well supported by commercial or open source tools. In this paper, we propose a precise environment modeling methodology fitting these requirements and discuss how these models can be used to generate environment simulators. The environment models are expressed using UML/MARTE and OCL, which are international standards for real-time systems and constraint modeling. The presented techniques are evaluated on a set of three artificial problems and on two industrial RTES.", "num_citations": "5\n", "authors": ["24"]}
{"title": "MODUS: A goal-based approach for quantitative assessment of technical systems\n", "abstract": " Modern technical systems are often complex combinations of mechanical and electronic devices that are controlled and monitored by embedded software. This combination leads to massive improvements in product performance and flexibility, effectively turning conventionally designed mechanical and electronic equipment into smart devices [18]. Heart pacemakers, DVD players, and (new) cars are all examples of modern technical systems. At larger scales, many technical systems are in fact systems of systems, built by integrating independent, self-contained applications that, taken as a whole, satisfy a specified need [4]. Examples of such large-scale systems include airplanes, ships, and oil rigs.Technical systems typically have a long life-span with significant potential impact on their users and the environment. As a result, these systems are usually subject to various requirements concerning Reliability (continuity of correct service), Availability (readiness for correct service), Maintainability (ability to undergo modifications and repairs) and Safety (absence of catastrophic failures leading to injury or environmental damage). These quality factors together with the high cost of manufacturing also means that technical systems are expensive to develop. While the opportunities related to applying these systems are significant, there are risks associated with the impact and performance of the systems. It is therefore crucial to assess and manage risks throughout the system life cycle, starting from the inception phase to the maintenance and all through to the final decommissioning.", "num_citations": "5\n", "authors": ["24"]}
{"title": "The Role of Controlled Experiments Working Group Results\n", "abstract": " The purpose of this working group was to identify which role controlled experiments play in the field of empirical Software Engineering. The discussions resulted in a list of motivational factors, challenges, and improvements suggestions. The main outcome is that, although the empirical Software Engineering community, over the last 14 years, has matured with regard to doing controlled experiments, there is still room for improvement. By now the community has understood under which conditions it is possible to empirically evaluate Software Engineering methods, techniques, and tools, but the way controlled experiments are designed, performed, and reported still lacks the level of quality of other disciplines such as social sciences or medicine. The generalizability of the results of controlled experiments is one major concern. Furthermore, more emphasis should be put on the role of empirical Software\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["24"]}
{"title": "The dimensions of coupling in object-oriented design\n", "abstract": " Many coupling measures exist in the object-oriented literature [BDW96]. However, since current theory regarding coupling on object-oriented (OO) systems is weak, it is difficult to assess what underlying attributes (or factors) the measures capture and whether these attributes are different. In [BDM97], a three-faceted classification yielding 18 different types of coupling has been proposed. However, we do not know whether this classification is (1) consistent with the real (unknown) underlying attributes that constitute OO coupling,(2) complete or (3) at an adequate level of granularity. In this paper, we perform a dimensional analysis based on 23 of the main OO design coupling measures in the literature. This was performed in the context of a controlled study with C++ systems developed by student subjects at the University of Maryland. More information regarding the experimental setting can be found in [BBM96].", "num_citations": "5\n", "authors": ["24"]}
{"title": "On systematically building a controlled natural language for functional requirements\n", "abstract": " Natural language (NL) is pervasive in software requirements specifications (SRSs). However, despite its popularity and widespread use, NL is highly prone to quality issues such as vagueness, ambiguity, and incompleteness. Controlled natural languages (CNLs) have been proposed as a way to prevent quality problems in requirements documents, while maintaining the flexibility to write and communicate requirements in an intuitive and universally understood manner. In collaboration with an industrial partner from the financial domain, we systematically develop and evaluate a CNL, named Rimay, intended at helping analysts write functional requirements. We rely on Grounded Theory for building Rimay and follow well-known guidelines for conducting and reporting industrial case study research. Our main contributions are:(1) a qualitative methodology to systematically define a CNL for functional requirements\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["24"]}
{"title": "Reinforcement learning for test case prioritization\n", "abstract": " Continuous Integration (CI) significantly reduces integration problems, speeds up development time, and shortens release time. However, it also introduces new challenges for quality assurance activities, including regression testing, which is the focus of this work. Though various approaches for test case prioritization have shown to be very promising in the context of regression testing, specific techniques must be designed to deal with the dynamic nature and timing constraints of CI. Recently, Reinforcement Learning (RL) has shown great potential in various challenging scenarios that require continuous adaptation, such as game playing, real-time ads bidding, and recommender systems. Inspired by this line of work and building on initial efforts in supporting test case prioritization with RL techniques, we perform here a comprehensive investigation of RL-based test case prioritization in a CI context. To this end\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["24"]}
{"title": "Automated demarcation of requirements in textual specifications: a machine learning-based approach\n", "abstract": " A simple but important task during the analysis of a textual requirements specification is to determine which statements in the specification represent requirements. In principle, by following suitable writing and markup conventions, one can provide an immediate and unequivocal demarcation of requirements at the time a specification is being developed. However, neither the presence nor a fully accurate enforcement of such conventions is guaranteed. The result is that, in many practical situations, analysts end up resorting to after-the-fact reviews for sifting requirements from other material in a requirements specification. This is both tedious and time-consuming. We propose an automated approach for demarcating requirements in free-form requirements specifications. The approach, which is based on machine learning, can be applied to a wide variety of specifications in different domains and with different\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["24"]}
{"title": "Model Driven Engineering for Data Protection and Privacy: Application and Experience with GDPR\n", "abstract": " In Europe and indeed worldwide, the General Data Protection Regulation (GDPR) provides protection to individuals regarding their personal data in the face of new technological developments. GDPR is widely viewed as the benchmark for data protection and privacy regulations that harmonizes data privacy laws across Europe. Although the GDPR is highly beneficial to individuals, it presents significant challenges for organizations monitoring or storing personal information. Since there is currently no automated solution with broad industrial applicability, organizations have no choice but to carry out expensive manual audits to ensure GDPR compliance. In this paper, we present a complete GDPR UML model as a first step towards designing automated methods for checking GDPR compliance. Given that the practical application of the GDPR is influenced by national laws of the EU Member States, we suggest a two-tiered description of the GDPR, generic and specialized. In this paper, we provide (1) the GDPR conceptual model we developed with complete traceability from its classes to the GDPR, (2) a glossary to help understand the model, (3) the plain-English description of 35 compliance rules derived from GDPR along with their encoding in OCL, and (4) the set of 20 variations points derived from GDPR to specialize the generic model. We further present the challenges we faced in our modeling endeavor, the lessons we learned from it, and future directions for research.", "num_citations": "4\n", "authors": ["24"]}
{"title": "Model-driven run-time enforcement of complex role-based access control policies\n", "abstract": " A Role-based Access Control (RBAC) mechanism prevents unauthorized users to perform an operation, according to authorization policies which are defined on the user's role within an enterprise. Several models have been proposed to specify complex RBAC policies. However, existing approaches for policy enforcement do not fully support all the types of policies that can be expressed in these models, which hinders their adoption among practitioners. In this paper we propose a model-driven enforcement framework for complex policies captured by GemRBAC+CTX, a comprehensive RBAC model proposed in the literature. We reduce the problem of making an access decision to checking whether a system state (from an RBAC point of view), expressed as an instance of the GEMRBAC+CTX model, satisfies the constraints corresponding to the RBAC policies to be enforced at run time. We provide enforcement\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["24"]}
{"title": "TemPsy-Check: a tool for model-driven trace checking of pattern-based temporal properties\n", "abstract": " TemPsy (Temporal Properties made easy) is a pattern-based, domain-specific language for the specification of temporal properties. In this paper we provide an overview of TemPsy-Check, a tool that implements a model-driven approach for performing offline trace checking of temporal properties written in TemPsy. TemPsy-Check relies on an optimized mapping of temporal requirements written in TemPsy into Object Constraint Language (OCL) constraints on a conceptual model of execution traces.", "num_citations": "4\n", "authors": ["24"]}
{"title": "Cocotest: a tool for model-in-the-loop testing of continuous controllers\n", "abstract": " We present CoCoTest, a tool for automated testing of continuous controllers at the Model-in-the-Loop stage. CoCoTest combines explorative and exploitative search algorithms to identify scenarios in the controller input space that violate or are likely to violate the controller requirements. This enables a scalable and systematic way to test continuous properties of such controllers. Our experiments show that CoCoTest identifies critical flaws in the controller design that are rarely found by manual testing and go unnoticed until late stages of embedded software system development.", "num_citations": "4\n", "authors": ["24"]}
{"title": "Guided interactive configuration of embedded software systems using constraint satisfaction over finite domains\n", "abstract": " Modern society is increasingly dependent on highly-configurable software systems, in particular, architecturally configurable software such as that embedded in Integrated Control Systems (ICSs). Configuring ICSs is expensive, time-consuming and error-prone. This is due, in large part, to the fact that the hardware and software configuration processes are, typically, rather isolated from one another, resulting in many configuration errors to be detected only after the integration of software and hardware. In this work, we propose a model-based configuration approach that allows us to configure software in a stepwise manner, to automate some configuration decisions, and to iteratively validate software and hardware configuration decisions. Our approach has two major steps. In the first step (modeling), a generic model describing an ICS family is built. In the second step (configuration), we interactively guide a user to derive a particular product specification complying with the generic model of its ICS family. We use a constraint solver, SICStus Prolog, to evaluate user decisions at each round, to automatically infer configuration decisions and to ensure that software and hardware configurations are consistent. We evaluated our approach by applying it to a real subsea production system. Specifically, we rebuilt a number of existing verified product configurations of our industry partner. Our experience shows that our approach successfully enforces consistency of configurations, can automatically infer up to 50% of the configuration decisions, and reduces the complexity of making configuration decisions by guiding the user during the configuration process.", "num_citations": "4\n", "authors": ["24"]}
{"title": "An introduction to regression testing\n", "abstract": " \u0393\u00c7\u00f3 For example, the modifications from P to P\u0393\u00c7\u00d6change some equivalence classes. t\u0393\u00ea\u00ea T used to exercise a boundary, which is no longer a boundary. t\u0393\u00ea\u00ea T is obsolete as it does no longer test a construct (boundary) of interest.\u0393\u00c7\u00f4A structural test case t\u0393\u00ea\u00ea T may no longer contribute to the structural coverage of the program.", "num_citations": "4\n", "authors": ["24"]}
{"title": "An Approach to Detecting Design Patterns in MOF-Based Domain-Specific Models with QVT\n", "abstract": " A design pattern is a recurring and well-understood design fragment. In the context of a domain-specific modeling language (DSML), a design pattern is represented as a structure of constrained and inter-related model elements. Techniques that analyze models by detecting occurrences of known design patterns can simplify model comprehension and maintenance. Though each DSML may have its own unique set of design patterns, it is not practical to learn a separate detection technology for each specific DSML or family of design patterns. This paper describes a generic approach to specify domainspecific design patterns for MOF-based DSMLs at the metamodel level and automatically detect their occurrences in models. The approach is based on QVT (Query/View/Transformation). Patterns are specified declaratively with QVT-Relations (QVTr) transformations from DSML models, where elements playing pattern roles are identified, to a newly-defined DSML model for reporting identified occurrences. Pattern detection is implemented by executing these transformations. The approach has been prototyped using Eclipse technologies and used in a case study to detect the well-known GoF patterns in a design model of a large open-source system. Results were then analyzed for accuracy using precision and recall as metrics, confirming the adequacy of the approach to detect pattern occurrences with high accuracy.", "num_citations": "4\n", "authors": ["24"]}
{"title": "Improving state-based coverage criteria using data flow information\n", "abstract": " Empirical evaluations have revealed limitations of exis\u2229\u00bc\u00fcng stato\u0393\u00c7\u00f6based coveragc criteria. As these criteria can be considered as executing the control \u2229\u00bc\u00e9ow structure of the statechart, we are attempting to investigate how data \u2229\u00bc\u00e9ow information can be used to improve these criteria. This thesis presents a comprehensive methodology to perform data \u2229\u00bc\u00e9ow analysis of UML statecharts, applies it to the transition tree criterion and uses it on two case studies. The results of the case studies show that data \u2229\u00bc\u00e9ow information can be used to select the best transition tree when more than one satis\u2229\u00bc\u00fces the transition tree criterion. We further propose a more optimal strategy for the transition tree criterion, in terms of cost and effectiveness. The improved tree strategy is evaluated through the two case studies and the results suggest that it is a cost\u0393\u00c7\u00f6effec\u2229\u00bc\u00fcve strategy that would \u2229\u00bc\u00fct into many practical situations. iii", "num_citations": "4\n", "authors": ["24"]}
{"title": "Trace-checking CPS properties: Bridging the cyber-physical gap\n", "abstract": " Cyber-physical systems combine software and physical components. Specification-driven trace-checking tools for CPS usually provide users with a specification language to express the requirements of interest, and an automatic procedure to check whether these requirements hold on the execution traces of a CPS. Although there exist several specification languages for CPS, they are often not sufficiently expressive to allow the specification of complex CPS properties related to the software and the physical components and their interactions.In this paper, we propose (i) the Hybrid Logic of Signals (HLS), a logic-based language that allows the specification of complex CPS requirements, and (ii) ThEodorE, an efficient SMT-based trace-checking procedure. This procedure reduces the problem of checking a CPS requirement over an execution trace, to checking the satisfiability of an SMT formula.We evaluated our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["24"]}
{"title": "An automated framework for the extraction of semantic legal metadata from legal texts\n", "abstract": " Semantic legal metadata provides information that helps with understanding and interpreting legal provisions. Such metadata is therefore important for the systematic analysis of legal requirements. However, manually enhancing a large legal corpus with semantic metadata is prohibitively expensive. Our work is motivated by two observations:(1) the existing requirements engineering (RE) literature does not provide a harmonized view on the semantic metadata types that are useful for legal requirements analysis;(2) automated support for the extraction of semantic legal metadata is scarce, and it does not exploit the full potential of artificial intelligence technologies, notably natural language processing (NLP) and machine learning (ML). Our objective is to take steps toward overcoming these limitations. To do so, we review and reconcile the semantic legal metadata types proposed in the RE literature. Subsequently\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["24"]}
{"title": "Can we predict the quality of spectrum-based fault localization?\n", "abstract": " Fault localization and repair are time-consuming and tedious. There is a significant and growing need for automated techniques to support such tasks. Despite significant progress in this area, existing fault localization techniques are not widely applied in practice yet and their effectiveness varies greatly from case to case. Existing work suggests new algorithms and ideas as well as adjustments to the test suites to improve the effectiveness of automated fault localization. However, important questions remain open: Why is the effectiveness of these techniques so unpredictable? What are the factors that influence the effectiveness of fault localization? Can we accurately predict fault localization effectiveness? In this paper, we try to answer these questions by collecting 70 static, dynamic, test suite, and fault-related metrics that we hypothesize are related to effectiveness. Our analysis shows that a combination of only a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["24"]}
{"title": "Leveraging natural-language requirements for deriving better acceptance criteria from models\n", "abstract": " In many software and systems development projects, analysts specify requirements using a combination of modeling and natural language (NL). In such situations, systematic acceptance testing poses a challenge because defining the acceptance criteria (AC) to be met by the system under test has to account not only for the information in the (requirements) model but also that in the NL requirements. In other words, neither models nor NL requirements per se provide a complete picture of the information content relevant to AC. Our work in this paper is prompted by the observation that a reconciliation of the information content in NL requirements and models is necessary for obtaining precise AC. We perform such reconciliation by devising an approach that automatically extracts AC-related information from NL requirements and helps modelers enrich their model with the extracted information. An existing AC\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["24"]}
{"title": "Automating system test case classification and prioritization for use case-driven testing in product lines\n", "abstract": " Product Line Engineering (PLE) is a crucial practice in many software development environments where software systems are complex and developed for multiple customers with varying needs. At the same time, many development processes are use case-driven and this strongly influences their requirements engineering and system testing practices. In this paper, we propose, apply, and assess an automated system test case classification and prioritization approach specifically targeting system testing in the context of use case-driven development of product families. Our approach provides: (i) automated support to classify, for a new product in a product family, relevant and valid system test cases associated with previous products, and (ii) automated prioritization of system test cases using multiple risk factors such as fault-proneness of requirements and requirements volatility in a product family. Our evaluation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["24"]}
{"title": "Using machine learning to assist with the selection of security controls during security assessment\n", "abstract": " Context In many domains such as healthcare and banking, IT systems need to fulfill various requirements related to security. The elaboration of security requirements for a given system is in part guided by the controls envisaged by the applicable security standards and best practices. An important difficulty that analysts have to contend with during security requirements elaboration is sifting through a large number of security controls and determining which ones have a bearing on the security requirements for a given system. This challenge is often exacerbated by the scarce security expertise available in most organizations.   Objective In this article, we develop automated decision support for the identification of security controls that are relevant to a specific system in a particular context.   Method and Results Our approach, which is based on machine learning, leverages historical data from security assessments\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["24"]}
{"title": "Bridging the gap between requirements modeling and behavior-driven development\n", "abstract": " Acceptance criteria (AC) are implementation agnostic conditions that a system must meet to be consistent with its requirements and be accepted by its stakeholders. Each acceptance criterion is typically expressed as a natural-language statement with a clear pass or fail outcome. Writing AC is a tedious and error-prone activity, especially when the requirements specifications evolve and there are different analysts and testing teams involved. Analysts and testers must iterate multiple times to ensure that AC are understandable and feasible, and accurately address the most important requirements and workflows of the system being developed. In many cases, analysts express requirements through models, along with natural language, typically in some variant of the UML. AC must then be derived by developers and testers from such models. In this paper, we bridge the gap between requirements models and AC by\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["24"]}
{"title": "HITECS: A UML Profile and Analysis Framework for Hardware-in-the-Loop Testing of Cyber Physical Systems\n", "abstract": " Hardware-in-the-loop (HiL) testing is an important step in the development of cyber physical systems (CPS). CPS HiL test cases manipulate hardware components, are time-consuming and their behaviors are impacted by the uncertainties in the CPS environment. To mitigate the risks associated with HiL testing, engineers have to ensure that (1) HiL test cases are well-behaved, ie, they implement valid test scenarios and do not accidentally damage hardware, and (2) HiL test cases can execute within the time budget allotted to HiL testing. This paper proposes an approach to help engineers systematically specify and analyze CPS HiL test cases. Leveraging the UML profile mechanism, we develop an executable domain-specific language, HITECS, for HiL test case specification. HITECS builds on the UML Testing Profile (UTP) and the UML action language (Alf). Using HITECS, we provide analysis methods to check\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["24"]}
{"title": "Evolutionary robustness testing of data processing systems using models and data mutation (T)\n", "abstract": " System level testing of industrial data processing software poses several challenges. Input data can be very large, even in the order of gigabytes, and with complex constraints that define when an input is valid. Generating the right input data to stress the system for robustness properties (e.g. to test how faulty data is handled) is hence very complex, tedious and error prone when done manually. Unfortunately, this is the current practice in industry. In previous work, we defined a methodology to model the structure and the constraints of input data by using UML class diagrams and OCL constraints. Tests were automatically derived to cover predefined fault types in a fault model. In this paper, to obtain more effective system level test cases, we developed a novel search-based test generation tool. Experiments on a real-world, large industrial data processing system show that our automated approach can not only\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["24"]}
{"title": "A model-driven approach to offline trace checking of temporal properties with ocl\n", "abstract": " Offline trace checking is a procedure for evaluating requirements over a log of events produced by a system. The goal of this paper is to present a practical and scalable solution for the offline checking of the temporal requirements of a system, which can be used in contexts where model-driven engineering is already a practice, where temporal specifications should be written in a domain-specific language not requiring a strong mathematical background, and where relying on standards and industry-strength tools for property checking is a fundamental prerequisite. The main contributions are: the TemPsy language, a domainspecific specification language based on common property specification patterns, and extended with new constructs; a model-driven offline trace checking procedure based on the mapping of requirements written in TemPsy into OCL (Object Constraint Language) constraints on a conceptual model on execution traces, which can be evaluated using an OCL checker; the implementation of this trace checking procedure in the TemPsy-Check tool; the evaluation of the scalability of TemPsy-Check and its comparison to a state-of-the-art alternative technology. The proposed approach has been applied to a case study developed in collaboration with a public service organization, active in the domain of business process modeling for eGovernment.", "num_citations": "3\n", "authors": ["24"]}
{"title": "Black-box sql injection testing\n", "abstract": " [en] Web services are increasingly adopted in various domains, from finance and e-government to social media. As they are built on top of the web technologies, they suffer also an unprecedented amount of attacks and exploitations like the Web. Among the attacks, those that target SQL injection vulnerabilities have consistently been top-ranked for the last years. Testing to detect such vulnerabilities before making web services public is crucial. We present in this report an automated testing approach, namely \u256c\u255d4SQLi, and its underpinning set of mutation operators. \u256c\u255d4SQLi can produce effective inputs that lead to executable and harmful SQL statements. Executability is key as otherwise no injection vulnerability can be exploited. Our evaluation demonstrated that the approach outperforms contemporary known attacks in terms of vulnerability detection and the ability to get through an application firewall, which is a popular configuration in real world.", "num_citations": "3\n", "authors": ["24"]}
{"title": "Empirical evaluation in software engineering: role, strategy, and limitations\n", "abstract": " Though there is a wide agreement that software technologies should be empirically investigated and assessed, software engineering faces a number of specific challenges and we have reached a point where it is time to step back and reflect on them. Technologies evolve fast, there is a wide variety of conditions (including human factors) under which they can possibly be used, and their assessment can be made with respect to a large number of criteria. Furthermore, only limited resources can be dedicated to the evaluation of software technologies as compared to their development. If we take an example, the development and evaluation of the Unified Modeling Language (UML) as an analysis and design representation, major revisions of the standard are proposed every few years, many specialized \u0393\u00c7\u00a3profiles\u0393\u00c7\u00a5 of UML are being developed (e.g., for performance and real-time) and evolved, it can be used\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["24"]}
{"title": "Model Driven Engineering Languages and Systems: 8th International Conference, MoDELS 2005, Montego Bay, Jamaica, October 2-7, 2005, Proceedings\n", "abstract": " The MoDELS (Model Driven Engineering, Languages, and Systems) conference is a continuation of the successful series of UML conferences. This volume contains the? nal versions of the technical papers presented at MoDELS 2005 in Montego Bay, Jamaica, October 2\u0393\u00c7\u00f47, 2005. The UML series began in 1988 at Mulhouse, France when the Uni? ed Modeling Language was relatively new. Since then the conferences have been annually, with an increase in both attendance and the breadth of the work presentedattheconferences. Theappearanceofnewresearchareasandtopicsin prior conferences was the motivation for renaming the conference to re? ect the broadermissiontheconferenceswereenabling. Amongthenewareastakingtheir place alongside UML and related standards, such as Model Driven Architecture (MDA), are model refactoring, aspect oriented modeling, and model quality control. The call for papers for MoDELS 2005 resulted in the submission of 215 abstracts and 166 papers. Each submission was reviewed by at least 3 referees assignedfromthesetof51ProgramCommitteemembers. Afterseveralroundsof discussion within the ProgramCommittee, 46 papers (40 scienti? c papers and 6 experience papers) were selected for publication. The Program Committee also selected a paper for the Best Paper MoDELS 2005 Award. The paper is by Friedrich Steimann and is titled \u0393\u00c7\u00a3Domain Models are Aspect Free.\u0393\u00c7\u00a5 The review process was managed using the VirtualChair reviewing system, developed by Vahid Garousi at Carleton University, Ottawa, Canada.", "num_citations": "3\n", "authors": ["24"]}
{"title": "Reply to''Comments to the Paper: Briand, El Emam, Morasca: On the Application of Measurement Theory in Software Engineering''\n", "abstract": " In his comments, Zuse discusses a few aspects of (Briand, El Emam and Morasca, 1996) that he reckons incorrect. Here, we demonstrate that there is nothing wrong with those statements. We believe that it will be beneficial to the field if these controversies are sorted out clearly and rapidly. This will allow for faster, more consolidated progress in the field. Therefore, in this reply, we concentrate on each of Zuse\u0393\u00c7\u00d6s objections.", "num_citations": "3\n", "authors": ["24"]}
{"title": "Defining and validating design coupling measures in object-oriented systems\n", "abstract": " This paper proposes a comprehensive suite of measures to quantify the level of class coupling during the design of object-oriented systems. This suite takes into account the different OO design mechanisms provided by the C++ language (eg, friendship between classes, specialization, and aggregation) but it can be tailored to other OO languages. The different measures in our suite thus reflect different hypotheses about the different mechanisms of coupling in OO systems.Based on actual project defect data, the hypotheses underlying our coupling measures are empirically validated by analyzing their relationship with the probability of fault detection across classes. The results demonstrate that some of these coupling measures may be useful early quality indicators of the design of OO systems. These measures are conceptually different from the OO design measures defined by Chidamber and Kemerer; in addition, our data suggests that they are complementary quality indicators.", "num_citations": "3\n", "authors": ["24"]}
{"title": "Quantitative empirical modeling for managing software development: constraints, needs and solutions\n", "abstract": " In order to plan, conlrol and evaluate the software development process, we need quantitative models. Because of our current lack of understanding/experience and the high complexity of the modeled processes, it appears very difficult to build theoretical models (eg SLIM for development resource management). Therefore, based on collected historical data, we attempt to build multivariate empirical models valid in a specific environment. Because the available information is always incomplete, these models are by nalure stochastic. We need models to be able to predict but also to understand. Being able to interpret the generated models is therefore indispensable in order to take preventive/corrective actions and elaborate better standards for the development of software systems within an organization. Basili has inlroduced a paradigm of measurement based, improvement-oriented software development, called\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["24"]}
{"title": "Using domain-specific corpora for improved handling of ambiguity in requirements\n", "abstract": " Ambiguity in natural-language requirements is a pervasive issue that has been studied by the requirements engineering community for more than two decades. A fully manual approach for addressing ambiguity in requirements is tedious and time-consuming, and may further overlook unacknowledged ambiguity \u0393\u00c7\u00f4 the situation where different stakeholders perceive a requirement as unambiguous but, in reality, interpret the requirement differently. In this paper, we propose an automated approach that uses natural language processing for handling ambiguity in requirements. Our approach is based on the automatic generation of a domain-specific corpus from Wikipedia. Integrating domain knowledge, as we show in our evaluation, leads to a significant positive improvement in the accuracy of ambiguity detection and interpretation. We scope our work to coordination ambiguity (CA) and prepositional-phrase\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["24"]}
{"title": "Signal-Based Properties of Cyber-Physical Systems: Taxonomy and Logic-based Characterization\n", "abstract": " The behavior of a cyber-physical system (CPS) is usually defined in terms of the input and output signals processed by sensors and actuators. Requirements specifications of CPSs are typically expressed using signal-based temporal properties. Expressing such requirements is challenging, because of (1) the many features that can be used to characterize a signal behavior; (2) the broad variation in expressiveness of the specification languages (i.e., temporal logics) used for defining signal-based temporal properties. Thus, system and software engineers need effective guidance on selecting appropriate signal behavior types and an adequate specification language, based on the type of requirements they have to define.In this paper, we present a taxonomy of the various types of signal-based properties and provide, for each type, a comprehensive and detailed description as well as a formalization in a temporal\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["24"]}
{"title": "Schedulability Analysis of Real-Time Systems with Uncertain Worst-Case Execution Times\n", "abstract": " Schedulability analysis is about determining whether a given set of real-time software tasks are schedulable, i.e., whether task executions always complete before their specified deadlines. It is an important activity at both early design and late development stages of real-time systems. Schedulability analysis requires as input the estimated worst-case execution times (WCET) for software tasks. However, in practice, engineers often cannot provide precise point WCET estimates and prefer to provide plausible WCET ranges. Given a set of real-time tasks with such ranges, we provide an automated technique to determine for what WCET values the system is likely to meet its deadlines, and hence operate safely. Our approach combines a search algorithm for generating worst-case scheduling scenarios with polynomial logistic regression for inferring safe WCET ranges. We evaluated our approach by applying it to a satellite on-board system. Our approach efficiently and accurately estimates safe WCET ranges within which deadlines are likely to be satisfied with high confidence.", "num_citations": "2\n", "authors": ["24"]}
{"title": "SMRL: a metamorphic security testing tool for web systems\n", "abstract": " We present a metamorphic testing tool that alleviates the oracle problem in security testing. The tool enables engineers to specify metamorphic relations that capture security properties of Web systems. It automatically tests Web systems to detect vulnerabilities based on those relations. We provide a domain-specific language accompanied by an Eclipse editor to facilitate the specification of metamorphic relations. The tool automatically collects the input data and transforms the metamorphic relations into executable Java code in order to automatically perform security testing based on the collected data. The tool has been successfully evaluated on a commercial system and a leading open source system (Jenkins). Demo video: https://youtu. be/9kx6u9LsGxs.", "num_citations": "2\n", "authors": ["24"]}
{"title": "Dynamic adaptive network configuration for IoT systems: a search-based approach\n", "abstract": " The concept of Internet of Things (IoT) has led to the development of many complex and critical systems such as smart emergency management systems. IoT-enabled applications typically depend on a communication network for transmitting large volumes of data in unpredictable and changing environments. These networks are prone to congestion when there is a burst in demand, eg, as an emergency situation is unfolding. In this paper, we propose a dynamic adaptive network configuration approach for IoT systems. The approach enables resolving congestion in real time while minimizing network utilization, data transmission delays and adaptation costs. Our approach relies on the research field of dynamic adaptive search-based software engineering (SBSE) to reconfigure an IoT network while simultaneously ensuring multiple quality of service criteria. We evaluate our approach on an industrial national emergency management system, which is aimed at detecting disasters and emergencies, and facilitating recovery and rescue operations by providing first responders with a reliable communication infrastructure. Our results indicate that (1) our approach is able to efficiently and effectively adapt an IoT network to dynamically resolve congestion, and (2) compared to two baseline data forwarding algorithms that are static and non-adaptive, our approach increases the data transmission rate by a factor of at least 3 and decreases data loss by at least 70", "num_citations": "2\n", "authors": ["24"]}
{"title": "A Model-driven Approach to Trace Checking of Temporal Properties with Aggregations\n", "abstract": " [en] The verification of complex software systems often requires to check quantitative properties that rely on aggregation operators (eg, the average response time of a service). One way to ease the specification of these properties is to use property specification patterns, such as the ones for \u0393\u00c7\u00a3service provisioning\u0393\u00c7\u00a5, previously proposed in the literature.In this paper we focus on the problem of performing offline trace checking of temporal properties containing aggregation operators. We first present TemPsy-AG, an extension of TemPsy\u0393\u00c7\u00f6an existing pattern-based language for the specification of temporal properties\u0393\u00c7\u00f6to support service provisioning patterns that use aggregation operators. We then extend an existing model-driven procedure for trace checking, to verify properties expressed in TemPsy-AG. The trace checking procedure relies on the efficient mapping of temporal properties written in TemPsy-AG into OCL constraints on a meta-model of execution traces. We have implemented this procedure in the TemPsy-Check-AG tool and evaluated its performance: our approach scales linearly with respect to the length of the input trace and can deal with much larger traces than a state-of-the-art tool.", "num_citations": "2\n", "authors": ["24"]}
{"title": "From RELAW Research to Practice: Reflections on an Ongoing Technology Transfer Project\n", "abstract": " Over the past years, we have been studying the topic of automated metadata extraction from legal texts. While our research has been motivated primarily by RE problems, we have observed that the interdisciplinarity of the research on legal metadata, and indeed on several other topics considered by the RELAW community, has the potential to trigger innovation beyond the traditional RE. In particular, legal metadata is a key enabler for the rapidly-expanding field of Legal Technology (LegalTech). In this short paper, we describe the preliminary steps we have taken toward transitioning a prototype tool for legal metadata extraction (developed in our previous work) into a platform that is palatable to the LegalTech market. We hope that our findings would provide useful insights about the value chain for legal metadata and further offer a concrete example of a technology transfer attempt that is rooted in RELAW research.", "num_citations": "2\n", "authors": ["24"]}
{"title": "An empirical evaluation of visualisation in software design modelling: the VCL vs UML+ OCL experiment\n", "abstract": " This document describes the design of a controlled experiment to evaluate the effectiveness of the Visual Contract Language (VCL). This is done by comparing VCL against UML supplemented with OCL, which are considered to be industry standards. The following sections briefly explain the motivations behind the visual nature of VCL, give a short explanation of VCL and its development and present the actual experiment.", "num_citations": "2\n", "authors": ["24"]}
{"title": "Measurement and Modeling in Software Engineering\n", "abstract": " \u0393\u00c7\u00f4Purpose: classification, continuous prediction, decision, optimization\u0393\u00c7\u00aa\u0393\u00c7\u00f4Variables: type, number, interactions, relationships\u0393\u00c7\u00f4Data: amount, missing data,\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["24"]}
{"title": "Defining and validating measures for object-based high-level design\n", "abstract": " The availability of significant measures in the early phases of the software development life-cycle allows for better management of the later phases, and more effective quality assessment when quality can be more easily affected by preventive or corrective actions. In this paper, we introduce and compare various high-level design measures for object-based software systems. The measures are derived based on an experimental goal, identifying fault-prone software parts, and several experimental hypotheses arising from the development of Ada systems for Flight Dynamics Software at the NASA Goddard Space Flight Center (NASA/GSFC). Specifically, we define a set of measures for cohesion and coupling, and theoretically analyze them by checking their compliance with a previously published set of mathematical properties that we deem important. We then investigate their relationship to fault-proneness on three\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["24"]}
{"title": "A hybrid method for software cost estimation and risk assessment\n", "abstract": " Current cost estimation techniques have a number of drawbacks. For example, developing algorithmic models requires extensive past project data. Also, off-the-shelf models have been found to be inaccurate without calibration. Approaches based on experienced estimators depend on estimators' availability and are not easily repeatable, as well as not being much more accurate than algorithmic techniques. In this paper we present a method for cost estimation that combines aspects of algorithmic and experential approaches. We find through a case study that cost estimates using this approach have average ARE of 0.09, and also the results can be used for risk assessment.", "num_citations": "2\n", "authors": ["24"]}
{"title": "Process Modelling and Empirical Studies of Software Evolution (PMESSE'97) Workshop Report\n", "abstract": " Much progress is being made in both the areas of process modelling and software metrics. However, neither of these concepts is complete without the other: processes cannot be improved if no assessment of quality is available, and metrics are useless if they cannot be applied in order to assess the evolution of systems. The PMESSE (Process Modelling and Empirical Studies of Software Evolution) Workshop, held in Boston MA, on May 18, 1997, brought together researchers and practitioners from both of these fields, and stimulated some very lively debate on these issues. This collection of reports reflects the work done by the Workshops five Working Groups.               Dialogue between the software metrics and process modelling communities is essential. This workshop succeeded in bringing together researchers with a wide range of research interests, and the resulting discussions were very animated\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["24"]}
{"title": "Response to \u0393\u00c7\u00a3handling regression subsets in software modeling\u0393\u00c7\u00a5 by Ronald Gulezian, the journal of systems and software, April 1996\n", "abstract": " Response to \u0393\u00c7\u00a3handling regression subsets in software modeling\u0393\u00c7\u00a5 by Ronald Gulezian, the Journal of Systems and Software, April 1996 | Journal of Systems and Software ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Journal of Systems and Software Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsJournal of Systems and SoftwareVol. , No. Response to \u0393\u00c7\u00a3handling regression subsets in software modeling\u0393\u00c7\u00a5 by Ronald Gulezian, the Journal of Systems and Software, April 1996 article Response to \u0393\u00c7\u00a3handling regression subsets in software modeling\u0393\u00c7\u00a5 by Ronald Gulezian, the Journal of Systems and Software, April 1996 Share on Authors: Lionel C \u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["24"]}
{"title": "Test Case Selection and Prioritization Using Machine Learning: A Systematic Literature Review\n", "abstract": " Regression testing is an essential activity to assure that software code changes do not adversely affect existing functionalities. With the wide adoption of Continuous Integration (CI) in software projects, which increases the frequency of running software builds, running all tests can be time-consuming and resource-intensive. To alleviate that problem, Test case Selection and Prioritization (TSP) techniques have been proposed to improve regression testing by selecting and prioritizing test cases in order to provide early feedback to developers. In recent years, researchers have relied on Machine Learning (ML) techniques to achieve effective TSP (ML-based TSP). Such techniques help combine information about test cases, from partial and imperfect sources, into accurate prediction models. This work conducts a systematic literature review focused on ML-based TSP techniques, aiming to perform an in-depth analysis of the state of the art, thus gaining insights regarding future avenues of research. To that end, we analyze 29 primary studies published from 2006 to 2020, which have been identified through a systematic and documented process. This paper addresses five research questions addressing variations in ML-based TSP techniques and feature sets for training and testing ML models, alternative metrics used for evaluating the techniques, the performance of techniques, and the reproducibility of the published studies.", "num_citations": "1\n", "authors": ["24"]}
{"title": "MAANA: An Automated Tool for DoMAin-specific HANdling of Ambiguity\n", "abstract": " MAANA (in Arabic: \"meaning\") is a tool for performing domain-specific handling of ambiguity in requirements. Given a requirements document as input, MAANA detects the requirements that are potentially ambiguous. The focus of MAANA is on coordination ambiguity and prepositional-phrase attachment ambiguity; these are two common ambiguity types that have been studied in the requirements engineering literature. To detect ambiguity, MAANA utilizes structural patterns and a set of heuristics derived from a domain-specific corpus. The generated analysis file after running the tool can be reviewed by requirements analysts. Through combining different knowledge sources, MAANA highlights also the requirements that might contain unacknowledged ambiguity. That is when the analysts understand different interpretations for the same requirement, without explicitly discussing it with the other analysts due to time\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["24"]}
{"title": "Combining Genetic Programming and Model Checking to Generate Environment Assumptions\n", "abstract": " Software verification may yield spurious failures when environment assumptions are not accounted for. Environment assumptions are the expectations that a system or a component makes about its operational environment and are often specified in terms of conditions over the inputs of that system or component. In this article, we propose an approach to automatically infer environment assumptions for Cyber-Physical Systems (CPS). Our approach improves the state-of-the-art in three different ways: First, we learn assumptions for complex CPS models involving signal and numeric variables; second, the learned assumptions include arithmetic expressions defined over multiple variables; third, we identify the trade-off between soundness and informativeness of environment assumptions and demonstrate the flexibility of our approach in prioritizing either of these criteria. We evaluate our approach using a public domain benchmark of CPS models from Lockheed Martin and a component of a satellite control system from LuxSpace, a satellite system provider. The results show that our approach outperforms state-of-the-art techniques on learning assumptions for CPS models, and further, when applied to our industrial CPS model, our approach is able to learn assumptions that are sufficiently close to the assumptions manually developed by engineers to be of practical value.", "num_citations": "1\n", "authors": ["24"]}
{"title": "Automatic Test Suite Generation for Key-points Detection DNNs Using Many-Objective Search\n", "abstract": " Automatically detecting the positions of key-points (e.g., facial key-points or finger key-points) in an image is an essential problem in many applications, such as driver's gaze detection and drowsiness detection in automated driving systems. With the recent advances of Deep Neural Networks (DNNs), Key-Points detection DNNs (KP-DNNs) have been increasingly employed for that purpose. Nevertheless, KP-DNN testing and validation have remained a challenging problem because KP-DNNs predict many independent key-points at the same time -- where each individual key-point may be critical in the targeted application -- and images can vary a great deal according to many factors. In this paper, we present an approach to automatically generate test data for KP-DNNs using many-objective search. In our experiments, focused on facial key-points detection DNNs developed for an industrial automotive application, we show that our approach can generate test suites to severely mispredict, on average, more than 93% of all key-points. In comparison, random search-based test data generation can only severely mispredict 41% of them. Many of these mispredictions, however, are not avoidable and should not therefore be considered failures. We also empirically compare state-of-the-art, many-objective search algorithms and their variants, tailored for test suite generation. Furthermore, we investigate and demonstrate how to learn specific conditions, based on image characteristics (e.g., head posture and skin color), that lead to severe mispredictions. Such conditions serve as a basis for risk analysis or DNN retraining.", "num_citations": "1\n", "authors": ["24"]}
{"title": "Supporting dnn safety analysis and retraining through heatmap-based unsupervised learning\n", "abstract": " Deep neural networks (DNNs) are increasingly important in safety-critical systems, for example in their perception layer to analyze images. Unfortunately, there is a lack of methods to ensure the functional safety of DNN-based components. We observe three major challenges with existing practices regarding DNNs in safety-critical systems: (1) scenarios that are underrepresented in the test set may lead to serious safety violation risks, but may, however, remain unnoticed; (2) characterizing such high-risk scenarios is critical for safety analysis; (3) retraining DNNs to address these risks is poorly supported when causes of violations are difficult to determine. To address these problems in the context of DNNs analyzing images, we propose HUDD, an approach that automatically supports the identification of root causes for DNN errors. HUDD identifies root causes by applying a clustering algorithm to heatmaps capturing the relevance of every DNN neuron on the DNN outcome. Also, HUDD retrains DNNs with images that are automatically selected based on their relatedness to the identified image clusters. We evaluated HUDD with DNNs from the automotive domain. HUDD was able to identify all the distinct root causes of DNN errors, thus supporting safety analysis. Also, our retraining approach has shown to be more effective at improving DNN accuracy than existing approaches.", "num_citations": "1\n", "authors": ["24"]}
{"title": "Scalable inference of system-level models from component logs\n", "abstract": " Behavioral software models play a key role in many software engineering tasks; unfortunately, these models either are not available during software development or, if available, they quickly become outdated as the implementations evolve. Model inference techniques have been proposed as a viable solution to extract finite-state models from execution logs. However, existing techniques do not scale well when processing very large logs, such as system-level logs obtained by combining component-level logs. Furthermore, in the case of component-based systems, existing techniques assume to know the definitions of communication channels between components. However, this information is usually not available in the case of systems integrating 3rd-party components with limited documentation. In this paper, we address the scalability problem of inferring the model of a component-based system from the individual component-level logs, when the only available information about the system are high-level architecture dependencies among components and a (possibly incomplete) list of log message templates denoting communication events between components. Our model inference technique, called SCALER, follows a divide and conquer approach. The idea is to first infer a model of each system component from the corresponding logs; then, the individual component models are merged together taking into account the dependencies among components, as reflected in the logs. We evaluated SCALER in terms of scalability and accuracy, using a dataset of logs from an industrial system; the results show that SCALER can process much\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["24"]}
{"title": "A SysML-based methodology for model testing of cyber-physical systems\n", "abstract": " A Cyber-Physical System (CPS) is an integration of computation with physical processes [2]. CPSs are characterized by the presence of a, potentially large number of embedded computing and communication devices, which monitor and control the physical world through sensors and actuators. These devices interact and exchange data with each other through networks, that might be potentially extended over a large geographical area. CPSs are already present in sectors such as transportation, energy, medicine, space or manufacturing, and they are expected to become ubiquitous in a near future. CPSs have the potential to revolutionize how we construct and operate engineered systems, thus transforming the way in which we interact with the physical world. Applications of CPSs are expected to have an enormous impact on society and the economy. Considering the expected impact and pervasiveness of CPSs, ensuring their dependability is a matter of the utmost importance. Validation and Verification (V&V) is essential to ensure dependability of software systems. Among all V&V techniques, the most prevalent one is testing. Unfortunately, aspects that characterize the behavior of a CPS, such as the continuous and complex interactions with the physical world, or the deep intertwining of hardware and software, turn the testing of these systems into a highly expensive, time-consuming process, at best, or make testing infeasible, at worst. These challenges in the application of existing testing techniques, render CPSs untestable in practice. To enable testing of untestable systems, such as CPSs, model testing [1] was recently proposed as a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["24"]}
{"title": "Testing Autonomous Cars for Feature Interaction Failures using Many-Objective Search\n", "abstract": " [en] Complex systems such as autonomous cars are typically built as a composition of features that are independent units of functionality. Features tend to interact and impact one another\u0393\u00c7\u00d6s behavior in unknown ways. A challenge is to detect and manage feature interactions, in particular, those that violate system requirements, hence leading to failures. In this paper, we propose a technique to detect feature interaction failures by casting our approach into a search-based test generation problem. We define a set of hybrid test objectives (distance functions) that combine traditional coverage-based heuristics with new heuristics specifically aimed at revealing feature interaction failures. We develop a new search-based test generation algorithm, called FITEST, that is guided by our hybrid test objectives. FITEST extends recently proposed many-objective evolutionary algorithms to reduce the time required to compute fitness values. We evaluate our approach using two versions of an industrial self-driving system. Our results show that our hybrid test objectives are able to identify more than twice as many feature interaction failures as two baseline test objectives used in the software testing literature (ie, coverage-based and failure-based test objectives). Further, the feedback from domain experts indicates that the detected feature interaction failures represent real faults in their systems that were not previously identified based on analysis of the system features and their requirements.", "num_citations": "1\n", "authors": ["24"]}
{"title": "Mining SQL injection and cross site scripting vulnerabilities using hybrid program analysis.(2013)\n", "abstract": " In previous work, we proposed a set of static attributes that characterize input validation and input sanitization code patterns. We showed that some of the proposed static attributes are significant predictors of web application vulnerabilities related to SQL injection and cross site scripting. Static attributes have the advantage of reflecting general properties of a program. Yet, dynamic attributes collected from execution traces may reflect more specific code characteristics that are complementary to static attributes. Hence, to improve our initial work, in this paper, we propose the use of dynamic attributes to complement static attributes in the prediction of vulnerabilities. Furthermore, since existing work relies on supervised learning, it is dependent on the availability of training data labeled with known vulnerabilities. This paper presents prediction models that are based on both classification and clustering in order to predict vulnerabilities, working in the presence or absence of labeled training data, respectively. In our experiments across six applications, our new supervised vulnerability predictors based on hybrid (static and dynamic) attributes achieved, on average, 90% recall and 85% precision, that is a sharp increase in recall when compared to static analysis-based predictions. Though not nearly as accurate, our unsupervised predictors based on clustering achieved, on average, 76% recall and 39% precision, thus suggesting they can be useful in the absence of labeled training data.", "num_citations": "1\n", "authors": ["24"]}
{"title": "\u252c\u2561TIL: Mutation-based Statistical Test Inputs Generation for Automatic Fault Localization\n", "abstract": " Automatic Fault Localization (AFL) is a process to locate faults automatically in software programs. Essentially, an AFL method takes as input a set of test cases including failed test cases, and ranks the statements of a program from the most likely to the least likely to contain a fault. As a result, the efficiency of an AFL method depends on the \"quality\" of the test cases used to rank statements. More specifically, in order to improve the accuracy of their ranking within test budget constraints, we have to ensure that program statements are executed by a reasonably large number of test cases which provide a coverage as uniform as possible of the input domain. This paper proposes \u256c\u255dTIL, a new statistical test inputs generation method dedicated to AFL, based on constraint solving and mutation testing. Using mutants where the locations of injected faults are known, \u256c\u255dTIL is able to significantly reduce the length of an AFL test\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["24"]}
{"title": "A model-based approach to the automated reuse of configuration data based on internal similarities\n", "abstract": " Product configuration in families of embedded software systems, such as Integrated Control Systems (ICSs), involves resolving thousands of configurable parameters and is, therefore, time-consuming and error-prone. Typically, these systems consist of highly similar components that need to be configured similarly. For largescale systems, a considerable portion of the configuration data can be reused, based on such similarities, during the configuration of each individual product. In this paper, we propose a model-based approach to automate the reuse of configuration data based on the similarities within an ICS product. Our approach provides configuration engineers with appropriate means for manipulating the reuse of configuration data, and provides the required formalism for ensuring the consistency of the reused data. Our investigation of a number of product configurations with an industry partner shows that more than 60% of configuration data can be automatically reused using our similarity-based approach, thereby reducing configuration effort.", "num_citations": "1\n", "authors": ["24"]}
{"title": "Conducting and Analyzing Empirical Studies in Search-Based Software Engineering.\n", "abstract": " Background\u0393\u00c7\u00f3 Search techniques are increasingly used in SE to solve a variety of problems, ranging from requirements prioritization, re-engineering, to test generation and fault fixing", "num_citations": "1\n", "authors": ["24"]}
{"title": "Traceability from Requirements to Design in Support of Safety Certification: A SysML-Based Framework and an Industrial Case Study\n", "abstract": " Traceability is one of the basic tenets of all software safety standards and a key prerequisite for certification of software. Despite this, the safety-critical software industry is still suffering from a chronic lack of guidelines on traceability. An acute traceability problem that we have identified through observing the software safety certification process has to do with the link between safety requirements and software design. In the current state of practice, this link often lacks sufficient detail to support the systematic inspections conducted by the certifiers of the software safety documentation. As a result, the suppliers often have to remedy the traceability gaps after the fact which can be very expensive and the outcome might be far from satisfactory. The first goal of this paper is to provide a traceability information model targeting software safety certification by applying and specializing the Systems Modeling Language (SysML\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["24"]}
{"title": "Model-driven development and search-based software engineering: an opportunity for research synergy\n", "abstract": " There is a sharply increasing research activity in the area of model-driven development, in particular in the context of the OMG standard named Model-Driven Architecture (MDA), which is relying on the Unified Modeling Language (UML) and its extensions. The basic idea is to carry software development as a series of model transformations, going from requirements to a platform independent model, to a platform specific model, and then to code generation. Development is therefore model-centric and many activities, including early design analysis and test case generation, are based on models using UML or adequate extensions. This keynote address will explain why there is a great opportunity for synergy between MDA and SBSE research. This will be illustrated by examples from recent research and industry collaborations.", "num_citations": "1\n", "authors": ["24"]}
{"title": "AINSI-An Inductive Software Process Improvement Method: Concrete Steps and Guidelines\n", "abstract": " Top-down approaches to process improvement based on generic \"best practice\" models (e.g., CMM, TRILLIUM, BOOTSTRAP, SPICE) have become popular. Despite the idiosyncrasies of each of these approaches, they share some common characteristics: all of them are based on numerous assumptions about what are best practices, and about the business goals of organizations and the problems they face.  Other organizations, like the Software Engineering Laboratory of the NASA Goddard Space Flight Center, HP and CRIM in Canada, have adopted the Quality Improvement Paradigm (QIP). The QIP stipulates a more bottom-up and inductive approach to process improvement. The focus of this paradigm is to first understand what processes exist in the organization and to determine what causes the most significant problems. Based on this, opportunities for improvement are devised, and empirical studies are conducted to evaluate potential solutions. In this paper, we present a method, named AINSI (An INductive Software process Improvment method), which defines general but concrete steps and guidelines for putting in place the QIP. This method is the result of the collective experiences of the authors and integrates many lessons learned from process improvement efforts in different environments.  It also integrates many complementary techniques such as qualitative analysis, methods for data collection (e.g., the Goal/Question/Metric paradigm), and quantitative evaluation. (Also cross-referenced as UMIACS-TR-95-77)", "num_citations": "1\n", "authors": ["24"]}
{"title": "Quality modeling based on coupling measures in a commercial object-oriented system\n", "abstract": " Th is p ap er p rop oses a com p reh ensive suite ofm easures to quantify th e levelof class coup ling d uring th ed esig n ofobject-oriented (OO) system s. Th is suite takes into account d ifferent OO d esig nm ech anism s, such as usag e, sp ecialization, and ag g reg ation, th us cap turing d ifferent kind s ofcoup ling in OO system s. Based on d ata about op erationalfailuresofa com m ercialsoftw are system, our coup ling m easures are em p irically investig ated by analyz ing th eir relationsh ip w ith th ep robability offault d etection across classes. Th e results d emonstrate th at som e ofth ese coup ling m easures, along w ith som e ofCh id amber and Kem erer\u0393\u00c7\u00d6sm easures, m ay be usefulearly quality ind icators ofth ed esig n ofOO system s. In ad d ition, p rincip alcom p onent analysis sh ow s th at th e und erlying th eory on wh ich are based our coup ling m easures is p artially supp orted by evid ence. Th e results are th\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["24"]}
{"title": "Empirical research in object\u0393\u00c7\u00c9oriented quality\n", "abstract": " From a general perspective, one of the most fundamental issues in software engineering is to understand the relationships between the use of quality guidelines and their impact on the software products\u0393\u00c7\u00d6 internal structure and external quality attributes of the product such as maintainability and reliability. An inappropriate internal product structure puts a higher cognitive complexity on individuals trying to understand how the product works, which, in turn, may cause the product to be less maintainable and reliable. In particular, with the extensive development of object-oriented (OO) methodologies and products in industry, we need to better understand what success factors make the transition to OO technologies beneficial and how to optimize the quality of resulting products.", "num_citations": "1\n", "authors": ["24"]}
{"title": "Software Engineering R&D group at CRIM\n", "abstract": " Top-down approaches to process improvement based on generic'best practice'models (eg, the CMh4 and the SPICE reference model) have become popular. Despite the idiosyncrasies of these approaches, they share some common characteristics: all of them make considerable assumptions about what are best practices, and about the business goals of organizations and the problems they face.The software engineering R&D group at CRIM have adopted the Quality Improvement Paradigm (QIP) that has been applied previously at the Software Engineering Laboratory of the NASA Goddard Space Flight Center. The QIP stipulates a more bottom-up and inductive approach to process improvement. The focus of this paradigm is to first understand what processes exist in the organization and to determine what causes the most sigruficant problems. Based on this, opportunities for improvement are devised, and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["24"]}
{"title": "Optimized set reduction for empirically guiding software development\n", "abstract": " Classification modeling is presented for software engineering errors. Errors are categorized into different groups and a code is developed to detect these faults and classify them accordingly. All information is presented in viewgraph format. (H.A.)", "num_citations": "1\n", "authors": ["24"]}
{"title": "A scalable approach for malware detection through bounded feature space behavior modeling.(2013)\n", "abstract": " In recent years, malware (malicious software) has greatly evolved and has become very sophisticated. The evolution of malware makes it difficult to detect using traditional signature-based malware detectors. Thus, researchers have proposed various behavior-based malware detection techniques to mitigate this problem. However, there are still serious shortcomings, related to scalability and computational complexity, in existing malware behavior modeling techniques. This raises questions about the practical applicability of these techniques.This paper proposes and evaluates a bounded feature space behavior modeling (BOFM) framework for scalable malware detection. BOFM models the interactions between software (which can be malware or benign) and security-critical OS resources in a scalable manner. Information collected at run-time according to this model is then used by machine learning algorithms to learn how to accurately classify software as malware or benign. One of the key problems with simple malware behavior modeling (eg, n-gram model) is that the number of malware features (ie, signatures) grows proportional to the size of execution traces, with a resulting malware feature space that is so large that it makes the detection process very challenging. On the other hand, in BOFM, the malware feature space is bounded by an upper limit N, a constant, and the results of our experiments show that its computation time and memory usage are vastly lower than in currently reported, malware detection techniques, while preserving or even improving their high detection accuracy.", "num_citations": "1\n", "authors": ["24"]}
{"title": "Technical Report 2010-01: Model Transformations as a Strategy to Automate Model-Based Testing\u0393\u00c7\u00f4A Tool and Industrial Case Studies, Version 1.0\n", "abstract": " In recent years, Model-Based Testing (MBT) has attracted an increasingly wide interest from industry and academia. The beneficial use of MBT, however, requires tools that not only automate the testing process, but that also rely in an extensible and configurable architecture that make them adaptable to various contexts of application. Though a number of tools have been developed to support MBT, this technical report introduces a new approach for designing and developing MBT tools that is based on model transformation technology. We report on the experimental development of a novel MBT tool, TRansformation-based tool for Uml-baSed Testing (TRUST), which software architecture and implementation strategy supports configurable and extensible features such as input models, test models, coverage criteria, test data generation strategies, and test script languages. Based on two industrial case studies, we demonstrate the configurability and extensibility of TRUST. We also investigate the challenges and likely cost savings when compared to manual test generation.", "num_citations": "1\n", "authors": ["24"]}