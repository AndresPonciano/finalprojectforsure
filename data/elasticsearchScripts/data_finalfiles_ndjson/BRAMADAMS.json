{"title": "A large-scale empirical study of just-in-time quality assurance\n", "abstract": " Defect prediction models are a well-known technique for identifying defect-prone files or packages such that practitioners can allocate their quality assurance efforts (e.g., testing and code reviews). However, once the critical files or packages have been identified, developers still need to spend considerable time drilling down to the functions or even code snippets that should be reviewed or tested. This makes the approach too time consuming and impractical for large software systems. Instead, we consider defect prediction models that focus on identifying defect-prone (\u0393\u00c7\u00a3risky\u0393\u00c7\u00a5) software changes instead of files or packages. We refer to this type of quality assurance activity as \u0393\u00c7\u00a3Just-In-Time Quality Assurance,\u0393\u00c7\u00a5 because developers can review and test these risky changes while they are still fresh in their minds (i.e., at check-in time). To build a change risk model, we use a wide range of factors based on the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "425\n", "authors": ["60"]}
{"title": "The impact of code review coverage and code review participation on software quality: A case study of the qt, vtk, and itk projects\n", "abstract": " Software code review, ie, the practice of having third-party team members critique changes to a software system, is a well-established best practice in both open source and proprietary software domains. Prior work has shown that the formal code inspections of the past tend to improve the quality of software delivered by students and small teams. However, the formal code inspection process mandates strict review criteria (eg, in-person meetings and reviewer checklists) to ensure a base level of review quality, while the modern, lightweight code reviewing process does not. Although recent work explores the modern code review process qualitatively, little research quantitatively explores the relationship between properties of the modern code review process and software quality. Hence, in this paper, we study the relationship between software quality and:(1) code review coverage, ie, the proportion of changes that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "304\n", "authors": ["60"]}
{"title": "An empirical study of the impact of modern code review practices on software quality\n", "abstract": " Software code review, i.e., the practice of having other team members critique changes to a software system, is a well-established best practice in both open source and proprietary software domains. Prior work has shown that formal code inspections tend to improve the quality of delivered software. However, the formal code inspection process mandates strict review criteria (e.g., in-person meetings and reviewer checklists) to ensure a base level of review quality, while the modern, lightweight code reviewing process does not. Although recent work explores the modern code review process, little is known about the relationship between modern code review practices and long-term software quality. Hence, in this paper, we study the relationship between post-release defects (a popular proxy for long-term software quality) and: (1) code review coverage, i.e., the proportion of changes that have been code\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "215\n", "authors": ["60"]}
{"title": "Do developers feel emotions? an exploratory analysis of emotions in software artifacts\n", "abstract": " Software development is a collaborative activity in which developers interact to create and maintain a complex software system. Human collaboration inevitably evokes emotions like joy or sadness, which can affect the collaboration either positively or negatively, yet not much is known about the individual emotions and their role for software development stakeholders. In this study, we analyze whether development artifacts like issue reports carry any emotional information about software development. This is a first step towards verifying the feasibility of an automatic tool for emotion mining in software development artifacts: if humans cannot determine any emotion from a software artifact, neither can a tool. Analysis of the Apache Software Foundation issue tracking system shows that developers do express emotions (in particular gratitude, joy and sadness). However, the more context is provided about an issue\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "191\n", "authors": ["60"]}
{"title": "Security versus performance bugs: a case study on firefox\n", "abstract": " A good understanding of the impact of different types of bugs on various project aspects is essential to improve software quality research and practice. For instance, we would expect that security bugs are fixed faster than other types of bugs due to their critical nature. However, prior research has often treated all bugs as similar when studying various aspects of software quality (eg, predicting the time to fix a bug), or has focused on one particular type of bug (eg, security bugs) with little comparison to other types. In this paper, we study how different types of bugs (performance and security bugs) differ from each other and from the rest of the bugs in a software project. Through a case study on the Firefox project, we find that security bugs are fixed and triaged much faster, but are reopened and tossed more frequently. Furthermore, we also find that security bugs involve more developers and impact more files in a project\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "185\n", "authors": ["60"]}
{"title": "Do faster releases improve software quality? an empirical case study of mozilla firefox\n", "abstract": " Nowadays, many software companies are shifting from the traditional 18-month release cycle to shorter release cycles. For example, Google Chrome and Mozilla Firefox release new versions every 6 weeks. These shorter release cycles reduce the users' waiting time for a new release and offer better marketing opportunities to companies, but it is unclear if the quality of the software product improves as well, since shorter release cycles result in shorter testing periods. In this paper, we empirically study the development process of Mozilla Firefox in 2010 and 2011, a period during which the project transitioned to a shorter release cycle. We compare crash rates, median uptime, and the proportion of post-release bugs of the versions that had a shorter release cycle with those having a traditional release cycle, to assess the relation between release cycle length and the software quality observed by the end user. We\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "175\n", "authors": ["60"]}
{"title": "An empirical study on inconsistent changes to code clones at the release level\n", "abstract": " To study the impact of code clones on software quality, researchers typically carry out their studies based on fine-grained analysis of inconsistent changes at the revision level. As a result, they capture much of the chaotic and experimental nature inherent in any on-going software development process. Analyzing highly fluctuating and short-lived clones is likely to exaggerate the ill effects of inconsistent changes on the quality of the released software product, as perceived by the end user. To gain a broader perspective, we perform an empirical study on the effect of inconsistent changes on software quality at the release level. Based on a case study on three open source software systems, we observe that only 1.02%\u0393\u00c7\u00f44.00% of all clone genealogies introduce software defects at the release level, as opposed to the substantially higher percentages reported by previous studies at the revision level. Our findings suggest\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "172\n", "authors": ["60"]}
{"title": "Assisting developers of big data analytics applications when deploying on hadoop clouds\n", "abstract": " Big data analytics is the process of examining large amounts of data (big data) in an effort to uncover hidden patterns or unknown correlations. Big Data Analytics Applications (BDA Apps) are a new type of software applications, which analyze big data using massive parallel processing frameworks (e.g., Hadoop). Developers of such applications typically develop them using a small sample of data in a pseudo-cloud environment. Afterwards, they deploy the applications in a large-scale cloud environment with considerably more processing power and larger input data (reminiscent of the mainframe days). Working with BDA App developers in industry over the past three years, we noticed that the runtime analysis and debugging of such applications in the deployment phase cannot be easily addressed by traditional monitoring and debugging approaches. In this paper, as a first step in assisting developers of BDA\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "169\n", "authors": ["60"]}
{"title": "Understanding reuse in the android market\n", "abstract": " Mobile apps are software products developed to run on mobile devices, and are typically distributed via app stores. The mobile app market is estimated to be worth billions of dollars, with more than hundred of thousands of apps, and still increasing in number. This explosion of mobile apps is astonishing, given the short time span that they have been around. One possible explanation for this explosion could be the practice of software reuse. Yet, no research has studied such practice in mobile app development. In this paper, we intend to analyze software reuse in the Android mobile app market along two dimensions: (a) reuse by inheritance, and (b) class reuse. Since app stores only distribute the byte code of the mobile apps, and not the source code, we used the concept of Software Bertillonage to track code across mobile apps. A case study on thousands of mobile apps across five different categories in the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "158\n", "authors": ["60"]}
{"title": "Will my patch make it? and how fast? case study on the linux kernel\n", "abstract": " The Linux kernel follows an extremely distributed reviewing and integration process supported by 130 developer mailing lists and a hierarchy of dozens of Git repositories for version control. Since not every patch can make it and of those that do, some patches require a lot more reviewing and integration effort than others, developers, reviewers and integrators need support for estimating which patches are worthwhile to spend effort on and which ones do not stand a chance. This paper crosslinks and analyzes eight years of patch reviews from the kernel mailing lists and committed patches from the Git repository to understand which patches are accepted and how long it takes those patches to get to the end user. We found that 33% of the patches makes it into a Linux release, and that most of them need 3 to 6 months for this. Furthermore, that patches developed by more experienced developers are more easily\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "146\n", "authors": ["60"]}
{"title": "An empirical study of build maintenance effort\n", "abstract": " The build system of a software project is responsible for transforming source code and other development artifacts into executable programs and deliverables. Similar to source code, build system specifications require maintenance to cope with newly implemented features, changes to imported Application Program Interfaces (APIs), and source code restructuring. In this paper, we mine the version histories of one proprietary and nine open source projects of different sizes and domain to analyze the overhead that build maintenance imposes on developers. We split our analysis into two dimensions: (1) Build Coupling, i.e., how frequently source code changes require build changes, and (2) Build Ownership, i.e., the proportion of developers responsible for build maintenance. Our results indicate that, despite the difference in scale, the build system churn rate is comparable to that of the source code, and build changes\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "145\n", "authors": ["60"]}
{"title": "Studying re-opened bugs in open source software\n", "abstract": " Bug fixing accounts for a large amount of the software maintenance resources. Generally, bugs are reported, fixed, verified and closed. However, in some cases bugs have to be re-opened. Re-opened bugs increase maintenance costs, degrade the overall user-perceived quality of the software and lead to unnecessary rework by busy practitioners. In this paper, we study and predict re-opened bugs through a case study on three large open source projects\u0393\u00c7\u00f6namely Eclipse, Apache and OpenOffice. We structure our study along four dimensions: (1) the work habits dimension (e.g., the weekday on which the bug was initially closed), (2) the bug report dimension (e.g., the component in which the bug was found) (3) the bug fix dimension (e.g., the amount of time it took to perform the initial fix) and (4) the team dimension (e.g., the experience of the bug fixer). We build decision trees using the aforementioned\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "138\n", "authors": ["60"]}
{"title": "Predicting re-opened bugs: A case study on the eclipse project\n", "abstract": " Bug fixing accounts for a large amount of the software maintenance resources. Generally, bugs are reported, fixed, verified and closed. However, in some cases bugs have to be re-opened. Re-opened bugs increase maintenance costs, degrade the overall user-perceived quality of the software and lead to unnecessary rework by busy practitioners. In this paper, we study and predict re-opened bugs through a case study on the Eclipse project. We structure our study along 4 dimensions: (1) the work habits dimension (e.g., the weekday on which the bug was initially closed on), (2) the bug report dimension (e.g., the component in which the bug was found) (3) the bug fix dimension (e.g., the amount of time it took to perform the initial fix) and (4) the team dimension (e.g., the experience of the bug fixer). Our case study on the Eclipse Platform 3.0 project shows that the comment and description text, the time it took to fix\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "135\n", "authors": ["60"]}
{"title": "Energy profiles of java collections classes\n", "abstract": " We created detailed profiles of the energy consumed by common operations done on Java List, Map, and Set abstractions. The results show that the alternative data types for these abstractions differ significantly in terms of energy consumption depending on the operations. For example, an ArrayList consumes less energy than a LinkedList if items are inserted at the middle or at the end, but consumes more energy than a LinkedList if items are inserted at the start of the list. To explain the results, we explored the memory usage and the bytecode executed during an operation. Expensive computation tasks in the analyzed bytecode traces appeared to have an energy impact, but memory usage did not contribute. We evaluated our profiles by using them to selectively replace Collections types used in six applications and libraries. We found that choosing the wrong Collections type, as indicated by our profiles, can cost\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "133\n", "authors": ["60"]}
{"title": "A qualitative study on performance bugs\n", "abstract": " Software performance is one of the important qualities that makes software stand out in a competitive market. However, in earlier work we found that performance bugs take more time to fix, need to be fixed by more experienced developers and require changes to more code than non-performance bugs. In order to be able to improve the resolution of performance bugs, a better understanding is needed of the current practice and shortcomings of reporting, reproducing, tracking and fixing performance bugs. This paper qualitatively studies a random sample of 400 performance and non-performance bug reports of Mozilla Firefox and Google Chrome across four dimensions (Impact, Context, Fix and Fix validation). We found that developers and users face problems in reproducing performance bugs and have to spend more time discussing performance bugs than other kinds of bugs. Sometimes performance\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "132\n", "authors": ["60"]}
{"title": "Design recovery and maintenance of build systems\n", "abstract": " The build system forms an indispensable part of any software project. It needs to evolve in parallel with the source code in order to build, test and install the software. Unfortunately, little tool support exists to help maintainers gain insight into the build system, much less to refactor it. In this paper, we therefore present the design and implementation of a re(verse)-engineering framework for build systems named MAKAO. At its heart the framework makes the build's dependency graph available in a tangible way. Aside from visualisation, this enables powerful querying of all build-related data, as well as various filtering techniques to define views on the build architecture. If desired, all this gathered information can be put to use to write aspects for refactoring the build. Afterwards, validation rules can help in assessing failure or success. We applied our implementation on an industrial C system and the Linux 2.6.16.18\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "126\n", "authors": ["60"]}
{"title": "An industrial study on the risk of software changes\n", "abstract": " Modelling and understanding bugs has been the focus of much of the Software Engineering research today. However, organizations are interested in more than just bugs. In particular, they are more concerned about managing risk, ie, the likelihood that a code or design change will cause a negative impact on their products and processes, regardless of whether or not it introduces a bug. In this paper, we conduct a year-long study involving more than 450 developers of a large enterprise, spanning more than 60 teams, to better understand risky changes, ie, changes for which developers believe that additional attention is needed in the form of careful code or design reviewing and/or more testing. Our findings show that different developers and different teams have their own criteria for determining risky changes. Using factors extracted from the changes and the history of the files modified by the changes, we are able\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "120\n", "authors": ["60"]}
{"title": "High-impact defects: a study of breakage and surprise defects\n", "abstract": " The relationship between various software-related phenomena (eg, code complexity) and post-release software defects has been thoroughly examined. However, to date these predictions have a limited adoption in practice. The most commonly cited reason is that the prediction identifies too much code to review without distinguishing the impact of these defects. Our aim is to address this drawback by focusing on high-impact defects for customers and practitioners. Customers are highly impacted by defects that break pre-existing functionality (breakage defects), whereas practitioners are caught off-guard by defects in files that had relatively few pre-release changes (surprise defects). The large commercial software system that we study already had an established concept of breakages as the highest-impact defects, however, the concept of surprises is novel and not as well established. We find that surprise defects\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "120\n", "authors": ["60"]}
{"title": "What do programmers know about software energy consumption?\n", "abstract": " Traditionally, programmers received a range of training on programming languages and methodologies, but they rarely receive training on software energy consumption. Yet, the popularity of mobile devices and cloud computing requires increased awareness of software energy consumption. On mobile devices, battery life often limits computation. Under the demands of cloud computing, datacenters struggle to reduce energy consumption through virtualization and datacenter-infrastructure-management systems. Efficient software energy consumption is increasingly becoming an important nonfunctional requirement for programmers. However, are programmers knowledgeable enough about software energy consumption? Do they base their implementation decision on popular beliefs? Researchers surveyed more than 100 programmers regarding their knowledge of software energy consumption. They found that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "119\n", "authors": ["60"]}
{"title": "Understanding the impact of code and process metrics on post-release defects: a case study on the eclipse project\n", "abstract": " Research studying the quality of software applications continues to grow rapidly with researchers building regression models that combine a large number of metrics. However, these models are hard to deploy in practice due to the cost associated with collecting all the needed metrics, the complexity of the models and the black box nature of the models. For example, techniques such as PCA merge a large number of metrics into composite metrics that are no longer easy to explain. In this paper, we use a statistical approach recently proposed by Cataldo et al. to create explainable regression models. A case study on the Eclipse open source project shows that only 4 out of the 34 code and process metrics impacts the likelihood of finding a post-release defect. In addition, our approach is able to quantify the impact of these metrics on the likelihood of finding post-release defects. Finally, we demonstrate that our simple\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "118\n", "authors": ["60"]}
{"title": "Modern release engineering in a nutshell--why researchers should care\n", "abstract": " The release engineering process is the process that brings high quality code changes from a developer's workspace to the end user, encompassing code change integration, continuous integration, build system specifications, infrastructure-as-code, deployment and release. Recent practices of continuous delivery, which bring new content to the end user in days or hours rather than months or years, have generated a surge of industry-driven interest in the release engineering pipeline. This paper argues that the involvement of researchers is essential, by providing a brief introduction to the six major phases of the release engineering pipeline, a roadmap of future research, and a checklist of three major ways that the release engineering process of a system under study can invalidate the findings of software engineering studies. The main take-home message is that, while release engineering technology has\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "114\n", "authors": ["60"]}
{"title": "Mining performance regression testing repositories for automated performance analysis\n", "abstract": " Performance regression testing detects performance regressions in a system under load. Such regressions refer to situations where software performance degrades compared to previous releases, although the new version behaves correctly. In current practice, performance analysts must manually analyze performance regression testing data to uncover performance regressions. This process is both time-consuming and error-prone due to the large volume of metrics collected, the absence of formal performance objectives and the subjectivity of individual performance analysts. In this paper, we present an automated approach to detect potential performance regressions in a performance regression test. Our approach compares new test results against correlations pre-computed performance metrics extracted from performance regression testing repositories. Case studies show that our approach scales well to large\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "102\n", "authors": ["60"]}
{"title": "The evolution of the R software ecosystem\n", "abstract": " Software ecosystems form the heart of modern companies' collaboration strategies with end users, open source developers and other companies. An ecosystem consists of a core platform and a halo of user contributions that provide value to a company or project. In order to sustain the level and number of high-quality contributions, it is crucial for companies and contributors to understand how ecosystems tend to evolve and can be maintained successfully over time. As a first step, this paper explores the evolution characteristics of the statistical computing project GNU R, which is a successful, end-user programming ecosystem. We find that the ecosystem of user-contributed R packages has been growing steadily since R's conception, at a significantly faster rate than core packages, yet each individual package remains stable in size. We also identified differences in the way user-contributed and core packages are\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "98\n", "authors": ["60"]}
{"title": "Studying software evolution using topic models\n", "abstract": " Topic models are generative probabilistic models which have been applied to information retrieval to automatically organize and provide structure to a text corpus. Topic models discover topics in the corpus, which represent real world concepts by frequently co-occurring words. Recently, researchers found topics to be effective tools for structuring various software artifacts, such as source code, requirements documents, and bug reports. This research also hypothesized that using topics to describe the evolution of software repositories could be useful for maintenance and understanding tasks. However, research has yet to determine whether these automatically discovered topic evolutions describe the evolution of source code in a way that is relevant or meaningful to project stakeholders, and thus it is not clear whether topic models are a suitable tool for this task.In this paper, we take a first step towards evaluating\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "97\n", "authors": ["60"]}
{"title": "Validating the use of topic models for software evolution\n", "abstract": " Topics are collections of words that co-occur frequently in a text corpus. Topics have been found to be effective tools for describing the major themes spanning a corpus. Using such topics to describe the evolution of a software system's source code promises to be extremely useful for development tasks such as maintenance and re-engineering. However, no one has yet examined whether these automatically discovered topics accurately describe the evolution of source code, and thus it is not clear whether topic models are a suitable tool for this task. In this paper, we take a first step towards deter-mining the suitability of topic models in the analysis of software evolution by performing a qualitative case study on 12 releases of JHotDraw, a well studied and documented system. We define and compute various metrics on the identified topics and manually investigate how the metrics evolve over time. We find that topic\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "97\n", "authors": ["60"]}
{"title": "Modeling the evolution of topics in source code histories\n", "abstract": " Studying the evolution of topics (collections of co-occurring words) in a software project is an emerging technique to automatically shed light on how the project is changing over time: which topics are becoming more actively developed, which ones are dying down, or which topics are lately more error-prone and hence require more testing. Existing techniques for modeling the evolution of topics in software projects suffer from issues of data duplication, ie, when the repository contains multiple copies of the same document, as is the case in source code histories. To address this issue, we propose the Diff model, which applies a topic model only to the changes of the documents in each version instead of to the whole document at each version. A comparative study with a state-of-the-art topic evolution model shows that the Diff model can detect more distinct topics as well as more sensitive and accurate topic evolutions\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "94\n", "authors": ["60"]}
{"title": "Automated detection of performance regressions using statistical process control techniques\n", "abstract": " The goal of performance regression testing is to check for performance regressions in a new version of a software system. Performance regression testing is an important phase in the software development process. Performance regression testing is very time consuming yet there is usually little time assigned for it. A typical test run would output thousands of performance counters. Testers usually have to manually inspect these counters to identify performance regressions. In this paper, we propose an approach to analyze performance counters across test runs using a statistical process control technique called control charts. We evaluate our approach using historical data of a large software team as well as an open-source software project. The results show that our approach can accurately identify performance regressions in both software systems. Feedback from practitioners is very promising due to the simplicity\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "90\n", "authors": ["60"]}
{"title": "Can we refactor conditional compilation into aspects?\n", "abstract": " Systems software uses conditional compilation to manage crosscutting concerns in a very fine-grained and efficient way, but at the expense of tangled and scattered conditional code. Refactoring of conditional compilation into aspects gets rid of these issues, but it is not clear yet for which patterns of conditional compilation aspects make sense and whether or not current aspect technology is able to express these patterns. To investigate these two problems, this paper presents a graphical``preprocessor blueprint''model which offers a queryable representation of the syntactical interaction of conditional compilation and the source code. A case study on the Parrot VM shows that preprocessor blueprints are able to express and query for the four commonly known patterns of conditional compilation usage, and that they allow to discover seven additional important patterns. By correlating each pattern's potential for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "90\n", "authors": ["60"]}
{"title": "A case study of bias in bug-fix datasets\n", "abstract": " Software quality researchers build software quality models by recovering traceability links between bug reports in issue tracking repositories and source code files. However, all too often the data stored in issue tracking repositories is not explicitly tagged or linked to source code. Researchers have to resort to heuristics to tag the data (e.g., to determine if an issue is a bug report or a work item), or to link a piece of code to a particular issue or bug. Recent studies by Bird et al. and by Antoniol et al. suggest that software models based on imperfect datasets with missing links to the code and incorrect tagging of issues, exhibit biases that compromise the validity and generality of the quality models built on top of the datasets. In this study, we verify the effects of such biases for a commercial project that enforces strict development guidelines and rules on the quality of the data in its issue tracking repository. Our results show\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "85\n", "authors": ["60"]}
{"title": "The emotional side of software developers in JIRA\n", "abstract": " Issue tracking systems store valuable data for testing hypotheses concerning maintenance, building statistical prediction models and (recently) investigating developer affectiveness. For the latter, issue tracking systems can be mined to explore developers emotions, sentiments and politeness -affects for short. However, research on affect detection in software artefacts is still in its early stage due to the lack of manually validated data and tools. In this paper, we contribute to the research of affects on software artefacts by providing a labeling of emotions present on issue comments. We manually labeled 2,000 issue comments and 4,000 sentences written by developers with emotions such as love, joy, surprise, anger, sadness and fear. Labeled comments and sentences are linked to software artefacts reported in our previously published dataset (containing more than 1K projects, more than 700K issue reports and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "84\n", "authors": ["60"]}
{"title": "Release Practices for Mobile Apps--What do Users and Developers Think?\n", "abstract": " Large software organizations such as Facebook or Netflix, who otherwise make daily or even hourly releases of their web applications using continuous delivery, have had to invest heavily into a customized release strategy for their mobile apps, because the vetting process of app stores introduces lag and uncertainty into the release process. Amidst these large, resourceful organizations, it is unknown how the average mobile app developer organizes her app's releases, even though an incorrect strategy might bring a premature app update to the market that drives away customers towards the heavy market competition. To understand the common release strategies used for mobile apps, the rationale behind them and their perceived impact on users, we performed two surveys with users and developers. We found that half of the developers have a clear strategy for their mobile app releases, since especially the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "81\n", "authors": ["60"]}
{"title": "Revisiting prior empirical findings for mobile apps: An empirical case study on the 15 most popular open-source Android apps\n", "abstract": " Our increasing reliance on mobile devices has led to the explosive development of millions of mobile apps across multiple platforms that are used by millions of people around the world every day. However, most software engineering research is performed on large desktop or server-side software applications (eg, Eclipse and Apache). Unlike the software applications that we typically study, mobile apps are 1) designed to run on devices with limited, but diverse, resources (eg, limited screen space and touch interfaces with diverse gestures) and 2) distributed through centralized \u0393\u00c7\u00a3app stores,\u0393\u00c7\u00a5 where there is a low barrier to entry and heavy competition. Hence, mobile apps may differ from traditionally studied desktop or server side applications, the extent that existing software development \u0393\u00c7\u00a3best practices\u0393\u00c7\u00a5 may not apply to mobile apps. Therefore, we perform an exploratory study, comparing mobile apps to commonly studied large applications and smaller applications along two dimensions: the size of the code base and the time to fix defects. Finally, we discuss the impact of our findings by identifying a set of unique software engineering challenges posed by mobile apps.", "num_citations": "80\n", "authors": ["60"]}
{"title": "Monitoring sentiment in open source mailing lists: exploratory study on the apache ecosystem.\n", "abstract": " Large software projects, both open and closed source, are constructed and maintained collaboratively by teams of developers and testers, who are typically geographically dispersed. This dispersion creates a distance between team members, hiding feelings of distress or (un) happiness from their manager, which prevents him or her from using remediation techniques for those feelings. This paper evaluates the usage of automatic sentiment analysis to identify distress or happiness in a development team. Since mailing lists are one of the most popular media for discussion in distributed software projects, we extracted sentiment values of the user and developer mailing lists of two of the most successful and mature projects of the Apache software foundation. The results show that (1) user and developer mailing lists carry both positive and negative sentiment and have a slightly different focus, while (2) work is needed to customize automatic sentiment analysis techniques to the domain of software engineering, since they lack precision when facing technical terms", "num_citations": "76\n", "authors": ["60"]}
{"title": "Why do automated builds break? an empirical study\n", "abstract": " To detect integration errors as quickly as possible, organizations use automated build systems. Such systems ensure that (1) the developers are able to integrate their parts into an executable whole, (2) the testers are able to test the built system, (3) and the release engineers are able to leverage the generated build to produce the upcoming release. The flipside of automated builds is that any incorrect change can break the build, and hence testing and releasing, and (even worse) block other developers from continuing their work, delaying the project even further. To measure the impact of such build breakage, this empirical study analyzes 3,214 builds produced in a large software company over a period of 6 months. We found a high ratio of build breakage (17.9%), and also quantified the cost of such build breakage as more than 336.18 man-hours. Interviews with 28 software engineers from the company helped to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "74\n", "authors": ["60"]}
{"title": "The evolution of Java build systems\n", "abstract": " Build systems are responsible for transforming static source code artifacts into executable software. While build systems play such a crucial role in software development and maintenance, they have been largely ignored by software evolution researchers. However, a firm understanding of build system aging processes is needed in order to allow project managers to allocate personnel and resources to build system maintenance tasks effectively, and reduce the build maintenance overhead on regular development activities. In this paper, we study the evolution of build systems based on two popular Java build languages (i.e., ANT and Maven) from two perspectives: (1) a static perspective, where we examine the complexity of build system specifications using software metrics adopted from the source code domain; and (2) a dynamic perspective, where the complexity and coverage of representative build\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "74\n", "authors": ["60"]}
{"title": "An exploratory study of the evolution of communicated information about the execution of large software systems\n", "abstract": " Substantial research in software engineering focuses on understanding the dynamic nature of software systems in order to improve software maintenance and program comprehension. This research typically makes use of automated instrumentation and profiling techniques after the fact, that is, without considering domain knowledge. In this paper, we examine another source of dynamic information that is generated from statements that have been inserted into the code base during development to draw the system administrators' attention to important run\u0393\u00c7\u00c9time phenomena. We call this source communicated information (CI). Examples of CI include execution logs and system events. The availability of CI has sparked the development of an ecosystem of Log Processing Apps (LPAs) that surround the software system under analysis to monitor and document various run\u0393\u00c7\u00c9time constraints. The dependence of LPAs on\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "72\n", "authors": ["60"]}
{"title": "The jira repository dataset: Understanding social aspects of software development\n", "abstract": " Issue tracking systems store valuable data for testing hypotheses concerning maintenance, building statistical prediction models and recently investigating developers\" affectiveness\". In particular, the Jira Issue Tracking System is a proprietary tracking system that has gained a tremendous popularity in the last years and offers unique features like the project management system and the Jira agile kanban board. This paper presents a dataset extracted from the Jira ITS of four popular open source ecosystems (as well as the tools and infrastructure used for extraction) the Apache Software Foundation, Spring, JBoss and CodeHaus communities. Our dataset hosts more than 1K projects, containing more than 700K issue reports and more than 2 million issue comments. Using this data, we have been able to deeply study the communication process among developers, and how this aspect affects the development\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "70\n", "authors": ["60"]}
{"title": "Exploring the development of micro-apps: A case study on the blackberry and android platforms\n", "abstract": " The recent meteoric rise in the use of smart phones and other mobile devices has led to a new class of applications, i.e., micro-apps, that are designed to run on devices with limited processing, memory, storage and display resources. Given the rapid succession of mobile technologies and the fierce competition, micro-app vendors need to release new features at break-neck speed, without sacrificing product quality. To understand how different mobile platforms enable such a rapid turnaround-time, this paper compares three pairs of feature-equivalent Android and Blackberry micro-apps. We do this by analyzing the micro-apps along the dimensions of source code, code dependencies and code churn. BlackBerry micro-apps are much larger and rely more on third party libraries. However, they are less susceptible to platform changes since they rely less on the underlying platform. On the other hand, Android micro\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "69\n", "authors": ["60"]}
{"title": "Identifying crosscutting concerns using historical code changes\n", "abstract": " Detailed knowledge about implemented concerns in the source code is crucial for the cost-effective maintenance and successful evolution of large systems. Concern mining techniques can automatically suggest sets of related code fragments that likely contribute to the implementation of a concern. However, developers must then spend considerable time understanding and expanding these concern seeds to obtain the full concern implementation. We propose a new mining technique (COMMIT) that reduces this manual effort. COMMIT addresses three major shortcomings of current concern mining techniques: 1) their inability to merge seeds with small variations, 2) their tendency to ignore important facets of concerns, and 3) their lack of information about the relations between seeds. A comparative case study on two large open source C systems (Post-greSQL and NetBSD) shows that COMMIT recovers up to 87.5\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "65\n", "authors": ["60"]}
{"title": "Co-evolution of infrastructure and source code-an empirical study\n", "abstract": " Infrastructure-as-code automates the process of configuring and setting up the environment (e.g., servers, VMs and databases) in which a software system will be tested and/or deployed, through textual specification files in a language like Puppet or Chef. Since the environment is instantiated automatically by the infrastructure languages' tools, no manual intervention is necessary apart from maintaining the infrastructure specification files. The amount of work involved with such maintenance, as well as the size and complexity of infrastructure specification files, have not yet been studied empirically. Through an empirical study of the version control system of 265 Open Stack projects, we find that infrastructure files are large and churn frequently, which could indicate a potential of introducing bugs. Furthermore, we found that the infrastructure code files are coupled tightly with the other files in a project, especially test\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "64\n", "authors": ["60"]}
{"title": "Mapreduce as a general framework to support research in mining software repositories (MSR)\n", "abstract": " Researchers continue to demonstrate the benefits of Mining Software Repositories (MSR) for supporting software development and research activities. However, as the mining process is time and resource intensive, they often create their own distributed platforms and use various optimizations to speed up and scale up their analysis. These platforms are project-specific, hard to reuse, and offer minimal debugging and deployment support. In this paper, we propose the use of MapReduce, a distributed computing platform, to support research in MSR. As a proof-of-concept, we migrate J-REX, an optimized evolutionary code extractor, to run on Hadoop, an open source implementation of MapReduce. Through a case study on the source control repositories of the Eclipse, BIRT and Datatools projects, we demonstrate that the migration effort to MapReduce is minimal and that the benefits are significant, as running time of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "63\n", "authors": ["60"]}
{"title": "Code of conduct in open source projects\n", "abstract": " Open source projects rely on collaboration of members from all around the world using web technologies like GitHub and Gerrit. This mixture of people with a wide range of backgrounds including minorities like women, ethnic minorities, and people with disabilities may increase the risk of offensive and destroying behaviours in the community, potentially leading affected project members to leave towards a more welcoming and friendly environment. To counter these effects, open source projects increasingly are turning to codes of conduct, in an attempt to promote their expectations and standards of ethical behaviour. In this first of its kind empirical study of codes of conduct in open source software projects, we investigated the role, scope and influence of codes of conduct through a mixture of quantitative and qualitative analysis, supported by interviews with practitioners. We found that the top codes of conduct are\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "62\n", "authors": ["60"]}
{"title": "Feature toggles: practitioner practices and a case study\n", "abstract": " Continuous delivery and rapid releases have led to innovative techniques for integrating new features and bug fixes into a new release faster. To reduce the probability of integration conflicts, major software companies, including Google, Facebook and Netflix, use feature toggles to incrementally integrate and test new features instead of integrating the feature only when it's ready. Even after release, feature toggles allow operations managers to quickly disable a new feature that is behaving erratically or to enable certain features only for certain groups of customers. Since literature on feature toggles is surprisingly slim, this paper tries to understand the prevalence and impact of feature toggles. First, we conducted a quantitative analysis of feature toggle usage across 39 releases of Google Chrome (spanning five years of release history). Then, we studied the technical debt involved with feature toggles by mining a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "62\n", "authors": ["60"]}
{"title": "Impact of ad libraries on ratings of android mobile apps\n", "abstract": " One of the most popular ways to monetize a free app is by including advertisements in the app. Several advertising (ad) companies provide these ads to app developers through ad libraries that need to be integrated in the app. However, the demand for ads far exceeds the supply. This obstacle may lead app developers to integrate several ad libraries from different ad companies in their app to ensure they receive an ad with each request. However, no study has explored how many ad libraries are commonly integrated into apps. Additionally, no research to date has examined whether integrating many different ad libraries impacts an app's ratings. This article examines these two issues by empirically examining thousands of Android apps. The authors find that there are apps with as many as 28 ad libraries, but they find no evidence that the number of ad libraries in an app is related to its possible rating in the app\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "61\n", "authors": ["60"]}
{"title": "Should I contribute to this discussion?\n", "abstract": " Development mailing lists play a central role in facilitating communication in open source projects. Since these lists frequently host design and project discussions, knowledgeable contribution to these discussion threads is essential to avoid mis-communication that might slow-down the progress of a project. However, given the sheer volume of emails on these lists, it is easy to miss important discussions. To find out how developers are able to deal with mailing list discussions, we study the main factors that encourage developers to contribute to the development mailing lists. We develop personalized models to automatically identify discussion threads that a developer would contribute to based on his previous contribution behavior. Case studies on development mailing lists of three open source projects (Apache, PostgreSQL and Python) show that the average accuracy of our models is 89-85% and that the models\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "57\n", "authors": ["60"]}
{"title": "The evolution of ANT build systems\n", "abstract": " Build systems are responsible for transforming static source code artifacts into executable software. While build systems play such a crucial role in software development and maintenance, they have been largely ignored by software evolution researchers. With a firm understanding of build system aging processes, project managers could allocate personnel and resources to build system maintenance tasks more effectively, reducing the build maintenance overhead on regular development activities. In this paper, we study the evolution of ANT build systems from two perspectives: (1) a static perspective, where we examine the build system specifications using software metrics adopted from the source code domain; and (2) a dynamic perspective where representative sample build runs are conducted and their output logs are analyzed. Case studies of four open source ANT build systems with a combined history of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "55\n", "authors": ["60"]}
{"title": "Studying the relationship between source code quality and mobile platform dependence\n", "abstract": " The recent meteoric rise in the use of smartphones and other mobile devices has led to a new class of software applications (i.e., mobile apps). One reason for this success is the extensive support available to mobile app developers through the APIs provided by mobile platforms (e.g., Android). In our previous research, we found that mobile apps tend to depend highly on these platform-specific APIs. High dependence on a particular mobile platform may introduce instability and defects, as these mobile platforms are rapidly evolving. Therefore, the extent of platform dependence may be an indicator of software quality. In this paper, we examine the relationship between platform dependence and defect proneness of the source code files of an Android app to determine whether software metrics based on platform dependence can be used to prioritize software quality assurance efforts. We find that (1) source\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "54\n", "authors": ["60"]}
{"title": "An industrial case study on the automated detection of performance regressions in heterogeneous environments\n", "abstract": " A key goal of performance testing is the detection of performance degradations (i.e., regressions) compared to previous releases. Prior research has proposed the automation of such analysis through the mining of historical performance data (e.g., CPU and memory usage) from prior test runs. Nevertheless, such research has had limited adoption in practice. Working with a large industrial performance testing lab, we noted that a major hurdle in the adoption of prior work (including our own work) is the incorrect assumption that prior tests are always executed in the same environment (i.e., labs). All too often, tests are performed in heterogenous environments with each test being run in a possibly different lab with different hardware and software configurations. To make automated performance regression analysis techniques work in industry, we propose to model the global expected behaviour of a system as an\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "53\n", "authors": ["60"]}
{"title": "Understanding the impact of rapid releases on software quality\n", "abstract": " Many software companies are shifting from the traditional multi-month release cycle to shorter release cycles. For example, Google Chrome and Mozilla Firefox release new versions every 6 weeks. These shorter release cycles reduce the users\u0393\u00c7\u00d6 waiting time for a new release and offer better feedback and marketing opportunities to companies, but it is unclear if the quality of the software product improves as well, since developers and testers are under more pressure. In this paper, we extend our previous empirical study of Mozilla Firefox on the impact of rapid releases on quality assurance with feedback by Mozilla project members. The study compares crash rates, median uptime, and the proportion of pre- and post-release bugs in traditional releases with those in rapid releases, and we also analyze the source code changes made by developers to identify potential changes in the development process\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "51\n", "authors": ["60"]}
{"title": "Mining co-change information to understand when build changes are necessary\n", "abstract": " As a software project ages, its source code is modified to add new features, restructure existing ones, and fix defects. These source code changes often induce changes in the build system, i.e., the system that specifies how source code is translated into deliverables. However, since developers are often not familiar with the complex and occasionally archaic technologies used to specify build systems, they may not be able to identify when their source code changes require accompanying build system changes. This can cause build breakages that slow development progress and impact other developers, testers, or even users. In this paper, we mine the source and test code changes that required accompanying build changes in order to better understand this co-change relationship. We build random forest classifiers using language-agnostic and language-specific code change characteristics to explain when code\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "51\n", "authors": ["60"]}
{"title": "Management of community contributions\n", "abstract": " In recent years, many companies have realized that collaboration with a thriving user or developer community is a major factor in creating innovative technology driven by market demand. As a result, businesses have sought ways to stimulate contributions from developers outside their corporate walls, and integrate external developers into their development process. To support software companies in this process, this paper presents an empirical study on the contribution management processes of two major, successful, open source software ecosystems. We contrast a for-profit (ANDROID) system having a hybrid contribution style, with a not-for-profit (LINUX kernel) system having an open contribution style. To guide our comparisons, we base our analysis on a conceptual model of contribution management that we derived from a total of seven major open-source software systems. A quantitative comparison\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "49\n", "authors": ["60"]}
{"title": "On the relationship between comment update practices and software bugs\n", "abstract": " When changing source code, developers sometimes update the associated comments of the code (a consistent update), while at other times they do not (an inconsistent update). Similarly, developers sometimes only update a comment without its associated code (an inconsistent update). The relationship of such comment update practices and software bugs has never been explored empirically. While some (in)consistent updates might be harmless, software engineering folklore warns of the risks of inconsistent updates between code and comments, because these updates are likely to lead to out-of-date comments, which in turn might mislead developers and cause the introduction of bugs in the future. In this paper, we study comment update practices in three large open-source systems written in C (FreeBSD and PostgreSQL) and Java (Eclipse). We find that these practices can better explain and predict future\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "49\n", "authors": ["60"]}
{"title": "Automatic comparison of load tests to support the performance analysis of large enterprise systems\n", "abstract": " Load testing is crucial to uncover functional and performance bugs in large-scale systems. Load tests generate vast amounts of performance data, which needs to be compared and analyzed in limited time across tests. This helps performance analysts to understand the resource usage of an application and to find out if an application is meeting its performance goals. The biggest challenge for performance analysts is to identify the few important performance counters in the highly redundant performance data. In this paper, we employed a statistical technique, Principal Component Analysis (PCA) to reduce the large volume of performance counter data, to a smaller, more meaningful and manageable set. Furthermore, our methodology automates the process of comparing the important counters across load tests to identify performance gains/losses. A case study on load test data of a large enterprise application\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "48\n", "authors": ["60"]}
{"title": "Continuously mining distributed version control systems: an empirical study of how Linux uses Git\n", "abstract": " Distributed version control systems (D-VCSs \u0393\u00c7\u00f6such as git and mercurial) and their hosting services (such as Github and Bitbucket) have revolutionalized the way in which developers collaborate by allowing them to freely exchange and integrate code changes in a peer-to-peer fashion. However, this flexibility comes at a price: code changes are hard to track because of the proliferation of code repositories and because developers modify (\u0393\u00c7\u00a3rebase\u0393\u00c7\u00a5) and filter (\u0393\u00c7\u00a3cherry-pick\u0393\u00c7\u00a5) the history of these changes to streamline their integration into the repositories of other developers. As a consequence, researchers and practitioners, who typically only consider the (cleaned up) history in the official project repository, are unaware of important elements and activities in the collaborative software development process. In this paper, we present a method that continuously mines all known D-VCSs of a software project to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "47\n", "authors": ["60"]}
{"title": "Reproducing context-sensitive crashes of mobile apps using crowdsourced monitoring\n", "abstract": " While the number of mobile apps published by app stores keeps on increasing, the quality of these apps varies widely. Unfortunately, for many apps, end-users continue experiencing bugs and crashes once installed on their mobile device. Crashes are annoying for end-users, but they denitely are for app developers who need to reproduce the crashes as fast as possible beforefinding the root cause of the reported issues. Given the heterogeneity in hardware, mobile platform releases, and types of users, the reproduction step currently is one of the major challenges for app developers. This paper introduces MoTiF, a crowdsourced approach to support app developers in automatically reproducing context-sensitive crashes faced by end-users in the wild. In particular, by analyzing recurrent patterns in crash data, the shortest sequence of events reproducing a crash is derived, and turned into a test suite. We evaluate\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "46\n", "authors": ["60"]}
{"title": "A large-scale empirical study of the relationship between build technology and build maintenance\n", "abstract": " Build systems specify how source code is translated into deliverables. They require continual maintenance as the system they build evolves. This build maintenance can become so burdensome that projects switch build technologies, potentially having to rewrite thousands of lines of build code. We aim to understand the prevalence of different build technologies and the relationship between build technology and build maintenance by analyzing version histories in a corpus of 177,039 repositories spread across four software forges, three software ecosystems, and four large-scale projects. We study low-level, abstraction-based, and framework-driven build technologies, as well as tools that automatically manage external dependencies. We find that modern, framework-driven build technologies need to be maintained more often and these build changes are more tightly coupled with the source code than low\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "45\n", "authors": ["60"]}
{"title": "An exploratory qualitative and quantitative analysis of emotions in issue report comments of open source systems\n", "abstract": " Software development\u0393\u00c7\u00f6just like any other human collaboration\u0393\u00c7\u00f6inevitably evokes emotions like joy or sadness, which are known to affect the group dynamics within a team. Today, little is known about those individual emotions and whether they can be discerned at all in the development artifacts produced during a project. This paper analyzes (a) whether issue reports\u0393\u00c7\u00f6a common development artifact, rich in content\u0393\u00c7\u00f6convey emotional information and (b) whether humans agree on the presence of these emotions. From the analysis of the issue comments of 117 projects of the Apache Software Foundation, we find that developers express emotions (in particular gratitude, joy and sadness). However, the more context is provided about an issue report, the more human raters start to doubt and nuance their interpretation. Based on these results, we demonstrate the feasibility of a machine learning classifier\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "39\n", "authors": ["60"]}
{"title": "On ad library updates in Android apps\n", "abstract": " With more than 90% of mobile apps today being free-to-download, advertisement within apps is one of the key business models to generate revenue. Advertisements are served through the embedding of specialized code, ie, ad libraries. Unlike other types of libraries, developers cannot ignore new versions of the embedded ad libraries or new ad libraries without risking a loss in revenue. However, updating ad libraries also has expenses, which can become a major problem as ad library updates are becoming more prevalent in mobile apps.Hence in this paper, we first discuss the various expenses involved in updating ad libraries, then empirically explore the prevalence of \u0393\u00c7\u00a3ad library updates\u0393\u00c7\u00a5 within Android apps. An analysis of 13,983 versions of 5,937 Android apps collected over 12 months shows that almost half (48.98%) of the studied versions had an ad library update (ie, ad library was added, removed, or updated). Interestingly, in 13.75% of app updates (new version in the Google Play store) with at least one case of ad library update, we found no changes to the app\u0393\u00c7\u00d6s own API, which suggests substantial additional effort for developers to maintain ad libraries. We also explore the rationales for why such updates are carried out.", "num_citations": "39\n", "authors": ["60"]}
{"title": "An experience report on scaling tools for mining software repositories using mapreduce\n", "abstract": " The need for automated software engineering tools and techniques continues to grow as the size and complexity of studied systems and analysis techniques increase. Software engineering researchers often scale their analysis techniques using specialized one-off solutions, expensive infrastructures, or heuristic techniques (eg, search-based approaches). However, such efforts are not reusable and are often costly to maintain. The need for scalable analysis is very prominent in the Mining Software Repositories (MSR) field, which specializes in the automated recovery and analysis of large data stored in software repositories. In this paper, we explore the scaling of automated software engineering analysis techniques by reusing scalable analysis platforms from the web field. We use three representative case studies from the MSR field to analyze the potential of the MapReduce platform to scale MSR tools with\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "38\n", "authors": ["60"]}
{"title": "Is lines of code a good measure of effort in effort-aware models?\n", "abstract": " ContextEffort-aware models, e.g., effort-aware bug prediction models aim to help practitioners identify and prioritize buggy software locations according to the effort involved with fixing the bugs. Since the effort of current bugs is not yet known and the effort of past bugs is typically not explicitly recorded, effort-aware bug prediction models are forced to use approximations, such as the number of lines of code (LOC) of the predicted files.ObjectiveAlthough the choice of these approximations is critical for the performance of the prediction models, there is no empirical evidence on whether LOC is actually a good approximation. Therefore, in this paper, we investigate the question: is LOC a good measure of effort for use in effort-aware models?MethodWe perform an empirical study on four open source projects, for which we obtain explicitly-recorded effort data, and compare the use of LOC to various complexity, size and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "37\n", "authors": ["60"]}
{"title": "An empirical study of integration activities in distributions of open source software\n", "abstract": " Reuse of software components, either closed or open source, is considered to be one of the most important best practices in software engineering, since it reduces development cost and improves software quality. However, since reused components are (by definition) generic, they need to be customized and integrated into a specific system before they can be useful. Since this integration is system-specific, the integration effort is non-negligible and increases maintenance costs, especially if more than one component needs to be integrated. This paper performs an empirical study of multi-component integration in the context of three successful open source distributions (Debian, Ubuntu and FreeBSD). Such distributions integrate thousands of open source components with an operating system kernel to deliver a coherent software product to millions of users worldwide. We empirically identified seven major\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "36\n", "authors": ["60"]}
{"title": "An empirical study of build system migrations in practice: Case studies on kde and the linux kernel\n", "abstract": " As the build system, i.e. the infrastructure that constructs executable deliverables out of source code and other resources, tries to catch up with the ever-evolving source code base, its size and already significant complexity keep on growing. Recently, this has forced some major software projects to migrate their build systems towards more powerful build system technologies. Since at all times software developers, testers and QA personnel rely on a functional build system to do their job, a build system migration is a risky and possibly costly undertaking, yet no methodology, nor best practices have been devised for it. In order to understand the build system migration process, we empirically studied two failed and two successful attempts of build system migration in two major open source projects, i.e. Linux and KDE, by mining source code repositories and tens of thousands of developer mailing list messages. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "36\n", "authors": ["60"]}
{"title": "Who needs release and devops engineers, and why?\n", "abstract": " The recent surge in interest in continuous delivery has opened up the job market for release and DevOps engineers. However, despite an increasing number of conferences and publications on continuous delivery, smaller companies and start-ups still have a hard time determining the core tasks their future release and DevOps engineers should be responsible for (and what the differences between those two roles are), while universities are not sure what essential techniques and skills they should teach to their students. This paper performs an empirical analysis of online job postings to determine and compare the main tasks of release and DevOps engineers, globally and across countries. Our qualitative analysis shows that automation is the most important activity across the three roles, as articulated in job posting description data, and that the release engineer role combines the top activities of the DevOps and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "34\n", "authors": ["60"]}
{"title": "Pinpointing the subsystems responsible for the performance deviations in a load test\n", "abstract": " Large scale systems (LSS) contain multiple subsystems that interact across multiple nodes in sometimes unforeseen and complicated ways. As a result, pinpointing the subsystems that are the source of performance degradation for a load test in LSS can be frustrating, and might take several hours or even days. This is due to the large volume of performance counter data collected such as CPU utilization, Disk I/O, memory consumption and network traffic, the limited operational knowledge of analysts about all subsystems of an LSS and the unavailability of up-to-date documentation in a LSS. We have developed a methodology that automatically ranks the subsystems according to the deviation of their performance in a load test. Our methodology uses performance counter data of a load test to craft performance signatures for the LSS subsystems. Pair-wise correlations among the performance signatures of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "34\n", "authors": ["60"]}
{"title": "Software configuration engineering in practice interviews, survey, and systematic literature review\n", "abstract": " Modern software applications are adapted to different situations (e.g., memory limits, enabling/disabling features, database credentials) by changing the values of configuration options, without any source code modifications. According to several studies, this flexibility is expensive as configuration failures represent one of the most common types of software failures. They are also hard to debug and resolve as they require a lot of effort to detect which options are misconfigured among a large number of configuration options and values, while comprehension of the code also is hampered by sprinkling conditional checks of the values of configuration options. Although researchers have proposed various approaches to help debug or prevent configuration failures, especially from the end users' perspective, this paper takes a step back to understand the process required by practitioners to engineer the run-time\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["60"]}
{"title": "Supplementary bug fixes vs. re-opened bugs\n", "abstract": " A typical bug fixing cycle involves the reporting of a bug, the triaging of the report, the production and verification of a fix, and the closing of the bug. However, previous work has studied two phenomena where more than one fix are associated with the same bug report. The first one is the case where developers re-open a previously fixed bug in the bug repository (sometimes even multiple times) to provide a new bug fix that replace a previous fix, whereas the second one is the case where multiple commits in the version control system contribute to the same bug report (\"supplementary bug fixes\"). Even though both phenomena seem related, they have never been studied together, i.e., are supplementary fixes a subset of re-opened bugs or the other way around? This paper investigates the interplay between both phenomena in five open source software projects: Mozilla, Net beans, Eclipse JDT Core, Eclipse\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["60"]}
{"title": "Analyzing ad library updates in android apps\n", "abstract": " Because more than 90 percent of mobile apps are free, advertising on them is a key revenue source for their developers. Advertisements are served on apps through embedded specialized code called ad libraries. Unlike with other types of libraries, app developers can't ignore new ad libraries or new versions of embedded ad libraries without risking revenue loss. However, updating ad libraries incurs costs, which can become problematic as these updates become more frequent. Researchers investigated the costs of updating ad libraries and explored the frequency of ad library updates in Android apps. An analysis of numerous versions of Android apps over 12 months showed that almost half underwent ad library updates (an ad library was added, removed, or updated). Moreover, in nearly 14 percent of the app updates with at least one ad library update, no changes to the app's API occurred. This suggests that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["60"]}
{"title": "Using Pig as a data preparation language for large-scale mining software repositories studies: An experience report\n", "abstract": " The Mining Software Repositories (MSR) field analyzes software repository data to uncover knowledge and assist development of ever growing, complex systems. However, existing approaches and platforms for MSR analysis face many challenges when performing large-scale MSR studies. Such approaches and platforms rarely scale easily out of the box. Instead, they often require custom scaling tricks and designs that are costly to maintain and that are not reusable for other types of analysis. We believe that the web community has faced many of these software engineering scaling challenges before, as web analyses have to cope with the enormous growth of web data. In this paper, we report on our experience in using a web-scale platform (i.e., Pig) as a data preparation language to aid large-scale MSR studies. Through three case studies, we carefully validate the use of this web platform to prepare (i.e\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "28\n", "authors": ["60"]}
{"title": "A lightweight approach to uncover technical artifacts in unstructured data\n", "abstract": " Developer communication through email, chat, or issue report comments consists mostly of largely unstructured data, i.e., natural language text, mixed with technical artifacts such as project-specific jargon, abbreviations, source code patches, stack traces and identifiers. These technical artifacts represent a valuable source of knowledge on the technical part of the system, with a wide range of applications from establishing traceability links to creating project-specific vocabularies. However, the lack of well-defined boundaries between natural language and technical content make the automated mining of technical artifacts challenging. As a first step towards a general-purpose technique to extracting technical artifacts from unstructured data, we present a lightweight approach to untangle technical artifacts and natural language text. Our approach is based on existing spell checking tools, which are well-understood\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "28\n", "authors": ["60"]}
{"title": "Automated verification of load tests using control charts\n", "abstract": " Load testing is an important phase in the software development process. It is very time consuming but there is usually little time for it. As a solution to the tight testing schedule, software companies automate their testing procedures. However, existing automation only reduces the time required to run load tests. The analysis of the test results is still performed manually. A typical load test outputs thousands of performance counters. Analyzing these counters manually requires time and tacit knowledge of the system-under-test from the performance engineers. The goal of this study is to derive an approach to automatically verify load tests' results. We propose an approach based on a statistical quality control technique called control charts. Our approach can a) automatically determine if a test run passes or fails and b) identify the subsystem where performance problem originated. We conduct two case studies on a large\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["60"]}
{"title": "An empirical study of unspecified dependencies in make-based build systems\n", "abstract": " Software developers rely on a build system to compile their source code changes and produce deliverables for testing and deployment. Since the full build of large software systems can take hours, the incremental build is a cornerstone of modern build systems. Incremental builds should only recompile deliverables whose dependencies have been changed by a developer. However, in many organizations, such dependencies still are identified by build rules that are specified and maintained (mostly) manually, typically using technologies like make. Incomplete rules lead to unspecified dependencies that can prevent certain deliverables from being rebuilt, yielding incomplete results, which leave sources and deliverables out-of-sync. In this paper, we present a case study on unspecified dependencies in the make-based build systems of the glib, openldap, linux and qt open source projects. To uncover\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["60"]}
{"title": "Identifying performance deviations in thread pools\n", "abstract": " Large-scale software systems handle increasingly larger workloads by implementing highly concurrent and distributed design patterns. The thread pool pattern uses pools of pre-existing and reusable threads to limit thread lifecycle over-head (thread creation and destruction) and resource thrashing (thread proliferation). However, these advantages are weighed against performance issues caused by concurrency risks, like synchronization errors or deadlock, and thread pool-specific risks, like poorly tuned pool size or thread leakage. Detecting these performance issues during load testing requires a thorough understanding of how thread pools behave, yet most performance analysts have limited knowledge of the system and are flooded with terabytes of data from load tests. We propose a methodology to identify threads with performance deviations in thread pools. Our methodology ranks threads based on the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["60"]}
{"title": "Aspect orientation for C: Express yourself\n", "abstract": " In this paper, we propose Aspicere, our effort to bring aspectoriented software development to the C programming language. We focus primarily on the pointcut language offered by Aspicere, as it differs significantly from other aspect lan-guages, for C as well as for Java. We illustrate the need for a more complex pointcut language, by means of a simple and prototypical concern, present in a real-world application, that can not be captured adequately with current-day... aspect technologies.", "num_citations": "26\n", "authors": ["60"]}
{"title": "On cross-stack configuration errors\n", "abstract": " Today's web applications are deployed on powerful software stacks such as MEAN (JavaScript) or LAMP (PHP), which consist of multiple layers such as an operating system, web server, database, execution engine and application framework, each of which provide resources to the layer just above it. These powerful software stacks unfortunately are plagued by so-called cross-stack configuration errors (CsCEs), where a higher layer in the stack suddenly starts to behave incorrectly or even crash due to incorrect configuration choices in lower layers. Due to differences in programming languages and lack of explicit links between configuration options of different layers, sysadmins and developers have a hard time identifying the cause of a CsCE, which is why this paper (1) performs a qualitative analysis of 1,082 configuration errors to understand the impact, effort and complexity of dealing with CsCEs, then (2\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["60"]}
{"title": "The impact of human discussions on just-in-time quality assurance: An empirical study on openstack and eclipse\n", "abstract": " In order to spot defect-introducing code changes during review before they are integrated into a project's version control system, a variety of defect prediction models have been designed. Most of these models focus exclusively on source code properties, like the number of added or deleted lines, or developer-related measures like experience. However, a code change is only the outcome of a much longer process, involving discussions on an issue report and review discussions on (different versions of) a patch. % Ignoring the characteristics of these activities during prediction is unfortunate, since Similar to how body language implicitly can reveal a person's real feelings, the length, intensity or positivity of these discussions can provide important additional clues about how risky a particular patch is or how confident developers and reviewers are about the patch. In this paper, we build logistic regression models to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["60"]}
{"title": "Replicating and re-evaluating the theory of relative defect-proneness\n", "abstract": " A good understanding of the factors impacting defects in software systems is essential for software practitioners, because it helps them prioritize quality improvement efforts (e.g., testing and code reviews). Defect prediction models are typically built using classification or regression analysis on product and/or process metrics collected at a single point in time (e.g., a release date). However, current defect prediction models only predict if a defect will occur, but not when, which makes the prioritization of software quality improvements efforts difficult. To address this problem, Koru et al. applied survival analysis techniques to a large number of software systems to study how size (i.e., lines of code) influences the probability that a source code module (e.g., class or file) will experience a defect at any given time. Given that 1) the work of Koru et al. has been instrumental to our understanding of the size-defect relationship, 2\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["60"]}
{"title": "Prioritizing the creation of unit tests in legacy software systems\n", "abstract": " Test\u0393\u00c7\u00c9driven development (TDD) is a software development practice that prescribes writing unit tests before writing implementation code. Recent studies have shown that TDD practices can significantly reduce the number of pre\u0393\u00c7\u00c9release defects. However, most TDD research thus far has focused on new development. We investigate the adaptation of TDD\u0393\u00c7\u00c9like practices for already\u0393\u00c7\u00c9implemented code, in particular legacy systems. We call such an adaptation \u0393\u00c7\u00ffTest\u0393\u00c7\u00c9driven maintenance\u0393\u00c7\u00d6 (TDM). In this paper, we present a TDM approach that assists software development and testing managers to use the limited resources they have for testing legacy systems efficiently. The approach leverages the development history of a project to generate a prioritized list of functions that managers should focus their unit test writing resources on. The list is updated dynamically as the development of the legacy system progresses. We\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["60"]}
{"title": "Using load tests to automatically compare the subsystems of a large enterprise system\n", "abstract": " Enterprise systems are load tested for every added feature, software updates and periodic maintenance to ensure that the performance demands on system quality, availability and responsiveness are met. In current practice, performance analysts manually analyze load test data to identify the components that are responsible for performance deviations. This process is time consuming and error prone due to the large volume of performance counter data collected during monitoring, the limited operational knowledge of analyst about all the subsystem involved and their complex interactions and the unavailability of up-to-date documentation in the rapidly evolving enterprise. In this paper, we present an automated approach based on a robust statistical technique, Principal Component Analysis (PCA) to identify subsystems that show performance deviations in load tests. A case study on load test data of a large\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["60"]}
{"title": "Mining test repositories for automatic detection of UI performance regressions in Android apps\n", "abstract": " The reputation of a mobile app vendor is crucial to survive amongst the ever increasing competition. However this reputation largely depends on the quality of the apps, both functional and non-functional. One major non-functional requirement of mobile apps is to guarantee smooth UI interactions, since choppy scrolling or navigation caused by performance problems on a mobile device's limited hardware resources, is highly annoying for end-users. The main research challenge of automatically identifying UI performance problems on mobile devices is that the performance of an app highly varies depending on its context---ie, the hardware and software configurations on which it runs.", "num_citations": "19\n", "authors": ["60"]}
{"title": "Co-Evolution of source code and the build system: impact on the introduction of AOSD in legacy systems\n", "abstract": " Software is omnipresent in our daily lives. As users demand ever more advanced features, software systems have to keep on evolving. In practice, this means that software developers need to adapt the description of a software application. Such a description not only consists of source code written down in a programming language, as a lot of knowledge is hidden in lesser known software development artifacts, like the build system. As its name suggests, the build system is responsible for building an executable program, ready for use, from the source code. There are various indications that the evolution of source code is strongly related to that of the build system. When the source code changes, the build system has to co-evolve to safeguard the ability to build an executable program. A rigid build system on the other hand limits software developers. This phenomenon especially surfaces when drastic changes in the source code are coupled with an inflexible build system, as is the case for the introduction of AOSD technology in legacy systems. AOSD is a young software development approach which enables developers to structure and compose source code in a better way. Legacy systems are old software systems which are still mission-critical, but of which the source code and the build system are no longer fully understood, and which typically make use of old(-fashioned) technology. This PhD dissertation focuses on finding an explanation for this co-evolution of source code and the build system, and on finding developer support to grasp and manage this phenomenon. We postulate four \"roots of co-evolution\" which represent four different\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["60"]}
{"title": "Measuring the progress of projects using the time dependence of code changes\n", "abstract": " Tracking the progress of a project is often done through imprecise manually gathered information, like progress reports, or through automatic metrics such as Lines Of Code (LOC). Such metrics are too coarse-grained and too imprecise to capture all facets of a project. In this paper, we mine the code changes in the source code repository and study the concept of time dependence of code changes. Using this concept, we can track the progress of a software project as the progress of a building. We can examine how changes build on each other over time and determine the impact of these changes on the quality of a project. In particular, we study whether new changes are built just-in-time or if they build on older, stable code. Through a case study on two large open source projects (PostgreSQL and FreeBSD), we show that time dependence varies across projects and throughout the lifetime of each project. We also\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "18\n", "authors": ["60"]}
{"title": "Disentangling virtual machine architecture\n", "abstract": " Virtual machine (VM) implementations are made of intricately intertwined subsystems, interacting largely through implicit dependencies. As the degree of crosscutting present in VMs is very high, VM implementations exhibit significant internal complexity. This study proposes an architecture approach for VMs that regards a VM as a composite of service modules coordinated through explicit bidirectional interfaces. Aspect-oriented programming techniques are used to establish these interfaces, to coordinate module interaction, and to declaratively express concrete VM architectures. A VM architecture description language is presented in a case study, illustrating the application of the proposed architectural principles.", "num_citations": "18\n", "authors": ["60"]}
{"title": "The impact of cross-distribution bug duplicates, empirical study on Debian and Ubuntu\n", "abstract": " Although open source distributions like Debian and Ubuntu are closely related, sometimes a bug reported in the Debian bug repository is reported independently in the Ubuntu repository as well, without the Ubuntu users nor developers being aware. Such cases of undetected cross-distribution bug duplicates can cause developers and users to lose precious time working on a fix that already exists or to work individually instead of collaborating to find a fix faster. We perform a case study on Ubuntu and Debian bug repositories to measure the amount of cross-distribution bug duplicates and estimate the amount of time lost. By adapting an existing within-project duplicate detection approach (achieving a similar recall of 60%), we find 821 cross-duplicates. The early detection of such duplicates could reduce the time lost by users waiting for a fix by a median of 38 days. Furthermore, we estimate that developers from the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["60"]}
{"title": "Make it simple-an empirical analysis of gnu make feature use in open source projects\n", "abstract": " Make is one of the oldest build technologies and is still widely used today, whether by manually writing Make files, or by generating them using tools like Auto tools and CMake. Despite its conceptual simplicity, modern Make implementations such as GNU Make have become very complex languages, featuring functions, macros, lazy variable assignments and (in GNU Make 4.0) the Guile embedded scripting language. Since we are interested in understanding how widespread such complex language features are, this paper studies the use of Make features in almost 20,000 Make files, comprised of over 8.4 million lines, from more than 350 different open source projects. We look at the popularity of features and the difference between hand-written Make files and those generated using various tools. We find that generated Make files use only a core set of features and that more advanced features (such as function\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["60"]}
{"title": "Using aspect orientation in legacy environments for reverse engineering using dynamic analysis\u0393\u00c7\u00f6An industrial experience report\n", "abstract": " This paper reports on the challenges of using aspect-oriented programming (AOP) to aid in re-engineering a legacy C application. More specifically, we describe how AOP helps in the important reverse engineering step which typically precedes a re-engineering effort. We first present a comparison of the available AOP tools for legacy C code bases, and then argue on our choice of Aspicere, our own AOP implementation for C. Then, we report on Aspicere\u0393\u00c7\u00d6s application in reverse engineering a legacy industrial software system and we show how we apply a dynamic analysis to regain insight into the system. AOP is used for instrumenting the system and for gathering the data. This approach works and is conceptually very clean, but comes with a major quid pro quo: integration of AOP tools with the build system proves an important issue. This leads to the question of how to reconcile the notion of modular reasoning\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["60"]}
{"title": "Aspect mining in the presence of the C preprocessor\n", "abstract": " In systems software, the C preprocessor is heavily used to manage variability and improve efficiency. It is the primary tool to model crosscutting concerns in a very fine-grained way, but leads to extremely tangled and scattered preprocessor code. In this paper, we explore the process of aspect mining and extraction in the context of preprocessor-driven systems. Our aim is to identify both opportunities (extracting conditional compilation into advice) and pitfalls (mining on unpreprocessed code) in migrating preprocessor code to aspects. We distill five trade-offs which give a first impression about the usefulness of replacing the preprocessor by aspects. Preprocessor-driven systems prove to be a real challenge for aspect mining, but they could become on the other hand one of the most promising applications of AOP.", "num_citations": "16\n", "authors": ["60"]}
{"title": "A qualitative study of the benefits and costs of logging from developers' perspectives\n", "abstract": " Software developers insert logging statements in their source code to collect important runtime information of software systems. In practice, logging appropriately is a challenge for developers. Prior studies aimed to improve logging by proactively inserting logging statements in certain code snippets or by learning where to log from existing logging code. However, there exists no work that systematically studies developers' logging considerations, i.e., the benefits and costs of logging from developers' perspectives. Without understanding developers' logging considerations, automated approaches for logging decisions are based primarily on researchers' intuition which may not be convincing to developers. In order to fill the gap between developers' logging considerations and researchers' intuition, we performed a qualitative study that combines a survey of 66 developers and a case study of 223 logging-related issue\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["60"]}
{"title": "The open-closed principle of modern machine learning frameworks\n", "abstract": " Recent advances in computing technologies and the availability of huge volumes of data have sparked a new machine learning (ML) revolution, where almost every day a new headline touts the demise of human experts by ML models on some task. Open source software development is rumoured to play a significant role in this revolution, with both academics and large corporations such as Google and Microsoft releasing their ML frameworks under an open source license. This paper takes a step back to examine and understand the role of open source development in modern ML, by examining the growth of the open source ML ecosystem on GitHub, its actors, and the adoption of frameworks over time. By mining LinkedIn and Google Scholar profiles, we also examine driving factors behind this growth (paid vs. voluntary contributors), as well as the major players who promote its democratization (companies vs\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["60"]}
{"title": "Broadcast vs. unicast review technology: Does it matter?\n", "abstract": " Code review is the process of having other team members examine changes to a software system in order to evaluate their technical content and quality. Over the years, multiple tools have been proposed to help software developers conduct and manage code reviews. Some software organizations have been migrating from broadcast review technology to a more advanced unicast review approach such as Jira, but it is unclear if these unicast review technology leads to better code reviews. This paper empirically studies review data of five Apache projects that switched from broadcast based code review to unicast based, to understand the impact of review technology on review effectiveness and quality. Results suggest that broadcast based review is twice faster than review done with unicast based review technology. However, unicast's review quality seems to be better than that of the broadcast based. Our findings\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["60"]}
{"title": "Recovering a balanced overview of topics in a software domain\n", "abstract": " Domain analysis is a crucial step in the development of product lines and software reuse in general, in which domain experts try to identify the commonalities and variability between different products of a particular domain. This identification is challenging, since it requires significant manual analysis of requirements, design documents, and source code. In order to support domain analysts, this paper proposes to use topic modeling techniques to automatically identify common and unique concepts (topics) from the source code of different software products in a domain. An empirical case study of 19 projects, spread across the domains of web browsers and operating systems (totaling over 39 MLOC), shows that our approach is able to identify commonalities and variabilities at different levels of granularity (sub-domain and domain). In addition, we show how the commonalities are evenly spread across all projects of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["60"]}
{"title": "Co-evolution of source code and the build system\n", "abstract": " A build system breathes life into source code, as it configures and directs the construction of a software system from textual source code modules. Surprisingly, build languages and tools have not received considerable attention by academics and practitioners, making current build systems a mysterious and frustrating resource to work with. Our dissertation presents a conceptual framework with tool support to recover, analyze and refactor a build system. We demonstrate the applicability of our framework by analyzing the evolution of the Linux kernel build system and the introduction of AOSD technology in five legacy build systems. In all cases, we found that the build system is a complex software system of its own, trying to co-evolve in a synchronized way with the source code while working around shortcomings of the underlying build technology. Based on our findings, we hypothesize four conceptual reasons of co\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["60"]}
{"title": "Feature location using crowd-based screencasts\n", "abstract": " Crowd-based multi-media documents such as screencasts have emerged as a source for documenting requirements of agile software projects. For example, screencasts can describe buggy scenarios of a software product, or present new features in an upcoming release. Unfortunately, the binary format of videos makes traceability between the video content and other related software artifacts (eg, source code, bug reports) difficult. In this paper, we propose an LDA-based feature location approach that takes as input a set of screencasts (ie, the GUI text and/or spoken words) to establish traceability link between the features described in the screencasts and source code fragments implementing them. We report on a case study conducted on 10 WordPress screencasts, to evaluate the applicability of our approach in linking these screencasts to their relevant source code artifacts. We find that the approach is able to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["60"]}
{"title": "Experience report: An empirical study of API failures in OpenStack cloud environments\n", "abstract": " Stories about service outages in cloud environments have been making the headlines recently. In many cases, the reliability of cloud infrastructure Application Programming Interfaces (APIs) were at fault. Hence, understanding the factors affecting the reliability of these APIs is important to improve the availability of cloud services. In this study, we mined bugs of 25 modules within the 5 most important OpenStack APIs to understand API failures and characteristics. Our results show that in OpenStack, only one third of all API-related changes are due to fixing failures, with 7% of all fixes even changing the API interface, potentially breaking clients. Through qualitative analysis of 230 sampled API failures we observed that the majority of API related failures are due to small programming faults. Fortunately, the subject, message and stack trace as well as reply lag between comments included in these failures' bug reports\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["60"]}
{"title": "Botched releases: Do we need to roll back? Empirical study on a commercial web app\n", "abstract": " Few minutes after a web-based software release, the release team might encounter log traces showing the new system crashing, hanging, or having poor performance. This is the start of the most nerve-wrecking moments of a product's release cycle, i.e., should one run the risk of not doing anything and users losing precious data, or of prematurely engaging the tedious (and costly) roll-back procedure towards the previous release? Thus far, only little attention has been paid by researchers to these so-called \"botched releases\", partly because of lack of release log data. This paper studies 345 releases of a large e-commerce web app over a period of 1.5 years, in which we identified 17 recurrent root causes of botched releases, classified into four major categories. We then build explanatory models to understand which root causes are the most important, and to explore the factors leading to botched releases.", "num_citations": "13\n", "authors": ["60"]}
{"title": "Collecting and leveraging a benchmark of build system clones to aid in quality assessments\n", "abstract": " Build systems specify how sources are transformed into deliverables, and hence must be carefully maintained to ensure that deliverables are assembled correctly. Similar to source code, build systems tend to grow in complexity unless specifications are refactored. This paper describes how clone detection can aid in quality assessments that determine if and where build refactoring effort should be applied. We gauge cloning rates in build systems by collecting and analyzing a benchmark comprising 3,872 build systems. Analysis of the benchmark reveals that:(1) build systems tend to have higher cloning rates than other software artifacts,(2) recent build technologies tend to be more prone to cloning, especially of configuration details like API dependencies, than older technologies, and (3) build systems that have fewer clones achieve higher levels of reuse via mechanisms not offered by build technologies. Our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["60"]}
{"title": "Multi-layer software configuration: Empirical study on wordpress\n", "abstract": " Software can be adapted to different situations and platforms by changing its configuration. However, incorrect configurations can lead to configuration errors that are hard to resolve or understand, especially in the case of multi-layer architectures, where configuration options in each layer might contradict each other or be hard to trace to each other. Hence, this paper performs an empirical study on the occurrence of multi-layer configuration options across Wordpress (WP) plugins, WP, and the PHP engine. Our analyses show that WP and its plugins use on average 76 configuration options, a number that increases across time. We also find that each plugin uses on average 1.49% to 9.49% of all WP database options, and 1.38% to 15.18% of all WP configurable constants. 85.16% of all WP database options, 78.88% of all WP configurable constants, and 52 PHP configuration options are used by at least two plugins\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["60"]}
{"title": "Prioritizing unit test creation for test-driven maintenance of legacy systems\n", "abstract": " Test-Driven Development (TDD) is a software development practice that prescribes writing unit tests before writing implementation code. Recent studies have shown that TDD practices can significantly reduce the number of pre-release defects. However, most TDD research thus far has focused on new development. We investigate the adaptation of TDD-like practices for already implemented code, in particular legacy systems. We call this adaptation of TDD-like practices for already implemented code ``Test-Driven Maintenance'' (TDM). In this paper, we present an approach that assists software development and testing managers, who employ TDM, utilize the limited resources they have for testing legacy systems efficiently. The approach leverages the development history of the project to generate a prioritized list of functions that managers should focus their unit test writing resources on. The list is updated\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["60"]}
{"title": "A study of the time dependence of code changes\n", "abstract": " Much of modern software development consists of building on older changes. Older periods provide the structure (e.g., functions and data types) on which changes in future periods will build. Given a particular period in the lifetime of a project, one can determine prior periods on which it builds, and future periods which build on it. Using this knowledge, managers can identify foundational periods in the lifetime of a project, which provide the structural foundation for a large number of future periods. A good understanding and detailed documentation of events and decisions in such foundational periods is essential for the smooth evolution of a project. This paper examines how changes build on older changes by measuring the time dependence between code changes. Using our approach, we can create time dependence relations between periods and study the characteristics of such dependence relations. We apply\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["60"]}
{"title": "Aspect-orientation for revitalising legacy business software\n", "abstract": " This paper relates on a first attempt to see if aspect-oriented programming (AOP) and logic meta-programming (LMP) can help with the revitalisation of legacy business software. By means of four realistic case studies covering reverse engineering, restructuring and integration, we discuss the applicability of the aspect-oriented paradigm in the context of two major programming languages for legacy environments: Cobol and C.", "num_citations": "10\n", "authors": ["60"]}
{"title": "Toward solving social and technical problems in open source software ecosystems: using cause-and-effect analysis to disentangle the causes of complex problems\n", "abstract": " Many open source software (OSS) products today are market leaders, 1 which suggests that the development of OSS is key to the growth of the software industry. OSS projects increasingly tend to be incorporated in large-scale projects or \"software ecosystems\" to reduce effort and accelerate innovation.", "num_citations": "9\n", "authors": ["60"]}
{"title": "Does the choice of configuration framework matter for developers? empirical study on 11 java configuration frameworks\n", "abstract": " Configuration frameworks are routinely used in software systems to change application behavior without recompilation. Selecting a suitable configuration framework among the vast variety of existing choices is a crucial decision for developers, as it can impact project reliability and its maintenance profile. In this paper, we analyze almost 2,000 Java projects on GitHub to investigate the features and properties of 11 major Java configuration frameworks. We analyze the popularity of the frameworks and try to identify links between the maintenance effort involved with the usage of these frameworks and the frameworks' properties. More basic frameworks turn out to be the most popular, but in half of the cases are complemented by more complex frameworks. Furthermore, younger, more active frameworks with more detailed documentation, support for hierarchical configuration models and/or more data formats seem to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["60"]}
{"title": "Do Not Trust Build Results at Face Value-An Empirical Study of 30 Million CPAN Builds\n", "abstract": " Continuous Integration (CI) is a cornerstone of modern quality assurance, providing on-demand builds (compilation and tests) of code changes or software releases. Despite the myriad of CI tools and frameworks, the basic activity of interpreting build results is not straightforward, due to not only the number of builds being performed but also, and especially, due to the phenomenon of build inflation, according to which one code change can be built on dozens of different operating systems, run-time environments and hardware architectures. As existing work mostly ignored this inflation, this paper performs a large-scale empirical study of the impact of OS and run-time environment on build failures on 30 million builds of the CPAN ecosystem's CI environment. We observe the evolution of build failures over time, and investigate the impact of OSes and environments on build failures. We show that distributions may fail\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["60"]}
{"title": "Identifying and understanding header file hotspots in c/c++ build processes\n", "abstract": " Software developers rely on a fast build system to incrementally compile their source code changes and produce modified deliverables for testing and deployment. Header files, which tend to trigger slow rebuild processes, are most problematic if they also change frequently during the development process, and hence, need to be rebuilt often. In this paper, we propose an approach that analyzes the build dependency graph (i.e., the data structure used to determine the minimal list of commands that must be executed when a source code file is modified), and the change history of a software system to pinpoint header file hotspots\u0393\u00c7\u00f6header files that change frequently and trigger long rebuild processes. Through a case study on the GLib, PostgreSQL, Qt, and Ruby systems, we show that our approach identifies header file hotspots that, if improved, will provide greater improvement to the total future build cost of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["60"]}
{"title": "A Dataset of the Activity of the git Super-repository of Linux in 2012\n", "abstract": " This dataset documents the activity in the public portion of the git Super-repository of the Linux kernel during 2012. In a distributed version control system, such as git, the Super-repository is the collection of all the repositories (repos) used for development. In such a Super-repository, some repos will be accessible only by their owners (they are private, and are located in places that are unreachable to other users) while others are available to other members of the team. The latter public repositories are used as avenues through which commits flow from one developer to another. During the last six weeks of 2011, we proceeded to automatically discover the public portion of the Super-repository of Linux. Then, in 2012, every 3 hrs, each of these public repositories was queried to see what new commits it had and what commits had disappeared from it using a process we call continuous mining. This resulted in the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["60"]}
{"title": "Makao\n", "abstract": " This demo presents MAKAO (Makefile Architecture Kernel featuring Aspect Orientation), a re(verse)-engineering framework for build systems. At its heart, MAKAO makes the build's dependency graph available in a tangible way. Aside from visualisation, this enables powerful querying of all build-related data, as well as various filtering techniques to define views on the build architecture. If desired, all this gathered information can be put to use to write aspects for refactoring the build. Afterwards, validation rules can help in assessing failure or success.", "num_citations": "9\n", "authors": ["60"]}
{"title": "cregit: Token-level blame information in git version control repositories\n", "abstract": " The blame feature of version control systems is widely used\u0393\u00c7\u00f6both by practitioners and researchers\u0393\u00c7\u00f6to determine who has last modified a given line of code, and the commit where this contribution was made. The main disadvantage of blame is that, when a line is modified several times, it only shows the last commit that modified it\u0393\u00c7\u00f6occluding previous changes to other areas of the same line. In this paper, we developed a method to increase the granularity of blame in git: instead of tracking lines of code, this method is capable of tracking tokens in source code. We evaluate its effectiveness with an empirical study in which we compare the accuracy of blame in git (per line) with our proposed blame-per-token method. We demonstrate that, in 5 large open source systems, blame-per-token is capable of properly identifying the commit that introduced a token with an accuracy between 94.5% and 99.2%, while\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["60"]}
{"title": "On mining crowd-based speech documentation\n", "abstract": " Despite the globalization of software development, relevant documentation of a project, such as requirements and design documents, often still is missing, incomplete or outdated. However, parts of that documentation can be found outside the project, where it is fragmented across hundreds of textual web documents like blog posts, email messages and forum posts, as well as multimedia documents such as screencasts and podcasts. Since dissecting and filtering multimedia information based on its relevancy to a given project is an inherently difficult task, it is necessary to provide an automated approach for mining this crowd-based documentation. In this paper, we are interested in mining the speech part of YouTube screencasts, since this part typically contains the rationale and insights of a screencast. We introduce a methodology that transcribes and analyzes the transcribed text using various Information\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["60"]}
{"title": "Inherent characteristics of traceability artifacts less is more\n", "abstract": " This paper describes ongoing work to characterize the inherent ease or \u0393\u00c7\u00a3traceability\u0393\u00c7\u00a5 with which a textual artifact can be traced using an automated technique. Software traceability approaches use varied measures to build models that automatically recover links between pairs of natural language documents. Thus far, most of the approaches use a single-step model, such as logistic regression, to identify new trace links. However, such approaches require a large enough training set of both true and false trace links. Yet, the former are by far in the minority, which reduces the performance of such models. Therefore, this paper formulates the problem of identifying trace links as the problem of finding, for a given logistic regression model, the subsets of links in the training set giving the best accuracy (in terms of G-metric) on a test set. Using hill climbing with random restart for subset selection, we found that, for the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["60"]}
{"title": "An aspect for idiom-based exception handling: (using local continuation join points, join point properties, annotations and type parameters)\n", "abstract": " The last couple of years, various idioms used in the 15 MLOC C code base of ASML, the world's biggest lithography machine manufacturer, have been unmasked as crosscutting concerns. However, finding a scalable aspect-based implementation for them did not succeed thusfar, prohibiting sufficient separation of concerns and introducing possibly dangerous programming mistakes. This paper proposes a concise aspect-based implementation in Aspicere2 for ASML's exception handling idiom, based on prior work of join point properties, annotations and type parameters, to which we add the new concept of (local) continuation join points. Our solution takes care of the error value propagation mechanism (which includes aborting the main success scenario), logging, resource cleanup, and allows for local overrides of the default aspect-based recovery. The highly idiomatic nature of the problem in tandem with the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["60"]}
{"title": "1st international workshop on release engineering (releng 2013)\n", "abstract": " Release engineering deals with all activities in between regular development and actual usage of a software product by the end user, i.e., integration, build, test execution, packaging and delivery of software. Although research on this topic goes back for decades, the increasing heterogeneity and variability of software products along with the recent trend to reduce the release cycle to days or even hours starts to question some of the common beliefs and practices of the field. For example, a project like Mozilla Firefox releases every 6 weeks, generating updates for dozens of existing Firefox versions on 5 desktop, 2 mobile and 3 mobile desktop platforms, each of which for more than 80 locales. In this context, the International Workshop on Release Engineering (RELENG) aims to provide a highly interactive forum for researchers and practitioners to address the challenges of, find solutions for and share experiences\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["60"]}
{"title": "Acre: An automated aspect creator for testing C++ applications\n", "abstract": " We present ACRE, an Automated aspect creator, to use aspect-oriented programming (AOP) to perform memory, invariant and interferences testing for software programs written in C++. ACRE allows developers without knowledge in AOP to use aspects to test their programs without modifying the behavior of their source code. ACRE uses a domain-specific language (DSL), which statements testers insert into the source code like comments to describe the aspects to be used. The presence of DSL statements in the code does not modify the program's compilation and behavior. ACRE parses the DSL statements and automatically generates appropriate aspects that are then weaved into the source code to identify bugs due to memory leaks, incorrect algorithm implementation, or interference among threads. Thanks to the use of aspects and ACRE, testers can add or remove tests easily. Using an aspect generated by\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["60"]}
{"title": "Preserving knowledge in software projects\n", "abstract": " Up-to-date preservation of project knowledge like developer communication and design documents is essential for the successful evolution of software systems. Ideally, all knowledge should be preserved, but since projects only have limited resources, and software systems continuously grow in scope and complexity, one needs to prioritize the subsystems and development periods for which knowledge preservation is more urgent. For example, core subsystems on which the majority of other subsystems build are obviously prime candidates for preservation, yet if these subsystems change continuously, picking a development period to start knowledge preservation and to maintain knowledge for over time become very hard. This paper exploits the time dependence between code changes to automatically determine for which subsystems and development periods of a software project knowledge preservation would\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["60"]}
{"title": "A Large Scale Empirical Study on User-Centric Performance Analysis\n", "abstract": " Measuring the software performance under load is an important task in both test and production of a software development. In large scale systems, a large amount of metrics and usage logs are analyzed to measure the performance of the software. Most of these metrics are analyzed by aggregating across all users to get general results for the scenario, i.e., how individual users have perceived the performance is typically not considered in software performance research and practice. To analyze a software's performance, user's perception of software performance metrics should be considered along with the scenario-centric perspective of system tester or operator. In our empirical study, we analyzed the impact of performance on individual users to see if performance analysis results based on the user's perception is really different from the scenario-centric (aggregated) one. Case studies on common use case\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["60"]}
{"title": "Towards a dsal for object layout in virtual machines\n", "abstract": " High-level language virtual machine implementations offer a challenging domain for modularization, not only because they are inherently complex, but also because efficiency is not likely to be traded for modularity. The central data structure used throughout the VM, the object layout, cannot be succinctly modularised by current aspect technology, as provisions for static crosscutting are not fine-grained enough. This position paper motivates the need for a declarative, domain-specific language for handling the tangled object layout concern. Based on observations in real-world VM implementations, we propose such a language, D4OL. It combines a two-level layout mapping, constraints and an engine to divide responsibilities between VM component and VM developers. We consider a domain-specific language like D4OL a necessary complement to behavioural aspect languages in order to modularize VM\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["60"]}
{"title": "AOP on the C-side\n", "abstract": " Although aspect-oriented programming originally emerged to overcome fundamental modularity problems in object-oriented applications, its ideas have long been backported to legacy languages like Cobol, C,... As systems written in these languages are prime tar-gets for re (verse)-engineering efforts, aspects can now be used for these purposes. Before applying dynamic analysis techniques on an industrial case study (453 KLOC of C) using aspects, we de-vised a list of requirements for possible aspect frameworks. In this paper we explain why no existing framework for C fulfilled all our requirements. We discuss the problems we encountercd with As-picere, our own aspect language for C. We also suggest points of improvement for future reverse-engineering efforts.", "num_citations": "6\n", "authors": ["60"]}
{"title": "Soul: data sharing for robot swarms\n", "abstract": " Interconnected devices and mobile multi-robot systems are increasingly present in many real-life scenarios. To be effective, these systems need to collect large amounts of data from their environment, and often these data need to be aggregated, shared, and distributed. Many multi-robot systems are designed to share state information and commands, but their communication infrastructure is often too limited for significant data transfers. This paper introduces Swarm-Oriented Upload of Labeled data, a mechanism that allows members of a fully distributed system to share data with their peers. We leverage a BitTorrent-like strategy to share data in smaller chunks, or datagrams, with policies that minimize reconstruction time. We performed extensive simulations to study the properties of the system and to demonstrate its scalability. We report experiments conducted with real robots following two realistic\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["60"]}
{"title": "Using indexed sequence diagrams to recover the behaviour of AJAX applications\n", "abstract": " AJAX is an asynchronous client-side technology that enables feature-rich, interactive Web 2.0 applications. AJAX applications and technologies are very complex compared to classic web applications, having to cope with asynchronous communication over (unstable) network connections. Yet, AJAX developers still rely on the ad hoc development processes and techniques of the early '00s. To determine how the inherent complexity of AJAX impacts the design and maintenance of AJAX applications, this paper studies the amount of code reuse across the different features of an AJAX application. Furthermore, we analyze how the design of existing AJAX systems deal with AJAX-specific crosscutting concerns, such as handling the loss of network connectivity. We use dynamic analysis to recover the run-time behaviour of AJAX applications in the form of sequence diagrams that are indexed by the different\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["60"]}
{"title": "DiffLDA: topic evolution in software projects\n", "abstract": " Previous research has shown that topics can be automatically discovered in a software project\u0393\u00c7\u00d6s source code. Topics are collections of words that co-occur frequently in a text collection and are discovered using topic models such as latent Dirichlet allocation (LDA). Tracking how topics evolve, ie, grow and spread, over time is useful for supporting software maintenance, comprehension, and re-engineering activities. The evolution of topics is typically recovered by applying LDA to all versions of a project\u0393\u00c7\u00d6s source code at once, followed by post processing to map topics across versions. Although this technique works well in applications where each version of the data is completely different, for example in the analysis of conference proceedings, the technique does not work well with source code, which typically changes only incrementally and contains significant duplication across versions. In this paper, we present a new approach, called DiffLDA, for automatically mining topic evolution in source code. The approach addresses LDA\u0393\u00c7\u00d6s sensitivity to document duplication by operating on the differences between versions of a source code document, resulting in a more accurate, finer-grained representation of topic evolution. We validate our approach through case studies on simulated data and two open source projects. 1", "num_citations": "5\n", "authors": ["60"]}
{"title": "Language-independent aspect weaving\n", "abstract": " Building an aspect weaver right into the heart of the GCC compiler not only improves efficiency of weaving process and woven code in the context of C programs. Exploiting GCC 4.0\u0393\u00c7\u00d6s new intermediate representation also enables us to advise a C application with code written in eg Fortran or Java. This way, crosscutting concerns can be expressed in the most suitable language for their purpose. Issues like language interaction, programming conventions as well as a concrete implementation still need to be looked at.", "num_citations": "5\n", "authors": ["60"]}
{"title": "Swarm relays: Distributed self-healing ground-and-air connectivity chains\n", "abstract": " The coordination of robot swarms \u0393\u00c7\u00f4 large decentralized teams of robots \u0393\u00c7\u00f4 generally relies on robust and efficient inter-robot communication. Maintaining communication between robots is particularly challenging in field deployments where robot motion, unstructured environments, limited computational resources, low bandwidth, and robot failures add to the complexity of the problem. In this paper we propose a novel lightweight algorithm that lets a heterogeneous group of robots navigate to a target in complex 3D environments while maintaining connectivity with a ground station by building a chain of robots. The fully decentralized algorithm is robust to robot failures, can heal broken communication links, and exploits heterogeneous swarms: when a target is unreachable by ground robots, the chain is extended with flying robots. We test the performance of our algorithm using up to 100 robots in a physics-based\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["60"]}
{"title": "Migrating a Software Factory to Design Thinking: Paying Attention to People and Mind-Sets\n", "abstract": " Design thinking (DT) has found its way into software engineering, promising better requirements elicitation, customer relations, and cohesion within the development team. We report on Proaction Technologies' migration toward DT and evaluate the process through interviews with employees and clients.", "num_citations": "4\n", "authors": ["60"]}
{"title": "Understanding GCC builtins to develop better tools\n", "abstract": " C programs can use compiler builtins to provide functionality that the C language lacks. On Linux, GCC provides several thousands of builtins that are also supported by other mature compilers, such as Clang and ICC. Maintainers of other tools lack guidance on whether and which builtins should be implemented to support popular projects. To assist tool developers who want to support GCC builtins, we analyzed builtin use in 4,913 C projects from GitHub. We found that 37% of these projects relied on at least one builtin. Supporting an increasing proportion of projects requires support of an exponentially increasing number of builtins; however, implementing only 10 builtins already covers over 30% of the projects. Since we found that many builtins in our corpus remained unused, the effort needed to support 90% of the projects is moderate, requiring about 110 builtins to be implemented. For each project, we\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["60"]}
{"title": "A comparison of bugs across the iOS and Android platforms of two open source cross platform browser apps\n", "abstract": " Mobile app developers want to maximize their revenue and hence want to reach as large an audience as possible. In order to do this, they need to build apps for multiple platforms - like Google's Android and Apple's iOS, and maintain them in parallel. Past research has examined properties of the issues addressed in either Android or iOS, but not to compare the work between both. Our main motivation has been to determine if there were differences in how issues manifest themselves in iOS and Android, when we control for the projects, by considering the same apps across multiple platforms. In this paper, we compare issues across two mobile platforms - iOS and Android - for two open source browsers - Mozilla Firefox and Google Chromium. We consider three dimensions of study: frequency of issue report submission, fixing time of issues, and type of issues (using topic modeling on the issue description to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["60"]}
{"title": "Failure-tolerant connectivity maintenance for robot swarms\n", "abstract": " Connectivity maintenance plays a key role in achieving a desired global behavior among a swarm of robots. However, connectivity maintenance in realistic environments is hampered by lack of computation resources, low communication bandwidth, robot failures, and unstable links. In this paper, we propose a novel decentralized connectivity-preserving algorithm that can be deployed on top of other behaviors to enforce connectivity constraints. The algorithm takes a set of targets to be reached while keeping a minimum number of redundant links between robots, with the goal of guaranteeing bandwidth and reliability. Robots then incrementally build and maintain a communication backbone with the specified number of links. We empirically study the performance of the algorithm, analyzing its time to convergence, as well as robustness to faults injected into the backbone robots. Our results statistically demonstrate the algorithm's ability to preserve the desired connectivity constraints and to reach the targets with up to 70 percent of individual robot failures in the communication backbone.", "num_citations": "4\n", "authors": ["60"]}
{"title": "Automated performance deviation detection across software versions releases\n", "abstract": " Performance is an important aspect and critical requirement in multi-process software architecture systems such as Google Chrome. While interacting closely with members of the Google Chrome engineering team, we observed that they face a major challenge in detecting performance deviations between releases, because of their very high release frequency and therefore limited amount of data on each. This paper describes a deep analysis on the data distributions followed by a comparative approach using median based confidence interval for software evaluation. This technique is capable of detecting performance related deviations. It is substantially different from the standard confidence interval, in that it can be used in the presence of outliers and random external influences since the median is less influenced by them. We conducted a bottom-up analysis, using stack traces in a very large pool of releases. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["60"]}
{"title": "Using the GPGPU for scaling up mining software repositories\n", "abstract": " The Mining Software Repositories (MSR) field integrates and analyzes data stored in repositories such as source control and bug repositories to support practitioners. Given the abundance of repository data, scaling up MSR analyses has become a major challenge. Recently, researchers have experimented with conventional techniques like a supercomputer or cloud computing, but these are either too expensive or too hard to configure. This paper proposes to scale up MSR analysis using \u0393\u00c7\u00a3general-purpose computing on graphics processing units\u0393\u00c7\u00a5 (GPGPU) on off-the-shelf video cards. In a representative MSR case study to measure co-change on version history of the Eclipse project, we find that the GPU approach is up to a factor of 43.9 faster than a CPU-only approach.", "num_citations": "4\n", "authors": ["60"]}
{"title": "Modeling the performance of Ultra-Large-Scale systems using layered simulations\n", "abstract": " The backbone of cloud computing platforms like Amazon S3 and Salesforce is formed by Ultra-Large-Scale (ULS) systems, i.e., complex, globally distributed infrastructure consisting of heterogeneous sets of software and hardware nodes. To ensure that a ULS system can scale to handle increasing service demand, it is important to understand the system's performance behaviour, for example to pro-actively plan for hardware upgrades. A good performance model should address concerns from all stakeholders at the level appropriate to their knowledge, interest, and experience. However, this is not straightforward, since stakeholders of ULS systems have a wide range of backgrounds and concerns: software developers are more interested in the performance of individual software components in the system, whereas managers are concerned about the performance of the entire system in different configurations. In\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["60"]}
{"title": "Workshop on Mining Unstructured Data (MUD) because\" Mining Unstructured Data is Like Fishing in Muddy Waters\"!\n", "abstract": " In software development, the knowledge of developers, architects and end users is spread out across dozens of development artifacts. Historically, structured development artifacts such as source code have been the primary focus of software engineering research, but the last couple of years have seen a dramatic increase of research on unstructured data, such as free-form text requirements and specifications, mailing lists and bug reports. Mining such data is very challenging, since it typically requires dealing with natural language fragments. Research communities in information retrieval, data mining and natural language processing have explored techniques to mine unstructured data. These techniques are usually limited in scope and intended for use in specific scenarios. We feel that the knowledge gathered by these research efforts should be consolidated and propagated to the empirical software engineering\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["60"]}
{"title": "cHALO, stateful aspects in C\n", "abstract": " History-based pointcut languages are a very expressive and powerful means to obtain robust pointcuts. To implement them in an efficient way, people have proposed various optimisations and program history retention strategies, especially for Java-based aspect languages. In this paper, we focus on history-based pointcut languages for C, which has no provisions for weak references or automatic garbage collection. Because C programmers want explicit control over pointcut behaviour and memory footprint, we claim that a limited set of fine-grained temporal pointcut primitives with well-known memory behaviour strikes a good balance between flexibility and memory consumption. A working prototype (cHALO) has been designed and implemented, based on the HALO pointcut language for Lisp.", "num_citations": "4\n", "authors": ["60"]}
{"title": "On the Co-evolution of ML Pipelines and Source Code-Empirical Study of DVC Projects\n", "abstract": " The growing popularity of machine learning (ML) applications has led to the introduction of software engineering tools such as Data Versioning Control (DVC), MLFlow and Pachyderm that enable versioning ML data, models, pipelines and model evaluation metrics. Since these versioned ML artifacts need to be synchronized not only with each other, but also with the source and test code of the software applications into which the models are integrated, prior findings on co-evolution and coupling between software artifacts might need to be revisited. Hence, in order to understand the degree of coupling between ML-related and other software artifacts, as well as the adoption of ML versioning features, this paper empirically studies the usage of DVC in 391 Github projects, 25 of which in detail. Our results show that more than half of the DVC files in a project are changed at least once every one-tenth of the project\u0393\u00c7\u00d6s\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["60"]}
{"title": "What should your run-time configuration framework do to help developers?\n", "abstract": " The users or deployment engineers of a software system can adapt such a system to a wide range of deployment and usage scenarios by changing the value of configuration options, for example by disabling unnecessary features, tweaking performance-related parameters or specifying GUI preferences. However, the literature agrees that the flexibility of such options comes at a price: misconfigured options can lead a software system to crash in the production environment, while even in the absence of such configuration errors, a large number of configuration options makes a software system more complicated to deploy and use. In earlier work, we also found that developers who intend to make their application configurable face 22 challenges that impact their configuration engineering activities, ranging from technical to management-related or even inherent to the domain of configuration engineering. In this\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["60"]}
{"title": "A study of build inflation in 30 million CPAN builds on 13 Perl versions and 10 operating systems\n", "abstract": " Continuous Integration (CI) is a cornerstone of modern quality assurance, providing on-demand builds (compilation and tests) of code changes or software releases. Yet the many existing CI systems do not help developers in interpreting build results, in particular when facing build inflation. Build inflation arises when each code change has to be built on dozens of combinations (configurations) of runtime environments (REs), operating systems (OSes), and hardware architectures (HAs). A code change C1 sent to the CI system may introduce programming faults that result in all these builds to fail, while a change C2 introducing a new library dependency might only lead one particular build configuration to fail. Consequently, the one build failure due to C2 will be \u0393\u00c7\u00a3hidden\u0393\u00c7\u00a5 among the dozens of build failures due to C1 when the CI system reports the results of the builds. We have named this phenomenon build inflation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["60"]}
{"title": "Comparing the communication tone and responses of users and developers in two R mailing lists: Measuring positive and negative emails\n", "abstract": " The R programming language user and developer communities use email lists to communicate. We compared the characteristics of different communication tones and their relation to email replies for a 10-year period. Our results suggest that different tones may produce small differences in responses across users and developers.", "num_citations": "3\n", "authors": ["60"]}
{"title": "A longitudinal study on the maintainers' sentiment of a large scale open source ecosystem\n", "abstract": " Software development is a collaborative activity in which feelings and emotions can affect the developer's productivity, creativity, and contribution satisfaction. For example, the Linux Kernel Mailing List (LKML), which is used by subsystem maintainers to review patches sent by contributors, is known for its direct communication style, which is sometimes blamed as having a negative impact on contributors. In September 28, 2018, the kernel's lead maintainer, Linus Torvalds, announced that he would take a temporary break from the community, which led numerous members of the kernel community and observers from other communities to wonder to what extent this unexpected event could raise awareness about respectful interactions between community members. This paper performs an exploratory study in which we use an off-the-shelf sentiment mining tool to assess whether the maintainers' sentiment changed\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["60"]}
{"title": "The unbroken telephone game: keeping swarms connected\n", "abstract": " Connectivity maintenance plays a key role in achieving a desired global behaviour among a swarm of robots. Yet, lack of computation resources, low communication bandwidth, robot failures, and unstable links are tough challenges for connectivity maintenance in realistic environments. In this paper, we propose a novel decentralized connectivity-preserving algorithm that can be deployed on top of other behaviours to enforce connectivity constraints. The algorithm takes a set of targets to be reached while keeping a minimum number of redundant links between robots, with the goal of guaranteeing bandwidth and reliability. We empirically study the performance of the algorithm, analyzing its time to convergence and robustness to failure.", "num_citations": "3\n", "authors": ["60"]}
{"title": "Backslicer: A lightweight backward slicer\n", "abstract": " Source code slicing allow a user to focus only a part of the program that may have a relation with a given line. From its introduction by Weiser, source code slicing is a technique that becomes popular, and which is widely used to resolve different kind of problems in software engineering. However, source code slicing is a time-consuming technique and requires an important memory size. In this paper, we present an approach to make the static slicing less time consuming, scalable, and that do not requires a huge memory size, especially in the case of big softwares. Our tool takes less than 15 minutes to perform the slicing for the whole source code of Linux instead of hours or days.", "num_citations": "3\n", "authors": ["60"]}
{"title": "How much does integrating this commit cost?-a position paper\n", "abstract": " Integration is a common development activity that fuses multiple pieces of source code together, either developed in-house or acquired from a third-party (eg, Git commit). Integration essentially combines software that was developed in parallel by different developers in different teams or organization. Thus it requires to iron out inconsistent and conflicting changes. Inconsistencies are costly and can only be identified a posterior. It is very hard to judge if it is worth integrating a code commit, or which of two equivalent commits or libraries to integrate. If there is an indication about the effort the integration will take, it will help the integrators to make dicisions easier. In this paper, we propose the ISOMO model to quantify the projected cost of integrating a code commit. This will help people to evaluate how much effort a commit will cost and decide if it is worth being integrated.", "num_citations": "3\n", "authors": ["60"]}
{"title": "On the relationship between program evolution and fault-proneness: An empirical study\n", "abstract": " Over the years, many researchers have studied the evolution and maintenance of object-oriented source code in order to understand the possibly costly erosion of the software. However, many studies thus far did not link the evolution of classes to faults. Since (1) some classes evolve independently, other classes have to do it together with others (co-evolution), and (2) not all classes are meant to last forever, but some are meant for experimentation or to try out an idea that was then dropped or modified. In this paper, we group classes based on their evolution to infer their lifetime models and coevolution trends. Then, we link each group's evolution to faults. We create phylogenetic trees showing the evolutionary history of programs and we use such trees to facilitate spotting the program code decay. We perform an empirical study, on three open-source programs: ArgoUML, JFreechart, and XercesJ, to examine the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["60"]}
{"title": "An aspect for idiom-based exception handling\n", "abstract": " The last couple of years, various idioms used in the 15 MLOC C code base of ASML, the world\u0393\u00c7\u00d6s biggest lithography machine manufacturer, have been unmasked as crosscutting concerns. However, finding a scalable aspect-based implementation for them did not succeed thusfar, prohibiting sufficient separation of concerns and introducing possibly dangerous programming mistakes. This paper proposes a concise aspect-based implementation in Aspicere2 for ASML\u0393\u00c7\u00d6s exception handling idiom, based on prior work of join point properties, annotations and type parameters, to which we add the new concept of (local) continuation join points. Our solution takes care of the error value propagation mechanism (which includes aborting the main success scenario), logging, resource cleanup, and allows for local overrides of the default aspect-based recovery. The highly idiomatic nature of the problem in tandem with the aforementioned concepts renders our aspects very robust and tolerant to future base code evolution.", "num_citations": "3\n", "authors": ["60"]}
{"title": "Why do builds fail?\u0393\u00c7\u00f6A conceptual replication study\n", "abstract": " Previous studies have investigated a wide range of factors potentially explaining software build breakages, focusing primarily on build-triggering code changes or previous CI outcomes. However, code quality factors such as the presence of code/test smells have not been yet evaluated in the context of CI, even though such factors have been linked to problems of comprehension and technical debt, and hence might introduce bugs and build breakages. This paper performs a conceptual replication study on 27,675 Travis CI builds of 15 GitHub projects, considering the features reported by Rausch et al. and Zolfagharinia et al., as well as those related to code/test smells. Using a multivariate model constructed from nine dimensions of features, results indicate a precision (recall) ranging between 58.3% and 79.0% (52.4% and 69.6%) in balanced project datasets, and between 2.5% and 37.5% (2.5% and 12.4%) in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["60"]}
{"title": "A feature location approach for mapping application features extracted from crowd-based screencasts to source code\n", "abstract": " Crowd-based multimedia documents such as screencasts have emerged as a source for documenting requirements, the workflow and implementation issues of open source and agile software projects. For example, users can show and narrate how they manipulate an application\u0393\u00c7\u00d6s GUI to perform a certain functionality, or a bug reporter could visually explain how to trigger a bug or a security vulnerability. Unfortunately, the streaming nature of programming screencasts and their binary format limit how developers can interact with a screencast\u0393\u00c7\u00d6s content. In this research, we present an automated approach for mining and linking the multimedia content found in screencasts to their relevant software artifacts and, more specifically, to source code. We apply LDA-based mining approaches that take as input a set of screencast artifacts, such as GUI text and spoken word, to make the screencast content accessible and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["60"]}
{"title": "Analysis of Modern Release Engineering Topics:\u0393\u00c7\u00f4A Large-Scale Study using StackOverflow\u0393\u00c7\u00f4\n", "abstract": " Release engineers are continuously required to de-liver high-quality software products to the end-user. As a result, modern software companies are proposing new changes in their delivery process that adapt to new technologies such as continuous deployment and Infrastructure-as-Code. However, developers and release engineers still find these practices challenging, and resort to question and answer websites such as StackOverflow to find answers. This paper presents the results of our empirical study on release engineering questions in StackOverflow, to understand the modern release engineering topics of interest and their difficulty. Using topic modeling techniques, we find that (i) developers discuss on a broader range of 38 release engineering topics covering all the six phases of modern release engineering, (ii) the topics Merge Conflict, Branching & Remote Upstream are more popular, while topics Code\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["60"]}
{"title": "The missing link\u0393\u00c7\u00f4A semantic web based approach for integrating screencasts with security advisories\n", "abstract": " ContextCollaborative tools and repositories have been introduced to facilitate open source software development, allowing projects, developers, and users to share their knowledge and expertise through formal and informal channels such as repositories, Q&A websites, blogs and screencasts. While significant progress has been made in mining and cross-linking traditional software repositories, limited work exists in making multimedia content in the form of screencasts or audio recordings an integrated part of software engineering processes.ObjectiveThe objective of this research is to provide a standardized ontological representation that allows for a seamless knowledge integration of screencasts with other software artifacts across knowledge resource boundaries.MethodIn this paper, we propose a modeling approach that takes advantage of the Semantic Web and its inference services to capture and establish\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["60"]}
{"title": "Error leakage and wasted time: sensitivity and effort analysis of a requirements consistency checking process\n", "abstract": " Several techniques are used by requirements engineering practitioners to address difficult problems such as specifying precise requirements while using inherently ambiguous natural language text and ensuring the consistency of requirements. Often, these problems are addressed by building processes/tools that combine multiple techniques where the output from 1 technique becomes the input to the next. While powerful, these techniques are not without problems. Inherent errors in each technique may leak into the subsequent step of the process. We model and study 1 such process, for checking the consistency of temporal requirements, and assess error leakage and wasted time. We perform an analysis of the input factors of our model to determine the effect that sources of uncertainty may have on the final accuracy of the consistency checking process. Convinced that error leakage exists and negatively\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["60"]}
{"title": "Release synchronization in software ecosystems\n", "abstract": " Software ecosystems bring value by integrating software projects related to a given domain, such as Linux distributions integrating upstream open-source projects or the Android ecosystem for mobile Apps. Since each project within an ecosystem may potentially have its release cycle and roadmap, this creates an enormous burden for users who must expend the effort to identify and install compatible project releases from the ecosystem manually. Thus, many ecosystems, such as the Linux distributions, take it upon them to release a polished, well-integrated product to the end-user. However, the body of knowledge lacks empirical evidence about the coordination and synchronization efforts needed at the ecosystem level to ensure such federated releases. This paper empirically studies the strategies used to synchronize releases of ecosystem projects in the context of the OpenStack ecosystem, in which a central\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["60"]}
{"title": "On the impact of interlanguage dependencies in multilanguage systems empirical case study on java native interface applications (JNI)\n", "abstract": " Nowadays, developers are often using multiple programming languages to exploit the advantages of each language and to reuse code. However, dependency analysis across multilanguage is more challenging compared to mono-language systems. In this article, we introduce two approaches for multilanguage dependency analysis: static multilanguage dependency analyzer) and historical multilanguage dependency analyzer, which we apply on ten open-source multilanguage systems to empirically analyze the prevalence of the dependencies across languages, i.e., interlanguage dependencies and their impact on software quality and security. Our main results show that: the more interlanguage dependencies, the higher the risk of bugs and vulnerabilities being introduced, while this risk remains constant for intralanguage dependencies; the percentage of bugs within interlanguage dependencies is three times\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["60"]}
{"title": "On the Impact of Multi-language Development in Machine Learning Frameworks\n", "abstract": " The role of machine learning frameworks in software applications has exploded in recent years. Similar to non-machine learning frameworks, those frameworks need to evolve to incorporate new features, optimizations, etc., yet their evolution is impacted by the interdisciplinary development teams needed to develop them: scientists and developers. One concrete way in which this shows is through the use of multiple programming languages in their code base, enabling the scientists to write optimized low-level code while developers can integrate the latter into a robust framework. Since multi-language code bases have been shown to impact the development process, this paper empirically compares ten large open-source multi-language machine learning frameworks and ten large open-source multi-language traditional systems in terms of the volume of pull requests, their acceptance ratio i.e., the percentage of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["60"]}
{"title": "The Diversity Crisis of Software Engineering for Artificial Intelligence\n", "abstract": " Artificial Intelligence (AI) is experiencing a \"diversity crisis.\"1 Several reports1-3 have shown how the breakthrough of modern AI has not yet been able to improve on existing diversity challenges regarding gender, race, geography, and other factors, neither for the end users of those products nor the companies and organizations building them. Plenty of examples have surfaced in which biased data engineering practices or existing data sets led to incorrect, painful, or sometimes even harmful consequences for unassuming end users.4 The problem is that ruling out such biases is not straightforward due to the sheer number of different bias types.5 To have a chance to eliminate as many biases as possible, most of the experts agree that the teams and organizations building AI products should be made more diverse.1-3 This harkens back to Linus' law6 for open source development (\"given enough eyeballs, all bugs\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["60"]}
{"title": "Does geographical distance effect distributed development teams: How aggregation bias in software artifacts causes contradictory findings\n", "abstract": " Does geographic distance affect distributed software development teams? Researchers have been mining software artifacts to find evidence that geographic distance between software team members introduces delay in communication and deliverables. While some studies found that geographical distance negatively impacts software teams, other studies dispute this finding. It has been speculated that various confounding factors are the reason for the contradicting findings. For example, newer tools and practices that enable team members to communicate and collaborate more effectively, might have negated the effects of distance in some studies. In this study, we examine an alternate theory to explain the contradicting findings: the different aggregations of the software artifacts used in past studies. We call this type of bias: the aggregation bias. We replicated the previous studies on detecting the evidence of delay\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["60"]}
{"title": "Botched Releases: Do we Need to Roll Back?\n", "abstract": " Few minutes after a web-based software release, the release team might encounter log traces showing the new system crashing, hanging, or having poor performance. This is the start of the most nerve-wrecking moments of a product\u0393\u00c7\u00d6s release cycle, ie, should one run the risk of not doing anything and users losing precious data, or of prematurely engaging the tedious (and costly) roll-back procedure towards the previous release? Thus far, only little attention has been paid by researchers to these so-called \u0393\u00c7\u00a3botched releases\u0393\u00c7\u00a5, partly because of lack of release log data. This paper studies 345 releases of a large e-commerce web app over a period of 1.5 years, in which we identified 17 recurrent root causes of botched releases, classified into four major categories. We then build explanatory models to understand which root causes are the most important, and to explore the factors leading to botched releases.", "num_citations": "1\n", "authors": ["60"]}
{"title": "3rd international workshop on release engineering (RELENG 2015)\n", "abstract": " Release engineering deals with all activities inbetween regular development and actual usage of asoftware product by the end user, i.e., integration, build, testexecution, packaging and delivery of software. Although re-search on this topic goes back for decades, the increasing heterogeneity and variability of software products along withthe recent trend to reduce the release cycle to days or even hoursstarts to question some of the common beliefs and practicesof the field. For example, a project like Mozilla Firefox releasesevery 6 weeks, generating updates for dozens of existing Fire-fox versions on 5 desktop, 2 mobile and 3 mobile desktopplatforms, each of which for more than 80 locales. In this con-text, the International Workshop on Release Engineering(RELENG) aims to provide a highly interactive forum for re-searchers and practitioners to address the challenges of, findsolutions for and share experiences with\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["60"]}
{"title": "Made in MAKAO\n", "abstract": " Software systems do not solely consist of source code: various other types of artifacts play a role, notably the build system. Although nothing stands as close to the source code or lends itself better as a starting point to explore a system\u0393\u00c7\u00d6s high-level architecture, few techniques or tools recognize or exploit this. Still, as each considerable change to a software system potentially demands modifications of build-related files, maintainers and developers alike need to find their way around quickly. We present MAKAO (Makefile Architecture Kernel for Aspect Orientation), a re (verse)-engineering framework for build systems, as a means to extract as much knowledge from the build system as possible and to help solving typical build-related problems. MAKAO offers a DAG (Directed Acyclic Graph)-based view of a build system, supporting visualization, querying and (dynamic) modification.", "num_citations": "1\n", "authors": ["60"]}