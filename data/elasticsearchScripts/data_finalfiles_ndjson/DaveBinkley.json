{"title": "What's in a Name? A Study of Identifiers\n", "abstract": " Readers of programs have two main sources of domain information: identifier names and comments. When functions are uncommented, as many are, comprehension is almost exclusively dependent on the identifier names. Assuming that writers of programs want to create quality identifiers (e.g., include relevant domain knowledge) how should they go about it? For example, do the initials of a concept name provide enough information to represent the concept? If not, and a longer identifier is needed, is an abbreviation satisfactory or does the concept need to be captured in an identifier that includes full words? Results from a study designed to investigate these questions are reported. The study involved over 100 programmers who were asked to describe twelve different functions. The functions used three different \"levels\" of identifiers: single letters, abbreviations, and full words. Responses allow the level of\u00a0\u2026", "num_citations": "290\n", "authors": ["542"]}
{"title": "Source code analysis: A road map\n", "abstract": " The automated and semi-automated analysis of source code has remained a topic of intense research for more than thirty years. During this period, algorithms and techniques for source-code analysis have changed, sometimes dramatically. The abilities of the tools that implement them have also expanded to meet new and diverse challenges. This paper surveys current work on source-code analysis. It also provides a road map for future work over the next five-year period and speculates on the development of source-code analysis applications, techniques, and challenges over the next 10, 20, and 50 years.", "num_citations": "278\n", "authors": ["542"]}
{"title": "Semantics guided regression test cost reduction\n", "abstract": " Software maintainers are faced with the task of regression testing: retesting a modified program on an often large number of test cases. The cost of regression testing can be reduced if the size of the program is reduced and if old test cases and results can be reused. Two complimentary algorithms for reducing the cost of regression testing are presented. The first produces a program called Differences that captures the semantic change between Certified, a previously tested program, and Modified, a changed version of Certified. It is more efficient to test Differences, because it omits unchanged computations. The program Differences is computed using a combination of program slices. The second algorithm identifies test cases for which Certified and Modified produce the same output and existing test cases that test new components in Modified. The algorithm is based on the notion of common execution patterns\u00a0\u2026", "num_citations": "256\n", "authors": ["542"]}
{"title": "Information systems project management: an agency theory interpretation\n", "abstract": " The failure rate of information systems development projects is high. Agency theory offers a potential explanation for it. Structured interviews with 12 IS project managers about their experiences managing IS development projects show how it can be used to understand IS development project outcomes. Managers can use the results of the interviews to improve their own IS project management. Researchers can use them to examine agency theory with a larger number of project managers.", "num_citations": "189\n", "authors": ["542"]}
{"title": "The application of program slicing to regression testing\n", "abstract": " Software maintainers are faced with the task of regression testing: retesting a program after a modification. The goal of regression testing is to ensure that bug fixes and new functionality do not adversely affect the correct functionality inherited from the original program. Regression testing often involves running a large program on a large number of test cases; thus, it can be expensive in terms of both human and machine time. Many approaches for reducing the cost of regression testing have been proposed. Those that make use of program slicing are surveyed.", "num_citations": "183\n", "authors": ["542"]}
{"title": "Evolutionary testing in the presence of loop-assigned flags: A testability transformation approach\n", "abstract": " Evolutionary testing is an effective technique for automatically generating good quality test data. However, for structural testing, the technique degenerates to random testing in the presence of flag variables, which also present problems for other automated test data generation techniques. Previous work on the flag problem does not address flags assigned in loops.This paper introduces a testability transformation that transforms programs with loop--assigned flags so that existing genetic approaches can be successfully applied. It then presents empirical data demonstrating the effectiveness of the transformation. Untransformed, the genetic algorithm flounders and is unable to find a solution. Two transformations are considered. The first allows the search to find a solution. The second reduces the time taken by an order of magnitude and, more importantly, reduces the slope of the cost increase; thus, greatly increasing\u00a0\u2026", "num_citations": "150\n", "authors": ["542"]}
{"title": "To camelcase or under_score\n", "abstract": " Naming conventions are generally adopted in an effort to improve program comprehension. Two of the most popular conventions are alternatives for composing multi-word identifiers: the use of underscores and the use of camel casing. While most programmers have a personal opinion as to which style is better, empirical study forms a more appropriate basis for choosing between them. The central hypothesis considered herein is that identifier style affects the speed and accuracy of manipulating programs. An empirical study of 135 programmers and non-programmers was conducted to better understand the impact of identifier style on code readability. The experiment builds on past work of others who study how readers of natural language perform such tasks. Results indicate that camel casing leads to higher accuracy among all subjects regardless of training, and those trained in camel casing are able to\u00a0\u2026", "num_citations": "131\n", "authors": ["542"]}
{"title": "Amorphous program slicing\n", "abstract": " Traditional, syntax-preserving program slicing simplifies a program by deleting components (e.g., statements and predicates) that do not affect a computation of interest. Amorphous slicing removes the limitation to component deletion as the only means of simplification, while retaining the semantic property that a slice preserves the selected behaviour of interest from the original program. This leads to slices which are often considerably smaller than their syntax-preserving counterparts.A formal framework is introduced to define and compare amorphous and traditional program slicing. After this definition, an algorithm for computing amorphous slices, based on the system dependence graph, is presented. An implementation of this algorithm is used to demonstrate the utility of amorphous slicing with respect to code-level analysis of array access safety. The resulting empirical study indicates that programmers\u00a0\u2026", "num_citations": "129\n", "authors": ["542"]}
{"title": "Using semantic differencing to reduce the cost of regression testing.\n", "abstract": " This paper presents an algorithm that reduces the cost of regression testing by reducing the number of test cases that must be re-run and by reducing the size of the program that they must be run on. The algorithm uses dependence graphs and program slicing to partition the components of the new program into two sets: preserved points\u2014components that have unchanged run-time behavior; and affected points\u2014components that have changed run-time behavior. Only test cases that test the behavior of affected points must be re-run; the behavior of the preserved points is guaranteed to be the same in the old and new versions of the program. Furthermore, the algorithm produces a program differences, which captures the behavior of (only) the affected points. Thus, rather than re-testing the (large) new program on a large number of test cases, it is possible to certify the new program by running the (smaller) program differences on a (smaller) number of test cases.", "num_citations": "125\n", "authors": ["542"]}
{"title": "An empirical study of slice-based cohesion and coupling metrics\n", "abstract": " Software reengineering is a costly endeavor, due in part to the ambiguity of where to focus reengineering effort. Coupling and Cohesion metrics, particularly quantitative cohesion metrics, have the potential to aid in this identification and to measure progress. The most extensive work on such metrics is with slice-based cohesion metrics. While their use of semantic dependence information should make them an excellent choice for cohesion measurement, their wide spread use has been impeded in part by a lack of empirical study. Recent advances in software tools make, for the first time, a large-scale empirical study of slice-based cohesion and coupling metrics possible. Four results from such a study are presented. First, \u201chead-to-head\u201d qualitative and quantitative comparisons of the metrics identify which metrics provide similar views of a program and which provide unique views of a program. This study includes\u00a0\u2026", "num_citations": "110\n", "authors": ["542"]}
{"title": "Effective identifier names for comprehension and memory\n", "abstract": " Readers of programs have two main sources of domain information: identifier names and comments. When functions are uncommented, as many are, comprehension is almost exclusively dependent on the identifier names. Assuming that writers of programs want to create quality identifiers (e.g., identifiers that include relevant domain knowledge), one must ask how should they go about it. For example, do the initials of a concept name provide enough information to represent the concept? If not, and a longer identifier is needed, is an abbreviation satisfactory or does the concept need to be captured in an identifier that includes full words? What is the effect of longer identifiers on limited short term memory capacity? Results from a study designed to investigate these questions are reported. The study involved over 100 programmers who were asked to describe 12 different functions and then recall identifiers\u00a0\u2026", "num_citations": "108\n", "authors": ["542"]}
{"title": "Precise executable interprocedural slices\n", "abstract": " The notion of a program slice, originally introduced by Mark Weiser, is useful in program debugging, automatic parallelization, program integration, and software maintenance. A slice of a program is taken with respect to a program point p and a variable x; the slice consists of all statements of the program that might affect the value of x at point p. An interprocedural slice is a slice of an entire program, where the slice crosses the boundaries of procedure calls. Weiser's original interprocedural-slicing algorithm produces imprecise slices that are executable programs. A recent algorithm developed by Horwitz, Reps, and Binkley produces more precise (smaller) slices by more accurately identifying those statements that might affect the values of x at point p. These slices, however, are not executable. An extension to their algorithm that produces more precise executable interprocedural slices is described together with a\u00a0\u2026", "num_citations": "98\n", "authors": ["542"]}
{"title": "Normalizing source code vocabulary\n", "abstract": " Information Retrieval (IR) based tools complement traditional static and dynamic analysis tools by exploiting the natural language found within a program's text. Tools incorporating IR have tackled problems, such as feature location, that previously required considerable human effort. However, to reap the full benefit of IR-based techniques, the language used across all software artifacts (e.g., requirement and design documents, test plans, as well as the source code) must be consistent. Vocabulary normalization aligns the vocabulary found in source code with that found in other software artifacts. Normalization both splits an identifier into its constituent parts and expands each part into a full dictionary word to match vocabulary in other artifacts. An algorithm for normalization is presented. Its current implementation incorporates a greatly improved splitter that exploits a collection of resources including several\u00a0\u2026", "num_citations": "89\n", "authors": ["542"]}
{"title": "Using peer-led team learning to increase participation and success of under-represented groups in introductory computer science\n", "abstract": " This paper describes the implementation and evaluation of a program that uses active recruiting and peer-led team learning to try to increase the participation and success of women and minority students in undergraduate computer science. These strategies were applied at eight universities starting in the fall of 2004. There have been some impressive results: We succeeded in attracting under-represented students who would not otherwise have taken a CS course. Evaluation shows that participation in our program significantly improves retention rates and grades, especially for women. Students in the program, as well as the students who served as peer leaders, are uniformly enthusiastic about their experience.", "num_citations": "89\n", "authors": ["542"]}
{"title": "Experimental validation of new software technology\n", "abstract": " When to apply a new technology in an organization is a critical decision for every software development organization. Earlier work defines a set of methods that the research community uses when a new technology is developed. This chapter presents a discussion of the set of methods that industrial organizations use before adopting a new technology. First there is a brief definition of the earlier research methods and then a definition of the set of industrial methods. A survey taken by experts from both the research and industrial communities provides insights into how these communities differ in their approach toward technology innovation and technology transfer.", "num_citations": "87\n", "authors": ["542"]}
{"title": "Slice-based cohesion metrics and software intervention\n", "abstract": " Software reconstruction is a costly endeavor, due in part to the ambiguity of where to focus reengineering effort. Cohesion metrics, and particularly quantitative cohesion metrics, have the potential to aid in this identification and to measure progress. The most extensive work on such metrics is with slice-based cohesion metrics. While their use of semantic dependence information should make them an excellent choice for cohesion measurement, their wide spread use has been impeded by a lack of empirical study. Recent advances in software tools make, for the first time, a large-scale empirical study of slice-based cohesion metrics possible. Three results from such a study are presented. First, base-line values for slice-based metrics are provided. These values act as targets for reengineering efforts with modules having values outside the expected range being the most in need of attention. Second, two longitudinal\u00a0\u2026", "num_citations": "84\n", "authors": ["542"]}
{"title": "Leveraged quality assessment using information retrieval techniques\n", "abstract": " The goal of this research is to apply language processing techniques to extend human judgment into situations where obtaining direct human judgment is impractical due to the volume of information that must be considered. On aspect of this is leveraged quality assessments, which can be used to evaluate third-party coded subsystems, to track quality across the versions of a program, to assess the compression effort (and subsequent cost) required to make a change, and to identify parts of a program in need of preventative maintenance. A description of the QALP tool, its output from just under two million lines of code, and an experiment aimed at evaluating the tool's use in leveraged quality assessment are presented. Statistically significant results from this experiment validate the use of the QALP tool in human leverage quality assessment", "num_citations": "77\n", "authors": ["542"]}
{"title": "Extracting meaning from abbreviated identifiers\n", "abstract": " Informative identifiers are made up of full (natural language) words and (meaningful) abbreviations. Readers of programs typically have little trouble understanding the purpose of identifiers composed of full words. In addition, those familiar with the code can (most often) determine the meaning of abbreviations used in identifiers. However, when faced with unfamiliar code, abbreviations often carry little useful information. Furthermore, tools that focus on the natural language used in the code have a hard time in the presence of abbreviations. One approach to providing meaning to programmers and tools is to translate (expand) abbreviations into full words. This paper presents a methodology for expanding identifiers and evaluates the process on a code based of just over 35 million lines of code. For example, using phrase extraction,  fs_exists  is expanded to  file_status_exists  illustrating how the expansion process\u00a0\u2026", "num_citations": "76\n", "authors": ["542"]}
{"title": "Reducing the cost of regression testing by semantics guided test case selection\n", "abstract": " Software maintainers are faced with the task of regression testing: retesting a modified program on a (large) number of test cases. The cost of regression testing can be reduced if old test cases and old test results can be reused. Reuse avoids the costly construction of new test cases and the unproductive rerunning of existing test cases when it can be guaranteed that the modified and original programs will produce the same results. An algorithm that uses language semantics to provide such a guarantee is presented. This algorithm uses semantic (not syntactic) differences and similarities between the old and new programs. The algorithm is based on the notion of common execution patterns, which is the interprocedural extension of equivalent execution patterns. Program components with common execution patterns are computed using a new type of interprocedural slice called a calling context slice. Whereas an\u00a0\u2026", "num_citations": "75\n", "authors": ["542"]}
{"title": "Expanding identifiers to normalize source code vocabulary\n", "abstract": " Maintaining modern software requires significant tool support. Effective tools exploit a variety of information and techniques to aid a software maintainer. One area of recent interest in tool development exploits the natural language information found in source code. Such Information Retrieval (IR) based tools compliment traditional static analysis tools and have tackled problems, such as feature location, that otherwise require considerable human effort. To reap the full benefit of IR-based techniques, the language used across all software artifacts (e.g., requirements, design, change requests, tests, and source code) must be consistent. Unfortunately, there is a significant proportion of invented vocabulary in source code. Vocabulary normalization aligns the vocabulary found in the source code with that found in other software artifacts. Most existing work related to normalization has focused on splitting an identifier into\u00a0\u2026", "num_citations": "74\n", "authors": ["542"]}
{"title": "An empirical comparison of techniques for extracting concept abbreviations from identifiers\n", "abstract": " When a programmer is faced with the task of modifying code written by others, he or she must first gain an understanding of the concepts and entities used by the program. Comments and identifiers are the two main sources of such knowledge. In the case of identifiers, the meaning can be hidden in abbreviations that make comprehension more difficult. A tool that can automatically replace abbreviations with their full word meanings would improve the comprehension ability (especially of less experienced programmers) to understand and work with the code. Such a tool first needs to isolate abbreviations within the identifiers. When identifiers are separated by division markers such as underscores or camel-casing, this isolation task is trivial. However, many identifiers lack these division markers. Therefore, the first task of automatic expansion is separation of identifiers into their constituent parts. Presented here is a comparison of three techniques that accomplish this task: a random algorithm (used as a straw man), a greedy algorithm, and a neural network based algorithm. The greedy algorithm\u2019s performance ranges from 75 to 81 percent correct, while the neural network\u2019s performance ranges from 71 to 95 percent correct.", "num_citations": "72\n", "authors": ["542"]}
{"title": "Understanding LDA in source code analysis\n", "abstract": " Latent Dirichlet Allocation (LDA) has seen increasing use in the understanding of source code and its related artifacts in part because of its impressive modeling power. However, this expressive power comes at a cost: the technique includes several tuning parameters whose impact on the resulting LDA model must be carefully considered. An obvious example is the burn-in period; too short a burn-in period leaves excessive echoes of the initial uniform distribution. The aim of this work is to provide insights into the tuning parameter's impact. Doing so improves the comprehension of both, 1) researchers who look to exploit the power of LDA in their research and 2) those who interpret the output of LDA-using tools. It is important to recognize that the goal of this work is not to establish values for the tuning parameters because there is no universal best setting. Rather, appropriate settings depend on the problem being\u00a0\u2026", "num_citations": "68\n", "authors": ["542"]}
{"title": "Information retrieval applications in software maintenance and evolution\n", "abstract": " There is a growing interest in creating tools that can assist engineers in all phases of the software life cycle. This assistance requires techniques that go beyond traditional static and dynamic analysis. An example of such a technique is the application of information retrieval (IR), which exploits information found in a project\u2019s natural language. Such information can be extracted from the source code\u2019s identifiers and comments and in artifacts associated with the project, such as the requirements. The techniques described pertain to the maintenance and evolution phase of the software life cycle and focus on problems such as feature location and impact analysis. These techniques highlight the bright future that IR brings to addressing software engineering problems.", "num_citations": "67\n", "authors": ["542"]}
{"title": "Improving identifier informativeness using part of speech information\n", "abstract": " Recent software development tools have exploited the mining of natural language information found within software and its supporting documentation. To make the most of this information, researchers have drawn upon the work of the natural language processing community for tools and techniques. One such tool provides part-of-speech information, which finds application in improving the searching of software repositories and extracting domain information found in identifiers.", "num_citations": "66\n", "authors": ["542"]}
{"title": "Interprocedural slicing using dependence graphs\n", "abstract": " A slice of a program with respect to a program point p and variable x consists of all statements of the program that might affect the value of x at point p. This paper concerns the problem of interprocedural slicing -- generating a slice of an entire program, where the slice crosses the boundaries of procedure calls. To solve this problem, we introduce a new kind of graph to represent programs, called a system dependence graph, which extends previous dependence representations to incorporate collections of procedures (with procedure calls) rather than just monolithic programs. Our main result is an algorithm for interprocedural slicing that uses the new representation.The chief difficulty in interprocedural slicing is correctly accounting for the calling context of a called procedure. To handle this problem, system dependence graphs include some data-dependence edges that represent transitive dependencies due to the\u00a0\u2026", "num_citations": "63\n", "authors": ["542"]}
{"title": "Quantifying identifier quality: an analysis of trends\n", "abstract": " Identifiers, which represent the defined concepts in a program, account for, by some measures, almost three quarters of source code. The makeup of identifiers plays a key role in how well they communicate these defined concepts. An empirical study of identifier quality based on almost 50 million lines of code, covering thirty years, four programming languages, and both open and proprietary source is presented. For the purposes of the study, identifier quality is conservatively defined as the possibility of constructing the identifier out of dictionary words or known abbreviations. Four hypotheses related to identifier quality are considered using linear mixed effect regression models. For example, the first hypothesis is that modern programs include higher quality identifiers than older ones. In this case, the results show that better programming practices are producing higher quality identifies. Results also confirm\u00a0\u2026", "num_citations": "62\n", "authors": ["542"]}
{"title": "A formalisation of the relationship between forms of program slicing\n", "abstract": " The widespread interest in program slicing within the source code analysis and manipulation community has led to the introduction of a large number of different forms of slicing. Each preserves some aspect of a program\u2019s behaviour and simplifies the program to focus exclusively upon this behaviour. In order to understand the similarities and differences between forms of slicing, a formal mechanism is required. This paper further develops a formal framework for comparing forms of slicing using a theory of program projection. This framework is used to reveal the ordering relationship between various static, dynamic, simultaneous and conditioned forms of slicing.", "num_citations": "58\n", "authors": ["542"]}
{"title": "Introduction: Cultural studies and anti-consumerism: A critical encounter\n", "abstract": " A recent mock-article in the satirical US-based newspaper The Onion announced that \u2018consumer product diversity\u2019\u00c1 the sheer number and volume of different commodities out there in the world \u00c1 has now replaced biodiversity.\u2018In the light of the crumbling global ecology\u2019the parodic news story argued,\u2018it is vital that we furnish the diversity of the global marketplace by buying the widest range of consumer products possible\u2019. If we do so,\u2018lush, highly developed supermarkets\u2019 will replace the deteriorating ecosystems symbolized by fallen rainforests and melting glaciers. 1 The tone \u00c1 like so much in The Onion \u00c1 is at once ironic, rueful and critical. Beginning from the precept that we have our head in the sand about the implications of current levels of consumption, it pastiches the right-wing, pro-corporate positions that fuel it, and, at the same time endorses the pleasurable comforts of a robustly distanced perspective that\u00a0\u2026", "num_citations": "53\n", "authors": ["542"]}
{"title": "An empirical study of amorphous slicing as a program comprehension support tool\n", "abstract": " Amorphous program slicing relaxes the syntactic constraint of traditional slicing and can therefore produce considerably smaller slices. This simplification power can be used to answer questions a software engineer might have about a program by first augmenting the program to make the question explicit and then slicing out an answer. One benefit of this technique is that the answer is in the form of a program and thus, in a language that the software engineer understands well. To test the usefulness of amorphous slicing in answering such questions, the question of array access safety is considered. A safety slice (an amorphous slice of an augmented program) is used to guide a software engineer to potential array bounds violations. A series of experiments was conducted to determine whether the safety slice was an effective aid to an engineer. 76 subjects participated in the controlled experiments. For\u00a0\u2026", "num_citations": "48\n", "authors": ["542"]}
{"title": "Analysis and visualization of predicate dependence on formal parameters and global variables\n", "abstract": " Empirical data concerning the qualitative and quantitative nature of program dependence is presented for a set of 20 programs ranging from 600 lines of code to 167,000 lines of code. The sources of dependence considered are global variables and formal parameters and the targets considered are a program's predicate nodes. The results show that as the number of formal parameters available to a predicate increases, there is a decrease in the proportion of these formal parameters which are depended upon by the predicate. No such correlation was found for global variables. Results from theoretical and actual computation time analysis indicate that the computation of dependence information is practical, suggesting that the analysis may be beneficial to several application areas. The paper also presents results concerning correlations that provide strong evidence that the global and formal dependence sources\u00a0\u2026", "num_citations": "44\n", "authors": ["542"]}
{"title": "Computing amorphous program slices using dependence graphs\n", "abstract": " This paper presents an algorithm for computing amorphous program slices. Amorphous program slicing relaxes the syntactic condition of traditional slicing and can therefore produce considerably smaller slices. Existing algorithms slice the program\u2019s control-flow graph and then apply a collection of axiomatic rules. A more unified approach is obtained using the sysrem dependence graph for both steps. Slicing this graph requires linear time. Furthermore, it contains sufficient information to allow the rules to be easily specified and applied. Some of these rules treat the system dependence graph as a data-flow graph and use a data-flow evaluation model, while others perform standard compiler optimizations. The application of amorphous slicing to the problem of determining array access safety is considered.", "num_citations": "43\n", "authors": ["542"]}
{"title": "An empirical study of rules for well\u2010formed identifiers\n", "abstract": " Readers of programs have two main sources of domain information: identifier names and comments. In order to efficiently maintain source code, it is important that the identifier names (as well as comments) communicate clearly the concepts they represent. Dei\u00dfenb\u00f6ck and Pizka recently introduced two rules for creating well\u2010formed identifiers: one considers the consistency of identifiers and the other their conciseness. These rules require a mapping from identifiers to the concepts they represent, which may be costly to develop after the initial release of a system. An approach for verifying whether identifiers are well formed without any additional information (e.g., a concept mapping) is developed. Using a pool of 48 million lines of code, experiments with the resulting syntactic rules for well\u2010formed identifiers illustrate that violations of the syntactic pattern exist. Two case studies show that three\u2010quarters of these\u00a0\u2026", "num_citations": "40\n", "authors": ["542"]}
{"title": "Identifier length and limited programmer memory\n", "abstract": " Because early variable mnemonics were limited to as few as six to eight characters, many early programmers abbreviated concepts in their variable names. The past thirty years have seen a steady increase in permitted name length and, slowly, an increase in the actual identifier length. However, in theory names can be too long for programmers to comprehend and manipulate effectively. Most obviously, in object-oriented programs, entity naming often involves chaining of method calls and field selectors (e.g., class.firstAssignment().name.trim()). While longer names bring the potential for better comprehension through more embedded sub-words, there are practical limits to their length given limited human memory resources.The driving hypothesis behind the presented study is that names used in modern programs have reached this limit. Thus, a goal of the study is to better understand the balance between longer\u00a0\u2026", "num_citations": "38\n", "authors": ["542"]}
{"title": "Formalizing executable dynamic and forward slicing\n", "abstract": " This paper uses a projection theory of slicing to formalize the definition of executable dynamic and forward program slicing. Previous definitions, when given, have been operational, and previous descriptions have been algorithmic. The projection framework is used to provide a declarative formulation in terms of the different equivalences preserved by the different forms of slicing. The analysis of dynamic slicing reveals that the slicing criterion introduced by Korel and Laski contains three inter-woven criteria. It is shown how these three conceptually distinct criteria can be disentangled to reveal two new criteria. The analysis of dynamic slicing also reveals that the subsumes relationship between static and dynamic slicing is more intricate that previous authors have claimed. Finally, the paper uses the projection theory to investigate theoretical properties of forward slicing. This is achieved by first re-formulating forward\u00a0\u2026", "num_citations": "38\n", "authors": ["542"]}
{"title": "FlagRemover: a testability transformation for transforming loop-assigned flags\n", "abstract": " Search-Based Testing is a widely studied technique for automatically generating test inputs, with the aim of reducing the cost of software engineering activities that rely upon testing. However, search-based approaches degenerate to random testing in the presence of flag variables, because flags create spikes and plateaux in the fitness landscape. Both these features are known to denote hard optimization problems for all search-based optimization techniques. Several authors have studied flag removal transformations and fitness function refinements to address the issue of flags, but the problem of loop-assigned flags remains unsolved. This article introduces a testability transformation along with a tool that transforms programs with loop-assigned flags into flag-free equivalents, so that existing search-based test data generation approaches can successfully be applied. The article presents the results of an empirical\u00a0\u2026", "num_citations": "36\n", "authors": ["542"]}
{"title": "Culture conflicts in software engineering technology transfer\n", "abstract": " Although the need to transition new technology to improve the process of developing quality software products is well understood, the computer software industry has done a poor job of carrying out that need. All too often new software technology is touted as the next\" silver bullet\" to be adopted, only to fail and disappear within a very short period. New technologies are often adopted without any convincing evidence that they will be effective, yet other technologies are ignored despite the published data that they will be useful. In this paper we discuss a study conducted among a large group of computer software professionals in order to understand what techniques can be used to support the introduction of new technologies, and to understand the biases and opinions of those charged with researching, developing or implementing those new technologies. This study indicates which evaluation techniques are viewed as most successful under various conditions. We show that the research and industrial communities do indeed have different perspectives, which leads to a conflict between the goals of the technology researchers and the needs of the technology users.", "num_citations": "33\n", "authors": ["542"]}
{"title": "Program slicing in the presence of pointers\n", "abstract": " This paper reports on computing program slices for languages with pointers. We use a variation on symbolic execution to assign addresses to pointer variables and to build lists of possible addresses contained by each pointer at each statement. The number of lists is kept small by using a concept similar to control flow based basic blocks called basic pointer blocks, a contiguous set of statements headed by an assignment to a pointer variable.", "num_citations": "33\n", "authors": ["542"]}
{"title": "Increasing diversity: Natural language measures for software fault prediction\n", "abstract": " While challenging, the ability to predict faulty modules of a program is valuable to a software project because it can reduce the cost of software development, as well as software maintenance and evolution. Three language-processing based measures are introduced and applied to the problem of fault prediction. The first measure is based on the usage of natural language in a program\u2019s identifiers. The second measure concerns the conciseness and consistency of identifiers. The third measure, referred to as the QALP score, makes use of techniques from information retrieval to judge software quality. The QALP score has been shown to correlate with human judgments of software quality.Two case studies consider the language processing measures applicability to fault prediction using two programs (one open source, one proprietary). Linear mixed-effects regression models are used to identify relationships\u00a0\u2026", "num_citations": "32\n", "authors": ["542"]}
{"title": "Software fault prediction using language processing\n", "abstract": " Accurate prediction of faulty modules reduces the cost of software development and evolution. Two case studies with a language-processing based fault prediction measure are presented. The measure, refereed to as a QALP score, makes use of techniques from information retrieval to judge software quality. The QALP score has been shown to correlate with human judgements of software quality. The two case studies consider the measure's application to fault prediction using two programs (one open source, one proprietary). Linear mixed-effects regression models are used to identify relationships between defects and QALP score. Results, while complex, show that little correlation exists in the first case study, while statistically significant correlations exists in the second. In this second study the QALP score is helpful in predicting faults in modules (files) with its usefulness growing as module size increases.", "num_citations": "32\n", "authors": ["542"]}
{"title": "Interprocedural constant propagation using dependence graphs and a data-flow model\n", "abstract": " Aggressive compilers employ a larger number of well understood optimizations in the hope of improving compiled code quality. Unfortunately, these optimizations require a variety of intermediate program representations. A first step towards unifying these optimizations to a common intermediate representation is described. The representation chosen is the program dependence graph, which captures both control-flow and data-flow information from a program.             The optimization of (interprocedural) constant propagation is studied. The algorithm developed combines a program dependence graph called the system dependence graph (SDG) with the ideas of data-flow computing and graph rewriting. The algorithm safely finds the classes of constants found by other intraprocedural and intraprocedural constant propagation algorithms. In addition, the SDG allows constants to propagate through procedures\u00a0\u2026", "num_citations": "28\n", "authors": ["542"]}
{"title": "Learning to rank improves IR in SE\n", "abstract": " Learning to Rank (LtR) encompasses a class of machine learning techniques developed to automatically learn how to better rank the documents returned for an information retrieval (IR) search. Such techniques offer great promise to software engineers because they better adapt to the wider range of differences in the documents and queries seen in software corpora. To encourage the greater use of LtR in software maintenance and evolution research, this paper explores the value that LtR brings to two common maintenance problems: feature location and traceability. When compared to the worst, median, and best models identified from among hundreds of alternative models for performing feature location, LtR ubiquitously provides a statistically significant improvement in MAP, MRR, and MnDCG scores. Looking forward a further motivation for the use of LtR is its ability to enable the development of software\u00a0\u2026", "num_citations": "27\n", "authors": ["542"]}
{"title": "Syntax-directed amorphous slicing\n", "abstract": " An amorphous slice of a program is constructed with respect to a set of variables. The amorphous slice is an executable program which preserves the behaviour of the original on the variables of interest. Unlike syntax-preserving slices, amorphous slices need not preserve a projection of the syntax of a program. This makes the task of amorphous slice construction harder, but it also often makes the result thinner and thereby preferable in applications where syntax preservation is unimportant.               This paper describes an approach to the construction of amorphous slices which is based on the Abstract Syntax Tree of the program to be sliced, and does not require the construction of control flow graphs nor of program dependence graphs. The approach has some strengths and weaknesses which the paper discusses.               The amorphous slicer, is part of the GUSTT slicing system, which includes syntax\u00a0\u2026", "num_citations": "25\n", "authors": ["542"]}
{"title": "An implementation of and experiment with semantic differencing\n", "abstract": " Software maintainers face a wide range of difficult tasks including impact analysis and regression testing. Understanding semantic relationships, such as the semantic cohesiveness in a program or the semantic differences between two programs, can help a maintainer address these problems. However, semantic analysis is a difficult problem. For example, few semantic differencing algorithms and even fewer implementations exist. The first semantic differencing implementation for the C language is presented and studied. A large collection of semantic differences of 10 programs are computed. The average size reduction was 37.70%. The study presented illustrates the practicality of semantics differencing. Finally, the application of semantic differencing in the area of program testing and impact analysis is considered.", "num_citations": "25\n", "authors": ["542"]}
{"title": "Application of the pointer state subgraph to static program slicing\n", "abstract": " A new technique for performing static analysis of programs that contain unconstrained pointers is presented. The technique is based on the pointer state subgraph: a reduced control flow graph that takes advantage of the fact that in any program there exists a smaller program that computes only the values of pointer variables. The pointer state subgraph is useful in building static analysis tools. As an example, the application of the pointer state subgraph to program slicing is considered. Finally, some experimental results, obtained using the ANSI-C slicer Unravel, are reported. These results show a clear reduction in the time taken to compute data-flow information from programs that contain pointers. They also show a substantial reduction in the space needed to store this information.", "num_citations": "25\n", "authors": ["542"]}
{"title": "Slicing in the presence of parameter aliasing\n", "abstract": " The notion of a program slice, originally introduced by Mark Weiser, is useful in program debugging, automatic parallelization, program integration, and software maintenance. A slice of a program is taken with respect to a program point p and a variable x; the slice consists of all statements of the program that might affect the value of x at point p. Prior work on interprocedural slicing, where a slice crosses the boundaries of procedure calls, has largely ignored the practical problem of aliasing (two variables are aliases if they refer to the same memory location). This papers presents an algorithm for interprocedural slicing in the presence of parameter aliases: two formal parameters that refer to the same memory location. The algorithm is parameterized by a set of parameter aliasing information. Better information yields smaller slices.", "num_citations": "25\n", "authors": ["542"]}
{"title": "Information retrieval applications in software development\n", "abstract": " Information retrieval (IR) extracts and organizes natural-language information found in unstructured text. Many of the challenges faced by software engineers can be addressed using IR techniques on the unstructured text provided by source code and its associated documents. A survey of IR-based techniques applied to software engineering challenges during the initial development process is presented. In particular, the following problems are considered: requirements discovery, maintaining software repositories, establishing traceability links, efficient software reuse, and effective software metrics. These techniques highlight the bright future that IR brings to addressing SE problems.", "num_citations": "21\n", "authors": ["542"]}
{"title": "Results from a large-scale study of performance optimization techniques for source code analyses based on graph reachability algorithms\n", "abstract": " Internally, many source code analysis tools make use of graphs. For example, one of the oldest and most widely used internal graphs is the control-flow graph developed for use within a compiler. Work on compilation has also led to the development of the call graph, the procedure dependence graph (PDG), and the static-single assignment (SSA) graph. Compilers are not the only source-code analysis tools to use internal graphs. A variety of software engineering tools incorporate a variety of different graphs. A study of techniques that improve graph-based program analysis is presented. Several different techniques are considered, including forming strongly-connected components, topological sorting, and removing transitive edges. Graph reachability, a pervasive graph analysis operation, is used as a representative graph analysis operation in the study. Data collected from a test bed of just over 1000000 lines of\u00a0\u2026", "num_citations": "21\n", "authors": ["542"]}
{"title": "Guided sampling via weak motion models and outlier sample generation for epipolar geometry estimation\n", "abstract": " The problem of automatic robust estimation of the epipolar geometry in cases where the correspondences are contaminated with a high percentage of outliers is addressed. This situation often occurs when the images have undergone a significant deformation, either due to large rotation or wide baseline of the cameras. An accelerated algorithm for the identification of the false matches between the views is presented. The algorithm generates a set of weak motion models (WMMs). Each WMM roughly approximates the motion of correspondences from one image to the other. The algorithm represents the distribution of the median of the geometric distances of a correspondence to the WMMs as a mixture model of outlier correspondences and inlier correspondences. The algorithm generates a sample of outlier correspondences from the data. This sample is used to estimate the outlier rate and to estimate the\u00a0\u2026", "num_citations": "20\n", "authors": ["542"]}
{"title": "Flow insensitive points-to sets\n", "abstract": " Pointer analysis is an important part of source code analysis. Many programs that manipulate source code take points-to sets as part of their input. Points-to related data collected from 27 mid-sized C programs (ranging in size from 1168 to 87,579 lines of code) is presented. The data shows the relative sizes and the complexities of computing points-to sets. Such data is useful in improving algorithms for the computation of points-to sets as well as algorithms that make use of this information in other operations.", "num_citations": "19\n", "authors": ["542"]}
{"title": "Multi-procedure program integration\n", "abstract": " The magnitude and complexity of existing and future software systems heightens the need for tools that assist programmers with the task of maintaining and developing software. One recurring problem that arises in system development is the need to reconcile multiple divergent lines of program development When solved by hand, this reconciliation or integration process is often tedious and error prone. A better solution is the use of a program integration tool-a tool that takes as input several variants of a base pro-gram, automatically determines the changes in each variant with respect to the base program, and incor-porates these changes, along with the portion of the base program preserved in all the variants, into a merged program.Previous algorithms that solve the program-integration problem include text-based approaches, such as that used by the UNIX diff3 utility. However, the text-based approach is\u00a0\u2026", "num_citations": "18\n", "authors": ["542"]}
{"title": "The need for software specific natural language techniques\n", "abstract": " For over two decades, software engineering (SE) researchers have been importing tools and techniques from information retrieval (IR). Initial results have been quite positive. For example, when applied to problems such as feature location or re-establishing traceability links, IR techniques work well on their own, and often even better in combination with more traditional source code analysis techniques such as static and dynamic analysis. However, recently there has been growing awareness among SE researchers that IR tools and techniques are designed to work under different assumptions than those that hold for a software system. Thus it may be beneficial to consider IR-inspired tools and techniques that are specifically designed to work with software. One aim of this work is to provide quantitative empirical evidence in support of this observation. To do so a new technique is introduced that captures\u00a0\u2026", "num_citations": "17\n", "authors": ["542"]}
{"title": "Enabling improved ir-based feature location\n", "abstract": " Recent solutions to software engineering problems have incorporated tools and techniques from information retrieval (IR). The use of IR requires choosing an appropriate retrieval model and deciding on a query that best captures a particular information need. Taking feature location as a representative example, three research questions are investigated: (1) the impact of query preprocessing, (2) the impact that different scraping techniques for queries have on retrieval performance, (3) the performance impact that the underlying retrieval model has on identifying the correct source-code functions (the correct documents). These research questions are addressed using the five open source projects released as part of the SEMERU dataset. In the experiments, five methods of scraping queries from modification requests and seven retrieval model instances are considered. Using the standard evaluation metric Mean\u00a0\u2026", "num_citations": "15\n", "authors": ["542"]}
{"title": "Theory and algorithms for slicing unstructured programs\n", "abstract": " Program slicing identifies parts of a program that potentially affect a chosen computation. It has many applications in software engineering, including maintenance, evolution and re-engineering of legacy systems. However, these systems typically contain programs with unstructured control-flow, produced using goto statements; thus, effective slicing of unstructured programs remains an important topic of study.This paper shows that slicing unstructured programs inherently requires making trade-offs between three slice attributes: termination behaviour, size, and syntactic structure. It is shown how different applications of slicing require different tradeoffs. The three attributes are used as the basis of a three-dimensional theoretical framework, which classifies slicing algorithms for unstructured programs. The paper proves that for two combinations of these dimensions, no algorithm exists and presents algorithms for the\u00a0\u2026", "num_citations": "15\n", "authors": ["542"]}
{"title": "An empirical study of the effect of semantic differences on programmer comprehension\n", "abstract": " Software engineers face a wide range of difficult tasks. Understanding semantic relationships, such as the semantic differences between two programs, should aid a software engineer address many of these tasks. A series of experiments was conducted with an implementation of a semantic differencing algorithm for the C language. Sixty-three subjects participated in two controlled experiments. There is evidence that the experimental group, which had access to semantic differences, performed significantly faster (p=0.023) and more accurately (p=0.047) than the control group. The study provides empirical support to the assertion that semantic information assists program comprehension.", "num_citations": "15\n", "authors": ["542"]}
{"title": "Vocabulary normalization improves ir-based concept location\n", "abstract": " Tool support is crucial to modern software development, evolution, and maintenance. Early tools reused the static analysis performed by the compiler. These were followed by dynamic analysis tools and more recently tools that exploit natural language. This later class has the advantage that it can incorporate not only the code, but artifacts from all phases of software construction and its subsequent evolution. Unfortunately, the natural language found in source code often uses a vocabulary different from that used in other software artifacts and thus increases the vocabulary mismatch problem. This problem exists because many natural-language tools imported from Information Retrieval (IR) and Natural Language Processing (NLP) implicitly assume the use of a single natural language vocabulary. Vocabulary normalization, which goes well beyond simple identifier splitting, brings the vocabulary of the source into\u00a0\u2026", "num_citations": "14\n", "authors": ["542"]}
{"title": "Agile and other trends in software engineering\n", "abstract": " Successfully developing and delivering multi-year, multi-person software projects remains a highly challenging task. Software engineering researchers have spent considerable energy investigating ways to improve this situation by developing various processes, techniques, and tools over the last five decades. Understanding trends in the current state of the practice is crucial to identifying the challenges that software engineers face today, the changes their organizations are tackling, and how these challenges and changes impact industrial software production. This paper reports survey results from 99 software engineering developers and managers regarding their choice of process, technique, and tools, as well as their impressions as to the contributing factors towards project success or failure. In particular, the paper includes a focus on trends in adoption of agile practices. The data reinforces some known\u00a0\u2026", "num_citations": "11\n", "authors": ["542"]}
{"title": "The impact of vocabulary normalization\n", "abstract": " Software development, evolution, and maintenance depend on ever increasing tool support. Recent tools have incorporated increasing analysis of the natural language found in source code, predominately in the identifiers and comments. However, when coders combine abbreviations and acronyms to form multi\u2010word identifiers, they, in essence, invent new vocabulary making the source code's vocabulary differ from that of other software artifacts. This vocabulary mismatch is a potential problem for many techniques imported from information retrieval and natural language processing, which implicitly assume the use of a single common vocabulary. Vocabulary normalization aims to bring the vocabulary of the source in line with that of other artifacts. A prior small\u2010scale experiment demonstrated the value of vocabulary normalization for C code. A more comprehensive experiment using Java code is presented where\u00a0\u2026", "num_citations": "11\n", "authors": ["542"]}
{"title": "Impact of limited memory resources\n", "abstract": " Since early variable mnemonics were limited to as few as six to eight characters, many early programmers abbreviated concepts in their variable names. The past thirty years has seen a steady increase in permitted name length and, slowly, an increase in the actual length of identifiers. However, in theory names can be too long. Most obviously, in object-oriented programs, names often involve chaining of method calls and field selectors (e.g., class.firstAssignment().name.trim()). While longer names bring the potential for easier comprehension through more embedded sub-words, there are practical limits to length given limited human memory resources. The central hypothesis studied herein is that names used in modern programs have reached this limit. Statistical models derived from an experiment involving 158 programmers of varying degrees of experience show that longer names extracted from production\u00a0\u2026", "num_citations": "10\n", "authors": ["542"]}
{"title": "C++ in safety critical systems\n", "abstract": " The safety and reliability of software is influenced by the choice of implementation language and the choice of programming idioms. C++ is gaining popularity as the implementation language of choice for large software projects because of its promise to reduce the complexity and cost of their construction. But is C++ an appropriate choice for such projects? An assessment of how well C++ fits into recent software guidelines for safety critical systems is presented along with a collection of techniques and idioms for the construction of safer C++ code.", "num_citations": "10\n", "authors": ["542"]}
{"title": "Applications of information retrieval to software development\n", "abstract": " Information retrieval (IR) extracts and organizes natural language information found in unstructured text. Many of the challenges faced by software engineers can be addressed using IR techniques where IR\u2019s unstructured text is taken from the source code and its associated documents. A survey of IR based techniques applied to software engineering challenges during the initial development process is presented. In particular, the following problems are considered requirements discovery, maintaining software repositories, establishing traceability links, efficient software reuse, and effective software metrics. These techniques highlight the bright future that IR brings to addressing SE problems.", "num_citations": "9\n", "authors": ["542"]}
{"title": "And-or dependence graphs for slicing statecharts\n", "abstract": " The construction of an And-Or dependence graphs is illustrated, and its use in slicing statecharts is described. The additional structure allows for more precise slices to be constructed in the event of additional information, such as may be provided by static analysis and model checking, and with constraints on the global state and external events.", "num_citations": "9\n", "authors": ["542"]}
{"title": "The feedback compiler\n", "abstract": " The feedback compiler provides information from the backend of a compiler. This information is useful for beginning programmers, experienced programmers, teaching programming, and teaching compiler construction. Activities that benefit from feedback include debugging optimized code, learning effective programming techniques, and porting programs to new architectures. Common subexpression and loop skewing are considered as example optimizations for which backend feedback is useful. Results of an experiment involving the feedback compiler are presented.", "num_citations": "9\n", "authors": ["542"]}
{"title": "Maintenance and Evolution: Information Retrieval Applications.\n", "abstract": " There is a growing interest in creating tools that can assist engineers in all phases of the software life cycle. This assistance requires techniques that go beyond traditional static and dynamic analysis. An example of such a technique is the application of information retrieval (IR), which exploits information found in a project\u2019s natural language. Such information can be extracted from the source code\u2019s identifiers and comments and in artifacts associated with the project, such as the requirements. The techniques described pertain to the maintenance and evolution phase of the software life cycle and focus on problems such as feature location and impact analysis. These techniques highlight the bright future that IR brings to addressing software engineering problems.", "num_citations": "8\n", "authors": ["542"]}
{"title": "An enabling optimization for C++ virtual functions\n", "abstract": " Computer Science Department Loyola College in Maryland 4501 N. Charles Street Baltimore, Maryland 21210-2699", "num_citations": "8\n", "authors": ["542"]}
{"title": "On the value of bug reports for retrieval-based bug localization\n", "abstract": " Software engineering researchers have been applying tools and techniques from information retrieval (IR) to problems such as bug localization to lower the manual effort required to perform maintenance tasks. The central challenge when using an IR-based tool is the formation of a high-quality query. When performing bug localization, one easily accessible source of query words is the bug report. A recent paper investigated the sufficiency of this source by using a genetic algorithm (GA) to build high quality queries. Unfortunately, the GA in essence \"cheats\" as it makes use of query performance when evolving a good query. This raises the question, is it feasible to attain similar results without \"cheating?\" One approach to providing cheat-free queries is to employ automatic summarization. The performance of the resulting summaries calls into question the sufficiency of the bug reports as a source of query words. To\u00a0\u2026", "num_citations": "7\n", "authors": ["542"]}
{"title": "Web service slicing: Intra and inter-operational analysis to test changes\n", "abstract": " We introduce Web Service Slicing, a technique that captures a functional subset of a large-scale web service using an interface slice captured as a WSDL slice (a subset of a service's WSDL). An interface (WSDL) slice provides access to an interoperable slice, which is a functional subset of the service's code. The technique uses intra-operational and inter-operational analysis to identify web service changes. With the aid of an associative code-test mapping, we leverage the identification of affected operations to reduce the cost of web-service regression testing by extracting a subset of the existing test cases. Used in conjunction with a web service slice, this subset reduces the cost of web-service regression testing by enabling the running of fewer tests. Furthermore, we exploit two approaches: Operationalized Regression Testing of Web Services (ORTWS) and Parameterized Regression Testing of Web Services\u00a0\u2026", "num_citations": "7\n", "authors": ["542"]}
{"title": "Development: Information Retrieval Applications.\n", "abstract": " Information retrieval (IR) extracts and organizes natural-language information found in unstructured text. Many of the challenges faced by software engineers can be addressed using IR techniques on the unstructured text provided by source code and its associated documents. A survey of IR-based techniques applied to software engineering (SE) challenges during the initial development process is presented. In particular, the following problems are considered: requirements discovery, maintaining software repositories, establishing traceability links, efficient software reuse, and effective software metrics. These techniques highlight the bright future that IR brings to addressing SE problems.", "num_citations": "7\n", "authors": ["542"]}
{"title": "Source code analysis with LDA\n", "abstract": " Latent Dirichlet allocation (LDA) has seen increasing use in the understanding of source code and its related artifacts in part because of its impressive modeling power. However, this expressive power comes at a cost: The technique includes several tuning parameters whose impact on the resulting LDA model must be carefully considered. The aim of this work is to provide insights into the tuning parameters' impact. Doing so improves the comprehension of both researchers who look to exploit the power of LDA in their research and those who interpret the output of LDA\u2010using tools. It is important to recognize that the goal of this work is not to establish values for the tuning parameters because there is no universal best setting. Rather, appropriate settings depend on the problem being solved, the input corpus (in this case, typically words from the source code and its supporting artifacts), and the needs of the engineer\u00a0\u2026", "num_citations": "6\n", "authors": ["542"]}
{"title": "Slicing functional programs by calculation\n", "abstract": " Program slicing is a well known family of techniques used to identify code fragments which depend on or are depended upon specific program entities. They are particularly useful in the areas of reverse engineering, program understanding, testing and software maintenance. Most slicing methods, usually targeting either the imperative or the object oriented paradigms, are based on some sort of graph structure representing program dependencies. Slicing techniques amount, therefore, to (sophisticated) graph transversal algorithms. This paper proposes a completely different approach to the slicing problem for functional programs. Instead of extracting program information to build an underlying dependencies\u00c3\u00a2 \u00e2 \u201a\u00ac \u00e2 \u201e\u00a2 structure, we resort to standard program calculation strategies, based on the so-called Bird-Meertens formalism. The slicing criterion is specified either as a projection or a hiding function which, once composed with the original program, leads to the identification of the intended slice. Going through a number of examples, the paper suggests this approach may be an interesting, even if not completely general alternative to slicing functional programs.", "num_citations": "6\n", "authors": ["542"]}
{"title": "Identifier splitting: A study of two techniques\n", "abstract": " Any time source code is analyzed, whether for maintenance or general code comprehension, identifiers play a key role. Their makeup, therefore, is very important. Each identifier can be thought of as constructed from individual words, where each word may be found in a natural language dictionary or might be an abbreviation, a programming language key word, a program-specific word, or just plain gibberish. A summary of the types of words found in the code can indicate, at a glance, how comprehensible the code will be.The problem of determining the words that make up an identifier in the absence of explicit word divisions such as underscores and camel-casing is addressed. One approach considered is a greedy algorithm. However, greedy algorithms can be time and space inefficient, making them less desirable. One interesting alternative is an artificial neural network. Neural networks are fast and have been used for speech and vision recognition, as well as a host of other patter-recognition tasks. This paper describes both a greedy algorithm and an a neural network (using the C-implementation of the Fast Artificial Neural Network) that are used to split non-well separated identifiers.", "num_citations": "5\n", "authors": ["542"]}
{"title": "A longitudinal and comparative study of slice-based metrics\n", "abstract": " Slice-based software metrics were introduced more than a decade ago. Recent advances in software tools make, for the rst time, a large-scale empirical study possible. Two results from such a study are presented: First, two longitudinal studies show that slice-based metrics quantify the deterioration of a program as it ages. This serves to validate the metrics. Second, a\\head-to-head\" qualitative and quantitative comparison of slice-based metrics provides linear models relating the dierent metrics and identies those metrics that provide similar views of a program and those that provide unique views of a program.", "num_citations": "5\n", "authors": ["542"]}
{"title": "Service evolution analytics: change and evolution mining of a distributed system\n", "abstract": " Changeability and evolvability analysis can aid an engineer tasked with a maintenance or an evolution task. This article applies  change mining  and  evolution mining  to evolving distributed systems. First, we propose a  Service Change Classifier  based Interface Slicing algorithm that mines change information from two versions of an evolving distributed system. To compare old and new versions, the following change classification labels are used: inserted, deleted, and modified. These labels are then used to identify subsets of the operations in our newly proposed Interface (WSDL) Slicing algorithm. Second, we proposed four  Service Evolution Metrics  that capture the evolution of a system's  Version Series  VS = { V 1 ,  V 2 ,\u2026, VN }. Combined the two form the basis of our proposed  Service Evolution Analytics  model, which includes learning during its development phase. We prototyped the model in an\u00a0\u2026", "num_citations": "4\n", "authors": ["542"]}
{"title": "TR 140512: on information retrieval models\n", "abstract": " This technical report reviews six different information retrieval models: vector space model with cosine similarity, vector space model with weighted sum, latent semantic indexing, query likelihood model with Dirichlet smoothing, query likelihood model with linear smoothing, and query likelihood model with Dirchlet and LDA topic model smoothing.", "num_citations": "4\n", "authors": ["542"]}
{"title": "Slicing extended finite state machines\n", "abstract": " Slicing of Extended Finite State Machines Page 1 Slicing of Extended Finite State Machines Kelly Androutsopoulos1, David Clark1, Nicolas Gold1, Mark Harman1, Rob Hierons2, Zheng Li1 and Laurence Tratt3 Centre for Research in Evolution, Search & Testing Centre for Research in Evolution, Search & Testing 1CREST, King\u2019s College London 2Brunel University 3Bournemouth University Page 2 SLIcing state based Models SLIcing state based Models EPSRC Project SLIM 2008 \u2013 2011 CREST Dependence Analysis Data Dependence Control Dependence Nontermination Sensitive Non- termination Insensitive Order Dependence Types of Slicing Dependence Based Slicing Event Restriction Slicing Correctness Semantics Syntax Applications Comprehension Model debugging Dependence Based Slicing - Dependence Analysis - Marked Transitions - \u03b5-elimination - Minimisation Event Restriction Slicing - \u2026", "num_citations": "3\n", "authors": ["542"]}
{"title": "Entropy as a lens into LDA model understanding\n", "abstract": " Latent Dirichlet Allocation (LDA) has seen increasing application to source code and its related artifacts in part because of its impressive modeling power. However, this expressive power comes at a cost: the technique includes several tuning parameters whose impact on the resulting LDA model must be carefully considered. The goal of this work is to highlight the initial promise of combined entropy as a technique for better understanding the tuning parameters' impact. Doing so aims to improve the effectiveness of both researchers seeking to exploit the power of LDA in their work and tool users who must interpret the output of LDA-using tools. Initial results obtained using seven production systems find that combined entropy shows very little variation from system to system indicating that it has strong external validity. These results also highlight how effective combined entropy can be as a lens for understanding\u00a0\u2026", "num_citations": "2\n", "authors": ["542"]}
{"title": "The qalp projects use of information retrieval techniques in software engineering\n", "abstract": " In order for information retrieval to be used in software engineering, text must be available. The most obvious sources for this text are the code, comments, and external documentation. The challenge then becomes determining the appropriate information retrieval tools and techniques to apply. The choice most often depends on the task under consideration. One task might be to facilitate the identification of code that needs modification in a maintenance task. Such a system would require a search engine that could make use of relevance models, for example. However, other tasks may benefit from the work in machine translation or summarization.Many of the projects that have incorporated information retrieval in software engineering tasks have made use of LSI, Latent Semantic Indexing. This is a technique used in the vector space model approach to information retrieval, which collapses the dimensionality of the\u00a0\u2026", "num_citations": "2\n", "authors": ["542"]}
{"title": "Impediments to software engineering technology transfer\n", "abstract": " Although the need to transition new technology to improve the process of developing quality software products is well understood, the computer software industry has done a poor job of carrying out that need. All toooften new software technology is touted as the next silver bullet to be adopted, only to fail and disappear within a very short period. New technologies are often adopted without any convincing evidence that they will be effective, yet other technologies are ignored despite the published data that they will be useful. In this paper we discuss a study conducted among a large group of computer software professionals in order to understand what techniques can be used to support the introduction of new technologies, and to understand the biases and opinions of those charged with researching, developing or implementing those new technologies. This study indicates which evaluation techniques are viewed\u00a0\u2026", "num_citations": "2\n", "authors": ["542"]}
{"title": "Crozzle: an np-complete problem\n", "abstract": " At the 1996 Symposium on Applied Computing, it was argued that the R-by-C Crozzle problem was NP-Hard, but not in NP. The original Crozzle problem is a word puzzle that appears, with a cash reward for the highest score, in The Australian Women's Weekly. The R-by-C Crozzle problem generalizes the original. We argue that both problems are, in fact, NP-Complete. This follows from the reduction of exact 3-set cover to R-by-C Crozzle and the demonstration of a non-deterministic polynomial time algorithm for solving an arbitrary instance of the R-by-C Crozzle problem. A Java implementation of this algorithm is also considered.* supported in part by Nati~) nal Science Foundation grantCCR-9411861", "num_citations": "2\n", "authors": ["542"]}
{"title": "In search of a customizable and uniform user interface\n", "abstract": " Modern software development techniques lack effective methods for building good user interfaces. Today's ad hoc methods require a significant proportion of total development time, while the resulting interfaces tend to fall short in key areas. Previous attempts to ease the burden of designing and building good user interfaces also fall short of expectations.A good user interface is both customizable and uniform. A customizable interface allows a user to change the manner in which she or he interacts with the application. A uniform interface provides the user with the same look-and-feel. Uniformity applies to interactions within a given application and to interactions throughout a set of applications.", "num_citations": "2\n", "authors": ["542"]}
{"title": "Understanding LDA for Software Engineering\n", "abstract": " Latent Dirichlet Allocation (LDA) has seen increasing use in the analysis of source code in part because of its flexibility and modeling power. However, this expressive power comes at a cost: the technique includes several tuning parameters whose impact on the resulting LDA model are not always intuitive. The goal of this work is not to establish values for the tuning parameters because doing so is extremely dependent on both the problem that is being addressed and the input corpus (typically words from the source code and its supporting artifacts). Instead, the goal of this work is to provide insight and intuition regarding the impact of the tuning parameters and thus to improve the use of LDA in source code analysis. This is done using a synthesized corpus to illustrate the influences of most of the tuning parameters on LDA models.", "num_citations": "2\n", "authors": ["542"]}
{"title": "A case for software specific natural language techniques\n", "abstract": " For over two decades, software engineering (SE) researchers have been importing tools and techniques from information retrieval (IR). Initial results have been quite positive. For example, when applied to problems such as feature location or re-establishing traceability links, IR techniques work well on their own, and often even better in combination with more traditional source code analysis techniques such as static and dynamic analysis. However, recently there has been growing awareness among SE researchers that IR tools and techniques are designed to work under a different set of assumptions than those that hold for a software system. Thus it may be beneficial to consider IR inspired tools and techniques that are specifically designed to work with software. One aim of this work is to provide quantitative empirical evidence in support of this observation. To do so a new technique is introduced that captures the\u00a0\u2026", "num_citations": "1\n", "authors": ["542"]}
{"title": "An Investigation of Hierarchical Bit Vectors\n", "abstract": " Hierarchical bit vectors represent sets of integers using a collection of bit vectors. At the top level, a single bit is set iff the set is non-empty. The bits of this next level summarize ranges of the elements. In the case of a binary hierarchical bit vector the two bits of the next level summarize two ranges: the lower half and the upper half of the possible elements. At the lowest level each bit records the membership of a particular integer.Hierarchical bit vectors find application in information retrieval, bioinformatics, non-averaging sets, and the conversion of/VMs to DMs. Competing data structures for such applications include simple bit vectors and tree-based structures such as skiplists. A comparison of hierarchical bit vectors with two other representations (simple bit vectors and binary search trees) is presented. The comparison includes both analytical and empirical analysis of the hierarchical bit vectors. The analytical results show that as the size of the set used increases, hierarchical bit vectors enjoy an advantage over tree-based structures and that as the sets used become sparser, hierarchical bit vectors perform better than standard bit vectors. The empirical results confirm that hierarchical bit vectors sit in between the other two. The empirical investigation also highlights the impact that the processor cache has on the break-even points.", "num_citations": "1\n", "authors": ["542"]}
{"title": "Dependence cluster causes\n", "abstract": " A dependence cluster is a maximal set of program components that all depend upon one another. For small programs, programmers as well as static-analysis tools can overcome the negative effects of large dependence clusters. However, this ability diminished as program size increases. Thus, the existence of large dependence clusters presents a serious challenge to the scalability of modern software. Recent ongoing work into the existence and causes of dependence clusters is presented. A better understanding of clusters and their causes is a precursor to the construction of more informed analysis tools and ideally the eventual breaking or proactive avoidance of large dependence clusters.", "num_citations": "1\n", "authors": ["542"]}
{"title": "Extending c global surveyor\n", "abstract": " Software failure are noted for their blowing large sums of money and sometimes even human life, in particular in the area of safety critical mission software. The most well-known desaster happened in 1996 when Ariane 501 exploded shortly after launch. The least it did was to cost the European space program half a billion US $ due to an overflow in an arithmetic conversion. The Automated Software Engineering Group at the NASA Ames Research Center has developed C Global Surveyor (CGS), a static analysis tool based on abstract interpretation. It particularly concentrates on runtime errors that are hard to find during development such as out-of-bound array accesses, acesses to non-initialised variables, and de-references of null pointers. CGS proved to analyse large, pointer intensive and heavily multithreaded code (up to 280 KLoC) in a couple of hours with a constant precision of 80%. It is used to successfully analyse mission-critical flight software of NASA's\" Mars Path-Finder\"(MPF) and Deep Space 1 (DS1) legacy as well as software of the Mars Exploration Rover (MER) mission (650 KLoC) and other JPL-based missions. However, the abstract interpretation techniques on which CGS is based, need to be augmented by complimentary program analysis techniques in order to enhance CGS and support the developer when analysing very large systems. As a first step, we included the construction of control flow graphs that represent the programs to be analysed. It is a first step towards the application of more advanced techniques such as program slicing.", "num_citations": "1\n", "authors": ["542"]}
{"title": "05451 Group 5--Bananas, Dark Worlds, and AspectH\n", "abstract": " This report summarises our idea of code clone detection in Haskell code and refactorings based on identified clones as it evolved in our working group-of-three discussion at the Dagstuhl seminar\" Beyond Program Slicing\".", "num_citations": "1\n", "authors": ["542"]}