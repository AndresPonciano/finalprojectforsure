{"title": "Challenges in the deployment and operation of machine learning in practice\n", "abstract": " Machine learning has recently emerged as a powerful technique to increase operational efficiency or to develop new value propositions. However, the translation of a prediction algorithm into an operationally usable machine learning model is a time-consuming and in various ways challenging task. In this work, we target to systematically elicit the challenges in deployment and operation to enable broader practical dissemination of machine learning applications. To this end, we first identify relevant challenges with a structured literature analysis. Subsequently, we conduct an interview study with machine learning practitioners across various industries, perform a qualitative content analysis, and identify challenges organized along three distinct categories as well as six overarching clusters. Eventually, results from both literature and interviews are evaluated with a comparative analysis. Key issues identified include automated strategies for data drift detection and handling, standardization of machine learning infrastructure, and appropriate communication and expectation management.", "num_citations": "36\n", "authors": ["1101"]}
{"title": "How to cope with change?-preserving validity of predictive services over time\n", "abstract": " Companies more and more rely on predictive services which are constantly monitoring and analyzing the available data streams for better service offerings. However, sudden or incremental changes in those streams are a challenge for the validity and proper functionality of the predictive service over time. We develop a framework which allows to characterize and differentiate predictive services with regard to their ongoing validity. Furthermore, this work proposes a research agenda of worthwhile research topics to improve the long-term validity of predictive services. In our work, we especially focus on different scenarios of true label availability for predictive services as well as the integration of expert knowledge. With these insights at hand, we lay an important foundation for future research in the field of valid predictive services.", "num_citations": "23\n", "authors": ["1101"]}
{"title": "Handling Concept Drifts in Regression Problems--the Error Intersection Approach\n", "abstract": " Machine learning models are omnipresent for predictions on big data. One challenge of deployed models is the change of the data over time, a phenomenon called concept drift. If not handled correctly, a concept drift can lead to significant mispredictions. We explore a novel approach for concept drift handling, which depicts a strategy to switch between the application of simple and complex machine learning models for regression tasks. We assume that the approach plays out the individual strengths of each model, switching to the simpler model if a drift occurs and switching back to the complex model for typical situations. We instantiate the approach on a real-world data set of taxi demand in New York City, which is prone to multiple drifts, e.g. the weather phenomena of blizzards, resulting in a sudden decrease of taxi demand. We are able to show that our suggested approach outperforms all regarded baselines significantly.", "num_citations": "8\n", "authors": ["1101"]}
{"title": "Handling Concept Drift for Predictions in Business Process Mining\n", "abstract": " Predictive services nowadays play an important role across all business sectors. However, deployed machine learning models are challenged by changing data streams over time which is described as concept drift. Prediction quality of models can be largely influenced by this phenomenon. Therefore, concept drift is usually handled by retraining of the model. However, current research lacks a recommendation which data should be selected for the retraining of the machine learning model. Therefore, we systematically analyze different data selection strategies in this work. Subsequently, we instantiate our findings on a use case in process mining which is strongly affected by concept drift. We can show that we can improve accuracy from 0.5400 to 0.7010 with concept drift handling. Furthermore, we depict the effects of the different data selection strategies.", "num_citations": "6\n", "authors": ["1101"]}
{"title": "Will the customers be happy? Identifying unsatisfied customers from service encounter data\n", "abstract": " PurposeWhile the understanding of customer satisfaction is a key success factor for service enterprises, existing elicitation approaches suffer from several drawbacks such as high manual effort or delayed availability. However, the rise of analytical methods allows for the automatic and instant analysis of encounter data captured during service delivery in order to identify unsatisfied customers.Design/methodology/approachBased on encounter data of 1,584 IT incidents in a real-world service use case, supervised machine learning models to predict unsatisfied customers are trained and evaluated.FindingsWe show that the identification of unsatisfied customers from encounter data is well feasible: via a logistic regression approach, we predict dissatisfied customers already with decent accuracy\u2014a substantial improvement to the current situation of \u201cflying blind\u201d. In addition, we are able to quantify the impacts of key\u00a0\u2026", "num_citations": "5\n", "authors": ["1101"]}
{"title": "How to Conduct Rigorous Supervised Machine Learning in Information Systems Research: The Supervised Machine Learning Report Card\n", "abstract": " In the last decade, applying supervised machine learning (SML) has become increasingly popular in the information systems (IS) field. However, SML results rely on many different data-preprocessing techniques, algorithms, and ways to implement them, which has contributed to an inconsistency in the way researchers have documented their SML efforts and, thus, the degree to which others can reproduce their results. In one sense, we can understand this inconsistency given the goals and motivations for SML applications vary and the research area\u2019s rapid evolution. However, for the IS research community, the inconsistency poses a big challenge because, even with full access to the data, researchers can neither completely evaluate the SML approaches that previous research has adopted or replicate previous research results. Therefore, in this paper, we provide the IS community with guidelines for comprehensively and rigorously conducting and documenting SML research. First, we review the literature concerning steps and SML process frameworks to extract relevant problem characteristics that researchers should report and relevant choices that they should make in applying SML. Second, we integrate these characteristics and choices into a comprehensive \u201cSupervised Machine Learning Report Card (SMLR)\u201d that researchers can use in future SML endeavors. Third, we apply this report card to a set of 121 relevant papers published in renowned IS outlets between 2010 and 2018 and demonstrate how and where these papers\u2019 authors could have improved their documentation and, thus, how and where researchers can better document\u00a0\u2026", "num_citations": "3\n", "authors": ["1101"]}
{"title": "Utilizing Concept Drift for Measuring the Effectiveness of Policy Interventions: The Case of the COVID-19 Pandemic\n", "abstract": " As a reaction to the high infectiousness and lethality of the COVID-19 virus, countries around the world have adopted drastic policy measures to contain the pandemic. However, it remains unclear which effect these measures, so-called non-pharmaceutical interventions (NPIs), have on the spread of the virus. In this article, we use machine learning and apply drift detection methods in a novel way to measure the effectiveness of policy interventions: We analyze the effect of NPIs on the development of daily case numbers of COVID-19 across 9 European countries and 28 US states. Our analysis shows that it takes more than two weeks on average until NPIs show a significant effect on the number of new cases. We then analyze how characteristics of each country or state, e.g., decisiveness regarding NPIs, climate or population density, influence the time lag until NPIs show their effectiveness. In our analysis, especially the timing of school closures reveals a significant effect on the development of the pandemic. This information is crucial for policy makers confronted with difficult decisions to trade off strict containment of the virus with NPI relief.", "num_citations": "2\n", "authors": ["1101"]}
{"title": "Detecting Concept Drift With Neural Network Model Uncertainty\n", "abstract": " Deployed machine learning models are confronted with the problem of changing data over time, a phenomenon also called concept drift. While existing approaches of concept drift detection already show convincing results, they require true labels as a prerequisite for successful drift detection. Especially in many real-world application scenarios-like the ones covered in this work-true labels are scarce, and their acquisition is expensive. Therefore, we introduce a new algorithm for drift detection, Uncertainty Drift Detection (UDD), which is able to detect drifts without access to true labels. Our approach is based on the uncertainty estimates provided by a deep neural network in combination with Monte Carlo Dropout. Structural changes over time are detected by applying the ADWIN technique on the uncertainty estimates, and detected drifts trigger a retraining of the prediction model. In contrast to input data-based drift detection, our approach considers the effects of the current input data on the properties of the prediction model rather than detecting change on the input data only (which can lead to unnecessary retrainings). We show that UDD outperforms other state-of-the-art strategies on two synthetic as well as ten real-world data sets for both regression and classification tasks.", "num_citations": "1\n", "authors": ["1101"]}
{"title": "Human vs. supervised machine learning: Who learns patterns faster?\n", "abstract": " The capabilities of supervised machine learning (SML), especially compared to human abilities, are being discussed in scientific research and in the usage of SML. This study provides an answer to how learning performance differs between humans and machines when there is limited training data. We have designed an experiment in which 44 humans and three different machine learning algorithms identify patterns in labeled training data and have to label instances according to the patterns they find. The results show a high dependency between performance and the underlying patterns of the task. Whereas humans perform relatively similarly across all patterns, machines show large performance differences for the various patterns in our experiment. After seeing 20 instances in the experiment, human performance does not improve anymore, which we relate to theories of cognitive overload. Machines learn slower but can reach the same level or may even outperform humans in 2 of the 4 of used patterns. However, machines need more instances compared to humans for the same results. The performance of machines is comparably lower for the other 2 patterns due to the difficulty of combining input features.", "num_citations": "1\n", "authors": ["1101"]}
{"title": "Switching Scheme: A Novel Approach for Handling Incremental Concept Drift in Real-World Data Sets\n", "abstract": " Machine learning models nowadays play a crucial role for many applications in business and industry. However, models only start adding value as soon as they are deployed into production. One challenge of deployed models is the effect of changing data over time, which is often described with the term concept drift. Due to their nature, concept drifts can severely affect the prediction performance of a machine learning system. In this work, we analyze the effects of concept drift in the context of a real-world data set. For efficient concept drift handling, we introduce the switching scheme which combines the two principles of retraining and updating of a machine learning model. Furthermore, we systematically analyze existing regular adaptation as well as triggered adaptation strategies. The switching scheme is instantiated on New York City taxi data, which is heavily influenced by changing demand patterns over time. We can show that the switching scheme outperforms all other baselines and delivers promising prediction results.", "num_citations": "1\n", "authors": ["1101"]}
{"title": "Utilizing Adaptive AI-based Information Systems to Analyze the Effectiveness of Policy Measures in the Fight of COVID-19\n", "abstract": " A novel coronavirus has been spreading around the world since early 2020. Due to its high infectiousness and lethality, countries around the world have adopted drastic policy measures to contain the pandemic\u2014 with far-reaching implications for economies and societies. We analyze the effect of two non- pharmaceutical interventions (NPI), namely closure of schools as well as a lockdown, on the development of daily case numbers of COVID-19 patients in several countries and US states. By applying Adaptive AI- based Information Systems (AAIS) based on concept drift detection, we conclude that it takes more than two weeks on average until NPIs show a significant effect on the number of new cases. This information is crucial for policymakers as they are currently confronted with difficult decisions to trade off the reopening of the economy with a strict containment of the virus.", "num_citations": "1\n", "authors": ["1101"]}