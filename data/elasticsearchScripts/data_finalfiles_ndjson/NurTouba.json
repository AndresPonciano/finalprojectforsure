{"title": "Survey of test vector compression techniques\n", "abstract": " Test data compression consists of test vector compression on the input side and response, compaction on the output side. This vector compression has been an active area of research. This article summarizes and categories these techniques. The focus is on hardware-based test vector compression techniques for scan architectures. Test vector compression schemes fall broadly into three categories: code-based schemes use data compression codes to encode test cubes; linear-decompression-based schemes decompress the data using only linear operations (that is LFSRs and XOR networks) and broadcast-scan-based schemes rely on broadcasting the same values to multiple scan chains", "num_citations": "516\n", "authors": ["503"]}
{"title": "Static compaction techniques to control scan vector power dissipation\n", "abstract": " Excessive switching activity during scan testing can cause average power dissipation and peak power during test to be much higher than during normal operation. This can cause problems both with heat dissipation and with current spikes. Compacting scan vectors greatly increases the power dissipation for the vectors (generally the power becomes several times greater). The compacted scan vectors often can exceed the power constraints and hence cannot be used. It is shown here that by carefully selecting the order in which pairs of test cubes are merged during static compaction, both average power and peak power for the final test set can be greatly reduced. A static compaction procedure is presented that can be used to find a minimal set of scan vectors that satisfies constraints on both average power and peak power. The proposed approach is simple yet effective and can be easily implemented in the\u00a0\u2026", "num_citations": "463\n", "authors": ["503"]}
{"title": "Scan vector compression/decompression using statistical coding\n", "abstract": " A compression/decompression scheme based on statistical coding is presented for reducing the amount of test data that must be stored on a tester and transferred to each core in a core-based design. The test vectors provided by the core vendor are stored in compressed form in the tester memory and transferred to the chip where they are decompressed and applied to the core. Given the set of test vectors for a core, a statistical code is carefully selected so that it satisfies certain properties. These properties guarantee that it can be decoded by a simple pipelined decoder (placed at the serial input of the core's scan chain) which requires very small area. Results indicate that the proposed scheme can use a simple decoder to provide test data compression near that of an optimal Huffman code. The compression results in a two-fold advantage since both test storage and test time are reduced.", "num_citations": "399\n", "authors": ["503"]}
{"title": "Test vector decompression via cyclical scan chains and its application to testing core-based designs\n", "abstract": " A novel test vector compression/decompression technique is proposed for reducing the amount of test data that must be stored on a tester and transferred to each core when testing a core-based design. A small amount of on-chip circuitry is used to reduce both the test storage and test time required for testing a core-based design. The fully specified test vectors provided by the core vendor are stored in compressed form in the tester memory and transferred to the chip where they are decompressed and applied to the core (the compression is lossless). Instead of having to transfer each entire test vector from the tester to the core, a smaller amount of compressed data is transferred instead. This reduces the amount of test data that must be stored on the tester and hence reduces the total amount of test time required for transferring the data with a given test data bandwidth.", "num_citations": "372\n", "authors": ["503"]}
{"title": "Cost-effective approach for reducing soft error failure rate in logic circuits\n", "abstract": " In this paper, a new paradigm for designing logic circuits with concurrent error detection (CED) is described. The key idea is to exploit the asymmetric soft error susceptibility of nodes in a logic circuit. Rather than target all modeled faults, CED is targeted towards the nodes that have the highest soft error susceptibility to achieve cost-effective tradeoffs between overhead and reduction in the soft error failure rate. Under this new paradigm, we present one particular approach that is based on partial duplication and show that it is capable of reducing the soft error failure rate significantly with a fraction of the overhead required for full duplication. A procedure for characterizing the soft error susceptibility of nodes in a logic circuit, and a heuristic procedure for selecting the set of nodes for partial duplication are described. A full set of experimental results demonstrate the cost-effective tradeoffs that can be achieved.", "num_citations": "361\n", "authors": ["503"]}
{"title": "An efficient test vector compression scheme using selective Huffman coding\n", "abstract": " This paper presents a compression/decompression scheme based on selective Huffman coding for reducing the amount of test data that must be stored on a tester and transferred to each core in a system-on-a-chip (SOC) during manufacturing test. The test data bandwidth between the tester and the SOC is a bottleneck that can result in long test times when testing complex SOCs that contain many cores. In the proposed scheme, the test vectors for the SOC are stored in compressed form in the tester memory and transferred to the chip where they are decompressed and applied to the cores. A small amount of on-chip circuitry is used to decompress the test vectors. Given the set of test vectors for a core, a modified Huffman code is carefully selected so that it satisfies certain properties. These properties guarantee that the codewords can be decoded by a simple pipelined decoder (placed at the serial input of the core\u00a0\u2026", "num_citations": "354\n", "authors": ["503"]}
{"title": "Test vector encoding using partial LFSR reseeding\n", "abstract": " A new form of LFSR reseeding that provides higher encoding efficiency and hence greater reduction in test data storage requirements is described. Previous forms of LFSR reseeding have been static (i.e. test generation is stopped and the seed is loaded at one time) and have required full reseeding (i.e. n=r bits are used for an r-bit LFSR). The new form of LFSR reseeding proposed here is dynamic (i.e. the seed is incrementally modified while test generation proceeds) and allows partial reseeding (i.e. n<r bits can be used). Full static forms of LFSR reseeding are shown to be a special case of the new partial dynamic form of LFSR reseeding. In addition to providing better encoding efficiency, partial dynamic LFSR reseeding has a simpler hardware implementation than previous schemes based on multiple-polynomial LFSRs, and can generate each test vector in fewer clock cycles. Experimental results demonstrate\u00a0\u2026", "num_citations": "297\n", "authors": ["503"]}
{"title": "Altering a pseudo-random bit sequence for scan-based BIST\n", "abstract": " This paper presents a low-overhead scheme for the built-in self-test (BIST) of circuits with scan. Complete (100%) fault coverage is obtained without modifying the function logic and without degrading system performance (beyond using scan). Deterministic test cubes that detect the random-pattern-resistant faults are embedded in a pseudo-random sequence of bits generated by a linear feedback shift register (LFSR). This is accomplished by altering the pseudo-random sequence by adding logic at the LFSR's serial output to \"fix\" certain bits. A procedure for synthesizing the bit-fixing logic for embedding the test cubes is described. Experimental results indicate that complete fault coverage can be obtained with low hardware overhead. Also, the proposed approach permits the use of small LFSRs for generating the pseudo-random bit sequence. The faults that are not detected because of linear dependencies in the\u00a0\u2026", "num_citations": "247\n", "authors": ["503"]}
{"title": "Reducing test data volume using LFSR reseeding with seed compression\n", "abstract": " A new lossless test vector compression scheme is presented which combines linear feedback shift register (LFSR) reseeding and statistical coding in a powerful way. Test vectors can be encoded as LFSR seeds by solving a system of linear equations. The solution space of the linear equations can be quite large. The proposed method takes advantage of this large solution space to find seeds that can be efficiently encoded using a statistical code. Two architectures for implementing LFSR reseeding with seed compression are described. One configures the scan cells themselves to perform the LFSR functionality while the other uses a new idea of \"scan windows\" to allow the use of a small separate LFSR whose size is independent of the number of scan cells. The proposed scheme can be used either for applying a fully deterministic test set or for mixed-mode built-in self-test (BIST), and it can be used in conjunction\u00a0\u2026", "num_citations": "231\n", "authors": ["503"]}
{"title": "Synthesis of circuits with low-cost concurrent error detection based on Bose-Lin codes\n", "abstract": " This paper presents a procedure for synthesizing sequential machines with concurrent error detection based on Bose-Lin codes. Bose-Lin codes are an efficient solution for providing concurrent error detection as they are separable codes and have a fixed number of check bits, independent of the number of information bits. Furthermore, Bose-Lin code checkers have a simple structure as they are based on modulo operations. Procedures are described for synthesizing circuits in a way that their structure ensures that all single-point faults can only cause errors that are detected by a Bose-Lin code. This paper presents an efficient scheme for concurrent error detection in sequential circuits with no constraint on the state encoding. Concurrent error detection for both the state bits and the output bits is based on a Bose-Lin code and their checking is combined such that one checker suffices. Results indicate low\u00a0\u2026", "num_citations": "225\n", "authors": ["503"]}
{"title": "Logic synthesis of multilevel circuits with concurrent error detection\n", "abstract": " This paper presents a procedure for synthesizing multilevel circuits with concurrent error detection. All errors caused by single stuck-at faults are detected using a parity-check code. The synthesis procedure (implemented in Stanford CRCs TOPS synthesis system) fully automates the design process, and reduces the cost of concurrent error detection compared with previous methods. An algorithm for selecting a good parity-check code for encoding the circuit outputs is described. Once the code has been selected, a new procedure called structure-constrained logic optimization is used to minimize the area of the circuit as much as possible while still using a circuit structure that ensures that single stuck-at faults cannot produce undetected errors. It is proven that the resulting implementation is path fault secure, and when augmented by a checker, forms a self-checking circuit. The actual layout areas required for self\u00a0\u2026", "num_citations": "221\n", "authors": ["503"]}
{"title": "Multiple bit upset tolerant memory using a selective cycle avoidance based SEC-DED-DAEC code\n", "abstract": " Conventional error correcting code (ECC) schemes used in memories and caches cannot correct double bit errors caused by a single event upset (SEU). As memory density increases, multiple bit upsets in nearby cells become more frequent. A methodology is proposed here for deriving an error correcting code through heuristic search that can detect and correct the most likely double bit errors in a memory while minimizing the miscorrection probability of the unlikely double bit errors. A key feature of the proposed ECC is that it uses the same number of check bits as the conventional single error correcting/double error detecting (SEC-DED) codes commonly used, and has nearly identical syndrome generator/encoder area and timing overhead. Hence, there is very little additional cost to using the proposed ECC. The proposed ECC can be used instead of or in addition to bit interleaving to provide greater flexibility\u00a0\u2026", "num_citations": "213\n", "authors": ["503"]}
{"title": "Reducing power dissipation during test using scan chain disable\n", "abstract": " A novel approach for minimizing power during scan testing is presented. The idea is that given a full scan module or core that has multiple scan chains, the test set is generated and ordered in such a way that some of the scan chains can have their clock disabled for portions of the test set. Disabling the clock prevents flip-flops from transitioning, and hence reduces switching activity in the circuit. Moreover, disabling the clock also reduces power dissipation in the clock tree which often is a major source of power. The only hardware modification that is required to implement this approach is to add the capability for the tester to gate the clock for one subset of the scan chains in the core. A procedure for generating and ordering the test set to maximize the we of scan disable is described. Experimental results are shown indicating that the proposed approach can significantly reduce both logic and clock power during testing.", "num_citations": "196\n", "authors": ["503"]}
{"title": "Reducing test data volume using external/LBIST hybrid test patterns\n", "abstract": " A common approach for large industrial designs is to use logic built-in self-test (LBIST) followed by test data from an external tester. Because the fault coverage with LBIST alone is not sufficient, there is a need to top-up the fault coverage with additional deterministic test patterns from an external tester. This paper proposes a technique of combining LBIST and deterministic ATPG to form \"hybrid test patterns\" which merge pseudo-random and deterministic test data. Experiments have been done on the Motorola PowerPC/sup TM/ microprocessor core to study the proposed hybrid test patterns. Hybrid test patterns provide several advantages: (1) can be applied using STUMPS architecture (Bardell, 82) with a minor modification, (2) significantly reduce external test data stored in tester memory, (3) reduce the number of pseudorandom patterns by orders of magnitude, thus addressing power issues.", "num_citations": "153\n", "authors": ["503"]}
{"title": "Test point insertion based on path tracing\n", "abstract": " This paper presents an innovative method for inserting test points in the circuit-under-test to obtain complete fault coverage for a specified set of test patterns. Rather than using probabilistic techniques for test point placement, a path tracing procedure is used to place both control and observation points. Rather than adding extra scan elements to drive the control points, a few of the existing primary inputs to the circuit are ANDed together to form signals that drive the control points. By selecting which patterns the control point is activated for, the effectiveness of each control point is maximized. A comparison is made with the best previously published results for other test point insertion methods, and it is shown that the proposed method requires fewer test points and less overhead to achieve the same or better fault coverage.", "num_citations": "151\n", "authors": ["503"]}
{"title": "Weight-based codes and their application to concurrent error detection of multilevel circuits\n", "abstract": " This paper proposes a new class of codes termed \"weight-based codes\" where each output bit is assigned a weight and the check bits represent the stem of the weights of the output bits which have value \"1\". A Berger code is a special member of this proposed class of codes where each output bit is assigned a weight of one. This paper describes the application of these codes for the efficient on-line error detection of arbitrary multilevel circuits. The overall probability of detecting any number of erroneous bits at the output caused by a single internal fault is shown to be higher for weight-based codes than standard error detecting codes. Further, a very efficient design exists for the checker. The checker is area and speed efficient, has low power consumption, and can be tested by a small set of incoming code words. There is always a tradeoff between the fault detection capability and area overhead requirement of an\u00a0\u2026", "num_citations": "149\n", "authors": ["503"]}
{"title": "Controlling peak power during scan testing\n", "abstract": " This paper presents a procedure for modifying a given set of scan vectors so that the peak power during scan testing is kept below a specified limit without reducing fault coverage. The proposed approach works for any conventional full-scan design-no extra design-for-test (DFT) logic is required. If the peak power in a clock cycle during scan testing exceeds a specified limit (which depends on the amount of peak power that can be safely handled without causing a failure that would not occur during normal functional operation) then a \"peak power violation\" occurs. Given a set of scan vectors, simulation is done to identify and classify the scan vectors that are causing peak power violations during scan testing. The problem scan vectors are then modified in a way that eliminates the peak power violations while preserving the fault coverage. Experimental results indicate the proposed procedure is very effective in\u00a0\u2026", "num_citations": "140\n", "authors": ["503"]}
{"title": "Virtual scan chains: A means for reducing scan length in cores\n", "abstract": " A novel design-for-test (DFT) technique is presented for designing a core with a \"virtual scan chain\" which looks (to the system integrator) like it is shorter than the real scan chain inside the core. The I/O pins of a core with a virtual scan chain are identical to the I/O pins of a core with a normal scan chain. For the system integrator, testing a core with a virtual scan chain is identical to testing a core with a normal scan chain. The only difference is that the virtual scan chain is much shorter so the size of the scan vectors and output response is smaller resulting in less test data and fewer scan shift cycles. The process of mapping the virtual scan vectors to real scan vectors is handled inside the core and is completely transparent to the system integrator. It is done by using LFSRs to \"expand\" the shorter virtual test vector into a full test vector. Results indicate that virtual scan chains can be designed which are several times\u00a0\u2026", "num_citations": "135\n", "authors": ["503"]}
{"title": "Test data compression using dictionaries with selective entries and fixed-length indices\n", "abstract": " We present a dictionary-based test data compression approach for reducing test data volume in SOCs. The proposed method is based on the use of a small number of ATE channels to deliver compressed test patterns from the tester to the chip and to drive a large number of internal scan chains in the circuit under test. Therefore, it is especially suitable for a reduced pin-count and low-cost DFT test environment, where a narrow interface between the tester and the SOC is desirable. The dictionary-based approach not only reduces test data volume but it also eliminates the need for additional synchronization and handshaking between the SOC and the ATE. The dictionary entries are determined during the compression procedure by solving a variant of the well-known clique partitioning problem from graph theory. Experimental results for the ISCAS-89 benchmarks and representative test data from IBM show that the\u00a0\u2026", "num_citations": "128\n", "authors": ["503"]}
{"title": "Partial error masking to reduce soft error failure rate in logic circuits\n", "abstract": " A new methodology for designing logic circuits with partial error masking is described. The key idea is to exploit the asymmetric soft error susceptibility of nodes in a logic circuit by targeting the error masking capability towards the nodes with the highest soft error susceptibility to achieve cost-effective tradeoffs between overhead and reduction in the soft error failure rate. Such techniques can be used in cost-sensitive high volume mainstream applications to satisfy soft error failure rate requirements at minimum cost. Two reduction heuristics, cluster sharing reduction and dominant value reduction, are used to reduce the soft error failure rate significantly with a fraction of the overhead required for conventional TMR.", "num_citations": "123\n", "authors": ["503"]}
{"title": "Bit-fixing in pseudorandom sequences for scan BIST\n", "abstract": " A low-overhead scheme for achieving complete (100%) fault coverage during built-in self test of circuits with scan is presented. It does not require modifying the function logic and does not degrade system performance (beyond using scan). Deterministic test cubes that detect the random-pattern-resistant (r.p.r.) faults are embedded in a pseudorandom sequence of bits generated by a linear feedback shift register (LFSR). This is accomplished by altering the pseudorandom sequence by adding logic at the LFSR's serial output to \"fix\" certain bits. A procedure for synthesizing the bit-fixing logic for embedding the test cubes is described. Experimental results indicate that complete fault coverage can be obtained with low hardware overhead. Further reduction in overhead is possible by using a special correlating automatic test pattern generation procedure that is described for finding test cubes for the r.p.r. faults in a way\u00a0\u2026", "num_citations": "120\n", "authors": ["503"]}
{"title": "Testing embedded cores using partial isolation rings\n", "abstract": " Intellectual property cores pose a significant test challenge. The core supplier may not give any information about the internal logic of the core, but simply provide a set of test vectors for the core which guarantees a particular fault coverage. If the core is embedded within a larger design, then the problem is how to apply the specified test vectors to the core and how to test the user-defined logic around the core. A simple and fast solution is to place a full isolation ring (i.e., boundary scan) around the core, however, the area and performance overhead for this may not be acceptable in many applications. This paper presents a systematic method for designing a partial isolation ring that provides the same fault coverage as a full isolation ring, but avoids adding MUXes on critical timing paths and reduces area overhead. Efficient ATPG techniques are used to analyze the user-defined logic surrounding the core and identify\u00a0\u2026", "num_citations": "117\n", "authors": ["503"]}
{"title": "Low cost concurrent error detection based on module weight-based codes\n", "abstract": " This paper explains the concept of \"modulo weight-based codes\" and presents an efficient scheme for concurrent error detection of arbitrary multilevel circuits based on these codes. It is shown that in most cases modulo weight-based codes can achieve high fault detection capabilities of around 99% with just three check bits and without constraining the circuit structure. Modulo weight-based codes present a way of exploiting the circuit structure to increase fault detection capability. An optimum choice of weights for the code can be made using information about the circuit structure. The paper presents an efficient algorithm to achieve the weight assignment based on simple estimates of the probabilities of the existence of sensitized paths from the signal line to the circuit outputs. Finally, a self-checking checker design is presented for the proposed module weight-based codes.", "num_citations": "115\n", "authors": ["503"]}
{"title": "LFSR-reseeding scheme achieving low-power dissipation during test\n", "abstract": " This paper presents a new low-power test-data-compression scheme based on linear feedback shift register (LFSR) reseeding. A drawback of compression schemes based on LFSR reseeding is that the unspecified bits are filled with random values, which results in a large number of transitions during scan-in, thereby causing high-power dissipation. A new encoding scheme that can be used in conjunction with any LFSR-reseeding scheme to significantly reduce test power and even further reduce test storage is presented. The proposed encoding scheme acts as the second stage of compression after LFSR reseeding. It accomplishes two goals. First, it reduces the number of transitions in the scan chains (by filling the unspecified bits in a different manner). Second, it reduces the number of specified bits that need to be generated via LFSR reseeding. Experimental results indicate that the proposed method\u00a0\u2026", "num_citations": "114\n", "authors": ["503"]}
{"title": "Synthesis of mapping logic for generating transformed pseudo-random patterns for BIST\n", "abstract": " During built-in self-test (BIST), the set of patterns generated by a pseudo-random pattern generator may not provide a sufficiently high fault coverage. This paper presents a new technique for synthesizing combinational mapping logic to transform the set of patterns that are generated. The goal is to satisfy test length and fault coverage requirements while minimizing area overhead. For a given pseudo-random pattern generator and circuit under test, there are many possible mapping functions that will provide a desired fault coverage for a given test length. This paper formulates the problem of finding a mapping function that can be implemented with a small number of gates as a one of finding a minimum rectangle cover in a binate matrix. A procedure is described for selecting a mapping function and synthesizing mapping logic to implement it. Experimental results for the procedure are compared with published\u00a0\u2026", "num_citations": "114\n", "authors": ["503"]}
{"title": "Improving logic obfuscation via logic cone analysis\n", "abstract": " Logic obfuscation can protect designs from reverse engineering and IP piracy. In this paper, a new attack strategy based on applying brute force iteratively to each logic cone is described and shown to significantly reduce the number of brute force key combinations that need to be tried by an attacker. It is shown that inserting key gates based on MUXes is an effective approach to increase security against this type of attack. Experimental results are presented quantifying the threat posed by this type of attack along with the relative effectiveness of MUX key gates in countering it.", "num_citations": "103\n", "authors": ["503"]}
{"title": "Transformed pseudo-random patterns for BIST\n", "abstract": " This paper presents a new approach for on-chip test pattern generation. The set of test patterns generated by a pseudo-random pattern generator (e.g., an LFSR) is transformed into a new set of patterns that provides the desired fault coverage. The transformation is performed by a small amount of mapping logic that decodes sets of patterns that don't detect any new faults and maps them into patterns that detect the hard-to-detect faults. The mapping logic is purely combinational and is placed between the pseudo-random pattern generator and the circuit under test (CUT). A procedure for designing the mapping logic so that it satisfies test length and fault coverage requirements is described. Results are shown for benchmark circuits which indicate that an LFSR plus a small amount of mapping logic reduces the test length required for a particular fault coverage by orders of magnitude compared with using an LFSR\u00a0\u2026", "num_citations": "100\n", "authors": ["503"]}
{"title": "Synthesis of zero-aliasing elementary-tree space compactors\n", "abstract": " A new method is presented for designing space compactors for either deterministic testing or pseudo-random testing. A tree of elementary gates (AND, OR, NAND, NOR) is used to combine the outputs of the circuit-under-test (CUT) in a way that zero-aliasing is guaranteed with no modification of the CUT. The elementary-tree is synthesized by adding one gate at a time without introducing redundancy. The end result is a cascaded network CUT followed by space compactor, that is irredundant and has fewer outputs than the CUT alone. All faults in the CUT and space compactor can be tested. Only the outputs of the space compactor need to be observed during testing. Experimental results are surprising; they show that very high compaction ratios can be achieved with zero-aliasing elementary-tree space compactors. Compared with parity trees and other space compactor designs that have been proposed, the\u00a0\u2026", "num_citations": "89\n", "authors": ["503"]}
{"title": "Inserting test points to control peak power during scan testing\n", "abstract": " This paper presents a procedure for inserting test points at the outputs of scan elements of a full-scan circuit in such a manner that the peak power during scan testing is kept below a specified limit while maintaining the original fault coverage. If the power in a clock cycle during scan testing exceeds a specified limit (which depends on the peak power the chip has been designed to supply), a \"peak power violation\" is said to occur. Given a set of vectors, simulation is used to identify the cycles in which peak power violations occur (called \"violating cycles\"). For each violating cycle, the reduction in power caused by a control-0 and control-1 test point at each scan element is determined by simulation. The optimization problem then is to select as few test points as possible to eliminate all violating cycles. We present a heuristic procedure for minimizing the number of test points using integer linear programming techniques\u00a0\u2026", "num_citations": "85\n", "authors": ["503"]}
{"title": "X-canceling MISR\u2014An X-tolerant methodology for compacting output responses with unknowns using a MISR\n", "abstract": " A new X-tolerant multiple-input signature register (MISR) compaction methodology is proposed which can compact output streams containing unknown (X) values. Unlike conventional X-masking approaches, it does not require any masking logic at the input of the MISR. Instead it uses symbolic simulation to express each bit of the MISR signature as a linear equation in terms of the X's. Linearly dependent combinations of the signature bits are identified with Gaussian elimination and XORed together using a programmable XOR to cancel out all X values thereby yielding deterministic values that are invariant of what the final values of the X's end up being during the test. These X-canceled values can be compacted in a separate MISR to generate a final X-free signature. Each intermediate signature for an m-bit MISR can tolerate k X's present anywhere in the output stream with error detection capability equivalent to\u00a0\u2026", "num_citations": "84\n", "authors": ["503"]}
{"title": "Low power test data compression based on LFSR reseeding\n", "abstract": " Many test data compression schemes are based on LFSR reseeding. A drawback of these schemes is that the unspecified bits are filled with random values resulting in a large number of transitions during scan-in thereby causing high power dissipation. This paper presents a new encoding scheme that can be used in conjunction with any LFSR reseeding scheme to significantly reduce test power and even further reduce test storage. The proposed encoding scheme acts as a second stage of compression after LFSR reseeding. It accomplishes two goals. First, it reduces the number of transitions in the scan chains (by filling the unspecified bits in a different manner), and second, it reduces the number of specified bits that need to be generated via LFSR reseeding. Experimental results indicate that the proposed method significantly reduces test power and in most cases provides greater test data compression than\u00a0\u2026", "num_citations": "83\n", "authors": ["503"]}
{"title": "Relationship between entropy and test data compression\n", "abstract": " The entropy of a set of data is a measure of the amount of information contained in it. Entropy calculations for fully specified data have been used to get a theoretical bound on how much that data can be compressed. This paper extends the concept of entropy for incompletely specified test data (i.e., that has unspecified or don't care bits) and explores the use of entropy to show how bounds on the maximum amount of compression for a particular symbol partitioning can be calculated. The impact of different ways of partitioning the test data into symbols on entropy is studied. For a class of partitions that use fixed-length symbols, a greedy algorithm for specifying the don't cares to reduce entropy is described. It is shown to be equivalent to the minimum entropy set cover problem and thus is within an additive constant error with respect to the minimum entropy possible among all ways of specifying the don't cares. A\u00a0\u2026", "num_citations": "81\n", "authors": ["503"]}
{"title": "Using an embedded processor for efficient deterministic testing of systems-on-a-chip\n", "abstract": " If a system-on-a-chip (SOC) contains an embedded processor, the paper presents a novel approach for using the processor to aid in testing the other components of the SOC. The basic idea is that the tester loads a program along with compressed test data into the processor's on-chip memory. The processor executes the program which decompresses the test data and applies it to scan chains in the other components of the SOC to test them. This approach both reduces the amount of data that must be stored on the tester and reduces the test time. Moreover, it enables at-speed scan shifting even with a slow tester (i.e. a tester whose maximum clock rate is slower than the SOC's normal operating clock rate). A procedure is described for converting a set of test cubes (i.e., test vectors where unspecified inputs are left as X's) into a compressed form. A program that can be run on an embedded processor is given for\u00a0\u2026", "num_citations": "76\n", "authors": ["503"]}
{"title": "Weighted pseudorandom hybrid BIST\n", "abstract": " This paper presents a new test data-compression scheme that is a hybrid approach between external testing and built-in self-test (BIST). The proposed approach is based on weighted pseudorandom testing and uses a novel approach for compressing and storing the weight sets. Three levels of compression are used to greatly reduce test costs. Experimental results show that the proposed scheme reduces tester storage requirements and tester bandwidth requirements by orders of magnitude compared to conventional external testing, but requires much less area overhead than a full BIST implementation providing the same fault coverage. No test points or any modifications are made to the function logic. The paper describes the proposed hybrid BIST architecture as well as two different ways of storing the weight sets, which are an integral part of this scheme.", "num_citations": "75\n", "authors": ["503"]}
{"title": "Fault diagnosis in scan-based BIST using both time and space information\n", "abstract": " A new technique for diagnosis in a scan-based BIST environment is presented. It allows non-adaptive identification of both the scan cells that capture errors (space information) as well as a subset of the failing test vectors (time information). Having both space and time information allows a faster and more precise diagnosis. Previous techniques for identifying the failing test vectors during BIST have been limited in the multiplicity of errors that can be handled and/or require a very large hardware overhead. The proposed approach, however, uses only two cycling registers at the output of the scan chain to accurately identify a subset of the failing BIST test vectors. This is accomplished using some novel pruning techniques that efficiently extract information from the signatures of the cycling registers. While not all the failing BIST test vectors can be identified, results indicate that a significant number of them can be. This\u00a0\u2026", "num_citations": "75\n", "authors": ["503"]}
{"title": "Altering bit sequences to contain predetermined patterns\n", "abstract": " A low-overhead scheme for built-in self-test of digital designs incorporating scan allows for complete (100%) fault coverage without modifying the function logic and without degrading system performance (beyond using scan). By altering a pseudo-random bit sequence with bit-fixing logic at an LFSR's serial output, deterministic test cubes that detect random pattern-resistant faults are generated. A procedure for synthesizing the bit-fixing logic allows for complete fault coverage with low hardware overhead. Also, the present approach permits the use of small LFSR's for generating the pseudo-random bit sequence. The faults that are not detected because of linear dependencies in the LFSR can be detected by generating more deterministic cubes at the expense of additional bit-fixing logic.", "num_citations": "72\n", "authors": ["503"]}
{"title": "Adjustable width linear combinational scan vector decompression\n", "abstract": " A new scheme for combinational linear expansion is proposed for decompression of scan vectors. It has the capability to adjust the width of the linear expansion each clock cycle. This eliminates the requirement that every scan bit-slice be in the output space of the linear decompressor. Depending on how specified the current bit-slice is, the decompressor may load all scan chains or may load only a subset of the scan chains. This provides the nice feature that any scan vector can be generated using the proposed scheme regardless of the number or distribution of the specified bits. Thus, the proposed scheme allows the use of any ATPG procedure without any constraints. Moreover, it allows greater compression to be achieved than fixed width expansion techniques since the ratio of the number of scan chains to the number of tester channels can be scaled much larger. A procedure for designing and optimizing the\u00a0\u2026", "num_citations": "71\n", "authors": ["503"]}
{"title": "Synthesis of low power CED circuits based on parity codes\n", "abstract": " An automated design procedure is described for synthesizing circuits with low power concurrent error detection. It is based on pre-synthesis selection of a parity-check code followed by structure constrained logic optimization that produces a circuit in which all single point faults are guaranteed to be detected. Two new contributions over previous work include (1) the use of a k-way partitioning algorithm combined with local search to select a parity-check code, and (2) a methodology for minimizing power consumption in the CED circuitry. Results indicate significant reductions in area overhead due to the new code selection procedure as well as the ability to find low power implementations for use in power conscious applications.", "num_citations": "70\n", "authors": ["503"]}
{"title": "A rapid and scalable diagnosis scheme for BIST environments with a large number of scan chains\n", "abstract": " This paper presents a rapid and scalable built-in-self-test (BIST) diagnosis scheme for handling BIST environments with a large number of scan chains. The problem of identifying which scan cells captured errors during the BIST session is formulated here as a search problem. A scheme for adding a small amount of additional hardware that provides the capability of performing very efficient search techniques to locate the error-capturing scan cells is proposed. The scheme can accurately diagnose any number of error-capturing scan cells. The error-capturing scan cells can be located in time complexity that is logarithmic in the total number of scan cells in the design using the proposed approach. The technique scales well for very large designs. The hardware overhead is logarithmic in the number of scan cells and linear in the number of scan chains.", "num_citations": "68\n", "authors": ["503"]}
{"title": "A low cost approach for detecting, locating, and avoiding interconnect faults in FPGA-based reconfigurable systems\n", "abstract": " An FPGA-based reconfigurable system may contain boards of FPGAs which are reconfigured for different applications and must work correctly. This paper presents a novel approach for rapidly testing the interconnect in the FPGAs each time the system is reconfigured. A low-cost configuration-dependent test method is used to both detect and locate faults in the interconnect. The \"original configuration\" is modified by only changing the logic function of the CLBs to form \"test configurations\" that can be used to quickly test the interconnect using the \"walking-1\" approach. The test procedure is rapid enough to be performed on the fly whenever the system is reconfigured. All stuck-at faults and bridging faults in the interconnect are guaranteed to be detected and located with a short test length. The fault location information can he used to reconfigure the system to avoid the faulty hardware.", "num_citations": "67\n", "authors": ["503"]}
{"title": "Expanding trace buffer observation window for in-system silicon debug through selective capture\n", "abstract": " Trace buffers are commonly used to capture data during in-system silicon debug. This paper exploits the fact that it is not necessary to capture error-free data in the trace buffer since that information is obtainable from simulation. The trace buffer need only capture data during clock cycles in which errors are present. A three pass methodology is proposed. During the first pass, the rough error rate is measured, in the second pass, a set of suspect clock cycles where errors may be present is determined, and then in the third pass, the trace buffer captures only during the suspect clock cycles. In this manner, the effective observation window of the trace buffer can be expanded significantly, by up to orders of magnitude. This greatly increases the effectiveness of a given size trace buffer and can rapidly speed up the debug process. The suspect clock cycles are determined through a two dimensional (2-D) compaction\u00a0\u2026", "num_citations": "65\n", "authors": ["503"]}
{"title": "Automated selection of signals to observe for efficient silicon debug\n", "abstract": " Internal signals of a circuit are observed to analyze, understand, and debug nonconforming chip behavior. The number of signals that can be observed is limited by bandwidth and storage requirements. This paper presents an automated procedure to select which signals to observe to facilitate early detection of circuit malfunction to help find the root cause of a bug. This paper exploits the nature of error propagation in sequential circuits by observing signals which are most often sensitized to possible errors. Given a functional input vector set, an error transmission matrix is generated by analyzing which flip-flops are sensitized to other flip-flops. Signal observability is enhanced by merging data from relatively independent flip-flops. The final set of signals to observe is determined through integer linear programming (ILP) which provides a set of locations that maximally cover the possible error sites within given\u00a0\u2026", "num_citations": "59\n", "authors": ["503"]}
{"title": "Reliable network-on-chip using a low cost unequal error protection code\n", "abstract": " The network-on-chip (NoC) paradigm is seen as a way of facilitating the integration of a large number of computational and storage blocks on a chip to meet several performance and power constraints. However due to continued scaling of process technologies, the devices and interconnects have become more susceptible to single event upsets. This paper presents a low cost error correcting code based technique to protect NoC routers against single event upset induced soft errors. An unequal protection error correcting code based methodology is provided for the most commonly used store-and-forward routing strategy. The proposed codes have the same check bit overhead as the conventional single error correcting (SEC) code. The encoding/decoding overhead and latency are also similar to the conventional low cost SEC code. The proposed codes belong to the class of unequal error protection codes as they\u00a0\u2026", "num_citations": "58\n", "authors": ["503"]}
{"title": "Using partial isolation rings to test core-based designs\n", "abstract": " A partial isolation ring provides the same fault coverage as a full isolation ring but avoids adding multiplexers on critical timing paths and reduces area overhead. The authors examine several partial isolation ring selection strategies that vary in computational complexity.", "num_citations": "57\n", "authors": ["503"]}
{"title": "Synthesis of low-cost parity-based partially self-checking circuits\n", "abstract": " A methodology for the synthesis of partially self-checking multilevel logic circuits with low-cost parity-based concurrent error detection (CED) is described. A subset of the inputs of the circuit is selected to realize a simple characteristic function such that CED is disabled whenever the inputs belong to the OFF-set of the characteristic function. This don't-care space in the operation of the CED circuitry is used to optimize the CED circuitry during synthesis. It is shown that this methodology is very effective at targeting faults with a high sensitization probability. Experimental results show that the proposed approach, which is of special interest in applications where a low-cost CED solution is desired, achieves a significant reduction in the error rate in logic circuits.", "num_citations": "56\n", "authors": ["503"]}
{"title": "Joint minimization of power and area in scan testing by scan cell reordering\n", "abstract": " This paper describes a technique for re-ordering of scan cells to minimize power dissipation that is also capable of reducing the area overhead of the circuit compared to a random ordering of the scan cells. For a given test set, our proposed greedy algorithm finds the (locally) optimal scan cell ordering for a given value of /spl lambda/, which is a trade-off parameter that can be used by the designer to specify the relative importance of area overhead minimization and power minimization. The strength of our algorithm lies in the fact that we use a novel dynamic minimum transition fill (MT-fill) technique to fill the unspecified bits in the test vector. Experiments performed on the ISCAS-89 benchmark suite show a reduction in power (70% for s13207, /spl lambda/ = 500) as well as a reduction in layout area (6.72% for s13207, /spl lambda/ = 500).", "num_citations": "56\n", "authors": ["503"]}
{"title": "Reducing power consumption in memory ECC checkers\n", "abstract": " A method is proposed for reducing power consumption in memory ECC checker circuitry that provides SEC-DED. The degrees of freedom in selecting the parity check matrix are used to minimize power with little or no impact on area and delay. The power minimization method is applied to two popular SEC-DED codes: standard Hamming codes and odd-column-weight Hsiao codes. Experiments on actual memory traces of Spec and MediaBench benchmarks indicate that considering power in addition to area and delay when selecting the parity check matrix can result in power reductions of up to 27% for Hsiao codes and up to 41% for Hamming codes.", "num_citations": "55\n", "authors": ["503"]}
{"title": "Logic synthesis techniques for reduced area implementation of multilevel circuits with concurrent error detection\n", "abstract": " This paper presents new logic synthesis techniques for generating multilevel circuits with concurrent error detection based on a parity-check code scheme that can detect all errors caused by single stuck-at faults. These synthesis techniques fully automate the design process and allow for a better quality result than previous methods thereby reducing the cost of concurrent error detection. An algorithm is described for selecting a good parity-check code for encoding the outputs of a circuit. Once the code has been chosen, a new procedure called structureconstrained logic optimization is used to minimize the area of the circuit as much as possible while still using a circuit structure that ensures that single stuck-at faults cannot produce undetected errors. The implementation that is generated is path fault secure and when augmented by a checker forms a self-checking circuit. Results indicate that self-checking multilevel circuits can be generated which require significantly less area than using duplication.", "num_citations": "55\n", "authors": ["503"]}
{"title": "3-stage variable length continuous-flow scan vector decompression scheme\n", "abstract": " This paper presents a 3-stage continuous-flow linear decompression scheme for scan vectors that uses a variable number of bits to encode each vector. By using 3-stages of decompression, it can efficiently compress any test cube (i.e., deterministic test vector where the unassigned bit positions are left as don't cares) regardless of the number of specified (care) bits. As a result of this feature, there is no need for any constraints on the automatic test generation process (ATPG). Any ATPG can be used with any amount of static or dynamic compaction. Experimental results are shown which demonstrate that the proposed scheme achieves extremely high encoding efficiency.", "num_citations": "54\n", "authors": ["503"]}
{"title": "Relating entropy theory to test data compression\n", "abstract": " The entropy of a set of data is related to the amount of information that it contains and provides a theoretical bound on the amount of compression that can be achieved. While calculating entropy is well understood for fully specified data, this paper explores the use of entropy for incompletely specified test data and shows how theoretical bounds on the maximum amount of test data compression can be calculated. An algorithm for specifying don\u2019t cares to minimize entropy for fixed length symbols is presented, and it is proven to provide the lowest entropy among all ways of specifying the don\u2019t cares. The impact of different ways of partitioning the test data into symbols on entropy is studied. Different test data compression techniques are analyzed with respect to their entropy bounds. Entropy theory is used to show the limitations and advantages of certain types of test data encoding strategies.", "num_citations": "52\n", "authors": ["503"]}
{"title": "Improving linear test data compression\n", "abstract": " The output space of a linear decompressor must be sufficiently large to contain all the test cubes in the test set. The ideas proposed in this paper transform the output space of a linear decompressor so as to reduce the number of inputs required thereby increasing compression while still keeping all the test cubes in the output space. Scan inversion is used to invert a subset of the scan cells while reconfiguration modifies the linear decompressor. Any existing method for designing a linear decompressor (either combinational or sequential) can be used first to obtain the best linear decompressor that it can. Using that linear decompressor as a starting point, the proposed methods improve the compression further. The key property of scan inversion is that it is a linear transformation of the output space and, thus, the output space remains a linear subspace spanned by a Boolean matrix. Using this property, a systematic\u00a0\u2026", "num_citations": "50\n", "authors": ["503"]}
{"title": "Automated logic synthesis of random pattern testable circuits\n", "abstract": " Previous approaches to designing random pattern testable circuits use post-synthesis test point insertion to eliminate random pattern resistant (r.p.r.) faults. The approach taken in this paper is to consider random pattern testability during logic synthesis. An automated logic synthesis procedure is presented which takes as an input a two-level representation of a circuit and a constraint on the minimum fault detection probability (threshold below which faults are considered r.p.r.) and generates a multilevel implementation that satisfies the constraint while minimizing the literal count. The procedure identifies r.p.r. faults and attempts to \"eliminate\" them through algebraic factoring. If that is not possible, then test points are inserted during the synthesis process in a way that minimizes the number of test points that are required. Results are shown for benchmark circuits which indicate that the proposed procedure can\u00a0\u2026", "num_citations": "47\n", "authors": ["503"]}
{"title": "Achieving high encoding efficiency with partial dynamic LFSR reseeding\n", "abstract": " Previous forms of LFSR reseeding have been static (i.e., test application is stopped while each seed is loaded) and have required full reseeding (i.e., the length of the seed is equal to the length of the LFSR). A new form of LFSR reseeding is described here that is dynamic (i.e., the seed is incrementally modified while test application proceeds) and allows partial reseeding (i.e. length of the seed is less than that of the LFSR). In addition to providing better encoding efficiency, partial dynamic LFSR reseeding has a simpler hardware implementation than previous schemes based on multiple-polynomial LFSRs.", "num_citations": "44\n", "authors": ["503"]}
{"title": "Test point insertion with control points driven by existing functional flip-flops\n", "abstract": " This paper presents a novel test point insertion method for pseudorandom built-in self-test (BIST) to reduce the area overhead. The proposed method replaces dedicated flip-flops for driving control points by existing functional flip-flops. For each control point, candidate functional flip-flops are identified by using logic cone analysis that investigates the path inversion parity, logical distance, and reconvergence from each control point. Four types of new control point structures are introduced based on the logic cone analysis results to avoid degrading the testability. Experimental results indicate that the proposed method significantly reduces test point area overhead by replacing the dedicated flip-flops and achieves essentially the same fault coverage as conventional test point implementations using dedicated flip-flops driving the control points.", "num_citations": "39\n", "authors": ["503"]}
{"title": "Exploiting unused spare columns to improve memory ECC\n", "abstract": " Spare columns are often included in memories for the purpose of allowing for repair in the presence of defective cells or bit lines. In many cases, the repair process will not use all spare columns. This paper proposes an extremely low cost method to exploit these unused spare columns to improve the reliability of the memory by enhancing its existing error correcting code (ECC). Memories are generally protected with single-error-correcting, double-error-detecting (SEC-DED) codes using the minimum number of check bits. In the proposed method, unused spare columns are exploited to store additional check bits which can be used to reduce the miscorrection probability for triple errors in SEC-DED codes or non-adjacent double errors in single adjacent error correcting codes (SEC-DAEC) codes.", "num_citations": "39\n", "authors": ["503"]}
{"title": "Configuration self-test in FPGA-based reconfigurable systems\n", "abstract": " An FPGA-based reconfigurable system may contain boards of FPGAs which are reconfigured for different applications and must work correctly. This paper presents a novel approach for rapidly testing the configuration in the FPGAs each time the system is reconfigured. A low-cost configuration-dependent test method is used to detect faults in the circuit. The \"original configuration\" is modified by only changing the logic function of the CLBs to form \"test configurations\" that can be used to quickly test the circuit. The test procedure is rapid enough to be performed on the fly whenever the system is reconfigured. The technique is independent of any fault model since it partitions the circuit into segments and tests each segment exhaustively.", "num_citations": "38\n", "authors": ["503"]}
{"title": "Using limited dependence sequential expansion for decompressing test vectors\n", "abstract": " Existing techniques that incorporate decompressor constraints in the ATPG search/backtrace (e.g., Illinois scan) are based on combinational expansion in which each scan slice must be encoded using only the free-variables arriving from the tester in the current clock cycle. Sequential expansion is more powerful as it allows free-variables across multiple clock cycles to be used, however conventional approaches for sequential expansion that are based on linear finite state machines (LFSRs) and ring generators are not amenable to including the constraints in the ATPG backtrace because the constraints are too complex. This paper investigates the use of limited dependence sequential expansion to combine the benefits of sequential decompression with the benefits of incorporating the decompressor constraints in the ATPG backtrace. Analytical and experimental results are presented showing the benefits of the\u00a0\u2026", "num_citations": "33\n", "authors": ["503"]}
{"title": "A systematic approach for diagnosing multiple delay faults\n", "abstract": " In the presence of multiple delay faults, automated diagnostic procedures that make a single fault assumption may give an incorrect diagnosis. In this paper, a systematic approach is proposed for delay fault diagnosis under a multiple fault assumption. Information from the failing test vectors are used to construct a list of single and multiple fault suspects that may have caused all of the observed faulty response. The list of suspects is then pruned and ranked in a novel way by using information from the passing test vectors combined with static timing information.", "num_citations": "32\n", "authors": ["503"]}
{"title": "Improved trace buffer observation via selective data capture using 2-D compaction for post-silicon debug\n", "abstract": " This paper presents a novel technique for extending the capacity of trace buffers when capturing debug data during post-silicon debug. It exploits the fact that is it not necessary to capture error-free data in the trace buffer since that information can be obtained from simulation. A selective data capture method is proposed in this paper that only captures debug data during clock cycles in which errors are present. The proposed debug method requires only three debug sessions. The first session estimates a rough error rate, the second session identifies a set of suspect clock cycles where errors may be present, and the third session captures the suspect clock cycles in the trace buffer. The suspect clock cycles are determined through a 2-D compaction technique using multiple-input signature register signatures and cycling register signatures. Intersecting both signatures generates a small number of suspect clock\u00a0\u2026", "num_citations": "31\n", "authors": ["503"]}
{"title": "Deterministic test vector compression/decompression for systems-on-a-chip using an embedded processor\n", "abstract": " A novel approach for using an embedded processor to aid in deterministic testing of the other components of a system-on-a-chip (SOC) is presented. The tester loads a program along with compressed test data into the processor\u2019s on-chip memory. The processor executes the program which decompresses the test data and applies it to scan chains in the other components of the SOC to test them. The program itself is very simple and compact, and the decompression is done very rapidly, hence this approach reduces both the amount of data that must be stored on the tester and reduces the test time. Moreover, it enables at-speed scan shifting even with a slow tester (i.e., a tester whose maximum clock rate is slower than the SOC\u2019s normal operating clock rate). A procedure is described for converting a set of test cubes (i.e., test vectors where the unspecified inputs are left as X\u2019s) into a compressed form. A\u00a0\u2026", "num_citations": "31\n", "authors": ["503"]}
{"title": "Generating burst-error correcting codes from Orthogonal Latin Square codes--A graph theoretic approach\n", "abstract": " The paper proposes a scheme by which an Orthogonal Latin Square code (OLS) can be modified to correct burst-errors of specific length. The method discussed in this paper models it as a graph coloring problem where the goal is to resolve conflicts in the existing OLS code in order for it to correct burst-errors. Conflicts are resolved by reordering and/or reorganizing existing parity relations by inclusion of extra check bits. The graph coloring approach tries to minimize the number of additional check bits required. The final OLS code after reordering and/or reorganizing would be capable of correcting burst-errors of specific length in addition to its original error correction capabilities.", "num_citations": "29\n", "authors": ["503"]}
{"title": "Increasing output compaction in presence of unknowns using an X-canceling MISR with deterministic observation\n", "abstract": " Recently, an X-canceling MISR methodology was proposed in Touba (2007) which was based on providing very high probabilistic error coverage by canceling out X's in MISR signatures. This paper investigates a new methodology for using the X-canceling MISR architecture based on deterministically observing scan cells. The two main advantages of the proposed approach are (1) it can provide a higher amount of compaction, and (2) it is effective for larger percentages of X's in the output response. Also, this paper investigates a hybrid approach that combines X-masking with an X-canceling MISR. Experimental results indicate that significant amounts of output compression can be achieved with no loss of fault coverage.", "num_citations": "29\n", "authors": ["503"]}
{"title": "Low-power testing\n", "abstract": " Publisher SummaryNumerous studies from academia and industry have shown the need to reduce power consumption during testing of digital and memory designs. This chapter discusses issues arising from excessive power consumption during test application and provides structural and algorithmic solutions that can alleviate the low-power test problems. Both structural and algorithmic solutions are described along with their impacts on parameters such as fault coverage, test time, area overhead, circuit performance penalty, and design flow modification. These solutions cover a broad spectrum of testing environments, including scan testing, scan-based built-in self-test (BIST), test-per-clock BIST, test compression, and memory testing. Although the solutions presented can be used to address most of the problems caused by excessive test power, not all problems have been solved. One concern is when multiple\u00a0\u2026", "num_citations": "29\n", "authors": ["503"]}
{"title": "Selecting error correcting codes to minimize power in memory checker circuits\n", "abstract": " The approach proposed in this paper reduces power consumption in single-error correcting, double error-detecting checker circuits that perform memory ECC. Power is minimized with little or no impact on area and delay, using the degrees of freedom in selecting the parity check matrix of the error correcting code. The non-linear power optimization problem is solved using two methods, genetic algorithms and simulated annealing. Both the methods are applied to two SEC-DED codes: standard Hamming codes and odd-column-weight Hsiao codes. Experiments on actual memory traces of Spec and MediaBench benchmarks indicate that considering power along with area and delay when selecting the parity check matrix can result in power reductions of up to 27% for Hsiao codes and up to 41% for Hamming codes. Experiments are also performed to motivate the choice of parameters of the non-linear optimization\u00a0\u2026", "num_citations": "27\n", "authors": ["503"]}
{"title": "Matrix-based test vector decompression using an embedded processor\n", "abstract": " This paper describes a new compression/decompression methodology for using an embedded processor to test the other components of a system-on-a-chip (SoC). The deterministic test vectors for each core are compressed using matrix-based operations that significantly reduce the amount of test data that needs to be stored on the tester. The compressed data is transferred from the tester to the processor's on-chip memory. The processor executes a program which decompresses the data and applies it to the scan chains of each core-under-test. The matrix-based operations that are used to decompress the test vectors can be performed very efficiently by the embedded processor thereby allowing the decompression program to be very fast and provide high throughput of the test data to minimize test time. Experimental results demonstrate that the proposed approach provides greater compression than previous\u00a0\u2026", "num_citations": "27\n", "authors": ["503"]}
{"title": "An embedded core DFT scheme to obtain highly compressed test sets\n", "abstract": " This paper presents a novel design-for-test (DFT) technique that allows core vendors to reduce the test complexity of a core they are trying to market. The idea is to design a core so that it can be tested with a very small number of test vectors. The I/O pins of such a \"designed for high test compression\" (DFHTC) core are identical to the I/O pins of an ordinary core. For the system integrator, testing a DFHTC core is identical to testing an ordinary core. The only difference is that the DFHTC core has a significantly smaller number of test vectors resulting in less test data as well as less test time (fewer scan vectors). This is achieved by carefully combining a parallel \"test per clock\" BIST scheme inside the core with the normal external testing scheme using a tester. The BIST structure inside the core generates weighted pseudo-random test vectors which detect a large number of faults in the core. Results indicate that such\u00a0\u2026", "num_citations": "27\n", "authors": ["503"]}
{"title": "Hybrid BIST using an incrementally guided LFSR\n", "abstract": " A new hybrid BIST scheme is proposed which is based on using an \"incrementally guided LFSR\". It very efficiently combines external deterministic data from the tester with on-chip pseudo-random BIST. The hardware overhead is very small as a conventional STUMPS architecture (P. H. Bardell et al., Proc. Int. Test Conf., p.200-204, 1982) is used with only a small modification to the feedback of the LFSR which allows the tester to incrementally guide the LFSR so that it can embed patterns that detect the random-pattern-resistant faults in the pseudo-random sequence. Compared with external testing, the proposed approach achieves dramatic reductions in tester storage requirements while using very simple on-chip hardware. Results indicate that the proposed approach provides very attractive tradeoffs between test length and tester storage requirements.", "num_citations": "25\n", "authors": ["503"]}
{"title": "Improving memory repair by selective row partitioning\n", "abstract": " A new methodology for improving memory repair is presented which can be applied in either manufacture time repair or built-in self-repair (BISR) scenarios. In traditional memory repair, one spare column can only replace one column containing a defective cell. However, the proposed method allows a single spare column to be used to repair multiple defective cells in multiple columns. This is done by selectively decoding the row address bits when generating the control signals for the column MUXes. This logically segments the spare column allowing it to replace different columns in different partitions of the row address space. The hardware is the same for all chips, but fuses are used to customize the row decoding circuitry on a chip-by-chip basis. An algorithm is described for choosing which row address bits to decode given the defect map for a particular chip. This additional degree of freedom allows\u00a0\u2026", "num_citations": "23\n", "authors": ["503"]}
{"title": "Enhancing silicon debug via periodic monitoring\n", "abstract": " Scan-based debug methods give high observability of internal signals, however, they require halting the system to scan out responses from the circuit-under-debug (CUD). This is time consuming as many scan dumps may be required. In this paper, conventional scan chains that have non-destructive scan out capability are configured to operate as multiple MISRs during system operation. Information from the multiple MISRs is monitored periodically to identify erroneous behavior. A procedure for constructing the MISRs to maximize debug capability is described. A three step process is used to zero in on the first clock cycle in which an error is present with a small number of scan dumps. Moreover, a method for bypassing errors is described to permit debug in the presence of multiple bugs.", "num_citations": "23\n", "authors": ["503"]}
{"title": "Applying two-pattern tests using scan-mapping\n", "abstract": " This paper proposes a new technique, called scan-mapping, for applying two-pattern tests in a standard scan design environment. Scan-mapping is performed by shifting the first pattern (V/sub 1/) into the scan path and then using combinational mapping logic to generate the second pattern (V/sub 2/) in the next clock cycle. The mapping logic is placed in the scan path and avoids the performance degradation of using more complex scan elements to apply two-pattern tests. A procedure is described for synthesizing the mapping logic required to apply a set of two-pattern tests. Scan-mapping can be used in deterministic testing to apply two-pattern tests that can't be applied using scan-shifting or functional justification, and it can be used in built-in self-testing (BlST) to improve the fault coverage for delay faults. Experimental results indicate that, for deterministic testing, scan-mapping can reduce area overhead and test\u00a0\u2026", "num_citations": "22\n", "authors": ["503"]}
{"title": "X-canceling multiple-input signature register (MISR) for compacting output responses with unknowns\n", "abstract": " A method and apparatus for compacting test responses containing unknown (X) values in a scan-based integrated circuit using an X-canceling multiple-input signature register (MISR) to produce a known (non-X) signature. The known (non-X) signature is obtained by selectively exclusive-ORing (XORing) together combinations of MISR bits which are linearly dependent in terms of the unknown (X) values using a selective XOR network.", "num_citations": "20\n", "authors": ["503"]}
{"title": "BETSY: synthesizing circuits for a specified BIST environment\n", "abstract": " This paper presents a logic synthesis tool called BETSY (BIST Environment Testable SYnthesis) for synthesizing circuits that achieve complete (100%) fault coverage in a user specified BIST environment. Instead of optimizing the circuit for a generic pseudo-random test pattern generator (by maximizing its random pattern testability), the circuit is optimized for a specific test pattern generator, e.g., an LFSR with a specific characteristic polynomial and initial seed. This solves the problem of having to estimate fault detection probabilities during synthesis and guarantees that the resulting circuit achieves 100% fault coverage. BETSY considers the exact set of patterns that will be applied to the circuit during BIST and applies various transformations to generate an implementation that is fully tested by those patterns. When needed, BETSY inserts test points early in the synthesis process in an optimal way and accounts for\u00a0\u2026", "num_citations": "20\n", "authors": ["503"]}
{"title": "-Canceling MISR Architectures for Output Response Compaction With Unknown Values\n", "abstract": " In this paper, an  X -tolerant multiple-input signature register (MISR) compaction methodology that compacts output responses containing unknown  X  values is described. Each bit of the MISR signature is expressed as a linear combination in terms of  X s by symbolic simulation. Linearly dependent combinations of the signature bits are identified with Gaussian elimination and XORed to remove  X  values and yield deterministic values. Two  X -canceling MISR architectures are proposed and analyzed with industrial designs. This paper also shows the correlation between the estimated result based on idealized modeling and the actual data for real circuits for error coverage, hardware overhead, and other metrics. Experimental results indicate that high error coverage can be achieved with  X -canceling MISR configurations and it highly correlates with actual results.", "num_citations": "19\n", "authors": ["503"]}
{"title": "Test point insertion using functional flip-flops to drive control points\n", "abstract": " This paper presents a novel method for reducing the area overhead introduced by test point insertion. Test point locations are calculated as usual using a commercial tool. However, the proposed method uses functional flip-flops to drive control test points instead of test-dedicated flip-flops. Logic cone analysis that considers the distance and path inversion parity from candidate functional flip-flops to each control point is used to select an appropriate functional flip-flop to drive the control point which avoids adding additional timing constraints. Reconvergence is also checked to avoid degrading the testability. Experimental results indicate that the proposed method significantly reduces test point area overhead and achieves essentially the same fault coverage as the implementations using dedicated flip-flops driving the control points.", "num_citations": "19\n", "authors": ["503"]}
{"title": "Synthesis of nonintrusive concurrent error detection using an even error detecting function\n", "abstract": " A new method for synthesizing nonintrusive concurrent error detection (CED) circuitry is presented. The idea is to use single-bit parity to detect all errors affecting an odd number of bits and then synthesize a circuit to detect the even errors. A novel statistical sampling and expanding methodology is proposed for constructing the even error detection circuitry. A major feature of the proposed methodology is that it allows very efficient tradeoffs between error coverage and overhead. While CED schemes that use a fixed checker based on a particular error detecting code are not amenable to simplification without a major impact on coverage, the proposed scheme can easily facilitate significant reductions in overhead with only a small loss in coverage. Experimental results show that the proposed scheme can provide very high levels of soft error protection at a fraction of the cost of duplication", "num_citations": "19\n", "authors": ["503"]}
{"title": "Test data compression technique for embedded cores using virtual scan chains\n", "abstract": " This paper presents a design-for-test (DFT) technique to implement a \"virtual scan chain\" in a core that looks (to the system integrator) like it is shorter than the real scan chain inside the core. A core with a \"virtual scan chain\" is fully compatible with a core with a regular scan chain in terms of both the external test interface and tester program. The I/O pins of a core with a virtual scan chain are identical to the I/O pins of a core with a regular scan chain. For the system integrator, testing a core with a virtual scan chain is identical to testing a core with a regular scan chain (no special modes, control signals, or timing sequences are needed). The only difference is that the virtual scan chain is much shorter so the size of the scan vectors and output response is smaller resulting in less test data as well as less test time (fewer scan shift cycles). The process of mapping the virtual scan vectors to real scan vectors is handled\u00a0\u2026", "num_citations": "18\n", "authors": ["503"]}
{"title": "Combining linear and nonlinear test vector compression using correlation-based rectangular encoding\n", "abstract": " A technique is presented here for improving the compression achieved with any linear decompressor by adding a small nonlinear decoder that exploits bit-wise and pattern-wise correlation present in test vectors. The proposed nonlinear decoder has a regular and compact structure and allows continuous-flow decompression. It has a very important feature which is that its design does not depend on the test data. This simplifies the design flow and allows the decoder to be reused when testing multiple cores on a chip. Experimental results show that combining a linear decompressor with the small nonlinear decoder proposed here significantly improves the overall compression", "num_citations": "17\n", "authors": ["503"]}
{"title": "Using statistical transformations to improve compression for linear decompressors\n", "abstract": " Linear decompressors are the dominant methodology used in commercial test data compression tools. However, they are generally not able to exploit correlations in the test data, and thus the amount of compression that can be achieved with a linear decompressor is directly limited by the number of specified bits in the test data. The paper describes a scheme in which a nonlinear decoder is placed between the linear decompressor and the scan chains. The nonlinear decoder uses statistical transformations that exploit correlations in the test data to reduce the number of specified bits that need to be produced by the linear decompressor. Given a test set, a procedure is presented for selecting a statistical code that effectively \"compresses\" the number of specified bits (note that this is a novel and different application of statistical codes from what has been studied before and requires new algorithms). Results indicate\u00a0\u2026", "num_citations": "17\n", "authors": ["503"]}
{"title": "Lowering power consumption in concurrent checkers via input ordering\n", "abstract": " This paper presents an efficient and scalable technique for lowering power consumption in checkers used for concurrent error detection. The basic idea is to exploit the functional symmetry of concurrent checkers with respect to their inputs, and to order the inputs such that switching activity (and hence power consumption) in the checker is minimized. The inputs of the checker are usually driven by the outputs of the function logic and check symbol generator logic-spatial correlations between these outputs are analyzed to compute an input order that minimizes power consumption. The reduction in power consumption comes at no additional impact to area or performance and does not require any alteration to the design flow. It is shown that the number of possible input orders increases exponentially in the number of inputs to the checker. As a result, the computational cost of determining the optimum input order can\u00a0\u2026", "num_citations": "17\n", "authors": ["503"]}
{"title": "Improving encoding efficiency for linear decompressors using scan inversion\n", "abstract": " The output space of a linear decompressor must be sufficiently large to contain all the test cubes in the test set. The idea proposed in This work is to use scan inversion to transform the output space of a linear decompressor so as to reduce the number of inputs required thereby increasing the encoding efficiency while still keeping all the test cubes in the output space. Any existing method for designing a linear decompressor (either combinational or sequential) can be used first to obtain the best linear decompressor that it can. Using that linear decompressor as a starting point, the proposed method improves the encoding efficiency further. The key property used by the proposed method is that scan inversion is a linear transformation of the output space and thus the output space remains a linear subspace spanned by a Boolean matrix. Using this property, a systematic procedure based on linear algebra is described\u00a0\u2026", "num_citations": "17\n", "authors": ["503"]}
{"title": "Input ordering in concurrent checkers to reduce power consumption\n", "abstract": " A novel approach for reducing power consumption in checkers used for concurrent error detection is presented. Spatial correlations between the outputs of the circuit that drives the primary inputs of the checker are analyzed to order them such that switching activity (and hence power consumption) in the checker is minimized. The reduction in power consumption comes at no additional impact to area or performance and does not require any alteration to the design flow. Since the number of possible input orders increases exponentially in the number of inputs to the checker, the computational costs of determining the optimum order can be very expensive. We present a very effective technique to build a reduced cost function to solve the optimization problem to find a near optimal order.", "num_citations": "17\n", "authors": ["503"]}
{"title": "Reconfigurable linear decompressors using symbolic Gaussian elimination\n", "abstract": " A methodology for designing a reconfigurable linear decompressor is presented. A symbolic Gaussian elimination method to solve a constrained Boolean matrix is proposed and utilized for designing the reconfigurable network. The proposed scheme can be implemented in conjunction with any decompressor that has a combinational linear network. Using the given linear decompressor as a starting point, the proposed method improves the compression further. A nice feature of the proposed method is that it can be implemented with very little hardware overhead. Experimental results indicate that significant improvements can be achieved.", "num_citations": "16\n", "authors": ["503"]}
{"title": "Implementing triple adjacent error correction in double error correction orthogonal Latin squares codes\n", "abstract": " Soft errors have been a concern in memories for many years. In older technologies, soft errors typically affected a single memory cell but as technology scaled, Multiple Cell Upsets (MCUs) that affect a group of nearby cells have become more common. This trend is expected to continue making MCUs more frequent and also increasing the number of cells affected. To avoid data corruption in memories, Error Correction Codes (ECCs) are used. Single Error Correction (SEC) codes that can correct one bit error per word are effective only against single errors. To protect against MCUs, one option is to use more sophisticated error correction codes like for example, Orthogonal Latin Squares Codes (OLSC). In this paper, a modification of the OLSC decoding algorithm is proposed for codes that can correct two random errors. This modification has little impact on circuit complexity and enables triple adjacent error\u00a0\u2026", "num_citations": "15\n", "authors": ["503"]}
{"title": "Logic BIST architecture using staggered launch-on-shift for testing designs containing asynchronous clock domains\n", "abstract": " This paper presents a new at-speed logic built-in self-test (BIST) architecture using staggered launch-on-shift (LOS) for testing a scan-based BIST design containing asynchronous clock domains. The proposed approach can detect inter-clock-domain structural faults and intra-clock-domain delay and structural faults in the BIST design. This solves the long-standing problem of using the conventional one-hot LOS approach that requires testing one clock domain at a time which causes long test time or using the simultaneous LOS approach that requires adding capture-disabled circuitry to normal functional paths across interacting clock domains which causes fault coverage loss. Given a fixed number of BIST patterns, experimental results showed that the proposed staggered clocking scheme can detect more faults than one-hot clocking and simultaneous clocking.", "num_citations": "15\n", "authors": ["503"]}
{"title": "Eliminating non-determinism during test of high-speed source synchronous differential buses\n", "abstract": " The at-speed functional testing of deep sub-micron devices equipped with high-speed I/O ports and the asynchronous nature of such I/O transactions poses significant challenges. In this paper, the problem of nondeterminism in the output response of the device-under-test (DUT) is described. This can arise due to limited automated test equipment (ATE) edge placement accuracy(EPA) in the source synchronous clock of the stimulus stream to the high-speed I/O port from the tester. A simple yet effective solution that uses a trigger signal to initiate a deterministic transfer of test inputs into the core clock domain of the DUT from the high-speed I/O port is presented. The solution allows the application of at-speed functional patterns to the DUT while incurring a very small hardware overhead and trivial increase in test application time. An analysis of the probability of non-determinism as a function of clock speed and EPA is\u00a0\u2026", "num_citations": "15\n", "authors": ["503"]}
{"title": "Circular BIST with state skipping\n", "abstract": " Circular built-in self-test (BIST) is a \"test per clock\" scheme that offers many advantages compared with conventional BIST approaches in terms of low area overhead, simple control logic, and easy insertion. However, it has seen limited use because it does not reliably provide high fault coverage. This paper presents a systematic approach for achieving high fault coverage with circular BIST. The basic idea is to add a small amount of logic that causes the circular chain to skip to particular states. This \"state skipping\" logic can be used to break out of limit cycles, break correlations in the test patterns, and jump to states that detect random-pattern-resistant faults. The state skipping logic is added in the chain interconnect and not in the functional logic, so no delay is added on system paths. Results indicate that in many cases, this approach can boost the fault coverage of circular BIST to match that of conventional parallel\u00a0\u2026", "num_citations": "15\n", "authors": ["503"]}
{"title": "A methodology for automated insertion of concurrent error detection hardware in synthesizable Verilog RTL\n", "abstract": " This paper describes a methodology for automated insertion of concurrent error detection (CED) circuitry in synthesizable Verilog RTL that allows easy tradeoff between overhead and coverage. The insertion is done at the front-end of the synthesis process which is highly advantageous. Computer-aided design (CAD) tools that support the proposed methodology have been implemented and are described. The CAD tools allow the designer to select from different CED schemes that provide various cost/coverage tradeoffs. The CED circuitry is then automatically inserted into the Verilog design. Experimental results are shown which demonstrate the broad range of design points that the proposed methodology offers the designer to choose from to satisfy cost/coverage requirements. The methodology can be seamlessly integrated into the typical design flow used in industry.", "num_citations": "15\n", "authors": ["503"]}
{"title": "Improving test compression with scan feedforward techniques\n", "abstract": " Scan feedforward techniques are proposed for improving test compression for both sequential linear decompressors based on solving linear equations and for broadcast scan based decompressors in which the decompressor constraints are included in the automatic test pattern generation (ATPG) backtrace. The conventional approach is to first pre-load a sequential linear decompressor with free variables from the tester for some number of clock cycles before beginning to shift the scan chain. A scan feedforward scheme is proposed here which can shorten or eliminate the need for pre-loading the decompressor. More importantly, even if the pre-load phase is not changed, the proposed scheme significantly expands the free-variable dependence of each scan cell, which increases the probability of encoding a test cube. This permits more faults to be targeted by each test cube generated during ATPG (i.e., improved\u00a0\u2026", "num_citations": "14\n", "authors": ["503"]}
{"title": "Efficient trace signal selection for silicon debug by error transmission analysis\n", "abstract": " In this paper, a technique is presented for selecting signals to observe during silicon debug. Internal signals are used to analyze, understand, and debug circuit misbehavior. An automated procedure to select which signals to observe is proposed to facilitate early detection of circuit malfunction and to enhance the utilization of hardware resources for storage. Signals that are most often sensitized to possible errors are observed in sequential circuits. Given a functional input vector set, an error transmission matrix is generated by analyzing which flip-flops are sensitized to other flip-flops. Relatively independent flip-flops are identified and a set of signals that maximally cover the possible error sites with given constraints are identified through integer linear programming. Experimental results show that the proposed approach can rapidly and precisely identify the nonconforming chip behavior and thereby can speed up\u00a0\u2026", "num_citations": "14\n", "authors": ["503"]}
{"title": "Method and apparatus for low-pin-count scan compression\n", "abstract": " A low-pin-count scan compression method and apparatus for reducing test data volume and test application time in a scan-based integrated circuit. The scan-based integrated circuit contains one or more scan chains, each scan chain comprising one or more scan cells coupled in series. The method and apparatus includes a programmable pipelined decompressor comprising one or more shift registers, a combinational logic network, and an optional scan connector. The programmable pipelined decompressor decompresses a compressed scan pattern on its compressed scan inputs and drives the generated decompressed scan pattern at the output of the programmable pipelined decompressor to the scan data inputs of the scan-based integrated circuit. Any input constraints imposed by said combinational logic network are incorporated into an automatic test pattern generation (ATPG) program for generating the\u00a0\u2026", "num_citations": "14\n", "authors": ["503"]}
{"title": "Method and apparatus for pipelined scan compression\n", "abstract": " A pipelined scan compression method and apparatus for reducing test data volume and test application time in a scan-based integrated circuit without reducing the speed of the scan chain operation in scan-test mode or self-test mode. The scan-based integrated circuit contains one or more scan chains, each scan chain comprising one or more scan cells coupled in series. The method and apparatus includes a decompressor comprising one or more shift registers, a combinational logic network, and an optional scan connector. The decompressor decompresses a compressed scan pattern on its compressed scan inputs and drives the generated decompressed scan pattern at the output of the decompressor to the scan data inputs of the scan-based integrated circuit. Any input constraints imposed by said combinational logic network are incorporated into an automatic test pattern generation (ATPG) program for\u00a0\u2026", "num_citations": "14\n", "authors": ["503"]}
{"title": "CORDIC-based high-speed direct digital frequency synthesis\n", "abstract": " The circular-mode CORDIC (coordinate rotation digital computer) algorithm is analyzed in the context of direct digital frequency synthesis (DDFS). It is shown how the CORDIC parameters should be selected to meet given DDFS parameters, and, through simulations, it is demonstrated that jamming outperforms truncation as a datapath quantization scheme, making it an attractive alternative to rounding. It is also shown that the CORDIC output can be made exact to the digits by an additional rounding process, which is especially useful for DDFS applications where the CORDIC output is truncated to the final digital-to-analog converter (DAC) width.", "num_citations": "14\n", "authors": ["503"]}
{"title": "Reducing test power during test using programmable scan chain disable\n", "abstract": " A novel method for reducing average power during scan testing is presented. The flip-flops of a full-scan module. are assigned to scan chains and the vectors are reordered in such a way that some of the scan chains can have their clock disabled for portions of the test set. Disabling the clock prevents flip-flops from taking part in scan shifting, and hence reduces switching activity in the circuit. Moreover, disabling the clock also reduces power dissipation in the clock tree, which can be a major source of power consumption. The hardware modification that is required to implement this approach is to add the capability for the tester to gate the clock for any subset of the scan chains in the core.", "num_citations": "14\n", "authors": ["503"]}
{"title": "Synthesis techniques for pseudo-random built-in self-test\n", "abstract": " Built-in self-test (BIST) techniques enable an integrated circuit (IC) to test itself. BIST reduces test and maintenance costs for an IC by eliminating the need for expensive test equipment and by allowing fast location of failed ICs in a system. BIST also allows an IC to be tested at its normal operating speed which is very important for detecting timing faults. Despite all of these advantages, BIST has seen limited use in industry because of area and performance overhead and increased design time. This dissertation presents automated techniques for implementing BIST in a way that minimizes area and performance overhead.", "num_citations": "14\n", "authors": ["503"]}
{"title": "Low complexity burst error correcting codes to correct MBUs in SRAMs\n", "abstract": " Multiple bit upsets (MBUs) caused by high energy radiation is the most common source of soft errors in static random-access memories (SRAMs) affecting multiple cells. Burst error correcting Hamming codes have most commonly been used to correct MBUs in SRAM cell since they have low redundancy and low decoder latency. But with technology scaling, the number of bits being affected increases, thus requiring a need for increasing the burst size that can be corrected. However, this is a problem because it increases the number of syndromes exponentially thus increasing the decoder complexity exponentially as well. In this paper, a new burst error correcting code based on Hamming codes is proposed which allows much better scaling of decoder complexity as the burst size is increased. For larger burst sizes, it can provide significantly smaller and faster decoders than existing methods thus providing higher\u00a0\u2026", "num_citations": "13\n", "authors": ["503"]}
{"title": "Apparatus and method for protecting soft errors\n", "abstract": " An apparatus and method for soft-error resilience or correction with the ability to perform a manufacturing test operation, a slow-speed snapshot operation, a slow-speed signature analysis operation, an at-speed signature analysis operation, a defect tolerance operation, or any combination of the above operations. In one embodiment, an apparatus includes a system circuit, a shadow circuit, and an output joining circuit for soft-error resilience. The output joining circuit coupled to the output terminals of the system circuit and the shadow circuit includes at least an S-element for defect tolerance. In another embodiment, an apparatus includes a system circuit, a shadow circuit, a debug circuit, and an output joining circuit for soft-error correction. The output joining circuit coupled to the output terminals of the system circuit, the shadow circuit, and the debug circuit includes at least a V-element for defect tolerance.", "num_citations": "13\n", "authors": ["503"]}
{"title": "Robust scan synthesis for protecting soft errors\n", "abstract": " A method for performing robust scan synthesis for soft-error protection on a design for generating a robust scan design in a system. The system is modeled selectively at a register-transfer level (RTL) or a gate level; the design includes at least a sequential element or a scan cell for mapping to a robust scan cell of a select robust scan cell type. The method comprises performing a scan replacement and a scan stitching on the design database based on a given control information file for synthesizing the robust scan cell on the design database; and generating the synthesized robust scan design at a pre-determined RTL or a pre-determined gate level.", "num_citations": "13\n", "authors": ["503"]}
{"title": "On Designing Transformed Linear Feedback Shift Registers with Minimum Hardware Cost\n", "abstract": " This paper provides a proof that given a standard or modular linear feedback shift register (LFSR) that uses k 2-input XOR gates to generate pseudorandom sequences, any transformed LFSR (t-LFSR) implementing the same characteristic polynomial, f (x), as the standard or modular LFSR cannot use fewer than log2 (k+ 1) 2-input XOR gates when k is an odd number, or 1+ log2k 2-input XOR gates when k is an even number. This property applies to any nstage t-LFSR design regardless of whether f (x) is a primitive polynomial or not. A new class of minimum-cost LFSRs (min-LFSRs) is subsequently developed to reduce the hardware cost to a minimum.", "num_citations": "13\n", "authors": ["503"]}
{"title": "Designing a fast and adaptive error correction scheme for increasing the lifetime of phase change memories\n", "abstract": " This paper proposes an adaptive multi-bit error correcting code for phase change memories that provides a manifold increase in the lifetime of phase change memories thereby making them a more viable alternative for DRAM main memory. A novel aspect of the proposed approach is that the error correction code (ECC) is adapted over time as the number of failed cells in the phase change memory accumulates. The operating system (OS) monitors the number of errors corrected on a memory line, and when the number of errors on a line begins to exceed the strength of the ECC present, the ECC strength is adaptively increased. As this happens, the performance of the memory system gracefully degrades because more storage is taken up by check bits rather than data bits thereby reducing the effective size of a cache line since less data can be brought to the cache on each read operation to the PCM main memory\u00a0\u2026", "num_citations": "13\n", "authors": ["503"]}
{"title": "Reducing control bit overhead for X-masking/X-canceling hybrid architecture via pattern partitioning\n", "abstract": " An X-masking scheme prevents unknown (X) values from shifting into an output response compactor, whereas an X-canceling MISR methodology allows X's to enter the compactor, but then cancels them out through selective XORing. However, both approaches require significantly high volume of the control bits to remove X values to generate X-free output signatures. This paper proposes a method to reduce the control bit overhead by combining X-masking and X-canceling methodologies and exploiting the fact that unknown values tend to have high correlation in the scan cells. In this paper, correlation is considered across whole patterns in order to enhance reuse of control bits. The proposed hybrid method of X-canceling and X-masking reduces test time without losing fault coverage. The experimental results show that the proposed method significantly reduces control bits and test time compared to a\u00a0\u2026", "num_citations": "12\n", "authors": ["503"]}
{"title": "Compressing functional tests for microprocessors\n", "abstract": " In the past, test data volume reduction techniques have concentrated heavily on scan test data content. However, functional vectors continue to be utilized because they target unique defects and failure modes. Hence, functional vector compression can help alleviate the cost of functional test. Scan vector compression techniques are generally unsuitable in the functional domain and techniques specially tailored for functional test compression are required. Additionally, it may be possible to perform compression and decompression using software techniques without incurring the overhead of dedicated hardware. This paper proposes a set of software techniques targeted towards functional test compression.", "num_citations": "12\n", "authors": ["503"]}
{"title": "RP-SYN: synthesis of random pattern testable circuits with test point insertion\n", "abstract": " An automated logic synthesis procedure, called RP-SYN, is described for synthesizing random pattern testable circuits. RP-SYN takes as an input a two-level description of a circuit and a constraint on the minimum fault detection probability (threshold below which faults are considered random pattern-resistant), and generates a multilevel implementation which satisfies the constraint while minimizing the literal count. RP-SYN identifies random-pattern-resistant faults and eliminates them through testability-driven factoring combined with test point insertion. By moving the task of test point insertion from the back-end into the synthesis process, RP-SYN reduces design time and enables better optimization of the resulting implementation. Results are shown for benchmark circuits which indicate that RP-SYN can generally reduce the random pattern test length by at least an order of magnitude with only a small area\u00a0\u2026", "num_citations": "12\n", "authors": ["503"]}
{"title": "Improving test compression by retaining non-pivot free variables in sequential linear decompressors\n", "abstract": " Sequential linear decompressors are inherently efficient and attractive for compressing test cubes with many don't cares. The test cubes are encoded by solving a system of linear equations. In continuous decompression, typically a fixed number of free variables are used to encode each test cube in a \u201cone-size-fits-all\u201d manner. The non-pivot free variables used in Gaussian elimination are wasted when the decompressor is reset before decompressing the next test cube. This paper explores techniques for retaining the non-pivot free variables for a test cube and using them to help encode subsequent test cubes and hence improve encoding efficiency. This approach retains most of the non-pivot free variables with only a minimal increase in runtime for solving the equations and no added control information. Experimental results are presented showing that the encoding efficiency, and hence compression, can be\u00a0\u2026", "num_citations": "11\n", "authors": ["503"]}
{"title": "Diagnosing resistive bridges using adaptive techniques\n", "abstract": " A systematic procedure for locating resistive bridges is presented. Critical path tracing is used to identify a set of \"suspect\" bridges whose presence could explain all of the observed faulty behavior of the circuit for the original test set. The set of suspects is then reduced by adaptively applying additional tests derived from the failing vector pairs in the original test set. Unlike other approaches, the approach presented here is not based on any bridge fault modeling and does not require any fault simulation.", "num_citations": "11\n", "authors": ["503"]}
{"title": "Obtaining high fault coverage with circular BIST via state skipping\n", "abstract": " Despite all of the advantages that circular BIST offers compared to conventional BIST approaches in terms of low area overhead, simple control logic, and easy insertion, it has seen limited use because it does not reliably provide high fault coverage. This paper presents a systematic approach for achieving high fault coverage with circular BIST. The basic idea is to add a small amount of logic that causes the circular chain to skip to particular states. This \"state skipping\" logic can be used to break out of limit cycles, break correlations in the test patterns, and jump to states that detect random-pattern resistant faults. The state skipping logic is added in the chain interconnect and not in the functional logic, so no delay is added to system paths. Result indicate that in many cases, this approach can boost the fault coverage of circular BIST to match that of conventional parallel BIST approaches while still maintaining a\u00a0\u2026", "num_citations": "11\n", "authors": ["503"]}
{"title": "Reducing test point area for BIST through greater use of functional flip-flops to drive control points\n", "abstract": " Recently, a new test point insertion method for pseudo-random built-in self-test (BIST) was proposed in [Yang 09] which tries to use functional flip-flops to drive control test points instead of adding extra dedicated flip-flops for driving the control points. This paper investigates methods to further reduce the area overhead by replacing dedicated flip-flops which could not be replaced in [Yang 09]. A new algorithm (alternative selection algorithm) is proposed to find candidate flip-flops out of the fan-in cone of a test point. Experimental results indicate that most of the not-replaced flip-flops in [Yang 09] can be replaced and hence even more significant area reduction can be achieved with minimizing the loss of testability.", "num_citations": "10\n", "authors": ["503"]}
{"title": "SOC test compression scheme using sequential linear decompressors with retained free variables\n", "abstract": " A highly efficient SOC test compression scheme which uses sequential linear decompressors local to each core is proposed. Test data is stored on the tester in compressed form and brought over the TAM to the core before being decompressed. Very high encoding efficiency is achieved by providing the ability to share free variables across test cubes being compressed at the same time as well as in subsequent time steps. The idea of retaining unused non-pivot free variables when decompressing one test cube to help for encoding subsequent test cubes that was introduced in [Muthyala 12] is applied here in the context of SOC testing. It is shown that in this application, a first-in first-out (FIFO) buffer is not required. The ability to retain excess free variables rather than wasting them when the decompressor is reset avoids the need for high precision in matching the number of free variables used for encoding with the\u00a0\u2026", "num_citations": "9\n", "authors": ["503"]}
{"title": "An industrial case study for X-canceling MISR\n", "abstract": " An X-tolerant multiple-input signature register (MISR) compaction methodology that compacts output streams containing unknown (X) values was described in [Touba 07]. Unlike conventional approaches, it does not use X-masking logic at the input of the MISR. Instead it uses symbolic simulation to express each bit of the MISR signature as a linear equation in terms of the X's. Linearly dependent combinations of the signature bits are identified with Gaussian elimination and XORed together to cancel out all X values and yield deterministic values. This new X-canceling approach was applied to some industrial designs under the constraints imposed by an industrial test environment. Practical issues for implementing X-canceling are discussed, and a new architecture for implementing X-canceling based on using a shadow register with multiple selective XORs is presented. Experimental results are shown for industrial\u00a0\u2026", "num_citations": "9\n", "authors": ["503"]}
{"title": "Using multiple expansion ratios and dependency analysis to improve test compression\n", "abstract": " A methodology is presented for improving the amount of compression achieved by continuous-flow decompressors by using multiple ratios of scan chains to tester channels (i.e., expansion ratios). The idea is to start with a higher expansion ratio than normal and then progressively reduce the expansion ratio to detect any faults that remain undetected. By detecting faults at the highest expansion ratio possible, the amount of compression can be significantly improved compared with conventional approaches. The expansion ratio is progressively reduced by concatenating scan chains together using MUXes to make fewer and longer scan chains. Selecting which scan chains to concatenate is done by using a dependency analysis procedure that takes into account structural dependencies among the scan chains as well as free-variable dependencies in the logic driving the scan chains to improve the probability of\u00a0\u2026", "num_citations": "9\n", "authors": ["503"]}
{"title": "Matrix-based software test data decompression for systems-on-a-chip\n", "abstract": " This paper describes a new compression/decompression methodology for using an embedded processor to test the other components of a system-on-a-chip (SoC). The deterministic test vectors for each core are compressed using matrix-based operations that significantly reduce the amount of test data that needs to be stored on the tester. The compressed data is transferred from the tester to the processor's on-chip memory. The processor executes a program which decompresses the data and applies it to the scan chains of each core-under-test. The matrix-based operations that are used to decompress the test vectors can be performed very efficiently by the embedded processor thereby allowing the decompression program to be very fast and provide high throughput of the test data to minimize test time. Experimental results demonstrate that the proposed approach provides greater compression than previous\u00a0\u2026", "num_citations": "9\n", "authors": ["503"]}
{"title": "Pseudo-random pattern testing of bridging faults\n", "abstract": " While previous research has focused on deterministic testing of bridging faults, this paper studies pseudo-random testing of bridging faults and describes a means for achieving high fault coverage in a built-in self-test (BIST) environment. Bridging faults are generally more random pattern testable than stuck-at faults, but examples are shown to illustrate that some bridging faults can be much less random pattern testable than stuck-at faults. A fast method for identifying these random-pattern-resistant bridging faults is described. State-of-the-art test point insertion techniques, which are based on the stuck-at fault model, are inadequate. Data is presented which indicates that even after inserting test points that result in 100% single stuck-at fault coverage, many bridging faults are still not detected. A test point insertion procedure that targets both single stuck-at faults and non-feedback bridging faults is presented. It is\u00a0\u2026", "num_citations": "9\n", "authors": ["503"]}
{"title": "Logic synthesis for concurrent error detection\n", "abstract": " The structure of a circuit determines how the effects of a fault can propagate and hence affects the cost of concurrent error detection. By considering circuit structure during logic optimization, the overall cost of a concurrently checked circuit can be minimized. This report presents a new technique called structure-constrained logic optimization (SCLO) that optimizes a circuit under the constraint that faults in the resulting circuit can produce only a prescribed set of errors. Using", "num_citations": "9\n", "authors": ["503"]}
{"title": "Using partial masking in X-chains to increase output compaction for an X-canceling MISR\n", "abstract": " An X-Canceling MISR [Touba 07] provides the ability to tolerate unknowns (X's) in the output response with very little loss of observability of non-X values. When the density of X's is low, an X-Canceling MISR is extremely efficient as the number of control bits depends only on the total number of X's in the output response. However, for higher X-densities, an X-Canceling MISR becomes less efficient. This paper describes a very effective approach for using an X-Canceling MISR for designs with high X-density. It utilizes the idea of stitching together scan cells that capture the largest number of X's into \"X-chains\" as was proposed in [Wohl 08]: In the proposed approach, a partial X-masking approach is used for the X-chains to eliminate the vast majority of the X's at very little cost in terms of control bits. Only the X's coming from the scan cells not in the X-chains plus X's that are left unmasked in the X-chains need to be\u00a0\u2026", "num_citations": "8\n", "authors": ["503"]}
{"title": "Post-manufacturing ECC customization based on Orthogonal Latin Square codes and its application to ultra-low power caches\n", "abstract": " The paper proposes the idea of implementing a general multi-bit error correcting code (ECC) based on Orthogonal Latin Square (OLS) Codes in on-chip hardware, but then selectively, on a chip-by-chip basis, using only a subset of the code's check bits (subset of the rows in its H-matrix) depending on the defect map for a particular chip. The defect map is obtained from a memory characterization test which identifies which cells are defective or marginal. The idea proposed here is that if a general t-bit error correcting code is implemented in hardware and requires c full =n-k check bits for k information bits, then once the defect map is known, the defective cells become erasures w.r.t. the ECC. This fact can be used to select only a subset of the n-k rows in the H-matrix which are sufficient to provide the desired error detection/correction capability in the presences of the known erasures. By selectively reducing the\u00a0\u2026", "num_citations": "8\n", "authors": ["503"]}
{"title": "Test vector compression via statistical coding and dynamic compaction\n", "abstract": " This paper addresses the problem of increasingly longer test times and greater test data storage requirements for integrated circuits. A new compression/decompression technique is proposed for reducing the amount of data that must be stored on the tester and transferred to the chip. This technique uses static and dynamic compaction algorithms in conjunction with statistical coding to encode test vectors provided by circuit vendors. The decoding process is performed in hardware by a small amount of on-chip circuitry. Taken together, this compression/decompression algorithm results in both lowered tester storage requirements and reduced test times.", "num_citations": "8\n", "authors": ["503"]}
{"title": "Enhancing test compression with dependency analysis for multiple expansion ratios\n", "abstract": " Scan test data compression is widely used in industry to reduce test data volume (TDV) and test application time (TAT). This paper shows how multiple scan chain expansion ratios can help to obtain high test data compression in system-on-chips. Scan chains are partitioned with a higher expansion ratio than normal in scan compression mode and then are gradually concatenated based on a cost function to detect any faults that could not be detected at the higher expansion ratios. It improves the overall test compression ratio since it potentially allows faults to be detected at the highest expansion ratio. This paper introduces a new cost function to choose scan chain concatenation candidates for concatenation for multiple expansion ratios. To avoid TDV and TAT increase by scan concatenation, the proposed method takes a logic structure and scan chain length into consideration. Experiment results show the\u00a0\u2026", "num_citations": "7\n", "authors": ["503"]}
{"title": "Unified 3D test architecture for variable test data bandwidth across pre-bond, partial stack, and post-bond test\n", "abstract": " Conventional approaches for test architecture optimization are based on designing test access mechanisms (TAMs) and core wrappers for a particular test data bandwidth available from the tester. However, constructing three dimensional integrated circuits (3D-ICs) using known-good dies (KGD) and known-good stacks (KGS) requires pre-bond testing of die and optionally partial stack testing in addition to the final post-bond test. In each of these different test periods: pre-bond, partial stack, and final test, the test data bandwidth available for a particular die may be different. A test architecture optimized for one particular test data bandwidth may be very inefficient when the bandwidth changes. Previously proposed test optimization techniques for handling this involve designing different TAM architectures for pre-bond and post-bond test in order to minimize the test time for each different test data bandwidth. This paper\u00a0\u2026", "num_citations": "7\n", "authors": ["503"]}
{"title": "Testing core-based designs using partial isolation rings\n", "abstract": " Core-based designs pose a significant test challenge. A simple and fast solution is to place a full isolation ring (ie, boundary scan) around each core, however, the area and performance overhead for this may not be acceptable in many applications. A systematic method is presented for designing a partial isolation ring that provides the same fault coverage as a full isolation ring, but avoids adding MUXes on critical timing paths and reduces area overhead. Efficient ATPG techniques are used to analyze the user-defined logic surrounding the core and identify a maximal set of core inputs and outputs that do not need to be included in the partial isolation ring. In the case where one core is driving another core, the procedure identifies a maximal set of isolation ring elements that can be removed from the interface between the cores. Several different partial isolation ring selection strategies that vary in computational complexity are described. Experimental results are shown comparing the different strategies.", "num_citations": "7\n", "authors": ["503"]}
{"title": "Limited magnitude error correction using OLS codes for memories with multilevel cells\n", "abstract": " The dominant errors for memories with multilevel cells are due to interference and low data retention which causes the threshold voltage to shift compared to its original value. Thus, the dominant error type is limited magnitude errors. Orthogonal Latin squares (OLS) codes have been used in SRAM for their low latency decoding. In this paper, OLS codes are extended to correct limited magnitude errors. The proposed codes aim at lowering the redundancy by considering only a few bits from each symbol to compute the parity bits. An efficient methodology for constructing the decoding logic necessary to correct limited magnitude errors is described. Also, new hybrid codes are proposed in this paper which combine the proposed limited magnitude OLS codes together with low redundancy error correcting codes to further reduce the number of check-bits with some additional decoding complexity.", "num_citations": "6\n", "authors": ["503"]}
{"title": "Scan-based BIST diagnosis using an embedded processor\n", "abstract": " For system-on-chip designs that contain an embedded processor, this paper present a software based diagnosis scheme that can make use of the processor to aid in diagnosis in a scan-based built-in self-test (BIST) environment. The diagnosis scheme can be used to determine both the scan cells that capture errors as well as the failing test vectors during a BIST session, thereby allowing a faster and more precise diagnosis. The BIST session needs to be run only once for diagnosis. The scheme is based on pseudo-random linear compaction of the output response bits. The proposed scheme uses word-based linear operations that can be implemented very fast and efficiently on the embedded processor. Experimental results indicate that the proposed scheme is very powerful in identifying the failing test vectors and performs better than previous methods both in terms of the suspect set size and the accuracy of\u00a0\u2026", "num_citations": "6\n", "authors": ["503"]}
{"title": "Efficient non-binary Hamming codes for limited magnitude errors in MLC PCMs\n", "abstract": " Emerging non-volatile main memories (e.g. phase change memories) have been the continuous focus of research currently. These memories provide an attractive alternative to DRAM with their high density and low cost. But the dominant error models in these memories are of limited magnitude caused by resistance drifts. Hamming codes have been used extensively to protect DRAM due to their low decoding latency and low redundancy as well. But with limited magnitude errors, traditional Hamming codes prove to be inefficient. This paper proposes a new systematic limited magnitude error correcting non-binary Hamming code specifically to address limited magnitude errors in multilevel cell memories storing multiple bits per cell. A general construction methodology is presented to correct errors of limited magnitude and is compared to existing schemes addressing limited magnitude errors in phase change\u00a0\u2026", "num_citations": "5\n", "authors": ["503"]}
{"title": "Utilizing ATE Vector repeat with linear decompressor for test vector compression\n", "abstract": " Previous approaches for utilizing automatic test equipment (ATE) vector repeat are based on identifying runs of repeated scan data and directly generating that data using ATE vector repeat. Each run requires a separate vector repeat instruction, so the amount of compression is limited by the amount of ATE instruction memory available and the length of the runs (which typically will be much shorter than the length of a scan vector). In this paper, a new and more efficient approach is proposed for utilizing ATE vector repeat. The scan vector sequence is partitioned and decomposed into a common sequence which is the same for an entire cluster of test cubes and a unique sequence that is different for each test cube. The common sequence can be generated very efficiently using ATE vector repeat. Experimental results demonstrate that the proposed approach can achieve much greater compression while using many\u00a0\u2026", "num_citations": "5\n", "authors": ["503"]}
{"title": "Exploiting X-correlation in output compression via superset X-canceling\n", "abstract": " An alternative to masking unknown (X) values before the compactor (i.e., X-masking) is to capture X's in the MISR and cancel them out after compaction (i.e., X-canceling). Existing X-canceling methodologies require a number of control bits to perform the X-canceling that is linear in the number of X's to be canceled. This paper describes a new methodology for X-canceling which can exploit the fact that the scan cells in which X's are captured tend to be highly correlated in order to significantly reduce the number of control bits required for X-canceling. X's tend to be generated in certain portions of the design, and hence certain scan cells capture X's with much higher frequency than other scan cells. Instead of custom generating the control bits to cancel out only the X's in one MISR signature, the proposed approach finds a general superset solution which can cancel out the X's for many MISR signatures. This allows\u00a0\u2026", "num_citations": "5\n", "authors": ["503"]}
{"title": "X-Stacking-A Method for Reducing Control Data for Output Compaction\n", "abstract": " This paper proposes an X-stacking technique for efficiently reducing the number of unknown values (X's) in the output response, thereby, in turn reducing the amount of control data required for output compaction without losing fault coverage. Output response streams containing X's are fed into circular registers, which stack X's on top of one another. The contents of the circular registers are then passed on to conventional output compaction circuitry where each stack of X's can then be treated as a single X thus substantially reducing the overall number of X's present in the output data thereby requiring less control data to perform output compaction. Some loss of observability may occur due to non-X values getting stacked with X values, but this can be compensated by adding some additional top-up test vectors. Results indicate that the overall test compression even with factoring in the additional test vectors is\u00a0\u2026", "num_citations": "5\n", "authors": ["503"]}
{"title": "Iterative OPDD based signal probability calculation\n", "abstract": " This paper presents an improved method to accurately estimate signal probabilities using ordered partial decision diagrams (OPDDs) [Kodavarti 93] for partial representation of the functions at the circuit lines. OPDDs which are limited to a certain maximum number of nodes are built iteratively with different variable orderings to efficiently explore different regions of the function. Signal probability bounds (upper and lower) are computed from the OPDDs. From each OPDD, information is extracted to tighten the signal probability bound and guide the variable ordering for the next OPDD. By restricting the size of each OPDD to a small number of nodes, they can be constructed and processed quickly to obtain a fast and accurate estimate of signal probabilities. Experimental results demonstrate the effectiveness of the approach compared with existing methods", "num_citations": "5\n", "authors": ["503"]}
{"title": "Low power BIST based on scan partitioning\n", "abstract": " A built-in self-test (BIST) scheme is presented which both reduces overhead for detecting random-pattern-resistant (r.p.r.) faults as well as reduces power consumption during test. 3-valued weights are employed to detect the r.p.r. faults. The key idea is to use a new scan partitioning technique and decoding methodology that exploits correlations in the weight sets to greatly reduce the hardware overhead for multiple weight sets and reduce the number of transitions during scan shifting. The proposed scheme is simple to implement and only constrains the partitioning of scan elements into scan chains and not the scan order thereby having minimal impact on routing. Consequently, the proposed scheme can be easily implemented in standard design flows used in industry. Experiments indicate the scheme can achieve 100% fault coverage and 55% to 59% scan power reduction with relatively small hardware overhead.", "num_citations": "5\n", "authors": ["503"]}
{"title": "Low cost test vector compression/decompression scheme for circuits with a reconfigurable serial multiplier\n", "abstract": " Many chip designs contain one or more serial multipliers. A scheme is proposed to exploit this to compress the amount of data that needs to be stored on the tester and transferred to the CUT during manufacturing test. The test vectors are stored on the tester in a compressed format by expressing each test vector as a product of two numbers. While performing multiplication on these stored seeds in the Galois field modulo 2, GF(2), the multiplier states (i.e. the partial products) are tapped to reproduce the test vectors and fill the scan chains. In contrast with other test vector decompression schemes that add significant test specific hardware to the chip, the proposed scheme reduces hardware overhead by making use of existing functional circuitry. Experimental results demonstrate that a high encoding efficiency can be achieved using the proposed scheme.", "num_citations": "5\n", "authors": ["503"]}
{"title": "Layered-ECC: A class of double error correcting codes for high density memory systems\n", "abstract": " As memory technology scales, the demand for higher performance and reliable operation is increasing as well. For main memory, e.g., DRAM, a conventional single error correcting double error detecting (SEC-DED) code may not be sufficient. However, existing double error correcting (DEC) codes either have very high decoder latency or high data redundancy. For flash-based memories, e.g., NAND flash, using a highly complex decoding scheme with a large number of clock cycles for the whole procedure creates a performance bottleneck. In this paper, a layered DEC code is proposed with a simple decoding procedure. The codes are shown to strike a good balance between redundancy and decoder complexity. A general construction methodology is presented. Two different decoding schemes can be implemented using the proposed methodology. One is a low latency decoding scheme that is useful for main\u00a0\u2026", "num_citations": "4\n", "authors": ["503"]}
{"title": "Enhancing Superset-Canceling Method With Relaxed Constraints on Fault Observation\n", "abstract": " An X-tolerant multiple-input signature register (MISR) compaction methodology that compacts output streams containing unknown (X) values, called X-canceling, is an alternative to masking X values (i.e., X-masking). A number of control bits that is linear in the number of X's to be canceled are required to perform the X-canceling operation for existing X-canceling approaches. This paper proposes a new X-canceling method significantly reducing the number of control bits for X-canceling. We exploit the fact that 1) unknown values tend to be highly correlated in the scan cells (i.e., X's tend to be generated in certain portions of design) and 2) fault effects can typically be observed in a multiplicity of scan cells. Instead of custom generating the control bits to cancel out only the X's in one MISR signature, the proposed approach finds a general superset solution which can cancel out the X's for many MISR signatures without\u00a0\u2026", "num_citations": "4\n", "authors": ["503"]}
{"title": "Using asymmetric layer repair capability to reduce the cost of yield enhancement in 3D stacked memories\n", "abstract": " Three-dimensional (3D) technology makes it possible to organize memories as cell arrays stacked on logic where upper die layers contain the cell arrays and the bottom layer implements the peripheral logic. This creates new degrees of freedom that can be exploited to optimize the use of spare rows/columns to maximize yield. This paper proposes a new idea that exploits an additional degree of freedom that has not previously been utilized which is that the order of the die in the stack can be selected. The cell array dies can be ordered with the one with the most defective cells at the lowest layer, followed by next most defective, and so forth finishing with the die with the fewest defective cells on the top layer. All the cell array dies have identical designs and are manufactured identically. However, the peripheral logic die is designed in a way where it costs less to provide repair on the lower layers than it does on the\u00a0\u2026", "num_citations": "4\n", "authors": ["503"]}
{"title": "Guest Editors' Introduction: Progress in Test Compression\n", "abstract": " This special issue represents a snapshot of the progress in test compression, a key strategy for dealing with rapidly growing test data volume. Test compression involves encoding test data in a compressed form so that less data needs to be transferred, thereby reducing test time and the need for tester memory. A wide variety of test compression techniques have been developed, both for compressing test vectors and compressing output responses. In recent years, several researchers and companies have developed compression methods and products that achieve significant amounts of compression. These new methods have extended the life of legacy ATE and have been synergistic with the need for additional tests to detect the defects arising in nanometer designs. Thus, test compression continues to be a very active area.", "num_citations": "4\n", "authors": ["503"]}
{"title": "A low cost code-based methodology for tolerating multiple bit upsets in memories\n", "abstract": " Conventional error correcting code (ECC) schemes used in memories and caches cannot correct double bit errors caused by a single event upset (SEU). As memory density increases, multiple bit upsets in nearby cells become more frequent. A methodology is proposed here for deriving an error correcting code through heuristic search that can detect and correct the most likely double bit errors in a memory while minimizing the miscorrection probability of the unlikely double bit errors. A key feature of the proposed ECC is that it uses the same number of check bits as the conventional single error correcting/double error detecting (SEC-DED) codes commonly used, and has nearly identical syndrome generator/encoder area and timing overhead. Hence, there is very little additional cost to using the proposed ECC. The proposed ECC can be used instead of or in addition to bit interleaving to provide greater flexibility\u00a0\u2026", "num_citations": "4\n", "authors": ["503"]}
{"title": "Improving diagnostic resolution of delay faults in fpgas by exploiting reconfigurability\n", "abstract": " Given an FPGA that has failed to meet its timing specification, techniques are proposed to efficiently diagnose the cause of the faulty behavior. An initial list of suspect configuration logic blocks (CLBs) and interconnects is generated using six-valued fault-free simulation and critical path tracing. The initial list of suspects is then reduced by exploiting the reconfigurability of an FPGA. Experimental results indicate a dramatic reduction in the size of the suspect list.", "num_citations": "4\n", "authors": ["503"]}
{"title": "Enhanced limited magnitude error correcting codes for multilevel cell main memories\n", "abstract": " Technological advancements in emerging non-volatile memory (e.g., phase change memories) has led to viable alternative solutions to problems faced by technology scaling of conventional memories. Phase change memories suffer from the issue of resistance drifts wherein the value stored changes by a limited magnitude. Decimal arithmetic based limited magnitude Orthogonal Latin Square codes have been recently proposed to correct multiple errors based on asymmetric limited magnitude error models. This research letter proposes an additional technique which is combined with the decimal arithmetic based Orthogonal Latin Square codes previously proposed. This combined approach is shown to further reduce the data redundancy without incurring any significant penalty in terms of area and latency of the decoding and encoding circuit.", "num_citations": "3\n", "authors": ["503"]}
{"title": "Efficient one-step decodable limited magnitude error correcting codes for multilevel cell main memories\n", "abstract": " As technology scales further, dynamic random-access memory (DRAM) scaling faces numerous challenges. Emerging non-volatile main memories (e.g., phase change memories) provide an attractive solution to the challenges faced by DRAM scaling due to their high density and low cost. But phase change memories suffer from the problem of resistance drifts, which are of limited magnitude and causes read reliability degradation. For single error correction, Hamming codes and beyond single-error correction, Orthogonal Latin Square codes have been used due to their one-step decoding procedure. But with the limited magnitude error model, these codes are inefficient since they incur considerable redundancy. This paper proposes a new systematic limited magnitude error correcting non-binary Hamming code for a single-error correction specifically to address limited magnitude errors in multilevel cell memories\u00a0\u2026", "num_citations": "3\n", "authors": ["503"]}
{"title": "Online correction of hard errors and soft errors via one-step decodable OLS codes for emerging last level caches\n", "abstract": " The number of marginal cells in static random-access memory (SRAM) increases as technology scales further. Spin transfer torque magnetic random-access memory (STT-MRAM) which is gaining popularity as a L3 cache alternative due to their small size, nonvolatility, high endurance, fast speed, scalability and low power consumption, also suffers from various stochastic and process variations causing both hard-errors and soft-errors in field. Orthogonal Latin Square (OLS) codes have been conventionally used in SRAMs for their low latency decoding. In this paper, an error-correcting scheme is proposed based on OLS codes which corrects both hard- errors and soft-errors simultaneously in a single step. The idea proposed is that in general a t-error correcting OLS code can be modified so that it can correct a certain number (t s  <; t) of soft-errors while tolerating a certain number of hard errors as well. This is\u00a0\u2026", "num_citations": "3\n", "authors": ["503"]}
{"title": "Systematic b-adjacent symbol error correcting reed-solomon codes with parallel decoding\n", "abstract": " With technology scaling, the probability of write disturbances affecting neighboring memory cells in nonvolatile memories is increasing. Multilevel cell (MLC) phase change memories (PCM) specifically suffer from such errors which affects multiple adjacent memory cells. Reed Solomon (RS) codes offer good error protection since they can correct multi-bit symbols at a time. But beyond single symbol error correction, the decoding complexity as well as the decoding latency is very high. This paper proposes a systematic b-adjacent symbol error correcting code based on Reed-Solomon codes with a low latency and low complexity parallel one step decoding scheme. A general code construction methodology is presented which can correct any errors within b-adjacent symbols. The proposed codes are compared to existing adjacent symbol error correcting Reed-Solomon codes, and it is shown that the proposed codes\u00a0\u2026", "num_citations": "3\n", "authors": ["503"]}
{"title": "Exploiting unused spare columns and replaced columns to enhance memory ECC\n", "abstract": " Due to the emergence of extremely high density memory along with the growing number of embedded memories, memory yield is an important issue. Memory self-repair using redundancies to increase the yield of memories is widely used. Because high density memories are vulnerable to soft errors, memory error correction code (ECC) plays an important role in memory design. In this paper, methods to exploit spare columns including replaced defective columns are proposed to improve memory ECC. To utilize replaced defective columns, the defect information needs to be stored. Two approaches to store defect information are proposed-one is to use a spare column and the other is to use a content-addressable-memory. Experimental results show that the proposed method can significantly enhance the ECC performance.", "num_citations": "3\n", "authors": ["503"]}
{"title": "Using symbolic canceling to improve diagnosis from compacted response\n", "abstract": " This paper addresses the problem of performing diagnosis using production test results in a test compression environment. For linear response compactors, such as multiple-input shift register (MISRs), diagnosis must be performed from signatures. The key idea in this work is to use symbolic canceling in MISR signatures to extract information from both the MISR signature bits with errors as well as those that are error-free to provide more precise diagnostic information. A fundamentally new technique for precisely identifying error locations for propagation cones reaching fewer scan cells than the size of the MISR is described. The proposed approach does not require any additional hardware or extra data to be collected. It uses off-line software-based processing (symbolic simulation combined with Gaussian elimination) to extract information from signatures to deduce error locations even when there are a large\u00a0\u2026", "num_citations": "3\n", "authors": ["503"]}
{"title": "Improving X-tolerant combinational output compaction via input rotation\n", "abstract": " Combinational linear compactors can be used to compact the output response for a large number of scan chains into a smaller number of outputs. While some compactor designs can guarantee observation of all scan chains in the presence of a small number of X's, this may not be sufficient for designs with higher X densities. This paper describes an approach for using a combinational rotator between the scan chains and compactor to allow detection of faults even in the presence of high X densities. It is shown that the number of control inputs to the rotator is comparable to the number of control inputs required by conventional X masking approaches, but by not masking, the proposed approach is able to provide higher observability which translates to fewer test patterns, better compression, and better coverage of non-modeled faults. Moreover, the control data for the rotator has many more don't cares than the\u00a0\u2026", "num_citations": "3\n", "authors": ["503"]}
{"title": "CSER: BISER-based concurrent soft-error resilience\n", "abstract": " This paper presents a concurrent soft-error resilience (CSER) scheme with features that aid manufacturing test, online debug, and defect tolerance. The proposed CSER scheme is based on the built-in soft-error resilience (BISER) technique. A BISER cell is redesigned into various robust CSER cells that provide slow-speed snapshot, manufacturing test, slow-speed signature analysis, and defect tolerance capabilities. The cell-level area, power, and performance overhead of the robust CSER cells were found to be generally within 1% to 22% of the BISER cell.", "num_citations": "3\n", "authors": ["503"]}
{"title": "Efficiently utilizing ATE vector repeat for compression by scan vector decomposition\n", "abstract": " Previous approaches for utilizing ATE vector repeat are based on identifying runs of repeated scan data and directly generating that data using ATE vector repeat. Each run requires a separate vector repeat instruction, so the amount of compression is limited by the amount of ATE instruction memory available and the length of the runs (which typically will be much shorter than the length of a scan vector). In this paper a new and more efficient approach is proposed for utilizing ATE vector repeat. The scan vector sequence is partitioned and decomposed into a common sequence which is the same for an entire cluster of test cubes and a unique sequence that is different for each test cube. The common sequence can be generated very efficiently using ATE vector repeat. Experimental results demonstrate that the proposed approach can achieve much greater compression while using many fewer vector repeat\u00a0\u2026", "num_citations": "3\n", "authors": ["503"]}
{"title": "Fault-Tolerance Projects at Stanford CRC\n", "abstract": " This paper describes the fault-tolerant computing research currently active at Stanford University\u2019s Center for Reliable Computing. One focus is on tolerating hardware faults by means of software (software-implemented hardware fault tolerance). This work mainly targets faults caused by radiation induced upsets. An experiment evaluating the techniques that we have developed, is currently running on the ARGOS satellite. Another focus is on fault-tolerance techniques for adaptive computing systems implemented with field-programmable gate arrays (FPGAs).", "num_citations": "3\n", "authors": ["503"]}
{"title": "A new class of single burst error correcting codes with parallel decoding\n", "abstract": " With technology scaling, burst errors or clustered errors are becoming increasingly common in different types of memories. Multiple bit upsets due to particle strikes, write disturbance errors, and magnetic field coupling are a few of the mechanisms which cause clustered errors. In this article, a new class of single burst error correcting codes are presented which correct a single burst of any size b within a codeword. A code construction methodology is presented which enables us to construct the proposed scheme from existing codes, e.g., Hamming codes. A new single step decoding methodology for the proposed class of codes is also presented which enables faster decoding. Different code constructions using Hamming codes, and BCH codes have been presented in this paper and a comparison is made with existing schemes in terms of decoding complexity and data redundancy. The proposed scheme in all\u00a0\u2026", "num_citations": "2\n", "authors": ["503"]}
{"title": "Output compaction for high X-densities via improved input rotation compactor design\n", "abstract": " Testing requires checking whether the output response of a circuit or system is correct or has an error. Combinational linear compactors can be used to compact the output response for a large number of scan chains into a smaller number of outputs. While some compactor designs can guarantee observation of all scan chains in the presence of a small number of X's (unknowns), this may not be sufficient for designs with higher X densities. This paper presents a completely new methodology for designing an output compactor based on using an input rotator that is able to maintain high observability even in the presence of high X-densities while still achieving high compaction ratios. The key idea is to place a combinational rotator in front of a carefully designed XOR network that maximizes separation of the input dependence in adjacent inputs within a particular shift distance of the input rotator. A systematic\u00a0\u2026", "num_citations": "2\n", "authors": ["503"]}
{"title": "Method and apparatus for hybrid ring generator design\n", "abstract": " A method and apparatus for generating a pseudorandom sequence using a hybrid ring generator with low hardware cost. When a primitive polynomial over GF (2) is selected as the characteristic polynomial f (x) to construct a hybrid ring generator, the circuit implementing f (x) will generate a maximum-length sequence (m-sequence). The hybrid ring generator offers unmatched benefits over existing linear feedback shift register (LFSR) based maximum-length sequence generators (MLSGs). Assume k 2-input XOR gates are required in a standard or modular LFSR design. These benefits include requiring only (k+ 1)/2 2-input XOR gates, having at most one level of a 2-input XOR gate between any pair of flip-flops, enabling the output of each flip-flop to drive at most 2 fanout nodes, and creating a highly regular structure that makes the new design more layout and timing friendly.", "num_citations": "2\n", "authors": ["503"]}
{"title": "Test Point Insertion with Control Point by Greater Use of Existing Functional Flip\u2010Flops\n", "abstract": " This paper presents a novel test point insertion (TPI) method for a pseudo\u2010random built\u2010in self\u2010test (BIST) to reduce the area overhead. Recently, a new TPI method for BISTs was proposed that tries to use functional flip\u2010flops to drive control test points instead of adding extra dedicated flip\u2010flops for driving control points. The replacement rule used in a previous work has limitations preventing some dedicated flip\u2010flops from being replaced by functional flip\u2010flops. This paper proposes a logic cone analysis\u2013based TPI approach to overcome the limitations. Logic cone analysis is performed to find candidate functional flop\u2010flops for replacing dedicated flip\u2010flops. Experimental results indicate that the proposed method reduces the test point area overhead significantly with minimal loss of testability by replacing the dedicated flip\u2010flops.", "num_citations": "2\n", "authors": ["503"]}
{"title": "Reducing test time for 3D-ICs by improved utilization of test elevators\n", "abstract": " A highly efficient test compression scheme for 3D-ICs is proposed, which uses sequential linear decompressors local to each core. The compressed test data is brought from the tester over the test access mechanism (TAM) to the cores where they are decompressed. The idea is to provide flexibility in the utilization of the free variables (i.e., bits stored on the tester that can be assigned 0 or 1), so that the free variables that are not used in one core can be used to encode test cubes in other cores. The decompressors are daisy-chained, such that some of the free variables brought in to one decompressor are passed on to the other decompressors. Consequently, the free variables are also shared with the decompressors in other layers. This enables better utilization of free variables, since free variables not used in one decompressor can be used by any of the other decompressors with which they are shared. The\u00a0\u2026", "num_citations": "2\n", "authors": ["503"]}
{"title": "Efficient algorithm for test vector decompression using an embedded processor\n", "abstract": " A new algorithm for implementing test vector compression in software is presented. It is based on a compression procedure recently invented by Melhem, et al., called RDIS (recursively defined invertible set) [Melham 12]. While RDIS was originally proposed for a memory correction application, it is shown here to be very well suited for the problem of compressing test vectors. When the inputs that are unassigned during ATPG are left as don't cares (forming \u201ctest cubes\u201d), typically only 1-2% of the remaining bits are care bits. It is shown that the RDIS procedure is very efficient when the number of care bits is small and is able to achieve large amounts of compression. The RDIS procedure works by recursively constructing invertible sets and efficiently encoding the information with row and column counts. The compressed data is stored on the tester. The tester then transfers this data to an embedded processor's memory\u00a0\u2026", "num_citations": "2\n", "authors": ["503"]}
{"title": "Efficient compression of x-masking control data via dynamic channel allocation\n", "abstract": " A scheme is presented which uses one sequential linear decompressor to decompress test cubes and another sequential linear decompressor to decompress mask control data for masking unknown X's in the output response. However, instead of the conventional approach of using a fixed number of tester channels for driving each, a method for dynamically adjusting the number of channels driving each is proposed. The key idea is that by carefully ordering the test cubes such that the channel requirements of the decompressor used for masking the previous output response (currently being scanned out) can be balanced with the channel requirements of the decompressor used for scanning in the current test cube. By matching test cubes with more specified bits with output response with fewer X's, and test cubes with fewer specified bits with output response with more X's through careful ordering of the test cubes\u00a0\u2026", "num_citations": "2\n", "authors": ["503"]}
{"title": "Reducing Cost of Yield Enhancement in 3-D Stacked Memories Via Asymmetric Layer Repair Capability\n", "abstract": " One way to organize 3-D memories is cell arrays stacked on logic where the upper die layers contain the cell arrays and the bottom layer implements the peripheral logic. A new degree of freedom exists when constructing 3-D memories, which is that the order of the die in the stack can be selected. This paper proposes a new idea that exploits this additional degree of freedom to reduce the cost of yield enhancement. In the proposed approach, the cell array die with the most defective cells is placed in the lowest layer, followed by the next most defective cells in the second lowest layer, and so forth finishing with the die with the fewest defective cells on the top layer. The bottommost layer (peripheral logic) is designed such that it costs less to tolerate the defects on the lower layers than it does on higher layers of the cell arrays. This is done by limiting the domain over which some spares can be used thereby reducing\u00a0\u2026", "num_citations": "2\n", "authors": ["503"]}
{"title": "Fault-tolerant design\n", "abstract": " Publisher SummaryThis chapter provides an overview of a number of fault-tolerant design schemes suitable for nanometer system-on-chip applications. Fault tolerance is the ability of a system to continue error-free operation in the presence of an unexpected fault. There are many different fault-tolerant design schemes. Choosing the scheme to use in a particular design depends on what types of faults need to be tolerated such as temporary or permanent, single or multiple point failures, and the constraints on area, performance, and power. As technology continues to scale and circuits become increasingly prone to failure, achieving sufficient fault tolerance will be a major design issue. It starts with an introduction to the basic concepts in fault-tolerant design and the metrics used to specify and evaluate the dependability of the design. Fault tolerance requires some form of redundancy in time, hardware, or information\u00a0\u2026", "num_citations": "2\n", "authors": ["503"]}
{"title": "Altering a Pseudo-Random Bit Sequence for Mixed-Mode Scan BIST\n", "abstract": " One approach for built-in self-test (BIST) of circuits with scan is to use a linear feedback shift register (LFSR) to shift a pseudo-random sequence of bits into the scan chain. When a pattern has been shifted into the scan chain, it is applied to the circuit-under-test (CUT) and the response is loaded back into the scan chain and shifted out into a signature register for compaction as the next pattern is shifted into the scan chain. Figure 1 shows a block diagram for this\" test-per-scan\" approach. Unfortunately, many circuits contain random-pattern-resistant (rpr) faults which limit the fault coverage that can be achieved with this approach. One solution is to insert test points in the circuit to increase the detection probabilities of the rpr faults. However, test points require that the function logic be modified, and more importantly, they add delay to the circuit which can degrade system performance. Another solution is to use\" mixed-mode\" testing in which deterministic patterns are used to detect the rpr faults that the pseudo-random patterns miss. This paper presents a new approach for mixed-mode testing in which deterministic test cubes (ie, deterministic patterns in which the unspecified inputs are left as X's) are embedded in the pseudo-random sequence of bits. Logic is added at the serial output of the LFSR to alter the pseudo-random bit sequence so that it is guaranteed to contain patterns that detect the rpr faults. The new approach can be used for either a\" test-per-scan\" or\" test-per-clock\" scheme and is capable of providing complete fault coverage with a reasonable test length. For circuits with scan, no function logic modification is required, and there is no\u00a0\u2026", "num_citations": "2\n", "authors": ["503"]}
{"title": "Test Point Insertion for Non-Feedback Bridging Faults\n", "abstract": " This paper studies pseudo-random pattern testing of bridging faults. Although bridging faults are generally more random pattern testable than stuck-at faults, examples are shown to illustrate that some bridging faults can be much less random pattern testable than stuck-at faults. A fast method for identifying these random-pattern-resistant bridging faults is described. It is shown that state-of-the-art test point insertion techniques, which are based on the stuck-at fault model, are inadequate. Data is presented which indicates that even after inserting test points that result in 100% single stuck-at fault coverage, many bridging faults are still not detected. A test point insertion procedure that targets both single stuck-at faults and non-feedback bridging faults is presented. It is shown that by considering both types of faults when selecting the location for test points, higher fault coverage can be obtained with little or no increase in overhead. Thus, the test point insertion procedure described here is a lowcost way to improve the quality of built-in self-test. While this paper considers only non-feedback bridging faults, the techniques that are described can be applied to feedback bridging faults in a straightforward manner.", "num_citations": "2\n", "authors": ["503"]}
{"title": "Selective Checksum based On-line Error Correction for RRAM based Matrix Operations\n", "abstract": " Resistive RAM technology with it's in memory computation and matrix vector multiplication capabilities has paved the way for efficient hardware implementations of neural networks. The ability to store the training weights and perform a direct matrix vector multiplication with the applied inputs thus producing the outputs directly reduces a lot of memory transfer overhead. But such schemes are prone to various soft errors and hard errors due to immature fabrication processes creating marginal cells, read disturbance errors, etc. Soft errors are of concern in this case since they can potentially cause mi-classification of objects leading to catastrophic consequences for safety critical applications. Since the location of soft errors are not known previously, they can potentially manifest in the field leading to data corruption. In this paper, a new on-line error correcting scheme is proposed based on partial and selective\u00a0\u2026", "num_citations": "1\n", "authors": ["503"]}
{"title": "A graph theory approach towards IJTAG security via controlled scan chain isolation\n", "abstract": " The IEEE Std. 1687 (IJTAG) was designed to provide on-chip access to the various embedded instruments (e.g. built-in self test, sensors, etc.) in complex system-on-chip designs. IJTAG facilitates access to on-chip instruments from third party intellectual property providers with hidden test-data registers. Although access to on-chip instruments provides valuable data specifically for debug and diagnosis, it can potentially expose the design to untrusted sources and instruments that can sniff and possibly manipulate the data that is being shifted through the IJTAG network. This paper provides a comprehensive protection scheme against data sniffing and data integrity attacks by selectively isolating the data flowing through the IJTAG network. The proposed scheme is modeled as a graph coloring problem to optimize the number of isolation signals required to protect the design. It is shown that combining the proposed\u00a0\u2026", "num_citations": "1\n", "authors": ["503"]}
{"title": "Computing with obfuscated data in arbitrary logic circuits via noise insertion and cancellation\n", "abstract": " In secure computing, sensitive data must be kept private by protecting it from being obtained by an attacker. Existing techniques for computing with encrypted data are either prohibitively expensive (e.g., fully homomorphic encryption) or only work for special cases. (e.g., only for linear circuits). This paper presents a lightweight methodology for computing with noise-obfuscated data by carefully selecting internal locations for noise cancellation in arbitrary logic circuits. Noise is inserted in the data before computation and then partially cancelled during the computation and fully cancelled at the outputs. While the proposed methodology does not provide the level of strong encryption that fully homomorphic encryption would provide, it has the advantage of being lightweight, easy to implement, and can be deployed with relatively minimal performance impact. A key idea in the proposed approach is to reduce the\u00a0\u2026", "num_citations": "1\n", "authors": ["503"]}
{"title": "Compacting output responses containing unknowns using an embedded processor\n", "abstract": " In system-on-chip (SOC) designs, embedded processors are frequently present as part of the functional design and can be used to help test the chip or system by providing a software-based test. Previous work has looked at compacting output responses in software by performing signature analysis using either arithmetic operations or by implementing a multi-input signature register (MISR) in software. However, these approaches cannot be used when the output response contains unknown (X) values. While it is possible to precisely mask all X's present in the output response in software, a straightforward approach would require a very large amount of mask data to specify which bits to mask. This paper proposes an efficient method for compacting output responses with X's in software using the concept of canceling X's from signatures as proposed in [Touba 07], Whereas the efficiency of the hardware\u00a0\u2026", "num_citations": "1\n", "authors": ["503"]}
{"title": "Implementing defect tolerance in 3D-ICs by exploiting degrees of freedom in assembly\n", "abstract": " When assembling a three-dimensional integrated circuit (3D-IC), there are several degrees of freedom including which die are stacked together, in what order, and with what rotational symmetry. This paper describes strategies for exploiting these degrees of freedom to reduce the cost and complexity of implementing defect tolerance. Conventional defect tolerance schemes involve bypassing defects by reconfiguring the circuitry so that system operation is performed using defect-free circuitry. Explicit reconfiguration circuitry is required to perform the reconfiguration, and the power distribution network must be designed to support all redundant elements. The schemes proposed in this paper use the degrees of freedom that exist when a 3D-IC is assembled at manufacture time to implicitly bypass manufacturing defects without the need for explicit reconfiguration circuitry. Defects are identified during manufacture test\u00a0\u2026", "num_citations": "1\n", "authors": ["503"]}
{"title": "High-Speed Hybrid Ring Generator Design Providing Maximum-Length Sequences with Low Hardware Cost\n", "abstract": " A new class of hybrid ring generators is developed to generate maximum-length sequences with low hardware cost. The new design improves the operational speed of the hybrid linear feedback shift register (LFSR) proposed in [12] to receive the high speed and simplified layout benefits of the ring generator offered in [6]. As a result, the hybrid ring generator offers unmatched benefits over existing linear feedback shift register (LFSR) based designs. Assume k 2-input XOR gates are required in a standard or modular LFSR design. These benefits include requiring only (k+ 1)/2 XOR gates, having at most one level of a 2-input XOR gate between any pair of flip-flops, enabling the output of each flip-flop to drive at most 2 fanout nodes, and creating a highly regular structure that makes the new design more layout and timing friendly.", "num_citations": "1\n", "authors": ["503"]}
{"title": "Correlation-based rectangular encoding\n", "abstract": " In this paper, a technique is presented for improving the compression achieved with any linear decompressor by adding a small nonlinear decoder that exploits bit-wise and pattern-wise correlations present in test vectors. The proposed nonlinear decoder has a regular and compact structure, and allows continuous-flow decompression. It has a very important feature, which is that its design does not depend on the test data. This simplifies the design flow and allows the decoder to be reused when testing multiple cores on a chip. Experimental results show that combining a linear decompressor with the small nonlinear decoder proposed here significantly improves the overall compression.", "num_citations": "1\n", "authors": ["503"]}
{"title": "Fundamentals of CMOS design\n", "abstract": " Publisher SummaryThis chapter discusses a few basic and important concepts of complementary metal oxide semiconductor (CMOS) technology to aid in the learning process and facilitates greater understanding of the electronic design automation (EDA) subjects. An overview of the fundamental integrated-circuit technology and CMOS logic design is provided, and a few more advanced CMOS technologies that can be used to reduce transistor count, increase circuit speed, or reduce power consumption for modern VLSI designs are dealt with. The physical design aspects, how to translate a CMOS logic design to a CMOS physical design for fabrication are reviewed. CMOS technology has been the backbone of the many advances that have taken place in the past two decades, powering consumer appliances, automotives, personal and scientific computing, as well as many fascinating science and space\u00a0\u2026", "num_citations": "1\n", "authors": ["503"]}
{"title": "Test Compression\n", "abstract": " Publisher SummaryThis chapter introduces the the basic concepts and principles of test compression. Test compression involves compressing the amount of test data (both stimulus and response) that must be stored on automatic test equipment (ATE) for testing with a deterministic (automatic test pattern generation [ATPG]-generated) test set. The idea in test compression is to compress the amount of test data (both stimulus and response) that is stored on the tester. This provides two advantages: the first is that it reduces the amount of tester memory that is required, and the second advantage is that it reduces test time because less test data has to be transferred across the low bandwidth link between the tester and the chip. The chapter focuses on test stimulus compression and describes three different categories of schemes: using data compression codes, employing linear decompression, and broadcasting the\u00a0\u2026", "num_citations": "1\n", "authors": ["503"]}
{"title": "Methods for improving test compression\n", "abstract": " The approach that the author takes for this panel is to start with a vanilla sequential linear decompressor and look at the results it obtains. Then the author adds different enhancements to it along the lines of the factors discussed here and see what impact these have on the results", "num_citations": "1\n", "authors": ["503"]}
{"title": "HTPG: hybrid test pattern generation for reducing test storage\n", "abstract": " This paper describes a hybrid test pattern generation (HTPG) scheme that is a hybrid approach between BIST and conventional external testing. HTPG speeds up conventional ATPG and also reduces tester storage compared to full external testing. Experimental results show a comparison between the proposed HTPG scheme with conventional ATPG and BIST and also the effectiveness of different percentages of pseudo-random and deterministic data in a test pattern generated by HTPG. The results show that having 10-15% deterministic data per scan vector is most effective", "num_citations": "1\n", "authors": ["503"]}