{"title": "Decentralized voting platform based on ethereum blockchain\n", "abstract": " In centralized environments, the results of voting events have always been questionable and perceived differently by voters. Most existing E-Voting systems are based on centralized servers where the voters must trust the organizing authority for the integrity of the results. In this paper we propose a novel approach for a decentralized trustless voting platform that relies on Blockchain technology to solve the trust issues. The main features of this system include ensuring data integrity and transparency, and enforcing one vote per mobile phone number for every poll with ensured privacy. To accomplish this, the Ethereum Virtual Machine (EVM) is used as the Blockchain runtime environment, on which transparent, consistent and deterministic smart contracts will be deployed by organizers for each voting event to run the voting rules. Users are authenticated through their mobile phone numbers without the need of a third\u00a0\u2026", "num_citations": "45\n", "authors": ["1676"]}
{"title": "Efficient neural chaotic generator for image encryption\n", "abstract": " In this paper, we propose a new implementation of chaotic generator using artificial neural network. Neural network can act as an efficient source of perturbation in the chaotic generator which increases the cycle\u02bcs length, and thus avoid the dynamical degradation due to the used finite dimensional space. On the other hand, the use of neural network enlarges the key space of the chaotic generator in an enormous way. The efficiency of the proposed neural chaotic generator is illustrated using some dynamical and NIST statistical tests. We also propose in this paper, a new image encryption method based on chaotic sequence, and the obtained results emphasize the efficiency of our technique.", "num_citations": "32\n", "authors": ["1676"]}
{"title": "A framework for analyzing verifiability in traditional and electronic exams\n", "abstract": " The main concern for institutions that organize exams is to detect when students cheat. Actually more frauds are possible and even authorities can be dishonest. If institutions wish to keep exams a trustworthy business, anyone and not only the authorities should be allowed to look into an exam\u2019s records and verify the presence or the absence of frauds. In short, exams should be verifiable. However, what verifiability means for exams is unclear and no tool to analyze an exam\u2019s verifiability is available. In this paper we address both issues: we formalize several individual and universal verifiability properties for traditional and electronic exams, so proposing a set of verifiability properties and clarifying their meaning, then we implement our framework in ProVerif, so making it a tool to analyze exam verifiability. We validate our framework by analyzing the verifiability of two existing exam systems \u2013 an electronic\u00a0\u2026", "num_citations": "24\n", "authors": ["1676"]}
{"title": "Formal analysis of e-cash protocols\n", "abstract": " Electronic cash (e-cash) aims at achieving client privacy at payment, similar to real cash. Several security protocols have been proposed to ensure privacy in e-cash, as well as the necessary unforgery properties. In this paper, we propose a formal framework to define, analyze, and verify security properties of e-cash systems. To this end, we model e-cash systems in the applied p-calculus, and we define two client privacy properties and three properties to prevent forgery. Finally, we apply our definitions to an e-cash protocol from the literature proposed by Chaum et al., which has two variants and a real implementation based on it. Using ProVerif, we demonstrate that our framework is suitable for an automated analysis of this protocol.", "num_citations": "21\n", "authors": ["1676"]}
{"title": "Formal analysis of electronic exams\n", "abstract": " Universities and other educational organizations are adopting computer and Internet-based assessment tools (herein called e-exams) to reach widespread audiences. While this makes examination tests more accessible, it exposes them to new threats. At present, there are very few strategies to check such systems for security, also there is a lack of formal security definitions in this domain. This paper fills this gap: in the formal framework of the applied n-calculus, we define several fundamental authentication and privacy properties and establish the first theoretical framework for the security analysis of e-exam protocols. As proof of concept we analyze two of such protocols with ProVerif. The first \u201csecure electronic exam system\u201d proposed in the literature turns out to have several severe problems. The second protocol, called Remark!, is proved to satisfy all the security properties assuming access control on the bulletin\u00a0\u2026", "num_citations": "18\n", "authors": ["1676"]}
{"title": "Monitoring electronic exams\n", "abstract": " Universities and other educational organizations are adopting computer-based assessment tools (herein called e-exams) to reach larger and ubiquitous audiences. While this makes examination tests more accessible, it exposes them to unprecedented threats not only from candidates but also from authorities, which organize exams and deliver marks. Thus, e-exams must be checked to detect potential irregularities. In this paper, we propose several monitors, expressed as Quantified Event Automata (QEA), to monitor the main properties of e-exams. Then, we implement the monitors using MarQ, a recent Java tool designed to support QEAs. Finally, we apply our monitors to logged data from real e-exams conducted by Universit\u00e9 Joseph Fourier at pharmacy faculty, as a part of Epreuves Classantes Nationales informatis\u00e9es, a pioneering project which aims to realize all french medicine exams electronically\u00a0\u2026", "num_citations": "12\n", "authors": ["1676"]}
{"title": "On the verifiability of (electronic) exams\n", "abstract": " The main concern for institutions that organize exams is to detect when students cheat.  Actually more frauds are possible and even authorities can be dishonest.  If institutions wish to keep exams a trustworthy business, anyone and not only the authorities should be allowed to look into an exam\u2019s records and verify the presence or the absence of frauds.  In short, exams should be verifiable.  However, what verifiability means for exams is unclear and no tool to analyze an exam\u2019s verifiability is available. In this paper we address both issues: we formalize several individual and universal verifiability properties for traditional and electronic exams, so proposing a set of verifiability properties and clarifying their meaning, then we implement our framework in ProVerif, so making it a tool to analyze exam verifiability.  We validate our framework by analyzing the verifiability of two existing exam systems \u2013 an electronic and a paper-and-pencil system.", "num_citations": "8\n", "authors": ["1676"]}
{"title": "Formal analysis and offline monitoring of electronic exams\n", "abstract": " More and more universities are moving toward electronic exams (in short e-exams). This migration exposes exams to additional threats, which may come from the use of the information and communication technology. In this paper, we identify and define several security properties for e-exam systems. Then, we show how to use these properties in two complementary approaches: model-checking and monitoring. We illustrate the validity of our definitions by analyzing a real e-exam used at the pharmacy faculty of University Grenoble Alpes (UGA ) to assess students. On the one hand, we instantiate our properties as queries for ProVerif, an automatic verifier of cryptographic protocols, and we use it to check our modeling of UGA exam specifications. ProVerif found some attacks. On the other hand, we express our properties as Quantified Event Automata (QEAs), and we synthesize them into monitors using\u00a0\u2026", "num_citations": "6\n", "authors": ["1676"]}
{"title": "New chaotic image encryption technique\n", "abstract": " This paper presents a new image encryption technique based on neural chaotic generator. This encryption technique includes two main operations, permutation at pixel level and masking and permutation at bit level. The chaotic generator used in the encryption of image is perturbed by a new technique done by artificial neural network. Simulations show that the proposed encryption technique is effective and has a high level security.", "num_citations": "5\n", "authors": ["1676"]}
{"title": "BISM: bytecode-level instrumentation for software monitoring\n", "abstract": " BISM (Bytecode-level Instrumentation for Software Monitoring) is a lightweight Java bytecode instrumentation tool which features an expressive high-level control-flow-aware instrumentation language. The language follows the aspect-oriented programming paradigm by adopting the joinpoint model, advice inlining, and separate instrumentation mechanisms. BISM provides joinpoints ranging from bytecode instruction to method execution, access to comprehensive context information, and instrumentation methods. BISM runs in two modes: build-time and load-time. We demonstrate BISM effectiveness using two experiments: a security scenario and a general runtime verification case. The results show that BISM instrumentation incurs low runtime and memory overheads.", "num_citations": "4\n", "authors": ["1676"]}
{"title": "Formal verification of e-reputation protocols\n", "abstract": " Reputation systems are often useful in large online communities in which most of the users are unknown to each other. They are good tools to force the users to act in truthfulness way. However, for a reputation system to work effectively users have to be willing to provide rates. In order to incentivize the users to provide honest rates, a reputation system have to ensure their privacy and anonymity. Users are also concerned about verifying the correctness of the reputation score. In the applied pi-calculus, we define a formal framework and several fundamental privacy, authentication, and verifiability properties suitable for the security analysis of e-reputation protocols. As proof of concept, using ProVerif, we analyze a simple additive decentralized reputation protocol proposed to ensure rate privacy if all users are honest.", "num_citations": "3\n", "authors": ["1676"]}
{"title": "Formal security analysis of traditional and electronic exams\n", "abstract": " Nowadays, students can be assessed not only by means of pencil-and-paper tests, but also by electronic exams which they take in examination centers or even from home. Electronic exams are appealing as they can reach larger audiences, but they are exposed to new threats that can potentially ruin the whole exam business. These threats are amplified by two issues: the lack of understanding of what security means for electronic exams (except the old concern about students cheating), and the absence of tools to verify whether an exam process is secure. This paper addresses both issues by introducing a formal description of several fundamental authentication and privacy properties, and by establishing the first theoretical framework for an automatic analysis of exam security. It uses the applied -calculus as a framework and ProVerif as a tool. Three exam protocols are checked in depth: two Internet\u00a0\u2026", "num_citations": "3\n", "authors": ["1676"]}
{"title": "Differential inference testing: A practical approach to evaluate sanitizations of datasets\n", "abstract": " In order to protect individuals' privacy, data have to be \"well-sanitized\" before sharing them, i.e. one has to remove any personal information before sharing data. However, it is not always clear when data shall be deemed well-sanitized. In this paper, we argue that the evaluation of sanitized data should be based on whether the data allows the inference of sensitive information that is specific to an individual, instead of being centered around the concept of re-identification. We propose a framework to evaluate the effectiveness of different sanitization techniques on a given dataset by measuring how much an individual's record from the sanitized dataset influences the inference of his/her own sensitive attribute. Our intent is not to accurately predict any sensitive attribute but rather to measure the impact of a single record on the inference of sensitive information. We demonstrate our approach by sanitizing two real\u00a0\u2026", "num_citations": "2\n", "authors": ["1676"]}
{"title": "Automated verification of e-cash protocols\n", "abstract": " Electronic cash (e-cash) permits secure e-payments by providing security and anonymity similar to real cash. Several protocols have been proposed to meet security and anonymity properties of e-cash. However, there are no general formal definitions that allow the automatic verification of e-cash protocols. In this paper, we propose a formal framework to define and verify security properties of e-cash protocols. To this end, we model e-cash protocols in the applied -calculus, and we formally define five relevant security properties. Finally, we validate our framework by analyzing, using the automatic tool ProVerif, four e-cash protocols: the online and the offline Chaum protocols, the Digicash protocol, and the protocol by Petersen and Poupard.", "num_citations": "2\n", "authors": ["1676"]}
{"title": "A more realistic model for verifying route validity in ad-hoc networks\n", "abstract": " Many cryptographic protocols aim at ensuring the route validity in ad-hoc networks, i.e. the established route representing an exists path in the network  . However, flaws have been found in some protocols that are claimed secure (e.g. the attack on SRP applied to DSR). Some formal models and reduction proofs have been proposed to give more guarantees when verifying route validity and facilitate verification process. The existing approaches assume the cooperative attacker model. In this paper, we consider the non-cooperative attacker model, and we show that verifying the route validity under the non-cooperative model requires to verify only five topologies, each containing four nodes, and to consider only three malicious (compromised) nodes. Furthermore, we prove that a protocol is secure for any topology under the non-cooperative model, if and only if, it is secure for any topology under the\u00a0\u2026", "num_citations": "2\n", "authors": ["1676"]}
{"title": "Multiple independent lazy intruders\n", "abstract": " We consider a model of multiple independent intruders that have no ability to share knowledge between each other. We use this model to analyze security in wireless ad-hoc networks, where each intruder has a local control in the network, ie, he can read and send messages only to his direct neighbors. Another application is the mobile ambient calculus where several intruder processes are not able to exchange their knowledge. Both these security problems can be reduced to satisfiability of lazy intruder constraint systems, for a bounded number of steps of the honest agents. However, the constraint-based verification method usually relies on a well-formedness property of constraints. This well-formedness entails that the constraints can be ordered so that the intruder knowledge is monotonically growing. This does not hold for several intruders that learn independent of each other. For the resulting generalized class of weak-well-formed constraints, we give a novel constraint reduction procedure and prove that it is sound, complete and terminating. We also prove that it is NP-complete.", "num_citations": "2\n", "authors": ["1676"]}
{"title": "Differential inference testing a practical approach to evaluate anonymized data\n", "abstract": " In order to protect individuals' privacy, data have to be ``well-sanitized'' ( ``well-anonymized'') before sharing them, i.e. one has to remove any personal information before sharing data. However, it is not always clear when data shall be deemed well-sanitized.  In this paper, we argue that the evaluation of sanitized data should be based on whether the data allows the inference of sensitive information that is specific to an individual, instead of being centered around the concept of re-identification.   We propose a framework to evaluate the effectiveness of different sanitization techniques on a given dataset by measuring  how much an individual's record from the sanitized dataset influences the inference of his/her own sensitive attribute.  Our intent is not to accurately predict any sensitive attribute but rather to measure the impact of a single record on the inference of sensitive information. We demonstrate our approach by sanitizing two real datasets in different privacy models (k-anonymity, l-diversity, and differential privacy) and evaluate/compare each sanitized dataset in our framework.", "num_citations": "1\n", "authors": ["1676"]}