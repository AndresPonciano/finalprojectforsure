{"title": "Object detection with discriminatively trained part-based models\n", "abstract": " We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and\u00a0\u2026", "num_citations": "11329\n", "authors": ["1576"]}
{"title": "A discriminatively trained, multiscale, deformable part model\n", "abstract": " This paper describes a discriminatively trained, multiscale, deformable part model for object detection. Our system achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person detection challenge. It also outperforms the best results in the 2007 challenge in ten out of twenty categories. The system relies heavily on deformable parts. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL challenge. Our system also relies heavily on new methods for discriminative training. We combine a margin-sensitive approach for data mining hard negative examples with a formalism we call latent SVM. A latent SVM, like a hidden CRF, leads to a non-convex training problem. However, a latent SVM is semi-convex and the training problem becomes convex once latent information is specified\u00a0\u2026", "num_citations": "3219\n", "authors": ["1576"]}
{"title": "Cascade object detection with deformable part models\n", "abstract": " We describe a general method for building cascade classifiers from part-based deformable models such as pictorial structures. We focus primarily on the case of star-structured models and show how a simple algorithm based on partial hypothesis pruning can speed up object detection by more than one order of magnitude without sacrificing detection accuracy. In our algorithm, partial hypotheses are pruned with a sequence of thresholds. In analogy to probably approximately correct (PAC) learning, we introduce the notion of probably approximately admissible (PAA) thresholds. Such thresholds provide theoretical guarantees on the performance of the cascade method and can be computed from a small sample of positive examples. Finally, we outline a cascade detection algorithm for a general class of models defined by a grammar formalism. This class includes not only tree-structured pictorial structures but also\u00a0\u2026", "num_citations": "1099\n", "authors": ["1576"]}
{"title": "Systematic nonlinear planning\n", "abstract": " This paper presents a simple, sound,  complete, and systematic algorithm for  domain independent STRIPS planning.  Simplicity is achieved by starting with a  ground procedure and then applying a  general and independently verifiable, lifting  transformation. Previous planners have been  designed directly as lifted procedures. Our  ground procedure is a ground version of  Tate's NONLIN procedure. In Tate's procedure  one is not required to determine whether a  prerequisite of a step in an unfinished plan is  guarnateed to hold in all linearizations. This  allows Tate\"s procedure to avoid the use of  Chapman\"s modal truth criterion.  Systematicity is the property that the same  plan, or partial plan, is never examined more  than once. Systematicity is achieved through a  simple modification of Tate's procedure.", "num_citations": "881\n", "authors": ["1576"]}
{"title": "Some pac-bayesian theorems\n", "abstract": " This paper gives PAC guarantees for \u201cBayesian\u201d algorithms\u2014algorithms that optimize risk minimization expressions involving a prior probability and a likelihood for the training data. PAC-Bayesian algorithms are motivated by a desire to provide an informative prior encoding information about the expected experimental setting but still having PAC performance guarantees over all IID settings. The PAC-Bayesian theorems given here apply to an arbitrary prior measure on an arbitrary concept space. These theorems provide an alternative to the use of VC dimension in proving PAC bounds for parameterized concepts.", "num_citations": "577\n", "authors": ["1576"]}
{"title": "Evidence for invariants in local search\n", "abstract": " It is well known that the performance of a stochastic local search procedure depends upon the setting of its noise parameter, and that the optimal setting varies with the problem distribution. It is therefore desirable to develop general priniciples for tuning the procedures. We present two statistical measures of the local search process that allow one to quickly find the optimal noise settings. These properties are independent of the fine details of the local search strategies, and appear to be relatively independent of the structure of the problem domains. We applied these principles to the problem of evaluating new search heuristics, and discovered two promising new strategies.", "num_citations": "535\n", "authors": ["1576"]}
{"title": "CLP (Intervals) Revisited.\n", "abstract": " The design and implementation of constraint logic programming (CLP) languages over intervals is revisited. Instead of decomposing complex constraints in terms of simple primitive constraints as in CLP (BNR), complex constraints are manipulated as a whole, enabling more sophisticated narrowing procedures to be applied in the solver. This idea is embodied in a new CLP language Newton whose operational semantics is based on the notion of box-consistency, an approximation of arcconsistency, and whose implementation uses Newton interval method. Experimental results indicate that Newton outperforms existing languages by an order of magnitude and is competitive with some state-of-the-art tools on some standard benchmarks. Limitations of our current implementation and directions for further work are also identi ed.", "num_citations": "474\n", "authors": ["1576"]}
{"title": "Encoding plans in propositional logic\n", "abstract": " In recent work we showed that planning problems can be efficiently solved by general propositional satisfiability algorithms (Kautz and Sel-man 1996). A key issue in this approach is the development of practical reductions of planning to SAT. We introduce a series of different SAT encodings for STRIPS-style planning, which are sound and complete representations of the original STRIPS specification, and relate our encodings to the Graphplan system of Blum and Furst (1995). We analyze the size complexity of the various encodings, both in terms of number of variables and total length of the resulting for-mulas. This paper complements the empirical evaluation of several of the encodings reported in Kautz and Selman (1996). We also introduce a novel encoding based on the theory of causal planning, that exploits the notion of \u201clifting\" from the theorem-proving community. This new encoding strictly dominates the others in terms of asymptotic complexity. Finally, we consider fur-ther reductions in the number of variables used by our encodings, by compiling away either state-variables or action-variables. size of problem may be acceptable, while a reduction with an O (n^) may yield SAT instances that are too large to be solved by current algorithms. Of course, not all SAT prob-lems of a given size are equally difficult, and the known results that quantify the hardness of randomly-generated SAT problems based on its size and clause-ratio (Mitchell, Selman, and Levesque 1992) cannot be directly applied to structured problems. However, experiments reported in Kautz and Selman (1996) using the SATPLAN system on problems derived from planning\u00a0\u2026", "num_citations": "443\n", "authors": ["1576"]}
{"title": "PAC-Bayesian model averaging\n", "abstract": " PAC-Bayesian learning methods combine the informative priors of Bayesian methods with distribution-free PAC guarantees. Building on earlier methods for PAC-Bayesian model selection, this paper presents a method for PAC-Bayesian model averaging. The method constructs an optimized weighted mixture of concepts analogous to a Bayesian posterior distribution. Although the main result is stated for bounded loss, a preliminary analysis for unbounded loss is also given.", "num_citations": "396\n", "authors": ["1576"]}
{"title": "Solving polynomial systems using a branch and prune approach\n", "abstract": " This paper presents {\\tt Newton}, a branch and prune algorithm used to find all isolated solutions of a system of polynomial constraints. {\\tt Newton} can be characterized as a global search method which uses intervals for numerical correctness and for pruning the search space early. The pruning in {\\tt Newton} consists of enforcing at each node of the search tree a unique local consistency condition, called box-consistency, which approximates the notion of arc-consistency well known in artificial intelligence. Box-consistency is parametrized by an interval extension of the constraint and can be instantiated to produce the Hansen--Sengupta narrowing operator (used in interval methods) as well as new operators which are more effective when the computation is far from a solution. {\\tt Newton} has been evaluated on a variety of benchmarks from kinematics, chemistry, combustion, economics, and mechanics. On these\u00a0\u2026", "num_citations": "386\n", "authors": ["1576"]}
{"title": "Efficient joint segmentation, occlusion labeling, stereo and flow estimation\n", "abstract": " In this paper we propose a slanted plane model for jointly recovering an image segmentation, a dense depth estimate as well as boundary labels (such as occlusion boundaries) from a static scene given two frames of a stereo pair captured from a moving vehicle. Towards this goal we propose a new optimization algorithm for our SLIC-like objective which preserves connecteness of image segments and exploits shape regularization in the form of boundary length. We demonstrate the performance of our approach in the challenging stereo and flow KITTI benchmarks and show superior results to the state-of-the-art. Importantly, these results can be achieved an order of magnitude faster than competing approaches.", "num_citations": "381\n", "authors": ["1576"]}
{"title": "PAC generalization bounds for co-training\n", "abstract": " The rule-based bootstrapping introduced by Yarowsky, and its cotraining variant by Blum and Mitchell, have met with considerable empirical success. Earlier work on the theory of co-training has been only loosely related to empirically useful co-training algorithms. Here we give a new PAC-style bound on generalization error which justifies both the use of confidences\u2014partial rules and partial labeling of the unlabeled data\u2014and the use of an agreement-based objective function as suggested by Collins and Singer. Our bounds apply to the multiclass case, ie, where instances are to be assigned one of k labels for k> 2.", "num_citations": "381\n", "authors": ["1576"]}
{"title": "An indexed model of recursive types for foundational proof-carrying code\n", "abstract": " The proofs of \"traditional\" proof carrying code (PCC) are type-specialized in the sense that they require axioms about a specific type system. In contrast, the proofs of foundational PCC explicitly define all required types and explicitly prove all the required properties of those types assuming only a fixed foundation of mathematics such as higher-order logic. Foundational PCC is both more flexible and more secure than type-specialized PCC.For foundational PCC we need semantic models of type systems on von Neumann machines. Previous models have been either too weak (lacking general recursive types and first-class function-pointers), too complex (requiring machine-checkable proofs of large bodies of computability theory), or not obviously applicable to von Neumann machines. Our new model is strong, simple, and works either in \u03bb-calculus or on Pentiums.", "num_citations": "370\n", "authors": ["1576"]}
{"title": "Object detection with grammar models\n", "abstract": " Compositional models provide an elegant formalism for representing the visual appearance of highly variable objects. While such models are appealing from a theoretical point of view, it has been difficult to demonstrate that they lead to performance advantages on challenging datasets. Here we develop a grammar model for person detection and show that it outperforms previous high-performance systems on the PASCAL benchmark. Our model represents people using a hierarchy of deformable parts, variable structure and an explicit model of occlusion for partially visible objects. To train the model, we introduce a new discriminative framework for learning structured prediction models from weakly-labeled data.", "num_citations": "310\n", "authors": ["1576"]}
{"title": "An outlook on truth maintenance\n", "abstract": " Truth maintenance systems have been used  in several recent problem solving systems to  record justifications for deduced assertions,  to track down the assumptions which underlie  contradictions when they arise, and to  incrementally modify assertional data  structures when assumptions are retracted. A  TMS algorithm is described here that is  substantially different from previous systems.  This algorithm performs deduction in  traditional propositional logic in such a way  that the premise set from which deduction is  being done can be easily manipulated. A  novel approach is also taken to the role of a  TMS in larger deductive systems. In this  approach the TMS performs all propositional  deduction in a uniform manner while the  larger system is responsible for controlling  the instantiation of universally quantified  formulae and axiom schemas.", "num_citations": "293\n", "authors": ["1576"]}
{"title": "PAC-Bayesian stochastic model selection\n", "abstract": " PAC-Bayesian learning methods combine the informative priors of Bayesian methods with distribution-free PAC guarantees. Stochastic model selection predicts a class label by stochastically sampling a classifier according to a \u201cposterior distribution\u201d on classifiers. This paper gives a PAC-Bayesian performance guarantee for stochastic model selection that is superior to analogous guarantees for deterministic model selection. The guarantee is stated in terms of the training error of the stochastic classifier and the KL-divergence of the posterior from the prior. It is shown that the posterior optimizing the performance guarantee is a Gibbs distribution. Simpler posterior distributions are also derived that have nearly optimal performance guarantees.", "num_citations": "258\n", "authors": ["1576"]}
{"title": "Computational challenges in propositional reasoning and search\n", "abstract": " The past several years have seen much progress in the area of propositional reasoning and satis ability testing. There is a growing consensus by researchers on the key technical challenges that need to be addressed in order to maintain this momentum. This paper outlines concrete technical challenges in the core areas of systematic search, stochastic search, problem encodings, and criteria for evaluating progress in this area.", "num_citations": "251\n", "authors": ["1576"]}
{"title": "Discriminatively trained deformable part models, release 4\n", "abstract": " CiNii \u8ad6\u6587 - Discriminatively trained deformable part models, release 4 CiNii \u56fd\u7acb\u60c5\u5831\u5b66 \u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb \u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 Discriminatively trained deformable part models, release 4 FELZENSZWALB PF \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 FELZENSZWALB PF \u53ce\u9332\u520a\u884c\u7269 http://people.cs.uchicago.edu/pff/latent-release4/ http://people.cs.uchicago.edu/pff/latent-release4/ \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u7269\u4f53\u3068 \u52d5\u304d\u7279\u5fb4\u3092\u7528\u3044\u305f\u884c\u52d5\u8a8d\u8b58 \u52dd\u624b \u7f8e\u7d17 , \u5185\u6d77 \u3086\u3065\u5b50 , \u9ec4\u702c \u6d69\u4e00 \u96fb\u5b50\u60c5\u5831\u901a\u4fe1\u5b66\u4f1a\u6280\u8853\u7814\u7a76\u5831\u544a. PRMU, \u30d1\u30bf\u30fc\u30f3\u8a8d\u8b58\u30fb\u30e1\u30c7\u30a3\u30a2\u7406\u89e3 111(430), 125-126, 2012-02-02 \u53c2\u8003\u6587\u732e7\u4ef6 Tweet \u5404\u7a2e \u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 20000686706 \u8cc7\u6599\u7a2e\u5225 \u305d\u306e\u4ed6 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 CJP\u2026", "num_citations": "227\n", "authors": ["1576"]}
{"title": "Conspiracy numbers for min-max search\n", "abstract": " A new procedure is presented for growing min-max game trees. In large games, such as chess, decisions must be based on incomplete search trees. The new tree-growth procedure is based on \u201cconspiracy numbers\u201d as a measure of the accuracy of the root minimax value of an incomplete tree. Conspiracy numbers measure the number of leaf nodes whose value must change in order to change the minimax root value by a given amount. Trees are grown in a way that maximizes the conspiracy required to change the root value. The trees grown by this procedure are often deep and narrow. However, if all static values in a game are the same, this new procedure reduces to d-ply search with \u03b1-\u03b2 pruning. Unlike B\u2217 search, nonuniform growth is achieved without any modification of the static-board evaluator.", "num_citations": "218\n", "authors": ["1576"]}
{"title": "Truth Maintenance.\n", "abstract": " General purpose truth maintenance systems have received considerable attention in the past few years. This paper discusses the functionality of truth maintenance systems and compares various existing algorithms. Applications and directions for future research are also discussed.", "num_citations": "211\n", "authors": ["1576"]}
{"title": "GSAT and dynamic backtracking\n", "abstract": " There has been substantial recent interest in two new families of search techniques. One family consists of nonsystematic methods such as gsat; the other contains systematic approaches that use a polynomial amount of justification information to prune the search space. This paper introduces a new technique that combines these two approaches. The algorithm allows substantial freedom of movement in the search space but enough information is retained to ensure the systematicity of the resulting analysis. Bounds are given for the size of the justification database and conditions are presented that guarantee that this database will be polynomial in the size of the problem in question.", "num_citations": "208\n", "authors": ["1576"]}
{"title": "Simplified PAC-Bayesian margin bounds\n", "abstract": " The theoretical understanding of support vector machines is largely based on margin bounds for linear classifiers with unit-norm weight vectors and unit-norm feature vectors. Unit-norm margin bounds have been proved previously using fat-shattering arguments and Rademacher complexity. Recently Langford and Shawe-Taylor proved a dimension-independent unit-norm margin bound using a relatively simple PAC-Bayesian argument. Unfortunately, the Langford-Shawe-Taylor bound is stated in a variational form making direct comparison to fat-shattering bounds difficult. This paper provides an explicit solution to the variational problem implicit in the Langford-Shawe-Taylor bound and shows that the PAC-Bayesian margin bounds are significantly tighter. Because a PAC-Bayesian bound is derived from a particular prior distribution over hypotheses, a PAC-Bayesian margin bound also seems to provide\u00a0\u2026", "num_citations": "203\n", "authors": ["1576"]}
{"title": "Automatic recognition of tractability in inference relations\n", "abstract": " A procedure is given for recognizing sets of inference rules that generate polynomial time decidable inference relations. The procedure can automatically recognize the tractability of the inference rules underlying congruence closure. The recognition of tractability for that particular rule set constitutes mechanical verification of a theorem originally proved independently by Kozen and Shostak. The procedure is algorithmic, rather than heuristic, and the class of automatically recognizable tractable rule sets can be precisely characterized. A series of examples of rule sets whose tractability is nontrivial, yet machine recognizable, is also given. The technical framework developed here is viewed as a first step toward a general theory of tractable inference relations.", "num_citations": "186\n", "authors": ["1576"]}
{"title": "A Rearrangement Search Strategy for Determining Propositional Satisfiability.\n", "abstract": " We present a simple algorithm for determining the satisfiability of propositional formulas in Conjunctive Normal Form. As the procedure searches for a satisfying truth assignment it dynamically rearranges the order in which variables are considered. The choice of which variable to assign a truth value next is guided by an upper bound on the size of the search remaining; the procedure makes the choice which yields the smallest upper bound on the size of the remaining search. We describe several upper bound functions and discuss the tradeoff between accurate upper bound functions and the overhead required to compute the upper bounds. Experimental data shows that for one easily computed upper bound the reduction in the size of the search spa, ce more than compensates for the 0verhea. d involved in selecting the next variable.", "num_citations": "178\n", "authors": ["1576"]}
{"title": "The communication complexity of correlation\n", "abstract": " We examine the communication required for generating random variables remotely. One party  Alice  is given a distribution  D , and she has to send a message to  Bob , who is then required to generate a value with distribution exactly  D .  Alice and  Bob  are allowed to share random bits generated without the knowledge of  D . There are two settings based on how the distribution D provided to  Alice  is chosen. If  D  is itself chosen randomly from some set (the set and distribution are known in advance) and we wish to minimize the expected communication in order for  Alice  to generate a value y, with distribution  D , then we characterize the communication required in terms of the mutual information between the input to  Alice  and the output  Bob  is required to generate. If  D  is chosen from a set of distributions  D , and we wish to devise a protocol so that the expected communication (the randomness comes from\u00a0\u2026", "num_citations": "161\n", "authors": ["1576"]}
{"title": "Particle belief propagation\n", "abstract": " The popularity of particle filtering for inference in Markov chain models defined over random variables with very large or continuous domains makes it natural to consider sample\u2013based versions of belief propagation (BP) for more general (tree\u2013structured or loopy) graphs. Already, several such algorithms have been proposed in the literature. However, many questions remain open about the behavior of particle\u2013based BP algorithms, and little theory has been developed to analyze their performance. In this paper, we describe a generic particle belief propagation (PBP) algorithm which is closely related to previously proposed methods. We prove that this algorithm is consistent, approaching the true BP messages as the number of samples grows large. We then use concentration bounds to analyze the finite-sample behavior and give a convergence rate for the algorithm on tree\u2013structured graphs. Our convergence rate is O (1/\\sqrtn) where n is the number of samples, independent of the domain size of the variables.", "num_citations": "156\n", "authors": ["1576"]}
{"title": "Continuous markov random fields for robust stereo estimation\n", "abstract": " In this paper we present a novel slanted-plane model which reasons jointly about occlusion boundaries as well as depth. We formulate the problem as one of inference in a hybrid MRF composed of both continuous (i.e., slanted 3D planes) and discrete (i.e., occlusion boundaries) random variables. This allows us to define potentials encoding the ownership of the pixels that compose the boundary between segments, as well as potentials encoding which junctions are physically possible. Our approach outperforms the state-of-the-art on Middlebury high resolution imagery [1] as well as in the more challenging KITTI dataset [2], while being more efficient than existing slanted plane MRF methods, taking on average 2 minutes to perform inference on high resolution imagery.", "num_citations": "154\n", "authors": ["1576"]}
{"title": "Robust monocular epipolar flow estimation\n", "abstract": " We consider the problem of computing optical flow in monocular video taken from a moving vehicle. In this setting, the vast majority of image flow is due to the vehicle's ego-motion. We propose to take advantage of this fact and estimate flow along the epipolar lines of the egomotion. Towards this goal, we derive a slanted-plane MRF model which explicitly reasons about the ordering of planes and their physical validity at junctions. Furthermore, we present a bottom-up grouping algorithm which produces over-segmentations that respect flow boundaries. We demonstrate the effectiveness of our approach in the challenging KITTI flow benchmark [11] achieving half the error of the best competing general flow algorithm and one third of the error of the best epipolar flow algorithm.", "num_citations": "148\n", "authors": ["1576"]}
{"title": "Direct Loss Minimization for Structured Prediction.\n", "abstract": " In discriminative machine learning one is interested in training a system to optimize a certain desired measure of performance such as the BLEU score in machine translation or the intersection-over-union score in the PASCAL segmentation evaluation. We propose here a perceptron-like learning method based on computing a difference of feature vectors between two inferred output values where at least one of the outputs is inferred by lossadjusted inference. The main contribution of this paper is a theorem directly relating updates of this form to the gradient of the given loss function with respect to the system parameters. This provides a theoretical foundation for certain training methods which have already gained widespread use in machine translation. Empirical results on phonetic alignment are also given here surpassing all previously reported results on this problem.", "num_citations": "147\n", "authors": ["1576"]}
{"title": "Effective Bayesian inference for stochastic programs\n", "abstract": " In this paper, we propose a stochastic version of a general purpose functional programming language as a method of modeling stochastic processes. The language contains random choices, conditional statements, structured values, defined functions, and recursion. By imagining an experiment in which the program is \u201crun\u201d and the random choices made by sampling, we can interpret a program in this language as encoding a probability distribution over a (potentially infinite) set of objects. We provide an exact algorithm for computing conditional probabilities of the form Pr (P (z) 1 Q (z)) where x is chosen randomly from this distribution. This algorithm terminates precisely when sampling x and computing P (X) and Q (x) t erminates in all possible stochastic executions (under lazy evaluation semantics, in which only values needed to compute the output of the program are evaluated). We demonstrate the applicability of the language and the efficiency of the inference algorithm by encoding both Bayesian networks and stochastic context-free grammars in our language, and showing that our algorithm derives efficient inference algorithms for both. Our language easily supports interesting and useful extensions to these formalisms (eg, recursive Bayesian networks), to which our inference algorithm will automatically apply.", "num_citations": "147\n", "authors": ["1576"]}
{"title": "Hardening soft information sources\n", "abstract": " The web contains a large quantity of unstructured information. In many cases, it is possible to heuristically extract structured information, but the resulting databases are\\soft\": they contain inconsistencies and duplication, and lack unique, consistently-used object identifiers. Examples include large bibliographic databases harvested from raw scientific papers or databases constructed by merging heterogeneous\\hard\" databases. Here we formally model a soft database as a noisy version of some unknown hard database. We then consider the hardening problem, ie, the problem of inferring the most likely underlying hard database given a particular soft database. A key feature of our approach is that hardening is global| many sources of evidence for a given hard fact are taken into account. We formulate hardening as an optimization problem and give a nontrivial nearly linear time algorithm for finding a local optimum.", "num_citations": "146\n", "authors": ["1576"]}
{"title": "On the Convergence Rate of Good-Turing Estimators.\n", "abstract": " Good-Turing adjustments of word frequencies are an important tool in natural language modeling. In particular, for any sample of words, there is a set of words not occuring in that sample. The total probability mass of the words not in the sample is the so-called missing mass. Good showed that the fraction of the sample consisting of words that occur only once in the sample is a nearly unbiased estimate of the missing mass. Here, we give a high-probability confidence interval for the actual missing mass. More generally, for k 0, we give a confidence interval for the true probability mass of the set of words occuring k times in the sample.", "num_citations": "146\n", "authors": ["1576"]}
{"title": "Maximum margin semi-supervised learning for structured variables\n", "abstract": " Many real-world classification problems involve the prediction of multiple inter-dependent variables forming some structural dependency. Recent progress in machine learning has mainly focused on supervised classification of such structured variables. In this paper, we investigate structured classification in a semi-supervised setting. We present a discriminative approach that utilizes the intrinsic geometry of input patterns revealed by unlabeled data points and we derive a maximum-margin formulation of semi-supervised learning for structured variables. Unlike transductive algorithms, our formulation naturally extends to new test points.", "num_citations": "136\n", "authors": ["1576"]}
{"title": "Who did what: A large-scale person-centered cloze dataset\n", "abstract": " We have constructed a new \"Who-did-What\" dataset of over 200,000 fill-in-the-gap (cloze) multiple choice reading comprehension problems constructed from the LDC English Gigaword newswire corpus. The WDW dataset has a variety of novel features. First, in contrast with the CNN and Daily Mail datasets (Hermann et al., 2015) we avoid using article summaries for question formation. Instead, each problem is formed from two independent articles --- an article given as the passage to be read and a separate article on the same events used to form the question. Second, we avoid anonymization --- each choice is a person named entity. Third, the problems have been filtered to remove a fraction that are easily solved by simple baselines, while remaining 84% solvable by humans. We report performance benchmarks of standard systems and propose the WDW dataset as a challenge task for the community.", "num_citations": "125\n", "authors": ["1576"]}
{"title": "Approximate planning for factored POMDPs using belief state simplification\n", "abstract": " We are interested in the problem of planning for factored POMDPs. Building on the recent results of Kearns, Mansour and Ng, we provide a planning algorithm for factored POMDPs that exploits the accuracy-efficiency tradeoff in the belief state simplification introduced by Boyen and Koller.", "num_citations": "114\n", "authors": ["1576"]}
{"title": "A min-cover approach for finding salient curves\n", "abstract": " We consider the problem of deriving a global interpretation of an image in terms of a small set of smooth curves. The problem is posed using a statistical model for images with multiple curves. Besides having important applications to edge detection and grouping the curve finding task is a special case of a more general problem, where we want to explain the whole image in terms of a small set of objects. We describe a novel approach for estimating the content of scenes with multiple objects using a min-cover framework that is simple and powerful. The min-cover problem is NP-hard but there is a good approximation algorithm that sequentially selects objects minimizing a \"cost per pixel\" measure. In the case of curve detection we use a type of best-first search to quickly find good curves for the covering algorithm. The method integrates image data over long curves without relying on binary feature detection. We have\u00a0\u2026", "num_citations": "110\n", "authors": ["1576"]}
{"title": "Formal limitations on the measurement of mutual information\n", "abstract": " Measuring mutual information from finite data is difficult. Recent work has considered variational methods maximizing a lower bound. In this paper, we prove that serious statistical limitations are inherent to any method of measuring mutual information. More specifically, we show that any distribution-free high-confidence lower bound on mutual information estimated from N samples cannot be larger than O (ln N).", "num_citations": "106\n", "authors": ["1576"]}
{"title": "Affine algebraic decision diagrams (AADDs) and their application to structured probabilistic inference\n", "abstract": " We propose an affine extension to ADDs (AADD) capable of compactly representing context-specific, additive, and multiplicative structure. We show that the AADD has worstcase time and space performance within a multiplicative constant of that of ADDs, but that it can be linear in the number of variables in cases where ADDs are exponential in the number of variables. We provide an empirical comparison of tabular, ADD, and AADD representations used in standard Bayes net and MDP inference algorithms and conclude that the AADD performs at least as well as the other two representations, and often yields an exponential performance improvement over both when additive or multiplicative structure can be exploited. These results suggest that the AADD is likely to yield exponential time and space improvements for a variety of probabilistic inference algorithms that currently use tables or ADDs.", "num_citations": "105\n", "authors": ["1576"]}
{"title": "A PAC-Bayesian tutorial with a dropout bound\n", "abstract": " This tutorial gives a concise overview of existing PAC-Bayesian theory focusing on three generalization bounds. The first is an Occam bound which handles rules with finite precision parameters and which states that generalization loss is near training loss when the number of bits needed to write the rule is small compared to the sample size. The second is a PAC-Bayesian bound providing a generalization guarantee for posterior distributions rather than for individual rules. The PAC-Bayesian bound naturally handles infinite precision rule parameters,  regularization, {\\em provides a bound for dropout training}, and defines a natural notion of a single distinguished PAC-Bayesian posterior distribution. The third bound is a training-variance bound --- a kind of bias-variance analysis but with bias replaced by expected training loss. The training-variance bound dominates the other bounds but is more difficult to interpret. It seems to suggest variance reduction methods such as bagging and may ultimately provide a more meaningful analysis of dropouts.", "num_citations": "104\n", "authors": ["1576"]}
{"title": "Ontic: a knowledge representation system for mathematics\n", "abstract": " Ontic is an interactive system for developing and verifying mathematics. Ontics verification mechanism is capable of automatically finding and applying information from a library containing hundreds of mathematical facts. Starting with only the axioms of Zermelo-Fraenkel set theory, the Ontic system has been used to build a data base of definitions and lemmas leading to a proof of the Stone representation theorem for Boolean lattices. The Ontic system has been used to explore issues in knowledge representation, automated deduction, and the automatic use of large data bases. Keywords Cognitive models Inference Compilers Programming languages Stone representation theorem Automated reasoning.Descriptors:", "num_citations": "102\n", "authors": ["1576"]}
{"title": "The generalized A* architecture\n", "abstract": " We consider the problem of computing a lightest derivation of a global structure using a set of weighted rules. A large variety of inference problems in AI can be formulated in this framework. We generalize A* search and heuristics derived from abstractions to a broad class of lightest derivation problems. We also describe a new algorithm that searches for lightest derivations using a hierarchy of abstractions. Our generalization of A* gives a new algorithm for searching AND/OR graphs in a bottom-up fashion.", "num_citations": "99\n", "authors": ["1576"]}
{"title": "Machine comprehension with syntax, frames, and semantics\n", "abstract": " We demonstrate significant improvement on the MCTest question answering task (Richardson et al., 2013) by augmenting baseline features with features based on syntax, frame semantics, coreference, and word embeddings, and combining them in a max-margin learning framework. We achieve the best results we are aware of on this dataset, outperforming concurrentlypublished results. These results demonstrate a significant performance gradient for the use of linguistic structure in machine comprehension.", "num_citations": "96\n", "authors": ["1576"]}
{"title": "Exponentiated gradient algorithms for large-margin structured classification\n", "abstract": " We consider the problem of structured classification, where the task is to predict a label y from an input x, and y has meaningful internal struc-ture. Our framework includes supervised training of Markov random fields and weighted context-free grammars as special cases. We describe an algorithm that solves the large-margin optimization problem defined in [12], using an exponential-family (Gibbs distribution) representation of structured objects. The algorithm is efficient\u2014even in cases where the number of labels y is exponential in size-provided that certain expecta-tions under Gibbs distributions can be calculated efficiently. The method for structured labels relies on a more general result, specifically the application of exponentiated gradient updates [7, 8] to quadratic programs.", "num_citations": "96\n", "authors": ["1576"]}
{"title": "Linear-time subtransitive control flow analysis\n", "abstract": " We present a linear-time algorithm for bounded-type programs that builds a directed graph whose transitive closure gives exactly the results of the standard (cubic-time) Control-Flow Analysis (CFA) algorithm. Our algorithm can be used to list all functions calls from all call sites in (optimal) quadratic time. More importantly, it can be used to give linear-time algorithms for CFA-consuming applications such as: \u2022 effects analysis: find the side-effecting expressions in a program. \u2022 k-limited CFA: for each call-site, list the functions if there are only a few of them (\u2264 k) and otherwise output\" many\". \u2022 called-once analysis: identify all functions called from only one call-site.", "num_citations": "94\n", "authors": ["1576"]}
{"title": "Nondeterministic lisp as a substrate for constraint logic programming\n", "abstract": " We have implemented a comprehensive constraintbased programming language as an extension to Common Lisp. This constraint package provides a uni ed framework for solving both numeric and non-numeric systems of constraints using a combination of local propagation techniques including binding propagation, Boolean constraint propagation, generalized forward checking, propagation of bounds, and uni cation. The backtracking facility of the nondeterministic dialect of Common Lisp used to implement this constraint package acts as a general fallback constraint solving method mitigating the incompleteness of local propagation.", "num_citations": "94\n", "authors": ["1576"]}
{"title": "Natural language syntax and first-order inference\n", "abstract": " We have argued elsewhere that first-order inference can be made more efficient by using nonstandard syntax for first-order logic. In this paper we define a syntax for first-order logic based on the structure of natural language under Montague semantics. We show that, for a certain fairly expressive fragment of this language, satisfiability is polynomial time decidable. The polynomial time decision procedure can be used as a subroutine in general purpose inference systems and seems to be more powerful than analogous procedures based on either classical or taxonomic syntax.", "num_citations": "93\n", "authors": ["1576"]}
{"title": "A three valued truth maintenance system\n", "abstract": " Truth maintenance systems have been used  in recently developed problem solving  systems. A truth maintenance system (TMS)  is designed to be used by deductive systems  to maintain the logical relations among the  beliefs which those systems manipulate.  These relations are used to incrementally  modify the belief structure when premises are  changed, giving a more flexible context  mechanism than has been present in earlier  artificial intelligence systems. The relations  among beliefs can also be used to directly  trace the source of contradictions or failures,  resulting in far more efficient backtracking.", "num_citations": "91\n", "authors": ["1576"]}
{"title": "On the complexity analysis of static analyses\n", "abstract": " This paper argues that for many algorithms, and static analysis algorithms in particular, bottom-up logic program presentations are clearer and simpler to analyze, for both correctness and complexity, than classical pseudo-code presentations. The main technical contribution consists of two theorems which allow, in many cases, the asymptotic running time of a bottom-up logic program to be determined by inspection. It is well known that a datalog program runs in O(nk) time where k is the largest number of free variables in any single rule. The theorems given here are significantly more refined. A variety of algorithms are presented and analyzed as examples.", "num_citations": "90\n", "authors": ["1576"]}
{"title": "Case-factor diagrams for structured probabilistic modeling\n", "abstract": " We introduce a probabilistic formalism handling both Markov random fields of bounded tree width and probabilistic context-free grammars. Our models are based on case-factor diagrams (CFDs) which are similar to binary decision diagrams (BDDs) but are more concise for circuits of bounded tree width. A probabilistic model consists of a CFD defining a feasible set of Boolean assignments and a weight (or cost) for each individual Boolean variable. We give versions of the inside\u2013outside algorithm and the Viterbi algorithm for these models.", "num_citations": "89\n", "authors": ["1576"]}
{"title": "Active modular elastomer sleeve for soft wearable assistance robots\n", "abstract": " A proposed adaptive soft orthotic device performs motion sensing and production of assistive forces with a modular, pneumatically-driven, hyper-elastic composite. Wrapping the material around a joint will allow simultaneous motion sensing and active force response through shape and rigidity control. This monolithic elastomer sheet contains a series of miniaturized pneumatically-powered McKibben-type actuators that exert tension and enable adaptive rigidity control. The elastomer is embedded with conductive liquid channels that detect strain and bending deformations induced by the pneumatic actuators. In addition, the proposed system is modular and can be configured for a diverse range of motor tasks, joints, and human subjects. This modular functionality is accomplished with a decentralized network of self-configuring nodes that manage the collection of sensory data and the delivery of actuator feedback\u00a0\u2026", "num_citations": "87\n", "authors": ["1576"]}
{"title": "An architecture for action selection in robotic soccer\n", "abstract": " CMUnited-99 was the 1999 RoboCup robotic soccer simulator league champion. In the RoboCup-2000 competition, CMUnited-99 was entered again and despite being publicly available for the entire year, it still finished in 4th place. This paper presents some of the key elements behind\\attcmunited, one of the three teams that finished ahead of CMUnited-99 in RoboCup-2000 out of thirty four entrants. Playing against CMUnited-99,\\attcmunited\\scores an average of about 8 goals per opponent goal. This paper describes some of the key innovations that make this improvement possible.", "num_citations": "85\n", "authors": ["1576"]}
{"title": "Relating probabilistic grammars and automata\n", "abstract": " Both probabilistic context-free grammars (PCFGs) and shift-reduce probabilistic pushdown automata (PPDAs) have been used for language modeling and maximum likelihood parsing. We investigate the precise relationship between these two formalisms, showing that, while they define the same classes of probabilistic languages, they appear to impose different inductive biases.", "num_citations": "82\n", "authors": ["1576"]}
{"title": "Taxonomic syntax for first order inference\n", "abstract": " A new polynomial time decidable fragment of first order logic is identified, and a general method for using polynomial time inference procedures in knowledge representation systems is presented. The results shown in this paper indicate that a nonstandard \u201ctaxonomic\u201d syntax is essential in constructing natural and powerful polynomial time inference procedures. The central role of taxonomic syntax in the polynomial time inference procedures provides technical support for the often-expressed intuition that knowledge is better represented in terms of taxonomic relationships than classical first order formulas. To use the procedures in a knowledge representation system, a \u201cSocratic proof system\u201d is defined, which is complete for first order inference and which can be used as a semi-automated interface to a first order knowledge base.", "num_citations": "82\n", "authors": ["1576"]}
{"title": "On the complexity analysis of static analyses\n", "abstract": " This paper investigates bottom-up logic programming as a formalism for expressing static analyses. The main technical contribution consists of two meta-complexity theorems which allow, in many cases, the asymptotic running time of a bottom-up logic program to be determined by inspection. It is well known that a datalog program runs in O(n                         k) time where k is the largest number of free variables in any single rule. The theorems given here are significantly more refined. A variety of algorithms given as bottom-up logic programs are analyzed as examples.", "num_citations": "80\n", "authors": ["1576"]}
{"title": "Sound and complete models of contracts\n", "abstract": " Even in statically typed languages it is useful to have certain invariants checked dynamically. Findler and Felleisen gave an algorithm for dynamically checking expressive higher-order types called contracts. They did not, however, give a semantics of contracts. The lack of a semantics makes it impossible to define and prove soundness and completeness of the checking algorithm. (Given a semantics, a sound checker never reports violations that do not exist under that semantics; a complete checker is \u2013 in principle \u2013 able to find violations when violations exist.) Ideally, a semantics should capture what programmers intuitively feel is the meaning of a contract or otherwise clearly point out where intuition does not match reality. In this paper we give an interpretation of contracts for which we prove the Findler-Felleisen algorithm sound and (under reasonable assumptions) complete. While our semantics mostly matches\u00a0\u2026", "num_citations": "78\n", "authors": ["1576"]}
{"title": "Decision-theoretic bidding based on learned density models in simultaneous, interacting auctions\n", "abstract": " Auctions are becoming an increasingly popular method for transacting business, especially over the Internet. This article presents a general approach to building autonomous bidding agents to bid in multiple simultaneous auctions for interacting goods. A core component of our approach learns a model of the empirical price dynamics based on past data and uses the model to analytically calculate, to the greatest extent possible, optimal bids. We introduce a new and general boosting-based algorithm for conditional density estimation problems of this kind, ie, supervised learning problems in which the goal is to estimate the entire conditional distribution of the real-valued label. This approach is fully implemented as ATTac-2001, a top-scoring agent in the second Trading Agent Competition (TAC-01). We present experiments demonstrating the effectiveness of our boosting-based price predictor relative to several reasonable alternatives.", "num_citations": "77\n", "authors": ["1576"]}
{"title": "On the cubic bottleneck in subtyping and flow analysis\n", "abstract": " We prove that certain data-flow and control-flow problems are 2NPDA-complete. This means that these problems are in the class 2NPDA and that they are hard for that class. The fact that they are in 2NPDA demonstrates the richness of the class. The fact that they are hard for 2NPDA can be interpreted as evidence they can not be solved in sub-cubic time-the cubic time decision procedure for an arbitrary 2NPDA problem has not been improved since its discovery in 1968.", "num_citations": "77\n", "authors": ["1576"]}
{"title": "Concentration inequalities for the missing mass and for histogram rule error\n", "abstract": " This paper gives distribution-free concentration inequalities for the missing mass and the error rate of histogram rules. Negative association methods can be used to reduce these concentration problems to concentration questions about independent sums. Although the sums are independent, they are highly heterogeneous. Such highly heterogeneous independent sums cannot be analyzed using standard concentration inequalities such as Hoeffding\u2019s inequality, the Angluin-Valiant bound, Bernstein\u2019s inequality, Bennett\u2019s inequality, or McDiarmid\u2019s theorem. The concentration inequality for histogram rule error is motivated by the desire to construct a new class of bounds on the generalization error of decision trees.", "num_citations": "70\n", "authors": ["1576"]}
{"title": "Logical algorithms\n", "abstract": " It is widely accepted that many algorithms can be concisely and clearly expressed as logical inference rules. However, logic programming has been inappropriate for the study of the running time of algorithms because there has not been a clear and precise model of the run time of a logic program. We present a logic programming model of computation appropriate for the study of the run time of a wide variety of algorithms.", "num_citations": "70\n", "authors": ["1576"]}
{"title": "Visual object detection with deformable part models\n", "abstract": " We describe a state-of-the-art system for finding objects in cluttered images. Our system is based on deformable models that represent objects using local part templates and geometric constraints on the locations of parts. We reduce object detection to classification with latent variables. The latent variables introduce invariances that make it possible to detect objects with highly variable appearance. We use a generalization of support vector machines to incorporate latent information during training. This has led to a general framework for discriminative training of classifiers with latent variables. Discriminative training benefits from large training datasets. In practice we use an iterative algorithm that alternates between estimating latent values for positive examples and solving a large convex optimization problem. Practical optimization of this large convex problem can be done using active set techniques for adaptive\u00a0\u2026", "num_citations": "68\n", "authors": ["1576"]}
{"title": "Generalization bounds and consistency for latent structural probit and ramp loss\n", "abstract": " We consider latent structural versions of probit loss and ramp loss. We show that these surrogate loss functions are consistent in the strong sense that for any feature map (finite or infinite dimensional) they yield predictors approaching the infimum task loss achievable by any linear predictor over the given features. We also give finite sample generalization bounds (convergence rates) for these loss functions. These bounds suggest that probit loss converges more rapidly. However, ramp loss is more easily optimized on a given sample.", "num_citations": "68\n", "authors": ["1576"]}
{"title": "Natural language based inference procedures applied to Schubert's steamroller\n", "abstract": " We have previously argued that the syntactic structure of natural language can be exploited to construct powerful polynomial time inference procedures. This paper supports the earlier arguments by demonstrating that a natural language based polynomial time procedure can solve Schuberts steamroller in a single step.Descriptors:", "num_citations": "68\n", "authors": ["1576"]}
{"title": "Generalization Bounds for Decision Trees.\n", "abstract": " We derive a new bound on the error rate for decision trees. The bounds depends both on the structure of the tree and the speci c sample (not just the size of the sample). This bound is tighter than traditional bounds for unbalanced trees and justi es\\compositional\" algorithms for constructing decision trees.", "num_citations": "66\n", "authors": ["1576"]}
{"title": "New Results on Local Inference Relations.\n", "abstract": " We consider the concept of a local set of infer-ence rules. A local rule set can be automatically transformed into a rule set for which bottom up evaluation terminates in polyno mial time. The local rule set transformation gives polynomial time evaluation strategies for a large variety of rule sets that can not be given terminating evaluation strategies by any other known automatic technique. This paper discusses three new results. First, it is shown that every polynomial time predi-cate can be defined by an (unstratified) local rule set. Second, a new machine recognizable subclass of the local rule sets is identified. Fi-nally we show that locality, as a property of rule sets, is undecidable in general.", "num_citations": "66\n", "authors": ["1576"]}
{"title": "Object detection grammars.\n", "abstract": " We formulate a general grammar model motivated by the problem of object detection in computer vision. We focus on four aspects of modeling objects for the purpose of object detection. First, we are interested in modeling objects as having parts which are themselves (recursively) objects. For example a person can be represented as being composed of a face, a trunk, arms, and legs where a face is composed of eyes, a nose and a mouth. Second, we are interested modeling object (and part) categories as being composed of subcategories or subtypes. For example we might distinguish sitting people from standing people and smiling faces from frowning faces.Third, we are interested in modeling the relative positions of the parts that make up an object. For example, in a person, the position of the hand is related to the position of the lower arm which is related to the position of the upper arm which is related to the position of the torso. Fourth, we are interested in modeling the appearance of objects so that we can find them in images. For example, a pattern of edges in a particular location of an image might give evidence for, or against, the presence of a part at that location. These four aspects of models\u2014parts, subtypes, positions, and appearance\u2014can be represented in a single grammar formalism that we call an object detection grammar.", "num_citations": "63\n", "authors": ["1576"]}
{"title": "Modeling auction price uncertainty using boosting-based conditional density estimation\n", "abstract": " In complicated, interacting auctions, a fundamental problem is the prediction of prices of goods in the auctions, and more broadly, the modeling of uncertainty regarding these prices. In this paper, we present a machine-learning approach to this problem. The technique is based on a new and general boosting-based algorithm for conditional density estimation problems of this kind, ie, supervised learning problems in which the goal is to estimate the entire conditional distribution of the real-valued label. This algorithm, which we present in detail, is at the heart of ATTac-2001, a top-scoring agent in the recent Trading", "num_citations": "62\n", "authors": ["1576"]}
{"title": "A new meta-complexity theorem for bottom-up logic programs\n", "abstract": " Nontrivial meta-complexity theorems, proved once for a programming language as a whole, facilitate the presentation and analysis of particular algorithms. This paper gives a new meta-complexity theorem for bottom-up logic programs that is both more general and more accurate than previous such theorems. The new theorem applies to algorithms not handled by previous meta-complexity theorems, greatly facilitating their analysis.", "num_citations": "62\n", "authors": ["1576"]}
{"title": "Exploiting variable dependency in local search\n", "abstract": " Stochastic search has recently been shown to be successful for solving large boolean satis ability problems. However, systematic methods tend to be more e ective in problem domains with a large number of dependent variables: that is, variables whose truth values are directly determined by a smaller set of independent variables. In systematic search, truth values can be eciently propagated from the independent to the dependent variables by unit propagation. Such propagation is more expensive in traditional stochastic procedures. In this paper we propose a mechanism for e ectively dealing with dependent variables in stochastic search. We also provide empirical data showing the procedure outperforms the best previous stochastic and systematic search procedures on large formulas with a high ratio of dependent to independent variables.", "num_citations": "59\n", "authors": ["1576"]}
{"title": "Pac-bayesian approach for minimization of phoneme error rate\n", "abstract": " We describe a new approach for phoneme recognition which aims at minimizing the phoneme error rate. Building on structured prediction techniques, we formulate the phoneme recognizer as a linear combination of feature functions. We state a PAC-Bayesian generalization bound, which gives an upper-bound on the expected phoneme error rate in terms of the empirical phoneme error rate. Our algorithm is derived by finding the gradient of the PAC-Bayesian bound and minimizing it by stochastic gradient descent. The resulting algorithm is iterative and easy to implement. Experiments on the TIMIT corpus show that our method achieves the lowest phoneme error rate compared to other discriminative and generative models with the same expressive power.", "num_citations": "57\n", "authors": ["1576"]}
{"title": "Reasoning Utility Package User's Manual. Version One.\n", "abstract": " RUP Reasoning Utility Package is a collection of procedures for performing various computations relevant to automated reasoning. RUP contains a truth maintenance system TMS which can be used to perform simple propositional deduction unit clause resolution, to record justifications, to track down underlying assumptions, and to perform incremental modifications when premises are changed. This TMS can be used with an automatic premise controller which automatically retracts assumptions before solid facts when contradictions arise and searches for the most solid proof of an assertion. RUP also contains a procedure for efficiently computing all the relevant consequences of any set of equalities between ground terms. A related utility computes substitution simplifications of terms under an arbitrary set of unquantified equalities and a user defined simplicity order. RUP also contains demon writing macros which allow one to write PLANNER like demons that trigger on various types of events in the data base. Finally there is a utility for reasoning about partial orders and arbitrary transitive relations. In writing all of these utilities an attempt has been made to provide a maximally flexible environment for automated reasoning. AuthorDescriptors:", "num_citations": "57\n", "authors": ["1576"]}
{"title": "Boosting using branching programs\n", "abstract": " It is known that decision tree learning can be viewed as a form of boosting. Given a weak learning hypothesis one can show that the training error of a decision tree declines as| T|\u2212 \u03b2 where| T| is the size of the decision tree and \u03b2 is a constant determined by the weak learning hypothesis. Here we consider the case of decision DAGs\u2014decision trees in which a given node can be shared by different branches of the tree, also called branching programs (BP). Node sharing allows a branching program to be exponentially more compact than the corresponding decision tree. We show that under the same weak learning assumption used for decision tree learning there exists a greedy BP-growth algorithm whose training error is guaranteed to decline as 2\u2212 \u03b2| T|, where| T| is the size of the branching program and \u03b2 is a constant determined by the weak learning hypothesis. Therefore, from the perspective of boosting theory\u00a0\u2026", "num_citations": "49\n", "authors": ["1576"]}
{"title": "Screamer: A portable efficient implementation of nondeterministic Common Lisp\n", "abstract": " Nondeterministic LISP is a simple extension of LISP which provides automatic backtracking. Nondeterminism allows concise description of many search tasks which form the basis of much AI research. This paper discusses SCREAMER, an efficient implementation of nondeterministic LISP as a fully portable extension of COMMON LISP. In this paper we present the basic nondeterministic LISP constructs, motivate the utility of the language via numerous short examples, and discuss the compilation techniques.", "num_citations": "49\n", "authors": ["1576"]}
{"title": "Computable Shell Decomposition Bounds.\n", "abstract": " Haussler, Kearns, Seung and Tishby introduced the notion of a shell decomposition of the union bound as a means of understanding certain empirical phenomena in learning curves such as phase transitions. Here we use a variant of their ideas to derive an upper bound on the generalization error of a hypothesis computable from its training error and the histogram of training errors for the hypotheses in the class. In most cases this new bound is significantly tighter than traditional bounds computed from the training error and the cardinality of the class. Our results can also be viewed as providing a rigorous foundation for a model selection algorithm proposed by Scheffer and Joachims.", "num_citations": "47\n", "authors": ["1576"]}
{"title": "ATTac-2001: A learning, autonomous bidding agent\n", "abstract": " Auctions are becoming an increasingly popular method for transacting business, especially over the Internet. This paper presents a general approach to building autonomous bidding agents to bid in multiple simultaneous auctions for interacting goods. The core of our approach is learning a model of the empirical price dynamics based on past data and using the model to analytically calculate, to the greatest extent possible, optimal bids. This approach is fully implemented as ATTac-2001, a top-scoring agent in the second Trading Agent Competition (TAC-01). ATTac-2001 uses boosting techniques to learn conditional distributions of auction clearing prices. We present experiments demonstrating the effectiveness of this predictor relative to several reasonable alternatives.", "num_citations": "45\n", "authors": ["1576"]}
{"title": "Walther recursion\n", "abstract": " Primitive recursion is a well known syntactic restriction on recursive definitions which guarantees termination. Unfortunately many natural definitions, such as the most common definition of Euclid's GCD algorithm, are not primitive recursive. Walther has recently given a proof system for verifying termination of a broader class of definitions. Although Walther's system is highly automatible, the class of acceptable definitions remains only semi-decidable. Here we simplify Walther's calculus and give a syntactic criterion on definitions which guarantees termination. This syntactic criteria generalizes primitive recursion and handles most of the examples given by Walther. We call the corresponding class of acceptable definitions \u201cWalther recursive\u201d.", "num_citations": "45\n", "authors": ["1576"]}
{"title": "Likelihood, probability, and knowledge 1\n", "abstract": " The modal logic LL was introduced by Halpern and Rabin as a means of doing qualitative reasoning about likelihood. Here the relationship between LL and probability theory is examined. It is shown that there is a way of translating probability assertions into LL in a sound manner, so that LL in some sense can capture the probabilistic interpretation of likelihood. However, the translation is subtle; several more obvious attempts are shown to lead to inconsistencies. We also extend LL by adding modal operators for knowledge. This allows us to reason about the interaction between knowledge and likelihood. The propositional version of the resulting logic LLK is shown to have a complete axiomatization and to be decidable in exponential time, provably the best possible.", "num_citations": "39\n", "authors": ["1576"]}
{"title": "Unsupervised learning of stereo vision with monocular cues\n", "abstract": " We demonstrate unsupervised learning of a 62 parameter slanted plane stereo vision model involving shape from texture cues. Our approach to unsupervised learning is based on maximizing conditional likelihood. The shift from joint likelihood to conditional likelihood in unsupervised learning is analogous to the shift from Markov random fields (MRFs) to conditional random fields (CRFs). The performance achieved with unsupervised learning is close to that achieved with supervised learning for this model.", "num_citations": "38\n", "authors": ["1576"]}
{"title": "Polynomial-time computation via local inference relations\n", "abstract": " We consider the concept of a local set of inference rules. A local rule set can be automatically transformed into a rule set for which bottom-up evaluation terminates in polynomial time. The local-rule-set transformation gives polynomial-time evaluation strategies for a large variety of rule sets that cannot be given terminating evaluation strategies by any other known automatic technique. This article discusses three new results. First, it is shown that every polynomial-time predicate can be defined by an (unstratified) local rule set. Second, a new machine-recognizable subclass of the local rule sets is identified. Finally, we show that locality, as a property of rule sets, is undecidable in general.", "num_citations": "38\n", "authors": ["1576"]}
{"title": "Convex max-product algorithms for continuous MRFs with applications to protein folding\n", "abstract": " This paper investigates convex belief propagation algorithms for Markov random fields (MRFs) with continuous variables. Our first contribution is a theorem generalizing properties of the discrete case to the continuous case. Our second contribution is an algorithm for computing the value of the Lagrangian relaxation of the MRF in the continuous case based on associating the continuous variables with an ever-finer interval grid. A third contribution is a particle method which uses convex max-product in re-sampling particles. This last algorithm is shown to be particularly effective for protein folding where it outperforms particle methods based on standard max-product resampling.", "num_citations": "37\n", "authors": ["1576"]}
{"title": "Evidence sentence extraction for machine reading comprehension\n", "abstract": " Remarkable success has been achieved in the last few years on some limited machine reading comprehension (MRC) tasks. However, it is still difficult to interpret the predictions of existing MRC models. In this paper, we focus on extracting evidence sentences that can explain or support the answers of multiple-choice MRC tasks, where the majority of answer options cannot be directly extracted from reference documents. Due to the lack of ground truth evidence sentence labels in most cases, we apply distant supervision to generate imperfect labels and then use them to train an evidence sentence extractor. To denoise the noisy labels, we apply a recently proposed deep probabilistic logic learning framework to incorporate both sentence-level and cross-sentence linguistic indicators for indirect supervision. We feed the extracted evidence sentences into existing MRC models and evaluate the end-to-end performance on three challenging multiple-choice MRC datasets: MultiRC, RACE, and DREAM, achieving comparable or better performance than the same models that take as input the full reference document. To the best of our knowledge, this is the first work extracting evidence sentences for multiple-choice MRC.", "num_citations": "36\n", "authors": ["1576"]}
{"title": "Boolean classes\n", "abstract": " We extend the notion of class so that any Boolean combinations of classes is also a class. Boolean classes allow greater precision and conciseness in naming the class of objects governed a particular method. A class can be viewed as a predicate which is either true or false of any given object. Unlike predicates however classes have an inheritance hierarchy which is known at compile time. Boolean classes extend the notion of class, making classes more like predicates, while preserving the compile time computable inheritance hierarchy.", "num_citations": "34\n", "authors": ["1576"]}
{"title": "Observations on cognitive judgments\n", "abstract": " It is obvious to anyone familiar with the rules of the game of chess that a king on an empty board can reach every square. It is true, but not obvious, that a knight can reach every square. Why is the first fact obvious but the second fact not This paper presents an analytic theory of a class of obviousness judgments of this type. Whether or not the specifics of this analysis are correct, it seems that the study of obviousness judgments can be used to construct integrated theories of linguistics, knowledge representation, and inference.Descriptors:", "num_citations": "31\n", "authors": ["1576"]}
{"title": "Tarskian set constraints\n", "abstract": " We investigate set constraints over set expressions with Tarskian functional and relational operations. Unlike the Herbrand constructor symbols used in recent set constraint formalisms, the meaning of a Tarskian function symbol is interpreted in an arbitrary first order structure. We show that satisfiability of Tarskian set constraints is decidable in nondeterministic doubly exponential time. We also give complexity results and open problems for various extensions of the language.", "num_citations": "28\n", "authors": ["1576"]}
{"title": "Non-Deterministic Lisp with Dependency-directed Backtracking.\n", "abstract": " Extending functional Lisp with McCarthy\u2019s nondeterministic operator AHFJ yields a language which can concisely express search problems. Dependencydirected backtracking is a powerful search strategy.", "num_citations": "28\n", "authors": ["1576"]}
{"title": "Grammar rewriting\n", "abstract": " We present a term rewriting procedure based on congruence closure that can be used with arbitrary equational theories. This procedure is motivated by the pragmatic need to handle equational theories where confluence can not be achieved. The procedure uses context free grammars to represent equivalence classes of terms. The procedure rewrites grammars rather than terms and uses congruence closure to maintain certain congruence properties of the grammar. Grammars provide concise representations of large term sets. Infinite term sets can be represented with finite grammars and exponentially large term sets can be represented with linear sized grammars. Although the procedure is primarily intended for use in nonconfluent theories, it also provides a new kind of confluence that can be used to give canonical rewriting systems for theories that are difficult to handle in other ways.", "num_citations": "27\n", "authors": ["1576"]}
{"title": "Broad context language modeling as reading comprehension\n", "abstract": " Progress in text understanding has been driven by large datasets that test particular capabilities, like recent datasets for reading comprehension (Hermann et al., 2015). We focus here on the LAMBADA dataset (Paperno et al., 2016), a word prediction task requiring broader context than the immediate sentence. We view LAMBADA as a reading comprehension problem and apply comprehension models based on neural networks. Though these models are constrained to choose a word from the context, they improve the state of the art on LAMBADA from 7.3% to 49%. We analyze 100 instances, finding that neural network readers perform well in cases that involve selecting a name from the context based on dialogue or discourse cues but struggle when coreference resolution or external knowledge is needed.", "num_citations": "24\n", "authors": ["1576"]}
{"title": "Tarskian set constraints\n", "abstract": " We investigate set constraints over set expressions with Tarskian functional and relational operations. Unlike the Herbrand constructor symbols used in recent set constraint formalisms, the meaning of a Tarskian function symbol is interpreted in an arbitrary first order structure. We show that satisfiability of Tarskian set constraints is decidable in nondeterministic doubly exponential time. We also give complexity results and open problems for various extensions and restrictions of the language.", "num_citations": "22\n", "authors": ["1576"]}
{"title": "A logical algorithm for ML type inference\n", "abstract": " This paper gives a bottom-up logic programming formulation of the Hindley-Milner polymorphic type inference algorithm. We show that for programs of bounded order and arity the given algorithm runs in O(n\u03b1(n) + dn) time where n is the length of the program, d is the \u201cscheme depth\u201d of the program, and \u03b1 is the inverse of Ackermann\u2019s function. It is argued that for practical programs d will not exceed 5 even for programs with hundreds of module layers. This formulation of the Hindley-Milner algorithm is intended as a case study in \u201clogical algorithms\u201d, i.e., algorithms presented and analyzed as bottom-up inference rules.", "num_citations": "21\n", "authors": ["1576"]}
{"title": "Partial order backtracking\n", "abstract": " Ginsberg has recently introduced a polynomial space aggressive dependency directed backtrack search technique. Aggressive dependency directed techniques have the property that the size of the search space generated by a union of disjoint subproblems is proportional to the sum of the search space generated by each problem independently. Earlier polynomial space dependency directed methods did not have this property. This paper presents two additional polynomial space aggressive dependency directed algorithms that allow greater exibility in the use of variable and value ordering heuristics. One of the most important heuristics in constraint satisfaction search involves dynamic rearrangement of the order in which variables are considered. The second procedure presented here, which we call partial order backtracking, allows the order of past variables to be dynamically improved during the search.", "num_citations": "21\n", "authors": ["1576"]}
{"title": "Emergent predication structure in hidden state vectors of neural readers\n", "abstract": " A significant number of neural architectures for reading comprehension have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of \"predication structure\" in the hidden state vectors of these readers. More specifically, we provide evidence that the hidden state vectors represent atomic formulas  where  is a semantic property (predicate) and  is a constant symbol entity identifier.", "num_citations": "18\n", "authors": ["1576"]}
{"title": "On the complexity of set-based analysis\n", "abstract": " We define a general notion of set-based analysis --- any language whose operational semantics is defined by environment evaluation has a well defined set-based abstraction. This general definition covers both Aiken and Wimmers' type system and Heintze' set-based analysis. Aiken and Wimmers give a nondeterministic exponential time algorithm for their analysis. Heintze gives an O(n3) procedure. We show that this discrepancy is due to the complexity of the case statements analyzed. For polymorphic programs with deep patterns in case statements (as in the Aiken-Wimmers language) we show that the problem of determining type-safety under set-based abstraction is complete for deterministic exponential time. The problem remains complete for deterministic exponential time for programs without let (monovariant programs). However, for monovariant programs in which all patterns in case statements are\u00a0\u2026", "num_citations": "18\n", "authors": ["1576"]}
{"title": "The use of equality in deduction and knowledge representation\n", "abstract": " This report describes a system which  maintains canonical expressions for  designators under a set of equalities.  Substitution is used to maintain all knowledge  in terms of these canonical expressions. A  partial order on designators, termed the  better-name relation, is used in the choice of  canonical expressions. It is shown that with  an appropriate better-name relation an  important engineering reasoning technique,  propagation of constraints, can be  implemented as a special case of this  substitution process. Special purpose  algebraic simplification procedures are  embedded such that they interact effectively  with the equality system. An electrical circuit  analysis system is developed which relies  upon constraint propagation and algebraic  simplification as primary reasoning  techniques. The reasoning is guided by a  better-name relation in which referentially  transparent terms are preferred to referentially  opaque ones. Multiple description of  subcircuits are shown to interact strongly with  the reasoning mechanism.", "num_citations": "17\n", "authors": ["1576"]}
{"title": "Discriminative metric learning by neighborhood gerrymandering\n", "abstract": " We formulate the problem of metric learning for k nearest neighbor classification as a large margin structured prediction problem, with a latent variable representing the choice of neighbors and the task loss directly corresponding to classification error. We describe an efficient algorithm for exact loss augmented inference, and a fast gradient descent algorithm for learning in this model. The objective drives the metric to establish neighborhood boundaries that benefit the true class labels for the training points. Our approach, reminiscent of gerrymandering (redrawing of political boundaries to provide advantage to certain parties), is more direct in its handling of optimizing classification accuracy than those previously proposed. In experiments on a variety of data sets our method is shown to achieve excellent results compared to current state of the art in metric learning.", "num_citations": "16\n", "authors": ["1576"]}
{"title": "Walksat in the 2004 SAT Competition\n", "abstract": " The first convincing demonstration that local search could be used to solve challenging satisfiability problems was provided by the GSAT algorithm [9]. GSAT performs gradient descent search in the space of complete truth assignments, where adjacent assignments differ on a single variable and the objective function is the number of clauses not satisfied by the assignment. Like all local search routines GSAT can become trapped in a local minima. One technique for reducing this problem is to randomly alternate between greedy minimizing moves and \u201cnoisy\u201d moves that are randomly selected from the variables that appear in unsatisfied clauses [7].The Walksat algorithm [8] is based on the insight that such noisy moves could be made the basis for local search. Rather than trying to globally determine the best move, Walksat first randomly chooses an unsatisfied clause, and then selects a variable to flip within the clause. Because Walksat may overlook the best global move it is said to perform hill-climbing rather than gradient descent. The fact that a clause is unsatisfied means that at least one of the variables in the clause must be flipped in order to reach a global solution. If variables are chosen randomly from the clause and the clause length is bounded by a constant k it is easy to see that each flip as a 1/k or better chance of being correct. When k= 2 a pure random walk strategy will solve a satisfiable formula over n variables with high probability in O (n2) time [5]. For larger values of k the only worst-case guarantees for pure random walk are exponential. In practice, therefore, the variable to be flipped is chosen from the (randomly selected\u00a0\u2026", "num_citations": "16\n", "authors": ["1576"]}
{"title": "Comparing policy-gradient algorithms\n", "abstract": " We present a series of formal and empirical results comparing the efficiency of various policy-gradient methods\u2014methods for reinforcement learning that directly update a parameterized policy according to an approximation of the gradient of performance with respect to the policy parameter. Such methods have recently become of interest as an alternative to value-function-based methods because of superior convergence guarantees, ability to find stochastic policies, and ability to handle large and continuous action spaces. Our results include: 1) formal and empirical demonstrations that a policy-gradient method suggested by Sutton et al.(2000) and Konda and Tsitsiklis (2000) is no better than REINFORCE, 2) derivation of the optimal baseline for policy-gradient methods, which differs from the widely used V \u03c0 (s) previously thought to be optimal, 3) introduction of a new all-action policy-gradient algorithm that is unbiased and requires no baseline, and demonstrating empirically and semi-formally that it is more efficient than the methods mentioned above, and 4) an overall comparison of methods on the mountain-car problem including value-function-based methods and bootstrapping actor-critic methods. One general conclusion we draw is that the bias of conventional value functions is a feature, not a bug; it seems required is order for the value function to significantly accelerate learning.", "num_citations": "16\n", "authors": ["1576"]}
{"title": "Automated Inductive Reasoning about Logic Programs.\n", "abstract": " This paper describes a verifier for logic progralis that uses a form of strong induction to obtain induction hypotheses directly from formulas to be proved. The use of strong rather than weak induction simplifies the process of generating induction hypotheses, yet it ap-pears not to cause the new verifier to be less powerful than previous systeins.", "num_citations": "16\n", "authors": ["1576"]}
{"title": "Direct error rate minimization of hidden markov models\n", "abstract": " We explore discriminative training of HMM parameters that directly minimizes the expected error rate. In discriminative training one is interested in training a system to minimize a desired error function, like word error rate, phone error rate, or frame error rate. We review a recent method (McAllester, Hazan and Keshet, 2010), which introduces an analytic expression for the gradient of the expected error-rate. The analytic expression leads to a perceptron-like update rule, which is adapted here for training of HMMs in an online fashion. While the proposed method can work with any type of the error function used in speech recognition, we evaluated it on phoneme recognition of TIMIT, when the desired error function used for training was frame error rate. Except for the case of GMM with a single mixture per state, the proposed update rule provides lower error rates, both in terms of frame error rate and phone error rate\u00a0\u2026", "num_citations": "15\n", "authors": ["1576"]}
{"title": "Alpha-beta-conspiracy search\n", "abstract": " We introduce a variant of \u03b1-\u03b2 search in which each node is associated with two depths rather than one. The purpose of \u03b1-\u03b2 search is to find strategics for each player that together establish a value for the root position. A max strategy establishes a lower bound and the min strategy establishes an upper bound. It has long been observed that forced moves should be searched more deeply. Here we make the observation that in the max strategy we are only concerned with the forcedness of max moves and in the min strategy we are only concerned with the forcedness of min moves. This leads to two measures of depth\u2014one for each strategy\u2014and to a two-depth variant of \u03b1-\u03b2 called ABC search. The two-depth approach can be formally derived from conspiracy theory and the structure of the ABC procedure is justified by two theorems relating ABC search and conspiracy numbers.", "num_citations": "15\n", "authors": ["1576"]}
{"title": "Learning theory and language modeling\n", "abstract": " We consider some of our recent work on Good-Turing estimators in the larger context of learning theory and language modeling. The Good-Turing estimators have played a signi cant role in natural language modeling for the past twenty years. We have recently shown that these particular leave-one-out estimators converge rapidly. We present these results and consider possible consequences for language modeling in general. In particular, other leave-one-out estimators, such as for the cross entropy of various forms of language models, might also be shown to be rapidly converging using proof methods similar to those used for the Good-Turing estimators. This could have broad rami cation in the analysis and development of language modeling methods. We suggest that, in language modeling at least, leave-one-out estimation may be more signi cant than Occam's razor.", "num_citations": "14\n", "authors": ["1576"]}
{"title": "A proof of strong normalization for F2, F\u03c9, and beyond\n", "abstract": " We present an evaluation technique for proving strong normalization (SN). We use the technique to give SN proofs for F2, F-bounded quantification, subtypes, and F\u03c9. The evaluation technique derives SN as a corollary of the soundness of the typing rules under an appropriate evaluation semantics. The evaluation semantics yields simpler type sets than those used in the earlier SN proofs. The type sets discussed here form a complete lattice under classical union and intersection. The simplified type sets allow a simplified treatment of a variety of type constructors.", "num_citations": "13\n", "authors": ["1576"]}
{"title": "Information theoretic co-training\n", "abstract": " This paper introduces an information theoretic co-training objective for unsupervised learning. We consider the problem of predicting the future. Rather than predict future sensations (image pixels or sound waves) we predict \"hypotheses\" to be confirmed by future sensations. More formally, we assume a population distribution on pairs  where we can think of  as a past sensation and  as a future sensation. We train both a predictor model  and a confirmation model  where we view  as hypotheses (when predicted) or facts (when confirmed). For a population distribution on pairs  we focus on the problem of measuring the mutual information between  and . By the data processing inequality this mutual information is at least as large as the mutual information between  and  under the distribution on triples  defined by the confirmation model . The information theoretic training objective for  and  can be viewed as a form of co-training where we want the prediction from  to match the confirmation from .", "num_citations": "12\n", "authors": ["1576"]}
{"title": "Inferring recursive data types\n", "abstract": " This paper contains ve results on the problem of inferring types. The rst is that type inference over recursive types with unions and data constructors can be done in cubic time using a ow analysis. The second is a general theorem characterizing the time complexity of bottom-up logic programs. The O (n3) running time of the ow analysis is a corollary of this bottom-up run time theorem. The third is that for shallow case statements typability by the semantic types of Aiken, Wimmers and Lakshman is equivalent to typability by recursive types and hence can be determined by ow analysis. The fourth is that, even for rst order programs of arity one, typability by recursive types is PSPACE hard for polymorphic programs. The nal result is that for any xed bound on order and arity Hindley-Milner typability can be determined in pseudo-linear time, ie, O (n (n)) where is the inverse Ackerman function. The last two results suggest that let-polymorphism over simple types is fundamentally more tractable than let-polymorphism over recursive types.", "num_citations": "12\n", "authors": ["1576"]}
{"title": "Discriminatively trained mixtures of deformable part models\n", "abstract": " Discriminatively Trained Mixtures of Deformable Part Models Page 1 Discriminatively Trained Mixtures of Deformable Part Models Pedro Felzenszwalb and Ross Girshick University of Chicago David McAllester Toyota Technological Institute at Chicago Deva Ramanan UC Irvine http://www.cs.uchicago.edu/~pff/latent Page 2 Model Overview \u2022 Mixture of deformable part models (pictorial structures) \u2022 Each component has global template + deformable parts \u2022 Fully trained from bounding boxes alone Page 3 2 component bicycle model root filters coarse resolution part filters finer resolution deformation models Page 4 Object Hypothesis Image pyramid HOG feature pyramid Multiscale model captures features at two resolutions Score of object hypothesis is sum of filter scores minus deformation costs Score of filter is dot product of filter with HOG features underneath it Page 5 Connection with linear classifier concatenation \u2026", "num_citations": "11\n", "authors": ["1576"]}
{"title": "Keeping the ball from CMUnited-99\n", "abstract": " This paper presents preliminary results achieved during our current development of a team for simulated robotic soccer in the RoboCup soccer server [2]. We have constructed a team that plays a simplified \u201ckeepaway\u201d game. Playing keepaway against the 1999 RoboCup champion CMUnited-99 team, our new program holds the ball for an average of 25 second with an average distance of 24 meters from the opponents end of the field. CMUnited-99 playing against itself holds the ball for an average of only 6 seconds. Here we describe the design of the keepaway team. The principal technique used is the vector sum of force-fields for governing player motion when they are not in possession of the ball.", "num_citations": "11\n", "authors": ["1576"]}
{"title": "Random-world semantics and syntactic independence for expressive languages\n", "abstract": " We consider three desiderata for a language combining logic and probability: logical expressivity, random-world semantics, and the existence of a useful syntactic condition for probabilistic independence. Achieving these three desiderata simultaneously is nontrivial. Expressivity can be achieved by using a formalism similar to a programming language, but standard approaches to combining programming languages with probabilities sacrifice random-world semantics. Naive approaches to restoring random-world semantics undermine syntactic independence criteria. Our main result is a syntactic independence criterion that holds for a broad class of highly expressive logics under random-world semantics. We explore various examples including Bayesian networks, probabilistic context-free grammars, and an example from Mendelian genetics. Our independence criterion supports a case-factor inference technique that reproduces both variable elimination for BNs and the inside algorithm for PCFGs.", "num_citations": "10\n", "authors": ["1576"]}
{"title": "Constraint satisfaction search\n", "abstract": " For the past thirty ve years artificial intelligence researchers have been studying heuristic search techniques. People writing AI programs have had strong intuitions about what it means for a program to search. Intuitively they classified programs according to the kinds of search performed. Certain programs, such as matrix multiplication routines, did no search whatsoever. Other programs searched for solutions to a fixed nite set of constraints. Others searched for paths in graphs, or for strategies in game trees, or for proofs in formal inference systems. The intuitive classification of search programs seems to roughly correspond to complexity classes. The no-search procedures correspond to the complexity class P. Constraint satisfaction search procedures correspond to the complexity class NP, graph search and game search procedures to the class PSPACE, and search in theorem proving to the class of recursively enumerable functions. This chapter discusses heuristic techniques for solving search problems of the simplest type| constraint satisfaction problems. Heuristic techniques for other forms of search are discussed", "num_citations": "10\n", "authors": ["1576"]}
{"title": "Algebraic Approximations.\n", "abstract": " There are several artificial intelligence domains, such as electronic circuit analysis or chemical engineering, where handling symbolic mathematics is important. While symbolic mathematics packages exist [I] they often prove inadequate to handle the complexity of the symbolic expressions in such domains [2]. This paper proposes some techniques for using assumptions about relative magnitudes to simplifyexpressions while maintaining a known degree of accuracy. There is a strong analogy between the proposed technique and the use of exponential notation in the approximate representation of real numbers.The necessity of making assumptions about the relative sizes of the values of variables introduces the possibility that they will be proven wrong during the course of solving some set of equations. Thus a truth maintenance system [3][4] may prove useful in keeping track of the assumptions underlyingspecificsim plifications and solutions. The number of assumptions needed can be reduced by employing constraint propogation (5) as a primary technique for solving equations.", "num_citations": "10\n", "authors": ["1576"]}
{"title": "Att-cmunited-2000: Third place finisher in the robocup-2000 simulator league\n", "abstract": " The ATT-CMUnited-2000 simulator team finished in third place at Robocup- 2000. It was one of only three teams to finish ahead of the previous year\u2019s champion, CMUnited-99. In controlled experiments, ATT-CMUnited-2000 beats CMUnited-99 by an average score of 2.6-0.2.", "num_citations": "9\n", "authors": ["1576"]}
{"title": "Boosting with Multi-Way Branching in Decision Trees.\n", "abstract": " It is known that decision tree learning can be viewed as a form of boosting. However, existing boosting theorems for decision tree learning allow only binary-branching trees and the generalization to multi-branching trees is not immediate. Practical decision tree algorithms, such as CART and C4. 5, implement a trade-o between the number of branches and the improvement in tree quality as measured by an index function. Here we give a boosting justi cation for a particular quantitative trade-o curve. Our main theorem states, in essence, that if we require an improvement proportional to the log of the number of branches then top-down greedy construction of decision trees remains an e ective boosting algorithm.", "num_citations": "9\n", "authors": ["1576"]}
{"title": "Large margin methods for structured classification: Exponentiated gradient algorithms and PAC-Bayesian generalization bounds\n", "abstract": " We consider the problem of structured classification, where the task is to predict a label \u043d from an input \u043c, and \u043d has meaningful internal structure. Our framework includes supervised training of both Markov random fields and weighted context-free grammars as special cases. We describe an algorithm that solves the large-margin optimization problem defined in [12], using an exponentialfamily (Gibbs distribution) representation of structured objects. The algorithm is efficient\u2013even in cases where the number of labels \u043d is exponential in size\u2013provided that certain expectations under Gibbs distributions can be calculated efficiently. The optimization method we use for structured labels relies on a more general result, specifically the application of exponentiated gradient (EG) updates [4, 5] to quadratic programs (QPs). We describe a new method for solving QPs based on these techniques, and give bounds on its rate of convergence. In addition to their application to the structured-labels task, the EG updates lead to simple algorithms for optimizing \u201cconventional\u201d binary or multiclass SVM problems. Finally, we give a new generalization bound for structured classification, using PAC-Bayesian methods for the analysis of large margin classifiers.", "num_citations": "7\n", "authors": ["1576"]}
{"title": "Three Cuts for Accelerated Interval Propagation.\n", "abstract": " This paper addresses the problem of nonlinear multivariate root finding. In an earlier paper we describe a system called Newton which finds roots of systems of nonlinear equations using refinements of interval methods. The refinements are inspired by Al constraint propagation techniques. Newton is competitive with continuation methods on most benchmarks and can handle a variety of cases that are infeasible for continuation methods. This paper presents three cuts which we believe capture the essential theoretical ideas behind the success of Newton. This paper describes the cuts in a concise and abstract manner which, we believe, makes the theoretical content of our work more apparent. Any implementation will need to adopt some heuristic control mechanism. Heuristic control of the cuts is only briefly discussed here.Descriptors:", "num_citations": "7\n", "authors": ["1576"]}
{"title": "Variational Attribute Grammars for Computer Aided Design\n", "abstract": " This document describes a variational attribute grammar (VAG) design language. VAG is a functional programming language speci cally designed for the rapid implementation of domain speci c CAD systems. VAG is intended to support the implementations of CAD systems in such diverse areas as mechanical, electrical, and software design. The main feature of the VAG language is a constraint based reasoning mechanisms to support the CAD user in analyzing partial designs in a wide variety of domains.", "num_citations": "7\n", "authors": ["1576"]}
{"title": "Pac-bayesian theory\n", "abstract": " The PAC-Bayesian framework is a frequentist approach to machine learning which encodes learner bias as a \u201cprior probability\u201d over hypotheses. This chapter reviews basic PAC-Bayesian theory, including Catoni\u2019s basic inequality and Catoni\u2019s localization theorem.", "num_citations": "6\n", "authors": ["1576"]}
{"title": "Domain modeling in engineering computer-based systems\n", "abstract": " Domain modeling is believed to be a key factor in developing an economical and scalable means for constructing families of related software systems. In this paper, we review the current state of domain modeling, and present some of our work on the ADAGE (Avionics Domain Application Generation Environment) project, an integrated environment that relies heavily on domain models for generating real-time avionics applications. Specifically, we explain how we detect errors in the design of avionics systems that are expressed in terms of compositions of components. We also offer insights on how domain modeling can benefit the engineering of computer-based systems in other domains.", "num_citations": "6\n", "authors": ["1576"]}
{"title": "Information-Theoretic Segmentation by Inpainting Error Maximization\n", "abstract": " We study image segmentation from an information-theoretic perspective, proposing a novel adversarial method that performs unsupervised segmentation by partitioning images into maximally independent sets. More specifically, we group image pixels into foreground and background, with the goal of minimizing predictability of one set from the other. An easily computed loss drives a greedy search process to maximize inpainting error over these partitions. Our method does not involve training deep networks, is computationally cheap, class-agnostic, and even applicable in isolation to a single unlabeled image. Experiments demonstrate that it achieves a new state-of-the-art in unsupervised segmentation quality, while being substantially faster and more general than competing approaches.", "num_citations": "5\n", "authors": ["1576"]}
{"title": "Domain-independent dominance of adaptive methods\n", "abstract": " From a simplified analysis of adaptive methods, we derive AvaGrad, a new optimizer which outperforms SGD on vision tasks when its adaptability is properly tuned. We observe that the power of our method is partially explained by a decoupling of learning rate and adaptability, greatly simplifying hyperparameter search. In light of this observation, we demonstrate that, against conventional wisdom, Adam can also outperform SGD on vision tasks, as long as the coupling between its learning rate and adaptability is taken into account. In practice, AvaGrad matches the best results, as measured by generalization accuracy, delivered by any existing optimizer (SGD or adaptive) across image classification (CIFAR, ImageNet) and character-level language modelling (Penn Treebank) tasks. When training GANs, AvaGrad improves upon existing optimizers.", "num_citations": "5\n", "authors": ["1576"]}
{"title": "Particle-based belief propagation for structure from motion and dense stereo vision with unknown camera constraints\n", "abstract": " In this paper, we present a specific use of the Particle-based Belief Propagation (PBP) algorithm as an approximation scheme for the joint distribution over many random variables with very large or continuous domains. After formulating the problem to be solved as a probabilistic graphical model, we show that by running loopy Belief Propagation on the whole graph, in combination with an MCMC method such as Metropolis-Hastings sampling at each node, we can approximately estimate the posterior distribution of each random variable over the state space. We describe in details the application of PBP algorithm to the problem of sparse Structure from Motion and the dense Stereo Vision with unknown camera constraints. Experimental results from both cases are demonstrated. An experiment with a synthetic structure from motion arrangement shows that its accuracy is comparable with the state-of-the-art\u00a0\u2026", "num_citations": "5\n", "authors": ["1576"]}
{"title": "Probabilistic programming\n", "abstract": " Probabilistic Programming Page 1 Probabilistic Programming Daniel M. Roy Department of Statistical Sciences Department of Computer Science University of Toronto Workshop on Uncertainty in Computation 2016 Program on Logical Structures in Computation Simons Institute for the Theory of Computing Page 2 1. Simple story: Probabilistic programming automates Bayesian inference 2. Real story: It\u2019s complicated 1/50 Page 3 Probabilistic programming 1. Represent probability distributions by formulas programs that generate samples. 2. Build generic algorithms for probabilistic conditioning using probabilistic programs as representations. Bayesian statistics 1. Express statistical assumptions via probability distributions. Pr(parameters, data) | {z } joint = Pr(parameters) | {z } prior Pr(data | parameters) | {z } model/likelihood 2. Statistical inference from data ! parameters via conditioning. Pr(parameters, data), x \u2026", "num_citations": "5\n", "authors": ["1576"]}
{"title": "Meta-complexity theorems: Talk abstract\n", "abstract": " A variety of authors have argued that bottom-up logic programs provide a clear presentation of a wide variety of algorithms, e.g.,[6,4,5,2,3]. Recently meta-complexity theorems have been given that allow, in many cases, the running time of a bottom-up logic program to be determined by inspection [1]. This talk abstract presents the fundamental meta-complexity theorem. New meta-compleixty theorems useful for optimization algorithms were presented in the talk.", "num_citations": "5\n", "authors": ["1576"]}
{"title": "Bellman equations for stochastic programs\n", "abstract": " This paper gives a model-checking style algorithm for verifying the decision-theoretic utility of a hand-written robot controller. The paper presents a simple Turing-complete programming language for expressing robot control programs. The robot control language supports stochastic nondeterminism, complex data structures, conditional expressions, and recursive procedures. A similar Turing-complete programming language for writing models of the robot's environment is also given. The paper shows that if a given controller running in a given environment has only nitely many behaviors (as de ned in the paper) then the value of that controller in that environment can be computed by iterating appropriately generalized Bellman equations.", "num_citations": "5\n", "authors": ["1576"]}
{"title": "The rise of nonlinear mathematical programming\n", "abstract": " The rise of nonlinear mathematical programming | ACM Computing Surveys ACM Digital Library Logo ACM Logo Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search ACM Computing Surveys Journal Home Forthcoming Latest Issue Archive Authors Author List Author Guidelines Submission Site ACM Author Policies Editors Editorial Board Associate Editor Guidelines Associate Editors Welcome Video Reviewers Reviewer Guidelines Affiliations Award Winners About Charter Announcements Abstracting/Indexing Contact Us More HomeACM JournalsACM Computing SurveysVol. , No. 4esThe rise of nonlinear mathematical programming article The rise of nonlinear mathematical programming Share on Author: David Allen McAllester profile image \u2026", "num_citations": "5\n", "authors": ["1576"]}
{"title": "Morphoid type theory\n", "abstract": " Morphoid type theory (MorTT) is a typed foundation for mathematics extending classical predicate calculus under Platonic compositional semantics and supporting the concept of isomorphism. MorTT provides a formal account of the substitution of isomorphics, the distinction between general functions and natural maps, and \u201cVoldemort\u2019s theorem\u201d stating that certain objects exist but cannot be named. For example, there is no natural point on a geometric circle\u2014no point on a geometric circle can be named by a well-typed expression. Similarly it is not possible to name any particular basis for a vector space or any particular isomorphism of a finite dimensional vector space with its dual. Homotopy type theory (HoTT) also provides a formal account of isomorphism but extends constructive logic rather than classical predicate calculus. MorTT\u2019s classical approach avoids HoTT\u2019s propositionsas-types, path induction, squashing and higher order isomorphisms. Unlike HoTT, MorTT is designed to be compatible with Platonic mathematical thought.", "num_citations": "4\n", "authors": ["1576"]}
{"title": "Lsvm-mdpm release 4 notes\n", "abstract": " This note describes some recent advances that improve the performance of the object detection system described in [2]. Some of the improvements included here comprised the UoC-TTI LSVMMDPM entry in the PASCAL VOC 2009 comp3 challenge [1] and others were developed subsequently. Complete source code for the latest version of the object detection system can be found at http://people. cs. uchicago. edu/~ pff/latent/.", "num_citations": "4\n", "authors": ["1576"]}
{"title": "A statistical mechanics approach to large deviations theorems\n", "abstract": " Cherno bounds and related large deviation bounds have a wide variety of applications in statistics and learning theory. This paper proves that for any real-valued random variable X the probability of a deviation to value x is bounded by eS (x) where S (x) is the entropy at energy x of a physical system corresponding to the variable X. It is a well known fact of statistical mechanics that entropy is equal to a double integral of the reciprocal of energy variance. So we get a general bound on large deviation probabilities in terms of variance over a range of temperatures. This greatly simpli es the derivation of Cherno and Hoe ding bounds and leads immediately to a variety of apparently new large deviation theorems.", "num_citations": "4\n", "authors": ["1576"]}
{"title": "Automated Deduction-CADE-17: 17th International Conference on Automated Deduction Pittsburgh, PA, USA, June 17-20, 2000 Proceedings\n", "abstract": " For the past 25 years the CADE conference has been the major forum for the presentation of new results in automated deduction. This volume contains the papers and system descriptions selected for the 17th International Conference on Automated Deduction, CADE-17, held June 17-20, 2000, at Carnegie Mellon University, Pittsburgh, Pennsylvania (USA). Fifty-three research papers and twenty system descriptions were submitted by researchers from? fteen countries. Each submission was reviewed by at least three reviewers. Twenty-four research papers and? fteen system descriptions were accepted. The accepted papers cover a variety of topics related to t-orem proving and its applications such as proof carrying code, cryptographic protocol veri? cation, model checking, cooperating decision procedures, program veri? cation, and resolution theorem proving. The program also included three invited lectures:\u201cHigh-level veri? cation using theorem proving and formalized mathematics\u201d by John Harrison,\u201cSc-able Knowledge Representation and Reasoning Systems\u201d by Henry Kautz, and \u201cConnecting Bits with Floating-Point Numbers: Model Checking and Theorem Proving in Practice\u201d by Carl Seger. Abstracts or full papers of these talks are included in this volume. In addition to the accepted papers, system descriptions, andinvited talks, this volumecontains one page summaries of four tutorials and? ve workshops held in conjunction with CADE-17.", "num_citations": "4\n", "authors": ["1576"]}
{"title": "Tarskian set constraints\n", "abstract": " We investigate set constraints over set expressions with Tarskian functional and relational operations. Unlike the Herbrand constructor symbols used in recent set constraint formalisms, the meaning of a Tarskian function symbol is interpreted in an arbitrary first order structure. We show that satisfiability of Tarskian set constraints is decidable in nondeterministic doubly exponential time. We also give complexity results and open problems for various extensions and restrictions of the language. Keywords: Set constraints, decision procedures, dynamic logic, mu-calculus. 1. Introduction There has been considerable interest recently in formalisms for describing and reasoning about sets. Here we consider a family of formalisms that have received surprisingly little attention. Consider a set expression of the form where denote sets. In recent work on set constraints, operation symbols are interpreted as Herbrand term constructors so that the set expression denotes the set of terms wher...", "num_citations": "4\n", "authors": ["1576"]}
{"title": "Automatically inferring properties of computer programs\n", "abstract": " This thesis presents two independent pieces of research. First, we consider the problem of automatically inferring properties of programs. Our approach is to explore the application of familiar type inference principles to a \u201ctype system\u201d sufficiently expressive that the typing problem is effectively the check-ing of program specifications. We present such a type system, and use familiar syntax-directed type inference rules to give a polynomial-time procedure for inferring type theorems in this type system. We discuss examples of simple functional programs and the specification information this procedure automatically infers. The enriched notion of type allows the definition of any recursively enumerable set as a type, and includes argument-dependent output types for functions. The inference procedure is capable for example of automatically inferring that an insertion sort program always returns a sorted permutation of its\u00a0\u2026", "num_citations": "4\n", "authors": ["1576"]}
{"title": "Lifting transformations\n", "abstract": " Lifting is a well-known technique in resolution theorem proving, logic programming, and term rewriting. In this paper, the authors formulate lifting as an efficiency-motivated program transformation applicable to a wide variety of nondeterministic procedures. This formulation allows the immediate lifting of complex procedures, such as the Davis-Putnam algorithm, which are otherwise difficult to lift. They treat both classical lifting, which is based on unification, and various closely related program transformations that they also call lifting transformations. These nonclassical lifting transformations are closely related to constraint techniques in logic programming, resolution, and term rewriting. Formulating these techniques as transformations on nondeterministic programs expands the range of procedures to which the techniques can be easily applied.Descriptors:", "num_citations": "4\n", "authors": ["1576"]}
{"title": "On-the-fly information retrieval augmentation for language models\n", "abstract": " Here we experiment with the use of information retrieval as an augmentation for pre-trained language models. The text corpus used in information retrieval can be viewed as form of episodic memory which grows over time. By augmenting GPT 2.0 with information retrieval we achieve a zero shot 15% relative reduction in perplexity on Gigaword corpus without any re-training. We also validate our IR augmentation on an event co-reference task.", "num_citations": "3\n", "authors": ["1576"]}
{"title": "Discriminative latent variable models for object detection\n", "abstract": " (To be presented by Deva Ramanan). In this talk, I will discuss recent work by colleagues and myself on discriminative latent-variable models for object detection. Object recognition is one of the fundamental challenges of computer vision. We specifically consider the task of localizing and detecting instances of a generic object category, such as people or cars, in cluttered real-word images. Recent benchmark competitions such as the PASCAL Visual Object Challenge suggest our method is the state-of-the-art system for such tasks. This success, combined with publicallyavailable code that runs orders of magnitude faster than comparable approaches, has turned our system into a standard baseline for contemporary research on object recognition (Felzenszwalb et al., 2008; 2009).This talk will focus on the machine learning aspects of our approach. Our system is trained with a latent variable extension of support vector machines that we call a latent SVM. The formulation is equivalent to the MI-SVM framework for multiple instance learning. Latent variables provide a formalism for modeling structured variation in object appearance due to deformation, viewpoint, and other factors. The resulting learning problem is no longer convex, but admits a coordinate descent algorithm that exploits a \u2018semiconvex\u2019property. Notable aspects of our system involve (a) weakly-supervised learning, in which hidden latent structure is automatically inferred;(b) out-ofcore learning algorithms for learning from large-scale datasets that do not fit in memory; and (c) efficient algorithms for searching over latent variables.", "num_citations": "3\n", "authors": ["1576"]}
{"title": "Logic for Programming and Automated Reasoning: 7th International Conference, LPAR 2000 Reunion Island, France, November 6-10, 2000 Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the 7th International Conference on Logic for Programming and Automated Reasoning, LPAR 2000, held in Reunion Island, France in November 2000. The 26 revised full papers presented together with four invited contributions were carefully reviewed and selected from 65 submissions. The papers are organized in topical sections on nonmonotonic reasoning, descriptive complexity, specification and automatic proof-assistants, theorem proving, verification, logic programming and constraint logic programming, nonclassical logics and the lambda calculus, logic and databases, program analysis, mu-calculus, planning and reasoning about actions.", "num_citations": "3\n", "authors": ["1576"]}
{"title": "Concentration inequalities for the missing mass and for histogram rule error\n", "abstract": " This paper gives distribution-free concentration inequalities for the miss-ing mass and the error rate of histogram rules. Negative association meth-ods can be used to reduce these concentration problems to concentration questions about independent sums. Although the sums are independent, they are highly heterogeneous. Such highly heterogeneous independent sums cannot be analyzed using standard concentration inequalities such as Hoeffding\u2019s inequality, the Angluin-Valiant bound, Bernstein\u2019s in-equality, Bennett\u2019s inequality, or McDiarmid\u2019s theorem.", "num_citations": "3\n", "authors": ["1576"]}
{"title": "Effective bayesian inference for stochastic programs\n", "abstract": " In this paper, we propose a stochastic version of a general purpose functional programming language as a method of modeling stochastic processes. The language contains random choices, conditional statements, structured values, defined functions, and recursion. By imagining an experiment in which the program is\" run\" and the random choices made by sampling, we can interpret a program in this language as encoding a probability distribution over a (potentially infinite) set of objects. We provide an exact algorithm for computing conditional probabilities of the form Pr (P (x) j Q (x)) where x is chosen randomly from this distribution. This algorithm terminates precisely when sampling x and computing P (x) and Q (x) terminates in all possible stochastic executions (under lazy evaluation semantics, in which only values needed to compute the output of the program are evaluated). We demonstrate the applicability of the languageand the efficiency of the inference algorithm by en...", "num_citations": "3\n", "authors": ["1576"]}
{"title": "Structure and motion from road-driving stereo sequences\n", "abstract": " We introduce a unified framework for scene structure and motion estimation on road-driving stereo sequences. This framework is based on the slanted-plane scene model that has become widely popular in the stereo vision community. Our algorithm iteratively and alternately solves for scene structure and motion. Surface estimation is done using our own slanted-plane stereo algorithm. Motion estimation is achieved by solving a MRF labeling problem using Loopy Belief Propagation. We show that with some specific assumptions about the motion of the camera and the scene, the motion estimation problem can be reduced to a 1D search problem along the epipolar lines. We also propose a novel evaluation metrics, based on the notion of view prediction error. This metrics can be used to evaluate the performance of structure and motion estimation algorithms on stereo sequences without ground truth data\u00a0\u2026", "num_citations": "2\n", "authors": ["1576"]}
{"title": "A discriminatively trained, multiscale, deformable part model\n", "abstract": " A Discriminatively Trained, Multiscale, Deformable Part Model Page 1 A Discriminatively Trained, Multiscale, Deformable Part Model Edward Hsiao 16-721 Learning Based Methods in Vision February 16, 2009 P. Felzenszwalb, D. McAllester, and D. Ramanan Images taken from P. Felzenszwalb, D. Ramanan, N. Dalal and B. Triggs Page 2 Overview Page 3 Histogram of Oriented Gradients (HOG) \u2022 Split detection window into 8x8 non-overlapping pixel regions called cells \u2022 Compute 1D histogram of gradients in each cell and discretize into 9 orientation bins \u2022 Normalize histogram of each cell with the total energy in the four 2x2 blocks that contain that cell -> 9x4 feature vector \u2022 Apply a linear SVM classifier Page 4 Histogram of Oriented Gradients (HOG) 9 orientation bins 0 - 180 degrees block cell 9x4 feature vector per cell normalize Feature vector f = [\u2026,\u2026,\u2026, ,\u2026] Page 5 Histogram of Oriented Gradients (HOG) 9 \u2026", "num_citations": "2\n", "authors": ["1576"]}
{"title": "Logic Programming and Automated Reasoning: 6th International Conference, LPAR'99, Tbilisi, Georgia, September 6-10, 1999, Proceedings\n", "abstract": " This volume contains the papers presented at the Sixth International Conference on Logic for Programming and Automated Reasoning (LPAR'99), held in Tbilisi, Georgia, September 6-10, 1999, and hosted by the University of Tbilisi. Forty-four papers were submitted to LPAR'99. Each of the submissions was reviewed by three program committee members and an electronic program com mittee meeting was held via the Internet. Twenty-three papers were accepted. We would like to thank the many people who have made LPAR'99 possible. We are grateful to the following groups and individuals: to the program committee and the additional referees for reviewing the papers in a very short time, to the organizing committee, and to the local organizers of the INTAS workshop in Tbilisi in April 1994 (Khimuri Rukhaia, Konstantin Pkhakadze, and Gela Chankvetadze). And last but not least, we would like to thank Konstantin-rovin, who maintained the program committee Web page; Uwe Waldmann, who supplied macros for these proceedings and helped us to install some programs for the electronic management of the program committee work; and Bill McCune, who implemented these programs.", "num_citations": "2\n", "authors": ["1576"]}
{"title": "Learning Theory 2004\n", "abstract": " Journal of Computer and System Sciences Page 1 Journal of Computer and System Sciences CONTENTS Volume 74, Number 1, February 2008 Special Issue: Learning Theory 2004 Guest Editor: Sanjoy Dasgupta Guest Editorial 1 Special issue on learning theory Sanjoy Dasgupta Articles 2 Learning with errors in answers to membership queries Laurence Bisht, Nader H. Bshouty, Lawrance Khoury 16 The complexity of properly learning simple concept classes Misha Alekhnovich, Mark Braverman, Vitaly Feldman, Adam R. Klivans, Toniann Pitassi 35 Learning intersections of halfspaces with a margin Adam R. Klivans, Rocco A. Servedio 49 Using mixture models for collaborative filtering Jon Kleinberg, Mark Sandler 70 A decentralized algorithm for spectral analysis David Kempe, Frank McSherry 84 Case-factor diagrams for structured probabilistic modeling David McAllester, Michael Collins, Fernando Pereira 97 \u2026", "num_citations": "2\n", "authors": ["1576"]}
{"title": "World-modeling vs. world-axiomatizing\n", "abstract": " A logic allows one to express statements (axioms) that are, perhaps approximately, true of the world. A model is a particular object that is similar to, or \u201cmodels\u201d, the world. For example, the growing field of model checking involves formal models of the behavior of physical computer chips. Bayesian networks, MPDs, and POMDPs are models of (real) probabilistic environments. This paper argues that world-modeling is more natural that world-axiomatizing. The main technical result is an algorithm for exactly computing the asymptotic average reward of a robot controller written in a high level programming language when run in a world model also defined in a high level language.", "num_citations": "2\n", "authors": ["1576"]}
{"title": "Bayesian networks\n", "abstract": " Introduction Common sense reasoning is usually uncertain. If you see someone walking into a lecture with wet hair you might conclude that it is raining outside. This conclusion could, of course, be wrong. If you see a man and a woman at a playground with a small child you might conclude that these people are the child's parents and that they are married. You could, of course, be wrong. If you discover that there is no cereal in your cabinet, and you know you just put some there yesterday, you might conclude that your house mates ate it. You could be wrong. Although many AI researchers prefer other nonprobabilistic formalisms, 1 it is possible to construct and use probabilistic approaches to common sense reasoning. The advantages of the probabilistic approaches to common sense are described quite well in [Pearl, 1988]. As pointed out in the notes on common sense inference, to use probability theory as a foundation for common sense reasoning one must either rea", "num_citations": "2\n", "authors": ["1576"]}
{"title": "Graph search and STRIPS planning\n", "abstract": " Introduction to Graph Search Consider the eight puzzle show in figure 1. This puzzle is played on a three by three square board containing eight tiles and an empty square. Any tile which is next to the empty square can be slid into the empty square leaving an opening (empty square) in the place that used to be occupied by the moved tile. Figure 1 shows how one state of the puzzle can be transformed into another configure by sliding a tile into the empty square. Given an initial state of the puzzle the objective is to find a sequence of legal moves that transform the initial state into some desired goal state. Figure 2 also shows a typical goal state. The problem of finding a sequence of moves that transforms a given initial state into a given goal state can be formulated as a graph search problem. The nodes of the graph are the possible states of the puzzle and the arcs of the graph correspond to legal moves that transform one state into another. The problem is to find a path", "num_citations": "2\n", "authors": ["1576"]}
{"title": "Three universal relations\n", "abstract": " This paper explores universal relations, ie, meta-mathematical concepts that mathematicians seem to employ in all domains of mathematical reasoning. This paper presents precise mathematical definitions for three universal relations: isomorphism (type identity), essential property, and isoonticity. Although no formal explanation is given as to how universal relations expedite mathematical reasoning, some intuitive arguments are presented as to why these relations, and iso-onticity in particular, seem so useful in mathematics.", "num_citations": "2\n", "authors": ["1576"]}
{"title": "Dependency Directed Backtracking in Generalized Satisficing Assignment Problems\n", "abstract": " Many authors have described search techniques for the satisficing assignment problem: the problem offinding an interpretation for a set of discrete variables that satisfies a given set of constraints. In this paperwe present a formal specification of dependency directed backtracking as applied to this problem. Wealso generalize the satisficing assignment problem to include limited resource constraints that arise inoperations research and industrial engineering. We discuss several new search heuristics that can beapplied to this generalized problem, and give some empirical results on the performance of theseheuristics.", "num_citations": "2\n", "authors": ["1576"]}
{"title": "Symmetric Set Theory, a General Theory of Isomorphism, Abstraction, and Representation.\n", "abstract": " It is possible to represent a finite set of points atoms by a finite sequence of points. However a finite set of points has no distinguished member and therefore it is impossible to define a function which takes a finite set of points and returns a first point in that set. Thus it is impossible to represent a finite sequence of points by a finite set of points. The theory of symmetric sets provides a framework in which this observation about sets and sequences can be proven. The theory of symmetric sets is similar to classical Zermello-Fraenkel set theory with the exception that the universe of symmetric sets includes points ur-elements. Points provide a basis for general notions of isomorphism and symmetry. The general notions of isomorphism and symmetry in turn provide a basis for natural, simple, and universal defintions of abstractness, essential properties and functions, canonicality, and representations. It is expected that these notions will play an important role in the theory of data structures and in the construction of general techniques for reasoning about data structures. AuthorDescriptors:", "num_citations": "2\n", "authors": ["1576"]}
{"title": "Solving uninterpreted equations with context free expression grammars\n", "abstract": " It is shown here that the equivalence class of an expression under the congruence closure of any finite set of equations between ground terms is a context free expression language. An expression is either a symbol or an n-tuple of expressions the difference between expressions and strings is that expressions have inherent phrase structure. The fact that context free expression languages are closed under intersection is used to derive an algorithm for computing a grammar for the equivalence class of a given expression under any finite disjunction of finite sets of equations between ground expressions. This algorithm can also be used to derive a grammar representing the equivalence class of conditional expressions of the form if P then u else v. The description of an equivalence class by a context free expression grammar can also be used to simplify expressions under well behaved simplicity orders. Specifically if G is a context free expression grammar which generates an equivalence class of expressions then for any well behaved simplicity order there is a subset G of the productions of G such that the expressions generated by G are exactly those expressions of the equivalence class which are simplicity bounds and whose subterms are also simplicity bounds. Furthermore G can be computed from G in order nlogn time plus the time required to do order nlogn comparisons between expressions where n is the size G.Descriptors:", "num_citations": "2\n", "authors": ["1576"]}
{"title": "MathZero, The Classification Problem, and Set-Theoretic Type Theory\n", "abstract": " AlphaZero learns to play go, chess and shogi at a superhuman level through self play given only the rules of the game. This raises the question of whether a similar thing could be done for mathematics -- a MathZero. MathZero would require a formal foundation and an objective. We propose the foundation of set-theoretic dependent type theory and an objective defined in terms of the classification problem -- the problem of classifying concept instances up to isomorphism. The natural numbers arise as the solution to the classification problem for finite sets. Here we generalize classical Bourbaki set-theoretic isomorphism to set-theoretic dependent type theory. To our knowledge we give the first isomorphism inference rules for set-theoretic dependent type theory with propositional set-theoretic equality. The presentation is intended to be accessible to mathematicians with no prior exposure to type theory.", "num_citations": "1\n", "authors": ["1576"]}
{"title": "Blending Learning and Inference in Structured Prediction\n", "abstract": " In this paper we derive an efficient algorithm to learn the parameters of structured predictors in general graphical models. This algorithm blends the learning and inference tasks, which results in a significant speedup over traditional approaches, such as conditional random fields and structured support vector machines. For this purpose we utilize the structures of the predictors to describe a low dimensional structured prediction task which encourages local consistencies within the different structures while learning the parameters of the model. Convexity of the learning task provides the means to enforce the consistencies between the different parts. The inference-learning blending algorithm that we propose is guaranteed to converge to the optimum of the low dimensional primal and dual programs. Unlike many of the existing approaches, the inference-learning blending allows us to learn efficiently high-order graphical models, over regions of any size, and very large number of parameters. We demonstrate the effectiveness of our approach, while presenting state-of-the-art results in stereo estimation, semantic segmentation, shape reconstruction, and indoor scene understanding.", "num_citations": "1\n", "authors": ["1576"]}
{"title": "Relating training algorithms to generalization bounds in structured classification\n", "abstract": " We consider the relationship between generalization bounds and training algorithms in the case of structured classification. First we show that by rephrasing existing generalization bounds they can be related in a more explicit way to SVM algorithms optimizing hinge loss. The explicit relationship between the generalization bound and hinge loss suggests a novel form of Hinge loss in the structured case. We also give a new generalization bound for the structured case tighter than previously published bounds.", "num_citations": "1\n", "authors": ["1576"]}
{"title": "Computational Learning Theory\n", "abstract": " We consider some of our recent work on Good-Turing estimators in the larger context of learning theory and language modeling. The Good-Turing estimators have played a significant role in natural language modeling for the past 20 years. We have recently shown that these particular leave-one-out estimators converge rapidly. We present these results and consider possible consequences for language modeling in general. In particular, other leave-one-out estimators, such as for the cross-entropy of various forms of language models, might also be shown to be rapidly converging using proof methods similar to those used for the Good-Turing estimators. This could have broad ramifications in the analysis and development of language modeling methods. We suggest that, in language modeling at least, leave-one-out estimation may be more significant than Occam's razor.", "num_citations": "1\n", "authors": ["1576"]}
{"title": "Logical reasoning systems\n", "abstract": " this article. However, it is possible to give a somewhat superficial description of its limitations. First order logic allows one to state properties of concepts but it does not generally allow us to define concepts. For example, suppose we want to describe the concept of a finite set. We can write a formal expression stating that\" a set is finite if it is either empty or can be derived by adding a single element to some other finite set\". Unfortunately this statement does not uniquely determine the concept---it is also true that\" a set is countable if it is either empty or can be derived by adding a single element to some other countable set\". Statements true of a concept often fail to uniquely specify them. Finiteness is not definable in first order logic. The bag of marbles inference mentioned above implicitly relies on finiteness. Almost all program verfication problems involve concepts not definable in first order logic. Methods of simulating more expressive logics in first order logic are generally inferior in practice to systems specifically designed to go beyond first order logic.", "num_citations": "1\n", "authors": ["1576"]}
{"title": "Object conversion is type-preserving\n", "abstract": " Closure conversion is a program transformation widely used in modern compilers. For a variety of reasons it is desirable that compilation preserve types. We consider a closure conversion mapping e to O e] which is naive in the sense that O e] does not contain pack or unpack programming constructs. The main theorem states that, for this xed notion of closure conversion, any\\normal\" type system T for the lambda calculus can be mechanically converted to an object-oriented type system T'such that for any lambda term e we have that e is typable by T i O e] is typable by T'. Simple types, partial types, union types, intersection types, Hindley-Milner types and system F are all normal. The main theorem can also be interpreted as a formalization of the common claim that objects subsume procedures and that rst-order object-oriented programs are as expressive as the lambda calculus.", "num_citations": "1\n", "authors": ["1576"]}
{"title": "Variational Attribute Grammars for Computer Aided Design (Release 3.0)\n", "abstract": " This document describes a variational attribute grammar (VAG) design language and release 3.0 of the VAG implementation. VAG is a functional programming language speci cally designed for the rapid implementation of domain speci c CAD systems. VAG is intended to support the implementations of CAD systems in such diverse areas as mechanical, electrical, and software design. The main feature of the VAG language is a constraint based reasoning mechanisms to support the CAD user in analyzing partial designs in a wide variety of domains. Release 3.0 provides a justi cation mechanism| no such mechanisms were provided in earlier releases.", "num_citations": "1\n", "authors": ["1576"]}
{"title": "The Ontic Language\n", "abstract": " Ontic is a higher order formal speci cation language designed for expressing formal concepts as clearly and concisely as possible. The consiceness and clarity is achieved through the use of nondeterminism. In a nondeterministic language each term has a set of possible values. This improves conciseness in a variety of ways. First, since terms denotes sets, nondeterminism allows terms to be used as types. Second, since each term has a set of possible values it is natural for a term not to have any values. This provides a natural representation of partial functions. Third, nondeterminism allows a single recursive de nition and induction mechanism to be used for both data type de nition and recursive function de nition. Fourth, nondeterminism provides a clean integration of programming languages and set theory. Ontic has the expressive power of ZFC set theory. This paper presents the syntax and semantics of Ontic together with a large collection of examples of Ontic de nitions. The examples are essential in demonstrating the conciseness and clarity of Ontic de nitions.", "num_citations": "1\n", "authors": ["1576"]}
{"title": "Boolean classes\n", "abstract": " Object-oriented programming languages all involve the notions of class and object. We extend the notion of class so that any Boolean combination of classes is also a class. Boolean classes allow greater precision and conciseness in naming the class of objects governed by a particular method. A class can be viewed as a predicate which is either true or false of any given object. Unlike predicates however classes have an inheritance hierarchy which is known at compile time. Boolean classes extend the notion of class, making classes more like predicates, while preserving the compile time computable inheritance hierarchy.Acknowledgments. This paper describes research done at the Arti\ufb01cial Intelligence Laboratory at the Massachusetts Institute of Technology, supported by the Advanced Research Projects Agency of the Department of Defense under Of\ufb01ce of Naval Research contract N00014\u201486\u2014K\u20140180\u00a0\u2026", "num_citations": "1\n", "authors": ["1576"]}