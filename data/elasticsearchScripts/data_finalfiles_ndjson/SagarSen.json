{"title": "Test case prioritization for continuous regression testing: An industrial case study\n", "abstract": " Regression testing in continuous integration environment is bounded by tight time constraints. To satisfy time constraints and achieve testing goals, test cases must be efficiently ordered in execution. Prioritization techniques are commonly used to order test cases to reflect their importance according to one or more criteria. Reduced time to test or high fault detection rate are such important criteria. In this paper, we present a case study of a test prioritization approach ROCKET (Prioritization for Continuous Regression Testing) to improve the efficiency of continuous regression testing of industrial video conferencing software. ROCKET orders test cases based on historical failure data, test execution time and domain-specific heuristics. It uses a weighted function to compute test priority. The weights are higher if tests uncover regression faults in recent iterations of software testing and reduce time to detection of faults\u00a0\u2026", "num_citations": "143\n", "authors": ["1524"]}
{"title": "A review of traceability research at the requirements engineering conferencere@21\n", "abstract": " Traceability between development artefacts and mainly from and to requirements plays a major role in system lifecycle, supporting activities such as system validation, change impact analysis, and regulation compliance. Many researchers have been working on this topic and have published their work throughout the editions of the Requirements Engineering Conference. This paper aims to analyse the research on traceability published in the past 20 years of this conference and to provide insights into its contribution to the traceability area. We have selected and reviewed 70 papers in the proceedings of the conference and summarised several aspects of traceability that have been addressed and by whom. The paper also discusses the evolution of the topic at the conference, compares the results with those reported in other publications, and proposes aspects on which further research should be conducted.", "num_citations": "78\n", "authors": ["1524"]}
{"title": "Clustering of relational data containing noise and outliers\n", "abstract": " The concept of noise clustering algorithm is applied to several fuzzy relational data clustering algorithms to make them more robust against noise and outliers. The methods considered include techniques proposed by Roubens (1978), Hathaway et al. (1994) and FANNY by Kaufman and Rouseeuw (1990). A new fuzzy relational data clustering (FRC) algorithm is proposed through generalization of FANNY. The FRC algorithm is shown to have the same objective functional as the relational fuzzy c-means algorithm. However, through use of direct objective function minimization based on the Lagrangian multiplier technique, the necessary conditions for minimization are derived without imposition of the restriction that the relational data is derived from Euclidean measure of distance from object data. Robustness of the new algorithm is demonstrated through several examples.", "num_citations": "73\n", "authors": ["1524"]}
{"title": "Practical pairwise testing for software product lines\n", "abstract": " One key challenge for software product lines is efficiently managing variability throughout their lifecycle. In this paper, we address the problem of variability in software product lines testing. We (1) identify a set of issues that must be addressed to make software product line testing work in practice and (2) provide a framework that combines a set of techniques to solve these issues. The framework integrates feature modelling, combinatorial interaction testing and constraint programming techniques. First, we extract variability in a software product line as a feature model with specified feature interdependencies. We then employ an algorithm that generates a minimal set of valid test cases covering all 2-way feature interactions for a given time interval. Furthermore, we evaluate the framework on an industrial SPL and show that using the framework saves time and provides better test coverage. In particular, our\u00a0\u2026", "num_citations": "62\n", "authors": ["1524"]}
{"title": "Chemical equation balancing: An integer programming approach\n", "abstract": " Presented here is an integer linear program (ILP) formulation for automatic balancing of a chemical equation. Also described is a integer nonlinear programming (INP) algorithm for balancing. This special algorithm is polynomial time O(n3), unlike the ILP approach, and uses the widely available conventional floating-point arithmetic, obviating the need for both rational arithmetic and multiple modulus residue arithmetic. The rational arithmetic is unsuitable due to intermediate number growth, while the residue arithmetic suffers from the lack of a priori knowledge of the set of prime bases that avoids a possible failure due to division by zero. Further, unlike the floating point arithmetic, both arithmetics are not built-in/standard and hence additional programming effort is needed. The INP algorithm has been tested on several typical chemical equations and found to be very successful for most problems in our extensive\u00a0\u2026", "num_citations": "36\n", "authors": ["1524"]}
{"title": "Titan: Test suite optimization for highly configurable software\n", "abstract": " Exhaustive testing of highly configurable software developed in continuous integration is rarely feasible in practice due to the configuration space of exponential size on the one hand, and strict time constraints on the other. This entails using selective testing techniques to determine the most failure-inducing test cases, conforming to highly-constrained time budget. These challenges have been well recognized by researchers, such that many different techniques have been proposed. In practice, however, there is a lack of efficient tools able to reduce high testing effort, without compromising software quality. In this paper we propose a test suite optimization technology TITAN, which increases the time-and cost-efficiency of testing highly configurable software developed in continuous integration. The technology implements practical test prioritization and minimization techniques, and provides test traceability and\u00a0\u2026", "num_citations": "19\n", "authors": ["1524"]}
{"title": "Protecting privacy in large datasets\u2014first we assess the risk; then we fuzzy the data\n", "abstract": " Background: Privacy of information is an increasing concern with the availability of large amounts of data from many individuals. Even when access to data is heavily controlled, and the data shared with researchers contain no personal identifying information, there is a possibility of reidentifying individuals. To avoid reidentification, several anonymization protocols are available. These include categorizing variables into broader categories to ensure more than one individual in each category, such as k-anonymization, as well as protocols aimed at adding noise to the data. However, data custodians rarely assess reidentification risks.Methods: We assessed the reidentification risk of a large realistic dataset based on screening data from over 5 million records on 0.9 million women in the Norwegian Cervical Cancer Screening Program, before and after we used old and new techniques of adding noise (fuzzification) of\u00a0\u2026", "num_citations": "13\n", "authors": ["1524"]}
{"title": "DevOps improvements for reduced cycle times with integrated test optimizations for continuous integration\n", "abstract": " DevOps, as a growing development practice that aims to enable faster development and efficient deployment of applications without compromising on quality, is often hampered by long cycle times. One contributing factor to long cycle times in DevOps is long build time. Automated testing in continuous integration is one of the build stages that is highly prone to long run-time due to software complexity and evolution, and inefficient due to unoptimized testing approaches. To be cost-effective, testing in continuous integration needs to use only a fast-running set of comprehensive tests that are able to ensure the level of quality needed for deployment to production. Known approaches use time-aware test selection methods to improve time-efficiency of continuous integration testing by providing optimized combinations and order of tests with respect to decreased run-time. However, focusing on time-efficiency as the sole\u00a0\u2026", "num_citations": "12\n", "authors": ["1524"]}
{"title": "Tailoring optical properties of TiO2-Cr co-sputtered films using swift heavy ions\n", "abstract": " Effect of 100\u202fMeV Au7+ ion irradiation on structure and optical properties of Cr-doped TiO2 films has been studied using X-ray photoelectron spectroscopy, soft X-ray absorption spectroscopy, UV-Visible spectroscopy, X-ray reflectivity, and atomic force microscopy. X-ray reflectivity measurement implied that film thickness reduces as a function of ion fluence while surface roughness increases. The variation in surface roughness is well correlated with AFM results. Ion irradiation decreases the band gap energy of the film. Swift heavy ion irradiation enhances the oxygen vacancies in the film, and the extra electrons in the vacancies act as donor-like states. In valence band spectrum, there is a shift in the Ti3d peak towards lower energies and the shift is equivalent to the band gap energy obtained from UV spectrum. Evidence for band bending is also provided by the corresponding Ti XPS peak which exhibits a shift\u00a0\u2026", "num_citations": "12\n", "authors": ["1524"]}
{"title": "FightHPV: design and evaluation of a mobile game to raise awareness about human papillomavirus and nudge people to take action against cervical cancer\n", "abstract": " Background: Human papillomavirus (HPV) is the most common sexually transmitted infection globally. High-risk HPV types can cause cervical cancer, other anogenital cancer, and oropharyngeal cancer; low-risk HPV types can cause genital warts. Cervical cancer is highly preventable through HPV vaccination and screening; however, a lack of awareness and knowledge of HPV and these preventive strategies represents an important barrier to reducing the burden of the disease. The rapid development and widespread use of mobile technologies in the last few years present an opportunity to overcome this lack of knowledge and create new, effective, and modern health communication strategies.Objective: This study aimed to describe the development of a mobile app called FightHPV, a game-based learning tool that educates mobile technology users about HPV, the disease risks associated with HPV infection, and existing preventive methods.Methods: The first version of FightHPV was improved in a design-development-evaluation loop, which incorporated feedback from a beta testing study of 40 participants, a first focus group of 6 participants aged between 40 and 50 years and a second focus group of 23 participants aged between 16 and 18 years. Gameplay data from the beta testing study were collected using Google Analytics (Google), whereas feedback from focus groups was evaluated qualitatively. Of the 29 focus group participants, 26 returned self-administered questionnaires. HPV knowledge before and after playing the game was evaluated in the 22 participants from the second focus group who returned a questionnaire.Results\u00a0\u2026", "num_citations": "11\n", "authors": ["1524"]}
{"title": "Testing a data-intensive system with generated data interactions\n", "abstract": " Testing data-intensive systems is paramount to increase our reliance on e-governance services. An incorrectly computed tax can have catastrophic consequences in terms of public image. Testers at Norwegian Customs and Excise reveal that faults occur from interactions between database features such as field values. Taxation rules, for example, are triggered due to an interaction between 10,000 items, 88 country groups, and 934 tax codes. There are about 12.9 trillion 3-wise interactions. Finding interactions to uncover specific faults is like finding a needle in a haystack. Can we surgically generate a test database for interactions that interest testers? We address this question with a methodology and tool Faktum to automatically populate a test database that covers all T-wise interactions for selected features. Faktum generates a constraint model of interactions in Alloy and solves it using a divide-and\u00a0\u2026", "num_citations": "11\n", "authors": ["1524"]}
{"title": "Multi-domain physical system modeling and control based on meta-modeling and graph rewriting\n", "abstract": " A methodology is presented which enables the specification and synthesis of software tools to aid in plant and controller modeling for multi-domain (electrical, mechanical, hydraulic, and thermal) physical systems. The methodology is based on meta-modeling and graph rewriting. The plant is modeled in a domain-specific formalism called the Real World Visual Model (RWVM). Such a model is successively transformed to an Idealized Physical Model (IPM), to an Acausal Bond Graph (ABG), and finally to a Causal Bond Graph (CBG). A Modelica (www.modelica.org) model, consisting of a Causal (algebraic and differential equation) Block Diagram (CBD), is generated from the CBG. All transformations are explicitly modeled using Graph Grammars. A PID controller model, specified in Modelica as a CBD is subsequently integrated with the plant model. AToM 3  (atom3.cs.mcgill.ca), A Tool for Multi-formalism and Meta\u00a0\u2026", "num_citations": "10\n", "authors": ["1524"]}
{"title": "Evaluating Reconfiguration Impact in Self-Adaptive Systems--An Approach Based on Combinatorial Interaction Testing\n", "abstract": " Self-adaptive software adapts its behavior to the operational context via automatic run-time reconfiguration of software components. Particular reconfigurations may negatively affect the system Quality of Service (QoS), and therefore their impact over the system performance needs to be thoroughly evaluated. In this paper, we present an approach, based on Combinatorial Interaction Testing (CIT), that generates a sequence of configurations aimed at evaluating the extent to which reconfigurations affect the system QoS. Specifically, we transform a Classification Tree Models (CTM) of the configurations domain to a Constraint Satisfaction Problem (CSP) in ALLOY, whose solution is a sequence of reconfigurations achieving T-wise coverage between system features, and R-wise coverage between configurations in the sequence. The resolution of the CSP is performed by an incremental growth algorithm that divides the\u00a0\u2026", "num_citations": "9\n", "authors": ["1524"]}
{"title": "Constraint-based verification of a mobile app game designed for nudging people to attend cancer screening\n", "abstract": " In Norway, cervical cancer prevention involves the participation of as many eligible women aged 25-69 years as possible. However, reaching and inviting every eligible women to attend cervical cancer screening and HPV vaccination is difficult. Using social nudging and gamification in modern means of communication can encourage the participation of unscreened people. Simula Research Laboratory together with the Cancer Registry of Norway have developed FightHPV, a mobile app game intended to inform adolescent and eligible women about cervical cancer screening and HPV vaccination while they play and, to facilitate their further participation to prevention campaigns. However, game design and health information transfer can be hard to reconcile, as the design of each game episode is more guided by the release of information than gameplay and playing difficulty. In this paper, we propose a constraint-based model of FightHPV to evaluate the difficulty of each episode and to help the game designer in improving the player experience. This approach is relevant to facilitate social nudging of eligible women to participate to cervical cancer screening and HPV vaccination, as shown by the initial deployment of FightHPV and tests performed in focus groups. The design of this mobile app can thus be regarded as a new application case of Artificial Intelligence techniques such as gamification and constraint programming.", "num_citations": "8\n", "authors": ["1524"]}
{"title": "Modeling and verifying combinatorial interactions to test data intensive systems: Experience at the norwegian customs directorate\n", "abstract": " Data-intensive systems in e-governance collect and process data to ensure conformance to a set of business rules. Testers meticulously verify data in test databases, extracted from different steps of a live production stream , for correct application of business rules. We simplify the process by allowing testers to model a test domain on a relational database and automatically generate test cases representing data interactions satisfying combinatorial interaction coverage criteria. This paper also introduces test cases with self-referential interactions, which is a necessity in real-world databases. We verify these test cases using our human-in-the-loop tool, Depict. Depict, with expert assistance, generates complex SQL queries for test cases and produces a visual report of test case satisfaction. We apply the approach to two scenarios: 1) simplify and optimize a periodic archiving operation and 2) verify fault codes within the\u00a0\u2026", "num_citations": "8\n", "authors": ["1524"]}
{"title": "Crowdpinion: Motivating People to Share their Momentary Opinion.\n", "abstract": " Many interesting social studies can be done by asking people about what they think, feel or experience at the moment when certain events occur in their daily lives. These studies can be conducted based on the event-contingent protocol of the Experience Sampling Method. We have implemented this protocol in Crowdpinion\u2013our software tool consisting of a web panel where the researchers can set up and control their studies and a mobile app for the responders. In order to extend the users\u2019 motivation beyond the will to contribute to research, we have applied some gamification elements based on fostering curiosity by gradually revealing the big picture from the overall study. In this paper we describe the concepts of Crowdpinion as a research tool, our approach to gamification, how we tested it in the beta version and present plans for future improvements", "num_citations": "6\n", "authors": ["1524"]}
{"title": "Girgit: a dynamically adaptive vision system for scene understanding\n", "abstract": " Modern vision systems must run in continually changing contexts. For example, a system to detect vandalism in train stations must function during the day and at night. The vision components for acquisition and detection used during daytime may not be the same as those used at night. The system must adapt to a context by replacing running components such as image acquisition from color to infra-red. This adaptation must be dynamic with detection of context, decision on change in system configuration, followed by the seamless execution of the new configuration. All this must occur while minimizing the impact of dynamic change on validity of detection and loss in performance. We present Girgit, a context-aware vision system for scene understanding, that dynamically orchestrates a set of components. A component encapsulates a vision-related algorithm such as from the OpenCV library. Girgit inherently\u00a0\u2026", "num_citations": "6\n", "authors": ["1524"]}
{"title": "Generating test sequences to assess the performance of elastic cloud-based systems\n", "abstract": " Elasticity is one of the main features of cloud-based systems (CBSs), where elastic adaptations, such as those to deal with scaling in or scaling out of computational resources, help meet performance requirements under varying workload. There is an industrial need to find configurations of elastic adaptations and workload that could lead to degradation of performance in a CBS, serving possibly millions of users. However, the potentially great number of such configurations poses a challenge: executing and verifying all of them on the cloud can be prohibitively expensive in both, time and cost. We present an approach to model elasticity adaptation due to workload changes as a classification tree model and consequently generate short test sequences of configurations that cover all T-wise interactions between parameters in the model. These test sequences, when executed, help us assess the performance of elastic\u00a0\u2026", "num_citations": "5\n", "authors": ["1524"]}
{"title": "Modelling data interaction requirements: A position paper\n", "abstract": " Data-intensive information systems constitute the backbone of e-commerce and e-governance services running worldwide. Structured data is a central artefact in these information systems. Requirements for structure in data are typically modelled in a database schema. However, information system behaviour is often a function of interactions that cross-cut database features such as field values in different tables. For instance, consultants at the Norwegian Customs and Excise reveal that taxation rules are triggered due to data interactions between 10,000 items, 88 country groups, and 934 tax codes. There are about 12.9 trillion possible three-wise interactions of which only about 220,000 interactions are used in reality as customs rules. Therefore, we ask, how can we model data interaction requirements to further bound the input domain of an information system? In this position paper, we address this question by\u00a0\u2026", "num_citations": "5\n", "authors": ["1524"]}
{"title": "Certus: an organizational effort towards research-based innovation in software verification and validation\n", "abstract": " What is gratifying to a software engineering researcher? Three of many possible answers to this question are (a) the intellectual exercise in developing/disseminating approaches that address emerging and existing challenges, (b) recognition from impact in a community of researchers and (c) widespread use of novel ideas, including software, in the society at large leading to enhancement of human ability and job creation. A culmination of these sources requires an organizational effort. This article presents a detailed account of a research-based innovation centre, Certus, to facilitate such a culmination for software engineering researchers. Certus has established a body of knowledge, methods and tools for the validation and verification of software systems in the Norwegian private and public sector. Certus works in close cooperation with five founding user partners and is hosted by the Simula Research\u00a0\u2026", "num_citations": "4\n", "authors": ["1524"]}
{"title": "Experience report: Verifying data interaction coverage to improve testing of data-intensive systems: The Norwegian customs and excise case study\n", "abstract": " Testing data-intensive systems is paramount to increase our reliance on information processed in e-governance, scientific/ medical research, and social networks. A common practice in the industrial testing process is to use test databases copied from live production streams to test functionality of complex database applications that manage well-formedness of data and its adherence to business rules in these systems. This practice is often based on the assumption that the test database adequately covers realistic scenarios to test, hopefully, all functionality in these applications. There is a need to systematically evaluate this assumption. We present a tool-supported method to model realistic scenarios and verify whether copied test databases actually cover them and consequently facilitate adequate testing. We conceptualize realistic scenarios as data interactions between fields cross-cutting a complex database\u00a0\u2026", "num_citations": "4\n", "authors": ["1524"]}
{"title": "Using UML/MARTE to support performance tuning and stress testing in real-time systems\n", "abstract": " Real-time embedded systems (RTESs) operating in safety-critical domains have to satisfy strict performance requirements in terms of task deadlines, response time, and CPU usage. Two of the main factors affecting the satisfaction of these requirements are the configuration parameters regulating how the system interacts with hardware devices, and the external events triggering the system tasks. In particular, it is necessary to carefully tune the parameters in order to ensure a satisfactory trade-off between responsiveness and usage of computational resources, and also to stress test the system with worst-case inputs likely to violate the requirements. Performance tuning and stress testing are usually manual, time-consuming, and error-prone processes, because the system parameters and input values range in a large domain, and their impact over performance is hard to predict without executing the system\u00a0\u2026", "num_citations": "3\n", "authors": ["1524"]}
{"title": "Scientific Hangman: Gamifying Scientific Evidence for General Public.\n", "abstract": " Governmental and private funding for research in many fields has resulted in a significant body of scientific evidence. Scientific evidence or content is made available in the form of thousands of articles communicated via digital libraries. This evidence is principally used by researchers, students and on occasions for societal impact such as commercial exploitation and popular science communication. How can we gamify communicating a large amount of scientific evidence to the general public? This is the question that intrigues us. We present the game of Scientific Hangman, based on the traditional game of hangman, to communicate scientific research in a fun manner. The puzzles in our game are based on automatic summarization of scientific article abstracts. Players play the game in an attempt to guess a word given a clue such as a paper abstract. Our first prototype, was evaluated on a focus group at the Cancer Registry of Norway by communicating information from invitation letters in cervical cancer screening. We also evaluated a second prototype of the game to have feedback on design improvements resulted from the first prototype.", "num_citations": "3\n", "authors": ["1524"]}
{"title": "Automatic effective model discovery\n", "abstract": " Les d\u00e9couvertes scientifiques aboutissent souvent \u00e0 la repr\u00e9sentation de structures dans l\u2019environnement sous forme de graphes d\u2019objets. Par exemple, certains r\u00e9seaux de r\u00e9actions biologiques visent \u00e0 repr\u00e9senter les processus vitaux tels que la consommation de gras ou l\u2019activation/d\u00e9sactivation des g\u00eanes. L\u2019extraction de connaissances \u00e0 partir d'exp\u00e9rimentations, l'analyse des donn\u00e9es et l\u2019inf\u00e9rence conduisent \u00e0 la d\u00e9couverte de structures effectives dans la nature. Ce processus de d\u00e9couverte scientifiques peut-il \u00eatre automatis\u00e9 au moyen de diverses sources de connaissances? Dans cette th\u00e8se, nous abordons la m\u00eame question dans le contexte contemporain de l'ing\u00e9nierie dirig\u00e9e par les mod\u00e8les (IDM) de syst\u00e8mes logiciels complexes. L\u2019IDM vise \u00e0 acc\u00e9l\u00e9rer la cr\u00e9ation de logiciels complexes en utilisant de artefacts de base appel\u00e9s mod\u00e8les. Tout comme le processus de d\u00e9couverte de structures effectives en science un modeleur cr\u00e9e dans un domaine de mod\u00e9lisation des mod\u00e8les effectifs, qui repr\u00e9sente des artefacts logiciels utiles. Dans cette th\u00e8se, nous consid\u00e9rons deux domaines de mod\u00e9lisation: m\u00e9tamod\u00e8les pour la mod\u00e9lisation des langages et des feature diagrams pour les lignes de produits (LPL) logiciels. Pouvons-nous automatiser la d\u00e9couverte de mod\u00e8les effectifs dans un domaine de mod\u00e9lisation? Le principal d\u00e9fi dans la d\u00e9couverte est la g\u00e9n\u00e9ration automatique de mod\u00e8les. Les mod\u00e8les sont des graphes d\u2019objets interconnect\u00e9s avec des contraintes sur leur structure et les donn\u00e9es qu'ils contiennent. Ces contraintes sont impos\u00e9es par un domaine de mod\u00e9lisation et des sources h\u00e9t\u00e9rog\u00e8nes de connaissances\u00a0\u2026", "num_citations": "3\n", "authors": ["1524"]}
{"title": "A shrinking-rectangle randomized algorithm with interpolation for a complex zero of a function\n", "abstract": " A polynomial-time deterministic randomised algorithm is described to compute a zero of a complex/real polynomial or a complex/real transcendental function in a complex plane. The algorithm starts with a specified rectangle enclosing a complex zero, shrinks it successively by at least 50% in each iteration somewhat like a two-dimensional bisection, and then a single application of linear two-variable interpolation in the highly shrunk rectangle provides the required zero. A parallel Implementation of this algorithm is discussed while its sequential and parallel computational complexities as well as its space complexity are presented. The algorithm is found to be reasonably good for zero clusters and also for multiple zeros. This method can be extended to minimize globally a polynomial or a transcendental function of several variables without resorting to the computation of its partial derivatives and can be used along with the deflation of the polynomial or with different specified initial rectangle", "num_citations": "3\n", "authors": ["1524"]}
{"title": "Linear program solver: evolutionary approach\n", "abstract": " An evolution-inspired linear program (LP) solver is presented. The solver has been called\" evolutionary\" or\" genetic\" although the actual resemblance to natural genetics was minimal. The paper focuses on applying such an evolutionary approach or, equivalently, evolutionary algorithm (EA) to solve linear programs (LPs) Maximize c''x'subject to A'x'sb', k'sx'sk, wherek'=[k's k'a.. k'n'and k=[k, ka.. kn]'(2 k') are n-vectors of real numbers and A'= \u017fa'y) is an mxn real matrix. The EA is inherently highly parallel and is readily implementable on a parallel machine and needs no slack/surplus (for conversion of inequalities to equations) and artificial variables (for consistency check). A sequential implementation of the algorithm is easy but cannot compete with the popular deterministic exterior/interior-point methods in terms of computing resource requirements. A sequential MATLAB version of the solver is appended. A parallel version, however, can possibly be competitive and is relatively easy to comprehend. The implementation of this algorithm, much unlike that of deterministic procedures, to solve nonlinear programs (NLPs) as well as integer NLPs and integer LPs is straightforward.", "num_citations": "3\n", "authors": ["1524"]}
{"title": "Yolo4Apnea: real-time detection of obstructive sleep apnea\n", "abstract": " Obstructive sleep apnea is a serious sleep disorder that affects an estimated one billion adults worldwide. It causes breathing to repeatedly stop and start during sleep which over years increases the risk of hypertension, heart disease, stroke, Alzheimer\u2019s, and cancer. In this demo, we present Yolo4Apnea a deep learning system extending You Only Look Once (Yolo) system to detect sleep apnea events from abdominal breathing patterns in real-time enabling immediate awareness and action. Abdominal breathing is measured using a respiratory inductance plethysmography sensor worn around the stomach. The source code is available at https://github. com/simula-vias/Yolo4Apnea", "num_citations": "2\n", "authors": ["1524"]}
{"title": "Opening the software engineering toolbox for the assessment of trustworthy ai\n", "abstract": " Trustworthiness is a central requirement for the acceptance and success of human-centered artificial intelligence (AI). To deem an AI system as trustworthy, it is crucial to assess its behaviour and characteristics against a gold standard of Trustworthy AI, consisting of guidelines, requirements, or only expectations. While AI systems are highly complex, their implementations are still based on software. The software engineering community has a long-established toolbox for the assessment of software systems, especially in the context of software testing. In this paper, we argue for the application of software engineering and testing practices for the assessment of trustworthy AI. We make the connection between the seven key requirements as defined by the European Commission's AI high-level expert group and established procedures from software engineering and raise questions for future work.", "num_citations": "2\n", "authors": ["1524"]}
{"title": "DevOps Enhancement with Continuous Test Optimization.\n", "abstract": " Growing evidence suggests the DevOps approach enables faster development and deployment, and easier maintenance of applications. Still, the efficiency of DevOps is constrained by long cycle times. This paper presents the approach for improving time-efficiency in DevOps, and in particular continuous integration testing, using continuous test optimization. The approach uses test redundancy analysis to discover test overlap with respect to feature interaction coverage, and based on detected redundancy to reduce the size of a test suite. Smallersize test suites execute faster and enable shorter test cycles, which further enables shorter release cycles. The approach has been experimentally evaluated using an industrial case study, against three metrics: industry practice of test selection for continuous integration testing, retest-all approach, and random test selection. The results suggest that the proposed test redundancy detection and reduction efficiently reduces test cycles in CI compared to industry practice and retest-all approach, and improves faultdetection effectiveness compared to random test selection1.", "num_citations": "2\n", "authors": ["1524"]}
{"title": "Portinari: a data exploration tool to personalize cervical cancer screening\n", "abstract": " Socio-technical systems play an important role in public health screening programs to prevent cancer. Cervical cancer incidence has significantly decreased in countries that developed systems for organized screening engaging medical practitioners, laboratories and patients. The system automatically identifies individuals at risk of developing the disease and invites them for a screening exam or a follow-up exam conducted by medical professionals. A triage algorithm in the system aims to reduce unnecessary screening exams for individuals at low-risk while detecting and treating individuals at high-risk. Despite the general success of screening, the triage algorithm is a one-sizefits all approach that is not personalized to a patient. This can easily be observed in historical data from screening exams. Often patients rely on personal factors to determine that they are either at high risk or not at risk at all and take action\u00a0\u2026", "num_citations": "2\n", "authors": ["1524"]}
{"title": "Detecting and Reducing Redundancy in Software Testing for Highly Configurable Systems\n", "abstract": " While redundancy in software development has beenused as a fault tolerance mechanism that can be useful forincreasing software quality, redundancy in test artifacts leads to decreased testing performance and increased effort, mainly due to repetitive testing and high test maintenance costs. The effect of redundancy is especially accentuated in testing software for highly configurable systems, which contain a large number of configurable mutually interacting options that overlap across a test suite and introduce unnecessary redundancy. In this paper we propose a methodology for detecting and reducing redundancy in test suits for highly configurable software. We use equivalence partitioning to segregate test cases covering the same sets of feature interactions, then identify partitions with multiple test cases, and finally we remove those test cases that do not have unique partitions. We evaluate the approach in the\u00a0\u2026", "num_citations": "2\n", "authors": ["1524"]}
{"title": "Traceability research at the requirements engineering conference: Results and extracted data\n", "abstract": " Traceability between development artifacts and mainly from and to requirements plays a major role in system lifecycle, supporting activities such as validation, change impact analysis, and regulation compliance. Many researchers have been working on this topic and have published their work throughout editions of the Requirements Engineering Conference. This paper aims to analyze the research on traceability published at this conference and provides insights into its contribution to the traceability area. For this purpose, papers on traceability in the proceedings of the conference have been reviewed for determination of (1) the traceability topics studied,(2) the challenges addressed,(3) the contributions made,(4) the tools features developed to support traceability,(5) the types of systems considered,(6) the types of artifacts traced,(7) the empirical methods used, and (8) the leaders in production. The paper also discusses the evolution of the topic at the conference, compares the results with those reported in other publications, and proposes aspects on which further research should be conducted.", "num_citations": "2\n", "authors": ["1524"]}
{"title": "Creation of an Wearable Startup: From a Laboratory Incubator to a Revenue Generating Business\n", "abstract": " The need to understand signals given by our own body is of great interest to most human beings. This quest for self-knowledge is both shared by academic researchers and businesses who want to bring value to consumers in the society. This paper presents a story of how a software engineering researcher who collaborated with hardware engineers and entrepreneurs in an incubator, Simula Garage, hosted by Simula Research Laboratory to create a wearable startup called Sweetzpot. Sweetzpot developed a respiratory inductance plethysomography sensor called Flow to measure breathing signals from ribcage and/or abdominal movements. The team grew to consist of software engineers, students of machine learning and physics, an industrial/interaction designer, a hardware engineer, a lawyer, and an accountant in addition to external collaborators. We present the sequence of events that led to creation and\u00a0\u2026", "num_citations": "1\n", "authors": ["1524"]}
{"title": "Good Practices in Aligning Software Engineering Research and Industry Practice\n", "abstract": " There is a long-standing challenge to narrow the gap between software engineering research and industry practice, to align their interests and realize true synergies between the two communities. Some difficulties to this challenge include mismatched agendas, priorities and expectations from the research collaboration on both sides. To overcome these difficulties, an initial step is to gain a clearer understanding of collaboration challenges from both perspectives. With this goal in mind, we organized the 5th International Workshop on Software Engineering Research and Industrial Practice, collocated with the International Conference on Software Engineering 2018. The workshop featured two keynote talks, one from industry and one from academia, followed by paper presentations and a round-table discussion session. Here we summarize experiences shared by the keynotes from industry and academia, along with\u00a0\u2026", "num_citations": "1\n", "authors": ["1524"]}
{"title": "Evolution of magnetic anisotropy by O ion implantation in Fe/Co/Fe trilayar\n", "abstract": " Implantation of O ions into Fe/Co/Fe thin film has been performed to modify its structural and magnetic properties. X-ray reflectivity and atomic force microscopy measurements indicate that after implantation, interface and surface roughnesses have been increased. X-ray reflectivity measurement of the implanted sample at higher fluence shows intermixing at the interface of Fe/Co and Co/Fe layer. 1\u202f\u00d7\u202f1016 ions/cm2 O-ions reduces the coercivity of film from 50\u202fOe to 31\u202fOe. After 5\u202f\u00d7\u202f1016 ions/cm2 O ion fluence, film exhibits two fold magnetic anisotropy and the magnetic moment of the film has been increased. Ion implantation induces broadening in the M\u00f6ssbauer spectrum suggest that the FeCoO and FeCo component at the interface have been formed, which enhances the magnetic moment of the film.", "num_citations": "1\n", "authors": ["1524"]}
{"title": "Certus: glimpses of a centre for research-based innovation in software verification and validation\n", "abstract": " What is gratifying to a software engineering researcher? Three of many possible answers to this question are (a) the in-tellectual exercise in developing/disseminating approaches that address novel challenges (b) recognition from impact in a community of researchers (c) widespread use of novel ideas, including software, in the society at large leading to enhancement of human ability and job creation. A culmina-tion of these sources requires an organizational effort. This article presents glimpses of a research-based innovation cen-tre, Certus, to facilitate such a culmination for software engi-neering researchers. Certus has established a body of knowl-edge, methods and tools for the validation and verication (V&V) of software systems in the local Norwegian indus-try. Certus works in close cooperation with five founding user partners and is hosted by the Simula Research Labo-ratory. We present the organizational\u00a0\u2026", "num_citations": "1\n", "authors": ["1524"]}
{"title": "D\u00e9couverte automatique de mod\u00e8les effectifs\n", "abstract": " Scientific discovery often culminates into representing structure in nature as networks (graphs) of objects. For instance, certain biological reaction networks aim to represent living processes such as burning fat or switching genes on/off. Knowledge from experiments, data analysis and mental tacit lead to the discovery of such effective structures in nature. Can this process of scientific discovery using various sources of knowledge be automated? In this thesis, we address the same question in the contemporary context of model-driven engineering (MDE) of complex software systems. MDE aims to grease the wheels of complex software creation using first class artifacts called models. Very much like the process of effective structure discovery in science a modeler creates effective models, representing useful software artifacts, in a modelling domain. In this thesis, we consider two such modelling domains: metamodels for modelling languages and feature diagrams for Software Product Lines (SPLs). Can we automate effective model discovery in a modelling domain? The central challenge in discovery is the automatic generation of models. Models are graphs of inter-connected objects with constraints on their structure and the data contained in them. These constraints are enforced by a modelling domain and heterogeneous sources of knowledge including several well-formedness rules. How can we automatically generate models that simultaneously satisfy these constraints? In this thesis, we present a model-driven framework to answer this question. The framework for automatic model discovery uses heterogeneous sources of knowledge to first\u00a0\u2026", "num_citations": "1\n", "authors": ["1524"]}