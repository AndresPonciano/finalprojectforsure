{"title": "DRPM: dynamic speed control for power management in server class disks\n", "abstract": " A large portion of the power budget in server environments goes into the I/O subsystem - the disk array in particular. Traditional approaches to disk power management involve completely stopping the disk rotation, which can take a considerable amount of time, making them less useful in cases where idle times between disk requests may not be long enough to outweigh the overheads. We present a new approach called DRPM to modulate disk speed (RPM) dynamically, and gives a practical implementation to exploit this mechanism. Extensive simulations with different workload and hardware parameters show that DRPM can provide significant energy savings without compromising much on performance. We also discuss practical issues when implementing DRPM on server disks.", "num_citations": "541\n", "authors": ["528"]}
{"title": "Relaxing non-volatility for fast and energy-efficient STT-RAM caches\n", "abstract": " Spin-Transfer Torque RAM (STT-RAM) is an emerging non-volatile memory technology that is a potential universal memory that could replace SRAM in processor caches. This paper presents a novel approach for redesigning STT-RAM memory cells to reduce the high dynamic energy and slow write latencies. We lower the retention time by reducing the planar area of the cell, thereby reducing the write current, which we then use with CACTI to design caches and memories. We simulate quad-core processor designs using a combination of SRAM- and STT-RAM-based caches. Since ultra-low retention STT-RAM may lose data, we also provide a preliminary evaluation for a simple, DRAM-style refresh policy. We found that a pure STT-RAM cache hierarchy provides the best energy efficiency, though a hybrid design of SRAM-based L1 caches with reduced-retention STT-RAM L2 and L3 caches eliminates\u00a0\u2026", "num_citations": "475\n", "authors": ["528"]}
{"title": "Using complete machine simulation for software power estimation: The softwatt approach\n", "abstract": " Power dissipation has become one of the most critical factors for the continued development of both high-end and low-end computer systems. We present a complete system power simulator, called SoftWatt, that models the CPU, memory hierarchy, and a low-power disk subsystem and quantifies the power behavior of both the application and operating system. This tool, built on top of the SimOS infrastructure, uses validated analytical energy models to identify the power hotspots in the system components, capture relative contributions of the user and kernel code to the system power profile, identify the power-hungry operating system services and characterize the variance in kernel power profile with respect to workload. Our results using Spec JVM98 benchmark suite emphasize the importance of complete system simulation to understand the power impact of architecture and operating system on application\u00a0\u2026", "num_citations": "293\n", "authors": ["528"]}
{"title": "Memory errors in modern systems: The good, the bad, and the ugly\n", "abstract": " Several recent publications have shown that hardware faults in the memory subsystem are commonplace. These faults are predicted to become more frequent in future systems that contain orders of magnitude more DRAM and SRAM than found in current memory subsystems. These memory subsystems will need to provide resilience techniques to tolerate these faults when deployed in high-performance computing systems and data centers containing tens of thousands of nodes. Therefore, it is critical to understand the efficacy of current hardware resilience techniques to determine whether they will be suitable for future systems. In this paper, we present a study of DRAM and SRAM faults and errors from the field. We use data from two leadership-class high-performance computer systems to analyze the reliability impact of hardware resilience schemes that are deployed in current systems. Our study has several\u00a0\u2026", "num_citations": "290\n", "authors": ["528"]}
{"title": "Feng shui of supercomputer memory positional effects in DRAM and SRAM faults\n", "abstract": " Several recent publications confirm that faults are common in high-performance computing systems. Therefore, further attention to the faults experienced by such computing systems is warranted. In this paper, we present a study of DRAM and SRAM faults in large high-performance computing systems. Our goal is to understand the factors that influence faults in production settings. We examine the impact of aging on DRAM, finding a marked shift from permanent to transient faults in the first two years of DRAM lifetime. We examine the impact of DRAM vendor, finding that fault rates vary by more than 4x among vendors. We examine the physical location of faults in a DRAM device and in a data center; contrary to prior studies, we find no correlations with either. Finally, we study the impact of altitude and rack placement on SRAM faults, finding that, as expected, altitude has a substantial impact on SRAM faults, and that\u00a0\u2026", "num_citations": "201\n", "authors": ["528"]}
{"title": "Dynamic prediction of architectural vulnerability from microarchitectural state\n", "abstract": " Transient faults due to particle strikes are a key challenge in microprocessor design. Driven by exponentially increasing transistor counts, per-chip faults are a growing burden. To protect against soft errors, redundancy techniques such as redundant multithreading (RMT) are often used. However, these techniques assume that the probability that a structural fault will result in a soft error (ie, the Architectural Vulnerability Factor (AVF)) is 100 percent, unnecessarily draining processor resources. Due to the high cost of redundancy, there have been efforts to throttle RMT at runtime. To date, these methods have not incorporated an AVF model and therefore tend to be ad hoc. Unfortunately, computing the AVF of complex microprocessor structures (eg, the ISQ) can be quite involved.", "num_citations": "162\n", "authors": ["528"]}
{"title": "ICR: In-Cache Replication for Enhancing Data Cache Reliability.\n", "abstract": " Processor caches already play a critical role in the performance of today\u2019s computer systems. At the same time, the data integrity of words coming out of the caches can have serious consequences on the ability of a program to execute correctly, or even to proceed. The integrity checks need to be performed in a time-sensitive manner to not slow down the execution when there are no errors as in the common case, and should not excessively increase the power budget of the caches which is already high. ECC and parity-based protection techniques in use today fall at either extremes in terms of compromising one criteria for another, ie, reliability for performance or vice-versa. This paper proposes a novel solution to this problem by allowing in-cache replication, wherein reliability can be enhanced without excessively slowing down cache accesses or requiring significant area cost increases. The mechanism is fairly power efficient in comparison to other alternatives as well. In particular, the solution replicates data that is in active use within the cache itself while evicting those that may not be needed in the near future. Our experiments show that a large fraction of the data read from the cache have replicas available with this optimization.", "num_citations": "160\n", "authors": ["528"]}
{"title": "GPU-Qin: A Methodology for Evaluating the Error Resilience of GPGPU Applications\n", "abstract": " While graphics processing units (GPUs) have gained wide adoption as accelerators for general-purpose applications (GPGPU), the end-to-end reliability implications of their use have not been quantified. Fault injection is a widely used method for evaluating the reliability of applications. However, building a fault injector for GPGPU applications is challenging due to their massive parallelism, which makes it difficult to achieve representativeness while being time-efficient. This paper makes three key contributions. First, it presents the design of a fault-injection methodology to evaluate end-to-end reliability properties of application kernels running on GPUs. Second, it introduces a fault-injection tool that uses real GPU hardware and offers a good balance between the representativeness and the efficiency of the fault injection experiments. Third, this paper characterizes the error resilience characteristics of twelve\u00a0\u2026", "num_citations": "140\n", "authors": ["528"]}
{"title": "Delivering on the promise of universal memory for spin-transfer torque RAM (STT-RAM)\n", "abstract": " Spin-Transfer Torque RAM (STT-RAM) has emerged as a potential candidate for Universal memory. However, there are two challenges to using STT-RAM in memory system design: (1) the intrinsic variation in the storage element, the Magnetic Tunnel Junction (MTJ), and (2) the high write energy. In this paper, we present a physically based thermal noise model for simulating the statistical variations of MTJs. We have implemented it in HSPICE and validated it against analytical results. We demonstrate its use in setting the write pulse width for a given write error rate. We then propose two write-energy reduction techniques. At the device level, we propose the use of a low-M S  ferromagnetic material that can reduce the write energy without sacrificing retention time. At the architecture level, we show that Invert Coding provides a 7% average reduction in the total write energy for the SPEC CPU2006 benchmark suite\u00a0\u2026", "num_citations": "126\n", "authors": ["528"]}
{"title": "Reducing disk power consumption in servers with DRPM\n", "abstract": " Although effective techniques exist for tackling disk power for laptops and workstations, applying them in a server environment presents a considerable challenge, especially under stringent performance requirements. Using a dynamic rotations per minute approach to speed control in server disk arrays can provide significant savings in I/O system power consumption without lessening performance.", "num_citations": "119\n", "authors": ["528"]}
{"title": "Differentiating the roles of IR measurement and simulation for power and temperature-aware design\n", "abstract": " In temperature-aware design, the presence or absence of a heatsink fundamentally changes the thermal behavior with important design implications. In recent years, chip-level infrared (IR) thermal imaging has been gaining popularity in studying thermal phenomena and thermal management, as well as reverse-engineering chip power consumption. Unfortunately, IR thermal imaging needs a peculiar cooling solution, which removes the heatsink and applies an IR-transparent liquid flow over the exposed bare die to carry away the dissipated heat. Because this cooling solution is drastically different from a normal thermal package, its thermal characteristics need to be closely examined. In this paper, we characterize the differences between two cooling configurations-forced air flow over a copper heatsink (AIR-SINK) and laminar oil flow over bare silicon (OIL-SILICON). For the comparison, we modify the HotSpot\u00a0\u2026", "num_citations": "99\n", "authors": ["528"]}
{"title": "Achieving exascale capabilities through heterogeneous computing\n", "abstract": " This article provides an overview of AMD's vision for exascale computing, and in particular, how heterogeneity will play a central role in realizing this vision. Exascale computing requires high levels of performance capabilities while staying within stringent power budgets. Using hardware optimized for specific functions is much more energy efficient than implementing those functions with general-purpose cores. However, there is a strong desire for supercomputer customers not to have to pay for custom components designed only for high-end high-performance computing systems. Therefore, high-volume GPU technology becomes a natural choice for energy-efficient data-parallel computing. To fully realize the GPU's capabilities, the authors envision exascale computing nodes that compose integrated CPUs and GPUs (that is, accelerated processing units), along with the hardware and software support to enable\u00a0\u2026", "num_citations": "92\n", "authors": ["528"]}
{"title": "Understanding the performance-temperature interactions in disk i/o of server workloads\n", "abstract": " This paper describes the first infrastructure for integrated studies of the performance and thermal behavior of storage systems. Using microbenchmarks running on this infrastructure, we first gain insight into how I/O characteristics can affect the temperature of disk drives. We use this analysis to identify the most promising, yet simple, \"knobs\" for temperature optimization of high speed disks, which can be implemented on existing disks. We then analyze the thermal profiles of real workloads that use such disk drives in their storage systems, pointing out which knobs are most useful for dynamic thermal management when pushing the performance envelope.", "num_citations": "89\n", "authors": ["528"]}
{"title": "Interplay of energy and performance for disk arrays running transaction processing workloads\n", "abstract": " The growth of business enterprises and the emergence of the Internet as a medium for data processing has led to a proliferation of applications that are server-centric. The power dissipation of such servers has a major consequence not only on the costs and environmental concerns of power generation and delivery, but also on their reliability and on the design of cooling and packaging mechanisms for these systems. This paper examines the energy and performance ramifications in the design of disk arrays which consume a major portion of the power in transaction processing environments. Using traces of TPC-C and TPC-H running on commercial servers, we conduct in-depth simulations of energy and performance behavior of disk arrays with different RAID configurations. Our results demonstrate that conventional disk power optimizations that have been previously proposed and evaluated for single disk\u00a0\u2026", "num_citations": "88\n", "authors": ["528"]}
{"title": "Method, System, and Computer Program Product for Malware Detection, Analysis, and Response\n", "abstract": " A method, system, and computer program product for detecting malware from outside the host operating system using a disk, virtual machine, or combination of the two. The method, system, and computer program product detects malware at the disk level while computer files in the host operating system are in actual program execution by identifying characteristic malware properties and behaviors associated with the disk requests made. The malware properties and behaviors are identified by using rules that can reliably detect file-infecting viruses. The method, system, and computer program product also uses the disk processor to provide accelerated scanning of virus signatures, which substantially decreases overhead incurred on the host operating system by existing malware detection techniques. In the event that malware is detected, the method, system, and computer program product can respond by limiting\u00a0\u2026", "num_citations": "87\n", "authors": ["528"]}
{"title": "Disk drive roadmap from the thermal perspective: A case for dynamic thermal management\n", "abstract": " The importance of pushing the performance envelope of disk drives continues to grow, not just in the server market but also in numerous consumer electronics products. One of the most fundamental factors impacting disk drive design is the heat dissipation and its effect on drive reliability, since high temperatures can cause off-track errors, or even head crashes. Until now, drive manufacturers have continued to meet the 40% annual growth target of the internal data rates (IDR) by increasing RPMs, and shrinking platter sizes, both of which have counter-acting effects on the heat dissipation within a drive. As this paper shows, we are getting to a point where it is becoming very difficult to stay on this roadmap. This paper presents an integrated disk drive model that captures the close relationships between capacity, performance and thermal characteristics over time. Using this model, we quantify the drop off in IDR\u00a0\u2026", "num_citations": "86\n", "authors": ["528"]}
{"title": "Real-World Design and Evaluation of Compiler-Managed GPU Redundant Multithreading\n", "abstract": " Reliability for general purpose processing on the GPU (GPGPU) is becoming a weak link in the construction of reliable supercomputer systems. Because hardware protection is expensive to develop, requires dedicated on-chip resources, and is not portable across different architectures, the efficiency of software solutions such as redundant multithreading (RMT) must be explored. This paper presents a real-world design and evaluation of automatic software RMT on GPU hardware. We first describe a compiler pass that automatically converts GPGPU kernels into redundantly threaded versions. We then perform detailed power and performance evaluations of three RMT algorithms, each of which provides fault coverage to a set of structures in the GPU. Using real hardware, we show that compilermanaged software RMT has highly variable costs. We further analyze the individual costs of redundant work scheduling\u00a0\u2026", "num_citations": "77\n", "authors": ["528"]}
{"title": "How I Learned to Stop Worrying and Love Flash Endurance.\n", "abstract": " Flash memory in Solid-State Disks (SSDs) has gained tremendous popularity in recent years. The performance and power benefits of SSDs are especially attractive for use in data centers, whose workloads are I/O intensive. However, the apparent limited write-endurance of flash memory has posed an impediment to the wide deployment of SSDs in data centers. Prior architecture and system level studies of flash memory have used simplistic endurance estimates derived from datasheets to highlight these concerns. In this paper, we model the physical processes that affect endurance, which include both stresses to the memory cells as well as a recovery process. Using this model, we show that the recovery process, which the prior studies did not consider, significantly boosts flash endurance. Using a set of real enterprise workloads, we show that this recovery process allows for orders of magnitude higher number of writes and erases than those given in datasheets. Our results indicate that SSDs that use standard wear-leveling techniques are much more resilient under realistic operating conditions than previously assumed and serve to explain some trends observed in recent flash measurement studies.", "num_citations": "77\n", "authors": ["528"]}
{"title": "A complexity-effective approach to alu bandwidth enhancement for instruction-level temporal redundancy\n", "abstract": " Previous proposals for implementing instruction-level temporal redundancy in out-of-order cores have reported a performance degradation of up to 45% in certain applications compared to an execution which does not have any temporal redundancy. An important contributor to this problem is the insufficient number of ALUs for handling the amplified load injected into the core. At the same time, increasing the number of ALUs can increase the complexity of the issue logic, which has been pointed out to be one of the most timing critical components of the processor. This paper proposes a novel extension of a prior idea on instruction reuse to ease ALU bandwidth requirements in a complexity-effective way by exploiting certain interesting properties of a dual (temporally redundant) instruction stream. We present microarchitectural extensions necessary for implementing an instruction reuse buffer (IRB) and integrating\u00a0\u2026", "num_citations": "68\n", "authors": ["528"]}
{"title": "SlicK: slice-based locality exploitation for efficient redundant multithreading\n", "abstract": " Transient faults are expected a be a major design consideration in future microprocessors. Recent proposals for transient fault detection in processor cores have revolved around the idea of redundant threading, which involves redundant execution of a program across multiple execution contexts. This paper presents a new approach to redundant threading by bringing together the concepts of slice-level execution and value and control-flow locality into a novel partial redundant threading mechanism called SlicK.The purpose of redundant execution is to check the integrity of the outputs propagating out of the core (typically through stores). SlicK implements redundancy at the granularity of backward-slices of these output instructions and exploits value and control-flow locality to avoid redundantly executing slices that lead to predictable outputs, thereby avoiding redundant execution of a significant fraction of\u00a0\u2026", "num_citations": "64\n", "authors": ["528"]}
{"title": "FlashPower: A detailed power model for NAND flash memory\n", "abstract": " Flash memory is widely used in consumer electronics products, such as cell-phones and music players, and is increasingly displacing hard disk drives as the primary storage device in laptops, desktops, and even servers. There is a rich microarchitectural design space for flash memory and there are several architectural options for incorporating flash into the memory hierarchy. Exploring this design space requires detailed insights into the power characteristics of flash memory. In this paper, we present FlashPower, a detailed analytical power model for Single-Level Cell (SLC) based NAND flash memory, which is used in high-performance flash products. We have integrated FlashPower with CACTI 5.3, which is widely used in the architecture community for studying memory organizations. FlashPower takes as input device technology and microarchitectural parameters to estimate the power consumed by a flash chip\u00a0\u2026", "num_citations": "55\n", "authors": ["528"]}
{"title": "Interaction of scaling trends in processor architecture and cooling\n", "abstract": " It is predicted that two important trends are likely to accompany traditional CMOS semiconductor technology scaling - chip multiprocessors and 3D integration. With the ever-increasing power consumption and the consequent difficulty in heat removal, it is important to consider the limits and implications of different cooling methods for the upcoming many-core and 3D era. In this paper, we consider both technology scaling and many-core architecture scaling trends in conjunction with conventional air cooling and advanced microchannel cooling for both 2D and 3D microprocessors and identify interesting inflection design points down the road.", "num_citations": "55\n", "authors": ["528"]}
{"title": "Quantized AVF: A means of capturing vulnerability variations over small windows of time\n", "abstract": " Architectural vulnerability factor (AVF) is the probability that a transient fault in a bit, gate, or transistor becomes a user-visible error. AVFs vary widely across time, applications, and bits. Usually AVFs averaged over time and across applications are used to compute the overall soft error rate of a processor. Average AVFs, however, cannot express the short-term vulnerability variations of a bit as they tend to settle down to a fixed value over time. To quantify the vulnerability of bits over short durations, we introduce the concept of Quantized AVF (Q-AVF). Q-AVF expresses the vulnerability of a bit to soft errors over short intervals of time. The average AVF of a bit for a specific interval can be computed as a weighted average of Q-AVFs of all the quanta in that interval. Our analysis of Q-AVF shows significant runtime variation\u2014as much as 80% or more for certain applications. By capturing vulnerability variations over short windows of time, Q-AVFs provide better opportunities for reducing the performance and power overhead of reliability solutions at run-time.To compute Q-AVF in hardware, linear regression analysis is used to create highly accurate equations that can be implemented with eight simple parameters. These parameters can accurately track Q-AVFs of various structures throughout the processor pipeline. Implementing these equations with fewer parameters is critical to reduce the complexity of run-time Q-AVF tracking, thereby making Q-AVF estimation in hardware practical.", "num_citations": "51\n", "authors": ["528"]}
{"title": "Calculating architectural vulnerability factors for spatial multi-bit transient faults\n", "abstract": " Reliability is an important design constraint in modern microprocessors, and one of the fundamental reliability challenges is combating the effects of transient faults. This requires extensive analysis, including significant fault modelling allow architects to make informed reliability tradeoffs. Recent data shows that multi-bit transient faults are becoming more common, increasing from 0.5% of static random-access memory (SRAM) faults in 180nm to 3.9% in 22nm. Such faults are predicted to be even more prevalent in smaller technology nodes. Therefore, accurately modeling the effects of multi-bit transient faults is increasingly important to the microprocessor design process. Architecture vulnerability factor (AVF) analysis is a method to model the effects of single-bit transient faults. In this paper, we propose a method to calculate AVFs for spatial multibittransient faults (MB-AVFs) and provide insights that can help reduce\u00a0\u2026", "num_citations": "48\n", "authors": ["528"]}
{"title": "Datacenter scale evaluation of the impact of temperature on hard disk drive failures\n", "abstract": " With the advent of cloud computing and online services, large enterprises rely heavily on their datacenters to serve end users. A large datacenter facility incurs increased maintenance costs in addition to service unavailability when there are increased failures. Among different server components, hard disk drives are known to contribute significantly to server failures; however, there is very little understanding of the major determinants of disk failures in datacenters. In this work, we focus on the interrelationship between temperature, workload, and hard disk drive failures in a large scale datacenter. We present a dense storage case study from a population housing thousands of servers and tens of thousands of disk drives, hosting a large-scale online service at Microsoft. We specifically establish correlation between temperatures and failures observed at different location granularities: (a) inside drive locations in a\u00a0\u2026", "num_citations": "47\n", "authors": ["528"]}
{"title": "Modeling power consumption of nand flash memories using flashpower\n", "abstract": " Flash is the most popular solid-state memory technology used today. A range of consumer electronics products, such as cell-phones and music players, use flash memory for storage and flash memory is increasingly displacing hard disk drives as the primary storage device in laptops, desktops, and servers. There is a rich microarchitectural design space for flash memory, and there are several architectural options for incorporating flash into the memory hierarchy. Exploring this design space requires detailed insights into the power characteristics of flash memory. In this paper, we present FlashPower, a detailed power model for the two most popular variants of NAND flash, namely, the single-level cell (SLC) and 2-bit Multi-Level Cell (MLC) based flash memory chips. FlashPower is built on top of CACTI, a widely used tool in the architecture community for studying various memory organizations. FlashPower takes\u00a0\u2026", "num_citations": "46\n", "authors": ["528"]}
{"title": "ePVF: An enhanced program vulnerability factor methodology for cross-layer resilience analysis\n", "abstract": " The Program Vulnerability Factor (PVF) has been proposed as a metric to understand the impact of hardware faults on software. The PVF is calculated by identifying the program bits required for architecturally correct execution (ACE bits). PVF, however, is conservative as it assumes that all erroneous executions are a major concern, not just those that result in silent data corruptions, and it also does not account for errorsthat are detected at runtime, i.e., lead to program crashes. A more discriminating metric can inform the choice of the appropriate resilience techniques with acceptable performance and energy overheads. This paper proposes ePVF, an enhancement of the original PVF methodology, which filters out the crash-causing bits from the ACE bits identified by the traditional PVF analysis. The ePVF methodology consists of an error propagation model that reasons about error propagation in the program, and\u00a0\u2026", "num_citations": "45\n", "authors": ["528"]}
{"title": "Recovery boosting: A technique to enhance NBTI recovery in SRAM arrays\n", "abstract": " Negative Bias Temperature Instability (NBTI) is an important lifetime reliability problem in microprocessors. SRAM-based structures within the processor are especially susceptible to NBTI since one of the PMOS devices in the memory cell always has an input of `0'. Previously proposed recovery techniques for SRAM cells aim to balance the degradation of the two PMOS devices by attempting to keep their inputs at a logic `0' exactly 50% of the time. However, one of the devices is always in the negative bias condition at any given time. In this paper, we propose a technique called Recovery Boosting that allows both PMOS devices in the memory cell to be put into the recovery mode by slightly modifying the design of conventional SRAM cells. We present the circuit-level design of an issue queue that uses such cells and perform SPICE-level simulations to verify its functionality and quantify area and power consumption\u00a0\u2026", "num_citations": "44\n", "authors": ["528"]}
{"title": "A multi-level approach to reduce the impact of nbti on processor functional units\n", "abstract": " NBTI is one of the most important silicon reliability problems facing processor designers today. The impact of NBTI can be mitigated at both the circuit and microarchitecture levels. In this paper, we propose a multi-level optimization approach, combining techniques at the circuit and microarchitecture levels, for reducing the impact of NBTI on the functional units (FUs) of a high-performance processor core. We perform SPICE simulations to evaluate the impact of circuit-level design optimizations to reduce the NBTI guardband in terms of area, delay, and power. We then propose a set of NBTI-aware dynamic instruction scheduling policies at the microarchitecture level and quantify their impact on application performance and guardband reduction through execution-driven simulation. We show that carefully combining techniques at both these levels provides the most attractive solution to reducing the guardband while\u00a0\u2026", "num_citations": "44\n", "authors": ["528"]}
{"title": "Intra-disk parallelism: An idea whose time has come\n", "abstract": " Server storage systems use a large number of disks to achieve high performance, thereby consuming a significant amount of power. In this paper, we propose to significantly reduce the power consumed by such storage systems via intra-disk parallelism, wherein disk drives can exploit parallelism in the I/O request stream. Intra-disk parallelism can facilitate replacing a large disk array with a smaller one, using the minimum number of disk drives needed to satisfy the capacity requirements. We show that the design space of intra-disk parallelism is large and present a taxonomy to formulate specific implementations within this space. Using a set of commercial workloads, we perform a limit study to identify the key performance bottlenecks that arise when we replace a storage array that is tuned to provide high performance with a single high-capacity disk drive. We show that it is possible to match, and even surpass, the\u00a0\u2026", "num_citations": "41\n", "authors": ["528"]}
{"title": "Modeling and analyzing NBTI in the presence of process variation\n", "abstract": " With continuous scaling of transistors in each technology generation, NBTI and Process Variation (PV) have become very important silicon reliability problems for the micro processor industry. In this paper, we develop an analytical model to capture the impact of NBTI in the presence of PV for use in architecture simulations. We capture the following aspects in the model: i) variation in NBTI related to stress and recovery due to workloads, ii) temporal variation in NBTI due to Random Charge Fluctuation (RCF) and iii) Random Dopant Fluctuation (RDF) due to process variation. We use this model to analyze the combined impact of NBTI and PV on a memory structure (register file) and a logic structure (Kogge-Stone adder). We show that the impact of the threshold voltage variations due to NBTI and PV over the nominal degradation can hurt the yield of the structures. Due to the combined effect of NBTI and PV across\u00a0\u2026", "num_citations": "40\n", "authors": ["528"]}
{"title": "Enhancing NBTI recovery in SRAM arrays through recovery boosting\n", "abstract": " Negative bias temperature instability (NBTI) is an important lifetime reliability problem in microprocessors. SRAM-based structures within the processor are especially susceptible to NBTI since one of the pMOS devices in the memory cell always has an input of \u201c0\u201d. Previously proposed recovery techniques for SRAM cells aim to balance the degradation of the two pMOS devices by attempting to keep their inputs at a logic \u201c0\u201d exactly 50% of the time. However, one of the devices is always in the negative bias condition at any given time. In this paper, we propose a technique called Recovery Boosting that allows both pMOS devices in the memory cell to be put into the recovery mode by slightly modifying to the design of conventional SRAM cells. We evaluate the circuit-level design of a physical register file and an issue queue that use such cells through SPICE-level simulations. We then conduct an architecture-level\u00a0\u2026", "num_citations": "37\n", "authors": ["528"]}
{"title": "Analysis and modeling of memory errors from large-scale field data collection\n", "abstract": " Main memory reliability plays a crucial role in overall system reliability. Unfortunately, our collective understanding of the rate, pattern, and impact of memory errors is inadequate and can hinder our ability to innovate new fault-tolerant designs. This paper presents an in-depth study of observed corrected error data from the main memory system of a large server population deployed in data centers. Our analysis includes multiple structures on the memory path, such as the memory controllers, busses, channels, and memory modules. Based on our observations, we present a taxonomy of potential faults in the memory path. We provide a detailed characterization of the faults and present novel insights into the nature of these faults and the errors that they induce.", "num_citations": "31\n", "authors": ["528"]}
{"title": "Exploring the thermal impact on manycore processor performance\n", "abstract": " Performance of processors with many simple parallel cores is limited by the serial part of the workload, requiring an asymmetric core organization with one or more aggressive \u201cprimary\u201d cores for better serial performance. A primary core introduces power-hungry microarchitectural structures and usually causes severe local hot spots. This paper explores the thermal impact on manycore processor architecture and evaluates its performance. Preliminary results show that thermal constraints reduce performance as expected, but also make performance almost insensitive to the complexity of the primary core across a diverse degrees of parallelism, which greatly reduces design complexity.", "num_citations": "30\n", "authors": ["528"]}
{"title": "The STeTSiMS STT-RAM simulation and modeling system\n", "abstract": " There is growing interest in emerging non-volatile memory technologies such as Phase-Change Memory, Memristors, and Spin-Transfer Torque RAM (STT-RAM). STT-RAM, in particular, is experiencing rapid development that can be difficult for memory systems researchers to take advantage of. What is needed are techniques that enable designers to explore the potential of recent STT-RAM designs and adjust the performance without needing a detailed understanding of the physics. In this paper, we present the STeTSiMS STT-RAM Simulation and Modeling System to assist memory systems researchers. After providing background on the operation of STT-RAM magnetic tunnel junctions (MTJs), we demonstrate how to fit three different published MTJ models to our model and normalize their characteristics with respect to common metrics. The high-speed switching behavior of the designs is evaluated using\u00a0\u2026", "num_citations": "29\n", "authors": ["528"]}
{"title": "Architectural vulnerability modeling and analysis of integrated graphics processors\n", "abstract": " Thanks to the massive parallel processing power and programmability of general-purpose graphics processing units (GPGPUs), many supercomputing centers as well as servers and high-end mobile devices are increasingly using GPUs for both graphics and general purpose computation. However, communication costs between host CPUs and GPUs have been a performance bottleneck. Recent industry trends towards accelerated processing units (APUs) that integrate CPUs and GPUs on a single die can significantly lower the communication costs and allow for more seamless use of these processing components. As the number of applications for APUs increases, reliability of the APU becomes paramount.In this paper, we describe an architectural vulnerability factor (AVF) modeling framework for APUs that we developed, and we present AVF results for several workloads. Our results include AVF characterization of key hardware structures in the GPU component of the APU and the variation in the AVF over time due to the workload execution on both the CPU and GPU sides of the APU. We also examine the impact of APU sizing on the AVF of the workloads.", "num_citations": "27\n", "authors": ["528"]}
{"title": "reFresh SSDs: Enabling high endurance, low cost flash in datacenters\n", "abstract": " Storage performance and power are critical issues in modern datacenters. Solid State Drives (SSDs) offer both performance and power advantages over hard disk drives. With the advent of MLC flash, the cost-per-Gigabyte of Flash has dropped significantly enough to make it attractive for use in largescale storage in datacenters. However, flash suffers from limited endurance and wears away after a certain number of writes and erases, thereby entailing higher replacement and service costs and hence an increase in the overall server infrastructure costs. In this paper, we develop a physically-accurate model of flash memory reliability and show that there exists a tradeoff between endurance and retention time. We then quantify this tradeoff and show how to exploit it without causing data integrity problems by refreshing the data in the flash memory cells. We propose and evaluate an Earliest-Deadline First (EDF) based refresh policy that can increase the endurance limit by 6-56% for an MLC-based SSD.", "num_citations": "27\n", "authors": ["528"]}
{"title": "Accelerating enterprise solid-state disks with non-volatile merge caching\n", "abstract": " Flash memory is now widely used in the design of solid-state disks (SSDs) as they are able to sustain significantly higher I/O rates than even high-performance hard disks, while using significantly less power. These characteristics make SSDs especially attractive for use in enterprise storage systems, and it is predicted that the use of SSDs will save 58,000 MWh/year by 2013. However, Flash-based SSDs are unable to reach peak performance on common enterprise data patterns such as log-file and metadata updates due to slow write speeds (an order-of-magnitude slower than reads) and the inability to do in-place updates. In this paper, we utilize an auxiliary, byte-addressable, non-volatile memory to design a general purpose merge cache that significantly improves write performance. We also utilize simple read policies that further improve the performance of the SSD without adding significant overhead\u00a0\u2026", "num_citations": "27\n", "authors": ["528"]}
{"title": "Architecting storage for the cloud computing era\n", "abstract": " Rapid changes in hardware technology and system requirements are opening up a variety of exciting lines of research that could dramatically change the design of future computer systems. This article presents the first in a periodic series of prolegomena brief introductions to recent and exciting research topics, in which the main challenges and vision for future research are outlined. The goal of this feature is to facilitate discussion and collaboration. In this first prolegomenon, Sudhanva Gurumurthi, an assistant professor of computer science at the University of Virginia, discusses the storage challenges posed by datacenter-scale workloads.", "num_citations": "26\n", "authors": ["528"]}
{"title": "Kleio: A hybrid memory page scheduler with machine intelligence\n", "abstract": " The increasing demand of big data analytics for more main memory capacity in datacenters and exascale computing environments is driving the integration of heterogeneous memory technologies. The new technologies exhibit vastly greater differences in access latencies, bandwidth and capacity compared to the traditional NUMA systems. Leveraging this heterogeneity while also delivering application performance enhancements requires intelligent data placement. We present Kleio, a page scheduler with machine intelligence for applications that execute across hybrid memory components. Kleio is a hybrid page scheduler that combines existing, lightweight, history-based data tiering methods for hybrid memory, with novel intelligent placement decisions based on deep neural networks. We contribute new understanding toward the scope of benefits that can be achieved by using intelligent page scheduling in\u00a0\u2026", "num_citations": "20\n", "authors": ["528"]}
{"title": "Designing giga-scale memory systems with STT-RAM\n", "abstract": " Spin-Transfer Torque RAM (STT-RAM) is an emerging non-volatile memory technology with the potential to be used as universal memory. The near-SRAM endurance and CMOS compatibility makes it suitable for use throughout the memory and storage hierarchies. However, the density is significantly lower than Flash, and the high write-currents limit the performance and energy-efficiency of STT-RAM caches. This dissertation presents tools and techniques for modeling and optimizing STT-RAM for use in high-speed memory system design. This makes it possible to compare published magnetic tunnel junction (MTJ) designs and perform first-order evaluations of cache and memory designs. Augmenting a Flash-based Solid-State Disk with a STT-RAM merge cache can reduce the response time by more than 75%, while sacrificing the retention-time of the memory cells improves both the performance and energy-efficiency of STT-RAM caches. Detailed error modeling makes it possible to design a refreshing scheme that maintains the reliability of the system, and dynamically adjusting the refresh rate according to current temperature reduces the refresh overhead. This adaptive refreshing can reduce the cell area by more than 28%, compared to STT-RAM with error, while simultaneously limiting the impact of performance and consumption.", "num_citations": "20\n", "authors": ["528"]}
{"title": "Understanding performance issues on both single core and multi-core architecture\n", "abstract": " This research intended to find the relationships between the memory system and performance in both single core and multicore context. For the single core part, several parameters have been considered to improve the performance. Based on these work we extend to the multi-core issues. Overall, the simulations showed results similar to configurations of many current consumer CMPs. Many results were expected, like a large L2 cache size certainly helped performance a lot and the L1 cache size, optimal system frequency was typical of many of the Intel Core 2 Duos, and that after already having 2-4 core any more cores do not help performance in FFT nearly as greatly. However, some results were more surprising like the low speedups that the n-core systems presented. Also, some of the experiments failed, like for 16-core systems and for cache coherence tests. Overall, the excess of 500 simulations recorded helped understand that in multi-core systems, communication overhead and memory latencies are a limiting factor in performance. Finding good cache configurations certainly help increase performance by taking the pressure off of the main memory and reducing communication and cache coherence latencies.", "num_citations": "20\n", "authors": ["528"]}
{"title": "A systematic methodology for evaluating the error resilience of gpgpu applications\n", "abstract": " The wide adoption of graphics processing units (GPUs) as accelerators for general-purpose applications makes the end-to-end reliability implications of their use increasingly significant. Fault injection is a widely adopted method to evaluate the resilience of applications. However, building a fault injector for general-purpose GPU applications is challenging due to their massive parallelism, which makes it difficult to achieve representativeness while being time-efficient. This paper makes four key contributions. First, it presents a fault-injection methodology to evaluate the end-to-end reliability properties of application kernels running on GPUs. Second, it introduces GPU-Qin, a fault-injection tool that uses real GPU hardware and offers a tunable and efficient balance between the representativeness and the cost of a fault-injection campaign. Third, it characterizes the error resilience characteristics of seventeen\u00a0\u2026", "num_citations": "18\n", "authors": ["528"]}
{"title": "Hardware based redundant multi-threading inside a GPU for improved reliability\n", "abstract": " A system and method for verifying computation output using computer hardware are provided. Instances of computation are generated and processed on hardware-based processors. As instances of computation are processed, each instance of computation receives a load accessible to other instances of computation. Instances of output are generated by processing the instances of computation. The instances of output are verified against each other in a hardware based processor to ensure accuracy of the output.", "num_citations": "17\n", "authors": ["528"]}
{"title": "Extra bits on SRAM and DRAM errors\u2013more data from the field\n", "abstract": " Several recent publications have shown that memory errors are common in high-performance computing systems, due to hardware faults in the memory subsystem. With exascale-class systems predicted to have 100-350x more DRAM and SRAM than current systems, these faults are predicted to become more common. Therefore, further study of the faults experienced by DRAM and SRAM is warranted. In this paper, we present a field study of DRAM and SRAM faults in Cielo, a leadershipclass high-performance computing system located at Los Alamos National Laboratory.Our DRAM results show that vendor choice has a significant impact on fault rates. We also show that command and address parity on the DDR channel, a required feature of DDR3 memory, is beneficial to memory reliability. For SRAM, we confirm that altitude has a significant impact on SRAM fault rates, and that the majority of these SRAM faults\u00a0\u2026", "num_citations": "17\n", "authors": ["528"]}
{"title": "Active storage revisited: the case for power and performance benefits for unstructured data processing applications\n", "abstract": " The proliferation of digital data has resulted in a mushrooming of data-intensive applications, especially in the area of unstructured data processing. Given the growing popularity of unstructured data processing applications (eg, FlickrTM, Google MapsTM), it is important to rethink system architectures to efficiently run these applications, from both the performance and power viewpoints. In this paper, we revisit active storage, which proposed offloading computation to disk drive processors, as a possible system architecture for these applications. Unlike previous work, we evaluate the microarchitectural aspects of active storage and perform an in-depth examination of the design of the offload processors. Using a set of unstructured data processing benchmarks, we examine two choices along the I/O path where the computation can be offloaded in existing system architectures--a disk drive processor and a disk array\u00a0\u2026", "num_citations": "17\n", "authors": ["528"]}
{"title": "Analyzing energy behavior of spatial access methods for memory-resident data\n", "abstract": " The proliferation of mobile and pervasive computing devices has brought energy constraints into the limelight, together with performance considerations. Energy-conscious design is important at all levels of the system architecture, and the software has a key role to play in conserving the battery energy on these devices. With the increasing popularity of spatial database applications, and their anticipated deployment on mobile devices (such as road atlases and GPS based applications), it is critical to examine the energy implications of spatial data storage and access methods for memory resident datasets. While there has been extensive prior research on spatial access methods on resource-rich environments, this is, perhaps, the first study to examine their suitability for resource-constrained environments. Using a detailed cycle-accurate energy estimation framework and four different datasets, this paper examines the pros and cons of three previously proposed spatial indexing alternatives from both the energy and performance angles. The results from this study can be beneficial to the design and implementation of embedded spatial databases, accelerating their deployment on numerous mobile devices.Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment.", "num_citations": "17\n", "authors": ["528"]}
{"title": "SODA: Sensitivity based optimization of disk architecture\n", "abstract": " Storage plays a pivotal role in the performance of many applications. Optimizing disk architectures is a design-time as well as a run-time issue and requires balancing between performance, power and capacity. The design space is large and there are many\" knobs\" that can be used to optimize disk drive behavior. Here we present a sensitivity-based optimization for disk architectures (SODA) which leverages results from digital circuit design. Using detailed models of the electro-mechanical behavior of disk drives and a suite of realistic workloads, we show how SODA can aid in design and runtime optimization.", "num_citations": "16\n", "authors": ["528"]}
{"title": "Power management of enterprise storage systems\n", "abstract": " Data-centric services, such as transaction processing systems and search-engines, sustain the demands of millions of users each day. These services rely heavily on the I/O subsystem for their data storage and processing requirements. Technological improvements in hard disk drive densities and data-rates have been key enablers in the realization of these storage systems. However, server storage systems consume a large amount of power, leading to higher running costs, increased stresses on the power supply, higher failure rates and detrimental environmental impacts.", "num_citations": "16\n", "authors": ["528"]}
{"title": "Evaluating the error resilience of parallel programs\n", "abstract": " As a consequence of increasing hardware fault rates, HPC systems face significant challenges in terms of reliability. Evaluating the error resilience of HPC applications is an essential step for building efficient fault-tolerant mechanisms for these applications. In this paper, we propose a methodology to characterize the resilience of OpenMP programs using fault-injection experiments. We find that the error resilience of OpenMP applications depends on the program structure and thread model, hence, these need to be taken into account while characterizing error resilience. We also report preliminary results about the correlation between the application's error resilience and the algorithm(s) used in the application.", "num_citations": "15\n", "authors": ["528"]}
{"title": "Balancing soft error coverage with lifetime reliability in redundantly multithreaded processors\n", "abstract": " Silicon reliability is a key challenge facing the microprocessor industry. Processors need to be designed such that they are resilient against both soft errors and lifetime reliability phenomena. However, techniques developed to address one class of reliability problems may impact other aspects of silicon reliability. In this paper, we show that redundant multi-threading (RMT), which provides soft error protection, exacerbates lifetime reliability. We then explore two different architectural approaches to tackle this problem, namely, dynamic voltage scaling (DVS) and partial RMT. We show that each approach has certain strengths and weaknesses with respect to performance, soft error coverage, and lifetime reliability. We then propose and evaluate a hybrid approach that combines DVS and partial RMT. We show that this approach provides better improvement in lifetime reliability than DVS or partial RMT alone, buys back\u00a0\u2026", "num_citations": "15\n", "authors": ["528"]}
{"title": "Sensitivity based power management of enterprise storage systems\n", "abstract": " Energy-efficiency is a key requirement in data centers today. Storage systems constitute a significant fraction of the energy consumed in a data center and therefore enterprise storage systems need to deliver high performance in an energy-efficient manner. Static tuning of the storage system is not sufficient since energy consumption is strongly dependent on runtime variations in workload characteristics. Although dynamic disk power management can enable the storage system to adapt to varying workload conditions, prior work in this area has resorted to ad hoc heuristics that cannot guarantee that the system meets energy-efficiency goals. In this paper, we present a novel approach to storage power management that uses the sensitivity-based optimization technique. Our approach systematically balances the dynamic knobs in the disks to operate the storage-system at a desired performance level while\u00a0\u2026", "num_citations": "13\n", "authors": ["528"]}
{"title": "Energy-performance trade-offs for spatial access methods on memory-resident data\n", "abstract": " The proliferation of mobile and pervasive computing devices has brought energy constraints into the limelight. Energy-conscious design is important at all levels of system architecture, and the software has a key role to play in conserving battery energy on these devices. With the increasing popularity of spatial database applications, and their anticipated deployment on mobile devices (such as road atlases and GPS-based applications), it is critical to examine the energy implications of spatial data storage and access methods for memory resident datasets. While there has been extensive prior research on spatial access methods on resource-rich environments, this is, perhaps, the first study to examine their suitability for resource-constrained environments. Using a detailed cycle-accurate energy estimation framework and four different datasets, this paper examines the pros and cons of three previously\u00a0\u2026", "num_citations": "13\n", "authors": ["528"]}
{"title": "Software-based dynamic reliability management for gpu applications\n", "abstract": " In this paper we propose a framework for dynamic reliability management (DRM) for GPU applications based on the idea of plug-n-play software-based reliability enhancement (SRE). The approach entails first assessing the vulnerability of GPU kernels to soft errors in program visible structures. This assessment is performed on a low level intermediate program representation rather than the application source. Second, this assessment guides selective injection of code implementing SRE techniques to protect the most vulnerable data. Code injection occurs transparently at runtime using a just-in-time (JIT) compiler. Thus, reliability enhancement is selective, transparent, on-demand, and customizable. This flexible, automated software-based DRM framework can provide an adaptable, cost-effective approach to scaling reliability of large systems. We present the results of a proof of concept implementation on NVIDIA\u00a0\u2026", "num_citations": "12\n", "authors": ["528"]}
{"title": "A benchmark suite for unstructured data processing\n", "abstract": " A large fraction of the data that will stored and accessed in future systems is expected to be unstructured, in the form of images, audio files, etc. Therefore, it is very important to design future I/O subsystems to provide efficient storage, and access to these vast and continuously growing repositories of unstructured data. To facilitate system design and evaluation, we first need benchmarks that capture the processing and I/O access characteristics of applications that operate on unstructured data. In this paper, we present an unstructured data processing benchmark suite that we have developed. We provide detailed descriptions of the workloads in the benchmark suite and discuss the larger space of application characteristics that each of them capture.", "num_citations": "12\n", "authors": ["528"]}
{"title": "Detecting errors in directory entries\n", "abstract": " In one embodiment, the present invention includes a system, which may be a multiprocessor system having multiple nodes, each with a processor and a cache. The system may include a directory stored in a memory that includes entries having coherency information. At least one of the nodes may be configured to detect an error in an entry of the directory based on a coherency protocol and a state of a status indicator and presence vector of the entry, and without the storage of error correction or parity information in the entry. In some embodiments, the node may correct the error using state information obtained from other nodes.", "num_citations": "11\n", "authors": ["528"]}
{"title": "Thermal issues in disk drive design: Challenges and possible solutions\n", "abstract": " The importance of pushing the performance envelope of disk drives continues to grow in the enterprise storage market. One of the most fundamental factors impacting disk drive design is heat dissipation, since it directly affects drive reliability. Until now, drive manufacturers have continued to meet the 40% annual growth target of the internal data-rates (IDR) by increasing RPMs and shrinking platter sizes, both of which have counteracting effects on the heat dissipation within a drive. In this article, we shall show that we are getting to a point where it is going to be very difficult to stay on this roadmap. We first present detailed models that capture the close relationships between capacity, performance, and thermal characteristics over time. Using these models, we quantify the drop-off in IDR growth rates over the next decade if we are to adhere to the thermal design envelope. We motivate the need for continued\u00a0\u2026", "num_citations": "11\n", "authors": ["528"]}
{"title": "Using intradisk parallelism to build energy-efficient storage systems\n", "abstract": " Server storage systems use numerous disks to achieve high performance, thereby consuming a significant amount of power. This paper discussed the intradisk parallelism that can significantly reduce such systems' power consumption by letting disk drives exploit parallelism in the I/O request stream. By doing so, it's possible to match, and even surpass, a storage array's performance for these workloads using a single, high-capacity disk drive.", "num_citations": "9\n", "authors": ["528"]}
{"title": "Towards transient fault tolerance for heterogeneous computing platforms\n", "abstract": " The computing demands of applications coupled with the power wall problem in modern processors are expected to pave the way for heterogeneous computing platforms that are composed of a variety of processors and hardware accelerators. While current heterogeneous platform design analyses assess area, performance, and power, the tremendous increase in transient fault rates requires that reliability analyses also be included, especially since fault protection mechanisms can directly affect the aforementioned area, performance, and power analyses\u2013and they affect these metrics differently when implemented on different processing components. Heterogeneous platform design therefore requires accurate characterization of fault protection mechanisms when used in different processing components. This work-inprogress report details the first step in this direction, providing a characterization of various transient fault protection mechanisms in ASICs and FPGAs.", "num_citations": "9\n", "authors": ["528"]}
{"title": "Sensitivity-based optimization of disk architecture\n", "abstract": " Storage plays a pivotal role in the performance of many applications. Many applications, especially those that run on servers, are I/O intensive and therefore require high performance storage systems. These high-end storage systems consume a large amount of power, the bulk of which is due to the disk drives. Optimizing disk architectures is a design time as well as a run time issue and requires balancing between performance and power. There are different figures of merit, such as performance and energy, and a large space of design and runtime \"knobs\" that can be used to optimize disk drive behavior. Given such a large space, it is desirable to have a systematic methodology to optimally set these knobs to satisfy our figures of merit as efficiently as possible. In this paper we present the sensitivity-based optimization methodology for disk architectures (SODA), which leverages results previously obtained in digital\u00a0\u2026", "num_citations": "8\n", "authors": ["528"]}
{"title": "The need for temperature-aware storage systems\n", "abstract": " Storage has become ubiquitous. Disk drives are commonplace in most laptops and desktops. In addition, they are used in large numbers in high-end server systems. Storage devices has also proliferated the consumer electronics market with their use in products like cameras, and portable music devices. This widespread usage of disks has been the result of tremendous growth in both the density and speed of these devices. Over the past two decades, we have been enjoying a 40 percent annual growth in the data rate of disks, due to innovations in the recording technology coupled with a scaling up of the drive RPM. Since raising the RPM increases the heat that is generated due to viscous dissipation by nearly a cubic factor, in order to design the drives for a constant thermal envelope, the platter sizes are also shrunk, as the latter has a fifth power impact on the temperature. In this paper, it shall be shown that this\u00a0\u2026", "num_citations": "8\n", "authors": ["528"]}
{"title": "Energy and performance considerations in work partitioning for mobile spatial queries\n", "abstract": " A seamless infrastructure for information access and data processing is the backbone for the successful development and deployment of the envisioned ubiquitous/mobile applications of the near future. The development of such an infrastructure is a challenge due to the resource-constrained nature of the mobile devices, in terms of the computational power, storage capacities, wireless connectivity and battery energy. With spatial data and location-aware applications widely recognized as being significant beneficiaries of mobile computing, this paper examines an important topic with respect to spatial query processing from the resource-constrained perspective. Specifically, when faced with the task of answering different location-based queries on spatial data from a mobile device, this paper investigates the benefits of partitioning the work between the resource-constrained mobile device (client) and a resource-rich\u00a0\u2026", "num_citations": "8\n", "authors": ["528"]}
{"title": "MIDAS: an execution-driven simulator for active storage architectures\n", "abstract": " Many applications today are highly data intensive and have stringent performance requirements. In order to meet the performance demands of these applications, we need to optimize both the processing and I/O subsystems. One promising approach to optimize performance is to use \u201cActive Storage\u201d systems, where we use disk drive controllers and storage array controllers as offload processors for the data intensive parts of an application and exploit Data-Level Parallelism (DLP) across the ensemble of processing components. From the architecture viewpoint, the design space of such active storage systems is large. There are a number of choices for the microarchitecture of the processors at the disk, storage array controller, and host, the organization of the electro-mechanical parts of the disk drive, and the characteristics of the interconnection network between these components. Since these design choices can have a significant impact on performance, we need a simulator that can give us detailed insights into the behavior of these systems. This paper presents the Modeling Infrastructure for Dynamic Active Storage (MIDAS). MIDAS is an accurate execution-driven simulator that captures both the processing and I/O behavior of active storage systems. We describe the design of MIDAS, providing details about the various simulation models and how they interact. We then present three case studies that illustrate how MIDAS can be used to study architectural design tradeoffs in active storage systems.", "num_citations": "7\n", "authors": ["528"]}
{"title": "Software only intra-compute unit redundant multithreading for GPUs\n", "abstract": " A system, method and computer program product to execute a first and a second work-item, and compare the signature variable of the first work-item to the signature variable of the second work-item. The first and the second work-items are mapped to an identifier via software. This mapping ensures that the first and second work-items execute exactly the same data for exactly the same code without changes to the underlying hardware. By executing the first and second work-items independently, the underlying computation of the first and second work-item can be verified. Moreover, system performance is not substantially affected because the execution results of the first and second work-items are compared only at specified comparison points.", "num_citations": "6\n", "authors": ["528"]}
{"title": "Signature-based store checking buffer\n", "abstract": " A system and method for optimizing redundant output verification, are provided. A hardware-based store fingerprint buffer receives multiple instances of output from multiple instances of computation. The store fingerprint buffer generates a signature from the content included in the multiple instances of output. When a barrier is reached, the store fingerprint buffer uses the signature to verify the content is error-free.", "num_citations": "6\n", "authors": ["528"]}
{"title": "Nbti-aware dynamic instruction scheduling\n", "abstract": " NBTI is an important emerging silicon reliability problem. In this paper we explore a microarchitecture-level approach to mitigate NBTI related failures in the functional units of a superscalar processor. We analyze the impact of dynamic instruction scheduling on NBTI and show that the conventional approach to instruction scheduling can accelerate the wear out of certain functional units. We then propose and evaluate two different NBTI-aware instruction scheduling policies and quantify the tradeoffs between performance and reliability of each policy.", "num_citations": "6\n", "authors": ["528"]}
{"title": "Managing thermal emergencies in disk-based storage systems\n", "abstract": " Thermal-aware design of disk-drives is important because high temperatures can cause reliability problems. Dynamic thermal management (DTM) techniques have been proposed to operate the disk at the average case temperature, rather than at the worst case by modulating the activities to avoid thermal emergencies caused by unexpected events, such as fan-breaks, increased inlet air temperature, etc. A delay-based approach to adjust the disk seek activities is one such DTM solution for disk-drives. Even if such a DTM approach could overcome thermal emergencies without stopping disk activity, it suffers from long delays when servicing the requests. In this paper, we investigate the possibility of using a multispeed disk-drive (called dynamic rotations per minute (DRPM)), which dynamically modulates the rotational speed of the platter for implementing the DTM technique. Using a detailed performance and\u00a0\u2026", "num_citations": "6\n", "authors": ["528"]}
{"title": "Should disks be speed demons or brainiacs?\n", "abstract": " Disk drives play a critical role on the performance of I/O intensive applications. Over the years, disk drive performance has grown as a result of advances in magnetic recording density and faster rotational speeds. In essence, the performance driver in disks has been the data rate. In this paper, we show that data rate is going to be increasingly difficult to optimize, due to power/thermal constraints. We argue that disk drive designers should instead focus their efforts on providing more computational capabilities that data intensive applications could leverage in order to boost performance. We also discuss the scope for provisioning powerful processors inside disk drives to provide these computational capabilities.", "num_citations": "6\n", "authors": ["528"]}
{"title": "Using STEAM for thermal simulation of storage systems\n", "abstract": " As demands for higher data transfer rates mount, temperature is joining bandwidth and latency as a key consideration in storage system design. The STEAM simulator lets users abstract the details of low-level recording physics and heat transfer phenomena and facilitates exploration of a large design space of storage configurations under realistic workloads", "num_citations": "6\n", "authors": ["528"]}
{"title": "Thermal Attacks on Storage Systems\n", "abstract": " Disk drives are a performance bottleneck for data-intensive applications. Drive manufacturers have continued to increase the rotational speeds to meet performance requirements, but the faster drives consume more power and run hotter. Future drives will soon be operating at temperatures that threaten drive reliability. One strategy that has been proposed for increasing drive performance without sacrificing reliability is throttling. Throttling delays service to I/O requests after the disk temperature exceeds a set threshold temperature until the temperatures drops. In this paper, we explore the possibility that a malicious attacker with the ability to issue disk read requests may be able to exploit throttling to carry out a denial-ofservice attack on a storage system. Our results reveal that damaging attacks are possible when throttling is used, and argue for the use of variable speed disks as a less vulnerable thermal management alternative.", "num_citations": "6\n", "authors": ["528"]}
{"title": "Towards disk-level malware detection\n", "abstract": " Disk drive capabilities and processing power are steadily increasing, and this power gives us the possibility of using disks as data processing devices rather than merely for data transfers. In the area of malicious code (malware) detection, anti-virus (AV) engines are slow and have trouble correctly identifying many types of malware. Our goal is to help make malware detection more reliable and more efficient by using the disk drive\u2019s processor. Using the extra processing power available on modern disk drives can provide significant advantages in detecting malware including reducing the traditional AV engine\u2019s workload on the host CPU by partitioning the workload between the host AV engine and the disk drive, improving the detection of stealth malware by providing a low-level view of the system, and recognizing virus behavior by observing disk I/O traffic directly. Several research questions must be addressed before these benefits can be realized: how to correctly partition work between the AV engine and the disk drive processor, how to design interfaces between the operating system (OS) or host AV engine and the disk drive that provide satisfactory performance without compromising security, and how to recognize malicious behavior based on the dynamic analysis of low-level data accesses.", "num_citations": "6\n", "authors": ["528"]}
{"title": "Sos: Using speculation for memory error detection\n", "abstract": " With the problem of transient errors in integrated circuits becoming very critical in future process technologies, it is important to develop effective techniques for detection and possible recovery from such errors. Dense integration, lower supply voltages and higher frequencies make circuits more susceptible to bit flips due to cosmic-ray strikes. Memory cells, such as SRAMs and DRAMs, typically occupy a large portion of the real-estate and are hence particularly vulnerable to these transient faults. While techniques such as parity and ECC are being used to handle such situations, such safeguards can impose significant storage, performance, and power overheads, especially in highperformance designs. This paper investigates a fresh approach to developing transient-fault detection mechanisms. We show that speculative-execution resources inside the processor can be tailored to provide detection of errors that propagate in from memory. Using detailed experiments with different benchmarks, we demonstrate that the branch-predictor in conjunction with a lightweight value-predictor can catch most of the errors that come in to the processor from the memory system. We also show the microarchitectural extensions required for implementing such error-detectors.", "num_citations": "6\n", "authors": ["528"]}
{"title": "Software only inter-compute unit redundant multithreading for GPUs\n", "abstract": " A system, method and computer program product to execute a first and a second work-group, and compare the signature variables of the first work-group to the signature variables of the second work-group via a synchronization mechanism. The first and the second work-group are mapped to an identifier via software. This mapping ensures that the first and second work-groups execute exactly the same data for exactly the same code without changes to the underlying hardware. By executing the first and second work-groups independently, the underlying computation of the first and second work-groups can be verified. Moreover, system performance is not substantially affected because the execution results of the first and second work-groups are compared only at specified comparison points.", "num_citations": "5\n", "authors": ["528"]}
{"title": "Energy\u2010Efficient Storage Systems for Data Centers\n", "abstract": " This chapter contains sections titled:   Introduction   Disk Drive Operation and Disk Power   Disk and Storage Power Reduction Techniques   Using Nonvolatile Memory and Solid\u2010State Disks   Conclusions   References", "num_citations": "5\n", "authors": ["528"]}
{"title": "Is Traditional Power Management+ Prefetching== DRPM for Server Disks?\n", "abstract": " The I/O subsystem on servers consumes consider-able energy leading to cost, reliability and environmental concerns. Hence, there is a need for reduction of energy consumption of server disks. Most of the idle periods for server disks are shorter than the total time taken to spin down the disks and bring them back up. Traditional Power Management (TPM) schemes, which completely shut the disks down during periods of inactivity, are therefore ineffective. In a previous study, it has been shown that Dynamic Rotations per Minute (DRPM), a power management scheme that modulates disk rotation speed based on request arrival patterns is an effective solution to this problem. However, DRPM disks do not exist yet. This paper intends to evaluate both TPM schemes combined with I/O Prefetching and DRPM. Using both synthetic and real workloads and both idealistic and realistic versions of TPM, DRPM and Prefetching, we have a conducted simulations which reiterate the necessity of alternate techniques such as DRPM for server power management.", "num_citations": "5\n", "authors": ["528"]}
{"title": "Resilient vertical stacked chip network for routing memory requests to a plurality of memory dies\n", "abstract": " Systems, apparatuses, and methods for routing traffic through vertically stacked memory are disclosed. A computing system includes a host processor die and multiple vertically stacked memory dies. The host processor die generates memory access requests for the data stored in the multiple memory array banks in the memory dies. At least one memory die uses an on-die network switch with a programmable routing table for routing packets corresponding to the generated memory requests. Routes use both vertical hops and horizontal hops to reach the target memory array bank and to avoid any congested or failed resources along the route. The vertically stacked memory dies use through silicon via interconnects and at least one via does not traverse through all of the memory dies. Accordingly, the host processor die does not have a direct connection to one or more of the multiple memory dies.", "num_citations": "4\n", "authors": ["528"]}
{"title": "System and method for protecting GPU memory instructions against faults\n", "abstract": " A system and method for protecting memory instructions against faults are described. The system and method include converting the slave instructions to dummy operations, modifying memory arbiter to issue up to N master and N slave global/shared memory instructions per cycle, sending master memory requests to memory system, using slave requests for error checking, entering master requests to the GM/LM FIFO, storing slave requests in a register, and comparing the entered master requests with the stored slave requests.", "num_citations": "4\n", "authors": ["528"]}
{"title": "Challenges of high-capacity dram stacks and potential directions\n", "abstract": " With rapid growth in data volumes and an increase in number of CPU/GPU cores per chip, the capacity and bandwidth of main memory can be scaled up to accommodate performance requirements of data-intensive applications. Recent 3D-stacked in-package memory devices such as high-bandwidth memory (HBM) and similar technologies can provide high amounts of memory bandwidth at low access energy. However, 3D-stacked in-package memory have limited memory capacity. In this paper, we study and present challenges of scaling the capacity of 3D-stacked memory devices by stacking more DRAM dies within a device and building taller memory stacks. We also present potential directions and mitigations to building tall HBM stacks of DRAM dies. Although taller stacks are a potentially interesting approach to increase HBM capacity, we show that more research is needed to enable high-capacity memory\u00a0\u2026", "num_citations": "4\n", "authors": ["528"]}
{"title": "Soft failures in large datacenters\n", "abstract": " A major problem in managing large-scale datacenters is diagnosing and fixing machine failures. Most large datacenter deployments have a management infrastructure that can help diagnose failure causes, and manage assets that were fixed as part of the repair process. Previous studies identify only actual hardware replacements to calculate Annualized Failure Rate (AFR) and component reliability. In this paper, we show that service availability is significantly affected by soft failures and that this class of failures is becoming an important issue at large datacenters with minimum human intervention. Soft failures in the datacenter do not require actual hardware replacements, but still result in service downtime, and are equally important because they disrupt normal service operation. We show failure trends observed in a large datacenter deployment of commodity servers and motivate the need to modify conventional\u00a0\u2026", "num_citations": "4\n", "authors": ["528"]}
{"title": "Evaluating the resilience of parallel applications\n", "abstract": " Reliability is a significant design constraint for supercomputers and large-scale data centers. Modeling the effects of faults on applications targeted to such systems allows system architects and software designers to provision resilience features, that improve fidelity of results and reduce runtimes. In this paper, we propose mechanisms to improve existing techniques to model the effect of transient faults on realistic applications. First, we extend the existing Program Vulnerability Factor metric to model multi-threaded applications. Then we demonstrate how to measure the multi-threaded PVF of an application in simulation and introduce the ability to account for software detection of hardware faults, differentiating faults that cause detected, uncorrected errors (DUE) from faults that cause silent data corruption (SDC).", "num_citations": "3\n", "authors": ["528"]}
{"title": "Examining the impact of ace interference on multi-bit avf estimates\n", "abstract": " Until recently, soft error reliability has been focused on single-bit errors and as a consequence, methodologies for Architectural Vulnerability Factor (AVF) analysis have been well defined and established for single-bit faults. However, studies have shown that multi-bit faults are becoming more prevalent with technology scaling [1]. If this trend continues, multi-bit faults will eventually become a high percentage of all microprocessor faults.Research into modeling methodologies for multi-bit faults is scarce. Recently, we presented an Architecturally Correct Execution (ACE) analysis methodology to evaluate the AVF of spatial multi-bit faults (MB-AVF)[2]. We used our methodology to study multi-bit AVF by extending the ACE analysis infrastructure of a performance simulator. While this methodology precisely measures AVF for Detected Unrecoverable Errors (DUEs), it only approximates AVF for Silent Data Corruptions\u00a0\u2026", "num_citations": "3\n", "authors": ["528"]}
{"title": "Power availability provisioning in large data centers\n", "abstract": " Enterprise data centers are provisioned with conservative redundancies built into their power infrastructures to handle failures. Conservative over-provisioning of power capacity for availability reasons results in significant capital investment for large enterprises because this capacity is designed for failure conditions that do not happen often. On the other hand, underprovisioning this capacity runs the risk of affecting the performance of the data center when failures do happen, through either service unavailability or degraded service performance. Hence, there are interactions and tradeoffs between power capacity utilization, power redundancy, and data center performance that is often overlooked. Our work proposes a provisioning methodology for the power delivery infrastructure called power availability provisioning that addresses this challenge. We provide observations on power infrastructure design based on\u00a0\u2026", "num_citations": "3\n", "authors": ["528"]}
{"title": "Dynamic speed control for server class disks\n", "abstract": " A large portion of the power budget in server environments goes into the I/O subsystem-the disk array in particular. Traditional approaches to disk power management involve completely stopping the disk rotation, which can take a considerable amount of time, making them less useful in cases where idle times between disk requests may not be long enough to outweigh the overheads. This paper presents a new approach called DRPM to modulate disk speed (RPM) dynamically, and gives a practical implementation to exploit this mechanism. Extensive simulations with different workload and hardware parameters show that DRPM can provide significant energy savings without compromising much on performance. This paper also discusses practical issues when implementing DRPM on server disks.", "num_citations": "3\n", "authors": ["528"]}
{"title": "Graceful Operation of Disk Drives under Thermal Emergencies\n", "abstract": " Thermal-aware design of disk-drives is an important concern because high temperatures can cause reliability problems. Hence, dynamic thermal management (DTM) has been proposed to operate the disk at the average case, rather than the worst case by modulating the activities to avoid thermal emergencies at run time while pushing the performance. A delay-based approach to adjust the disk seek activities is one DTM solution for normal disk-drives. Even if it could overcome thermal emergencies without stopping disk activity, it suffers from long delays when servicing the requests. This paper investigates the possibility of using a multi-speed disk-drive which dynamically modulates the rotational speed of the platter (called DRPM) for implementing DTM, Using a detailed performance and thermal simulator of storage system the authors evaluate DTM policies and observe that the DRPM technique is the best to\u00a0\u2026", "num_citations": "2\n", "authors": ["528"]}
{"title": "Power profiling of modern die-stacked memory\n", "abstract": " Die-stacked memories that integrate multiple DRAM dies into the processor package have reduced the interface bottleneck and improved efficiency, but demands for memory capacity and bandwidth remain unfulfilled. Additionally, the introduction of memory into the package further complicates heat removal. Memory power is therefore becoming a key architectural concern. To provide insight into these challenges, an architectural power model for High Bandwidth Memory is developed, validated, and used to provide detailed power profiles. Based on the resulting power trends, power is projected for potential future memory configurations with increased bandwidth and capacity. The results suggest that, without significant improvements in memory technology or architecture, the power utilization of in-package memories will continue to grow and limit the system power budget.", "num_citations": "1\n", "authors": ["528"]}
{"title": "Resilience Proportionality\u2014A Paradigm for Efficient and Reliable System Design\n", "abstract": " Reliability, Availability, and Serviceability (RAS) are key considerations in hardware design, be it for mobile devices or high-end servers. However, provisioning RAS is often at odds with meeting performance and energy targets and increases the overall cost of design of the chip. As a result of this tension, chip design companies have to make difficult decisions about how much RAS they can incorporate into each product in their portfolio and even what customers and market segments they can realistically target. On the other hand, highly scaled silicon technology nodes are susceptible to a variety of reliability problems and emerging technologies such as die-stacking and non-volatile memory, while critical for meeting the demands of future computing needs, have significant reliability challenges of their own. RAS features can actually serve to reduce the deployment costs of these technologies (e.g., by\u00a0\u2026", "num_citations": "1\n", "authors": ["528"]}
{"title": "Infrastructure to support accelerator computation models for active storage\n", "abstract": " A method, a system, and a non-transitory computer readable medium for generating application code to be executed on an active storage device are presented. The parts of an application that can be executed on the active storage device are determined. The parts of the application that will not be executed on the active storage device are converted into code to be executed on a host device. The parts of the application that will be executed on the active storage device are converted into code of an instruction set architecture of a processor in the active storage device.", "num_citations": "1\n", "authors": ["528"]}
{"title": "Using redundant transactions to verify the correctness of program code execution\n", "abstract": " In the described embodiments, a processor core (eg, a GPU core) receives a section of program code to be executed in a transaction from another entity in a computing device. The processor core sends the section of program code to one or more compute units in the processor core to be executed in a first transaction and concurrently executed in a second transaction, thereby creating a \u201credundant transaction pair.\u201d When the first transaction and the second transaction are completed, the processor core compares a read-set of the first transaction to a read-set of the second transaction and compares a write-set of the first transaction to a write-set of the second transaction. When the read-sets and the write-sets match and no transactional error condition has occurred, the processor core allows results from the first transaction to be committed to an architectural state of the computing device.", "num_citations": "1\n", "authors": ["528"]}
{"title": "Single-Threaded Mode AVF Prediction During Redundant Execution\n", "abstract": " Transient faults can lead to serious errors in ex-ecution. Providing protection for the processor core against these faults requires redundant execution, which leads to a performance loss. However, not all bit flips have equal impact on the processor. The Architectural Vulnerability Factor (AVF) quantifies when a soft error is likely to alter the final output and when it has little impact due to the effects of masking. Thus, redundancy is only important during periods of high AVF. Although calculating the AVF typically requires post-execution analysis of the microarchitectural behavior of a program, recent work has shown it can be estimated online. However, redundant execution changes the bits that flow through the processor, exposing bottlenecks that single-threaded execution may not display and slowing overall execution to an unpredictable degree. This variability complicates estimation of the single-threaded AVF during redundant execution, making it difficult to decide when protection is unnecessary due to low vulnerability. To leverage these low AVF periods without leaving the processor vulnerable to transient faults, we need a way to track the single-threaded AVF even when protection is enabled. Our solution is to investigate the predictability of the single-threaded AVF during redundant execution and develop predictors for the underlying AVF of three processor structures. We then evaluate these predictors in a partial RMT implementation using intelligent toggling with a sample reliability policy.", "num_citations": "1\n", "authors": ["528"]}