{"title": "Software engineering economics\n", "abstract": " All of use software engineering economics decision analysis techniques discussed above are only as good as the input data we can provide for them. For software decisions, the most I3t'iT-ititii and difficult of these inputs to provide are est. t',|'nates of the cost of a proposed software project. in this section, we will summarize:1] the major software cost estimation techniques available, and their relative strengths and difficulties; 1} algorithmic models for software cost estimation; 3} outstanding research issues i_rt software cost estimation,", "num_citations": "12087\n", "authors": ["5"]}
{"title": "A spiral model of software development and enhancement\n", "abstract": " A short description is given of software process models and the issues they address. An outline is given of the process steps involved in the spiral model, an evolving risk-driven approach that provides a framework for guiding the software process, and its application to a software project is shown. A summary is given of the primary advantages and implications involved in using the spiral model and the primary difficulties in using it at its current incomplete level of elaboration.< >", "num_citations": "7565\n", "authors": ["5"]}
{"title": "Software risk management: principles and practices\n", "abstract": " The emerging discipline of software risk management is described. It is defined as an attempt to formalize the risk-oriented correlates of success into a readily applicable set of principles and practices. Its objectives are to identify, address, and eliminate risk items before they become either threats to successful software operation or major sources of software rework. The basic concepts are set forth, and the major steps and techniques involved in software risk management are explained. Suggestions for implementing risk management are provided.< >", "num_citations": "2670\n", "authors": ["5"]}
{"title": "Balancing agility and discipline: A guide for the perplexed\n", "abstract": " Agility and discipline: These apparently opposite attributes are, in fact, complementary values in software development. Plan-driven developers must also be agile; nimble developers must also be disciplined. The key to success is finding the right balance between the two, which will vary from project to project according to the circumstances and risks involved. Developers, pulled toward opposite ends by impassioned arguments, ultimately must learn how to give each value its due in their particular situations. Balancing Agility and Discipline sweeps aside the rhetoric, drills down to the operational core concepts, and presents a constructive approach to defining a balanced software development strategy. The authors expose the bureaucracy and stagnation that mark discipline without agility, and liken agility without discipline to unbridled and fruitless enthusiasm. Using a day in the life of two development teams and ground-breaking case studies, they illustrate the differences and similarities between agile and plan-driven methods, and show that the best development strategies have ways to combine both attributes. Their analysis is both objective and grounded, leading finally to clear and practical guidance for all software professionals--showing how to locate the sweet spot on the agility-discipline continuum for any given project.", "num_citations": "2227\n", "authors": ["5"]}
{"title": "TRW systems engineering and integration division. Characteristics of software quality\n", "abstract": " TRW systems engineering and integration division. Characteristics of software quality - NASA/ADS Now on home page ads icon ads Enable full ADS view NASA/ADS TRW systems engineering and integration division. Characteristics of software quality Boehm, BW Abstract Publication: TRW Software Series: TRW-SS Pub Date: 1973 Bibcode: 1973tsei.book.....B No Sources Found \u252c\u2310 The SAO/NASA Astrophysics Data System adshelp[at]cfa.harvard.edu The ADS is operated by the Smithsonian Astrophysical Observatory under NASA Cooperative Agreement NNX16AC86A NASA logo Smithsonian logo Resources About ADS ADS Help What's New Careers@ADS Social @adsabs ADS Blog Project Switch to full ADS Is ADS down? (or is it just me...) Smithsonian Institution Smithsonian Privacy Notice Smithsonian Terms of Use Smithsonian Astrophysical Observatory NASA \u0393\u00c7\u00aa", "num_citations": "1815\n", "authors": ["5"]}
{"title": "A spiral model of software development and enhancement\n", "abstract": " A spiral model of software development and enhancement Page 1 A \u0393\u00fb\u00ac 0 M xwbE 4, o E x 1, o 1: 0 F' '~ c c 3 a H a \"4, 0 ov Eq . o V q> + m '[ , .x A E o ' m o o E ? v m N Cc .5 EU 'd E', O W 'tl 'O '~ v a 0 E g O! y a C Oi i. AEL a adi cd y E > E. w y A W o 'o' v v .4 m E E C w b 0 x d C ~ EO v N = o co C . \u0393\u00c7\u20a7 W 0 w a EC O ACC .Y. O y v 3 a v b 2 2 .co i ' v g ~o~nv . q .8 o o 0 C w Cl) yy .o 0 to o o U a . 1 -2 _ C o > p ( W ] \" A 2 3 d o, a \u0393\u00c7\u00f3 v u ' v E o y 3 ,~ o a c a. ro~ E 8:,-s ' ~ E m -o \u0393\u00c7\u00f3 .2 o v .~ ro \u0393\u00c7\u00f3 2 y r oo, ,cw 2 2 y N p u v N '' q >' ya 3 'O b o' E v '[ v \u0393\u00c7\u00f3 m m 'bb o y U .5 g' 4 o v a nm 7 3 oo - 0 3 o ; _\u0393\u00c7\u00f3 a c2 oi v.. 'D 'vy p O o . o 9 , >, a o a1 0 0 N M V 3 0\u0393\u00c7\u00f3 yvuomov Y v .7; m 2 U ya .0 y o -' 0 _a OE y A m Q v E doo E v - cn b ro E y OV .9 O v v yy C 'E v N 0 a ' :. -0 'v E0 3 m \u0393\u00c7\u00f3mv N y v 0, m v C v 0 o Uo \" ., c 5 U o - ; 4 m E - 3 v O T, > o 0 mo F N 0 0 A v' m E t A ' f,d a a co Page 2 a a 8 a 5. 0 A .0 .5 0 0 .5 '5 O e 5 aoV ^ 5 C a 0 4 .5 t, '5 V n F a F -a q . 8 0 > \u0393\u00c7\u00f3 '\u0393\u00c7\u00aa", "num_citations": "1732\n", "authors": ["5"]}
{"title": "Get ready for agile methods, with care\n", "abstract": " Although many of their advocates consider the agile and plan-driven software development methods polar opposites, synthesizing the two can provide developers with a comprehensive spectrum of tools and options. Real-world examples argue for and against agile methods. Responding to change has been cited as the critical technical success factor in the Internet browser battle between Microsoft and Netscape. But overresponding to change has been cited as the source of many software disasters, such as the $3 billion overrun of the US Federal Aviation Administration's Advanced Automation System for national air traffic control. The author believes that both agile and plan-driven approaches have a responsible center and overinterpreting radical fringes. Agile and plan-driven methods both form part of the planning spectrum. Thus, while each approach has a home ground within which it performs very well, and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1503\n", "authors": ["5"]}
{"title": "Software risk management\n", "abstract": " Although software risk management is not yet a fully articulated discipline, its contributions to date have helped many software projects avoid devastating pitfalls and reach successful conclusions. The techniques of risk assessment and risk control outlined here stimulate a \"no surprises\" approach to software management which improves project management visibility and control, and significantly reduces software rework. (Rework costs generally comprise 40\u0393\u00c7\u00f450% of overall software development costs; typically, 80% of the rework costs are caused by the highest-risk 20% of the software problems encountered.) And finally, risk management provides a useful framework for determining which software V&V activities to pursue, and at what level of effort.", "num_citations": "1263\n", "authors": ["5"]}
{"title": "Quantitative evaluation of software quality\n", "abstract": " The study reported in this paper establishes a conceptual framework and some key initial results in the analysis of the characteristics of software quality. Its main results and conclusions are:", "num_citations": "1151\n", "authors": ["5"]}
{"title": "Verifying and validating software requirements and design specifications\n", "abstract": " These recommendation provide a good starting point for identifying and resolving software problems early in life cycle when they're s relatively easy to handle", "num_citations": "1144\n", "authors": ["5"]}
{"title": "Understanding and controlling software costs\n", "abstract": " A discussion is presented of the two primary ways of understanding software costs. The black-box or influence-function approach provides useful experimental and observational insights on the relative software productivity and quality leverage of various management, technical, environmental, and personnel options. The glass-box or cost distribution approach helps identify strategies for integrated software productivity and quality improvement programs using such structures as the value chain and the software productivity opportunity tree. The individual strategies for improving software productivity are identified. Issues related to software costs and controlling them are examined and discussed. It is pointed out that a good framework of techniques exists for controlling software budgets, schedules, and work completed, but that a great deal of further progress is needed to provide an overall set of planning and control\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1083\n", "authors": ["5"]}
{"title": "Cost models for future software life cycle processes: COCOMO 2.0\n", "abstract": " Current software cost estimation models, such as the 1981 Constructive Cost Model (COCOMO) for software cost estimation and its 1987 Ada COCOMO update, have been experiencing increasing difficulties in estimating the costs of software developed to new life cycle processes and capabilities. These include non-sequential and rapid-development process models; reuse-driven approaches involving commercial off-the-shelf (COTS) packages, re-engineering, applications composition, and applications generation capabilities; object-oriented approaches supported by distributed middleware; and software process maturity initiatives. This paper summarizes research in deriving a baseline COCOMO 2.0 model tailored to these new forms of software development, including rationale for the model decisions. The major new modeling capabilities of COCOMO 2.0 are a tailorable family of software sizing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1033\n", "authors": ["5"]}
{"title": "Management challenges to implementing agile processes in traditional development organizations\n", "abstract": " Discussions with traditional developers and managers concerning agile software development practices nearly always contain two somewhat contradictory ideas. They find that on small, stand-alone projects, agile practices are less burdensome and more in tune with the software industry's increasing needs for rapid development and coping with continuous change. Managers face several barriers, real and perceived, when they try to bring agile approaches into traditional organizations. They categorized the barriers either as problems only in terms of scope or scale, or as significant general issues needing resolution. From these two categories, we've identified three areas - development process conflicts, business process conflicts, and people conflicts - that we believe are the critical challenges to software managers of large organizations in bringing agile approaches to bear in their projects.", "num_citations": "864\n", "authors": ["5"]}
{"title": "Software development cost estimation approaches\u0393\u00c7\u00f6A survey\n", "abstract": " This paper summarizes several classes of software cost estimation models and techniques: parametric models, expertise\u0393\u00c7\u00c9based techniques, learning\u0393\u00c7\u00c9oriented techniques, dynamics\u0393\u00c7\u00c9based models, regression\u0393\u00c7\u00c9based models, and composite\u0393\u00c7\u00c9Bayesian techniques for integrating expertise\u0393\u00c7\u00c9based and regression\u0393\u00c7\u00c9based models. Experience to date indicates that neural\u0393\u00c7\u00c9net and dynamics\u0393\u00c7\u00c9based techniques are less mature than the other classes of techniques, but that all classes of techniques are challenged by the rapid pace of change in software technology. The primary conclusion is that no single technique is best for all situations, and that a careful comparison of the results of several approaches is most likely to produce realistic estimates.", "num_citations": "841\n", "authors": ["5"]}
{"title": "A view of 20th and 21st century software engineering\n", "abstract": " George Santayana's statement,\" Those who cannot remember the past are condemned to repeat it,\" is only half true. The past also includes successful histories. If you haven't been made aware of them, you're often condemned not to repeat their successes. In a rapidly expanding field such as software engineering, this happens a lot. Extensive studies of many software projects such as the Standish Reports offer convincing evidence that many projects fail to repeat past successes. This paper tries to identify at least some of the major past software experiences that were well worth repeating, and some that were not. It also tries to identify underlying phenomena influencing the evolution of software engineering practices that have at least helped the author appreciate how our field has gotten to where it has been and where it is. A counterpart Santayana-like statement about the past and future might say,\" In an era of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "830\n", "authors": ["5"]}
{"title": "Improving software productivity\n", "abstract": " C bounds, while software produc-tivity seems to be barely holding its own. Central processing units, random access memories, and mass memories improve their price-performance ratios by orders of magnitude per decade, while software projects continue to grind out production-engineered code at the same old rate of one to two delivered lines of code per man-hour.Yet, if software is judged by the same standards as hardware, its productivity looks pretty good. One can produce a mil-lioncopies of Lotus 1-2-3 at least as cheaply as a million copies of the Intel 286. Database management systems that cost $5 million 20 years ago can now be purchased for $99.95.", "num_citations": "727\n", "authors": ["5"]}
{"title": "Theory-W software project management principles and examples\n", "abstract": " A software project management theory is presented called Theory W: make everyone a winner. The authors explain the key steps and guidelines underlying the Theory W statement and its two subsidiary principles: plan the flight and fly the plan; and, identify and manage your risks. Theory W's fundamental principle holds that software project managers will be fully successful if and only if they make winners of all the other participants in the software process: superiors, subordinates, customers, users, maintainers, etc. Theory W characterizes a manager's primary role as a negotiator between his various constituencies, and a packager of project solutions with win conditions for all parties. Beyond this, the manager is also a goal-setter, a monitor of progress towards goals, and an activist in seeking out day-to-day win-lose or lose-lose project conflicts confronting them, and changing them into win-win situations. Several\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "694\n", "authors": ["5"]}
{"title": "Value-based software engineering: reinventing\n", "abstract": " The Value-Based Software Engineering (VBSE) agenda described in the preceding article has the objectives of integrating value considerations into current and emerging software engineering principles and practices, and of developing an overall framework in which they compatibly reinforce each other. In this paper, we provide a case study illustrating some of the key VBSE practices, and focusing on a particular anomaly in the monitoring and control area: the \"Earned Value Management System.\" This is a most useful technique for monitoring and controlling the cost, schedule, and progress of a complex project. But it has absolutely nothing to say about the stakeholder value of the system being developed. The paper introduces an example order-processing software project, and shows how the use of Benefits Realization Analysis, stake-holder value proposition elicitation and reconciliation, and business case\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "681\n", "authors": ["5"]}
{"title": "Software and its impact: A quantitative assessment\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u251c\u00actre utilis\u251c\u2310 dans le cadre d\u0393\u00c7\u00d6une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u251c\u2592alado antes, el contenido de este registro bibliogr\u251c\u00edfico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "599\n", "authors": ["5"]}
{"title": "Using risk to balance agile and plan-driven methods\n", "abstract": " Both agile and plan-driven approaches have situation-dependent shortcomings that, if not addressed, can lead to project failure. The challenge is to balance the two approaches to take advantage of their strengths in a given situation while compensating for their weaknesses. The authors present a risk-based approach for structuring projects to incorporate both agile and plan-driven approaches in proportion to a project's needs.", "num_citations": "574\n", "authors": ["5"]}
{"title": "Identifying quality-requirement conflicts\n", "abstract": " Without a well-defined set of quality-attribute requirements, software projects are vulnerable to failure. The authors have developed QARCC, a knowledge-based tool that helps users, developers, and customers analyze requirements and identify conflicts among them.", "num_citations": "547\n", "authors": ["5"]}
{"title": "Prototyping versus specifying: a multiproject experiment\n", "abstract": " In this experiment, seven software teams developed versions of the same small-size (2000-4000 source instruction) application software product. Four teams used the Specifying approach. Three teams used the Prototyping approach. The main results of the experiment were the following. 1) Prototyping yielded products with roughly equivalent performance, but with about 40 percent less code and 45 percent less effort. 2) The prototyped products rated somewhat lower on functionality and robustness, but higher on ease of use and ease of learning. 3) Specifying produced more coherent designs and software that was easier to integrate. The paper presents the experimental data supporting these and a number of additional conclusions.", "num_citations": "516\n", "authors": ["5"]}
{"title": "Value-based software engineering\n", "abstract": " The IT community has always struggled with questions concerning the value of an organization\u0393\u00c7\u00d6s investment in software and hardware. It is the goal of value-based software engineering (VBSE) to develop models and measures of value which are of use for managers, developers and users as they make tradeoff decisions between, for example, quality and cost or functionality and schedule\u0393\u00c7\u00f4such decisions must be economically feasible and comprehensible to the stakeholders with differing value perspectives. VBSE has its roots in work on software engineering economics, pioneered by Barry Boehm in the early 1980s. However, the emergence of a wider scope that defines VBSE is more recent. VBSE extends the merely technical ISO software engineering definition with elements not only from economics, but also from cognitive science, finance, management science, behavioral sciences, and decision sciences\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "506\n", "authors": ["5"]}
{"title": "Empirical findings in agile methods\n", "abstract": " In recent years, the use of, interest in, and controversy about Agile methodologies have realized dramatic growth. Anecdotal evidence is rising regarding the effectiveness of agile methodologies in certain environments and for specified projects. However, collection and analysis of empirical evidence of this effectiveness and classification of appropriate environments for Agile projects has not been conducted. Researchers from four institutions organized an eWorkshop to synchronously and virtually discuss and gather experiences and knowledge from eighteen Agile experts spread across the globe. These experts characterized Agile Methods and communicated experiences using these methods on small to very large teams. They discussed the importance of staffing Agile teams with highly skilled developers. They shared common success factors and identified warning signs of problems in Agile projects\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "427\n", "authors": ["5"]}
{"title": "Anchoring the software process\n", "abstract": " Software organizations need common milestones to serve as a basis for their software development processes. The author proposes three such milestones, gives an example of their use, and discusses why they are success-critical for software projects. To avoid the problems of the previous model milestones-stakeholder mismatches, gold plating, inflexible point solutions, high risk downstream capabilities, and uncontrolled developments-software projects need a mix of flexibility and discipline. The risk driven content of the LCO, LCA, and IOC milestones let you tailor them to specific software situations and yet they remain general enough to apply to most software projects. And, because they emphasize stakeholder commitment to shared system objectives, they can provide your organization with a collaborative framework for successfully realizing software's most powerful capability: its ability to help people and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "422\n", "authors": ["5"]}
{"title": "Bayesian analysis of empirical software engineering cost models\n", "abstract": " Many parametric software estimation models have evolved in the last two decades (L.H. Putnam and W. Myers, 1992; C. Jones, 1997; R.M. Park et al., 1992). Almost all of these parametric models have been empirically calibrated to actual data from completed software projects. The most commonly used technique for empirical calibration has been the popular classical multiple regression approach. As discussed in the paper, the multiple regression approach imposes a few assumptions frequently violated by software engineering datasets. The paper illustrates the problems faced by the multiple regression approach during the calibration of one of the popular software engineering cost models, COCOMO II. It describes the use of a pragmatic 10 percent weighted average approach that was used for the first publicly available calibrated version (S. Chulani et al., 1998). It then moves on to show how a more\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "377\n", "authors": ["5"]}
{"title": "Developing groupware for requirements negotiation: lessons learned\n", "abstract": " Defining requirements is a complex and difficult process, and defects in the process often lead to costly project failures. There is no complete and well-defined set of requirements waiting to be discovered in system development. Different stakeholders: users, customers, managers, domain experts, and developers, come to the project with diverse expectations and interests. Requirements emerge in a highly collaborative, interactive, and interdisciplinary negotiation process that involves heterogeneous stakeholders. At the University of Southern California's Center for Software Engineering, we have developed a series of groupware implementations for the WinWin requirements negotiation approach. The WinWin approach involves having a system's success-critical stakeholders participate in a negotiation process so they can converge on a mutually satisfactory or win-win set of requirements. The WinWin groupware\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "371\n", "authors": ["5"]}
{"title": "Software economics: a roadmap\n", "abstract": " The fundamental goal of all good design and engineering is to create maximal value added for any given investment. There are many dimensions in which value can be assessed, from monetary profit to the solution of social problems. The benefits sought are often domain-specific, yet the logic is the same: design is an investment activity. Software economics is the field that seeks to enable significant improvements in software design and engineering through economic reasoning about product, process, program, and portfolio and policy issues. We summarize the state of the art and identify shortfalls in existing knowledge. Past work focuses largely on costs, not on benefits, thus not on value added; nor are current technical software design criteria linked clearly to value creation. We present a roadmap for research emphasizing the need for a strategic investment approach to software engineering. We discuss how\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "367\n", "authors": ["5"]}
{"title": "What we have learned about fighting defects\n", "abstract": " The Center for Empirically Based Software Engineering helps improve software development by providing guidelines for selecting development techniques, recommending areas for further research, and supporting software engineering education. A central activity toward achieving this goal has been the running of \"e- Workshops\" that capture expert knowledge with a minimum of overhead effort to formulate heuristics on a particular topic. The resulting heuristics are a useful summary of the current state of knowledge in an area based on expert opinion. This paper discusses the results to date of a series of e-Workshops on software defect reduction. The original discussion items are presented along with an encapsulated summary of the expert discussion. The reformulated heuristics can be useful both to researchers (for pointing out gaps in the current state of the knowledge requiring further investigation) and to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "355\n", "authors": ["5"]}
{"title": "Value-adaptive security threat modeling and vulnerability ranking\n", "abstract": " Among others, techniques and systems are disclosed for analyzing security threats associated with software and computer vulnerabilities. Stakeholder values relevant for a software system are identified. The identified stakeholder values are quantified using a quantitative decision making approach to prioritize vulnerabilities of the software system. A structured attack graph is generated to include the quantified stakeholder values to define a scalable framework to evaluate attack scenarios. The structured attack graph includes two or more nodes. Based on the generated structured attack graph, structured attack paths are identified with each attack path representing each attack scenario.", "num_citations": "314\n", "authors": ["5"]}
{"title": "COTS-based systems top 10 list\n", "abstract": " Presents a COTS-based system (CBS) software defect-reduction list as hypotheses, rather than results, that also serve as software challenges for enhancing our empirical understanding of CBSs. The hypotheses are: (1) more than 99% of all executing computer instructions come from COTS products (each instruction passed a market test for value); (2) more than half the features in large COTS software products go unused; (3) the average COTS software product undergoes a new release every 8-9 months, with active vendor support for only its latest three releases; (4) CBS development and post-deployment efforts can scale as high as the square of the number of independently developed COTS products targeted for integration; (5) CBS post-deployment costs exceed CBS development costs; (6) although glue-code development usually accounts for less than half the total CBS software development effort, the effort\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "314\n", "authors": ["5"]}
{"title": "Spiral development: Experience, principles, and refinements\n", "abstract": " Spiral development is a family of software development processes characterized by repeatedly iterating a set of elemental development processes and managing risk so it is actively being reduced. This paper characterizes spiral development by enumerating a few invariant properties that any such process must exhibit. For each, a set of variants is also presented, demonstrating a range of process definitions in the spiral development family. Each invariant excludes one or more hazardous spiral look-alike models, which are also outlined. This report also shows how the spiral model can be used for a more cost-effective incremental commitment of funds, via an analogy of the spiral model to stud poker. An important and relatively recent innovation to the spiral model has been the introduction of anchor point milestones. The latter part of the paper describes and discusses these.Descriptors:", "num_citations": "304\n", "authors": ["5"]}
{"title": "COTS integration: Plug and pray?\n", "abstract": " For most software applications, the use of commercial off-the-shelf products has become an economic necessity. Gone are the days when upsized industry and government information technology organizations had the luxury of trying to develop-and, at greater expense, maintain their own database, network, and user-interface management infrastructure. Viable COTS products are climbing up the protocol stack, from infrastructure into applications solutions in such areas as office and management support, electronic commerce, finance, logistics, manufacturing, law and medicine. For small and large commercial companies, time-to-market pressures also exert a strong pressure toward COTS-based solutions. However, most organizations have also found that COTS gains are accompanied by frustrating COTS pains. The paper summarizes experience on the relative advantages and disadvantages of COTS solutions.", "num_citations": "292\n", "authors": ["5"]}
{"title": "Industrial software metrics top 10 list\n", "abstract": " CiNii \u03a6\u00bd\u00fb\u00b5\u00fb\u00e7 - Industrial software metrics top 10 list CiNii \u03c3\u00a2\u255c\u03c4\u00bd\u00ef\u00b5\u00e2\u00e0\u03c3\u00e1\u2592\u03c3\u00a1\u00aa\u03c4\u00e1\u00f6\u03c4\u2310\u2562\u00b5\u00eb\u00c7 \u03c3\u00a1\u00aa\u03a6\u00ed\u00f4\u00b5\u00e2\u00e0\u03c3\u00e1\u2592\u03c0\u00e2\u00e8\u03c0\u00e2\u00f4\u03c0\u00e9\u2593\u03c0\u00e2\u255d\u03c0\u00e9\u2510[\u03c0\u00e9\u2561\u03c0\u00e9\u00f1\u03c0\u00e2\u00ef\u03c0\u00e9\u00fa ] \u00b5\u00f9\u00d1\u00b5\u00a3\u00bc\u03c0\u00fc\u00ab\u03a6\u00bd\u00fb\u00b5\u00fb\u00e7\u03c0\u00e9\u00c6\u03c0\u00fc\u00f2\u03c0\u00fc\u00ee\u03c0\u00fc\u00d6 \u03c3\u00f1\u00ba\u03c3\u00a1\u00aa\u03c3\u00a2\u2502\u00b5\u00a2\u2555\u0398\u00f1\u00bf\u03c0\u00fc\u00ab\u00b5\u00a3\u00bc\u03c0\u00e9\u00c6\u03c0\u00fc\u00f2\u03c0\u00fc\u00ee\u03c0\u00fc\u00d6 \u00b5\u00f9\u00d1\u00b5\u00a3\u00bc\u03c0\u00fc\u00ab\u03c3\u00ec\u00dc\u03c3\u00fa\u00bd\u03a6\u00bd\u00fb\u00b5\u00fb\u00e7\u03c0\u00e9\u00c6\u03c0\u00fc\u00f2\u03c0\u00fc\u00ee\u03c0\u00fc\u00d6 \u00b5\u00fb\u2591\u03a6\u00aa\u00c5\u03c4\u00d6\u2557\u0398\u00ee\u2593 \u03c0\u00e2\u00a1\u03c0\u00e9\u2591\u03c0\u00e9\u00f1\u03c0\u00e2\u2502 English \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 \u03c0\u00fc\u00d6\u03c0\u00fc\u2563\u03c0\u00fc\u00aa \u00b5\u00a3\u00bc\u00b5\u00fb\u00e7\u03c0\u00fc\u00e9\u03c0\u00e9\u00e8 \u03c0\u00fc\u00d6\u03c0\u00fc\u2563\u03c0\u00fc\u00aa \u00b5\u00a3\u00bc\u00b5\u00fb\u00e7\u03c0\u00fc\u00e9\u03c0\u00e9\u00e8 \u0398\u00fb\u00eb\u03c0\u00fc\u00ff\u03c0\u00e9\u00ef \u03c0\u00e9\u2510\u03c0\u00e9\u00f1\u03c0\u00e2\u00ea\u03c0\u00e2\u00bd \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0\u03c3\u00c9\u00ec \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0ID \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0\u00b5\u00eb\u00c7\u03c3\u2592\u20a7 \u03c3\u00ea\u00e8\u03a6\u00ed\u00ee\u03c4\u00eb\u2310\u03c3\u00c9\u00ec ISSN \u03c3\u2556\u2557\u03c3\u00c5\u2556 \u03c0\u00e2\u00dc\u03c0\u00e2\u255d\u03c0\u00e9\u2555 \u03c3\u00e7\u2551\u03c4\u00eb\u00ea\u03a6\u00c7\u00e0 \u03c3\u00c5\u00e9\u03a6\u00c7\u00e2\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab \u03c3\u00e7\u2551\u03c4\u00eb\u00ea\u03c3\u2563\u2524 \u03c3\u2563\u2524\u03c0\u00fc\u00ef\u03c0\u00e9\u00eb \u03c3\u2563\u2524\u03c0\u00fc\u255b\u03c0\u00fc\u00ba \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 CiNii\u03c4\u00ac\u00f4\u03c3\u00c5\u00fa\u00b5\u00d1\u00a1\u03c3\u00ef\u00d6\u03c0\u00fc\u00ab\u03c3\u00e5\u00ec\u0398\u00fb\u00ef\u03c0\u00fc\u00bd\u03c0\u00fc\u00f1\u03c0\u00fc\u00e4\u03c0\u00fc\u00aa Industrial software metrics top 10 list BOEHM BW \u03a6\u00f3\u00bd\u03c3\u255d\u00f2\u03c4\u00f6\u00bf\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab: 1\u03a3\u2557\u2562 \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0 BOEHM BW \u03c3\u00c5\u00c4\u0398\u00ee\u2593 \u03c3\u00ea\u00e8\u03a6\u00ed\u00ee\u03c4\u00eb\u2310 IEEE Software IEEE Software 4(5), 84-85, 1987 \u03a6\u00f3\u00bd\u03c3\u255d\u00f2\u03c4\u00f6\u00bf\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab: 1\u03a3\u2557\u2562\u03a3\u2555\u00a1 1-1\u03a3\u2557\u2562\u03c0\u00e9\u00c6 \u03a6\u00ed\u00bf\u03c4\u00f1\u2551 1 \u03c4\u00a2\u2555\u0398\u00fb\u00f3 \u03c0\u00e2\u00bd\u03c0\u00e2\u255d\u03c0\u00e2\u00bd\u03c0\u00e2\u20a7\u03c0\u00e9\u00f1\u03c0\u00e2\u00ef\u03c0\u00e2\u2502\u03c0\u00e9\u2591\u03c0\u00fc\u00bd\u03c0\u00e9\u00ea\u03c0\u00e9\u00ef\u03c0\u00e9\u255c\u03c0\u00e2\u00f2\u03c0\u00e2\u00ea\u03c0\u00e9\u00aa\u03c0\u00e9\u00ba\u03c0\u00e9\u00f3\u0398\u00fb\u00ef\u03c4\u00d6\u2551\u03c0\u00e2\u00f9\u03c0\u00e2\u00a1\u03c0\u00e9\u2555\u03c0\u00e9\u00ba\u03c0\u00e9\u00bb\u03c0\u00e2\u00ea\u03a3\u2555\u00a1\u03c0\u00fc\u00ab\u03c0\u00e2\u00ac\u03c0\u00e9\u2563\u03c0\u00e9\u00bb\u03a6\u00aa\u00fc\u03c3\u00a2\u00e1\u03c0\u00fc\u00ab\u03c3\u00ea\u00e5\u00b5\u20a7\u00c9 \u00b5\u2561\u00a3\u0398\u00e7\u00c4 \u03c3\u2551\u2556\u03a6\u00fa\u00f2 , \u03c3\u00f1\u2310\u03c3\u2561\u00a3 \u03a6\u00fc\u00ed\u03a3\u2557\u00ef , \u00b5\u2591\u2524\u0398\u00e7\u00c4 \u03a3\u2510\u00ab , \u03a6\u00c5\u00e8\u0398\u00e7\u00c4 \u03a3\u2551\u00bf , Yasuhiro Hamano , Sousuke Amasaki , Osamu Mizuno , Tohru Kikuno , \u03c3\u00f1\u00ba\u0398\u00ff\u00ac \u03c3\u00f1\u00ba\u03c3\u00a1\u00aa\u03c3\u00f1\u00ba\u03c3\u00a1\u00aa\u0398\u00d6\u00f3\u00b5\u00e2\u00e0\u03c3\u00e1\u2592\u03c4\u00ba\u00e6\u03c3\u00a1\u00aa\u03c4\u00e1\u00f6\u03c4\u2310\u2562\u03c4\u00ba\u00e6 , \u03c3\u00f1\u00ba\u0398\u00ff\u00ac\u03c3\u00f1\u00ba\u03c3\u00a1\u00aa\u03c3\u00f1\u00ba\u03c3\u00a1\u00aa\u0398\u00d6\u00f3\u00b5\u00e2\u00e0\u03c3\u00e1\u2592\u03c4\u00ba\u00e6\u03c3\u00a1\u00aa\u03c4\u00e1\u00f6\u03c4\u2310\u2562\u03c4\u00ba\u00e6 , \u03c3\u00f1\u00ba\u0398\u00ff\u00ac\u03c3\u00f1\u00ba\u03c3\u00a1\u00aa\u03c3\u00f1\u00ba\u03c3\u00a1\u00aa\u0398\u00d6\u00f3\u00b5\u00e2\u00e0\u03c3\u00e1\u2592\u03c4\u00ba\u00e6\u03c3\u00a1\u00aa\u03c4\u00e1\u00f6\u03c4\u2310\u2562\u03c4\u00ba\u00e6 , \u03c3\u00f1\u00ba\u0398\u00ff\u00ac\u03c3\u00f1\u00ba\u03c3\u00a1\u00aa\u03c3\u00f1\u00ba\u03c3\u00a1\u00aa\u0398\u00d6\u00f3\u00b5\u00e2\u00e0\u03c3\u00e1\u2592\u03c4\u00ba\u00e6\u03c3\u00a1\u00aa\u03c4\u00e1\u00f6\u03c4\u2310\u2562\u03c4\u00ba\u00e6 , Graduate School of Information Science and Osaka , of /\u0393\u00c7\u00aa", "num_citations": "281\n", "authors": ["5"]}
{"title": "Software requirements negotiation and renegotiation aids: A theory-W based spiral approach\n", "abstract": " A major problem in requirements engineering is obtaining requirements that address the concerns of multiple stakeholders. An approach to such a problem is the Theory-W based Spiral Model. This paper focuses on the problem of developing a support system for such a model. In particular it identifies needs and capabilities required to address the problem of negotiation and renegotiation that arises when the model is applied to incremental requirements engineering. The paper formulates elements of the support system, called WinWin, for providing such capabilities. The key elements of WinWin are described and their use in incremental requirements engineering are demonstrated, using an example renegotiation scenario from the domain of software engineering environments, for satellite ground stations.", "num_citations": "270\n", "authors": ["5"]}
{"title": "Some future trends and implications for systems and software engineering processes\n", "abstract": " In response to the increasing criticality of software within systems and the increasing demands being put onto 21st century systems, systems and software engineering processes will evolve significantly over the next two decades. This paper identifies eight relatively surprise\u0393\u00c7\u00c9free trends\u0393\u00c7\u00f6the increasing interaction of software engineering and systems engineering; increased emphasis on users and end value; increased emphasis on systems and software dependability; increasingly rapid change; increasing global connectivity and need for systems to interoperate; increasingly complex systems of systems; increasing needs for COTS, reuse, and legacy systems and software integration; and computational plenty. It also identifies two \u0393\u00c7\u00a3wild card\u0393\u00c7\u00a5 trends: increasing software autonomy and combinations of biology and computing. It then discusses the likely influences of these trends on systems and software engineering\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "248\n", "authors": ["5"]}
{"title": "Value-based software engineering: a case study\n", "abstract": " The information technology field's accelerating rate of change makes feedback control essential for organizations to sense, evaluate, and adapt to changing value propositions in their competitive marketplace. Although traditional project feedback control mechanisms can manage the development efficiency of stable projects in well-established value situations, they do little to address the project's actual value, and can lead to wasteful misuse of an organization's scarce resources. The value-based approach to software development integrates value considerations into current and emerging software engineering principles and practices, while developing an overall framework in which these techniques compatibly reinforce each other.", "num_citations": "245\n", "authors": ["5"]}
{"title": "Some experience with automated aids to the design of large-scale reliable software\n", "abstract": " Summarizes some recent experience in analyzing and eliminating sources of error in the design phase of large software projects. It points out some of the significant differences in software error incidence between large and small software projects. A taxonomy of software error causes, and some analyses of the design error data performed to obtain a better understanding of the nature of large-scale software design errors and to evaluate alternative methods of preventing, detecting, and eliminating them are presented. Based on this analysis of observational data, a hypothesis was derived regarding the potential cost effectiveness of an automated aid to detecting inconsistencies between assertions about the nature of inputs and outputs of the various elements of the software design.", "num_citations": "245\n", "authors": ["5"]}
{"title": "Balancing agility and discipline: Evaluating and integrating agile and plan-driven methods\n", "abstract": " Rapid change and increasing software criticality drive successful development and acquisition organizations to balance the agility and discipline of their key processes. The emergence of agile methods in the software community is raising the expectations of customers and management, but the methods have shortfalls and their compatibility with traditional plan-driven methods such as those represented by CMMI, ISO-15288, and UK-DefStan-00-55 is largely unexplored. This paper pragmatically examines the aspects of agile and plan-driven methods and provides an approach to balancing through examples and case studies.", "num_citations": "243\n", "authors": ["5"]}
{"title": "Requirements that handle IKIWISI, COTS, and rapid change\n", "abstract": " In the good old days, dealing with software requirements was relatively easy. Software requirements were the first order of business and took place before design, cost estimation, planning, or programming. Of course, it wasn't simple. Certain straightforward criteria required satisfaction: completeness; consistency; traceability; and testability. The recent developments of IKIWISI (I'll know it when I see it), COTS (commercial-off-the-shelf) software, and the increasingly rapid change in information technology have combined to unsettle the foundations of the old airtight requirements approach. These complicating factors are examined.", "num_citations": "235\n", "authors": ["5"]}
{"title": "Software requirements as negotiated win conditions\n", "abstract": " Current processes and support systems for software requirements determination and analysis often neglect the critical needs of important classes of stakeholders, and limit themselves to the concerns of the developers, users and customers. These stakeholders can include maintainers, interfacers, testers, product line managers, and sometimes members of the general public. This paper describes the results to date in researching and prototyping a next-generation process model (NGPM) and support system (NGPSS) which directly addresses these issues. The NGPM emphasizes collaborative processes, involving all of the significant constituents with a stake in the software product. Its conceptual basis is a set of \"theory W\" (win-win) extensions to the spiral model of software development.< >", "num_citations": "231\n", "authors": ["5"]}
{"title": "Managing software productivity and reuse\n", "abstract": " Your organization can choose from three main strategies for improving its software productivity. You can work faster, using tools that automate or speed up previously labor-intensive tasks. You can work smarter, primarily through process improvements that avoid or reduce non-value-adding tasks. Or you can avoid unnecessary work by reusing software artifacts instead of custom developing each project. Which strategy will produce the highest payoff? The author performed an extensive analysis that addressed this question for the US Department of Defense. The result of this analysis showed that work avoidance via software reuse produced the highest improvement in software productivity. The article gives advice on how to manage software reuse and the pitfalls to avoid.", "num_citations": "218\n", "authors": ["5"]}
{"title": "Software risk management\n", "abstract": " Software development\u0393\u00c7\u00d6s risky nature is easy enough to acknowledge in the abstract, but sadly, harder to acknowledge in real-world situations. Our culture has evolved such that owning up to risks is often confused with defeatism. Thus, a manager faced with a nearly impossible schedule may deliberately ignore risks to project a confident,\u0393\u00c7\u00a3can-do\u0393\u00c7\u00a5 attitude. Or is that assessment too severe? After all, youwouldn\u0393\u00c7\u00d6t ignore risks on your project. Or would you? To understand how risks get ignored, we must look beyond the kinds of risk that are subject to easy, obvious mitigation, such as:\u0393\u00c7\u00a3If we don\u0393\u00c7\u00d6t add two folks to the test team right away, we are never going to complete acceptance testing in time for a June 1 delivery.\u0393\u00c7\u00a5 Any manager capable of staying awake during the work day will leap at solving this one; ignoring it means missing a chance to take early and efficient corrective action.", "num_citations": "213\n", "authors": ["5"]}
{"title": "Observations on balancing discipline and agility\n", "abstract": " Agile development methodologies promise higher customer satisfaction, lower defect rates, faster development times and a solution to rapidly changing requirements. Plan-driven approaches promise predictability, stability, and high assurance. However, both approaches have shortcomings that, if left unaddressed, can lead to project failure. The challenge is to balance the two approaches to take advantage of their strengths and compensate for their weaknesses. We believe this can be accomplished using a risk-based approach for structuring projects to incorporate both agile and disciplined approaches in proportion to a project's needs. We present six observations drawn from our efforts to develop such an approach. We follow those observations with some practical advice to organizations seeking to integrate agile and plan-driven methods in their development process.", "num_citations": "188\n", "authors": ["5"]}
{"title": "Software engineering: R&D trends and defense needs\n", "abstract": " CiNii \u03a6\u00bd\u00fb\u00b5\u00fb\u00e7 - Software Engineering : R&D trends and defense needs CiNii \u03c3\u00a2\u255c\u03c4\u00bd\u00ef\u00b5\u00e2\u00e0\u03c3\u00e1\u2592\u03c3\u00a1\u00aa\u03c4\u00e1\u00f6\u03c4\u2310\u2562\u00b5\u00eb\u00c7 \u03c3\u00a1\u00aa\u03a6\u00ed\u00f4\u00b5\u00e2\u00e0\u03c3\u00e1\u2592\u03c0\u00e2\u00e8\u03c0\u00e2\u00f4\u03c0\u00e9\u2593\u03c0\u00e2\u255d\u03c0\u00e9\u2510[\u03c0\u00e9\u2561\u03c0\u00e9\u00f1\u03c0\u00e2\u00ef\u03c0\u00e9\u00fa] \u00b5\u00f9\u00d1\u00b5\u00a3\u00bc\u03c0\u00fc\u00ab\u03a6\u00bd\u00fb\u00b5\u00fb\u00e7\u03c0\u00e9\u00c6\u03c0\u00fc\u00f2\u03c0\u00fc\u00ee\u03c0\u00fc\u00d6 \u03c3\u00f1\u00ba\u03c3\u00a1\u00aa\u03c3\u00a2\u2502\u00b5\u00a2\u2555\u0398\u00f1\u00bf\u03c0\u00fc\u00ab\u00b5\u00a3\u00bc\u03c0\u00e9\u00c6\u03c0\u00fc\u00f2\u03c0\u00fc\u00ee\u03c0\u00fc\u00d6 \u00b5\u00f9\u00d1\u00b5\u00a3\u00bc\u03c0\u00fc\u00ab\u03c3\u00ec\u00dc\u03c3\u00fa\u00bd\u03a6\u00bd\u00fb\u00b5\u00fb\u00e7\u03c0\u00e9\u00c6\u03c0\u00fc\u00f2\u03c0\u00fc\u00ee\u03c0\u00fc\u00d6 \u00b5\u00fb\u2591\u03a6\u00aa\u00c5\u03c4\u00d6\u2557\u0398\u00ee\u2593 \u03c0\u00e2\u00a1\u03c0\u00e9\u2591\u03c0\u00e9\u00f1\u03c0\u00e2\u2502 English \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 \u03c0\u00fc\u00d6\u03c0\u00fc\u2563\u03c0\u00fc\u00aa \u00b5\u00a3\u00bc\u00b5\u00fb\u00e7\u03c0\u00fc\u00e9\u03c0\u00e9\u00e8 \u03c0\u00fc\u00d6\u03c0\u00fc\u2563\u03c0\u00fc\u00aa \u00b5\u00a3\u00bc\u00b5\u00fb\u00e7\u03c0\u00fc\u00e9\u03c0\u00e9\u00e8 \u0398\u00fb\u00eb\u03c0\u00fc\u00ff\u03c0\u00e9\u00ef \u03c0\u00e9\u2510\u03c0\u00e9\u00f1\u03c0\u00e2\u00ea\u03c0\u00e2\u00bd \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0\u03c3\u00c9\u00ec \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0ID \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0 \u00b5\u00eb\u00c7\u03c3\u2592\u20a7 \u03c3\u00ea\u00e8\u03a6\u00ed\u00ee\u03c4\u00eb\u2310\u03c3\u00c9\u00ec ISSN \u03c3\u2556\u2557\u03c3\u00c5\u2556\u03c0\u00e2\u00dc\u03c0\u00e2\u255d\u03c0\u00e9\u2555 \u03c3\u00e7\u2551\u03c4\u00eb\u00ea\u03a6\u00c7\u00e0 \u03c3\u00c5\u00e9\u03a6\u00c7\u00e2\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab \u03c3\u00e7\u2551\u03c4\u00eb\u00ea\u03c3\u2563\u2524 \u03c3\u2563\u2524\u03c0\u00fc\u00ef\u03c0\u00e9\u00eb \u03c3\u2563\u2524\u03c0\u00fc\u255b\u03c0\u00fc\u00ba \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 CiNii\u03c4\u00ac\u00f4\u03c3\u00c5\u00fa \u00b5\u00d1\u00a1\u03c3\u00ef\u00d6\u03c0\u00fc\u00ab\u03c3\u00e5\u00ec\u0398\u00fb\u00ef\u03c0\u00fc\u00bd\u03c0\u00fc\u00f1\u03c0\u00fc\u00e4\u03c0\u00fc\u00aa Software Engineering : R&D trends and defense needs BOEHM BW \u03a6\u00f3\u00bd\u03c3\u255d\u00f2\u03c4\u00f6\u00bf \u00b5\u00fb\u00e7\u03c4\u00ee\u00ab: 1\u03a3\u2557\u2562 \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0 BOEHM BW \u03c3\u00c5\u00c4\u0398\u00ee\u2593\u03c3\u00ea\u00e8\u03a6\u00ed\u00ee\u03c4\u00eb\u2310 Research Directions in Software Technology Research Directions in Software Technology, 1979 MIT Press \u03a6\u00f3\u00bd\u03c3\u255d\u00f2\u03c4\u00f6\u00bf\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab: 1\u03a3\u2557\u2562\u03a3\u2555\u00a1 1-1\u03a3\u2557\u2562\u03c0\u00e9\u00c6 \u03a6\u00ed\u00bf\u03c4\u00f1\u2551 1 FA\u03c0\u00e2\u00ec\u03c0\u00e2\u00e2\u03c0\u00e2\u00ea\u03c0\u00e2\u00bb\u03c0\u00e2\u255d\u03c0\u00e9\u00bb\u03c0\u00fc\u00bf\u03c0\u00e9\u2556\u03c0\u00e2\u255d\u03c0\u00e9\u2592\u03c0\u00e2\u2502\u03c0\u00e9\u2563\u03c3\u00ea\u2562\u03c3\u255b\u00ed\u03c0\u00fc\u00bd\u03c0\u00fc\u00e8\u03c0\u00fc\u00e6\u03c0\u00e9\u00ef\u03c4\u00e1\u00f6\u03c4\u2310\u2562\u03a6\u00ac\u2510\u00b5\u0192\u2557 \u03a3\u2555\u00c7\u0398\u00dc\u00c4 \u03a6\u00eb\u00bb\u03c4\u0192\u00d1 , \u03a6\u00fb\u00aa\u03c4\u00f6\u2591 \u00b5\u00e5\u2593\u03a3\u2563\u00e0 \u0398\u00a2\u2557\u00b5\u2591\u00f9\u03c3\u00a1\u00aa\u03a3\u255d\u00dc\u03c4\u00e1\u00f6\u03c4\u2310\u2562\u03a3\u255d\u00dc\u03a6\u2502\u00e7\u00b5\u00fb\u00d6. SC, \u03c0\u00e9\u2556\u03c0\u00e9\u2563\u03c0\u00e2\u00e5\u03c0\u00e2\u00e1\u03c3\u00ea\u2562\u03c3\u255b\u00ed\u03c4\u00e1\u00f6\u03c4\u2310\u2562\u03a3\u255d\u00dc 2001(31), 1-4, 2001-11-14 \u03c3\u00c5\u00e9\u03a6\u00c7\u00e2\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab52\u03a3\u2557\u2562 Tweet \u03c3\u00c9\u00e4\u03c4\u00bf\u00ab\u03c0\u00e9\u2502\u03c0\u00e2\u255d\u03c0\u00e2\u00eb NII\u03a6\u00bd\u00fb\u00b5\u00fb\u00e7ID(NAID) 10018983637 \u03a6\u2502\u00e7\u00b5\u00fb\u00d6\u03c4\u00bf\u00ab\u03c3\u00ea\u00d1 \u03c3\u00a2\u2502\u00b5\u00a2\u2555\u03c3\u00e0\u00bf\u03a3\u255c\u00f4 \u03c0\u00e2\u00e7\u03c0\u00e2\u255d\u03c0\u00e9\u2510\u00b5\u00c5\u00c9\u03a3\u255b\u00a2\u03c3\u00e0\u00e2 CJP\u03c3\u255d\u00f2\u03c4\u00f6\u00bf \u00b5\u00a2\u2555\u03c0\u00fc\u00ec\u03c3\u00e7\u2551\u03c0\u00fc\u00f9 RefWorks\u03c0\u00fc\u00bd\u00b5\u00a2\u2555\u03c0\u00fc\u00ec\u03c3\u00e7\u2551\u03c0\u00fc\u00f9 /| \u0393\u00c7\u00aa", "num_citations": "180\n", "authors": ["5"]}
{"title": "A collaborative spiral software process model based on theory W\n", "abstract": " A primary difficulty in applying the spiral model has been the lack of explicit process guidance in determining the prospective system's objectives, constraints, and alternatives that get elaborated in each cycle. This paper presents an extension of the spiral model, called the Next Generation Process Model (NGPM), which uses the Theory W(win-win) approach (Boehm-Ross, 1989) to converge on a system's next-level objectives, constraints, and alternatives. The refined Spiral Model explicitly addresses the need for concurrent analysis, risk resolution definition, and elaboration of both the software product and the software process in a collaborative manner. This paper also describes some of the key elements of the support system developed based on the model and refined through experiments with it. It reports on experiences in applying NGPM to a large Department of Defense program.< >", "num_citations": "174\n", "authors": ["5"]}
{"title": "The high cost of software\n", "abstract": " CiNii \u03a6\u00bd\u00fb\u00b5\u00fb\u00e7 - The high cost of software CiNii \u03c3\u00a2\u255c\u03c4\u00bd\u00ef\u00b5\u00e2\u00e0\u03c3\u00e1\u2592\u03c3\u00a1\u00aa\u03c4\u00e1\u00f6\u03c4\u2310\u2562\u00b5\u00eb\u00c7 \u03c3\u00a1\u00aa\u03a6\u00ed\u00f4\u00b5\u00e2\u00e0\u03c3\u00e1\u2592\u03c0\u00e2\u00e8\u03c0\u00e2\u00f4\u03c0\u00e9\u2593\u03c0\u00e2\u255d\u03c0\u00e9\u2510[\u03c0\u00e9\u2561\u03c0\u00e9\u00f1\u03c0\u00e2\u00ef\u03c0\u00e9\u00fa] \u00b5\u00f9\u00d1\u00b5\u00a3\u00bc\u03c0\u00fc\u00ab\u03a6\u00bd\u00fb\u00b5\u00fb\u00e7\u03c0\u00e9\u00c6\u03c0\u00fc\u00f2\u03c0\u00fc\u00ee\u03c0\u00fc\u00d6 \u03c3\u00f1\u00ba\u03c3\u00a1\u00aa\u03c3\u00a2\u2502\u00b5\u00a2\u2555\u0398\u00f1\u00bf\u03c0\u00fc\u00ab\u00b5\u00a3\u00bc\u03c0\u00e9\u00c6\u03c0\u00fc\u00f2\u03c0\u00fc\u00ee\u03c0\u00fc\u00d6 \u00b5\u00f9\u00d1\u00b5\u00a3\u00bc\u03c0\u00fc\u00ab\u03c3\u00ec\u00dc\u03c3\u00fa\u00bd\u03a6\u00bd\u00fb\u00b5\u00fb\u00e7\u03c0\u00e9\u00c6\u03c0\u00fc\u00f2\u03c0\u00fc\u00ee\u03c0\u00fc\u00d6 \u00b5\u00fb\u2591\u03a6\u00aa\u00c5\u03c4\u00d6\u2557\u0398\u00ee\u2593 \u03c0\u00e2\u00a1\u03c0\u00e9\u2591\u03c0\u00e9\u00f1\u03c0\u00e2\u2502 English \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 \u03c0\u00fc\u00d6\u03c0\u00fc\u2563\u03c0\u00fc\u00aa \u00b5\u00a3\u00bc\u00b5\u00fb\u00e7\u03c0\u00fc\u00e9\u03c0\u00e9\u00e8 \u03c0\u00fc\u00d6\u03c0\u00fc\u2563\u03c0\u00fc\u00aa \u00b5\u00a3\u00bc\u00b5\u00fb\u00e7\u03c0\u00fc\u00e9\u03c0\u00e9\u00e8 \u0398\u00fb\u00eb\u03c0\u00fc\u00ff\u03c0\u00e9\u00ef \u03c0\u00e9\u2510\u03c0\u00e9\u00f1\u03c0\u00e2\u00ea\u03c0\u00e2\u00bd \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0\u03c3\u00c9\u00ec \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0ID \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0\u00b5\u00eb\u00c7\u03c3\u2592\u20a7 \u03c3\u00ea\u00e8\u03a6\u00ed\u00ee\u03c4\u00eb\u2310\u03c3\u00c9\u00ec ISSN \u03c3\u2556\u2557\u03c3\u00c5\u2556 \u03c0\u00e2\u00dc\u03c0\u00e2\u255d\u03c0\u00e9\u2555 \u03c3\u00e7\u2551\u03c4\u00eb\u00ea\u03a6\u00c7\u00e0 \u03c3\u00c5\u00e9\u03a6\u00c7\u00e2\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab \u03c3\u00e7\u2551\u03c4\u00eb\u00ea\u03c3\u2563\u2524 \u03c3\u2563\u2524\u03c0\u00fc\u00ef\u03c0\u00e9\u00eb \u03c3\u2563\u2524\u03c0\u00fc\u255b\u03c0\u00fc\u00ba \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 CiNii\u03c4\u00ac\u00f4\u03c3\u00c5\u00fa\u00b5\u00d1\u00a1\u03c3\u00ef\u00d6\u03c0\u00fc\u00ab\u03c3\u00e5\u00ec\u0398\u00fb\u00ef\u03c0\u00fc\u00bd\u03c0\u00fc\u00f1\u03c0\u00fc\u00e4\u03c0\u00fc\u00aa The high cost of software BOEHM BW \u03a6\u00f3\u00bd\u03c3\u255d\u00f2\u03c4\u00f6\u00bf\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab: 1\u03a3\u2557\u2562 \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0 BOEHM BW \u03c3\u00c5\u00c4\u0398\u00ee\u2593\u03c3\u00ea\u00e8\u03a6\u00ed\u00ee\u03c4\u00eb\u2310 Practical Strategies for Developing Large Software Systems Practical Strategies for Developing Large Software Systems, 1975 Addison-Wesley Publishing Company \u03a6\u00f3\u00bd\u03c3\u255d\u00f2\u03c4\u00f6\u00bf\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab: 1\u03a3\u2557\u2562\u03a3\u2555\u00a1 1-1\u03a3\u2557\u2562\u03c0\u00e9\u00c6 \u03a6\u00ed\u00bf\u03c4\u00f1\u2551 1 \u00b5\u00c4\u00f3\u03c4\u2524\u00f3\u03c0\u00e2\u00f3\u03c0\u00e2\u00e7\u03c0\u00e2\u00bd\u03c0\u00e9\u00c6\u03c4\u00f6\u00bf\u03c0\u00fc\u00e4\u03c0\u00fc\u0192\u03c0\u00e9\u255c\u03c0\u00e2\u00f2\u03c0\u00e2\u00ea\u03c0\u00e9\u00aa\u03c0\u00e9\u00ba\u03c0\u00e9\u00f3\u03c0\u00e2\u00e5\u03c0\u00e9\u2563\u03c0\u00e2\u00ea\u03c0\u00fc\u00ab\u03a6\u2310\u00f2\u03a3\u255b\u00ed\u03c0\u00e2\u00f3\u03c0\u00e2\u00e7\u03c0\u00e2\u00bd(\u03a3\u2510\u00ed\u0398\u00e1\u255d\u00b5\u00c7\u00ba\u03c0\u00e9\u00c6\u00b5\u2555\u00bc\u03c0\u00e9\u00ef) \u00b5\u00a5\u255b\u03c3\u2591\u255b\u03a6\u2591\u2556 \u03c3\u255b\u2563 \u00b5\u00f9\u00d1\u00b5\u00a3\u00bc\u03a3\u2510\u00ed\u0398\u00e1\u255d\u00b5\u00c7\u00ba\u03c3\u00a1\u00aa\u03a3\u255d\u00dc\u03a6\u00ac\u00ee \u03a3\u2510\u00ed\u0398\u00e1\u255d\u00b5\u00c7\u00ba 27(3), 160-169, 2005 \u03c3\u00c5\u00e9\u03a6\u00c7\u00e2\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab29\u03a3\u2557\u2562 Tweet \u03c3\u00c9\u00e4\u03c4\u00bf\u00ab\u03c0\u00e9\u2502\u03c0\u00e2\u255d\u03c0\u00e2\u00eb NII\u03a6\u00bd\u00fb\u00b5\u00fb\u00e7ID(NAID) 10016794713 \u03a6\u2502\u00e7\u00b5\u00fb\u00d6\u03c4\u00bf\u00ab\u03c3\u00ea\u00d1 \u03c3\u00a2\u2502\u00b5\u00a2\u2555\u03c0\u00fc\u00ab\u03a3\u2555\u00c7\u0398\u00e2\u00bf \u03c0\u00e2\u00e7\u03c0\u00e2\u255d\u03c0\u00e9\u2510\u00b5\u00c5\u00c9\u03a3\u255b\u00a2\u03c3\u00e0\u00e2 CJP\u03c3\u255d\u00f2\u03c4\u00f6\u00bf \u00b5\u00a2\u2555\u03c0\u00fc\u00ec\u03c3\u00e7\u2551\u03c0\u00fc\u00f9 RefWorks\u03c0\u00fc\u00bd\u00b5\u00a2\u2555\u03c0\u00fc\u00ec\u03c3\u00e7\u2551\u03c0\u00fc\u00f9 EndNote\u03c0\u00fc\u00bd\u00b5\u00a2\u2555\u03c0\u00fc\u00ec\u03c3\u00e7\u2551\u03c0\u00fc\u00f9 \u03c0\u00fc\u00bd/\u0393\u00c7\u00aa", "num_citations": "170\n", "authors": ["5"]}
{"title": "The spiral model as a tool for evolutionary acquisition\n", "abstract": " Since its original publication [1], the spiral development model diagrammed in Figure 1 has been used successfully in many defense and commercial projects. To extend this base of success, the Department of Defense (DoD) has recently rewritten the defense acquisition regulations to incorporate \u0393\u00c7\u00a3evolutionary acquisition,\u0393\u00c7\u00a5 an acquisition strategy designed to mesh well with spiral development. In particular, DoD Instruction 5000.2 subdivides acquisition [2]:\u0393\u00c7\u00a3There are two... approaches, evolutionary and single step to full capability. An evolutionary approach is preferred.\u0393\u00c7\u00aa[In this] approach, the ultimate capability delivered to the user is divided into two or more blocks, with increasing increments of capability.\u0393\u00c7\u00a5(p. 20) Here, a block corresponds to a single product release. The text goes on to specify the use of spiral development within blocks:", "num_citations": "164\n", "authors": ["5"]}
{"title": "Finding the right data for software cost modeling\n", "abstract": " Good software cost models can significantly help software project managers. With good models, project stakeholders can make informed decisions about how to manage resources, how to control and plan the project, or how to deliver the project on time, on schedule, and on budget. Real-world data sets, such as those coming from software engineering projects, often contain noisy, irrelevant, or redundant variables. We propose that cost modelers should perform data-pruning experiments after data collection and before model building. Such pruning experiments are simple and fast.", "num_citations": "163\n", "authors": ["5"]}
{"title": "Megaprogramming\n", "abstract": " Publication: CSC'94: Proceedings of the 22nd annual ACM computer science conference on Scaling up: meeting the challenge of complexity in real-world computing applications: meeting the challenge of complexity in real-world computing applications March 1994 https://doi. org/10.1145/197530.197674", "num_citations": "159\n", "authors": ["5"]}
{"title": "Seven basic principles of software engineering\n", "abstract": " This paper attempts to distill the large number of individual aphorisms on good software engineering into a small set of basic principles. Seven principles have been determined which form a reasonably independent and complete set. These are: 1.(1) manage using a phased life-cycle plan.2.(2) perform continuous validation.3.(3) maintain disciplined product control.4.(4) use modern programming practices.5.(5) maintain clear accountability for results.6.(6) use better and fewer people.7.(7) maintain a commitment to improve the process. The overall rationale behind this set of principles is discussed, followed by a more detailed discussion of each of the principles.", "num_citations": "149\n", "authors": ["5"]}
{"title": "On the definition of software system architecture\n", "abstract": " Although several defi nitions of \u0393\u00c7\u00a3software architecture\u0393\u00c7\u00a5 have been presented, none of them to date enable a reviewer confronted with a complex of diagrams and symbols to determine whether it is an architecture for a system or not. We present a defi nition of \u0393\u00c7\u00a3software system architecture\u0393\u00c7\u00a5 which provides a set of criteria for making this determination. It is based on making the architectural rationale a fi rst-class citizen in the defi nition, and on requiring the rationale to ensure that the architecture\u0393\u00c7\u00d6s components, connections, and constraints defi ne a system that will satisfy a set of defi ned stakeholder needs for the system.", "num_citations": "148\n", "authors": ["5"]}
{"title": "A software product line life cycle cost estimation model\n", "abstract": " Most software product line cost estimation models are calibrated only to local product line data rather than to a broad range of product lines. They also underestimate the return on investment for product lines by focusing only on development vs. life-cycle savings, and by applying writing-for-reuse surcharges to the entire product rather that to the portions of the product being reused. This paper offers some insights based on the exploratory development and collaborative refinement of a software product line life cycle economics model, the Constructive Product Line Investment Model (COPLIMO) that addresses these shortfalls. COPLIMO consists of two components: a product line development cost model and an annualized post-development life cycle extension. It focuses on modeling the portions of the software that involve product-specific newly-built software, fully reused black-box product line components, and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "145\n", "authors": ["5"]}
{"title": "Achievements and challenges in cocomo-based software resource estimation\n", "abstract": " This article summarizes major achievements and challenges of software resource estimation over the last 40 years, emphasizing the Cocomo suite of models. Critical issues that have enabled major achievements include the development of good model forms, criteria for evaluating models, methods for integrating expert judgment and statistical data analysis, and processes for developing new models that cover new software development approaches. The article also projects future trends in software development and evolution processes, along with their implications and challenges for future software resource estimation capabilities.", "num_citations": "137\n", "authors": ["5"]}
{"title": "A SLOC counting standard\n", "abstract": " Source Lines of Code (SLOC or LOC) is one of the most widely used sizing metrics in industry and literature. It is the key input for most of major cost estimation models such as COCOMO, SLIM, and SEER-SEM. Although the SEI and the IEEE have established SLOC definitions and guidelines to standardize counting practice, inconsistency in SLOC measurements still exists in industry and research. This problem causes the incomparability of SLOC metric among organizations and the inaccuracy of cost estimation. This report presents a set of counting standards that defines what and how to count SLOC. Our experience with the development and use of the USC CodeCount\u0393\u00e4\u00f3 toolset, a popular utility that automates the SLOC counting process, suggests that this problem can be alleviated by the use of a reasonable and unambiguous counting standard guide and with the support of a configurable counting tool.", "num_citations": "131\n", "authors": ["5"]}
{"title": "COCOTS: A COTS software integration lifecycle cost model-model overview and preliminary data collection findings\n", "abstract": " As the use of commercial-of-the-shelf (COTS) components becomes ever more prevalent in the creation of large software systems, the need for the ability to reasonably predict the true lifetime cost of using such software components grows accordingly. In using COTS components, immediate short-term gains in direct development effort & schedule are possible, but usually as a trade-off for a more complicated long-term postdeployment maintenance environment. In addition, there are risks associated with COTS software separate from those of creating components from scratch. These unique risks can further complicate the development and postdeployment situations. This paper discusses a model being developed as an extension of the COCOMO II [1] cost model. COCOTS attempts to predict the lifecycle costs of using COTS components by capturing the more significant COTS risks in its modeling parameters. The current state of the model is presented, along with some preliminary findings suggested by an analysis of calibration data collected to date. The paper concludes with a discussion of the on-going effort to further refine the accuracy and scope of COCOTS.", "num_citations": "127\n", "authors": ["5"]}
{"title": "A software development environment for improving productivity\n", "abstract": " A major effort at improving productivity at TRW led to the creation of the software productivity project, or SPP, in 1981. The major thrust of this project is the establishment of a software development environment to support proje'ctactivities; this environment is calledthe software productivity system, or SPS. It involves a set of strategies, including the work environment; the evaluation and procurement of hardware equipment; the provision for immediate access to computing resources through local area networks; the building of an integrated set of tools to support the software development life cycle and all project personnel; and a user support function to transfer new technology. All of these strategies are being accomplished incrementally. The current architecture is Vax-based and uses the Unix operating system, a wideband local network, and a set of software tools.", "num_citations": "127\n", "authors": ["5"]}
{"title": "Using the incremental commitment model to integrate system acquisition, systems engineering, and software engineering\n", "abstract": " One of the top recommendations to emerge from the October 2006 Deputy Under Secretary of Defense (DUSD) Acquisition, Technology, and Logistics (ATL) Defense Software Strategy Summit was to find ways of better integrating software engineering into the systems engineering and acquisition process. Concurrently, the National Research Council (NRC) study was addressing the problem of better integrating human factors into the systems engineering and acquisition process. This article presents a model that emerged from these and related efforts that shows promise of improving integrations. This model, called the Incremental Commitment Model (ICM), organizes systems engineering and acquisition processes in ways that better accommodate the different strengths and difficulties of hardware, software, and human factors of engineering approaches. It also provides points at which they can synchronize and stabilize, and at which their risks of going forward can be better assessed and fitted into a risk-driven stakeholder resource commitment process.", "num_citations": "125\n", "authors": ["5"]}
{"title": "The COCOMO 2.0 software cost estimation model\n", "abstract": " CiNii \u03a6\u00bd\u00fb\u00b5\u00fb\u00e7 - The COCOMO 2.0 software cost estimation model CiNii \u03c3\u00a2\u255c\u03c4\u00bd\u00ef\u00b5\u00e2\u00e0\u03c3\u00e1\u2592\u03c3\u00a1\u00aa\u03c4\u00e1\u00f6\u03c4\u2310\u2562\u00b5\u00eb\u00c7 \u03c3\u00a1\u00aa\u03a6\u00ed\u00f4 \u00b5\u00e2\u00e0\u03c3\u00e1\u2592\u03c0\u00e2\u00e8\u03c0\u00e2\u00f4\u03c0\u00e9\u2593\u03c0\u00e2\u255d\u03c0\u00e9\u2510[\u03c0\u00e9\u2561\u03c0\u00e9\u00f1\u03c0\u00e2\u00ef\u03c0\u00e9\u00fa] \u00b5\u00f9\u00d1\u00b5\u00a3\u00bc\u03c0\u00fc\u00ab\u03a6\u00bd\u00fb\u00b5\u00fb\u00e7\u03c0\u00e9\u00c6\u03c0\u00fc\u00f2\u03c0\u00fc\u00ee\u03c0\u00fc\u00d6 \u03c3\u00f1\u00ba\u03c3\u00a1\u00aa\u03c3\u00a2\u2502\u00b5\u00a2\u2555\u0398\u00f1\u00bf\u03c0\u00fc\u00ab\u00b5\u00a3\u00bc\u03c0\u00e9\u00c6\u03c0\u00fc\u00f2\u03c0\u00fc\u00ee\u03c0\u00fc\u00d6 \u00b5\u00f9\u00d1\u00b5\u00a3\u00bc\u03c0\u00fc\u00ab\u03c3\u00ec\u00dc\u03c3\u00fa\u00bd\u03a6\u00bd\u00fb\u00b5\u00fb\u00e7\u03c0\u00e9\u00c6\u03c0\u00fc\u00f2\u03c0\u00fc\u00ee\u03c0\u00fc\u00d6 \u00b5\u00fb\u2591\u03a6\u00aa\u00c5\u03c4\u00d6\u2557\u0398\u00ee\u2593 \u03c0\u00e2\u00a1\u03c0\u00e9\u2591\u03c0\u00e9\u00f1\u03c0\u00e2\u2502 English \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 \u03c0\u00fc\u00d6\u03c0\u00fc\u2563\u03c0\u00fc\u00aa \u00b5\u00a3\u00bc\u00b5\u00fb\u00e7\u03c0\u00fc\u00e9\u03c0\u00e9\u00e8 \u03c0\u00fc\u00d6\u03c0\u00fc\u2563\u03c0\u00fc\u00aa \u00b5\u00a3\u00bc\u00b5\u00fb\u00e7\u03c0\u00fc\u00e9\u03c0\u00e9\u00e8 \u0398\u00fb\u00eb\u03c0\u00fc\u00ff\u03c0\u00e9\u00ef \u03c0\u00e9\u2510\u03c0\u00e9\u00f1\u03c0\u00e2\u00ea\u03c0\u00e2\u00bd \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0\u03c3\u00c9\u00ec \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0ID \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0 \u00b5\u00eb\u00c7\u03c3\u2592\u20a7 \u03c3\u00ea\u00e8\u03a6\u00ed\u00ee\u03c4\u00eb\u2310\u03c3\u00c9\u00ec ISSN \u03c3\u2556\u2557\u03c3\u00c5\u2556\u03c0\u00e2\u00dc\u03c0\u00e2\u255d\u03c0\u00e9\u2555 \u03c3\u00e7\u2551\u03c4\u00eb\u00ea\u03a6\u00c7\u00e0 \u03c3\u00c5\u00e9\u03a6\u00c7\u00e2\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab \u03c3\u00e7\u2551\u03c4\u00eb\u00ea\u03c3\u2563\u2524 \u03c3\u2563\u2524\u03c0\u00fc\u00ef\u03c0\u00e9\u00eb \u03c3\u2563\u2524\u03c0\u00fc\u255b\u03c0\u00fc\u00ba \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 CiNii\u03c4\u00ac\u00f4\u03c3\u00c5\u00fa \u00b5\u00d1\u00a1\u03c3\u00ef\u00d6\u03c0\u00fc\u00ab\u03c3\u00e5\u00ec\u0398\u00fb\u00ef\u03c0\u00fc\u00bd\u03c0\u00fc\u00f1\u03c0\u00fc\u00e4\u03c0\u00fc\u00aa The COCOMO 2.0 software cost estimation model BOEHM Barry \u03a6\u00f3\u00bd\u03c3\u255d\u00f2\u03c4\u00f6\u00bf \u00b5\u00fb\u00e7\u03c4\u00ee\u00ab: 1\u03a3\u2557\u2562 \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0 BOEHM Barry \u03c3\u00c5\u00c4\u0398\u00ee\u2593\u03c3\u00ea\u00e8\u03a6\u00ed\u00ee\u03c4\u00eb\u2310 American Programmer American Programmer, 1996 \u03a6\u00f3\u00bd\u03c3\u255d\u00f2\u03c4\u00f6\u00bf\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab: 1\u03a3\u2557\u2562\u03a3\u2555\u00a1 1-1\u03a3\u2557\u2562\u03c0\u00e9\u00c6 \u03a6\u00ed\u00bf\u03c4\u00f1\u2551 1 \u03c4\u2561\u00e4\u03a6\u255b\u255d\u03c0\u00fc\u2510\u03c0\u00e9\u255c\u03c0\u00e2\u00f2\u03c0\u00e2\u00ea\u03c0\u00e9\u00aa\u03c0\u00e9\u00ba\u03c0\u00e9\u00f3\u00b5\u00f6\u2563\u0398\u00c7\u00e1\u00b5\u00d6\u00e9\u03c0\u00fc\u00ab\u03a3\u255c\u00a3\u00b5\u00d1\u00a1\u0398\u00e0\u00ec\u03c3\u00ea\u00e5\u03c0\u00e9\u00c6\u03c3\u00ab\u2563\u00b5\u00ff\u00f4\u03c0\u00fc\u00bd\u03c0\u00fc\u00d6\u03c0\u00e9\u00ef\u03c3\u2591\u00c5\u03a6\u00aa\u00c5\u00b5\u00bf\u00ed\u03c0\u00fc\u00ac\u00b5\u00f6\u2563\u0398\u00c7\u00e1 \u03c3\u2556\u00d1\u00b5\u00f2\u2591\u03c0\u00fc\u00ab\u03a6\u00aa\u00ef\u03c4\u2310\u00ec\u03c0\u00e9\u00e9\u03c0\u00e9\u00e8\u03c3\u2591\u2551\u03c3\u2551\u00aa\u03c0\u00fc\u00ab\u00b5\u00c5\u00c9\u00b5\u00ed\u00ea \u0398\u00ff\u00ac\u03a3\u2551\u00f2 \u03a6\u00ac\u00e1 , \u03a3\u2563\u00e0\u03a3\u2510\u00a5\u03c4\u00f6\u2591 \u03c4\u00a2\u00e8\u03c3\u00c5\u2593 , \u00b5\u00a5\u255b\u00b5\u00a3\u00bc \u03c3\u00fc\u00d1\u03a3\u2555\u00c7 , \u0398\u2502\u00d1\u03c3\u2592\u00e0 \u03c3\u00ab\u00c5\u00b5\u00bc\u00ed \u0398\u00a2\u2557\u03c3\u00a1\u00c9\u00b5\u00e2\u00e0\u03c3\u00e1\u2592\u0398\u00c7\u00dc\u03a3\u2510\u00ed\u03c3\u00a1\u00aa\u03a3\u255d\u00dc\u00b5\u00e8\u00c7\u03a6\u00ed\u00f4 \u03c4\u00e1\u00f6\u03c4\u2310\u2562\u03c3\u00e1\u2592\u03c3\u00e6\u00e8. SS, \u03c0\u00e9\u255c\u03c0\u00e2\u00f2\u03c0\u00e2\u00ea\u03c0\u00e9\u00aa\u03c0\u00e9\u00ba\u03c0\u00e9\u00f3\u03c0\u00e9\u2561\u03c0\u00e9\u00f1\u03c0\u00e9\u00bf\u03c0\u00e2\u2502\u03c0\u00e9\u2563 98(675), 39-46, 1999-03-18 \u03c3\u00c5\u00e9\u03a6\u00c7\u00e2\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab15\u03a3\u2557\u2562 \u03c3\u00f1\u00ba\u03c3\u00a1\u00aa\u03c3\u00e0\u2592\u03c3\u00c9\u00ee\u03c3\u00ea\u2310\u03c4\u00f6\u00bf \u00b5\u2310\u0192\u0398\u00fb\u00f3\u03c0\u00e9\u2556\u03c0\u00e2\u2502\u03c0\u00e2\u00a5\u03c0\u00e9\u2555\u03c0\u00e9\u00aa\u03c0\u00e2\u00e1 Tweet \u03c3\u00c9\u00e4\u03c4\u00bf\u00ab\u03c0\u00e9\u2502\u03c0\u00e2\u255d\u03c0\u00e2\u00eb NII\u03a6\u00bd\u00fb\u00b5\u00fb\u00e7ID(NAID) 10022232477 \u03a6\u2502\u00e7\u00b5\u00fb\u00d6\u03c4\u00bf\u00ab\u03c3\u00ea\u00d1 \u0398\u00a2\u00e6\u03a6\u00ac\u00ee\u03a6\u00bd\u00fb\u00b5\u00fb\u00e7 \u03c0\u00e2\u00e7\u03c0\u00e2\u255d\u03c0\u00e9\u2510\u03c0\u00fc\u00bd/\u0393\u00c7\u00aa", "num_citations": "118\n", "authors": ["5"]}
{"title": "Foundations of empirical software engineering: the legacy of Victor R. Basili\n", "abstract": " Although software engineering can trace its beginnings to a NATO conf-ence in 1968, it cannot be said to have become an empirical science until the 1970s with the advent of the work of Prof. Victor Robert Basili of the University of Maryland. In addition to the need to engineer software was the need to understand software. Much like other sciences, such as physics, chemistry, and biology, software engineering needed a discipline of obs-vation, theory formation, experimentation, and feedback. By applying the scientific method to the software engineering domain, Basili developed concepts like the Goal-Question-Metric method, the Quality-Improvement-Paradigm, and the Experience Factory to help bring a sense of order to the ad hoc developments so prevalent in the software engineering field. On the occasion of Basili\u0393\u00c7\u00d6s 65th birthday, we present this book c-taining reprints of 20 papers that defined much of his work. We divided the 20 papers into 6 sections, each describing a different facet of his work, and asked several individuals to write an introduction to each section. Instead of describing the scope of this book in this preface, we decided to let one of his papers, the keynote paper he gave at the International C-ference on Software Engineering in 1996 in Berlin, Germany to lead off this book. He, better than we, can best describe his views on what is-perimental software engineering.", "num_citations": "117\n", "authors": ["5"]}
{"title": "The agile methods fray\n", "abstract": " Two of software's leading practitioners debate the particulars of implementing agile methods or extreme programming in software development. Agile means investing heavily in individual skill-building rather than organizational rule sets.", "num_citations": "116\n", "authors": ["5"]}
{"title": "The ROI of systems engineering: Some quantitative results for software\u0393\u00c7\u00c9intensive systems\n", "abstract": " This paper presents quantitative results on the return on investment of systems engineering (SE\u0393\u00c7\u00c9ROI) from an analysis of the 161 software projects in the COCOMO II database. The analysis shows that, after normalizing for the effects of other cost drivers, the cost difference between projects doing a minimal job of software systems engineering\u0393\u00c7\u00f6 as measured by the thoroughness of its architecture definition and risk resolution\u0393\u00c7\u00f6 and projects doing a very thorough job was 18% for small projects and 92% for very large software projects as measured in lines of code. The paper also presents applications of these results to project experience in determining \u0393\u00c7\u00ff\u0393\u00c7\u00ffhow much up front systems engineering is enough\u0393\u00c7\u00d6\u0393\u00c7\u00d6 for baseline versions of smaller and larger software projects, for both ROI\u0393\u00c7\u00c9driven internal projects and schedule\u0393\u00c7\u00c9driven outsourced systems of systems projects. \u252c\u2310 2008 Wiley Periodicals, Inc. Syst Eng", "num_citations": "112\n", "authors": ["5"]}
{"title": "How much software quality investment is enough: A value-based approach\n", "abstract": " This article draws on results from the emerging field of value-based software engineering (VBSE). VBSE aims to provide a quantitative approach to questions as how much software quality investment is enough. Based on the COCOMO II cost-estimation model and the COQUALMO quality-estimation model, quantitative risk analysis helps determine when to stop testing software and release the product. Further, we show how the model and approach can assess the relative payoff of value-based testing as compared to value-neutral testing", "num_citations": "106\n", "authors": ["5"]}
{"title": "Agility through discipline: A debate\n", "abstract": " While Beck asserts that agility is only possible through greater discipline on the part of everyone involved, Boehm counters that you don't broaden the definition of \"discipline\" by rejecting parts of it. Yet, from inside extreme programming, it seems that the only way to achieve the desired results is to view the world in 'both-and\" terms instead of \"either-or\" terms.", "num_citations": "104\n", "authors": ["5"]}
{"title": "Applying WinWin to quality requirements: a case study\n", "abstract": " Describes the application of the WinWin paradigm to identify and resolve conflicts in a series of real-client, student-developer digital library projects. The paper is based on a case study of the statistical analysis of 15 projects and an in-depth analysis of one representative project. These analyses focus on the conflict resolution process, stakeholders' roles and their relationships to quality artifacts, and tool effectiveness. We show that stakeholders tend to accept satisfactory rather than optimal resolutions. Users and customers are more proactive in stating win conditions, whereas developers are more active in working toward resolutions. Further, we suggest that knowledge-based automated aids have potential to significantly enhance process effectiveness and efficiency. Finally, we conclude that such processes and tools have theoretical and practical implications in the quest for better software requirements elicitation.", "num_citations": "104\n", "authors": ["5"]}
{"title": "An initial theory of value-based software engineering\n", "abstract": " This chapter presents an initial \u0393\u00c7\u00a34+1\u0393\u00c7\u00a5 theory of value-based software engineering (VBSE). The engine in the center is the stakeholder win-win Theory W, which addresses the questions of \u0393\u00c7\u00a3which values are important?\u0393\u00c7\u00a5 and \u0393\u00c7\u00a3how is success assured?\u0393\u00c7\u00a5 for a given software engineering enterprise. The four additional theories that it draws upon are utility theory (how important are the values?), decision theory (how do stakeholders\u0393\u00c7\u00d6 values determine decisions?), dependency theory (how do dependencies affect value realization?), and control theory (how to adapt to change and control value realization?). After discussing the motivation and context for developing a VBSE theory and the criteria for a good theory, the chapter discusses how the theories work together into a process for defining, developing, and evolving software-intensive systems. It also illustrates the application of the theory to a supply chain system\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "102\n", "authors": ["5"]}
{"title": "A stakeholder win\u0393\u00c7\u00f4win approach to software engineering education\n", "abstract": " We have been applying the stakeholder win\u0393\u00c7\u00f4win approach to software engineering education. The key stakeholders we are trying to simultaneously satisfy are the students; the industry recipients of our graduates; the software engineering community as parties interested in improved practices; and ourselves as instructors and teaching assistants. In order to satisfy the objectives or win conditions of these stakeholders, we have formed a strategic alliance with the USC Libraries to have software engineering student teams work with Library clients to define, develop, and transition USC digital library applications into operational use. This adds another set of key stakeholders: the Library clients of our class projects. This paper summarizes our experience in developing, conducting, and iterating the course. It concludes by evaluating the degree to which we have been able to meet the stakeholder-determined\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "102\n", "authors": ["5"]}
{"title": "Wikiwinwin: A wiki based system for collaborative requirements negotiation\n", "abstract": " Defining requirements is one of the most critical activities in the development of software intensive systems. The EasyWinWin system has been very good in capturing initial requirements involving heterogeneous stakeholders in over 150 client-developer requirements negotiations. However, it has been less easy to use in updating requirements and related information as a project proceeds and adapting to the evolving nature of the requirements. Because our clients are finding that wikis are easier to learn and use, and can organize information in a flexible and updatable manner, we have developed an initial version of a WikiWinWin system as a potential successor to EasyWinWin. We have conducted a case study of WikiWinWin, and the result shows that the initial WikiWinWin is basically good at facilitating stakeholder collaborative negotiation and learning, but has some limitations that we are now addressing.", "num_citations": "100\n", "authors": ["5"]}
{"title": "People factors in software management: lessons from comparing agile and plan-driven methods\n", "abstract": " While methodologies, management techniques, and technical approaches are valuable, a study of agile and plan-driven approaches has confirmed that the most critical success factors are much more likely to be in the realm of people factors. This paper discusses five areas where people issues can have a significant impact: staffing, culture, values, communications, and expectations management.", "num_citations": "100\n", "authors": ["5"]}
{"title": "Software economics: status and prospects\n", "abstract": " Software is valuable when it produces information in a manner that enables people and systems to meet their objectives more effectively. Software engineering techniques have value when they enable software developers to build more valuable software. Software economics is the sub-field of software engineering that seeks improvements which enable software engineers to reason more effectively about important economic aspects of software development, including cost, benefit, risk, opportunity, uncertainty, incomplete knowledge and the value of additional information, implications of competition, and so forth. In this paper, we survey the current status of selected parts of software economics, highlighting the gaps both between practice and theory and between our current understanding and what is needed.The sheer volume of current software costs makes the study and application of software economics techniques a significant area of concern and opportunity. Recent studies [1, 2] estimate roughly 2,000,000 software professionals in the US in 1998. At typical salaries of $60\u0393\u00c7\u00f480,000/year and a typical overhead rate of 150%, this translates into a $300\u0393\u00c7\u00f4400 billion annual expenditure on software development in the US alone. A conservative estimate of worldwide software costs is twice the US costs, or $600\u0393\u00c7\u00f4800 billion per year.", "num_citations": "100\n", "authors": ["5"]}
{"title": "COCOMO suite methodology and evolution\n", "abstract": " Over the years, software managers and software engineers have used various cost models such as the Constructive Cost Model (COCOMO) to support their software cost and estimation processes. These models have also helped them to reason about the cost and schedule implications of their development decisions, investment decisions, client negotiations and requested changes, risk management decisions, and process improvement decisions. Since that time, COCOMO has cultivated a user community that has contributed to its development and calibration. COCOMO has also evolved to meet user needs as the scope and complexity of software system development has grown. This eventually led to the current version of the model: COCOMO II. 2000.3. The growing need for the model to estimate different aspects of software development served as a catalyst for the creation of derivative models and extensions that could better address commercial off-the-shelf software integration, system engineering, and system-of-systems architecting and engineering. This article presents an overview of the models in the COCOMO suite that includes extensions and independent models, and describes the underlying methodologies and the logic behind the models and how they can be used together to support larger software system estimation needs. It concludes with a discussion of the latest University of Southern California Center for Software Engineering effort to unify these various models into a single, comprehensive, user-friendly tool.", "num_citations": "99\n", "authors": ["5"]}
{"title": "The incremental commitment spiral model: Principles and practices for successful systems and software\n", "abstract": " \u0393\u00c7\u00a3The title makes a huge promise: a way to divide commitment into increments that are both meetable (good news for developers) and meaningful (good news for managers and stakeholders). And the book makes good on that promise.\u0393\u00c7\u00a5\u0393\u00c7\u00f4Tom DeMarco, Principal, The Atlantic Systems Guild, author of Peopleware, Deadline, and Slack \u0393\u00c7\u00a3I am seriously impressed with this ICSM book. Besides being conceptually sound, I was amazed by the sheer number of clear and concise characterizations of issues, relationships, and solutions. I wanted to take a yellow highlighter to it until I realized I\u0393\u00c7\u00d6d be highlighting most of the book.\u0393\u00c7\u00a5\u0393\u00c7\u00f4Curt Hibbs, Chief Agile Evangelist, Boeing Use the ICSM to Generate and Evolve Your Life-Cycle Process Assets to Best Fit Your Organization\u0393\u00c7\u00d6s Diverse and Changing Needs Many systems development practitioners find traditional \u0393\u00c7\u00a3one-size-fits-all\u0393\u00c7\u00a5 processes inadequate for the growing complexity, diversity, dynamism, and assurance needs of their products and services. The Incremental Commitment Spiral Model (ICSM) responds with a principle-and risk-based framework for defining and evolving your project and corporate process assets, avoiding pitfalls and disruption, and leveraging opportunities to increase value. This book explains ICSM\u0393\u00c7\u00d6s framework of decision criteria and principles, and shows how to apply them through relevant examples. It demonstrates ICSM\u0393\u00c7\u00d6s potential for reducing rework and technical debt, improving maintainability, handling emergent requirements, and raising assurance levels. Its coverage includes What makes a system development successful ICSM\u0393\u00c7\u00d6s goals, principles, and usage as a process\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "98\n", "authors": ["5"]}
{"title": "Modeling software defect introduction and removal: COQUALMO (COnstructive QUALity MOdel)\n", "abstract": " Cost, schedule and quality are highly correlated factors in software development. They basically form three sides of the same triangle. Beyond a certain point (the \u0393\u00c7\u00a3Quality is Free\u0393\u00c7\u00a5 point), it is difficult to increase the quality without increasing either the cost or schedule or both for the software under development. Similarly, development schedule cannot be drastically compressed without hampering the quality of the software product and/or increasing the cost of development. Watts Humphrey, at the LA SPIN meeting in December'98, highlighted that\" Measuring Productivity without caring about Quality has no meaning\". Software estimation models can (and should) play an important role in facilitating the balance of cost/schedule and quality.Recognizing this important association, an attempt is being made to develop a quality model extension to COCOMO II; namely COQUALMO. An initial description of this model focusing on defect introduction was provided in [Chulani97a]. The model has evolved considerably since then and is now very well defined and calibrated to Delphi-gathered expert opinion. The data collection activity is underway and the aim is to have a statistically calibrated model by the onset of the next millennium.", "num_citations": "97\n", "authors": ["5"]}
{"title": "Escaping the software tar pit: Model clashes and how to avoid them\n", "abstract": " \"No scene from prehistory is quite so vivid as that of the mortal struggles of great beasts in the tar pits\u0393\u00c7\u00aa Large system programming has over the past decade been such a tar pit, and many great and powerful beasts have thrashed violently in it\u0393\u00c7\u00aa\"Everyone seems to have been surprised by the stickiness of the problem, and it is hard to discern the nature of it. But we must try to understand it if we are to solve it.\"Fred Brooks, 1975Several recent books and reports have confirmed that the software tar pit is at least as hazardous today as it was in 1975. Our research into several classes of models used to guide software development (product models, process models, property models, success models), has convinced us that the concept of model clashes among these classes of models helps explain much of the stickiness of the software tar-pit problem.We have been developing and experimentally evolving an approach\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "97\n", "authors": ["5"]}
{"title": "Project termination doesn't equal project failure\n", "abstract": " One of the most frequently cited software project statistics comes from the Standish Group's 1995 Chaos report (http://www.standishgroup.com/visitor/ chaos.htm): \"A staggering 31.1 percent of [software] projects will be canceled before they ever get completed.\" The Chaos report, and numerous documents citing it, label these canceled projects as \"failed\" and imply that all 31.1 percent of them were canceled because of poor software management. This implication is both false and hazardous. It is false because, particularly in an era of rapid change, a lot of software projects are properly started, well managed, and properly terminated before completion because their original assumptions have changed. It is hazardous because it often leaves software managers with the following temptation: \"It's becoming clear that continuing this project will waste company resources. I should probably have the project canceled now\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "95\n", "authors": ["5"]}
{"title": "Composing heterogeneous software architectures\n", "abstract": " A persistent problem in software engineering is how to put complex software systems together out of smaller subsystems, the problem of software composition. The emergence of software architectures and architectural styles has introduced a higher level of abstraction at which we can create and compose software systems. We examine the problem of providing formal semantics to the composition of different architectural styles within software systems, ie the problem of composing heterogeneous architectures. We describe a model of pure styles, and a model of their composition.", "num_citations": "91\n", "authors": ["5"]}
{"title": "Phase distribution of software development effort\n", "abstract": " Effort distribution by phase or activity is an important but often overlooked aspect compared to other steps in the cost estimation process. Poor effort allocation is among the major root causes of rework due to insufficiently resourced early activities. This paper provides results of an empirical study on phase effort distribution data of 75 industry projects, from the China Software Benchmarking Standard Group (CSBSG) database. The phase effort distribution patterns and variation sources are presented, and analysis results show some consistency in effects of software size and team size on code and test phase distribution variations, and some considerable deviations in requirements, design, and transition phases, compared with recommendations in the COCOMO model. Finally, this paper discusses the major findings and threats to validity and presents general guidelines in directing effort allocation. Empirical findings\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "88\n", "authors": ["5"]}
{"title": "Disaggregating and calibrating the CASE tool variable in COCOMO II\n", "abstract": " CASE (computer aided software engineering) tools are believed to have played a critical role in improving software productivity and quality by assisting tasks in software development processes since the 1970s. Several parametric software cost models adopt \"use of software tools\" as one of the environmental factors that affects software development productivity. Several software cost models assess the productivity impacts of CASE tools based only on breadth of tool coverage without considering other productivity dimensions such as degree of integration, tool maturity, and user support. This paper provides an extended set of tool rating scales based on the completeness of tool coverage, the degree of tool integration, and tool maturity/user support. Those scales are used to refine the way in which CASE tools are effectively evaluated within COCOMO (constructive cost model) II. In order to find the best fit of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "88\n", "authors": ["5"]}
{"title": "Safe and simple software cost analysis\n", "abstract": " Simple software cost-analysis meth-ods are readily available, but they aren\u0393\u00c7\u00d6t always safe. The simplest method is to base your cost estimate on the typical costs or productivity rates of your previous projects. That approach will work well if your new project doesn\u0393\u00c7\u00d6t have any cost-critical differences from those previous projects. But it won\u0393\u00c7\u00d6t be safe if some critical cost driver has degraded. Simple history-based software cost-analysis methods would be safer if you could identify which cost driver factors were likely to cause critical cost differences and estimate how much cost difference would result if a critical cost driver changed by a given degree. In this column, I\u0393\u00c7\u00d6ll provide a safe and simple method for doing both of these by using some recently published cost estimating relationships (Software Cost Estimation with COCOMO II, by Barry Boehm et al., Prentice Hall, 2000). COCOMO II is an updated and recalibrated version\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "88\n", "authors": ["5"]}
{"title": "Software requirements negotiation: some lessons learned\n", "abstract": " Negotiating requirements is one of the first steps in any software system life cycle, but its results have probably the most significant impact on the system's value. However, the processes of requirements negotiation are not well understood. We have had the opportunity to capture and analyze requirements negotiation behavior for groups of projects developing library multimedia archive systems, using an instrumented version of the USC WinWin groupware system for requirements negotiation. Some of the more illuminating results were: most stakeholder Win Conditions were noncontroversial (were not involved in issues); negotiation activity varied by stakeholder role; LCO package quality (measured by grading criteria) could be predicted by negotiation attributes; and WinWin increased cooperativeness, reduced friction, and helped focus on key issues.", "num_citations": "88\n", "authors": ["5"]}
{"title": "A survey of agile development methodologies\n", "abstract": " Plan-driven methods are those that begin with the solicitation and documentation of a set of requirements that is as complete as possible. Based on these requirements, one can then formulate a plan of development. Usually, the more complete the requirements, the better the plan. Some examples of plan-driven methods are various waterfall approaches and others such as the Personal Software Process (PSP)[28] and the Rational Unified Process (RUP)[30, 31]. An underlying assumption in plan-driven processes is that the requirements are relatively static. On the other hand, iterative methods, such as spiralmodel based approaches [12, 14], evolutionary processes described in [5, 22, 32, 33], and recently agile approaches [45] count on change and recognize that the only constant is change. The question is only of the degree and the impact of the change. Beginning in the mid-1990\u0393\u00c7\u00d6s, practitioners began finding\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "84\n", "authors": ["5"]}
{"title": "Gaining intellectual control of software development\n", "abstract": " Recent disruptions caused by several events have shown how thor-oughly the world has come to depend on software. The rapid pro-liferation of the Melissa virus hinted at a dark side to the ubiquitous connectivity that supports the information-rich Internet and lets e-commerce thrive. Although the oft-predicted Y2K apocalypse failed to materialize, many software experts insist that disaster was averted only because countries around the globe spent billions to ensure their critical software would be Y2K-compliant. When denial-of-service attacks shut down some of the largest sites on the Web last February, the concerns caused by the disruptions spread far beyond the complaints of frustrated customers, affecting even the stock prices of the targeted sites. Indeed, as software plays an ever-greater role in managing the daily functions of modern life, its economic importance becomes proportionately greater. It\u0393\u00c7\u00d6s no coincidence that technology stocks have led the upsurge of stock market indices, that the US government\u0393\u00c7\u00d6s antitrust case against Microsoft has become headline news around the world, or that some companies\u0393\u00c7\u00d6 aggressive pursuit of software patents has caused widespread controversy. Yet despite its critical importance, software remains surprisingly fragile. Prone to unpredictable performance, dangerously open to malicious attack, and vulnerable to failure at implementation despite the most rigorous development processes, in many cases software has been assigned tasks beyond its maturity and reliability.", "num_citations": "82\n", "authors": ["5"]}
{"title": "A constrained regression technique for COCOMO calibration\n", "abstract": " Building cost estimation models is often considered a search problem in which the solver should return an optimal solution satisfying an objective function. This solution also needs to meet certain constraints. For example, a solution for the estimates coefficients of COCOMO models must be non-negative. In this research, we introduce a constrained regression technique that uses objective functions and constraints to estimate the coefficients of the COCOMO models. To access the performance of the proposed technique, we run a cross-validation procedure and compare the prediction accuracy from different approaches such as least squares, stepwise, Lasso, and Ridge regression. Our result suggests that the regression model that minimizes the sum of relative errors and imposes non-negative coefficients is a favorable technique for calibrating the COCOMO model parameters.", "num_citations": "80\n", "authors": ["5"]}
{"title": "Bridging models across the software lifecycle\n", "abstract": " Numerous notations, methodologies, and tools exist to support software system modeling. While individual models help to clarify certain system aspects, the large number and heterogeneity of models may ultimately hamper the ability of stakeholders to communicate about a system. A major reason for this is the discontinuity of information across different models. In this paper, we present an approach for dealing with that discontinuity. We introduce a set of \u0393\u00c7\u00a3connectors\u0393\u00c7\u00a5 to bridge models, both within and across the \u0393\u00c7\u00a3upstream\u0393\u00c7\u00a5 activities in the software development lifecycle (specifically, requirements, architecture, and design). While the details of these connectors are dependent upon the source and destination models, they share a number of underlying characteristics. These characteristics can be used as a starting point in providing a general understanding of software model connectors. We illustrate our approach by\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "80\n", "authors": ["5"]}
{"title": "Software estimation perspectives\n", "abstract": " November/December 2000 IEEE SOFTWARE 23 number of bees in a hive without understanding the counting rules can produce a pretty useless estimate. The same is true for software: Does an estimate of 100 personmonths for \u0393\u00c7\u00a3software development\u0393\u00c7\u00a5 include analysis and design, integration and test, deployment, management, or uncompensated overtime? If you use the estimate without knowing the answers, you can get yourself into serious trouble.Point 2 highlights the fact that estimates have a number of uses, and you can often get both better and simpler estimates if you keep the use of your estimate in mind. For example, suppose you are doing estimates for a make-or-buy analysis. The vendors\u0393\u00c7\u00d6 quotes for the \u0393\u00c7\u00a3buy\u0393\u00c7\u00a5 option are clustering around $100 K for satisfactory-looking products, and it is looking like the \u0393\u00c7\u00a3make\u0393\u00c7\u00a5 option will be a good deal more expensive. In this case, you can make a simpler and even stronger estimate for the \u0393\u00c7\u00a3make\u0393\u00c7\u00a5 option by using optimistic assumptions about its size, complexity, and staff capability. If even the resulting optimistic \u0393\u00c7\u00a3make\u0393\u00c7\u00a5 estimate comes out at $130,000, you have both saved yourself a good deal of estimating effort and produced a stronger conclusion that the \u0393\u00c7\u00a3buy\u0393\u00c7\u00a5 option is better than even the best-case \u0393\u00c7\u00a3make\u0393\u00c7\u00a5 option.", "num_citations": "77\n", "authors": ["5"]}
{"title": "SimVBSE: Developing a game for value-based software engineering\n", "abstract": " The development of games in aid of improving and enriching a student's learning experience is again on the rise. The beer game (Sterman, 1989) in the field of system dynamics was developed to instill the key principles of production and distribution. SimSE (Navarro and van der Hoek, 2003) provides a simulated game for its players to take on the role of a project manager, and experience the fundamentals of software engineering through cause-effect models. In this paper we present an initial design of SimVBSE as a game for students to better understand value-based software engineering (Boehm, 2005), and its underlying theory (Boehmand Jain, 2005)", "num_citations": "76\n", "authors": ["5"]}
{"title": "Unifying software engineering and systems engineering\n", "abstract": " The author describes CMMI (Capability Maturity Model Integration) and the emerging project methods which demonstrate the opportunities for process improvement gains open to organizations. The organization that changes from separated software and system engineering processes to a more unified approach will find itself far more suited to developing dynamically changing, software-intensive systems. Culture change is never easy, but the alternative is even less palatable.", "num_citations": "74\n", "authors": ["5"]}
{"title": "Heterogeneous view integration and its automation\n", "abstract": " Software systems are characterized by unprecedented complexity. One effective means of dealing with that complexity is to consider a system from a particular perspective, or view (eg, architecture or design diagram). Views enable software developers to reduce the amount of information they have to deal with at any given time. They enable this by utilizing a divide-and-conquer strategy that allows large-scale software development problems to be broken up into smaller, more comprehensible pieces. Individual development issues can then be evaluated without the need of access to the whole body of knowledge about a given software system. The major drawback of views is that development concerns cannot truly be investigated by themselves, since concerns tend to affect one another. Successful and precise product development supported via multiple views requires that common assumptions and definitions\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "74\n", "authors": ["5"]}
{"title": "Value-based processes for COTS-based applications\n", "abstract": " Economic imperatives are changing the nature of software development processes to reflect both the opportunities and challenges of using COTS products. Processes are increasingly moving away from the time-consuming composition of custom software from lines of code (although these processes still apply for developing the COTS products themselves) toward assessment, tailoring, and integration of COTS or other reusable components. Two factors are driving this change: COTS or other reusable components can provide significant user capabilities within limited costs and development time, and more COTS products are becoming available to provide needed user functions.", "num_citations": "73\n", "authors": ["5"]}
{"title": "Value driven security threat modeling based on attack path analysis\n", "abstract": " This paper presents a quantitative threat modeling method, the threat modeling method based on attack path analysis (T-MAP), which quantifies security threats by calculating the total severity weights of relevant attack paths for commercial off the shelf (COTS) systems. Compared to existing approaches, T-MAP is sensitive to an organization's business value priorities and IT environment. It distills the technical details of thousands of relevant software vulnerabilities into management-friendly numbers at a high-level. T-MAP can help system designers evaluate the security performance of COTS systems and analyze the effectiveness of security practices. In the case study, we demonstrate the steps of using T-MAP to analyze the cost-effectiveness of how system patching and upgrades can improve security. In addition, we introduce a software tool that automates the T-MAP", "num_citations": "72\n", "authors": ["5"]}
{"title": "The ROI of software dependability: The iDAVE model\n", "abstract": " In most organizations, proposed investments in software dependability compete for limited resources with proposed investments in software and system functionality, response time, adaptability, speed of development, ease of use, and other system capabilities. The lack of good return-on-investment models for software dependability makes determining the overall business case for dependability investments difficult. So, with a weak business case, investments in software dependability and the resulting system dependability are frequently inadequate. Dependability models will need to support stakeholders in determining their desired levels for each dependability attribute and estimating the cost, value, and ROI for achieving those. At the University of Southern California, researchers have developed software cost- and quality-estimation models and value-based software engineering processes, methods, and tools\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "71\n", "authors": ["5"]}
{"title": "Not all CBS are created equally: COTS-intensive project types\n", "abstract": " COTS products affect development strategies and tactics, but not all CBS development efforts are equal. Based on our experiences with 20 large government and industry CBS projects assessed during our development of the COCOTS estimation model, and our hands-on experience with 52 small e-services CBS projects within USC\u0393\u00c7\u00d6s graduate level software engineering course, we have identi.ed four distinct CBS activity areas: assessment intensive, tailoring intensive, glue-code intensive, and non-COTS intensive. The CBS activity type fundamentally affects the COTS related activity effort and project risks. In this work we define the three COTS activity intensive CBS types and discuss their strategic comparisons based on an empirical study of the spectrum of large and small CBS projects.", "num_citations": "70\n", "authors": ["5"]}
{"title": "A research agenda for systems of systems architecting\n", "abstract": " This paper, documents the activity of a workshop on defining a research agenda for Systems of Systems SoS; Architecting, which was held at USC in October 2006. After two days of invited talks on critical success factors for SoS engineering, the authors of this paper convened for one day to brainstorm topics for the purpose of shaping the near-term research agenda of the newly convened USC Center for Systems and Software Engineering (CSSE). The output from the workshop is a list of ten high-impact items with corresponding research challenges in the context of SoS Architecting. Each item includes a description of the research challenges, its link to contemporary academic or industrial problems and reasons for advocacy of that area. The items were assessed in terms of value and difficulty to determine a prioritisation both for the CSSE's future research agenda and for others in the field.", "num_citations": "69\n", "authors": ["5"]}
{"title": "Software management\n", "abstract": " From the Publisher: Provides both the novice and experienced manager with the necessary materials to understand and use the basic theories, concepts, tools, and techniques of software engineering project management. This long-awaited 5th edition has been thoroughly revised and expanded with three new chapters, 33 new papers, and new case studies providing insight into tools and techniques that work. 550 pp. Pub: 9/97.", "num_citations": "67\n", "authors": ["5"]}
{"title": "Productivity trends in incremental and iterative software development\n", "abstract": " In an investigating study to trace the productivity changes of a commercial software project, which uses incremental and iterative development model, we've found evidence that attributes such as staffing stability, design compatibility/ adaptability to incremental and iterative development, and integration and testing would have significant impact on modifying the productivity outcome - either positive or negative. In this report, we will present an empirical analysis to review, evaluate, and discuss these influential attributes in regard of their correlations with productivity in incremental and iterative software development. We also hope that our approach and results would contribute to initiate more research in this subject area.", "num_citations": "66\n", "authors": ["5"]}
{"title": "An experiment in small-scale application software engineering\n", "abstract": " This paper reports the results of an experiment in applying large-scale software engineering procedures to small software projects. Two USC student teams developed a small, interactive application software product to the same specification, one using Fortran and one using Pascal. Several hypotheses were tested, and extensive experimenal data collected. The major conclusions were as follows.", "num_citations": "66\n", "authors": ["5"]}
{"title": "Educating software engineering students to manage risk\n", "abstract": " In 1996, the University of Southern California (USC) switched its core two-semester software engineering course from a hypothetical-project, homework-and-exam course based on the Bloom taxonomy of educational objectives (knowledge, comprehension, application, analysis, synthesis and evaluation). The revised course is a real-client team-project course based on the CRESST (Center for Research on Evaluation, Standards and Student Testing) model of learning objectives (content understanding, problem solving, collaboration, communication and self-regulation). We used the CRESST cognitive demands analysis to determine the necessary student skills required for software risk management and the other major project activities, and have been refining the approach over the last four years of experience, including revised versions for one-semester undergraduate and graduate project courses at Columbia\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "65\n", "authors": ["5"]}
{"title": "Calibrating software cost models using bayesian analysis\n", "abstract": " The COCOMO II research effort started in 1994 with the aim of updating software cost estimation models, such as the 1981 COnstructive COst MOdel and its 1987 Ada update. Both the earlier models experienced difficulties in estimating software projects of the 90s due to challenges such as non-sequential and rapiddevelopment process models; reuse-driven approaches involving commercial-off-the-shelf (COTS) packages, reengineering, applications composition, and application generation capabilities; object-oriented approaches supported by distributed middleware; software process maturity effects and process-driven quality estimation. The COCOMO II research effort aims at alleviating these problems and is concentrated on developing a model well-suited for the 1990s and then annually updating it for the forthcoming years.The initial definition of the COCOMO II model and its rationale are described in [Boehm95]. The model uses Source Lines of Code and/or Function Points for the sizing parameter, adjusted for reuse and breakage; a set of 17 multiplicative effort multipliers and a set of 5 exponential scale factors. The first calibration was based on a dataset of 83 actual projects collected from Commercial, Aerospace, Government and FFRDC organizations using a 10% weighted average multiple regression approach. It was published in 1997 and became popular as COCOMO II. 1997 [Clark98]. Since then, the COCOMO II database has grown to 161 datapoints. This paper focuses on the Bayesian approach used to calibrate the 1998 version of the model to 161 datapoints. It compares and contrasts the two successive versions, ie the 1997\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "63\n", "authors": ["5"]}
{"title": "Ada COCOMO and the Ada process model\n", "abstract": " Recently, three software development approaches have motivated the development of a revised version of COCOMO the use of the Ada programming language, the use of incremental development, and the use of the Ada process model capitalizing on the strengths of Ada to improve the efficiency of software development. This paper presents the portions of the revised Ada COCOMO dealing with the effects of Ada and the Ada process model. The remainder of this section of the paper discusses the objectives of Ada COCOMO. Section 2 describes the Ada Process Model and its overall effects on software development effort and schedule. Section 3 presents the structure and features of Ada COCOMO, and discusses the rationale behind the changes made from the earlier version of COCOMO called standard COCOMO in the remainder of the paper. Section 4 summarizes the current status of Ada COCOMO, including its calibration to date and its currently available implementations and Section 5 presents the resulting conclusions.Descriptors:", "num_citations": "63\n", "authors": ["5"]}
{"title": "Making a difference in the software century\n", "abstract": " In the 21st century, software engineers face the often formidable challenges of simultaneously dealing with rapid change, uncertainty and emergence, dependability, diversity, and interdependence, but they also have opportunities to make significant contributions that will make a difference for the better.", "num_citations": "62\n", "authors": ["5"]}
{"title": "Integrating collaborative processes and quality assurance techniques: experiences from requirements negotiation\n", "abstract": " Collaboration is essential in many mission-critical activities. Consequently, numerous methods and tools are available supporting collaborative processes such as strategic planning, risk management, requirements definition, and so on. These methods typically emphasize the collaborative, value-creating activities, but there is often less emphasis on quality aspects. Quality assurance (QA) techniques have been wellknown in engineering for a long time, and their effectiveness and efficiency has been empirically evaluated in many domains. In this paper, we propose to integrate repeatable QA techniques and collaborative processes. We evaluate our idea in the context of a collaborative process for requirements negotiation. We propose pre-process techniques to be used before the actual negotiation, in-process techniques for checking quality during a negotiation, as well as post-process inspection techniques\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "60\n", "authors": ["5"]}
{"title": "A quality-based cost estimation model for the product line life cycle\n", "abstract": " Existing SPL cost estimation models [5\u0393\u00c7\u00f49] do not significantly consider the software quality cost, which is spent on removing undetected defects after product release. In general, the future costs for correction of defects undetected at product release consume a large portion of total maintenance costs. The proposed model is recommended, which includes the software quality cost in the SPL business case analysis. qCOPLIMO consists of the following two cost models: Relative Cost of Writing for Reuse (RCWR) for initial product line development and Relative Cost for Reuse (RCR) for the following product development cases.", "num_citations": "57\n", "authors": ["5"]}
{"title": "Developing multimedia applications with the WinWin Spiral Model\n", "abstract": " Fifteen teams recently used the WinWin Spiral Model to perform the system engineering and architecting of a set of multimedia applications for the USC Library Information Systems. Six of the applications were then developed into an Initial Operational Capability. The teams consisted of USC graduate students in computer science. The applications involved extensions of USC's UNIX-based, text-oriented, client-server Library Information System to provide access to various multimedia archives (films, videos, photos, maps, manuscripts, etc.).             Each of the teams produced results which were on schedule and (with one exception) satisfactory to their various Library clients. This paper summarizes the WinWin Spiral Model approach taken by the teams, the experiences of the teams in dealing with project challenges, and the major lessons learned in applying the Model. Overall, the WinWin Spiral Model\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "57\n", "authors": ["5"]}
{"title": "The WinWin approach: using a requirements negotiation tool for rationale capture and use\n", "abstract": " A highly cost-effective approach for rationale capture and management is to provide automated support, and capture the resulting artifacts of the process by which software and system requirements and solutions are negotiated. The WinWin process model, equilibrium model, and collaborative negotiation tool provide capabilities for capturing the artifacts. The MBASE software process model provides an approach for using and updating the rationale artifacts and process to keep it in a win-win state. Supporting requirements negotiation with attaching rationale can have a high impact on all phases of development by enabling much better context for change impact analysis as the increasingly frequent requirements changes arrive. The WinWin approach involves having a system\u0393\u00c7\u00d6s successcritical stakeholders participate in a negotiation process so they can converge on a mutually satisfactory or win-win set of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "56\n", "authors": ["5"]}
{"title": "A controlled experiment in assessing and estimating software maintenance tasks\n", "abstract": " ContextSoftware maintenance is an important software engineering activity that has been reported to account for the majority of the software total cost. Thus, understanding the factors that influence the cost of software maintenance tasks helps maintainers to make informed decisions about their work.ObjectiveThis paper describes a controlled experiment of student programmers performing maintenance tasks on a C++ program. The objective of the study is to assess the maintenance size, effort, and effort distributions of three different maintenance types and to describe estimation models to predict the programmer\u0393\u00c7\u00d6s effort spent on maintenance tasks.MethodTwenty-three graduate students and a senior majoring in computer science participated in the experiment. Each student was asked to perform maintenance tasks required for one of the three task groups. The impact of different LOC metrics on maintenance effort\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "52\n", "authors": ["5"]}
{"title": "Some future software engineering opportunities and challenges\n", "abstract": " This paper provides an update and extension of a 2006 paper, \u0393\u00c7\u00a3Some Future Trends and Implications for Systems and Software Engineering Processes,\u0393\u00c7\u00a5 Systems Engineering, Spring 2006. Some of its challenges and opportunities are similar, such as the need to simultaneously achieve high levels of both agility and assurance. Others have emerged as increasingly important, such as the challenges of dealing with ultralarge volumes of data, with multicore chips, and with software as a service. The paper is organized around eight relatively surprise-free trends and two \u0393\u00c7\u00a3wild cards\u0393\u00c7\u00a5 whose trends and implications are harder to foresee. The eight surprise-free trends are:           1. Increasing emphasis on rapid development and adaptability;           2. Increasing software criticality and need for assurance;           3. Increased complexity, global systems of systems, and need for scalability and interoperability;           4\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "52\n", "authors": ["5"]}
{"title": "Planning Community Information Utilities.\n", "abstract": " Massive social changes are bound to occur with the extension of mass information utilities: the fundamental question is how shall this massive reconstruction of social information power be designed for the best interest of people. This book grew out of an American Federation of Information Processing Societies (AFIPS) conference, and is organized around three basic areas corresponding to the principal components of a prototypic plan to develop economically feasible and humanistically designed information utilities. Each part--information services, system design, and system management--consists of essays about such topics as municipal services, education and library services, telepurchasing and personal services, industrial and vocational services, entertainment and news services, computer communications for the community information utility, hardware, software, economic design, the municipal framework, the regulatory problem, consumer safegards, and public involvement and acceptance.(Author/SH)", "num_citations": "51\n", "authors": ["5"]}
{"title": "Negative results for software effort estimation\n", "abstract": " More than half the literature on software effort estimation (SEE) focuses on comparisons of new estimation methods. Surprisingly, there are no studies comparing state of the art latest methods with decades-old approaches. Accordingly, this paper takes five steps to check if new SEE methods generated better estimates than older methods. Firstly, collect effort estimation methods ranging from \u0393\u00c7\u00a3classical\u0393\u00c7\u00a5 COCOMO (parametric estimation over a pre-determined set of attributes) to \u0393\u00c7\u00a3modern\u0393\u00c7\u00a5 (reasoning via analogy using spectral-based clustering plus instance and feature selection, and a recent \u0393\u00c7\u00a3baseline method\u0393\u00c7\u00a5 proposed in ACM Transactions on Software Engineering). Secondly, catalog the list of objections that lead to the development of post-COCOMO estimation methods. Thirdly, characterize each of those objections as a comparison between newer and older estimation methods. Fourthly, using four\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "50\n", "authors": ["5"]}
{"title": "Applying process programming to the spiral model\n", "abstract": " CiNii \u03a6\u00bd\u00fb\u00b5\u00fb\u00e7 - Applying Process Programming to the Spiral Model CiNii \u03c3\u00a2\u255c\u03c4\u00bd\u00ef\u00b5\u00e2\u00e0\u03c3\u00e1\u2592\u03c3\u00a1\u00aa\u03c4\u00e1\u00f6\u03c4\u2310\u2562\u00b5\u00eb\u00c7 \u03c3\u00a1\u00aa\u03a6\u00ed\u00f4 \u00b5\u00e2\u00e0\u03c3\u00e1\u2592\u03c0\u00e2\u00e8\u03c0\u00e2\u00f4\u03c0\u00e9\u2593\u03c0\u00e2\u255d\u03c0\u00e9\u2510[\u03c0\u00e9\u2561\u03c0\u00e9\u00f1\u03c0\u00e2\u00ef\u03c0\u00e9\u00fa] \u00b5\u00f9\u00d1\u00b5\u00a3\u00bc\u03c0\u00fc\u00ab\u03a6\u00bd\u00fb\u00b5\u00fb\u00e7\u03c0\u00e9\u00c6\u03c0\u00fc\u00f2\u03c0\u00fc\u00ee\u03c0\u00fc\u00d6 \u03c3\u00f1\u00ba\u03c3\u00a1\u00aa\u03c3\u00a2\u2502\u00b5\u00a2\u2555\u0398\u00f1\u00bf\u03c0\u00fc\u00ab\u00b5\u00a3\u00bc\u03c0\u00e9\u00c6\u03c0\u00fc\u00f2\u03c0\u00fc\u00ee\u03c0\u00fc\u00d6 \u00b5\u00f9\u00d1\u00b5\u00a3\u00bc\u03c0\u00fc\u00ab\u03c3\u00ec\u00dc\u03c3\u00fa\u00bd\u03a6\u00bd\u00fb\u00b5\u00fb\u00e7\u03c0\u00e9\u00c6\u03c0\u00fc\u00f2\u03c0\u00fc\u00ee\u03c0\u00fc\u00d6 \u00b5\u00fb\u2591\u03a6\u00aa\u00c5 \u03c4\u00d6\u2557\u0398\u00ee\u2593 \u03c0\u00e2\u00a1\u03c0\u00e9\u2591\u03c0\u00e9\u00f1\u03c0\u00e2\u2502 English \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 \u03c0\u00fc\u00d6\u03c0\u00fc\u2563\u03c0\u00fc\u00aa \u00b5\u00a3\u00bc\u00b5\u00fb\u00e7\u03c0\u00fc\u00e9\u03c0\u00e9\u00e8 \u03c0\u00fc\u00d6\u03c0\u00fc\u2563\u03c0\u00fc\u00aa \u00b5\u00a3\u00bc\u00b5\u00fb\u00e7\u03c0\u00fc\u00e9\u03c0\u00e9\u00e8 \u0398\u00fb\u00eb\u03c0\u00fc\u00ff\u03c0\u00e9\u00ef \u03c0\u00e9\u2510\u03c0\u00e9\u00f1\u03c0\u00e2\u00ea\u03c0\u00e2\u00bd \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0\u03c3\u00c9\u00ec \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0ID \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0\u00b5\u00eb\u00c7\u03c3\u2592\u20a7 \u03c3\u00ea\u00e8\u03a6\u00ed\u00ee\u03c4\u00eb\u2310\u03c3\u00c9\u00ec ISSN \u03c3\u2556\u2557\u03c3\u00c5\u2556\u03c0\u00e2\u00dc\u03c0\u00e2\u255d\u03c0\u00e9\u2555 \u03c3\u00e7\u2551\u03c4\u00eb\u00ea\u03a6\u00c7\u00e0 \u03c3\u00c5\u00e9\u03a6\u00c7\u00e2\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab \u03c3\u00e7\u2551\u03c4\u00eb\u00ea\u03c3\u2563\u2524 \u03c3\u2563\u2524\u03c0\u00fc\u00ef\u03c0\u00e9\u00eb \u03c3\u2563\u2524\u03c0\u00fc\u255b\u03c0\u00fc\u00ba \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 CiNii\u03c4\u00ac\u00f4\u03c3\u00c5\u00fa\u00b5\u00d1\u00a1\u03c3\u00ef\u00d6 \u03c0\u00fc\u00ab\u03c3\u00e5\u00ec\u0398\u00fb\u00ef\u03c0\u00fc\u00bd\u03c0\u00fc\u00f1\u03c0\u00fc\u00e4\u03c0\u00fc\u00aa Applying Process Programming to the Spiral Model BOEHM BW \u03a6\u00f3\u00bd\u03c3\u255d\u00f2\u03c4\u00f6\u00bf\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab: 1\u03a3\u2557\u2562 \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0 BOEHM BW \u03c3\u00c5\u00c4\u0398\u00ee\u2593\u03c3\u00ea\u00e8\u03a6\u00ed\u00ee\u03c4\u00eb\u2310 Proc. of the 4th International Software Process Workshop Proc. of the 4th International Software Process Workshop, 46-56, 1988 \u03a6\u00f3\u00bd\u03c3\u255d\u00f2\u03c4\u00f6\u00bf\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab: 1\u03a3\u2557\u2562\u03a3\u2555\u00a1 1-1\u03a3\u2557\u2562\u03c0\u00e9\u00c6 \u03a6\u00ed\u00bf\u03c4\u00f1\u2551 1 \u03c0\u00e9\u255c\u03c0\u00e2\u00f2\u03c0\u00e2\u00ea\u03c0\u00e9\u00aa\u03c0\u00e9\u00ba\u03c0\u00e9\u00f3\u03c0\u00e2\u20a7\u03c0\u00e2\u00ec\u03c0\u00e9\u2555\u03c0\u00e2\u00ed\u03c0\u00e2\u2502\u03c0\u00e2\u00ea\u2229\u255d\u00dc\u03c0\u00e9\u255c\u03c0\u00e2\u00f2\u03c0\u00e2\u00ea\u03c0\u00e9\u00aa\u03c0\u00e9\u00ba\u03c0\u00e9\u00f3\u03c0\u00e2\u20a7\u03c0\u00e2\u00ec\u03c0\u00e9\u2555\u03c0\u00e2\u00ed\u03c0\u00e2\u2502\u03c0\u00e2\u00ea\u00b5\u00aa\u00e9\u03a6\u00ac\u00bc \u00b5\u00a5\u2592 \u03c3\u0192\u2551\u03a6\u00ed\u00a2 , \u03c4\u2524\u2591\u03a6\u2591\u2556 \u03c3\u00e2\u00dc\u03a3\u2555\u00c7 , \u0398\u00bd\u00ff\u00b5\u2310\u00ef \u03c3\u00ab\u00f9\u0398\u00a2\u00e4 \u00b5\u00e2\u00e0\u03c3\u00e1\u2592\u03c3\u00e7\u00aa\u03c4\u00c9\u00e5 33(8), 894-905, 1992-08-15 \u03c3\u00c5\u00e9\u03a6\u00c7\u00e2\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab22\u03a3\u2557\u2562 \u03a6\u00f3\u00bd\u03c3\u255d\u00f2\u03c4\u00f6\u00bf\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab4\u03a3\u2557\u2562 \u03c3\u00f1\u00ba\u03c3\u00a1\u00aa\u0398\u00d6\u00f3\u03a6\u00ac\u00bc\u00b5\u00ff\u00c4\u03a3\u255d\u00dc Tweet \u03c3\u00c9\u00e4\u03c4\u00bf\u00ab\u03c0\u00e9\u2502\u03c0\u00e2\u255d\u03c0\u00e2\u00eb NII\u03a6\u00bd\u00fb\u00b5\u00fb\u00e7ID(NAID) 10006726978 \u03a6\u2502\u00e7\u00b5\u00fb\u00d6\u03c4\u00bf\u00ab\u03c3\u00ea\u00d1 \u03a3\u255d\u00dc\u03a6\u00a1\u2591\u03a6\u2502\u00e7\u00b5\u00fb\u00d6 \u03c0\u00e2\u00e7\u03c0\u00e2\u255d\u03c0\u00e9\u2510\u00b5\u00c5\u00c9\u03a3\u255b\u00a2\u03c3\u00e0\u00e2 CJP\u03c3\u255d\u00f2\u03c4\u00f6\u00bf \u00b5\u00a2\u2555\u03c0\u00fc\u00ec\u03c3\u00e7\u2551\u03c0\u00fc\u00f9 \u03c0\u00fc\u00bd/\u0393\u00c7\u00aa", "num_citations": "50\n", "authors": ["5"]}
{"title": "Adaptive routing techniques for distributed communications systems\n", "abstract": " The concept of a netted or distributed communications system is significant because it offers a major advantage over conventional communications systems, i.e., a better chance of providing surviving lines of communication after an attack on the system. During and after an attack, however, an effective adaptive routing technique is necessary to adjust the routing tables of the message-switching control system to the changing situation. Previously investigated adaptive routing techniques are shown to be insufficient for the task, and a number of promising alternatives are formulated and investigated such as some stochastic techniques, which use information on messages passing through the network to adjust the tables, and some deterministic techniques, which use dynamic programming or graph-theoretic algorithms to recalculate changes in the tables from observed changes in the network. Each alternative has\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "50\n", "authors": ["5"]}
{"title": "Effect of multiple courses of antenatal corticosteroids on pituitary-adrenal function in preterm infants\n", "abstract": " AIM To evaluate the pituitary\u0393\u00c7\u00f4adrenal function of preterm infants whose mothers received multiple courses (8 or more doses) of antenatal dexamethasone.METHODS The pituitary\u0393\u00c7\u00f4adrenal function of 14 preterm infants whose mothers received eight or more doses of antenatal dexamethasone were assessed using the human corticotrophin releasing hormone (hCRH) stimulation test when 7 days (n = 14) and 14 days old (n = 12). During each test, blood samples were taken at 0 (baseline), 15, 30 and 60 minutes after an intravenous bolus dose of hCRH (1 \u256c\u255dg/kg). The corresponding hormone concentrations were compared between days 7 and 14, and with various associated factors.RESULTS The baseline (0 min) plasma adrenocorticotrophic hormone concentration was significantly higher at day 14 than at day 7 (p = 0.036). None of the corresponding poststimulation (15, 30, and 60 min) hormone concentrations\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "49\n", "authors": ["5"]}
{"title": "Aids for identifying conflicts among quality requirements\n", "abstract": " One of the biggest risks in software requirements engineering is the risk of overemphasizing one quality attribute requirement (eg, performance) at the expense of others at least as important (eg, evolvability and portability). This paper describes an exploratory knowledge-based tool for identifying potential conflicts among quality attributes early in the software/system life cycle.The Quality Attribute Risk and Conflict Consultant (QARCC) examines the quality attribute tradeoffs involved in software architecture and process strategies (eg, one can improve portability via a layered architecture, but usually at some cost in performance). It operates in the context of the USC-CSE WinWin system, a groupware support system for determining software and system requirements as negotiated win conditions.", "num_citations": "48\n", "authors": ["5"]}
{"title": "Software Technology in the 1990's: Using an Evolutionary Paradigm\n", "abstract": " The demand for computei-software is skvrocketing, anid the meanis to meet this demand are glaringly inadequate. In the national economy, the need for software is groving rapidly as organizations strive to increase pro-ductivitv through automationi and to improve product versatility and market appeal through the use ofcoin-puters.In systems where software is a ci-itical component, it is essential to obtain software that performs well, doesn't cost too mnuch, and is mainitainable over the projected systemn lifetime in short, software that is reliable, af-Jfordable, and adaptable. The cenitral questioni we address in this article is how to achieve a state-of-practice in the 1990's where we can build emibedded computer system software with adequate levels ot productiuit'v, udaptability, and reliabil-itv-that is, what will it take to achieve PAR by 1990? To answver this question, wve take a gliiripse at the quan titative dimensions\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "48\n", "authors": ["5"]}
{"title": "3.1. 1 COSYSMO: A Constructive Systems Engineering Cost Model Coming of Age\n", "abstract": " Building on the synergy between Systems Engineering and Software Engineering, the Center for Software Engineering (CSE) at the University of Southern California (USC), is in the process of developing a parametric model to estimate Systems Engineering costs. The goal of this model, called COSYSMO (Constructive Systems Engineering Cost Model), is to more accurately estimate the time and effort associated with performing the system engineering tasks defined by ISO/IEC 15288. This paper outlines the work accomplished thus far by USC, in collaboration with its Corporate Affiliates and the INCOSE Measurement Working Group, to develop the initial version of COSYSMO. This paper describes the development of the model, explains both its size and cost drivers, and outlines future steps that need to be taken to guarantee COSYSMO's successful adoption by the Systems Engineering community.", "num_citations": "47\n", "authors": ["5"]}
{"title": "Software engineering economics\n", "abstract": " This paper summarizes the current state of the art and recent trends in software engineering economics. It provides an overview of economic analysis techniques and their applicability to software engineering and management. It surveys the field of software cost estimation, including the major estimation techniques available, the state of the art in algorithmic cost models, and the outstanding research issues in software cost estimation.", "num_citations": "47\n", "authors": ["5"]}
{"title": "Supporting distributed collaborative prioritization for WinWin requirements capture and negotiations\n", "abstract": " One of the most common problems within a risk driven software collaborative development effort is prioritizing items such as requirements, goals, and stakeholder win-conditions. Requirements have proven particularly sticky in this as it is often the case that they can not be fully implemented when time and resources are limited introducing additional risk to the project. A practical approach to mitigating this risk in alignment with the WinWin development approach is to have the critical stakeholders for the project collaboratively negotiate requirements into priority bins which then are scheduled into an appropriate incremental development life cycle. We have constructed a system called the Distributed Collaboration Priorities Tool (DCPT) to assist in collaborative prioritization of development items. DCPT offers a structurally guided approach to collaborative prioritization much in the spirit of USC\u0393\u00c7\u00d6s WinWin requirements capture and negotiation system. In this paper, we will discuss the prioritization models implemented within DCPT via an actual prioritization of new WinWin system features. We also discuss DCPT\u0393\u00c7\u00d6s two-way integration with WinWin system, some experiences using DCPT, and current research directions.", "num_citations": "46\n", "authors": ["5"]}
{"title": "Value-based requirements prioritization: usage experiences\n", "abstract": " There are usually more requirements than feasible given budget and schedule constraints. Thus it's important to select the most valuable ones for implementation in order to ensure the delivery of a high value system. Simple prioritization approaches like 1- 10 ranking or MoSCoW lead to numerous ties requiring one to repeat the process for the tied items. In a previous study [1] we analyzed 17 different prioritization frameworks that could be used to perform value-based requirements prioritization (VBRP). The Technique of Ordered Preference by Similarity to Ideal Solution (TOPSIS) was selected as the framework of choice, as a result of the analysis. TOPSIS was deployed for use by a premier IT company in India. In this paper we present our experiences in using a decision analysis framework like TOPSIS to perform VBRP. We have seen successful applications of using such a decision analysis framework for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "45\n", "authors": ["5"]}
{"title": "Software cost modeling: Some lessons learned\n", "abstract": " This paper summarizes certain lessons we learned recently in developing a software cost estimation model for TRW. With respect to general needs in the cost estimation field, it was found particularly important and useful to address the following three needs: 1.1. to develop a set of well-defined, agreed-on criteria for the \u0393\u00c7\u00a3goodness\u0393\u00c7\u00a5 of a software cost model;2.2. to evaluate existing and future models with respect to these criteria;3.3. to emphasize \u0393\u00c7\u00a3constructive\u0393\u00c7\u00a5 models that relate their cost estimates to actual software phenomenology and project dynamics.This paper presents a set of criteria found to be particularly important in practical software cost estimation, together with examples of their importance.", "num_citations": "45\n", "authors": ["5"]}
{"title": "Tabular representation of multivariate functions\u0393\u00c7\u00f6with applications to topographic modeling\n", "abstract": " One of the outstanding problems confronting computer users is compact representation of functions of more than one variable. Most storage-limited computer programs devote a major part of their available memory to the representation of functions of more than one variable: eg, equation-of-state tables in physics problems, 1 optimal return functions in dymamic-programming problems, 2 routing tables in communications simulations, 3 tables of manpower and material in simulations of military theatre operation, 4 function representation in meteorological and astronomical investigations, 5 and representation of topographic information.", "num_citations": "45\n", "authors": ["5"]}
{"title": "COTS software integration cost modeling study\n", "abstract": " This research was sponsored by the USAF Electronic Systems Center through Rome Laboratory under DoD contract number F30602-94-C-1095. The original period of performance was March 1, 1996 through February 28, 1997. An amendment was made to the original contract providing for a four month no cost extension to the period of performance concluding June 29, 1997.", "num_citations": "44\n", "authors": ["5"]}
{"title": "Balancing opportunities and risks in component-based software development\n", "abstract": " The increasingly rapid change in information technology makes it essential for software development projects to continuously monitor and adapt to new sources of opportunity and risk. Software projects and organizations can increase their success rates in software development by better assessing and balancing their opportunities and risks. The authors summarize the incremental commitment model (ICM), a process framework for improved project monitoring and decision making based on balancing opportunities and risks. They give an example of how the ICM framework can improve component-based development choices based on assessment of opportunities and risks. They show how different opportunistic solutions result from different stakeholder value propositions. They elaborate on the risks involved in architectural mismatches among components, present a tool called the Integration Studio (iStudio) that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "43\n", "authors": ["5"]}
{"title": "Experiences with the spiral model as a process model generator\n", "abstract": " Many of the problems on software projects arise from mismatches between the process model used by the project and the project's real-world process drivers, such as budget, schedule, and available commercial off-the-shelf (COTS) software. The primary process modeling approach to date for avoiding these mismatches has been to try to develop the perfect process model: one which will work well for any combination of process drivers.Our position is that a good process model generator would be nearly as effective as the perfect process model, and much more likely to be achievable. A process model generator is a technique which operates on a project's process drivers as inputs to produce a process model for the project which is tailored to its particular process drivers.", "num_citations": "43\n", "authors": ["5"]}
{"title": "Discipline and practices of TDD: (test driven development)\n", "abstract": " This panel brings together practitioners with experience in Agile and XP methodologies to discuss the approaches and benefits of applying Test Driven Development (TDD). The goal of TDD is clean code that works. The mantra of TDD is: write a test; make it run; and make it right. Open questions to be addressed by the panel include:-How are TDD approaches to be applied to databases, GUIs, and distributed systems? What are the quantitative benchmarks that can demonstrate the value of TDD, and what are the best approaches to solve the ubiquitous issue of scalability.", "num_citations": "41\n", "authors": ["5"]}
{"title": "WinWin requirements negotiation processes: A multi-project analysis\n", "abstract": " Fifteen 6-member-teams were involved in negotiating requirements for multimedia software systems for the Library of the University of Southern California. The requirements negotiation used the Stakeholder WinWin success model and the USC WinWin negotiation model (Win Condition-Issue-Option-Agreement) and groupware system. The negotiated results were integrated into a Life Cycle Objectives (LCO) package for the project, including descriptions of the system\u0393\u00c7\u00d6s requirements, operational concept, architecture, life cycle plan, and feasibility rationale. These were subsequently elaborated into a Life Cycle Architecture package including a prototype; six of these were then implemented as products. A number of hypotheses were formulated, tested, and evolved regarding the WinWin negotiation processes and their effectiveness in supporting the development of effective LCO packages, in satisfying Library clients, and in stimulating cooperation among stakeholders. Other hypotheses involved identification of WinWin improvements, relationships among negotiation strategies on LCO package and project outcomes.", "num_citations": "41\n", "authors": ["5"]}
{"title": "Practical strategies for developing large software systems\n", "abstract": " In 1974, the major leverage point in the software process is at the software design and structuring stage. This stage begins with a statement of software requirements. Properly done, the software design and structuring process can identify the weak spots and mismatches in the requirements statement which are the main sources of unresponsiveness of delivered software. Once these weak spots slip by the design phase, they will generally stay until during or after delivery, where only costly retrofits can make the software responsive to operational needs.The software design and structuring phase ends with a detailed specification of the code that is to be produced, a plan for testing the code, and a draft set of users' manuals describing how the product is going to be used. Software costs and problems result from allowing coding to begin on a piece of software before its design aspects have been thoroughly worked out, verified, and validated. Figure 1 illustrates this quite well. It summarizes an analysis of 220 types of software errors found during a large, generally good (on-cost, on-schedule delivery) TRW software project [19, 20]. The great majority of the types of errors found in testing the code had originated in the design phase. Even more significantly, the design errors are generally not found until later in the test process. Of the 54 percent of the error types which were typically not found until during or after the acceptance test phase, only 9 percent were coding errors. The other 45 percent were design errors.", "num_citations": "41\n", "authors": ["5"]}
{"title": "Interactive problem-solving: an experimental study of\" lockout\" effects\n", "abstract": " One danger inherent in computer system design and management is an ever-present temptation to consider computer system performance as an end in itself, rather than as a means to better serve people. Such\" performance improvement\" methods as universal use of one language, large blocking of data input and output, and intricately designed code and procedures can increase machine productivity. However, it costs users an abnormally high effort to achieve any results. On the other hand, text editors, extended debugging aids, and conversational programming systems tend to reduce user-time investments at the expense of machine efficiency.", "num_citations": "41\n", "authors": ["5"]}
{"title": "Data processing system\n", "abstract": " 1,061,361. Editing data. INTERNATIONAL BUSINESS MACHINES CORPORATION. Feb. 11, 1965 [April 6, 1964], No. 5906/65. Heading G4A. In an electronic data processing system, data characters to be edited are transferred selectively and successively under partial control of a bi-stable device from a first (\" source\") storage field to a second (\" pattern\") storage field initially containing control and data characters (eg decimal point), whereby at the conclusion of an editing operation the second field contains selected characters from the first field selectively interspersed with data characters of the second field. Bytes each have 8 bits and comprise two binary-coded decimal digits or one such and a sign (\" packed\" format), or one such digit plus 4 zone bits (\" unpacked\" format). Provision is made for interchanging the two halves of a byte in a register to simplify test on a half, testing being done generally by subtracting a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "41\n", "authors": ["5"]}
{"title": "Bayesian analysis of software cost and quality models\n", "abstract": " Software cost and quality estimation has become an increasingly important field due to the increasingly pervasive role of software in today's world. In spite of the existence of about a dozen software estimation models, the field continues to remain not-too-well-understood, causing growing concerns in the software-engineering community.", "num_citations": "40\n", "authors": ["5"]}
{"title": "Making RAD work for your project\n", "abstract": " For several good business reasons, rapid application development has become increasingly popular. In general, RAD gives you earlier product payback and more payback time before the pace of technology makes your product obsolete. For software product sales, RAD also helps you debut a product earlier in a market window, which lets the product capture more market share, revenues, and profits. To gain maximum benefit from RAD, however, you must choose the RAD form that best suits your project. The article presents the various forms of RAD available and give advice on which to choose.", "num_citations": "39\n", "authors": ["5"]}
{"title": "Value-based software engineering: Seven key elements and ethical considerations\n", "abstract": " This chapter presents seven key elements that provide candidate foundations for value-based software engineering:                                1.                                     Benefits Realization Analysis                                                                2.                                     Stakeholder Value Proposition Elicitation and Reconciliation                                                                3.                                     Business Case Analysis                                                                4.                                     Continuous Risk and Opportunity Management                                                                5.                                     Concurrent System and Software Engineering                                                                6.                                     Value-Based Monitoring and Control                                                                7.                                     Change as Opportunity                                              Using a case study we show how some of these elements can be used to incorporate ethical considerations into daily\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "38\n", "authors": ["5"]}
{"title": "Improving quality through software process improvement in Thailand: initial analysis\n", "abstract": " For almost 10 years there have been attempts in Thailand to improve software quality by adopting western software process improvement models. Only 17 of the 380 companies in Thailand were able to implement software process models that we see here in the US. 14 out of the 17 companies were not able to improve their process to a higher level. Why were companies not successful in implementing these software process models? Did they find other ways to improve quality? The objective of this paper is to analyze the experiences of software developers, project managers and executive managers in implementing these software process models in Thailand. The results will show that cultural differences are a key factor to this problem. Thai people have different cultural values. which we will explore further in this paper.", "num_citations": "37\n", "authors": ["5"]}
{"title": "Composable process elements for developing COTS-based applications\n", "abstract": " Data collected from five years of developing e-service applications at USC-CSE reveals that an increasing fraction have been commercial-off-the-shelf (COTS)-based applications (CBA) projects: from 28% in 1997 to 60% in 2001. Data from both small and large CBA projects show that CBA effort is primarily distributed among the three activities of COTS assessments, COTS tailoring, and glue code development and integration, with wide variations in their distribution across projects. We have developed a set of data-motivated composable process elements, in terms of these three activities, for developing CBA's as well an overall decision framework for applying the process elements. We present data regarding the movement towards CBA's and effort distribution among them; we then proceed to describe the decision framework and to present a real-world example showing how it operates within the WinWin Spiral\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "37\n", "authors": ["5"]}
{"title": "Rebalancing your organization\u0393\u00c7\u00d6s agility and discipline\n", "abstract": " In these days of rapid change, many organizations find that their current balance between using agile and disciplined methods is not what it should be. (We realize that \u0393\u00c7\u00a3disciplined\u0393\u00c7\u00a5 is not the opposite of \u0393\u00c7\u00a3agile,\u0393\u00c7\u00a5 but it is our working label here for methods relying more on explicit documented knowledge than on tacit interpersonal knowledge). In a forthcoming book [1], we have analyzed many organizations\u0393\u00c7\u00d6 experiences with agile and disciplined methods, and have elaborated our previous characterization [2] of the \u0393\u00c7\u00a3home grounds\u0393\u00c7\u00a5 in which agile and disciplined methods have been most successful. This analysis has enabled us to determine five critical decision factors that organizations and projects can use to determine whether they are in either the agile or disciplined home grounds, or somewhere in between. These five decision factors are size, criticality, personnel, dynamism, and culture. In this paper, we\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "37\n", "authors": ["5"]}
{"title": "How to avoid drastic software process change (using stochastic stability)\n", "abstract": " Before performing drastic changes to a project, it is worthwhile to thoroughly explore the available options within the current structure of a project. An alternative to drastic change are internal changes that adjust current options within a software project. In this paper, we show that the effects of numerous internal changes can out-weigh the effects of drastic changes. That is, the benefits of drastic change can often be achieved without disrupting a project. The key to our technique is SEESAW, a novel stochastic stability tool that (a) considers a very large set of minor changes using stochastic sampling; and (b) carefully selects the right combination of effective minor changes. Our results show, using SEESAW, project managers have more project improvement options than they currently realize. This result should be welcome news to managers struggling to maintain control and continuity over their project in the face of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "35\n", "authors": ["5"]}
{"title": "System of systems lead system integrators: where do they spend their time and what makes them more or less efficient?\n", "abstract": " As organizations strive to expand system capabilities through the development of system\u0393\u00c7\u00c9of\u0393\u00c7\u00c9systems (SoS) architectures, they want to know \u0393\u00c7\u00a3how much effort\u0393\u00c7\u00a5 and \u0393\u00c7\u00a3how long.\u0393\u00c7\u00a5 In order to answer these questions, it is important to first understand the types of activities performed in SoS architecture development and integration and how these vary across different SoS implementations. This paper provides preliminary results of research conducted to determine types of SoS Lead System Integrator (LSI) activities and how these differ from the more traditional system engineering activities described in EIA 632 (Processes for Engineering a System). It also looks at concepts in organizational theory, complex adaptive systems, and chaos theory and how these might be applied to SoS LSI activities to improve success rates and efficiency in the development of these \u0393\u00c7\u00a3very large\u0393\u00c7\u00a5 complex systems. \u252c\u2310 2007 Wiley Periodicals\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "35\n", "authors": ["5"]}
{"title": "The TRW software productivity system\n", "abstract": " This paper presents an overview of the TRW Software Productivity System (SPS), an integrated software support environment based on the Unix operating system, a wide range of TRW software tools, and a wideband local network. Section 2 summarizes the quantitative and qualitative requirements analysis upon which the system is based. Section 3 describes the key architectural features and system components. Finally, section 4 discusses our conclusions and experience to date.", "num_citations": "35\n", "authors": ["5"]}
{"title": "Selecting an appropriate framework for value-based requirements prioritization\n", "abstract": " There are usually more requirements than feasible in a given schedule. Thus, it's imperative to be able to choose the most valuable ones for implementation to ensure the delivery of a high value software system. There are myriad requirements prioritization frameworks and selecting the most appropriate one is a decision problem in its own right. In this paper we present our approach in selecting the most appropriate value based requirements prioritization framework as per the requirements of our stakeholders. Based on our analysis a single framework was selected, validated by requirements engineers and project managers and deployed for company-wide use by a major IT player in India.", "num_citations": "34\n", "authors": ["5"]}
{"title": "Theory-W software project management: A case study\n", "abstract": " The search for a single unifying principle to guide software project management has been relatively unrewarding to date. Most candidate principles are either insufficiently general to apply in many situations, or so general that they provide no useful specific guidance.", "num_citations": "34\n", "authors": ["5"]}
{"title": "An empirical comparison between pair development and software inspection in Thailand\n", "abstract": " Although pair programming and software inspection have the common aim of minimizing the defects of the software product, each practice has its strengths and weaknesses. We need to understand their costs and benefits under given conditions to be able to select a practice to execute in a development project. The objective of this study is to compare the commonalities and differences between pair development and software inspection as verification techniques in Thailand. One classroom experiment and one industry experiment were conducted. The development effort and effect of quality were investigated with some additional calendar time comparisons. The classroom results showed that average development effort of the pair development group was 24% less than inspection group with the improved product quality. The industry experiment showed pair development to have about 4% more effort but about 40\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["5"]}
{"title": "An empirical study of eServices product UML sizing metrics\n", "abstract": " Size is one of the most fundamental measurements of software. For the past two decades, the source line of code (SLOC) and function point (FP) metrics have been dominating software sizing approaches. However both approaches have significant defects. For example, SLOC can only be counted when the software construction is complete, while the FP counting is time consuming, expensive, and subjective. In the late 1990s researchers have been exploring faster, cheaper, and more effective sizing methods, such as Unified Modeling Language (UML) based software sizing. We present an empirical 14-project-study of three different sizing metrics which cover different software life-cycle activities: requirement metrics (requirement), UML metrics (architecture), and SLOC metrics (implementation). Our results show that the software size in terms of SLOC was moderately well correlated with the number of external use\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["5"]}
{"title": "Towards better understanding of software quality evolution through commit-impact analysis\n", "abstract": " Developers intend to improve the quality of the software as it evolves. However, as software becomes larger and more complex, those intended actions may lead to unintended consequences. Analyzing change in software quality among different releases overlooks fine-grained changes that each commit introduces. We believe that studying software quality before and after each commit (commit-impact analysis) can reveal a wealth of information about how the software evolves and how each change impacts its quality. In this paper, we explore whether each commit has an impact on the source code, investigate the compilability of each impactful commit, examine how source code changes affect software quality metrics, and study the effectiveness of using a certain metric as software quality indicator. We analyze a total of 19,580 commits from 38 Apache Java software systems to better understand how change\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "32\n", "authors": ["5"]}
{"title": "Deterrents to the Adoption of Children in Foster Care\n", "abstract": " This study* springs from growing concern over the large number of children for whom foster care has become a permanent way of life. Many of these children have long since lost or have never had a meaningful relationship with their own families; they live in an emotional limbo without close ties, and without a family of their own where they can\" belong\u0393\u00c7\u00a5 and establish roots. Social workers have begun to make determined efforts to arrange adoptive placement for those without parental ties. To do this effectively, we need to know how these children differ from those whom social agencies and families consider adoptable. Moreover, we need to know how to prevent this dilemma for other children coming into boarding home and institutional care. Our study has shown that:A large proportion of children in foster care have no contact with their own parents. However, despite their need for permanent families, most of them differ so markedly from the children who are now being placed in adoption that adoption is unlikely unless special measures are taken in their behalf. This will require a change in agency practice and community attitudes. The study further reveals that many of these children could have been placed for adoption during the early period of agency placement, had a more adequate evaluation been made of the family situation and more active and planful measures taken to carry out adoptive placement.", "num_citations": "32\n", "authors": ["5"]}
{"title": "Software architectures: critical success factors and cost drivers\n", "abstract": " Some useful perspectives on the potential and the pitfalls of software architecture investments can be gained via analysis of software architecture critical success factors and their associated cost and benefit drivers. Basically, the potential of software architecture investments comes from appropriately identifying and exploiting positive cost-benefit relationships. The pitfalls come from neglecting critical success factors or from making unrealistic assumptions about their associated cost drivers. Examples from practice of both potential and pitfalls are given in the context of a table which summarizes a framework of software architecture critical success factors and cost drivers being developed at USC.<>", "num_citations": "31\n", "authors": ["5"]}
{"title": "Architected agile solutions for software-reliant systems\n", "abstract": " Systems are becoming increasingly reliant on software due to needs for rapid fielding of \u0393\u00c7\u00a370% capabilities,\u0393\u00c7\u00a5 interoperability, net-centricity, and rapid adaptation to change. The latter need has led to increased interest in agile methods of software development, in which teams rely on shared tacit interpersonal knowledge rather than explicit documented knowledge. However, such systems often need to be scaled up to higher level of performance and assurance, requiring stronger architectural support. Several organizations have recently transformed themselves by developing successful combinations of agility and architecture that can scale to projects of up to 100 personnel. This chapter identifies a set of key principles for such architected agile solutions for software-reliant systems, provides guidance for how much architecting is enough, and illustrates the key principles with several case studies.", "num_citations": "30\n", "authors": ["5"]}
{"title": "Assessing quality processes with ODC COQUALMO\n", "abstract": " Software quality processes can be assessed with the Orthogonal Defect Classification COnstructive QUALity MOdel (ODC COQUALMO) that predicts defects introduced and removed, classified by ODC types. Using parametric cost and defect removal inputs, static and dynamic versions of the model help one determine the impacts of quality strategies on defect profiles, cost and risk. The dynamic version provides insight into time trends and is suitable for continuous usage on a project. The models are calibrated with empirical data on defect distributions, introduction and removal rates; and supplemented with Delphi results for detailed ODC defect detection efficiencies. This work has supported the development of software risk advisory tools for NASA flight projects. We have demonstrated the integration of ODC COQUALMO with automated risk minimization methods to design higher value quality processes\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "30\n", "authors": ["5"]}
{"title": "Empirical results from an experiment on value-based review (VBR) processes\n", "abstract": " As part of our research on value-based software engineering, we conducted an experiment on the use of value-based review (VBR) processes. We developed a set of VBR checklists with issues ranked by success-criticality, and a set of VBR processes prioritized by issue criticality and stakeholder-negotiated product capability priorities. The experiment involved 28 independent verification and validation (IV&V) subjects (full-time working professionals taking a distance learning course) reviewing specifications produced by 18 real-client, full-time student e-services projects. The IV&V subjects were randomly assigned to use either the VBR approach or our previous value-neutral checklist-based reading (CBR) approach. The difference between groups was not statistically significant for number of issues reported, but was statistically significant for number of issues per review hour, total issue impact, and cost\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "30\n", "authors": ["5"]}
{"title": "The future of software processes\n", "abstract": " In response to increasing demands being put onto software-intensive systems, software processes will evolve significantly over the next two decades. This paper identifies seven relatively surprise-free trends \u0393\u00c7\u00f4 increased emphasis on users and end value; increasing software criticality and need for dependability; increasingly rapid change; increasingly complex systems of systems; increasing needs for COTS, reuse, and legacy software integration; and computational plenty \u0393\u00c7\u00f4 and two \u0393\u00c7\u00a3wild card\u0393\u00c7\u00a5 trends: increasing software autonomy and combinations of biology and computing; and discusses their likely influences on software processes between now and 2025. It also discusses limitations to software process improvement, and areas of significant software process research and education needs.", "num_citations": "30\n", "authors": ["5"]}
{"title": "A method for compatible cots component selection\n", "abstract": " Software projects involving integration of multiple commercial as well as in-house components, often confront interoperability problems. This is a result of the component selection process being limited to piecewise evaluation of system capabilities while neglecting a more thorough evaluation of interoperability between candidate components. Such problems often lead to increased costs and schedule overruns. Based on empirical data gathered from five years of developing e-services applications at USC-CSE, we have developed and applied a method for component selection that focuses on piecewise evaluation, as well as the interoperability between the candidate components. In this paper we describe the method and present a real-world example showing how it operates within the spiral process model generator.", "num_citations": "30\n", "authors": ["5"]}
{"title": "Cots-based systems\u0393\u00c7\u00f4twelve lessons learned about maintenance\n", "abstract": " This paper presents the twelve most significant lessons the CeBASE community has learned across a wide variety of projects, domains, and organizations about COTS-Based Systems (CBS) maintenance. Because many of the lessons identified are not intuitive, the source and implications of the lesson are discussed as well within the context of maintenance model for CBS.", "num_citations": "30\n", "authors": ["5"]}
{"title": "Developing small-scale application software product: some experimental results\n", "abstract": " CiNii \u03a6\u00bd\u00fb\u00b5\u00fb\u00e7 - Developing small-scale application software product : some experimental results CiNii \u03c3\u00a2\u255c\u03c4\u00bd\u00ef\u00b5\u00e2\u00e0\u03c3\u00e1\u2592\u03c3\u00a1\u00aa\u03c4\u00e1\u00f6\u03c4\u2310\u2562\u00b5\u00eb\u00c7 \u03c3\u00a1\u00aa\u03a6\u00ed\u00f4\u00b5\u00e2\u00e0\u03c3\u00e1\u2592\u03c0\u00e2\u00e8\u03c0\u00e2\u00f4\u03c0\u00e9\u2593\u03c0\u00e2\u255d\u03c0\u00e9\u2510[\u03c0\u00e9\u2561\u03c0\u00e9\u00f1\u03c0\u00e2\u00ef\u03c0\u00e9\u00fa] \u00b5\u00f9\u00d1\u00b5\u00a3\u00bc\u03c0\u00fc\u00ab\u03a6\u00bd\u00fb\u00b5\u00fb\u00e7\u03c0\u00e9\u00c6\u03c0\u00fc\u00f2\u03c0\u00fc\u00ee\u03c0\u00fc\u00d6 \u03c3\u00f1\u00ba\u03c3\u00a1\u00aa \u03c3\u00a2\u2502\u00b5\u00a2\u2555\u0398\u00f1\u00bf\u03c0\u00fc\u00ab\u00b5\u00a3\u00bc\u03c0\u00e9\u00c6\u03c0\u00fc\u00f2\u03c0\u00fc\u00ee\u03c0\u00fc\u00d6 \u00b5\u00f9\u00d1\u00b5\u00a3\u00bc\u03c0\u00fc\u00ab\u03c3\u00ec\u00dc\u03c3\u00fa\u00bd\u03a6\u00bd\u00fb\u00b5\u00fb\u00e7\u03c0\u00e9\u00c6\u03c0\u00fc\u00f2\u03c0\u00fc\u00ee\u03c0\u00fc\u00d6 \u00b5\u00fb\u2591\u03a6\u00aa\u00c5\u03c4\u00d6\u2557\u0398\u00ee\u2593 \u03c0\u00e2\u00a1\u03c0\u00e9\u2591\u03c0\u00e9\u00f1\u03c0\u00e2\u2502 English \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 \u03c0\u00fc\u00d6\u03c0\u00fc\u2563\u03c0\u00fc\u00aa \u00b5\u00a3\u00bc\u00b5\u00fb\u00e7\u03c0\u00fc\u00e9\u03c0\u00e9\u00e8 \u03c0\u00fc\u00d6\u03c0\u00fc\u2563\u03c0\u00fc\u00aa \u00b5\u00a3\u00bc\u00b5\u00fb\u00e7\u03c0\u00fc\u00e9\u03c0\u00e9\u00e8 \u0398\u00fb\u00eb\u03c0\u00fc\u00ff\u03c0\u00e9\u00ef \u03c0\u00e9\u2510\u03c0\u00e9\u00f1\u03c0\u00e2\u00ea\u03c0\u00e2\u00bd \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0\u03c3\u00c9\u00ec \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0ID \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0\u00b5\u00eb\u00c7\u03c3\u2592\u20a7 \u03c3\u00ea\u00e8\u03a6\u00ed\u00ee\u03c4\u00eb\u2310\u03c3\u00c9\u00ec ISSN \u03c3\u2556\u2557\u03c3\u00c5\u2556\u03c0\u00e2\u00dc\u03c0\u00e2\u255d\u03c0\u00e9\u2555 \u03c3\u00e7\u2551\u03c4\u00eb\u00ea\u03a6\u00c7\u00e0 \u03c3\u00c5\u00e9\u03a6\u00c7\u00e2 \u00b5\u00fb\u00e7\u03c4\u00ee\u00ab \u03c3\u00e7\u2551\u03c4\u00eb\u00ea\u03c3\u2563\u2524 \u03c3\u2563\u2524\u03c0\u00fc\u00ef\u03c0\u00e9\u00eb \u03c3\u2563\u2524\u03c0\u00fc\u255b\u03c0\u00fc\u00ba \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 \u00b5\u00f1\u00a3\u03c4\u2524\u00f3 CiNii\u03c0\u00fc\u00ab\u03c0\u00e9\u2561\u03c0\u00e2\u255d\u03c0\u00e2\u00f4\u03c0\u00e9\u2563\u03c0\u00fc\u00bd\u0398\u00fb\u00f3\u03c0\u00fc\u00d6\u03c0\u00e9\u00ef\u03c0\u00e9\u00f3\u03c0\u00e2\u2502\u03c0\u00e9\u2592\u03c0\u00e2\u255d\u03c0\u00e2\u00ea\u03c0\u00e9\u00c6\u03c3\u00ab\u0192\u00b5\u00fb\u255c\u03a3\u2555\u00a1\u03c0\u00fc\u00ba\u03c0\u00fc\u00d6\u2229\u255d\u00ea11/11(\u00b5\u2591\u2524 )-12/23(\u00b5\u2591\u2524)\u2229\u255d\u00eb CiNii Research\u03c0\u00e2\u00f9\u03c0\u00e2\u00bc\u03c4\u00eb\u00ea\u03c0\u00fc\u00ab\u03c3\u00e0\u00bc\u0398\u00fb\u00ef\u03c0\u00fc\u00bd\u03c0\u00fc\u00f1\u03c0\u00fc\u00e4\u03c0\u00fc\u00aa Developing small-scale application software product : some experimental results BOEHM Barry W. \u03a6\u00f3\u00bd\u03c3\u255d\u00f2\u03c4\u00f6\u00bf\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab: 2\u03a3\u2557\u2562 \u03a6\u00e6\u00f9\u03a6\u00c7\u00e0 BOEHM Barry W. \u03c3\u00c5\u00c4\u0398\u00ee\u2593\u03c3\u00ea\u00e8\u03a6\u00ed\u00ee\u03c4\u00eb\u2310 IFIP (International Federation for Information Processing) Congress, 1980 IFIP (International Federation for Information Processing) Congress, 1980, 1980 \u03a6\u00f3\u00bd\u03c3\u255d\u00f2\u03c4\u00f6\u00bf\u00b5\u00fb\u00e7\u03c4\u00ee\u00ab: 2\u03a3\u2557\u2562\u03a3\u2555\u00a1 1-2\u03a3\u2557\u2562\u03c0\u00e9\u00c6 \u03a6\u00ed\u00bf\u03c4\u00f1\u2551 1 \u03c0\u00e2\u00bc\u03c0\u00e9\u2563\u03c0\u00e2\u00ea\u03c0\u00e2\u2310\u03c0\u00e2\u2502\u00b5\u2502\u00bf\u00b5\u00fb\u00e7\u03c4\u00ab\u00ed\u03c4\u00c9\u00e5\u03c0\u00e9\u2556\u03c0\u00e9\u2563\u03c0\u00e2\u00e5\u03c0\u00e2\u00e1\u03c0\u00fc\u00bd\u03c0\u00fc\u00e8\u03c0\u00fc\u00e6\u03c0\u00e9\u00ef\u03c4\u2561\u2592\u03c3\u00c9\u00ea\u03c3\u20a7\u00ef\u03a6\u00aa\u00fc\u00b5\u2592\u00e9 \u03c0\u00e2\u00f9\u03c0\u00e2\u00a1\u03c0\u00e9\u2557\u03c0\u00e9\u2563\u03c0\u00fc\u00ab\u03a6\u00ac\u2510\u00b5\u0192\u2557 \u03c3\u00e1\u00c7 \u00b5\u00ff\u00a1\u03a3\u2555\u00eb , \u03a3\u2555\u00a1\u03a6\u2591\u2556 \u03c3\u00f1\u00dc\u03c3\u00f4\u00eb\u03c3\u00a1\u00c9 , \u03c4\u00eb\u00e7\u03c3\u2502\u00bb , \u0398\u2561\u00a3, \u0393\u00c7\u00aa", "num_citations": "30\n", "authors": ["5"]}
{"title": "Comparing software system requirements negotiation patterns\n", "abstract": " In a period of 2 years, two rather independent experiments were conducted at the University of Southern California (USC). In 1995, 23 three\u0393\u00c7\u00c9person teams negotiated the requirements for a hypothetical library system. Then, in 1996, 14 six\u0393\u00c7\u00c9person teams negotiated the requirements for real\u0393\u00c7\u00c9world digital library systems. A number of hypotheses were created to test how more realistic software projects differ from hypothetical ones. Other hypotheses address differences in uniformity and repeatability of negotiation processes and results. The results indicate that repeatability in 1996 was even harder to achieve then in 1995. Nevertheless, this article presents some surprising commonalties between both years that indicate some areas of uniformity. As such we found that the more realistic projects required more time to resolve conflicts and to identify options (alternatives) than the hypothetical ones. Further, the 1996\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["5"]}
{"title": "Some information processing implications of Air Force space missions: 1970-1980\n", "abstract": " Contents Hardware trends and operational requirements Most jobs will not have to strain the hardware On-board computing software costs On-board computing total system costs Real-time image processing Image processing considerations Information processing Sts innovations Sts hardware implications Sts software implications Difficulty of full software certification Apollo software certification and Relative importance of software testing.Descriptors:", "num_citations": "29\n", "authors": ["5"]}
{"title": "Existence of best rational Tchebycheff approximations\n", "abstract": " Some conditions are given which guarantee the existence of best Tchebycheff approximations to a given function f by generalized rational functions of the form", "num_citations": "29\n", "authors": ["5"]}
{"title": "The Incremental Commitment Model process patterns for rapid-fielding projects\n", "abstract": " To provide better services to customers and not to be left behind in a competitive business environment, a wide variety of ready-to-use software and technologies are available for one to grab and build up software systems at a very fast pace. Rapid fielding plays a major role in developing software systems to provide a quick response to the organization. This paper investigates the appropriateness of current software development processes and develops new software development process guidelines, focusing on four process patterns: Use single Non-Developmental Item (NDI), NDI-intensive, Services-intensive, and Architected Agile. Currently, there is no single software development process model that is applicable to all four process patterns, but the Incremental Commitment Model (ICM) can help a new project converge on a process that fits their process drivers. This paper also presents process\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "28\n", "authors": ["5"]}
{"title": "Stakeholder value driven threat modeling for off the shelf based systems\n", "abstract": " As the trend of the usage of third party commercial-off-the-shelf (COTS) and open source software continuously increases, COTS security has become a major concern for many organizations whose daily business extensively relies upon a healthy IT infrastructure. But, according to the 2006 CSI/FBI computer criminal survey, 47% of the surveyed organizations only spent no more than 2% of the IT budget in security. Often, competing with limited IT resources and the fast changing Internet threats, the ability to prioritize security vulnerabilities and address them efficiently has become a critical success factor for every security manager.", "num_citations": "28\n", "authors": ["5"]}
{"title": "The effects of CASE tools on software development effort\n", "abstract": " CASE (Computer Aided Software Engineering) tools have played a critical role in improving software productivity and quality by assisting tasks in software development processes since the 1970's. Several parametric software cost models adopt \u0393\u00c7\u00a3use of software tools\u0393\u00c7\u00a5 as one of the environmental factors that affect software development productivity. However, most software development teams use CASE tools that are assembled over time and adopt new tools without establishing formal evaluation criteria. Several software cost models assess the productivity impacts of CASE tools based just on breadth of tool coverage without considering other productivity dimensions such as degree of integration, tool maturity, and user support. This dissertation provides an extended set of tool rating scales based on the completeness of tool coverage, the degree of tool integration, and tool maturity/user support. It uses these scales\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "28\n", "authors": ["5"]}
{"title": "The art of expectations management\n", "abstract": " One of the most valuable skills a software professional can develop, expectations management is something surprisingly few people know or practice. The author has witnessed more than 100 stakeholder software requirements negotiations in which inflated expectations about the simplicity of the problem or ease of providing a solution have caused the most difficulty. Expectations management holds the key to providing winwin solutions to these situations. The author explains how clear communication, careful estimation, and precise planning can help you shape and meet realistic expectations.", "num_citations": "28\n", "authors": ["5"]}
{"title": "An analysis of trends in productivity and cost drivers over years\n", "abstract": " Background: Software engineering practices have evolved considerably over the last four decades, changing the way software systems are developed and delivered. Such evolvement may result in improvements in software productivity and changes in factors that affect productivity.Aims: This paper reports our empirical analysis on how changes in software engineering practices are reflected in COCOMO cost drivers and how software productivity has evolved over the years.Method: The analysis is based on the COCOMO data set of 341 software projects developed between 1970 and 2009. We analyze the productivity trends over the years, comparing productivity of different types and countries. To explain the overall impact of cost drivers on productivity and explain its trends, we propose a measure named Difficulty which is based on the COCOMO model and its cost drivers.Results: The results of our analysis\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["5"]}
{"title": "Accurate estimates without local data?\n", "abstract": " Models of software projects input project details and output predictions via their internal tunings. The output predictions, therefore, are affected by variance in the project details P and variance in the internal tunings T. Local data is often used to constrain the internal tunings (reducing T). While constraining internal tunings with local data is always the preferred option, there exist some models for which constraining tuning is optional. We show empirically that, for the USC COCOMO family of models, the effects of P dominate the effects of T i.e. the output variance of these models can be controlled without using local data to constrain the tuning variance (in ten case studies, we show that the estimates generated by only constraining P are very similar to those produced by constraining T with historical data). We conclude that, if possible, models should be designed such that the effects of the project options dominate the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["5"]}
{"title": "Applying the incremental commitment model to brownfield systems development\n", "abstract": " The Incremental Commitment Model (ICM) is a risk-driven process model generator for which common risk patterns generate common special-case system definition and development process models. The increasing importance of Brownfield (legacyconstrained) vs. Greenfield (unconstrained) application systems created a challenge of determining an ICM Brownfield risk pattern and an associated set of special-case process guidelines. This paper presents a case study of a failed Brownfield project, and shows how the concurrent-engineering activities, risk assessments, and anchor point milestones of the ICM could have avoided the failure. It compares the resulting ICM special-case Brownfield process with two leading Brownfield approaches, the CMU-SEI SMART approach and the IBM Brownfield approach, and finds them compatible and complementary.", "num_citations": "27\n", "authors": ["5"]}
{"title": "Using empirical testbeds to accelerate technology maturity and transition: the SCRover experience\n", "abstract": " This paper is an experience report on a first attempt to develop and apply a new form of software: a full-service empirical testbed designed to evaluate alternative software dependability technologies, and to accelerate their maturation and transition into project use. The SCRover testbed includes not only the specifications, code, and hardware of a public safety robot, but also the package of instrumentation, scenario drivers, seeded defects, experimentation guidelines, and comparative effort and defect data needed to facilitate technology evaluation experiments. The SCRover testbed's initial operational capability has been recently applied to empirically evaluate two architecture definition languages (ADLs) and toolsets, Mae and AcmeStudio. The testbed evaluation showed (1) that the ADL-based toolsets were complementary and cost-effective to apply to mission-critical systems; (2) that the testbed was cost\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["5"]}
{"title": "Using WinWin quality requirements management tools: a case study\n", "abstract": " Negotiating stakeholder WinWin relationships among software quality requirements is a technique that emerged during the 1990's in order to overcome the difficulties arising from contract-oriented specification compliance (popular in the 1970's) and service-oriented customer satisfaction (popular in the 1980's). Obstacles to adoption of negotiated win-win relationships include coordination of multiple stakeholder interests and priorities, reasoning of complicated dependencies, and scalability of an exponentially increasing resolution option space. Conflict identification and resolution techniques are key success factors to overcome the obstacles. This paper describes two exploratory knowledge-based tools (called \u0393\u00c7\u00a3QARCC\u0393\u00c7\u00a5 and \u0393\u00c7\u00a3S-COST\u0393\u00c7\u00a5)* for conflict identification and resolution and how they were used in the digital library projects of a USC Software Engineering class during the 1996/97 school year. A\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["5"]}
{"title": "Supporting distributed collaborative prioritization\n", "abstract": " Software developers are seldom able to implement stakeholders' requirements fully when time and resources are limited. To solve the problem, requirement engineers together with the stakeholders must prioritize requirements. The problem is exacerbated when the stakeholders are not all in the same place and/or can not collaborate at the same time. We have constructed a system called the Distributed Collaboration and Prioritization Tool (DCPT) to support the distributed and collaborative prioritization. We discuss the prioritization model implemented within DCPT and give examples of using the tool. We also discuss DCPT's integration with USC's WinWin requirements capture and negotiation system.", "num_citations": "27\n", "authors": ["5"]}
{"title": "Software design and structuring\n", "abstract": " In 1974, the major leverage point in the software process is at the software design and structuring stage. This stage begins with a statement of software requirements. Properly done, the software design and structuring process can identify the weak spots and mismatches in the requirements statement which are the main sources of unresponsiveness of delivered software. Once these weak spots slip by the design phase, they will gen-erally stay until during or after delivery, where only costly retrofits can make the software responsive to operational needs. The software design and structuring phase ends with a detailed specification of the code that is to be produced, a plan for test-ing the code, and a draft set of users' manuals describing how the product is going to be used. Software costs and problems result from allowing coding to begin on a piece of software before its design aspects have been thoroughly worked out, veri-fied, and validated. Figure 1 illustrates this quite well. It summa rizes an analysis of 220 types of software errors found during a large, generally good (on-cost, on-schedule delivery) TRW software project [19, 20). The great majority of the types of errors found in testing the code had originated in the design phase. Even more significantly, the design errors are gener-ally not found until later in the test process. Of the 54 percent of the error types which were typically not found until during or after the acceptance test phase, only 9 percent were coding er-rors. The other 45 percent were design errors. A particularly important stage is design verification and valida-tion. Many of the design errors could have been caught before", "num_citations": "27\n", "authors": ["5"]}
{"title": "Software cost estimation meets software diversity\n", "abstract": " The previous goal of having a one-size-fits-all software cost (and schedule) estimation model is no longer achievable. Sources of wide variation in the nature of software development and evolution processes, products, properties, and personnel (PPPPs) require a variety of estimation models and methods best fitting their situations. This Technical Briefing will provide a short history of pattern-breaking changes in software estimation methods, a summary of the sources of variation in software PPPPs and their estimation implications, a summary of the types of estimation methods being widely used or emerging, a summary of the best estimation-types for the various PPPP-types, and a process for guiding an organization's choices of estimation methods as their PPPP-types evolve.", "num_citations": "26\n", "authors": ["5"]}
{"title": "Improving software testing process: feature prioritization to make winners of success\u0393\u00c7\u00c9critical stakeholders\n", "abstract": " For a successful software project, acceptable quality must be achieved within an acceptable cost, demonstrating business value to customers and satisfactorily meeting delivery timeliness. Testing serves as the most widely used approaches to determine that the intended functionalities are performed correctly and achieve the desired level of services; however, it is also a labor\u0393\u00c7\u00c9intensive and expensive process during the whole software life cycle. Most current testing processes are often technique\u0393\u00c7\u00c9centered, rather than organized to maximize business value. In this article, we extend and elaborate the \u0393\u00c7\u00ff4+1\u0393\u00c7\u00d6 theoretical lenses of Value\u0393\u00c7\u00c9based Software Engineering (VBSE) framework in the software testing process; propose a multi\u0393\u00c7\u00c9objective feature prioritization strategy for testing planning and controlling, which aligns the internal testing process with value objectives coming from customers and markets. Our case study\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["5"]}
{"title": "Principles for successful systems engineering\n", "abstract": " This paper summarizes several iterations in developing a compact set of four key principles for successful systems engineering, which are 1) Stakeholder Value-Based System Definition and Evolution 2) Incremental Commitment and Accountability 3) Concurrent Multidiscipline System Definition and Development, and 4) Evidence-Based and Risk-based Decisionmaking. It provides a rationale for the principles, including short example case studies of failed projects that did not apply the principles, and of successful projects that did. It will compare the principles with other sets of principles such as the Lean Systems Engineering and the Hitchins set of principles for successful systems and systems engineering.", "num_citations": "26\n", "authors": ["5"]}
{"title": "Perspectives [The changing nature of software evolution; The inevitability of evolution]\n", "abstract": " Summary form only given. Traditionally, software evolution took place after software development put a system in place. However, the pace of change in technology and competition has changed the nature of software evolution to a continuous process, in which there's no neat boundary between development and evolution. Many traditional software development assumptions and practices haven't recognized this changing nature and increasingly find themselves in deep trouble as a result. Minimizing development costs by adopting numerous off-the-shelf products often leads to unaffordable evolution costs as vendors ship new releases and stop supporting the old ones. Assuming that a single form of evolutionary development covers all situations often leads to unrealistic commitments and dead-end systems as situations change.", "num_citations": "26\n", "authors": ["5"]}
{"title": "Maintenance effort estimation for open source software: A systematic literature review\n", "abstract": " Open Source Software (OSS) is distributed and maintained collaboratively by developers all over the world. However, frequent personnel turnover and lack of organizational management makes it difficult to capture the actual development effort. Various OSS maintenance effort estimation approaches have been developed to provide a way to understand and estimate development effort. The goal of this study is to identify the current state of art of the existing maintenance effort estimation approaches for OSS. We performed a systematic literature review on the relevant studies published in the period between 2000-2015 by both automatic and manual searches from different sources. We derived a set of keywords from the research questions and established selection criteria to carefully choose the papers to evaluate. 29 out of 3,312 papers were selected based on a well designed selection process. Our results show\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "25\n", "authors": ["5"]}
{"title": "Innovationsmarketing f\u251c\u255dr UMTS-Diensteangebote\n", "abstract": " Stephan B\u251c\u2562hm analysiert die mit der Einf\u251c\u255dhrung von UMTS-Mobilfunkdiensten verbundenen marketingspezifischen Fragestellungen und entwickelt eine Innovationsmarketingkonzeption f\u251c\u255dr UMTS-Netzbetreiber, die Ansatzpunkte zur marktorientierten Entwicklung von Diensteangeboten und zur Ausgestaltung der Marketinginstrumente in der Einf\u251c\u255dhrungsphase von innovativen UMTS-Diensteangeboten bietet.", "num_citations": "25\n", "authors": ["5"]}
{"title": "Bridge the gap between software test process and business value: a case study\n", "abstract": " For a software project to succeed, acceptable quality must be achieved within an acceptable cost, providing business value to the customers, and keeping delivery time short. Software testing is a strenuous and expensive process and is often not organized to maximize business value. In this article, we propose a practical value based software testing method which aligns the internal test process with the value objectives coming from the customers and the market. Our case study in a real-life business project shows that this method helps manage testing process effectively and efficiently.", "num_citations": "25\n", "authors": ["5"]}
{"title": "COCOTS: A software cots-based system (cbs) cost model\n", "abstract": " As the use of commercial-of-the-shelf (COTS) components becomes ever more prevalent in the creation of large software systems, the need for the ability to reasonably predict the true lifetime cost of using such software components grows accordingly. In using COTS software components, immediate short-term gains in direct development effort & schedule are possible, but usually as a trade-off for a more complicated long-term post-deployment maintenance environment. This paper discusses a COTS-based system cost model being developed as an extension of the COCOMO II cost model. This new model, COCOTS, attempts to predict the life cycle costs of using COTS components by capturing in its modeling parameters the more significant risks associated with using COTS components. A brief description of the COCOTS Development Phase model is presented, followed by a more detailed description of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "25\n", "authors": ["5"]}
{"title": "Spiral lifecycle increment modeling for new hybrid processes\n", "abstract": " The spiral lifecycle is being extended to address new challenges for Software-Intensive Systems of Systems (SISOS), such as coping with rapid change while simultaneously assuring high dependability. A hybrid plan-driven and agile process has been outlined to address these conflicting challenges with the need to rapidly field incremental capabilities. A system dynamics model has been developed to assess the incremental hybrid process and support project decision-making. It estimates cost and schedule for multiple increments of a hybrid process that uses three specialized teams. It considers changes due to external volatility and feedback from user-driven change requests, and dynamically re-estimates and allocates resources in response to the volatility. Deferral policies and team sizes can be experimented with, and it includes tradeoff functions between cost and the timing of changes within and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["5"]}
{"title": "Process elements: Components of software process architectures\n", "abstract": " To reduce the complexity and time spent in building life cycle plans, project managers often reuse process assets from past projects. Such impromptu reuse is risky when the assets being reused were not created with strategies that make it reusable. In elaborating Osterweil\u0393\u00c7\u00d6s \u0393\u00c7\u00a3Software Processes are Software Too\u0393\u00c7\u00a5 insight, Boehm et al have expressed the duality between software products and processes as: \u0393\u00c7\u00a3If a given approach is good for software products, then its process counterpart is good for software processes.\u0393\u00c7\u00a5 In this paper we discuss the duality between product and process reuse. We propose the development of process elements, \u0393\u00c7\u00a3process counterparts to software components,\u0393\u00c7\u00a5 which can be built with reusable strategies. These process elements can then be integrated with other process elements to develop software plans. We also present the results of an experiment that was conducted on\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["5"]}
{"title": "COCOTS software integration cost model: An overview\n", "abstract": " Software engineering in a fully connected world relies increasingly on the integration and tailoring of commercial-off-theshelf (COTS) software components. This pre-existing software is from commercial vendors who supply self-contained offthe-shelf components that can be plugged into a larger software system to provide capability that would otherwise have to be custom built. The two primary distinguishing characteristics of this COTS software are 1) that its source code is not available to the application developer, and 2) that its evolution is not under the control of the application developer.The rationale for using COTS based components is that they will involve less development time by taking advantage of existing, market proven, vendor supported products, thereby reducing overall system costs. But because of the two defining characteristics noted above (lack of access to product source code, and lack of control\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["5"]}
{"title": "Framework and initial phases for computer performance improvement\n", "abstract": " Computer performance analysis often evokes an image of a hardware monitor dictating a particular hardware modification that doubles the system's capacity. In fact, it usually involves measuring system performance, but is not necessarily limited to the use of hardware monitors, nor does it necessarily involve a hardware modification. It also includes the use of such measurement data sources as software monitors, computer accounting systems, sign-in logs, maintenance logs, and observations from computer operators, system programmers, and users. No specific improvement modification (hardware, etc.) is dictated by the measurements; the analyst must (1) formulate hypotheses about possible inefficiencies and/or bottlenecks in the system by gathering and analyzing computer performance data and (2) suggest alternative system modifications that will improve performance. Such modifications may deal with\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["5"]}
{"title": "An initial ontology for system qualities\n", "abstract": " This paper presents an initial ontology for reasoning about a system's System Qualities (SQs), ilities, or non\u0393\u00c7\u00c9functional requirements (reliability, usability, affordability, and more). The need for such ontology is based primarily on two factors. One is the importance of getting the SQs sufficiently well defined such that the system's definition, development, and evolution result in a satisfactory balance of SQ values for the system's success\u0393\u00c7\u00c9critical stakeholders, given the frequent system shortfalls and overruns that occur when the system does not achieve this balance. The other is that current system acquisition and evolution guidance descriptions have numerous deficiencies and inconsistencies in their coverage of SQ considerations. This situation is becoming more serious as systems and their stakeholders become increasingly complex, dynamic, and diverse. This paper provides an elaboration of the needs, a set of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["5"]}
{"title": "Analyzing and handling local bias for calibrating parametric cost estimation models\n", "abstract": " ContextParametric cost estimation models need to be continuously calibrated and improved to assure more accurate software estimates and reflect changing software development contexts. Local calibration by tuning a subset of model parameters is a frequent practice when software organizations adopt parametric estimation models to increase model usability and accuracy. However, there is a lack of understanding about the cumulative effects of such local calibration practices on the evolution of general parametric models over time.ObjectiveThis study aims at quantitatively analyzing and effectively handling local bias associated with historical cross-company data, thus improves the usability of cross-company datasets for calibrating and maintaining parametric estimation models.MethodWe design and conduct three empirical studies to measure, analyze and address local bias in cross-company dataset\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["5"]}
{"title": "A heuristic methodology for multi-criteria evaluation of web-based e-learning systems based on user satisfaction\n", "abstract": " A Heuristic Methodology for Multi-Criteria Evaluation of Web-Based E-Learning Systems Based on User Satisfaction - NASA/ADS Now on home page ads icon ads Enable full ADS view NASA/ADS A Heuristic Methodology for Multi-Criteria Evaluation of Web-Based E-Learning Systems Based on User Satisfaction Mahdavi, I. ; Fazlollaht, H. ; Heidarzade, A. ; Mahdavi-Am, N. ; Rooshan, YI Abstract Publication: Journal of Applied Sciences Pub Date: December 2008 DOI: 10.3923/jas..Bibcode: 2008JApSc...M full text sources Publisher | \u252c\u2310 The SAO/NASA Astrophysics Data System adshelp[at]cfa.harvard.edu The ADS is operated by the Smithsonian Astrophysical Observatory under NASA Cooperative Agreement NNX16AC86A NASA logo Smithsonian logo Resources About ADS ADS Help What's New Careers@ADS Social @adsabs ADS Blog Project Switch to full ADS Is ADS down? (or is it just me...) Smithsonian of \u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["5"]}
{"title": "Determining how much software assurance is enough? A value-based approach\n", "abstract": " A classical problem facing many software projects is how to determine when to stop testing and release the product for use. On the one hand, we have found that risk analysis helps to address such \"how much is enough?\" questions, by balancing the risk exposure of doing too little with the risk exposure of doing too much. In some cases, it is difficult to quantify the relative probabilities and sizes of loss in order to provide practical approaches for determining a risk-balanced \"sweet spot\" operating point. However, we have found some particular project situations in which tradeoff analysis helps to address such questions. In this paper, we provide a quantitative approach based on the COCOMO II cost estimation model and the COQUALMO qualify estimation model. We also provide examples of its use under the differing value profiles characterizing early startups, routine business operations, and high-finance\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["5"]}
{"title": "Critical success factors for rapid, innovative solutions\n", "abstract": " Many of today\u0393\u00c7\u00d6s problems are in search of new, innovative solutions. However, the development of new and innovative solutions has been elusive to many, resulting in considerable effort and dollars and no solution or a mediocre solution late to the marketplace or customer. This paper describes the results of research conducted to identify the critical success factors employed by several successful, high-performance organizations in the development of innovative systems. These critical success factors span technical, managerial, people, and cultural aspects of the innovative environment.", "num_citations": "22\n", "authors": ["5"]}
{"title": "Cost estimation for secure software & systems\n", "abstract": " BackgroundThe Center for Software Engineering (CSSE) at the University of Southern California (USC) is extending the widely\u0393\u00c7\u00f4used Constructive Cost Model version 2 (COCOMO II)[4] to account for developing secure software. CSSE is also developing a model for estimating the cost to acquire secure systems (emphasizing space systems), and is evaluating the effect of security goals on other models in the COCOMO family. We will present the work to date.", "num_citations": "22\n", "authors": ["5"]}
{"title": "Fifth workshop on software quality\n", "abstract": " Cost, schedule and quality are highly correlated factors in software development. They basically form three sides of the same triangle. Beyond a certain point (the \"Quality is Free\" point), it is difficult to increase the quality without increasing either cost or schedule or both for the software under development. As products and applications mature, users expect higher quality products. They want IT organizations to be responsible and accountable for the quality claims made by the product marketing teams. In the last couple decades, much software engineering research has focussed on standards, methodologies and techniques for improving software quality, measuring software quality and software quality assurance. Most of this research is focused on the internal/development view of quality. More recent studies done in conjunction with the marketing groups have made attempts to understand the customer view of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["5"]}
{"title": "Software engineering economics: background, current practices, and future directions\n", "abstract": " The field of software economics seeks to develop technical theories, guidelines, and practices of software development based on sound, established, and emerging models of value and value-creation-adapted to the domain of software development as necessary. The premise of the field is that software development is an ongoing investment activity-in which developers and managers continually make investment decisions requiring the expenditure of valuable resources, such as time, talent, and money. The overriding aim of this activity is to maximize the value added subject to an equitable distribution among the participating stakeholders. The goal of the paper is to expose the audience to this line of thinking and introduce the tools pertinent to its pursuit. The paper is designed to be self-contained and will cover concepts from introductory to advanced. Both practitioners and researchers with an interest in the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["5"]}
{"title": "Can we build software faster and better and cheaper?\n", "abstract": " \" Faster, Better, Cheaper\"(FBC) was a development philosophy adopted by the NASA administration in the mid to late 1990s. that lead to some some dramatic successes such as Mars Pathfinder as well as a number highly publicized mission failures, such as the Mars Climate Orbiter & Polar Lander.", "num_citations": "21\n", "authors": ["5"]}
{"title": "Using software project courses to integrate education and research: An experience report\n", "abstract": " At University of Southern California (USC), CSCI577ab is a graduate software engineering course that teaches best software engineering practices and allows students to apply the learned knowledge in developing real-client projects. The class is used as an experimental test-bed to deploy various research tools and approaches for validation of new methods and tools. Various research data have been collected as partial basis for twelve PhD dissertations. This paper reports how research and education are integrated via project experiments and how the results strengthen future educational experiences.", "num_citations": "21\n", "authors": ["5"]}
{"title": "Process implications of social networking-based requirements negotiation tools\n", "abstract": " Avoiding a major source of system and software project failures by finding more non-technical-user friendly methods of system definition and evolution has been a significant challenge. Five generations of the WinWin negotiation framework have improved such capabilities, but even the latest WikiWinWin toolset has encountered problems with non-technical stakeholder usage. With the advent of social networking and popularity of Facebook and Gmail, we have developed a radically different way for collaborative requirements management and negotiations. The new avatar of the WinWin framework called `Winbook' is based on the social networking paradigm, similar to Facebook and content organization using color coded labels, similar to Gmail. Initial usage results on 14 small projects involving non-technical stakeholders have shown profound implications on the way requirements are negotiated and used\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["5"]}
{"title": "Finding success in rapid collaborative requirements negotiation using wiki and shaper\n", "abstract": " Defining requirements without satisfying success critical stakeholders often leads to expensive project failures. Enabling interdisciplinary stakeholders to rapidly and effectively collaborate in development of globally-usable software-intensive systems remains a major challenge. At USC, 32 real-client, graduate-level team projects experimented with using the wiki-based requirements negotiation support tool WikiWinWin over a two- year period. Data collected from these projects indicated project outcome is correlated with several usage aspects, including early use, amount of use, frequency of use, shaper use, and evolving of negotiation artifacts. Several changes made based on our first-year's experience also showed improvements in cost-effectiveness. User feedback generally confirmed that using a wiki-based negotiation tool was beneficial, and that improving on wiki-tool ease of use would yield further client\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["5"]}
{"title": "A replicate empirical comparison between pair development and software development with inspection\n", "abstract": " In 2005, we studied the development effort and effect of quality comparisons between software development with Fagan's inspection and pair development. Three experiments were conducted in Thailand: two classroom experiments and one industry experiment. We found that in the classroom experiments, the pair development group had less average development effort than the inspection group with the same or higher level of quality. The industry experiment's result showed pair development to have a bit more effort but about 40% fewer major defects. However, since this set of experiments was conducted in Thailand, the results may be different if we conducted the experiment in other countries due to the impact of cultural differences. To investigate this we conducted another experiment with computer science graduate students at USC in Fall 2006. Unfortunately, the majority of the graduate students who\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["5"]}
{"title": "Effect of schedule compression on project effort\n", "abstract": " Schedule pressure is often faced by project managers and software developers who want to quickly deploy information systems. Typical strategies to compress project time scales might include adding more staff/personnel, investing in development tools, improving hardware, or improving development methods. The tradeoff between cost, schedule, and performance is one of the most important analyses performed during the planning stages of software development projects. In order to adequately compare the effects of these three constraints on the project it is essential to understand their individual influence on the project\u0393\u00c7\u00d6s outcome. In this paper, we present an investigation into the effect of schedule compression on software project development effort and cost and show that people are generally optimistic when estimating the amount of schedule compression. This paper is divided into three sections. First, we follow the Ideal Effort Multiplier (IEM) analysis on the SCED cost driver of the COCOMO II model. Second, compare the real schedule compression ratio exhibited by 161 industry projects and the ratio represented by the SCED cost driver. Finally, based on the above analysis, a set of newly proposed SCED driver ratings for COCOMO II are introduced which show an improvement of 6% in the model estimating accuracy.", "num_citations": "20\n", "authors": ["5"]}
{"title": "COSYSMO: a systems engineering cost Model\n", "abstract": " Building on the synergy between Systems Engineering and Software Engineering, the Center for Software Engineering (CSE) at the University of Southern California (USC), has initiated an effort to develop a parametric model to estimate Systems Engineering costs. The goal of this model, called COSYSMO (Constructive Systems Engineering Cost Model), is to more accurately estimate the time and effort associated with performing the system engineering tasks defined by ISO/IEC 15288. This paper describes the work done during the past two years by USC, in collaboration with its Corporate Affiliates and the INCOSE Measurement Working Group, to develop the initial version of COSYSMO. The paper focuses on the need for the model, describes how it was developed, summarizes its initial architecture, and identifies the size and cost drivers that participants in the model design thought to be significant. These were identified through a series of Delphi surveys where experts were asked to provide their opinions. The paper concludes with a summary of the results of these Delphi surveys and a discussion of future steps that need to be taken to guarantee COSYSMO\u0393\u00c7\u00ffs successful adoption by the Systems Engineering community.", "num_citations": "20\n", "authors": ["5"]}
{"title": "The MBASE life cycle architecture milestone package\n", "abstract": " This paper summarizes the primary criteria for evaluating software/system architectures in terms of key system stakeholders\u0393\u00c7\u00d6 concerns. It describes the Model Based Architecting and Software Engineering (MBASE) approach for concurrent definition of a system\u0393\u00c7\u00d6s architecture, requirements, operational concept, prototypes, and life cycle plans. It summarizes our experiences in using and refining the MBASE approach on 31 digital library projects. It concludes that a Feasibility Rationale demonstrating consistency and feasibility of the various specifications and plans is an essential part of the architecture\u0393\u00c7\u00d6s definition, and presents the current MBASE annotated outline and guidelines for developing such a Feasibility Rationale.", "num_citations": "20\n", "authors": ["5"]}
{"title": "Conceptual modeling challenges for model-based architecting and software engineering (MBASE)\n", "abstract": " The difference between failure and success in developing a software-intensive system can often be traced to the presence or absence of clashes among the models used to define the system\u0393\u00c7\u00d6s product, process, property, and success characteristics. (Here, we use a simplified version of one of Webster\u0393\u00c7\u00d6s definitions of \u0393\u00c7\u00a3model\u0393\u00c7\u00a5 a description or analogy used to help visualize something. We include analysis as a form of visualization).             Section 2 of this paper introduces the concept of model clashes, and provides examples of common clashes for each combination of product, process, property, and success models. Section 3 introduces the Model-Based Architecting and Software Engineering (MBASE) approach for endowing a software project with a mutually supportive base of models. Section 4 presents examples of applying the MBASE approach to a family of digital library projects.             Section 5\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["5"]}
{"title": "Assessing COTS integration risk using cost estimation inputs\n", "abstract": " Most risk analysis tools and techniques require the user to enter a good deal of information before they can provide useful diagnoses. In this paper, we describe an approach to enable the user to obtain a COTS glue code integration risk analysis with no inputs other than the set of glue code cost drivers the user submits to get a glue code integration effort estimate with the COnstructive COTS integration cost estimation (COCOTS) tool. The risk assessment approach is built on a knowledge base with 24 risk identification rules and a 3-level risk probability weighting scheme obtained from an expert Delphi analysis. Each risk rule is defined as one critical combination of two COCOTS cost drivers that may cause certain undesired outcome if they are both rated at their worst case ratings. The 3-level nonlinear risk weighting scheme represents the relative probability of risk occurring with respect to the individual cost driver\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["5"]}
{"title": "Estimating the cost of security for COTS software\n", "abstract": " This paper describes enhancements being made to the University of Southern California\u0393\u00c7\u00d6s COnstructive COTS (COCOTS) integration cost model to address security concerns. The paper starts by summarizing the actions we have taken to enhance COCOMO II to model the impact of security on development effort and duration. It then relates the COCOMO II approach to the COCOTS estimating framework so that the enhancements proposed can be incorporated into the COCOTS model. After summarizing the team\u0393\u00c7\u00d6s progress in developing counterpart COCOTS security cost drivers and expert-consensus cost driver parameter values, the paper points to the steps that will be taken to validate the findings and calibrate the model.", "num_citations": "19\n", "authors": ["5"]}
{"title": "Estimating with enhanced object points vs. function points\n", "abstract": " In the research community Function Points is considered as a de facto standard. Even according to Gartner Group\" function points will provide the primary means for measuring application size, reaching a penetration of approximately 50% of development organisations by the year 2000\"(Hotle, 1996). We do not know what sources this conjecture is based upon. However, we observe that in many major companies, like Andersen Consulting, Function Points are not used to size software for estimating purposes. In stead, various kinds of what we refer to as Enhanced Object Points (EOP) are used by project managers and other practitioners to size the product and estimate the effort. This paper describes EOP and compares the two metrics and points out some reasons why many practitioners may prefer Enhanced Object Points to Function Points.", "num_citations": "19\n", "authors": ["5"]}
{"title": "Function point analysis for software maintenance\n", "abstract": " Context: Software maintenance is required to fix defects, adapt to changes in the environment, and meet new or changed user requirements. The effort of these tasks need to be estimated to track progress, manage resources, and make decisions. Most widely used cost models use source lines of code (SLOC) as the software size input measure, due to its quantifiability and high correlation with effort. Estimating the SLOC of a project is very difficult in early stages of the software lifecycle. Function Points (FPs) represents software size by functions or modifications to functions, making them easier to calculate early in the lifecycle for new development projects or maintenance tasks. Several cost estimators use FPs to estimate the SLOC of a project to take advantage of existing cost models. Goal: Through empirical analysis, the authors want to determine whether FPs can effectively estimate maintenance tasks, as a better\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "18\n", "authors": ["5"]}
{"title": "A look at software engineering risks in a team project course\n", "abstract": " Risk identification, management, and mitigation are essential to the success of any software development projects. At the University of Southern California (USC), CSCI577ab is a graduate level software engineering course sequence that teaches the best software engineering practices, and allows students to apply the learned knowledge in developing real-client projects. This paper analyzes the risks encountered by teams, identifies the relationships between effective risk analysis and the success of the project, and discusses how risk patterns determine the student's course of action. This paper also reports the top risks of software development in the software engineering class.", "num_citations": "18\n", "authors": ["5"]}
{"title": "Software engineering is a value-based contact sport\n", "abstract": " The theory that most current students get covers may be 15 percent of the activities they encounter in practice. Most of it is based on a model of software engineering as a set-piece job of deriving and verifying code from a static set of requirements. This was a good model in the 1970s, but it is way out of date now. We should try to educate them for the software engineering situations they will encounter in the future.", "num_citations": "18\n", "authors": ["5"]}
{"title": "Calibration approach and results of the COCOMO II post-architecture model\n", "abstract": " Calibration Approach and Results of the COCOMO II Post- Architecture Model Page 1 Calibration Approach and Results of the COCOMO II PostArchitecture Model Sunita Chulani*, Brad Clark, Barry Boehm (USC-Center for Software Engineering) Bert Steece (USC-Marshall School of Business) ISPA \u0393\u00c7\u00ff98 Copyright USC-CSE 1998 *Email: sdevnani@sunset.usc.edu Page 2 Outline \u252c\u2593 Brief History of COCOMO x COCOMO II.1997 \u0393\u00c7\u00f4 Process \u0393\u00c7\u00f4 Prediction Accuracies \u0393\u00c7\u00f4 Comparison with COCOMO \u0393\u00c7\u00ff81 x Updates and Plans \u0393\u00c7\u00f4 Plans for Improving Prediction Accuracies \u0393\u00c7\u00f4 COCOMO II Research Aim \u0393\u00c7\u00f4 Effects of Process Maturity on Effort x Acknowledgements and Information Sources 2 Copyright USC-CSE 1998 Page 3 COnstructive COst MOdel (COCOMO) x COCOMO published since 1981 x Commercial implementations of COCOMO y CoCoPro, CB COCOMO, COCOMOID, COSTMODL, GECOMO Plus, SECOMO, etc. x Other y \u0393\u00c7\u00aa", "num_citations": "18\n", "authors": ["5"]}
{"title": "Software cost option strategy tool (s-cost)\n", "abstract": " The process of resolving cost-requirements conflicts is difficult because of incompatibilities among stakeholders' interests and priorities, complex cost requirements dependencies, and an exponentially increasing option space for larger systems. The paper describes an exploratory knowledge-based tool,the Software Cost Option Strategy Tool (S-COST), for assisting stakeholders to surface appropriate cost resolution options, to visualize the options, and to negotiate a mutually satisfactory balance of requirements and cost. S-COST operates in the context of the USC-CSE WinWin system (a groupware support system for determining software and system requirements as negotiated win conditions), QARCC (a support system for identifying conflicts in quality requirements), and COCOMO (constructive cost estimation model). Initial analyses of its capabilities indicate that its semiautomated approach provides users with\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "18\n", "authors": ["5"]}
{"title": "Session II Structured Programming: A Quantitative Assessment\n", "abstract": " It has now been about four years since the first impressive experience using structured programming* techniques on a large production software project.1 Yet one still hears wildly varying claims about its effectiveness in general. Here are some recent samples.", "num_citations": "18\n", "authors": ["5"]}
{"title": "How do defects hurt qualities? an empirical study on characterizing a software maintainability ontology in open source software\n", "abstract": " Beyond the functional requirements of a system, software maintainability is essential for project success. While there exists a large knowledge base of software maintainability, this knowledge is rarely used in open source software due to the large number of developers and inefficiency in identifying quality issues. To effectively utilize the current knowledge base in practice requires a deeper understanding of how problems associated with the different qualities arise and change over time. In this paper, we sample over 6000 real bugs found from several Mozilla products to examine how maintainability is expressed with subgroups of repairability and modifiability. Furthermore, we manually study how these qualities evolve as the products mature, what the root causes of the bugs are for each quality and the impact and dependency of each quality. Our results inform which areas should be focused on to ensure\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["5"]}
{"title": "An exploratory study on the influence of developers in technical debt\n", "abstract": " Software systems are often developed by many developers who have a varying range of skills and habits. These developers have a big impact on software quality. Understanding how different developers and developer characteristics impact the quality of a software is crucial to properly deploy human resources and help managers improve quality outcomes which is essential for software systems success. Addressing this concern, we conduct a study on how different developers and developer characteristics such as developer seniority in a system, frequency of commits, and interval between commits relate to Technical Debt (TD). We performed a large-scale analysis on 19,088 commits from 38 Apache Java systems and applied multiple statistical analysis tests to evaluate our hypotheses. Our empirical evaluation suggests that developers unequally increase and decrease TD, a developer seniority in a software\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["5"]}
{"title": "Rapid, evolutionary, reliable, scalable system and software development: The resilient agile process\n", "abstract": " The increasing pace of change in competition, technology, and complexity of software-intensive systems has increased the demand for rapid, reliable, scalable, and evolvable processes. Agile methods have made significant contributions to speeding up software development, but often encounter problems with reliability, scalability, and evolvability. Over the past 3 years, we have been experimenting with an approach called Resilient Agile (RA), which addresses these problems while also speeding up development by finding enablers for parallel systems engineering, development, and test. This paper summarizes our experience in defining and evolving RA by applying it to three representative emergent-technology applications: Location-Based Advertising, Picture Sharing, and Bad Driver Reporting. In comparison with the mainstream Architected Agile process that we had been using on similar systems, the RA\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["5"]}
{"title": "Architecting: How much and when?\n", "abstract": " This chapter presents research that can help projects and organizations determine whether and when to use Agile methods versus architecting approaches to software development. Even when the managers of software or software-intensive projects appreciate the importance of architecting, they all have limited resources to spend and frequently ask,\u0393\u00c7\u00a3How much architecting is enough?\u0393\u00c7\u00a5(See \u0393\u00c7\u00a3How Much Architecting Is Enough?\u0393\u00c7\u00a5 on page 162.)To provide practical guidelines for this inquiry, this chapter summarizes and ties together two sources of evidence that I and other researchers have accumulated across 40 years of software practice, and discusses some insights for software that have been drawn from these inquiries. The questions we\u0393\u00c7\u00d6ve researched can be framed as follows:", "num_citations": "17\n", "authors": ["5"]}
{"title": "Applying the value/petri process to erp software development in china\n", "abstract": " Commercial organizations increasingly need software processes sensitive to business value, quick to apply, and capable of early analysis for subprocess consistency and compatibility. This paper presents experience in applying a lightweight synthesis of a Value-Based Software Quality Achievement (VBSQA) process and an Object-Petri-Net-based process model (called VBSQA-OPN) to achieve a manager-satisfactory process for software quality achievement in an on-going ERP software project in China. The results confirmed that 1) the application of value-based approaches was inherently better than value-neutral approaches adopted by most ERP software projects; 2) the VBSQA-OPN model provided project managers with a synchronization and stabilization framework for process activities, success-critical stakeholders and their value propositions; 3) process visualization and simulation tools significantly\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["5"]}
{"title": "Functions whose best rational Chebyshev approximations are polynomials\n", "abstract": " For any function [continuous on the interval~ a, b~, it is well known (see ACH: ESER [1]) that there exists for every pair of nonnegative integers n, m, a unique best Chebyshev approximation r*(n, m) to/among all rational functions of form r (n, m; x)= ao+ alx+'\"+ a~ xn bo+ bl x+...+ bm xm'in which the numerator and denominator polynomials are relatively prime and", "num_citations": "17\n", "authors": ["5"]}
{"title": "Costing secure software development: A systematic mapping study\n", "abstract": " Building more secure software is a recent concern for software engineers due to increasing incidences of data breaches and other types of cyber attacks. However, software security, through the introduction of specialized practices in the software development life cycle, leads to an increase in the development cost. Although there are many studies on software cost models, few address the additional costs required to build secure software. We conducted a systematic review in the form of a mapping study to classify and analyze the literature related to the impact of security in software development costs. Our search strategy strove to achieve high completeness by the identification of a quasi-gold-standard set of papers, which we then used to establish a search string and retrieve papers from research databases automatically. The application of inclusion/exclusion criteria resulted in a final set of 54 papers, which were\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["5"]}
{"title": "Evaluating human-assessed software maintainability metrics\n", "abstract": " Being highly maintainable is the key to reduce approximately 75% of most systems\u0393\u00c7\u00d6 life cycle costs. Software maintainability is defined as the ease with which a software system or a component can be modified, to correct faults, improve performance or other attributes, or adapt to a changed environment. There exist metrics that can help developers measure and analyze the maintainability level of a project objectively. Most of these metrics involve automated analysis of the code. In this paper, we evaluate the software maintainability versus a set of human-evaluation factors used in the Constructive Cost Model II (COCOMO II) Software Understandability (SU) metric, through conducting a controlled experiment on humans assessing SU and performing change-request modifications on open source software (OSS) projects.", "num_citations": "16\n", "authors": ["5"]}
{"title": "Calibrating COCOMO (R) II for Projects with High Personnel Turnover\n", "abstract": " Software cost and effort estimation is a necessary step in the software development lifecycle to track progress, manage resources, and negotiate. Though many accepted cost models exist, local calibration results in more accurate estimates. Locally calibrating Unified Code Count (UCC)'s dataset based on COCOMO (Constructive Cost Model) \u252c\u00ab  II helped UCC's development team learn which factors affected the effort, the amount of fixed costs associated with training new personnel and required deliverables, and resulted in a well-fitting effort estimation model. These insights give the development team a better understanding of the environment and where improvements are most necessary and possible.", "num_citations": "16\n", "authors": ["5"]}
{"title": "Quantifying requirements elaboration to improve early software cost estimation\n", "abstract": " This paper describes an empirical study undertaken to investigate the quantitative aspects of the phenomenon of requirements elaboration which deals with transformation of high-level goals into low-level requirements. Prior knowledge of the magnitude of requirements elaboration is instrumental in developing early estimates of a project\u0393\u00c7\u00d6s cost and schedule. This study examines the data on two different types of goals and requirements \u0393\u00c7\u00f4 capability and level of service (LOS) \u0393\u00c7\u00f4 of 20 real-client, graduate-student, team projects done at USC. Metrics for data collection and analyses are described along with the utility of results they produce. Besides revealing a marked difference between the elaboration of capability goals and the elaboration of LOS goals, these results provide some initial relationships between the nature of projects and their ratios of elaboration of capability goals into capability or functional requirements.", "num_citations": "16\n", "authors": ["5"]}
{"title": "Estimating systems engineering reuse\n", "abstract": " Systems  engineering  reuse  is  the  utilization  of  previously  developed  systems  engineering  products  or  artifacts  such  as architectures, requirements, and test plans across different projects. Such reuse is intended as a means of reducing development cost, project schedule, or performance risk, by avoiding the repetition of some systems engineering activities. Although projects involving systems engineering  reuse are becoming more frequent, models or tools for estimating the cost, benefit, and overall impact on a project as a result of reusing products or artifacts have not yet been adequately developed. This paper provides an overview  of  systems  engineering  reuse  and  recent  developments  with  the  Constructive  Systems  Engineering  Cost  Model (COSYSMO) to estimate the effect of reuse on systems engineering effort. The overview of systems engineering reuse includes a review of how reuse is handled in other domains and results from an industry survey. The recent developments  in COSYSMO presents on-going research in the creation of a reuse extension for the model such as the identification of categories of systems engineering reuse, reuse extensions for the size drivers in the model, and a revised set of cost drivers.", "num_citations": "16\n", "authors": ["5"]}
{"title": "Assessing hybrid incremental processes for SISOS development\n", "abstract": " New processes are being assessed to address modern challenges for Software\u0393\u00c7\u00c9Intensive Systems of Systems (SISOS), such as coping with rapid change while simultaneously assuring high dependability. A hybrid agile and plan\u0393\u00c7\u00c9driven process based on the spiral lifecycle has been outlined to address these conflicting challenges with the need to rapidly field incremental capabilities in a value\u0393\u00c7\u00c9based framework. A system dynamics model has been developed to assess the incremental hybrid process and support project decision making. It estimates cost and schedule for multiple increments of a hybrid process that uses three specialized teams, and also considers the mission value of software capabilities. It considers changes due to external volatility and feedback from user\u0393\u00c7\u00c9driven change requests, and dynamically reestimates and allocates resources in response to the volatility. Deferral policies and team sizes\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["5"]}
{"title": "A framework for the assessment and selection of software components and connectors in cots-based architectures\n", "abstract": " Software systems today are composed from prefabricated commercial components and connectors that provide complex functionality and engage in complex interactions. Unfortunately, because of the distinct assumptions made by developers of these products, successfully integrating them into a software system can be complicated, often causing budget and schedule overruns. A number of integration risks can often be resolved by selecting the 'right' set of COTS components and connectors that can be integrated with minimal effort. In this paper we describe a framework for selecting COTS software components and connectors ensuring their interoperability in software-intensive systems. Our framework is built upon standard definitions of both COTS components and connectors and is intended for use by architects and developers during the design phase of a software system. We highlight the utility of our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["5"]}
{"title": "Unifying the software process spectrum\n", "abstract": " Software Process Workshop (SPW 2005) was held in Beijing on May 25-27, 2005. This paper introduces the motivation of organizing such a workshop, as well as its theme and paper gathering and review; and summarizes the main content and insights of 11 keynote speeches, 30 regular papers in five sessions of \u0393\u00c7\u00a3Process Content\u0393\u00c7\u00a5,\u0393\u00c7\u00a3Process Tools and Metrics\u0393\u00c7\u00a5,\u0393\u00c7\u00a3Process Management\u0393\u00c7\u00a5,\u0393\u00c7\u00a3Process Representation and Analysis\u0393\u00c7\u00a5, and \u0393\u00c7\u00a3Experience Reports\u0393\u00c7\u00a5, 8 software development support tools demonstration, and the ending panel \u0393\u00c7\u00a3Where Are We Now? Where Should We Go Next?\u0393\u00c7\u00a5.", "num_citations": "16\n", "authors": ["5"]}
{"title": "Spiral development of software-intensive systems of systems\n", "abstract": " Commercial, public service, and national security organizations are finding it increasingly attractive to integrate component system capabilities from many different \u0393\u00c7\u00a3best of breed\u0393\u00c7\u00a5 sources to achieve their objectives. In doing so, they find that they are increasingly dependent on software to integrate the systems and to rapidly adapt them in response to competitive opportunities or threats, new technologies, or new organizational priorities.", "num_citations": "16\n", "authors": ["5"]}
{"title": "Balancing plan-driven and agile methods in software engineering project courses\n", "abstract": " For the past 6 years, we have been teaching a two-semester software engineering project course. The students organize into 5-person teams and develop largely web-based electronic services projects for real USC campus clients. We have been using and evolving a method called Model- Based (System) Architecting and Software Engineering (MBASE) for use in both the course and in industrial applications. The MBASE Guidelines include a lot of documents. We teach risk-driven documentation: if it is risky to document something, and not risky to leave it out (e.g., GUI screen placements), leave it out. Even so, students tend to associate more documentation with higher grades, although our grading eventually discourages this. We are always on the lookout for ways to have students learn best practices without having to produce excessive documentation. Thus, we were very interested in analyzing the various\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["5"]}
{"title": "Reasoning about the composition of heterogeneous architectures\n", "abstract": " A persistent problem in software engineering is how to put software systems together out of smaller subsystems: the problem of software composition. The emergence of software architectures and architectural styles has focused attention on a new set of abstractions with which we can create and compose software systems. We examine the problem of providing a model for the composition of different architectural styles within software systems, ie the problem of composing heterogeneous architectures. We describe a model of pure styles and of their composition. We provide a disciplined approach to the process of architectural composition, and techniques for using the approach to determine architectural constraints and the conditions under which systems will fail to be composed.", "num_citations": "16\n", "authors": ["5"]}
{"title": "Formal methods in resilient systems design: application to multi-UAV system-of-systems control\n", "abstract": " Resilience approaches today rely on ad hoc methods that offer piecemeal solutions. Models used by these methods are difficult to verify and do not scale. Furthermore, it is difficult to assess their long-term impact. This chapter presents a resilient systems design approach based on formal methods that is intended to overcome these limitations. The approach combines deterministic and probabilistic modeling to create a new modeling construct that lends itself to designing scalable, resilient systems and system-of-systems (SoS). The formalism facilitates model verification and possesses requisite flexibility to handle nondeterminism. The target application domain is multi-UAV swarm control in uncertain, potentially hazardous, dynamic environments. However, the approach is sufficiently general for a variety of SoS including autonomous vehicle SoS networks.", "num_citations": "15\n", "authors": ["5"]}
{"title": "Architecture-based quality attribute synergies and conflicts\n", "abstract": " Large or critical software projects often identify particularly-critical quality attributes (QAs), often called non-functional requirements or ilities, and organize Integrated Product Teams (IPTs) to address them. Frequently, the resulting IPT solutions include architectural decisions that address the QA of interest, but seriously conflict with other QAs which are also important but less-well analyzed. These conflicts will often become major sources of significant technical debt and expensive architectural breakage to rectify. As the QA of interest also has synergies with other QAs, the rectification needs to be careful not to undo the synergies. This paper summarizes recent research to develop QA synergies and conflicts matrices that can be used by software system engineers to identify potential areas of concern in balancing a system's QAs.", "num_citations": "15\n", "authors": ["5"]}
{"title": "A model for estimating agile project process and schedule acceleration\n", "abstract": " Accelerating development schedules is increasingly important in a competitive world. Reduced time-to-market is a key response to competitive threats in the commercial sphere, and rapid response in deploying military systems may save lives in a geopolitical environment characterized by rapidly emerging and ever-changing physical threats. Agile/lean development methodologies show promise in providing the desired schedule acceleration, but it can be difficult for planners to determine the effects of these factors on schedule duration, and to make appropriate choices to optimize project performance. The Constructive Rapid Application Development Model (CORADMO) attempts to quantify the effects of key schedule drivers, and thus enable planners to estimate the relative schedule that will result from varying these parameters.", "num_citations": "15\n", "authors": ["5"]}
{"title": "Integrating Collaborative Requirements negotiation and prioritization processes: a match made in heaven\n", "abstract": " Selecting system and software requirements to implement in a particular product or release is a challenging decision problem. Business stakeholders strive to maximize return on investment by selecting the most valuable requirements for implementation. Deciding on the requirements to be selected entails a great deal of communication and coordination amongst the stakeholders to ascertain the priorities of the individual requirements. The prioritized requirements aid in the planning and sequencing of implementation activities associated with the software system and provides a basis of a prioritized backlog from which the requirements can be \u0393\u00c7\u00ffpulled\u0393\u00c7\u00d6for development. Changing business priorities may require a complete reprioritization of the backlog, leading to wasted effort. Individual change requests and new requirements need to be prioritized and inserted into the correct location in the backlog requiring high\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["5"]}
{"title": "COSYSMO: A systems engineering cost model\n", "abstract": " Building on the synergy between systems engineering and software engineering, we have developed a parametric model to estimate systems engineering costs. The goal of this model called COSYSMO (Constructive Systems Engineering Cost Model), is to more accurately estimate the time and effort associated with performing the system engineering tasks in complex systems. This article describes how COSYSMO was developed and summarizes its size drivers and effort multipliers. We conclude with an example estimate to illustrate the usage of the model to estimate systems engineering cost.", "num_citations": "15\n", "authors": ["5"]}
{"title": "Evaluation of Systems Engineering Methods, Processes and Tools on Department of Defense and Intelligence Community Programs-Phase 2\n", "abstract": " This report describes the results from the second in a series of related efforts to address systems engineering shortfalls in projects characterized as quick response, network-enabled or emergent. The objectives of this task were to 1 Gather additional information on methods, processes and tools MPTs associated with the environment identified in Phase 1 of this work and develop a taxonomy of MPTs identified 2 Investigate the use of micro-process modeling techniques to support the definition and evaluation of MPTs and, 3 Provide implementation guidance on the three MPTs recommended in Phase 1. The products of the research are directly relevant to the challenges currently being faced by the sponsor a the description of the three recommended MPTs in an expanded taxonomy and individual implementation guidance b the development of a micro-process model of Scrum in Little-JIL and successful demonstration of fault tree and finite state verification analyses c the identification of key critical success factors for rapid response and innovative development environments. The recommendations for future research based on these results are 1 continue the identification of useful MPTs and their description in the expanded taxonomy 2 investigate the practicality and usefulness of a test bed facility to evaluate incremental improvement of existing MPTs and new approaches to systems engineering, including new MPTs 3conduct empirical studies of anecdotal MPT claims, eg scalability of Scrum 4 use the gaps identified in this work to establish focused innovation teams to create, evaluate and if appropriate, pilot new systems engineering\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["5"]}
{"title": "Comparative experiences with electronic process guide generator tools\n", "abstract": " The primary objective of all software engineering courses is to help students learn how to develop successful software systems with good software engineering practices. Various tools and guidelines are used to assist students to gain the knowledge as much as possible. USC\u0393\u00c7\u00d6s Center for Systems and Software Engineering (CSSE) has found that the keystone course in learning software engineering is a year-long real-client team project course. Over the last ten years, CSSE has evolved a set of guidelines for the course, and has experimented with early tests for creating electronic process guides for MBASE (Model-Based (Systems) Architecting and Software Engineering) Guidelines using Spearmint/EPG. Currently, CSSE has been developing and experimenting with Eclipse Process Framework\u0393\u00c7\u00d6s (EPF) to situate the LeanMBASE Guidelines. This paper reports our comparative experiences of using the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["5"]}
{"title": "Conflict analysis and negotiation aids for cost-quality requirements\n", "abstract": " The process of resolving conflicts among software quality requirements is complex and difficult because of incompatibilities among stakeholders\u0393\u00c7\u00d6 interests and priorities, complex cost-quality requirements dependencies, and an exponentially increasing resolution option space for larger systems. This paper describes an exploratory knowledge-based tool, the Software Cost Option Strategy Tool (S-COST), which assists stakeholders to 1) surface appropriate resolution options for costquality conflicts; 2) visualize the options; and 3) negotiate a mutually satisfactory balance of quality requirements and cost.S-COST operates in the context of the USC-CSE WinWin system (a groupware support system for determining software and system requirements as negotiated win conditions), QARCC (Quality Attribute and Risk Conflict Consultant--a support system for identifying quality conflicts in software requirements), and COCOMO (COnstructive COst estimation MOdel). Initial analyses of its capabilities indicate that its semiautomated approach provides users with improved capabilities for addressing cost-quality requirements issues.", "num_citations": "15\n", "authors": ["5"]}
{"title": "An open architecture for software process asset reuse\n", "abstract": " The development and reuse of software engineering processes within an organization can be impeded by the lack of a solid process framework. An open process architecture provides a framework through the identification of architectural elements and the specification of element interfaces. This paper introduces one open process architecture and examines some architectural element interfaces.", "num_citations": "15\n", "authors": ["5"]}
{"title": "Software process management: lessons learned from history\n", "abstract": " Regarding history, George Santayana once said,\u0393\u00c7\u00a3Those who cannot remember the past are condemned to repeat it.\u0393\u00c7\u00a5", "num_citations": "15\n", "authors": ["5"]}
{"title": "System thinking: Educating T-shaped software engineers\n", "abstract": " With respect to system thinking, a T-shaped person is one who has technical depth in at least one aspect of the system's content, and a workable level of understanding of a fair number of the other system aspects. Many pure computer science graduates are strongly I-shaped, with a great deal of depth in software technology, but little understanding of the other disciplines involved in such areas as business, medicine, transportation, or Internets of Things. This leaves them poorly prepared to participate in the increasing numbers of projects involving multi-discipline system thinking, and in strong need of software skills. We have developed and evolved an MS-level software engineering curriculum that enables CS majors to become considerably more T-shaped than when they entered. It includes courses in software management and economics, human-computer interaction, embedded software systems, systems and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["5"]}
{"title": "System-of-systems cost estimation: analysis of lead system integrator engineering activities\n", "abstract": " As organizations strive to expand system capabilities through the development of system-of-systems (SoS) architectures, they want to know \u0393\u00c7\u00a3how much effort\u0393\u00c7\u00a5 and \u0393\u00c7\u00a3how long\u0393\u00c7\u00a5 to implement the SoS. In order to answer these questions, it is important to first understand the types of activities performed in SoS architecture development and integration and how these vary across different SoS implementations. This article provides results of research conducted to determine types of SoS lead system integrator (LSI) activities and how these differ from the more traditional system engineering activities described in Electronic Industries Alliance (EIA) 632 (\u0393\u00c7\u00a3Processes for Engineering a System\u0393\u00c7\u00a5). This research further analyzed effort and schedule issues on \u0393\u00c7\u00a3very large\u0393\u00c7\u00a5 SoS programs to more clearly identify and profile the types of activities performed by the typical LSI and to determine organizational characteristics that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["5"]}
{"title": "A spiral model of software development and enhancement\n", "abstract": " The Defense Science Board Task Force Report on Military Software [1] issued in 1987 highlighted the concern that traditional software process models were discouraging more effective approaches to software development such as prototyping and software reuse. The IEEE Computer Society has sponsored tutorials and workshops on software process models that have helped clarify many of the issues and stimulated advances in the field (see \u0393\u00c7\u00a3Further Reading\u0393\u00c7\u00a5).The spiral model presented in this article is one candidate for improving the software process model situation. The major distinguishing feature of the spiral model is that it creates a risk-driven approach to the software process rather than a primarily document-driven or code-driven process. It incorporates many of the strengths of other models and resolves many of their difficulties.", "num_citations": "14\n", "authors": ["5"]}
{"title": "Measuring Security Investment Benefit for Off the Shelf Software Systems-A Stakeholder Value Driven Approach.\n", "abstract": " This paper presents the Threat Modeling method based on Attacking Path Analysis (T-MAP) which quantifies security threats by calculating the total severity weights of relevant attacking paths for Commercial Off The Shelf (COTS) based systems. Compared to existing approaches, T-MAP is sensitive to an organization\u0393\u00c7\u00d6s business value priorities and IT environment. It distills the technical details of thousands of relevant software vulnerabilities into management-friendly numbers at a high-level, and systematically establishes the traceability and consistency from management-level organizational value propositions to technical-level security threats and corresponding mitigation strategies. In its initial usage in a large IT organization, T-MAP has demonstrated promising strength in prioritizing and estimating security investment effectiveness, as well as in evaluating the security performance of COTS systems. In the case study, we demonstrate the steps of using T-MAP to analyze the cost-effectiveness of how system patching, user account control and firewall can improve security. In addition, we introduce a software tool that automates the T-MAP.", "num_citations": "14\n", "authors": ["5"]}
{"title": "Attribute-based cots product interoperability assessment\n", "abstract": " Software systems today are frequently composed from prefabricated commercial components that provide complex functionality and engage in complex interactions. Such projects that utilize multiple commercial-off-the-shelf (COTS) products often confront interoperability conflicts resulting in budget and schedule overruns. These conflicts occur because of the incompatible assumptions made by developers of these products. Identification of such conflicts and planning strategies to resolve them is critical for developing such systems under budget and schedule constraints. Unfortunately, acquiring information to perform interoperability analysis is a time-intensive process. Moreover, increase in the number of COTS products available to fulfill similar functionality leads to hundreds of COTS product combinations, further complicating the COTS interoperability assessment landscape. In this paper we present a set of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["5"]}
{"title": "The future of software and systems engineering processes\n", "abstract": " In response to the increasing criticality of software within systems and the increasing demands being put onto software-intensive systems, software and systems engineering processes will evolve significantly over the next two decades. This paper identifies eight relatively surprise-free trends\u0393\u00c7\u00f4the increasing interaction of software engineering and systems engineering; increased emphasis on users and end value; increased emphasis on systems and software dependability; increasingly rapid change; increasing global connectivity and need for systems to interoperate; increasingly complex systems of systems; increasing needs for COTS, reuse, and legacy systems and software integration; and computational plenty. It also identifies two \u0393\u00c7\u00a3wild card\u0393\u00c7\u00a5 trends: increasing software autonomy and combinations of biology and computing. It then discusses the likely influences of these trends on software and systems engineering processes between now and 2025, and presents an emerging three-team adaptive process model for coping with the resulting challenges and opportunities of developing 21st century software-intensive systems and systems of systems.", "num_citations": "14\n", "authors": ["5"]}
{"title": "Using a model framework in developing and delivering a family of software engineering project courses\n", "abstract": " The University of Southern California (USC) teaches a two-semester real-client project course as a core course in USC's MSCS-SE (Master of Science in Computer Science and Software Engineering) degree program. The course has evolved rapidly, each year introducing many changes in order to satisfy the course stakeholder's win conditions. The course has also been our primary experimental testbed for evolving our MBASE (Model-Based Architecting and Software Engineering) model integration framework. In turn, this framework, along with the CRESST (Center for Research on Evaluation, Standards and Student Testing) cognitive demands analysis has served as an effective means of managing the course's rapid evolution. A further test of the framework has been its application to undergraduate software engineering project courses at other institutions. This paper provides a description and examples of USC\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["5"]}
{"title": "From multiple regression to bayesian analysis for calibrating COCOMO II\n", "abstract": " This paper compares and contrasts the two calibration approaches, namely the 10% weighted average multiple regression approach and the Bayesian approach; used to calibrate the successive versions of COCOMO 11, ie the 1997 and 1999 calibrations. It concludes that the Bayesian approach used in the 1999 calibration is better and more robust than the multiple regression approach used in the 1997 calibration. We note that the predictive performance of the Bayesian approach (ie COCOMO II. 1999) is significantly better than that of the multiple regression approach (ie COCOMO 11.1997). COCOMO II. 1999 gives predictions that are within 30% of the actuals 75% of the time where as COCOMO II. 1997 gives predictions within 30% of the actuals only 52% of the time.", "num_citations": "14\n", "authors": ["5"]}
{"title": "A comparison study in software requirements negotiation\n", "abstract": " In a period of two years, two rather independent experiments were conducted at the University of Southern California. In 1995, 23 threeperson teams negotiated the requirements for a hypothetical library system. Then in 1996, 14 six-person teams negotiated the requirements for real multimedia related library systems.", "num_citations": "14\n", "authors": ["5"]}
{"title": "Models for Composing Heterogeneous Software Architectures\n", "abstract": " A persistent problem in software engineering is the problem of software composition. The emergence of software architectures and architectural styles has focused attention on a new set of abstractions with which we can create and compose software systems. We examine the problem of providing a model for the composition of different architectural styles within software systems, ie the problem of composing heterogeneous architectures. We describe a model of pure styles that is based on a uniform representation. We provide a disciplined approach for analyzing some key aspects of architectural composition, and show the conditions under which systems will fail to be composed.", "num_citations": "14\n", "authors": ["5"]}
{"title": "Understanding feature requests by leveraging fuzzy method and linguistic analysis\n", "abstract": " In open software development environment, a large number of feature requests with mixed quality are often posted by stakeholders and usually managed in issue tracking systems. Thoroughly understanding and analyzing the real intents that feature requests imply is a labor-intensive and challenging task. In this paper, we introduce an approach to understand feature requests automatically. We generate a set of fuzzy rules based on natural language processing techniques that classify each sentence in feature requests into a set of categories: Intent, Explanation, Benefit, Drawback, Example and Trivia. Consequently, the feature requests can be automatically structured based on the classification results. We conduct experiments on 2,112 sentences taken from 602 feature requests of nine popular open source projects. The results show that our method can reach a high performance on classifying sentences from\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["5"]}
{"title": "A light-weight incremental effort estimation model for use case driven projects\n", "abstract": " Use case analysis has been widely adopted in modern software engineering due to its strength in capturing the functional requirements of a system. It is often done with a UML use case model that formalizes the interactions between actors and a system in the requirements elicitation iteration, and with architectural alternatives explored and user interface details specified in the following analysis and design iteration. On the other hand, to better support decision making in software management, effort estimation models are required to provide estimates about the required project effort at the very early stage of a project, which, however, provides little information for accurately evaluating system complexity. To solve this dilemma, an incremental approach of integrating information available throughout the early iterations to provide multiple effort estimations is preferred in keeping the balance between utility and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["5"]}
{"title": "Impact of software resource estimation research on practice: a preliminary report on achievements, synergies, and challenges\n", "abstract": " This paper is a contribution to the Impact Project in the area of software resource estimation. The objective of the Impact Project has been to analyze the impact of software engineering research investments on software engineering practice. The paper begins by summarizing the motivation and context for analyzing software resource estimation; and by summarizing the study's purpose, scope, and approach. The approach includes analyses of the literature; interviews of leading software resource estimation researchers, practitioners, and users; and value/impact surveys of estimators and users. The study concludes that research in software resource estimation has had a significant impact on the practice of software engineering, but also faces significant challenges in addressing likely future software trends.", "num_citations": "13\n", "authors": ["5"]}
{"title": "Software defect reduction top 10 list\n", "abstract": " Recently, a National Science Foundation grant enabled us to establish the Center for Empirically Based Software Engineering (CeBASE), which seeks to transform software engineering as much as possible from a fad-based practice to an engineering-based practice through derivation, organization, and dissemination of empirical data on software development and evolution phenomenology. The phrase \u0393\u00c7\u00a3as much as possible\u0393\u00c7\u00a5 reflects the fact that software development must remain a people-intensive and continually changing field. We have found, however, that researchers have established objective and quantitative data, relationships, and predictive models that help software developers avoid predictable pitfalls and improve their ability to predict and control efficient software projects.Here we describe developments in this area that have taken place since the publication of \u0393\u00c7\u00a3Industrial Metrics Top 10 List\u0393\u00c7\u00a5 in 1987 (B. Boehm, IEEE Software, Sept. 1987, pp. 84\u0393\u00c7\u00f485). Given that CeBASE places a high priority on software defect reduction, we think it is fitting to update that earlier article by providing the following Software Defect Reduction Top 10 List.", "num_citations": "13\n", "authors": ["5"]}
{"title": "Using a hybrid method for formalizing informal stakeholder requirements inputs\n", "abstract": " Success of software development depends on the quality of the requirements specification. Moreover, good - sufficiently complete, consistent, traceable, and testable - requirements are a prerequisite for later activities of the development project. Without understanding what the stakeholders really want and need, and writing these requirements, projects will not develop what the stakeholders wanted. During the development of the WinWin negotiation model and the EasyWinWin requirements negotiation method, we have gained considerable experience in capturing informal requirements in over 100 projects. However, the transition from informal representations to semi-formal and formal representations is still a challenging problem. Based on our analysis of the projects to date, we have developed an integrated set of gap-bridging methods as a hybrid method to formalize informal stakeholder requirements inputs\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["5"]}
{"title": "6.1. 3 A Value\u0393\u00c7\u00c9Based Theory of Systems Engineering\n", "abstract": " The INCOSE definition of \u0393\u00c7\u00a3systems engineering\u0393\u00c7\u00a5 is \u0393\u00c7\u00a3an interdisciplinary approach and means to enable the realization of successful systems.\u0393\u00c7\u00a5 The Value\u0393\u00c7\u00c9Based Theory of Systems Engineering presents necessary and sufficient conditions for realizing a successful system and elaborates them into an executable process. The theory and process are illustrated on a supply\u0393\u00c7\u00c9chain system example, and evaluated with respect to criteria for a good theory.", "num_citations": "13\n", "authors": ["5"]}
{"title": "How do Agile/XP development methods affect companies?\n", "abstract": " Does the discipline inherent in Agile/XP methods change the way a company does business in contrast to the influences of \"traditional\" plan-driven or ad-hoc software development practices? Are there differences in strategies for customer engagement, staff resourcing, and program management? Companies live or die depending on the accuracy of scheduling/budgeting projections and the ability to do more with less. Lean development, SCRUM, XP, and other agile methods may stress companies in hitherto unanticipated ways leading to both evolutionary and revolutionary organizational change. This panel will dis cuss the differences and similarities between XP/Agile and more traditional software development practices with regard to their impact on companies.", "num_citations": "13\n", "authors": ["5"]}
{"title": "Telecooperation experience with the WinWin system\n", "abstract": " WinWin is a telecooperation system supporting the definition of software-based applications as negotiated stakeholder win conditions. Our experience in using WinWin in defining over 30 digital library applications, including several telecooperation systems, is that it is important to supplement negotiation support systems such as WinWin with such capabilities as prototyping, tradeoff analysis tools, email, and videoconferencing. We also found that WinWin\u0393\u00c7\u00d6s social orientation around considering other stakeholders\u0393\u00c7\u00d6 win conditions has enabled stakeholders to achieve high levels of shared vision and mutual trust. Our subsequent experience in implementing the specified digital library systems in a rapidly changing web-based milieu indicated that achieving these social conditions among system stakeholders was more important than achieving precise requirements specifications, due to the need for team adaptability to requirements change. Finally, we found that the WinWin approach provides an effective set of methods of integrating ethical considerations into practical system definition processes via Rawls\u0393\u00c7\u00d6 stakeholder negotiation-based Theory of Justice.", "num_citations": "13\n", "authors": ["5"]}
{"title": "Analysis of system requirements negotiation behavior patterns\n", "abstract": " Roughly 35 three\u0393\u00c7\u00c9person teams played the roles of user, customer, and developer in negotiating the requirements of a library information system. Each team was provided with a suggested set of stakeholder goals and implementation options, but were encouraged to exercise creativity in expanding the stakeholder goals and in creating options for negotiating an eventually satisfactory set of system requirements. The teams consisted of students in a first\u0393\u00c7\u00c9year graduate course in software engineering at USC. They were provided with training in the Theory W (win\u0393\u00c7\u00c9win) (Boehm\u0393\u00c7\u00c9Ross, 1989) approach to system requirements determination and the associated USC WinWin groupware support system (Boehm, et al, 1995)(Horowitz, 1996). They were required to complete the assignment in two weeks. Data was collected on the negotiation process and results, with 23 projects providing sufficiently complete and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["5"]}
{"title": "Megaprogramming (Preliminary Version\n", "abstract": " \" Megaprogramming\" refers to the practice of building and evolving computer software component by component Megaprogramming builds on the processes and technologies of software reuse, software engineering environments, software architecture engineering, and application generation in order to provide a component-oriented product-line approach to software development In the product line approach, management incentives and technology support can", "num_citations": "13\n", "authors": ["5"]}
{"title": "The child in foster care\n", "abstract": " Implications It may be simplistic to emphasize poverty as a causative factor in place-ment, and the provision of financial resources as a major remedy. Slums cannot be wiped out overnight; discrimination, deprivation, and inferior education have far-reaching effects. The concept of\" cultural lag \u0393\u00c7\u00a3decries the hopes of early diminution of the need for placement; many of its proponents emphasize that it may take a generation until families now vul-", "num_citations": "13\n", "authors": ["5"]}
{"title": "Local bias and its impacts on the performance of parametric estimation models\n", "abstract": " Background: Continuously calibrated and validated parametric models are necessary for realistic software estimates. However, in practice, variations in model adoption and usage patterns introduce a great deal of local bias in the resultant historical data. Such local bias should be carefully examined and addressed before the historical data can be used for calibrating new versions of parametric models.Aims: In this study, we aim at investigating the degree of such local bias in a cross-company historical dataset, and assessing its impacts on parametric estimation model's performance.Method: Our study consists of three parts: 1) defining a method for measuring and analyzing the local bias associated with individual organization data subset in the overall dataset; 2) assessing the impacts of local bias on the estimation performance of COCOMO II 2000 model; 3) performing a correlation analysis to verify that local bias\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["5"]}
{"title": "Improving the ROI of software quality assurance activities: an empirical study\n", "abstract": " Review, process audit, and testing are three main Quality Assurance activities during the software development life cycle. They complement each other to examine work products for defects and improvement opportunities to the largest extent. Understanding the effort distribution and inter-correlation among them will facilitate software organization project planning, improve the software quality within the budget and schedule and make continuous process improvement. This paper reports some empirical findings of effort distribution pattern of the three types of QA activities from a series of incremental projects in China. The result of the study gives us some implications on how to identify which type of QA activity is insufficient while others might be overdone, how to balance the effort allocation and planning for future projects, how to improve the weak part of each QA activity and finally improve the Return On\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["5"]}
{"title": "Early identification of SE-related program risks\n", "abstract": " The mission of the DoD Systems Engineering Research Center SERC is to perform research leading to transformational SE methods, processes, and tools MPTs that enable DoD and Intelligence Community IC systems to achieve significantly improved mission successes. An elevator speech for the capabilities delivered by the Department of Defense DoD Systems Engineering Research Center SERC Systems Engineering SE Effectiveness Measurement EM task reads as follows for the DoD, whose Major Defense Acquisition Programs MDAPs frequently and significantly overrun their budgets and schedules and deliver incomplete systems, the SERC SE EM framework, operational concepts, and tools will empower MDAP sponsors and performers to collaboratively determine their early SE shortfalls and enable the development of successful systems within their resource constraints. Unlike traditional schedule-based and event-based reviews, the SERC SE EM technology enables sponsors and performers to agree on the nature and use of more effective evidence-based reviews. These enable early detection of missing SE capabilities or personnel competencies with respect to a framework of Goals, Critical Success Factors CSFs, and Questions determined by the EM task from the leading DoD early-SE CSF analyses. The EM tools enable risk-based prioritization of corrective actions, as shortfalls in evidence for each question are early uncertainties, which when combined with the relative system impact of a negative answer to the question, translates into the degree of risk that needs to be managed to avoid system overruns and incomplete\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["5"]}
{"title": "Experimental evaluation of wiki technology and the shaper role in rapid interdisciplinary requirements negotiation\n", "abstract": " The challenges driven by multi-culture, multi-discipline stakeholders collaborating in a rapidly changing global environment necessitates an easily approachable mechanism for negotiating WinWin outcomes. Wiki has become an enabling technology for interdisciplinary collaboration. Following our initial development of a wiki-based requirements negotiation support tool - WikiWinWin, we experimented with using WikiWinWin to negotiate requirements in 20 real-client projects at University of Southern California (USC). Our initial results indicated better project outcomes were achieved as the use of the tool increased, as compared with the previous EasyWinWin tool.", "num_citations": "12\n", "authors": ["5"]}
{"title": "A framework for identification and resolution of interoperability mismatches in COTS-based systems\n", "abstract": " Software systems today are frequently composed from prefabricated commercial components that provide complex functionality and engage in complex interactions. Such projects that utilize multiple commercial-off-the-shelf (COTS) products often confront interoperability conflicts resulting in budget and schedule overruns. These conflicts occur because of the incompatible assumptions made by developers of these products. Identification of such conflicts and planning strategies to resolve them is critical for developing such systems under budget and schedule constraints. In this paper we present an attribute-based framework that can be used to perform automated interoperability assessment to filter out COTS product combinations whose integration will not be feasible within the project constraints. Our framework is built upon standard definitions of both COTS components and connectors and is intended for use by\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["5"]}
{"title": "Developing a theory of value-based software engineering\n", "abstract": " This paper presents an initial \"4+1\" theory of value-based software engineering (VBSE) that builds around the stakeholder win-win Theory W, and addresses the questions of \"which values are important?\" and \"how is success assured?\" for a given software engineering enterprise. The central Theory W then draws upon four additional theories - utility theory (how important are the values?), decision theory (how do stakeholders' values determine decisions?), dependency theory (how do dependencies affect value realization?), and control theory (how to adapt to change and control value realization?).", "num_citations": "12\n", "authors": ["5"]}
{"title": "Economic analysis of software technology investments\n", "abstract": " Many large organizations are finding that:                                     Software technology is increasingly critical to their future organizational performance.                                                     Organizational expenditures on software are increasing.                                                     Investments in software technology provide opportunities to reduce software costs and increase organizational performance.", "num_citations": "12\n", "authors": ["5"]}
{"title": "Early phase cost models for agile software processes in the US DoD\n", "abstract": " Background: Software effort estimates are necessary and critical at an early phase for decision makers to establish initial budgets, and in a government context to select the most competitive bidder for a contract. The challenge is that estimated software requirements is the only size information available at this stage, compounded with the newly increasing adoption of agile processes in the US DoD. Aims: The objectives are to improve cost estimation by investigating available sizing measures, and providing practical effort estimation models for agile software development projects during the contract bidding phase or earlier. Method: The analysis explores the effects of independent variables for product size, peak staff, and domain on effort. The empirical data for model calibration is from 20 industrial projects completed recently for the US DoD, among a larger dataset of recent projects using other lifecycle processes\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["5"]}
{"title": "Impact of task switching and work interruptions on software development processes\n", "abstract": " Software developers often work on multiple projects and tasks throughout a work day, which may affect their productivity and quality of work. Knowing how working on several projects at a time affects productivity can improve cost and schedule estimations. It also can provide additional insights for better work scheduling and the development process. We want to achieve a better productivity without losing the benefits of work interruptions and multitasking for developers involved in the process. To understand how the development process can be improved, first, we identify work interruptions that mostly have a negative effect on productivity, second, we need to quantitatively evaluate impact of multitasking (task switching, work context switching) and work interruptions on productivity. In this research we study cross-project multitasking among the developers working on multiple projects in an educational setting. We\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["5"]}
{"title": "The key roles of maintainability in an ontology for system qualities\n", "abstract": " In our INCOSE IS 2015 paper, \u0393\u00c7\u00a3An Initial Ontology for System Qualities,\u0393\u00c7\u00a5 (SQs), we provided an IDEF5 class hierarchy of upper\u0393\u00c7\u00c9level SQs, where the top level reflected classes of stakeholder value propositions (Mission Effectiveness, Resource Utilization, Dependability, Flexibility), and the next level identified means\u0393\u00c7\u00c9ends enablers of the higher\u0393\u00c7\u00c9level SQs. In experimenting with, refining and formalizing the ontology, we focused on a depth\u0393\u00c7\u00c9first approach on a chosen SQ: Maintainability. It is key to reducing 75% of most systems\u0393\u00c7\u00d6 life cycle costs. Also Maintainability plays key roles in three of the four top\u0393\u00c7\u00c9level SQs: Resource Utilization (now called Life Cycle Efficiency) Dependability, and Flexibility (now called Changeability). Dependability also needs Maintainability to relate Reliability to Availability; and Changeability also needs Maintainability to address new system challenges and opportunities. This paper\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["5"]}
{"title": "Improving scenario testing process by adding value-based prioritization: an industrial case study\n", "abstract": " Most of the current testing strategies treat all aspects of software equally important in a value-neutral way; this becomes more risky when the testing resources are limited. Our goal in this case study aims at improving the testing cost-effectiveness of an industrial scenario testing process under time constraints. We proposed a value-based testing prioritization strategy which allows tests to be ranked by how well the tests can reduce risk exposure. Combining this with the tests\u0393\u00c7\u00d6 relative costs enables them to be prioritized in terms of return on investment (ROI) or risk reduction leverage (RRL). Besides, a new metric Average Percentage of Business Importance Earned (APBIE) is proposed to measure how quickly the testing can reduce the quality uncertainty and earn the relative business importance of the system under test (SUT). The results from one case study to prioritize operational testing scenarios in Galorath Inc\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["5"]}
{"title": "Systems 2020: Strategic Initiative\n", "abstract": " The Department of Defense DoD increasingly faces a mix of relatively foreseeable and unforeseeable threat and opportunity profiles. This means that DoD technological superiority relies on rapid and assured development, fielding, and evolution of progressively more complex and interoperable defense systems. Meeting these challenges requires DoD to design and build an entirely new class of adaptive systems that allow the Department to operate with far greater speed and agility. Mr. Lemnios, the Director, Defense Research and Engineering DDRE, requested a study of systems engineering research areas that enable agile, assured, efficient, and scalable systems engineering approaches to support the development of these systems. This report addresses the four highest-potential research areas determined by the study Model Based Engineering MBE, Platform Based Engineering PBE, Capability on DemandCOD, and Trusted System Design TSD. It elaborates each research area, characterizing them in terms of current state of the art and state of the practice, and identifies the most promising research topics. It then proposes next steps to create a DoD-wide Systems 2020 initiative based on a coordinated set of high-leverage, game-changing activities comprising systems engineering research, technology maturation, and pilot-based transition into practice.Descriptors:", "num_citations": "11\n", "authors": ["5"]}
{"title": "Evidence-based software processes\n", "abstract": " Many software projects fail because they commit to a set of plans and specifications with little evidence that if these are used on the project, they will lead to a feasible system being developed within the project\u0393\u00c7\u00d6s budget and schedule. An effective way to avoid this is to make the evidence of feasibility a first-class developer deliverable that is reviewed by independent experts and key decision milestones: shortfalls in evidence are risks to be considered in going forward. This further implies that the developer will create and follow processes for evidence development. This paper provides processes for developing and reviewing feasibility evidence, and for using risk to determine how to proceed at major milestones. It also provides quantitative result on \u0393\u00c7\u00a5how much investment in evidence is enough,\u0393\u00c7\u00a5 as a function of the project\u0393\u00c7\u00d6s size, criticality, and volatility.", "num_citations": "11\n", "authors": ["5"]}
{"title": "Accurate estimates without calibration?\n", "abstract": " Most process models calibrate their internal settings using historical data. Collecting this data is expensive, tedious, and often an incomplete process.               Is it possible to make accurate software process estimates without historical data? Suppose much of uncertainty in a model comes from a small subset of the model variables. If so, then after (a)\u252c\u00e1ranking variables by their ability to constrain the output; and (b)\u252c\u00e1applying a small number of the top-ranked variables; then it should be possible to (c)\u252c\u00e1make stable predictions in the constrained space.               To test that hypothesis, we combined a simulated annealer (to generate random solutions) with a variable ranker. The results where quite dramatic: in one of the studies in this paper, we found process options that reduced the median and variance of the effort estimates by a factor of 20. In ten case studies, we show that the estimates generated in this manner\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["5"]}
{"title": "Developing a process framework using principles of value\u0393\u00c7\u00c9based software engineering\n", "abstract": " In this article we present a software process framework using the 4 + 1 theory and principles of value\u0393\u00c7\u00c9based software engineering (VBSE). The value\u0393\u00c7\u00c9based process framework serves as a 6\u0393\u00c7\u00c9step process guide, and explains critical interactions between the five theories in the 4 + 1 theory of value\u0393\u00c7\u00c9based software engineering. This article also applies the process framework to a supply chain organization through a case study analysis to illustrate its strength in practice. Copyright \u252c\u2310 2007 John Wiley & Sons, Ltd.", "num_citations": "11\n", "authors": ["5"]}
{"title": "Improving software productivity\n", "abstract": " Computer hardware productivity continues to increase by leaps and bounds, while software productivity seems to be barely holding its own. Central processing units, random access memories, and mass memories improve their price\u0393\u00c7\u00f4performance ratios by orders of magnitude per decade, while software projects continue to grind out production-engineered code at the same old rate of one to two delivered lines of code per man-hour. Yet, if software is judged by the same standards as hardware, its productivity looks pretty good. One can produce a million copies of Lotus 1-2-3 at least as cheaply as a million copies of the Intel 286. Database management systems that cost $5 million 20 years ago can now be purchased for $99.95.The commodity for which productivity has been slow to increase is custom software. Clearly, if you want to improve your organization\u0393\u00c7\u00d6s software price\u0393\u00c7\u00f4performance, one major principle is \u0393\u00c7\u00a3Don\u0393\u00c7\u00d6t build custom software where mass-produced software will satisfy your needs.\u0393\u00c7\u00a5 However, even with custom software, a great deal is known about how to improve its productivity, and even increasing productivity by a factor of 2 will make a significant difference for most organizations. This article discusses avenues of improving productivity for both custom and massproduced software. Its main sections cover the following topics:", "num_citations": "11\n", "authors": ["5"]}
{"title": "A research agenda for systems of systems architecting\n", "abstract": " This paper documents the activity of a workshop on defining a research agenda for Systems of Systems (SoS) Architecting, which was held at USC in October 2006. After two days of invited talks on critical success factors for SoS engineering, the authors of this paper convened for one day to brainstorm topics for the purpose of shaping the near\u0393\u00c7\u00c9term research agenda of the newly convened USC Center for Systems & Software Engineering. The output from the workshop is a list of ten high\u0393\u00c7\u00c9impact items with corresponding research challenges in the context of SoS Architecting. Each item includes a description of the research challenges, its link to contemporary academic or industrial problems, and reasons for advocacy of that area. The items were assessed in terms of value and difficulty to determine a prioritization both for the CSSE's future research agenda and for others in the field.", "num_citations": "11\n", "authors": ["5"]}
{"title": "Coping with the cone of uncertainty: an empirical study of the SAIV process model\n", "abstract": " There is large uncertainty with the software cost in the early stages of software development due to requirement volatility, incomplete understanding of product domain, reuse opportunities, market change, etc. This makes it an increasingly challenging issue to deliver software on time, within budget, and with satisfactory quality in the IT field. In this paper, we introduce the Schedule as Independent Variable (SAIV) approach, and present the empirical study of how it is used to cope with the uncertainty of cost, and deliver customer satisfactory products in 8 USC (University of Southern California) projects. We also investigate the success factors and best practices in managing the uncertainty of cost.", "num_citations": "11\n", "authors": ["5"]}
{"title": "Formalizing informal stakeholder decisions--A hybrid method approach\n", "abstract": " Decisions are hard to make when available information is incomplete, inconsistent, and ambiguous. Moreover, good-sufficiently complete, consistent, traceable, and testable-requirements are a prerequisite for successful projects. Without understanding what the stakeholders really want and need and writing these requirements in a concise, understandable and testable manner, projects will not develop what the stakeholders wanted leading to either major late rework or project termination. During the development of the WinWin negotiation model and the EasyWinWin requirements negotiation method, we have gained considerable experience in capturing decisions made by stakeholders in over 100 projects. However, the transition from informal decisions to requirements specification is still a challenging problem. Based on our analysis of the projects to date, we have developed an integrated set of gap-bridging\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["5"]}
{"title": "Tailor the value-based software quality achievement process to project business cases\n", "abstract": " This paper proposes a risk-based process strategy decision-making approach. To improve the flexibility in applying the Value-Based Software Quality Achievement (VBSQA) process framework, we embed the risk-based process strategy decision-making approach into the VBSQA process framework. It facilitates project managers to tailor the VBSQA process framework to different project business cases (schedule-driven, product-driven, and market trend-driven). A real world ERP (Enterprise Resource Planning) software project (DIMS) in China is used as an example to illustrate different process strategies generated from process tailoring.", "num_citations": "11\n", "authors": ["5"]}
{"title": "Realizing the benefits of the CMMISM with the CeBASE method\n", "abstract": " Future systems will be increasingly software\u0393\u00c7\u00c9intensive, but the type of software development they will need is not well covered by current development and maturity models such as the waterfall model and Software Capability Maturity Model\u252c\u00ab (CMM\u252c\u00ab). Future development of software\u0393\u00c7\u00c9intensive systems will need situation\u0393\u00c7\u00c9specific balancing of discipline and flexibility to address such issues as COTS, open source, distribution, mobility rapid change, agents, collaboration support, and simultaneous achievement of rapid development and high dependability. This article shows how the CMMISM's integration of modern systems engineering, software engineering, and integrated process and product development concepts provides a framework for redressing the shortfalls of the Software CMM\u252c\u00ab, and for enabling projects and organizations to achieve the right balance of discipline and flexibility for their particular situations\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["5"]}
{"title": "Empirical analysis of CASE tool effects on software development effort\n", "abstract": " During the last couple of decades, CASE (Computer Aided Software Engineering) tools have played a critical role in improvement of software productivity and quality by assisting tasks in software development processes. Many initiatives in the field were pursued in the 1980\u0393\u00c7\u00d6s and 1990\u0393\u00c7\u00d6s to provide more effective CASE technologies and development environments. Even though the CASE field is no longer active research area, most software development teams use a huge range of CASE tools that are typically assembled over some period with the hope of productivity and quality improvements throughout the software development process. The variety and proliferation of tools in the current CASE market makes it difficult to understand what kinds of tasks are supported and how much effort can be reduced by using CASE tools. In this paper, we provide a classification of CASE tools by activity coverage in a software development lifecycle. We also report a experimental result of Bayesian analysis on CASE tool effects with a extended set of tool rating scales from COCOMO (COnstructive COst MOdel) II with which CASE tools are effectively evaluated.", "num_citations": "11\n", "authors": ["5"]}
{"title": "Empirical observations on COTS software integration effort based on the initial COCOTS calibration database\n", "abstract": " As the use of commercial-of-the-shelf (COTS) components becomes ever more prevalent in the creation of large software systems, the need for the ability to reasonably predict the true lifetime cost of using such software components grows accordingly. This paper presents empirically-based findings about the effort associated with activities found to be significant in the development of systems using COTS components. The findings are based upon data collected for the purpose of calibrating the COCOTS [1, 2] COTS software integration cost model, an extension to the COCOMO II [3] cost model designed to capture costs COCOMO does not. A brief overview of COCOTS is presented to put the data in perspective, including its relation to COCOMO II. A set of histograms is then shown summarizing the effort data collected to date. The paper concludes with some observations suggested by an examination of that calibration data.", "num_citations": "11\n", "authors": ["5"]}
{"title": "Employing unas technology for software architecture education at the university of southern california\n", "abstract": " There are many reasons for integrating industry's software technology into the university system at the graduate level. This paper discusses efforts by TRW, Rational Software Corporation, and the University of Southern California's Software Engineering department to integrate software architecture technology into USC's software engineering curriculum. University and industry goals and strategies are first addressed followed by a purpose for using megaprogramming and middleware technology within software engineering education. Next, the UNAS architectural paradigm and components are described as well as how UNAS fits into a software engineering curriculum. The next section proposes software engineering courseware useful in both industry as well as university settings. Finally, an account of completed activities and lessons learned.", "num_citations": "11\n", "authors": ["5"]}
{"title": "Defense Advanced Research Projects Agency,\u0393\u00c7\u00a5\n", "abstract": " \" Megaprogramming\" refers to the practice of building and evolving computer software component by component Megaprogramming builds on the processes and technologies of software reuse, software engineering environments, software architecture engineering, and application generation in order to provide a component-oriented product-line approach to software development In the product line approach, management incentives and technology support can be structured to favor the aggregate return on investment over a set of related software products, even when certain portions of the investment-whose benefits are realized across the product line-may be higher than they would be if products were managed individually.Particular aspects of the megaprogramming approach are in use in specific sectors, with significant impact. The merits of architecture-oriented design and reuse of software assets are well understood. Experience has been gained, starting in the 1950's with user-group component libraries such as the IBM SHARE (Society to Help Avoid Redundant Effort) library, and through seminal papers on mass-produced software components such as", "num_citations": "11\n", "authors": ["5"]}
{"title": "What we really need are process model generators\n", "abstract": " In the papers by Dr. Amdahl* and Dr. Chent an extremely powerful CPU was described. I would like to illustrate on some problems just how powerful it is, in order to better pinpoint its actual performance. With a CPU of this sort, we had a few problems in the actual design: to be able to get problems or applications in and out of the CPU, and to", "num_citations": "11\n", "authors": ["5"]}
{"title": "Computer Performance Analysis: Framework and Initial Phases for a Performance Improvement Effort\n", "abstract": " The report, designed primarily as an aid for getting started, provides a procedural framework for a performance improvement effort and suggests specific techniques for the initial phases. the suggested procedure consists of seven phases 1 understanding the system, 2 analyzing operations, 3 formulating performance improvement hypotheses, 4 analyzing probable cost-effectiveness of modifications, 5 testing specific hypotheses, 6 implementing appropriate combinations of modification and 7 testing the effectiveness of the implemented modifications.Descriptors:", "num_citations": "11\n", "authors": ["5"]}
{"title": "The value of software architecture recovery for maintenance\n", "abstract": " In order to maintain a system, it is beneficial to know its software architecture. In the common case that this architecture is unavailable, architecture recovery provides a way to recover an architectural view of the system. Many different methods and tools exist to provide such a view. While there have been taxonomies of different recovery methods and surveys of their results along with measurements of how these results conform to expert's opinions on the systems, there has not been a survey that goes beyond a simple automatic comparison. Instead, this paper seeks to answer questions about the viability of individual methods in given situations, the quality of their results and whether these results can be used to indicate and measure the quality and quantity of architectural changes. For our case study, we look at the results of recoveries of versions of Android, Apache Hadoop and Apache Chukwa obtained by\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["5"]}
{"title": "Improving missing issue-commit link recovery using positive and unlabeled data\n", "abstract": " Links between issue reports and corresponding fix commits are widely used in software maintenance. The quality of links directly affects maintenance costs. Currently, such links are mainly maintained by error-prone manual efforts, which may result in missing links. To tackle this problem, automatic link recovery approaches have been proposed by building traditional classifiers with positive and negative links. However, these traditional classifiers may not perform well due to the inherent characteristics of missing links. Positive links, which can be used to build link recovery model, are quite limited as the result of missing links. Since the construction of negative links depends on the number of positive links in many existing approaches, the available negative links also become restricted. In this paper, we point out that it is better to consider the missing link problem as a model learning problem by using positive and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["5"]}
{"title": "The incremental commitment spiral model (ICSM): principles and practices for successful systems and software.\n", "abstract": " \u0393\u00c7\u00f3 If you are experiencing any problems/issues, please press the F5 key on your keyboard if you\u0393\u00c7\u00d6re using Windows, or Command+ R if you\u0393\u00c7\u00d6re on a Mac, to refresh your console, or close and re-launch the presentation. You can also view the Webcast Help Guide, by clicking on the \u0393\u00c7\u00a3Help\u0393\u00c7\u00a5 widget in the bottom dock.", "num_citations": "10\n", "authors": ["5"]}
{"title": "Learning project management decisions: a case study with case-based reasoning versus data farming\n", "abstract": " Background: Given information on just a few prior projects, how do we learn the best and fewest changes for current projects? Aim: To conduct a case study comparing two ways to recommend project changes. 1) Data farmers use Monte Carlo sampling to survey and summarize the space of possible outcomes. 2) Case-based reasoners (CBR) explore the neighborhood around test instances. Method: We applied a state-of-the data farmer (SEESAW) and a CBR tool ()'V2) to software project data. Results: CBR with )'V2 was more effective than SEESAW's data farming for learning best and recommended project changes, effectively reducing runtime, effort, and defects. Further, CBR with )'V2 was comparably easier to build, maintain, and apply in novel domains, especially on noisy data sets. Conclusion: Use CBR tools like )'V2 when data are scarce or noisy or when project data cannot be expressed in the required\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["5"]}
{"title": "Analysis of stakeholder/value dependency patterns and process implications: A controlled experiment\n", "abstract": " Different classes of information system stakeholders depend on different values to be successful. Understanding stakeholders' value dependencies is critical for developing software intensive systems. However, there is no universal one-size-fits-all stakeholder/ value metric that can be applied for a given system. This paper presents an analysis of major classes of stakeholders' value priorities using the win-win prioritization results from 16 real-client graduate software engineering course projects. Findings from this controlled experiment further verify and extend the hypotheses that \"different stakeholders have different value propositions\", bridge the value understanding gaps among different stakeholders, beneficial for further reasoning about stakeholders' utility functions and for providing process guidance for software projects involving various classes of stakeholders.", "num_citations": "10\n", "authors": ["5"]}
{"title": "Cost models for future software life cycle processes: COCOMO 2.0\n", "abstract": " Current software cost estimation models, such as the 1981 Constructive Cost Model (COCOMO) for software cost estimation and its 1987 Ada COCOMO update, have been experiencing increasing difficulties in estimating the costs of software developed for new life cycle processes and capabilities. These include nonsequential and rapid-development process models; reuse-driven approaches involving commercial off-the-shelf (COTS) packages, re-engineering, applications composition, and applications generation capabilities; object-oriented approaches supported by distributed middleware; and software process maturity initiatives. This article summarizes research in deriving a baseline COCOMO 2.0 model tailored to these new forms of software development, including rationale for the model decisions. The major new modeling capabilities of COCOMO 2.0 are a tailorable family of software sizing models, involving Object Points, Function Points, and Source Lines of Code; nonlinear models for software reuse and re-engineering; an exponent-driver approach for modeling relative software diseconomies of scale; and several additions, deletions and updates to previous COCOMO effort-multiplier cost drivers. This model is serving as a framework for an extensive current data collection and analysis effort to further refine and calibrate the model\u0393\u00c7\u00d6s estimation capabilities.", "num_citations": "10\n", "authors": ["5"]}
{"title": "Issues in computer performance evaluation: some consensus, some divergence\n", "abstract": " This paper summarizes the results of an ACM/NBS Workshop on Computer Performance Evaluation. Computer Performance Evaluation (CPE) was selected as the subject of an ACM/NBS Workshop because of the significant leverage CPE activities can have on computer usage. This paper describes a number of conclusions abstracted from the discussions as well as presenting recommendations formally adopted by the participants. While several of these conclusions indicate that improvements are needed in performance analysis tools, another suggests that improved application of CPE could be achieved by better documentation of analysis approaches. More integration of data collection and modeling are considered necessary for the performance analysis field to develop its full potential. Participants noted that the common emphasis on data collection or modeling, to the exclusion of considering objectives, often\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["5"]}
{"title": "POGO: programmer-oriented graphics operation\n", "abstract": " Wide-scale application of interactive computer graphics (ICG) is currently inhibited by two major difficulties:", "num_citations": "10\n", "authors": ["5"]}
{"title": "Keeping the upper hand in the man-computer partnership\n", "abstract": " Contents Augmenting creativity Computer characteristics Artificial intelligence The AIAA computer subcommittee Hardware trends Large-scale integrated circuits LSIs Hardward performance Computer organization trends Man-Computer partnership On-Board The on-board information explosion Real time operations Conversational computing On-line systems and computer graphics Implementation problems Changing ones problem-solving methodology The computer downstairs Diversity and change New directions Social implications Numerical analysis Software Systems analysis Procrustean computer system Maintaining relevance Creativity aided, not replaced.Descriptors:", "num_citations": "10\n", "authors": ["5"]}
{"title": "Determining relevant training data for effort estimation using Window-based COCOMO calibration\n", "abstract": " ContextA software estimation model is often built using historical project data. As software development practices change over time, however, a model based on past data may not make accurate predictions for a new project.ObjectivesWe investigate the use of moving windows to determine relevant training data for COCOMO calibration.MethodWe present a windowing calibration approach to calibrating COCOMO and assess performance of effort estimation models calibrated using windows and all data.ResultsOur results show that calibrating COCOMO using small windows of the most recently completed projects generates superior estimates than using all available historical projects. Large windows tend to produce worse estimates.ConclusionsThis study provides empirical evidence to support the use of small windows of projects completed so far to calibrate models when COCOMO-like data is available\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["5"]}
{"title": "An evidence-based systems engineering (SE) data item description\n", "abstract": " Evidence-based SE is an extension of model-based SE that emphasizes not only using SysML or other system models as a basis of program decisions, but also the use of other models to produce evidence that the system models describe a feasible system. Such evidence is generally desired, but often is not produced because it is not identified as a project deliverable in a Data Item Description (DID). Going forward with such unproven solutions frequently leads to large program overruns.Based on experience in developing and using such a DID on a very large project, we summarize the content and form of such a DID, and a rationale for its use. Its basic content is evidence that if a system were produced with the specified Architecture, it would:.\u0393\u00c7\u00f3Satisfy the specified Operational Concept and Requirements;\u0393\u00c7\u00f3Be developable within the specified Budget and Schedule;\u0393\u00c7\u00f3Provide a superior return on investment over\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["5"]}
{"title": "Reducing estimation uncertainty with continuous assessment: tracking the\" cone of uncertainty\"\n", "abstract": " Accurate software cost and schedule estimations are essential especially for large software projects. However, once the required efforts have been estimated, little is done to recalibrate and reduce the uncertainty of the initial estimates. To address this problem, we have developed and used a framework to continuously monitor the software project progress and readjust the estimated effort utilizing the Constructive Cost Model II (COCOMO II) and the Unified CodeCount Tool developed by the University of Southern California (USC). As a software project progresses, we gain more information about the project itself, which can then be used to assess and re-estimate the effort required to complete the project. With more accurate estimations and less uncertainties, the quality and goal of project outcome can be assured within the available resources. The paper thus also provides and analyzes empirical data on how\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["5"]}
{"title": "Experiences in developing and applying a software engineering technology testbed\n", "abstract": " A major problem in empirical software engineering is to determine or ensure comparability across multiple sources of empirical data. This paper summarizes experiences in developing and applying a software engineering technology testbed. The testbed was designed to ensure comparability of empirical data used to evaluate alternative software engineering technologies, and to accelerate the technology maturation and transition into project use. The requirements for such software engineering technology testbeds include not only the specifications and code, but also the package of instrumentation, scenario drivers, seeded defects, experimentation guidelines, and comparative effort and defect data needed to facilitate technology evaluation experiments. The requirements and architecture to build a particular software engineering technology testbed to help NASA evaluate its investments in software\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["5"]}
{"title": "A risk-driven process decision table to guide system development rigor\n", "abstract": " The Incremental Commitment Model (ICM) organizes systems engineering and acquisition processes in ways that better accommodate the different strengths and difficulties of hardware, software, and human factors engineering approaches. As with other models trying to address a wide variety of situations, its general form is rather complex. However, its risk-driven nature has enabled us to determine a set of twelve common risk patterns and organize them into a decision table that can help new projects converge on a process that fits well with their particular process drivers. For each of the twelve special cases, the decision table provides top-level guidelines for tailoring the key activities of the ICM, along with suggested lengths between each internal system build and each external system increment delivery. This paper elaborates on each of the twelve cases and provides examples of their use.", "num_citations": "9\n", "authors": ["5"]}
{"title": "Seventh Workshop on software quality\n", "abstract": " Software Quality has been a major challenge throughout Information Technology projects. Whether it is in software development, in software integration or whether it is in the implementation or customization of shrink-wrapped software, quality is regarded as a major issue. In the last couple of decades, much software engineering research has focused on standards, methodologies and techniques for improving software quality, measuring software quality and software quality assurance. Most of this research is focused on the internal/development view of quality. More recent studies have made attempts to understand the stakeholder view of quality. With globalization, many new challenges affect software quality. Not only do we need to understand the many stakeholder views of quality, we now need to consider the cultural issues, and the outsourcing issues. The Seventh Workshop on Software Quality aims to bring\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["5"]}
{"title": "3.4. 3 The Value\u0393\u00c7\u00c9Based Theory of Systems Engineering: Identifying and Explaining Dependencies\n", "abstract": " The Value\u0393\u00c7\u00c9Based Theory of Systems Engineering brings together many interdisciplinary theoretical lenses into a state of synchrony and allows reasoning about systems in different dimensions, and at various levels of abstraction. The theory's primary strength is in its ability to identify and work through the dependencies of most socio\u0393\u00c7\u00c9political\u0393\u00c7\u00c9technical systems and explain success in such contexts by situating the success\u0393\u00c7\u00c9critical stakeholders at the forefront. In this paper we present the 4+1 theoretical lenses of the Value\u0393\u00c7\u00c9Based Theory of Systems Engineering with a core emphasis on the Dependency Theory \u0393\u00c7\u00f4 the first and most complex of the four component theories.", "num_citations": "9\n", "authors": ["5"]}
{"title": "A value-based software process framework\n", "abstract": " This paper presents a value-based software process framework that has been derived from the 4+1 theory of value-based software engineering (VBSE). The value-based process framework integrates the four component theories \u0393\u00c7\u00f4 dependency, utility, decision, and control, to the central theory W, and orients itself as a 7-step process guide to practice value-based software engineering. We also illustrate applying the process framework to a supply chain organization through a case study analysis.", "num_citations": "9\n", "authors": ["5"]}
{"title": "A contextualized study of COTS-based e-service projects\n", "abstract": " Properly recording the context factors of empirical results is essential for comparison and integration of results from different studies and for assessing the relevance of a given result to one\u0393\u00c7\u00d6s own environment. COTS-based application (CBA) developers need both empirical data and context data for choosing among current and newlyemerging candidate technologies based on solid evidence that they will work cost effectively under the conditions of their particular projects. Previous empirical research on COTS-based development (CBD) has produced various insights on the critical success factors of CBD. Such accumulations also produce various experience/knowledge bases on which the contextualized longitudinal analysis of CBA can be performed. This poster presents an initial contextualized longitudinal analysis of CBA\u0393\u00c7\u00d6s by identifying a set of project context factors as contextualizing meta-data which\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["5"]}
{"title": "Mastering rapid delivery and change with the SAIV process model\n", "abstract": " Ensuring on time, within-budget delivery is increasingly difficult in the information technology (IT) field because of the increasingly rapid rate of requirements volatility of IT systems under development. This paper describes the Model-Based (System) Architecting and Software Engineering (MBASE)'s Schedule as Independent Variable (SAIV) approach to this problem, and illustrates the nature of the solution with examples.", "num_citations": "9\n", "authors": ["5"]}
{"title": "NSF workshop on a software research program for the 21st century\n", "abstract": " In August 1998 the President's Information Technology Advisory Committee (PITAC) submitted an Interim Report emphasizing the importance of sottw~ se l: o the nation and calling for a significant new federal investment in software researck An NSF workshop subsequently brought together representatives of a broad segment of the software conananity to discuss the software research agenda. Workshop l~ trticipants included researchers and developers from geographically diverse organizations in academia and industry.A major theme of the PITAC Report was the\" fragility\" of our software infrastructm'e, where fragility means\" unreliability, lack of security, perforrm~ cg lapses, errors, and difficulty in upgrading.\" The P1TAC was:~ aicularly concerned by these failings, because software now affects almost every aspect of personal and professional life in the nation. It manages our telephone networks and nuclear\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["5"]}
{"title": "Use of Ada for FAA's advanced automation system (AAS)\n", "abstract": " Use of Ada for FAA's advanced automation system (AAS) | Software Risk Management ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksSoftware Risk ManagementUse of Ada for FAA's advanced automation system (AAS) chapter Use of Ada for FAA's advanced automation system (AAS) Share on Authors: Victor Robert Basili profile image VR Basili View Profile , Barry William Boehm profile image BW Boehm View Profile , Judith A Clapp profile image JA Clapp View Profile , Dale J Gaumer profile image D. Gaumer View Profile , M Holden profile image M. Holden View Profile , JK Summers profile image JK Summers View Profile & \u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["5"]}
{"title": "Computers in developing nations\n", "abstract": " The origins, the mechanisms, and the consequences of the explosive expansion of computing have been widely investigated in the context of Western socaety, but the development and impact of computing in nations with a less technological orientation has been largely ignored. In developing countries this impact is certain to be greater than in societies where technology and technocracy already have gained preeminence over a (relatively) long time. Moreover, the introduction and growth of computing will be greatly influenced by the conditions under which they occur.From another point of view, there is great need that this topic be investigated. There is a great potential impact of computers on even: the smallest and poorest of nations. Well-used, computers can increase the utility of human and material resources. Poorly used, they provide yet a further drain on those same strained resources. To avoid such waste it\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["5"]}
{"title": "The impact of software security practices on development effort: An initial survey\n", "abstract": " Background: Software projects are facing the need to adopt security practices during the software development life cycle (SDLC). Nevertheless, the amount of effort to be invested in order to achieve a certain level of software security is not clear yet. Aims: The goal of this study is to get an overview of the application of software security practices in the industry and to identify the impact of the introduction of such activities in software development projects in terms of effort/cost. Method: We conducted a survey on a software security group of a professional social network by applying a random sampling strategy to establish a representative set of participants. Results: The questionnaire was fully answered by 110 participants, from the 808 profiles that were invited from the sampling frame. The results show that security practices have been applied thoroughly in the projects and revealed high variability in secure software\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["5"]}
{"title": "Rapid realization of executable domain models via automatic code generation\n", "abstract": " The gap between design and implementation always exists because changes happen frequently throughout software development process, along with rapid release cycles, and accompanied by time constraints and limited resources. The focus of our work is to reduce this gap for service-oriented projects. We proposed an approach which considers both technical strategies and agile methods, trying to streamline the progression from design to implementation at a relatively early phase, and then throughout the whole development lifecycle. Automatic code generation has the potential to reduce above problems to a certain extent. This paper describes our efforts to enable rapid and continuous delivery while leveraging parallelism in development via automatic code generation - specifically making domain models instantly executable. We describe a code generator that has been built to enable parallel development\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["5"]}
{"title": "Tradespace and Affordability-Phase 2\n", "abstract": " The objective of this research project is to develop better methods of conducting system trades involving aspects of the system that are difficult to quantify, such as resilience and safety, the cost, schedule and performance impacts of such trades. This technical report summarizes the work done in phase 2 of RT46. The focus of Phase 2 is to apply the methods and tools developed in Phase 1 on problems relevant to DoD, ideally using the information available from development of a large weapon system, or a large automated information system, Ideally, the SERC will work with the system developer to gain a deep understanding of the strengths and limitations of the tradespace tools methods developed under phase 1. Phase 2 activities will expand the set of ilities represented in the tradespace. The information learned from Phase 2 will be used to improve the frameworks and tools developed in the phase activities.Descriptors:", "num_citations": "8\n", "authors": ["5"]}
{"title": "Making winners for both education and research: Verification and validation process improvement practice in a software engineering course\n", "abstract": " An empirical study is provided on teaching Verification & Validation (V&V) process practice in a real-client graduate level software engineering course which makes students and researchers mutual winners. From our observation and experiences during the course, on the education side, several reflection-in-action techniques are used to educate and train students. These include inspections, architecture review boards, grading criteria, monitoring of their quality management plans, student critiques of their project experiences, and client evaluations. On the research side, students' feedback, evaluation, and critiques provide not only previous empirical evidence for the researchers' research proposal, but also great opportunities to refine their research methods from lessons learned from the course, and in turn to improve the course quality.", "num_citations": "8\n", "authors": ["5"]}
{"title": "A Risk-Driven Decision Table for Software Process Selection.\n", "abstract": " A Risk-Driven Decision Table for Software Process Selection Page 1 University of Southern California Center for Systems and Software Engineering Barry Boehm, Jo Ann Lane, Supannika Koolmanojwong University of Southern California ICSP 2010 Keynote July 9, 2010 A Risk-Driven Decision Table for Software Process Selection Page 2 University of Southern California Center for Systems and Software Engineering July 9, 2010 \u252c\u2310 USC-CSSE 2 Outline \u0393\u00c7\u00f3 No one-size-fits-all software process models \u0393\u00c7\u00f3 A process model generator is better \u0393\u00c7\u00f4 Risk-driven Incremental Commitment Model (ICM) \u0393\u00c7\u00f3 A process decision table is even better \u0393\u00c7\u00f3 Determined from ICM usage risk patterns \u0393\u00c7\u00f4 Example: Architected Agile \u0393\u00c7\u00f3 Conclusions and references Page 3 University of Southern California Center for Systems and Software Engineering Candidate One-Size-Fits-All Model Shortfalls \u0393\u00c7\u00f3 Waterfall, V Models \u0393\u00c7\u00f4 Emergent requirements, COTS, \u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["5"]}
{"title": "Development of 3-Year Roadmap to Transform the Discipline of Systems Engineering\n", "abstract": " As systems continue to grow in size and complexity, it has become clear that existing Systems Engineering SE methods, processes and tools are becoming increasingly inadequate. The SET project is intended to identify the gaps and bring about the necessary transformation in Systems Engineering to satisfy the needs of the complex systems life cycle. Accomplishing this transformation requires a fundamental rethinking of current SE practices. The SET project is focused on first principles and stripping away non-essential activities while being cognizant of recent trends in SE. A number of trends collectively accelerate this challenge. Growing system complexity and criticality raise vulnerability. The ascendancy of software as the preferred solution continues in the face of significant gaps in our ability to understand, validate and manage large evolving software ecosystems. The increasing speed of technological change, the rapid evolution of threats, and the decreasing schedules for development all lead to the sense that time itself is compressing. New systems envisioned by the defense and intelligence communities reflect, embrace and reinforce these trends.Descriptors:", "num_citations": "8\n", "authors": ["5"]}
{"title": "Assessing and estimating corrective, enhancive, and reductive maintenance tasks: A controlled experiment\n", "abstract": " This paper describes a controlled experiment of student programmers performing maintenance tasks on a C++ program. The goal of the study is to assess the maintenance size, effort, and effort distribution of three different maintenance types and to describe estimation models to predict the programmer's effort on maintenance tasks. The results of our study suggest that corrective maintenance is much less productive than enhancive and reductive maintenance. Our study also confirms the previous results which conclude that corrective and reductive maintenance requires large proportions of effort on program comprehension activity. Moreover, the best effort model we obtained from fitting the experiment data can estimate the time of 79% of the programmers with the error of 30% or less.", "num_citations": "8\n", "authors": ["5"]}
{"title": "The ROI of systems engineering: Some quantitative results\n", "abstract": " This paper presents quantitative results on the return on investment of systems engineering (SE-ROI) from an analysis of the 161 software projects in the COCOMO II database. The analysis shows that, after normalizing for the effects of other cost drivers, the cost difference between projects doing a minimal job of software systems engineering - as measured by the thoroughness of its architecture definition and risk resolution - and projects doing a very thorough job was 18% for small projects and 92% for very large software projects as measured in lines of code.", "num_citations": "8\n", "authors": ["5"]}
{"title": "2.3. 2 Towards a Work Breakdown Structure for Net Centric System of Systems Engineering and Management\n", "abstract": " As the system engineering industry sees an increasing focus on the lifecycle development, acquisition, and sustainment of net\u0393\u00c7\u00c9centric Systems of Systems (SoS) and Family of Systems (FoS), organizations find the need to evolve current processes and tools to better handle the increased scope, scale, and complexity of these efforts. One such tool, the Work Breakdown Structure (WBS) is important in planning and execution of program activities as requirements and goals of the program evolve. This paper provides an overview of the limitations of current WBSs with respect to SoS efforts and presents a proposed WBS structure that more adequately reflects the evolving processes and cross\u0393\u00c7\u00c9organizational complexities.", "num_citations": "8\n", "authors": ["5"]}
{"title": "COCOTS risk analyzer\n", "abstract": " Most risk analysis tools and techniques require the user to enter a good deal of information before they can provide useful diagnoses. The COCOTS risk analyzer described here enables the user to obtain a COTS glue code integration risk analysis with no inputs other than the set of glue code cost drivers the user enters to obtain a COCOTS glue code integration effort estimate. The risk assessment is based on an expert Delphi analysis of the relative risks involved in the most critical combinations of COCOTS cost driver ratings. The evaluation of our approach shows that it has done an effective job of estimating the relative risk levels of a sample of small USC e-services projects.", "num_citations": "8\n", "authors": ["5"]}
{"title": "An Empirical Process for Building and Validating Software Engineering Parametric Models\n", "abstract": " Parametric modeling is a statistical technique whereby a dependent variable is estimated based on the values of and the relationships between the independent variable (s). The nature of the dependent variable can vary greatly based on one\u0393\u00c7\u00d6s domain of interest. In software engineering, parametric models are often used to help predict a system\u0393\u00c7\u00d6s development schedule, cost-to-build, and quality at various stages of the software lifecycle. In this paper, we discuss the use of parametric modeling in software engineering and present a nine-step parametric modeling process for creating, validating, and refining software engineering parametric models. We illustrate this process with three software engineering parametric models. Each of these models followed the nine-steps in different ways due to the research technique, the nature of the model, and the variability of the data. The three models have been shown to be effective estimators of their respective independent variables. This paper aims to assist other software engineers in creating parametric models by establishing important steps in the modeling process and by demonstrating three variations on following the nine-step process.", "num_citations": "8\n", "authors": ["5"]}
{"title": "Software model connectors: Bridging models across the software lifecycle\n", "abstract": " Numerous notations, methodologies, and tools exist to support software system modeling. While individual models help to clarify certain system aspects, the large number and heterogeneity of models may ultimately hamper the ability of stakeholders to communicate about a system. A major reason for this is the discontinuity of information across different models. In this paper, we present an approach for dealing with that discontinuity. We introduce a set of \u0393\u00c7\u00a3connectors\u0393\u00c7\u00a5 to bridge models, both within and across the \u0393\u00c7\u00a3upstream\u0393\u00c7\u00a5 activities in the software development lifecycle (specifically, requirements, architecture, and design). While the details of these connectors are dependent upon the source and destination models, they share a number of underlying characteristics. These characteristics can be used as a starting point in providing a general understanding of software model connectors. We illustrate our approach by applying it to a large-scale system we are currently designing and implementing in collaboration with a third-party organization.", "num_citations": "8\n", "authors": ["5"]}
{"title": "Optimizing software product integrity through life-cycle process integration\n", "abstract": " Managed and optimized\u0393\u00c7\u00f6these are the names for the levels 4 and 5 of the Capability Maturity Model (CMM) respectively. With that the Software Engineering Institute (SEI) pays tribute to the fact that, after the process has been defined, higher process maturity, and with that higher product maturity, can only be achieved by improving and optimizing the life-cycle process itself. In the last three years, we had had the opportunity to observe more than 50 software development teams in planning, specifying and building library related, real-world applications. This environment provided us with a unique way of introducing, validating and improving the life cycle process with new principles such as the WinWin approach to software development. This paper summarizes the lessons we have learned in our ongoing endeavor to integrate the WinWin life-cycle process. In doing so, we will not only describe what techniques have\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["5"]}
{"title": "Improving the life-cycle process in software engineering education\n", "abstract": " The success of software projects and the resulting software products are highly dependent on the initial stages of the life-cycle process\u0393\u00c7\u00f4the inception and elaboration stages. The most critical success factors in improving the outcome of software projects have often been identified as being the requirements negotiation and the initial architecting and planing of the software system.Not surprisingly, this area has thus received strong attention in the research community. It has, however, been hard to validate the effectiveness and feasibility of new or improved concepts because they are often only shown to work in a simplified and hypothesized project environment. Industry, on the other hand, has been cautious in adopting unproven ideas. This has led to a form of deadlock between those parties.", "num_citations": "8\n", "authors": ["5"]}
{"title": "Analysis of software requirements negotiation behavior patterns\n", "abstract": " Roughly 35 three-person teams played the roles of user, customer, and developer in negotiating the requirements of a library information system. Each team was provided with a suggested set of stakeholder goals and implementation options, but were encouraged to exercise creativity in expanding the stakeholder goals and in creating options for negotiating an eventually satisfactory set of requirements.The teams consisted of students in a first-year graduate course in software engineering at USC. They were provided with training in the Theory W (win-win)[3] approach to requirements determination and the associated USC WinWin groupware support system [5][8]. They were required to complete the assignment in two weeks.", "num_citations": "8\n", "authors": ["5"]}
{"title": "Information Processing/Data Automation Implications of Air Force Command and Control Requirements in the 1980s (CCIP-85). Executive Summary (Revised Edition)\n", "abstract": " The report summarizes the results of a mission analysis conducted at the SAMSO West Coast Study Facility on information processingdata automation implications of Air Force Command and Control Requirements in the 1980s. AuthorDescriptors:", "num_citations": "8\n", "authors": ["5"]}
{"title": "Convergence of best rational Tchebycheff approximations\n", "abstract": " Cf. Achieser [1]. In this paper, we investigate the behavior of the error functional E (n, m, f), in terms of the continuity properties of the function f, for various hypotheses on the growth of the degrees n and m of the numerator and denominator polynomials.? 2 generalizes some results of Walsh [2] concerning the conditions under which E (n, m, f) approaches zero with increasing n and m, while? 3 gives conditions uhder which E (n, m, f) does not approach zero; the results in these sections are shown to have counter-parts when other fundamental Markoff systems in C [a, b] replace the system {1, x, x2,***}.", "num_citations": "8\n", "authors": ["5"]}
{"title": "A systematic literature review of technical debt prioritization\n", "abstract": " Repaying all technical debt (TD) present in a system may be unfeasible, as there is typically a shortage in the resources allocated for TD repayment. Therefore, TD prioritization is essential to best allocate such resources to determine which TD items are to be repaid first and which items are to be delayed until later releases. This study conducts a systematic literature review (SLR) to identify and analyze the currently researched TD prioritization approaches. The employed search strategy strove to achieve high completeness through the identification of a quasi-gold standard set, which was used to establish a search string to automatically retrieve papers from select research databases. The application of selection criteria, along with forward and backward snowballing, identified 24 TD prioritization approaches. The analysis of the identified approaches revealed a scarcity of approaches that account for cost, value, and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["5"]}
{"title": "Recover and RELAX: Concern-oriented software architecture recovery for systems development and maintenance\n", "abstract": " The stakeholders of a system are legitimately interested in whether and how its architecture reflects their respective concerns at each point of its development and maintenance processes. Having such knowledge available at all times would enable them to continually adjust their systems structure at each juncture and reduce the buildup of technical debt that can be hard to reduce once it has persisted over many iterations. Unfortunately, software systems often lack reliable and current documentation about their architecture. In order to remedy this situation, researchers have conceived a number of architectural recovery methods, some of them concern-oriented. However, the design choices forming the bases of most existing recovery methods make it so none of them have a complete set of desirable qualities for the purpose stated above. Tailoring a recovery to a system is either not possible or only through iterative\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["5"]}
{"title": "How does contributors involvement influence open source systems\n", "abstract": " Open source software systems are based on the principle of open collaboration for innovation and production. They highly depend on volunteer developers contributions for their existence and continuity; attracting new volunteer developers is crucial for the OSS community sustainability. However, new developers might be hesitant to join and participate to a project due to many obstacles such as lack of awareness and guidelines in the OSS community and inability for long-term commitment and dedication which might result in a low retention rate. In the OSS community, contributors come from different backgrounds and skill levels, and they have different levels of participation in the system. They can be categorized into core and peripheral based on the frequency of the commits they author. While it is acknowledged that developers have different levels of participation to a software system, little is known about how\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["5"]}
{"title": "Simple empirical software effort estimation model\n", "abstract": " Context: An effort estimation model with more than 20 parameters is not very useful at early conceptual phase if you don't have a logical approach for specifying the input values.Goal: This paper presents a simple approach for predicting software development effort.Method: The regression model uses product size and application types to predict effort. Product size is measured in terms of the equivalent source lines of code. The analysis is based on empirical data collected from 317 very recent projects implemented within the United States Department of Defense over the course of nine years beginning in 2004.Result: Statistical results showed that source lines of code and application type are significant contributors to development effort.Conclusion: The equation is simpler and more viable to use for early estimates than traditional parametric cost models. The effect of product size on software effort shall be\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["5"]}
{"title": "COCOMO II parameters and IDPD: bilateral relevances\n", "abstract": " The phenomenon called Incremental Development Productivity Decline (IDPD) is presumed to be present in all incremental soft-ware projects to some extent. COCOMO II is a popular parametric cost estimation model that has not yet been adapted to account for the challenges that IDPD poses to cost estimation. Instead, its cost driver and scale factors stay constant throughout the increments of a project. While a simple response could be to make these parameters variable per increment, questions are raised as to whether the existing parameters are enough to predict the behavior of an incrementally developed project even in that case. Individual COCOMO II parameters are evaluated with regard to their development over the course of increments and how they influence IDPD. The reverse is also done. In light of data collected in recent experimental projects, additional new variable parameters that either extend\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["5"]}
{"title": "Educating software engineers to become systems engineers\n", "abstract": " Our two-semester USC core software engineering project course CS577ab devotes its first semester to having students learn and do systems engineering on a real-client project. This requires a good deal of just-in-time lectures, tutorials, and homework to prepare the students, and feedback in terms of mentoring, artifact grading, and live milestone reviews to help them succeed. This paper provides some initial motivation and context; discusses our approach to introduce systems engineering into software engineering relative to that in the GSwE 2009 curriculum guidelines, SEBOK draft 2010, and SWEBOK 2004; describes the course practices during the systems engineering and software engineering semesters; and summarizes the project results and conclusions.", "num_citations": "7\n", "authors": ["5"]}
{"title": "New processes for new horizons: the incremental commitment model\n", "abstract": " The wide variety of software-intensive systems needed to support the new horizons of evolving technology, system and software complexity, high dependability, global interoperability, emergent requirements, and adaptability to rapid change make traditional and current one-size-fits-all process models infeasible. This tutorial presents the process framework, principles, practices, and case studies for a new model developed and being used to address these challenges. It has a series of risk-driven decision points that enable projects to converge on whatever combination of agile, plan-driven, formal, legacy-oriented, reuse-oriented, or adaptive processes that best fit a project's situation. The tutorial discusses the decision table for common special cases; exit ramps for terminating non-viable projects; support of concurrent engineering of requirements, solutions and plans; and evidence-based commitment milestones\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["5"]}
{"title": "2.3. 1 Proposed Modification to COSYSMO Estimating Relationship\n", "abstract": " This paper proposes a modification to the Academic COSYSMO estimating relationship to remedy a critical limitation in its current implementation of the cost drivers. The effort multipliers defined for these drivers have an overdramatic impact on the nominal effort, which unrealistically amplifies or compresses the effort estimate. This problem severely limits its practical applications. The newly proposed parametric relationship is inspired by the COCOMO II modeling approach and based on the considerations of the life cycle impact of the cost drivers. Two additional cost drivers are also introduced. The feasibility of the new model definition is examined with a boundary analysis and validated by the analysis of historical data. This work is based on the practical implementation of COSYSMO at BAE Systems. In this paper, we present (1) an analysis of the problem with the current COSYSMO model definition; (2) a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["5"]}
{"title": "Composable risk-driven processes for developing software systems from commercial-off-the-shelf (COTS) products\n", "abstract": " Research experience has suggested that software processes should be thought of as a kind of software, which can be developed into composable component pieces that can be executed to perform different software lifecycle activities. General experience has indicated that the activities conducted while developing COTS-based applications (CBA) differ greatly from those conducted in traditional custom development. The primary research questions addressed in this dissertation are (1) Can these activity differences be characterized and statistically analyzed?(2) If so, can the primary CBA activity classes be organized into a decision framework for projects developing CBA's? The resulting research provides a value-based composable set of processes for CBAs that includes an associated Process Decision Framework (PDF), a set of Composable Process Elements (CPEs), and a COCOTS Risk Analyzer.", "num_citations": "7\n", "authors": ["5"]}
{"title": "Composable Process Elements for Developing COTS-Based Applications\n", "abstract": " In his ICSE 2002 keynote address [3], Robert Balzer issued a challenge to the software engineering community to provide better methods for dealing with COTS-based software systems, and to present them at subsequent ICSE\u0393\u00c7\u00d6s. This paper provides a partial response to this challenge. It presents some data that we have found useful in understanding COTS-based application (CBA) trends and effort distributions. The COTS effort distributions and sequences also suggest a framework for the primary contributions of the paper: a set of composable process elements and a decision framework for using them in the development of CBA\u0393\u00c7\u00d6s. Traditional sequential requirements-design-code-test (waterfall) processes do not work for CBA\u0393\u00c7\u00d6s [11], simply because the decision to use a COTS product constitutes acceptance of many, if not most, of the requirements that led to the product, and to its design and implementation. In fact, it is most often the case that a COTS product\u0393\u00c7\u00d6s capabilities will drive the \u0393\u00c7\u00a3required\u0393\u00c7\u00a5 feature set for the new product rather than the other way around, though the choice of COTS products to be used should be driven by the new project\u0393\u00c7\u00d6s initial set of \u0393\u00c7\u00a3most significant requirements.\u0393\u00c7\u00a5 Additionally, the volatility of COTS products [9] introduces a great deal of recursion and concurrency into CBA processes. Some recent CBA process models have partially addressed these issues by adding CBA extensions to a sequential process framework [8]. These work in some situations, but not in others where the requirements, architecture, and COTS choices evolve concurrently; the example in Section 4 illustrates this point. Other process frameworks\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["5"]}
{"title": "Introducing risk management techniques within project based software engineering courses\n", "abstract": " In 1996, USC switched its core two-semester software engineering course from a hypothetical-project, homework-and-exam course based on the Bloom taxonomy of educational objectives (knowledge, comprehension, application, analysis, synthesis, and evaluation). The revised course is a real-client team-project course based on the CRESST model of learning objectives (content understanding, problem solving, collaboration, communication, and self-regulation). We used the CRESST cognitive demands analysis to determine the necessary student skills required for software risk management and the other major project activities, and have been refining the approach over the last 5 years of experience, including revised versions for one-semester undergraduate and graduate project course at Columbia. This paper summarizes our experiences in evolving the risk management aspects of the project course. These\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["5"]}
{"title": "Risk-based strategic software design: how much COTS evaluation is enough?\n", "abstract": " Risk consideration is a valuable assessment aid when making strategic software design decisions. Expressing development considerations in terms of risk exposures over an independent variable (eg time, cumulative effort, etc.) enables the quantitative assessment of typically qualitative attributes. Assuming total risk exposure is additive over individual risk exposure functions, optimal levels for the individual considerations can be identified as function of loss-magnitude and loss-probability estimates for risk sources. Such levels provide strategic trade off considerations (with respect to risk) and have proven valuable in several previous applications such as \u0393\u00c7\u00a3how much testing is enough\u0393\u00c7\u00a5 with respect to defect removal and market window strategic risk considerations. Here we consider a similar application for making strategic design decisions in determining how much effort (or time) should be spent evaluating COTS products with respect to project cost, market window, and a multitude of COTS assessment attributes such as availability, ease of use, maturity, and vendor support.", "num_citations": "7\n", "authors": ["5"]}
{"title": "The winwin requirements negotiation system: A model-driven approach\n", "abstract": " Requirements Engineering constitutes an important part of Software Engineering. The USC WinWin requirements negotiation system addresses critical issues in requirements engineering including (1) multistakeholder consideration,(2) change management, and (3) groupware support. This paper presents our current research e orts on constructing and reconciling several formal and semi-formal models of the system and its operations, including inter-artifact relationship, artifact life cycles, and equilibrium model. It concentrates on determining the relationships among the various models or views of the WinWin requirements engineering process.", "num_citations": "7\n", "authors": ["5"]}
{"title": "Experimental Results from a Prototype Next-Generation Process Support System\n", "abstract": " The Next Generation Process Model (NGPM) uses the Theory W steps of win condition identification and negotiation to determine the objectives, constraints, and alternatives required to initiate each cycle of the Spiral Model of software development. The Next Generation Process Support System (NGPSS) is an evolving prototype of a groupware support environment for the NGPM. To test the scalability and process support capabilities of the initial NGPSS-0 prototype, we performed a bootstrap experiment using the NGPSS-0 to: 1) identify NGPSS user, customer, developer, and system engineer win conditions for future versions of the NGPSS, 2) identify win condition conflicts, and 3) resolve the conflicts into points of agreement which then transform into objectives, constraints, and alternatives for NGPSS. The experiment partially confirmed each of the four primary experimental hypotheses. These covered the adequacy of NGPSS-0; the comparability of NGPSS win conditions with a previous set of TRW software environment win conditions; the adequacy of win conditions as generators of Spiral Model objectives, constraints, and alternatives; and the adequacy of the bootstrap process in defining the next increment of NGPSS.Current software process models tend to reinforce a sequential consideration of the concerns of the various stakeholders in the software process and its resulting products. In the most frequent sequence, fairly vague expressions of user\u0393\u00c7\u00d6s information processing needs are translated by system analysts into a set of product specifications (sometimes very precise, sometimes fairly general, but often inaccurate in important respects).", "num_citations": "7\n", "authors": ["5"]}
{"title": "An Information System for Educational Management, Vol. 4: Functional Design.\n", "abstract": " The functional design of the information system proposed for the Los Angeles Unified School District (LAUSD) is detailed in this report. The design specifies input data, processing, file formats, and report formats. The implemented system will provide information in four general areas: personnel, program/budget, educational results, and community profile. Many information needs are not specifically provided for because existing information systems serve these functions within the District. For descriptive purposes, the information system may be divided into five subsystems. Four provide data in the categories listed above; the fifth, an inquiry system, allows the decisionmaker to access all information reposited within the system. Each subsystem is described.(Other reports in this series are: LI 003908 through 003910 and LI 003912).(Author)", "num_citations": "7\n", "authors": ["5"]}
{"title": "Probabilistic evaluation of satellite missions involving ground coverage\n", "abstract": " A relation P (a, s, h, i) is developed for the probahility of occurrence of the following event: that a point Q on earth at latitude a comes within a distance s of the ground track of a satellite in orbit at an altitude h and an inclination i, during a single satellite orbital revolution, under the assumption that the point Q is randomly located in longitude with respect to the ascending node of the orbit.", "num_citations": "7\n", "authors": ["5"]}
{"title": "Critical quality factors for rapid, scalable, agile development\n", "abstract": " Agile methods frequently have difficulties with qualities, often specifying quality requirements as stories, e.g., \"As a user, I need a safe and secure system.\" Such projects will generally schedule some capability releases followed by safety and security releases, only to discover user-developer misunderstandings and unsecurable agile code, leading to project failure. Very large agile projects also have further difficulties with project velocity and scalability. Examples are trying to use daily standup meetings, 2-week sprints, shared tacit knowledge vs. documents, and dealing with user-developer misunderstandings. At USC, our Parallel Agile, Executable Architecture research project shows some success at mid-scale (50 developers). We also examined several large (hundreds of developers) TRW projects that had succeeded with rapid, high-quality development. The paper elaborates on their common Critical Quality\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["5"]}
{"title": "Towards better understanding of software maintainability evolution\n", "abstract": " Due to effort and scalability challenges involved in understanding software maintainability evolution, and conflicts and synergies among software quality attributes on a large scale, researchers are forced to adjust the scope\u252c\u00e1and granularity of their analysis which oftentimes results in skipping the code or applying lightweight static analysis techniques over commit history. To address this scarcity, we developed SQUAAD, a comprehensive framework including a cloud-based automated infrastructure accompanied by a data analytics and visualization toolset. SQUAAD has been documented in multiple recent research publications empowering their empirical studies and is used by a major governmental entity.", "num_citations": "6\n", "authors": ["5"]}
{"title": "Is It a New Feature or Simply \u0393\u00c7\u00a3Don't Know Yet\u0393\u00c7\u00a5?: On Automated Redundant OSS Feature Requests Identification\n", "abstract": " Open source projects rely on issue tracking systems such as JIRA or online forums to keep track of users' feedback, expectations and requested features. However, since users are not fully aware of existing features, when submitting new feature requests, redundant requests often appear in the new feature list. It is a waste of time and effort for project contributors to manually identify and reject them, especially in complex systems with many features. Our research is aiming to find a suitable solution to identify redundant feature requests in OSS projects. We have conducted a survey on a well-known Open Source community, Hibernate and gathered all of its feature requests up-to-date. Through studying and categorizing the characteristics of these feature requests, we have found that about 37% of the feature requests were rejected and the most common rejection reason was redundancy. Also we have found that it is\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["5"]}
{"title": "Incremental development productivity decline\n", "abstract": " Incremental models are now being used by many organizations in order to reduce development risks while trying to deliver the product on time. It has become the most common method of software development with characteristics that influences the productivity of projects.", "num_citations": "6\n", "authors": ["5"]}
{"title": "Software Project Management\n", "abstract": " Building software was likened to manufacturing a car. Build individual components, assemble them in some defined sequence and you have your product ready to test and roll out. Of course it was mastered over sustained period of innovation and hard work. Although at a high level, software development follows a similar path, I believe that\u0393\u00c7\u00d6s where the similarity ends.Software is still relatively new as compared to auto industry. We are still abstracting and standardizing models and programming techniques to produce consistent and reliable software. In addition, changing landscape around improved computing power constantly redefines our expectations around performance and features of the software. It also begs of the professionals to be up to speed with their skill sets at all times. Product development cycles used to be long. Although dictated mostly by the scope, relatively medium to large scale projects taking 2\u0393\u00c7\u00f44 years was in the acceptable time range. This has significantly shrunk over last few years where product cycles are expected to be defined in weeks and months. It has now evolved into a more iterative cycle delivering incremental benefits. Part of the factor driving these short timelines is the need for business to react in near real time to changing business and market requirements. Enhanced tools available at an organizations disposal to facilitate such a schedule are also gaining favorable opinion with the stakeholders. With all the moving parts above one still needs to do the core work of utilizing and applying resources available in a manner that produces the desired output. Not realizing the potential of these resources can set back\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["5"]}
{"title": "Modern Tools to Support DoD Software Intensive System of Systems Cost Estimation\n", "abstract": " Many Department of Defense (DoD) organizations are attempting to provide new system capabilities through the net-centric integration of existing software-intensive systems into a new system often referred to as Software-Intensive System of Systems (SISOS). The goal of this approach is to build on existing capabilities to produce new capabilities not provided by the existing systems in a timely and cost-effective manner. Many of these new SISOS efforts such as the Future Combat Systems are of a size and complexity unlike their predecessor systems and cost estimation tools such as the Constructive Cost Model (COCOMO) suite are undergoing significant enhancements to address these challenges. This report describes the unique challenges of SISOS cost estimation, how current tools are changing to support these challenges, as well as on-going efforts to further support SISOS cost estimation needs. This report\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["5"]}
{"title": "2.6. 1 The Schedule as Independent Variable (SAIV) Process for Acquisition of Software\u0393\u00c7\u00c9Intensive Systems\n", "abstract": " Many system acquisitions do not achieve on\u0393\u00c7\u00c9time delivery because of delays in software development. This paper presents a highly successful approach for on\u0393\u00c7\u00c9time delivery of software\u0393\u00c7\u00c9intensive systems: the Schedule As Independent Variable (SAIV) process. The SAIV process involves prioritization of desired features; scoping a Core Capability of top\u0393\u00c7\u00c9priority features easily achievable within the available schedule; architecting for ease of dropping or adding borderline\u0393\u00c7\u00c9priority features; monitoring progress with respect to plans; and adding or dropping borderline\u0393\u00c7\u00c9priority features to meet the schedule target. The paper summarizes experiences and discusses critical success factors in applying the SAIV acquisition process across a range from small in\u0393\u00c7\u00c9house e\u0393\u00c7\u00c9services projects to very large Government systems of systems.", "num_citations": "6\n", "authors": ["5"]}
{"title": "Defect and fault seeding in dependability benchmarking\n", "abstract": " Defect and fault seeding is often considered for gathering empirical estimates within reliability models. Traditional defect seeding is fraught with difficult to resolve validity concerns when attempting to estimate true defect and fault populations. With dependability benchmarking we are less concerned about true defect and fault estimates, rather we wish to compare the relative effectiveness of dependability approaches. We propose that in this context the traditional concerns regarding defect and fault seeding techniques may not be as difficult to address and that potential new approaches may be useful as a means of benchmarking approaches to dependability.", "num_citations": "6\n", "authors": ["5"]}
{"title": "Cost vs. Quality Requirements: Conflict Analysis and Negotiation Aids\n", "abstract": " S-COST operates in the context of the University of Southern California Center for Software Engineering WinWin system, a groupware support system for determining software and system requirements as negotiated win conditions; quality attribute and risk conflict consultant (QARCC), a support system for identifying quality conflicts in software requirements; and the COnstructive COst estimation MOdel (COCOMO). Initial analyses of its capabilities indicate that its semiautomated approach provides users with improved capabilities for addressing cost-quality requirements issues.", "num_citations": "6\n", "authors": ["5"]}
{"title": "Critical success factors for knowledge-based software engineering applications\n", "abstract": " A key problem in KBSE is evaluating the prospective utility of proposed KBSA tools. This paper analyzes the utility of ten KBSA prototype applications that were developed as part of an advanced course. Based on such an analysis, we hypothesize three new critical success factors for explaining their utility. The factors identify key elements of KBSA tools that serve as indicators of their likely utility.", "num_citations": "6\n", "authors": ["5"]}
{"title": "Software and It's Impact: A Quantitative Assessment\n", "abstract": " \u0393\u00c7\u00a3You software guys are too much like the weavers in the story about the Emperor and his new clothes. When I go out to check on a software development the answers I get sound like,\u0393\u00c7\u00ffWe\u0393\u00c7\u00d6re fantastically busy weaving this magic cloth. Just wait a while and it\u0393\u00c7\u00d6ll look terrific.\u0393\u00c7\u00d6But there\u0393\u00c7\u00d6s nothing I can see or touch, no numbers I can relate to, no way to pick up signals that things aren\u0393\u00c7\u00d6t really all that great. And there are too many people I know who have come out at the end wearing a bunch of expensive rags or nothing at all.\u0393\u00c7\u00a5", "num_citations": "6\n", "authors": ["5"]}
{"title": "Computer systems analysis methodology-Studies in measuring, evaluating, and simulating computer systems\n", "abstract": " Computer systems analysis and simulation studies on design, development, and management of complex computer systems", "num_citations": "6\n", "authors": ["5"]}
{"title": "Same app, different countries: A preliminary user reviews study on most downloaded ios apps\n", "abstract": " Prior work on mobile app reviews has demonstrated that user reviews contain a wealth of information and are seen as a potential source of requirements. However, most of the studies done in this area mainly focused on mining and analyzing user reviews from the US App Store, leaving reviews of users from other countries unexplored. In this paper, we seek to understand if the perception of the same apps between users from other countries and that from the US differs through analyzing user reviews. We retrieve 300,643 user reviews of the 15 most downloaded iOS apps of 2018, published directly by Apple, from nine English-speaking countries over the course of 5 months. We manually classify 3,358 reviews into several software quality and improvement factors. We leverage a random forest based algorithm to identify factors that can be used to differentiate reviews between the US and other countries. Our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["5"]}
{"title": "Cosmic function points evaluation for software maintenance\n", "abstract": " The Common Software Measurement International Consortium (COSMIC) group reviewed the existing functional size methods, such as the International Function Points User Group (IFPUG)'s Function Points (FPs), to develop a functional size metric based on\" the basic principles\" that applies to a wide range of application domains. Though several empirical studies on the COSMIC method verify that COSMIC Function Points (CFPs) successfully accomplished the goal of being applicable to a wide range of application domains and that its size correlate well with effort over a very wide range of sizes, one study of telecom switching software noticed that the correlation between CFPs and cost is very low for small projects (5 CFPs or less). The COSMIC method does not explicitly size data manipulations (such as, mathematical algorithms), which causes it to be less effective for mathematically-intensive software. IFPUG's\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["5"]}
{"title": "Conflicts and synergies among quality requirements\n", "abstract": " Analyses of the interactions among quality requirements (QRs) have often found that optimizing on one QR will cause serious problems with other QRs. As just one relevant example, one large project had an Integrated Product Team optimize the system for Security. In doing so, it reduced its vulnerability profile by having a single-agent key distribution system and a single copy of the data base - only to have the Reliability engineers point on that these were system-critical single points of failure. The project's Security-optimized architecture also created conflicts with the system's Performance, Usability, and Modifiability. Of course, optimizing the system for Security had synergies with Reliability in having high levels of Confidentiality, Integrity, and Availability. This panel aims at fostering discussion on these relationships among QRs and how the use of data repositories may help discovering them.", "num_citations": "5\n", "authors": ["5"]}
{"title": "Using Software Non-Functional Assessment Process to Complement Function Points for Software Maintenance\n", "abstract": " Context: Most widely used cost models use source lines of code (SLOC) as the software size input measure, due to its quantifiability and high correlation with effort. Estimating the SLOC of a project is very difficult in early stages of the software lifecycle, especially for software maintenance tasks. Depending on the reuse model being used, one would need to size the existing code that needs modifications and the size of the changes being made in SLOC. Functional size measures, such as Function Points (FPs) and the Software Non-functional Assessment Process (SNAP), have been developed to improve the ability to estimate project size early in the lifecycle for both development and maintenance projects. While FPs represent software size by functions; SNAP complements FPs by sizing non-functional requirements, such as data operations and interface design. Goal: SNAP complements Function Points by sizing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["5"]}
{"title": "Lehman's laws and the productivity of increments: Implications for productivity\n", "abstract": " Which are the consequences of Lehman's Laws of Software Evolution for the productivity of incrementally developed projects? The concept of Incremental Development Productivity Decline (IDPD), which deals with how the productivity of incrementally developed software develops over its increments, is introduced. It is explained how Lehman's Laws of Software Evolution apply to it and how maintenance and reuse are relevant to both. Every Law of Software Evolution is discussed individually from a qualitative standpoint with regard to whether it could be a cause of IDPD. After that discussion, the overall situation is examined in light of how different courses of action cause which laws to apply different degrees of effects.", "num_citations": "5\n", "authors": ["5"]}
{"title": "Technical debt: past, present, and future (panel)\n", "abstract": " The term \u0393\u00c7\u00a3Technical Debt\u0393\u00c7\u00a5 was coined over 20 years ago by Ward Cunningham in a 1992 OOPSL A experience report to de scribe the tr ade-offs between delivering the most appropriate \u0393\u00c7\u00f6 albeit likely immature \u0393\u00c7\u00f6 product, in the shortest time possible. Since then the repercussions of going into \u0393\u00c7\u00a3technical debt\u0393\u00c7\u00a5 have become more visible, yet not necessarily more broadly understood. This panel will bring together practitioners to discuss and debate strategies for debt relief.", "num_citations": "5\n", "authors": ["5"]}
{"title": "COTIPMO: A constructive team improvement process model\n", "abstract": " Team synchronization and stabilization are essential - especially for large software projects. However, often little is done to assess and reduce the uncertainties and knowledge gaps that exist within the project. As the project progresses through its life cycle, the team can gain more information about the project and team's capabilities. These necessary data can be obtained through performing assessments on the team and project. As these assessments procedures are often complex, discouraging, and difficult to analyze, an effective framework and tool support can greatly enhance the process. Hence, with improved assessment methods, software project teams can quickly gather the necessary data, determine the actions to improve performance, and result in an improved project outcome in the end. The COnstructive Team Improvement Process MOdel (COTIPMO) is a framework developed to effectively improve\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["5"]}
{"title": "Towards richer process principles\n", "abstract": " Today's and tomorrow's complex, interdependent, dynamic systems require richer process principles than the simplistic principles in the Agile Manifesto or in simplistic sequential waterfall or Vee models. The resulting principles should capitalize on the strengths of these while avoiding their weaknesses.", "num_citations": "5\n", "authors": ["5"]}
{"title": "An investigation on application domains for software effort distribution patterns\n", "abstract": " Many software cost estimation models suggest a one-size-fit-all effort distribution patterns as the basis of their guidelines for resource allocation on software development activities. This set of distribution percentages is generally developed from experiences with limited data support. In this paper, we will examine a set of real world data points and share our findings that uncover the significant variations of effort distribution patterns when categorized by application domains. Furthermore, we will discuss the possibility of extracting active factors from application domains that may help us enhance the overall effort distribution patterns as in COCOMO II model.", "num_citations": "5\n", "authors": ["5"]}
{"title": "Evaluating the software design of a complex system of systems\n", "abstract": " Schedule-or event-driven reviews are a crucial element of any major software development project. Such reviews tend to focus on different aspects of development, and different types of reviews provide different benefits. The sum of these reviews, however, is inadequate to address the needs of software development in a complex system of systems SoS environment. What is needed is a true, evidence-driven, SoS-level evaluation capable of providing an overall assessment of, and insight into, the software development effort in that context. This report discusses the application of the Lifecycle Architecture LCA event to what was an enormously complex SoS program the Armys Future Combat Systems. From the FCS experience, readers will gain insight into the issues of applying the LCA in an SoS context and be ready to apply the lessons learned in their own domains.Descriptors:", "num_citations": "5\n", "authors": ["5"]}
{"title": "Software cost metrics manual\n", "abstract": " 2. No common SLOC reporting\u0393\u00c7\u00f4logical, physical, etc. 3. No standard definitions\u0393\u00c7\u00f4Application Domain, Build, Increment, Spiral,\u0393\u00c7\u00aa 4. No common effort reporting\u0393\u00c7\u00f4analysis, design, code, test, CM, QA,\u0393\u00c7\u00aa 5. Product size only reported in lines of code 6. No reporting of quality measures\u0393\u00c7\u00f4defect density, defect containment, etc.", "num_citations": "5\n", "authors": ["5"]}
{"title": "Better management of development risks: early feasibility evidence\n", "abstract": " The Incremental Commitment Model (ICM) organizes more rapid and thorough concurrent systems engineering and acquisition processes in ways that provide points at which they can synchronize and stabilize, and at which their risks of going forward can be better assessed and fitted into a risk-driven stakeholder resource commitment process. In particular, its concurrent activities of Understanding Needs, Envisioning Opportunities, System Scoping and Architecting, Feasibility Evidence Development, and Risk/Opportunity Assessment enable projects to focus specifically on their system constraints and environments and on opportunities to deal with them. This paper describes in detail the content, preparation, and role of feasibility evidence at key decision points in the system development cycle and how this can be used to effectively identify and manage risks throughout the development cycle. The feasibility evidence is not intended to assess just a single sequentially developed system definition element, but rather to assess the consistency, compatibility, and feasibility of several concurrently-engineered elements. To make this concurrency work, a set of anchor point milestone reviews are performed to ensure that the many concurrent activities are synchronized, stabilized, and risk-assessed at the end of each phase using developer-produced, expert-validated feasibility evidence.", "num_citations": "5\n", "authors": ["5"]}
{"title": "8.3. 1 The Macro Risk Model: An Early Warning Tool for Software\u0393\u00c7\u00c9Intensive Systems Projects\n", "abstract": " The Macro Risk Model is a tool for early detection and quantification of project risks associated with Software\u0393\u00c7\u00c9Intensive Systems (SIS). It has been calibrated and validated against detailed case studies. Structured questions guide a user in rating evidence and risk impacts with respect to critical project success factors. It provides color\u0393\u00c7\u00c9coded risk exposures, and allows for rationale and supporting artifacts. This paper describes the model and the basis of its risk evaluation methods, it reviews case studies of NASA exploration mission failures with respect to the model, it discusses a Delphi consensus\u0393\u00c7\u00c9seeking process that was used to analyze factors affecting model accuracy and to calibrate the model, and compares the model to other risk approaches.", "num_citations": "5\n", "authors": ["5"]}
{"title": "A Process Decision Table for Integrated Systems and Software Engineering\n", "abstract": " The Incremental Commitment Model (ICM), developed in a recent National Research Council study on integrating human factors into the systems development process, organizes systems engineering and acquisition processes in ways that better accommodate the different strengths and difficulties of hardware, software, and human factors engineering approaches. As with other models trying to address a wide variety of situations, its general form is rather complex. However, its risk-driven nature has enabled us to determine a set of ten common risk patterns and organize them into a decision table that can help new projects converge on a process that fits well with their particular process drivers. For each of the ten special cases, the decision table provides top-level guidelines for tailoring the key activities of the ICM, along with suggested lengths between each internal system build and each external system increment delivery. This paper elaborates on each of the ten cases and provides examples of their use.", "num_citations": "5\n", "authors": ["5"]}
{"title": "Retrospectives on Peopleware\n", "abstract": " Since its publication twenty years ago, \"Peopleware Productive Projects and Teams\" (Dorset House, 1987), by Tom DeMarco and Tim Lister, has enlightened software professionals and non-professionals alike. Peopleware introduced among other topics - \"team gel\", design patterns, the \"Furniture Police\" - to the software engineering community and suggested that \"sociology matters more than technology or even money.\" This unique session with the pioneers of our profession is an opportunity to learn, reflect, and share experiences -- looking forward to the future. This compendium consists of brief bios and first person retrospectives on \"Peopleware\".", "num_citations": "5\n", "authors": ["5"]}
{"title": "Comparative experiences with software process modeling tools for the incremental commitment model\n", "abstract": " The Incremental Commitment Model (ICM) is a new generation process model that focuses on the incremental growth of success critical stakeholder satisfaction, system definition and stakeholder commitment. ICM has been introduced in system engineering, but not software engineering. In the Fall 2008, ICM will be used as a process model to develop software system in the USC Software Engineering graduate course. Hence, two significantly different software process modeling tools are selected to create the electronic process guidelines for this course. This paper reports our comparative experiences between an adaptability tolerance framework, Eclipse Process Framework Composer (EPFC) and a precision oriented process definition language, Little-JIL in order to create ICM electronic guide. In addition, the paper provides a tool comparison analysis by using Humphrey and Kellner\u0393\u00c7\u00d6s criteria and a target group evaluative result. The evaluation identifies some research challenges and areas for future research work.", "num_citations": "5\n", "authors": ["5"]}
{"title": "Synthesis of Existing Cost Models to Meet System of Systems Needs\n", "abstract": " Today\u0393\u00c7\u00d6s need for more complex, more capable systems in a short timeframe is leading more organizations towards the integration of existing systems into network-centric, knowledge-based system-of-systems (SoS). Software and system cost model tools to date have focused on the software and system development activities of a single software system, but none to date adequately estimate the integration of multiple systems into an SoS. This paper presents an overview of the activities that must be included in an SoS cost model and describes an approach for estimating SoS effort using the Constructive Cost Model (COCOMO) suite of estimation tools to estimate SoS Lead System Integrator (LSI) effort as well as the total SoS development effort.", "num_citations": "5\n", "authors": ["5"]}
{"title": "Value-based verification and validation guidelines\n", "abstract": " ObjectiveTo identify defects as closely as possible to the point of occurrence in order to facilitate corrective action.", "num_citations": "5\n", "authors": ["5"]}
{"title": "Empirical evaluation of techniques and methods used for achieving and assessing software high dependability\n", "abstract": " For achieving high dependability of software intensive systems, not only product dependability benchmarking is needed but also benchmarking of technologies and processes for achieving and assessing software dependability. Dependability engineering, and more specifically technology management and assessment of effectiveness and efficiency of different technology interventions, is the objective of the work we introduce here. This work is performed as part of the High Dependability Computing Project (HDCP) 1 that is an incremental, five-year, cooperative agreement, part of a broad strategy for dependable computing, that links NASA, corporate partners and universities and research centers such as Carnegie Mellon, University of Maryland, Fraunhofer Center Maryland, University of Southern California, Massachusetts Institute of Technology, University of Washington and University of Wisconsin. For now the focus is on NASA projects, but the results will be captured and organized in an experience base, so that they could be disseminated and applied to other organizations. For example, the first step would be to extend the results to organizations that are members of the High Dependability Computing Consortium (HCC) 2 and the Sustainable Computing Consortium (SCC) 3.As part of our activities we are looking at a series of steps to evaluate such interventions. Developing high dependability software requires specifying the dependability requirements, using development techniques and methods (that we will call \u0393\u00c7\u00a3technologies\u0393\u00c7\u00a5) to build-in high-dependability as the product is developed, and also technologies to verify that the required\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["5"]}
{"title": "A Web Repository of Lessons Learned from COTS-Based Software Development1\n", "abstract": " A Web Repository of Lessons Learned from COTS-Based Software Development1 Page 1 Software Engineering Technology September 2002 www.stsc.hill.af.mil 25 The development of commercial offthe-shelf (COTS)-based software is different in many respects from in-house software development. Since COTS requires different activities and skills, we need to build a body of knowledge about COTSbased software development. Thus, the authors have built a Web-based repository of lessons learned, seeded with about 70 lessons extracted from literature, including journal articles [1], workshop presentations [2], and government reports [3, 4]. The authors also organized online eWorkshops [5] and are using these discussions to synthesize new lessons and refine existing ones2. They are also consolidating the repository with an unpublished set of lessons learned from the Software Engineering Institute. The lessons a \u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["5"]}
{"title": "Next Generation Software Processes and Their Environment Support.\n", "abstract": " The USC Center for Software Engineering has developed a negotiation-based approach to software system requirements engineering, architecting, development, and management. It is based on three primary foundations 1 Theory W, a management theory and approach. It is based on making winners of all of the systems key stakeholders as a necessary and sufficient condition for project success. 2 The WinWin Spiral Model, an extension to the Spiral Model of the software process. It is described further below. 3 The WinWin groupware tool for facilitating distributed stakeholders negotiation of mutually satisfactory WinWin system specifications. The original spiral model Boehm, 1988 uses a cyclic approach to develop increasingly detailed elaborations of a software systems definition, culminating in incremental releases of the systems operational capability. Each cycle involves four main activities 1 Elaborate the system or subsystems product and process objectives, constraints, and alternatives. 2 Evaluate the alternatives with respect to the objectives and constraints. Identify and resolve major sources of product and process risk. 3 Elaborate the definition of the product and process. 4 Plan the next cycle, and update the life cycle plan, including partition of the system into subsystems to be addressed in parallel cycles. This can include a plan to terminate the project if it is too risky or infeasible. Secure the managements commitment to proceed as planned.Descriptors:", "num_citations": "5\n", "authors": ["5"]}
{"title": "Detecting Architectural Mismatches During Systems Composition---An Extension to the AAA Model\n", "abstract": " The USC Architect\u0393\u00c7\u00d6s Automated Assistant (AAA) tool and method provides a capability for early detection of architectural style mismatches among four architectural styles: Main-Subroutine, Pipe-and-Filter, Event-Based, and Distributed Processes.The work proposed here is to formalize some additional architectural styles---namely Blackboard, Closed-Loop Feedback Control, Logic Programming, Real-Time, Rule-Based, and Transactional Database styles---and to extend the mismatch analysis capability to cover interactions of the original four styles with the new ones. The application of the mismatch analysis capability to a relevant problem will also be included in the future.", "num_citations": "5\n", "authors": ["5"]}
{"title": "R&D reports\n", "abstract": " access (CSMA) protocols may not be suitable for hard realtime communication, the authors argue, because the message transmission delay cannot be bounded. Virtual-time CSMA protocols can overcome the problem, they say. In virtual-time CSMA protocols, each node maintains two clocks: one realtime and one virtual. Whenever the channel finds a node to be idle, it resets its virtual clock, which runs at a higher rate than the realtime clock. A node transmits a waiting message when the time on the virtual clock is equal to some parameter of the message. Simulation studies performed by the authors show that the virtual-time CSMA protocols perform better than protocols that use the collision resolution scheme only.", "num_citations": "5\n", "authors": ["5"]}
{"title": "An experimental project course in software engineering\n", "abstract": " In another paper in this volume, we discussed some of the major industry needs for software engineering expertise which were not being provided within the traditional computer science curriculum. These included a familiarity with software requirements engineering and preliminary design, with data processing economics, with applications software development, with project management techniques, and with working in groups.", "num_citations": "5\n", "authors": ["5"]}
{"title": "Software Engineering Education: Some Industry Needs\n", "abstract": " TRW Systems and Energy, Inc. employs more than a thousand software people. Our software work is done generally for the U.S. Government. It ranges from one-shot, one-month simulation jobs to very large projects involving 300-person teams for several years. We generally hire about 25\u0393\u00c7\u00f450 software people per year from Universities. We find, in general, that they are very well prepared by Universities to perform some software engineering functions, primarily in the area of systems programming. However, they aren\u0393\u00c7\u00d6t very well educated to perform several software engineering functions which are particularly important in our work. Here is a list of the most important ones.", "num_citations": "5\n", "authors": ["5"]}
{"title": "System design\n", "abstract": " The next few chapters discuss the technological and economic implications of providing the range of services described in the previous chapters. In the main, they conclude that the service goals are tech-nically achievable, but still fairly formidable economically, although there are major uncertainties due to a lack of solid data on the likely demand for the various services.This chapter attempts, through a discussion of the system design aspects of a PCIU, to bridge the three major sections of the book. It begins by summarizing the system requirements implied in the \u0393\u00c7\u00a3Ser-vices\u0393\u00c7\u00a5 section within the context of the system design task. Next, it outlines the major elements which are considered in the\" System Design\u0393\u00c7\u00a5 section: people, consoles, communications, computers, software and data bases, and economics. Next, it presents a phased system de-", "num_citations": "5\n", "authors": ["5"]}
{"title": "Graphical aids to aerospace vehicle mission analysis\n", "abstract": " The paper points out some of the architectural considerations involved in designing such a system: hardware, program organization, user language, error detection and correction,. and file management aspects. It also discusses some current applications of the system, and indicates directions of future expansion; eg, incorporating on-line design of aerodynamic shapes and solid propellant grains.", "num_citations": "5\n", "authors": ["5"]}
{"title": "Existence, characterization, and convergence of best rational Tchebycheff approximations\n", "abstract": " Some new results in the theory of rational Tchebycheff approximation are presented, and should be of interest to mathematicians and computer scientists. AuthorDescriptors:", "num_citations": "5\n", "authors": ["5"]}
{"title": "Assessing software understandability in systems by leveraging fuzzy method and linguistic analysis\n", "abstract": " Many if not most software-intensive systems have their mission-critical software modified by people other than its developers. The resulting misunderstandings often seriously compromise the missions the system is supporting (over 80% of the functionality in most current ground, sea, air, and space vehicles depends on software). There is a major need for models, methods, processes, and tools for identifying sources and effects of software misunderstandings, both in preparation for the system\u0393\u00c7\u00d6s software use and evolution, and in evaluating modified software to avoid further examples of misunderstanding. Emphasizing high software understandability enables system maintainers to avoid these misunderstandings as they modify existing software systems. However, while many metrics for understandability have been developed, with the majority of them being source code based, little to no correlation has been found\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["5"]}
{"title": "Detailed use case points (ducps): a size metric automatically countable from sequence and class diagrams\n", "abstract": " Sequence and class diagrams are widely used to model the behavioral and structural aspects of a software system. A size metric that is defined automatically countable from sequence and class diagrams boosts both the efficiency and the accuracy of size estimation by producing reproducible software size measurements. To fulfill the purposes, a size metric called Detailed Use Case Points (DUCPs) is proposed based on the information automatically derived from sequence and class diagrams. The automation is largely supported by our proposed user-system interaction model (USIM) that fills the gap between the system abstraction by the sizing model and the metamodels of the UML diagrams. The effectiveness of our proposed size metric in project effort estimation is validated by an empirical study of 22 historical projects.", "num_citations": "4\n", "authors": ["5"]}
{"title": "Improving and balancing software qualities\n", "abstract": " This Technical Briefing describes the nature of Software Qualities (SQs), ilities, or non-functional requirements (reliability, usability, affordability, etc.), and discusses the importance of understanding their nature and interrelationships, and of bringing them into balance in the practice of software engineering. The relevance and timeliness of this topic reflects the current and future trends toward more software-intensive systems, with greater complexity, autonomy, speed of change, and need for interoperability within systems of systems, given the frequent system shortfalls and overruns that occur when their SQ balance is not achieved. It discusses the weaknesses of current SQ standards and guidance, and summarizes research toward strengthening current SQ definitions and relationships. This includes a set of initial SQ ontology elements and relationships, examples of their application to some key SQs, an\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["5"]}
{"title": "General Theories of Software Engineering (GTSE): Key Criteria and an Example\n", "abstract": " An essential consideration for a candidate General Theory of Software Engineering (GTSE) is the criteria by which it should be evaluated. This summary will include a proposed set of criteria and their rationale; an overview of an existing GTSE (Theory W: stakeholder win-win) that will be presented in the keynote address; and a summary of its ability to satisfy the criteria.", "num_citations": "4\n", "authors": ["5"]}
{"title": "High Maturity is not a procrustean bed\n", "abstract": " In Greek mythology, Procrustes was a rogue smith and bandit who invited travellers to rest in his perfectly sized bed. When they accepted, he forcibly bound them to it, then stretched them or cut off various body parts until they perfectly fit the bed. Too many organizations have a single model of high maturity to which they try to fit all their projects. Development and acquisition organizations are finding that competitive success requires systems that are a mix of high security assurance components opaque and dynamic COTS products and cloud services, and highly useful but kaleidoscopic apps and widgets. Approaching such systems with a one-size-fits all corporate process and maturity model often results in a procrustean fit. As a process model generator, the Incremental Commitment Spiral Model has a set of criteria for determining which process or processes best fit a particular system of interest. This article summarizes the criteria and illustrates how they have been successfully applied in various situations.Descriptors:", "num_citations": "4\n", "authors": ["5"]}
{"title": "Improved Method for Predicting Software Effort and Schedule\n", "abstract": " This paper presents a set of effort and schedule estimating relationships for predicting software development projects using empirical data from 317 very recent US DoD programs. The first set predicts effort as a function of product size and application type. The second predicts the duration of software development as a function of product size and staffing levels. The estimating relationships are simpler and more viable to use for early estimates than traditional parametric cost models. Practical productivity benchmarks are also provided to guide cost analysts in normalizing and validating data. These methods are applicable to all industry sectors.", "num_citations": "4\n", "authors": ["5"]}
{"title": "Principles for successful systems and software processes\n", "abstract": " This paper summarizes several iterations in developing a compact set of four key principles for successful systems engineering, which are 1) Stakeholder Value-Based Guidance 2) Incremental Commitment and Accountability 3) Concurrent Multidiscipline Engineering, and 4) Evidence-and Risk-based Decisions. It provides a rationale for the principles, including short example case studies of failed projects that did not apply the principles, and of successful projects that did. It will compare the principles with other sets of principles such as the Lean Systems Engineering and the Hitchins set of principles for successful systems and systems engineering, and indicate how the principles will help projects and organizations cope with increasing needs for process diversity and change.", "num_citations": "4\n", "authors": ["5"]}
{"title": "Software domains in incremental development productivity decline\n", "abstract": " This research paper expands on a previously introduced phenomenon called Incremental Development Productivity Decline (IDPD) that is presumed to be present in all incremental software projects to some extent. Incremental models are now being used by many organizations in order to reduce development risks. Incremental development has become the most common method of software development. Therefore its characteristics inevitably influence the productivity of projects. Based on their observed IDPD, incrementally developed projects are split into several major IDPD categories. Different ways of measuring productivity are presented and evaluated in order to come to a definition or set of definitions that is suitable to these categories of projects. Data has been collected and analyzed, indicating the degree of IDPD associated with each category. Several hypotheses have undergone preliminary evaluations\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["5"]}
{"title": "A value-based review process for prioritizing artifacts\n", "abstract": " As a new contribution to Value-based V&V process development, a systematic and multi-criteria process is proposed to quantitatively determine the Value-based V&V artifact priority that reviewers can follow for their reviews. This process enables reviewers to prioritize artifacts to be reviewed in a more cost-effective way based on more sophisticated and comprehensive factors, such as importance, quality risks, dependency and cost of V&V investments. Some qualitative and quantitative evidence is provided from a comparative experiment with 22 real-client e-services projects over two years of a graduate software engineering team-project course. It shows that the value-based artifact prioritization enabled reviewers to better focus on artifacts with high importance and risks, to capture issues with high impact in a timely manner, and to improve the cost-effectiveness of reviews.", "num_citations": "4\n", "authors": ["5"]}
{"title": "Extending software engineering research outside the digital box\n", "abstract": " Since software is developed to run on computers, there is a tendency to focus computer science and software engineering on how best to get software to run on computers. But, engineering is different from science: the Webster definition of\" engineering\" is\" the application of science and mathematics by which the properties of matter and the sources of energy in nature are made useful to people.\" Thus, it would follow that the responsibility of software engineering and its research would include the utility to people of the software and the software-reliant artifacts they use, beyond thinking within purely digital boxes. This position paper addresses two perspectives on the future of software engineering when viewed in this broader context.", "num_citations": "4\n", "authors": ["5"]}
{"title": "Software cost estimation in the incremental commitment model\n", "abstract": " Complex, software intensive systems \u0393\u00c7\u00f6 especially those with multiple software component developers \u0393\u00c7\u00f6 and Directed System of Systems (DSOS) or Acknowledged Systems of Systems (ASOS) need approaches to control the development and estimate the software development costs and schedules. This paper will introduce a next-generation synthesis of the spiral model and other leading process models into the Incremental Commitment Model (ICM). The ICM emphasizes architecting systems (or DSOSs) to encapsulate subsystems (or systems) undergoing the most rapid change, and having agile systems engineers handle longer-range change traffic to rebaseline the plans for future increments. Systems engineers do this, while largely plan-driven teams develop and continuously verify and validate (V&V) the current increment, as is usually required for safe or secure software.         Our approach for estimating\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["5"]}
{"title": "Future challenges for software data collection and analysis\n", "abstract": " \u0393\u00c7\u00f3 Not a measure of software size\u0393\u00c7\u00f3 Not a measure of software effort\u0393\u00c7\u00f3 Not a measure of delivered software capability\u0393\u00c7\u00f3 A quantity derived from software component sizes and reuse factors that helps estimate effort\u0393\u00c7\u00f3 Once a product or increment is developed, its ESLOC loses its identity", "num_citations": "4\n", "authors": ["5"]}
{"title": "An Empirical Study of Requirements Elaboration\n", "abstract": " This paper describes an empirical study undertaken to investigate the quantitative aspects of the phenomenon of requirements elaboration which deals with the transformation of high-level goals into low-level requirements. Prior knowledge of the magnitude of requirements elaboration is instrumental in developing early estimates of a project\u0393\u00c7\u00d6s cost and schedule. This study examines the data on capability goals and capability requirements of 20 realclient, MS-student, team projects done at USC. Metrics for data collection and analysis are described along with the utility of the results they produce. These results suggest some relationship between the nature of projects and the size of requirements elaboration.", "num_citations": "4\n", "authors": ["5"]}
{"title": "Using the incremental commitment model to achieve successful system development\n", "abstract": " Studies to evaluate the usage and success of the spiral development model have shown mixed results\u0393\u00c7\u00f6many successes but many misinterpretations and neglect of its underlying principles, leading to continuing development problems and failed systems. This article describes a recent improvement to the spiral development process model that is easier to understand, is harder to misinterpret, and better enables integration of the human, hardware, and software aspects of a softwareintensive system\u0393\u00c7\u00d6s development and evolution. This model, called the Incremental Commitment Model (ICM), embodies the principles underlying the spiral model, but organizes its process into multiple views (including a spiral view) that are more straightforward to apply and better aligned with mainstream system acquisition phases and milestones.", "num_citations": "4\n", "authors": ["5"]}
{"title": "Developing groupware for requirements negotiation: Lessons learned\n", "abstract": " Defining requirements is a complex and difficult process, and defects in the process often lead to costly project failures [1]. There is no complete and well-defined set of requirements waiting to be discovered in system development. Different stakeholders\u0393\u00c7\u00f6users, customers, managers, domain experts, and developers\u0393\u00c7\u00f6come to the project with diverse expectations and interests. Requirements emerge in a highly collaborative, interactive, and interdisciplinary negotiation process that involves heterogeneous stakeholders.At the University of Southern California\u0393\u00c7\u00d6s Center for Software Engineering, we have developed a series of groupware implementations for the WinWin requirements negotiation approach (see the Acknowledgments at the end of the article for a list of organizations that helped sponsor this research). The WinWin approach involves having a system\u0393\u00c7\u00d6s success-critical stakeholders participate in a negotiation process so they can converge on a mutually satisfactory or win\u0393\u00c7\u00f4win set of requirements. Groupware-supported methodologies are among the hardest to get right, and the rapidly moving technology of distributed interactive systems is a major challenge. This is due largely to the relative newness of the area and to the unfamiliarity of most software developers with the phenomena of group dynamics. However, an even bigger challenge is creating a system that works well with people of different backgrounds, in different places, and often at different times. In particular, collaborative technology that supports requirements negotiation must address stakeholder heterogeneity.", "num_citations": "4\n", "authors": ["5"]}
{"title": "Value-Based Design of Software V&V Processes for NASA Flight Projects\n", "abstract": " Software risk advisory tools have been developed to support Verification and Validation (V&V) processes for NASA flight projects on the Constellation program. The Orthogonal Defect Classification COnstructive QUALity MOdel (ODC COQUALMO) predicts software defects introduced and removed classifying them with ODC defect types, allowing various tradeoff analyses. We have been exploring methods to design and optimize V&V processes with static and dynamic versions of the quality model by integrating it with different risk minimization techniques. These techniques allow \u0393\u00c7\u00a3what-if\u0393\u00c7\u00a5 experimentation to determine the impact of V&V techniques on specific risks and overall flight risk. V&V techniques are quantified from a value-based perspective when the defect model is integrated with machine learning, strategic optimization and JPL\u0393\u00c7\u00d6s Defect Detection and Prevention (DDP) risk management method. Results to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["5"]}
{"title": "Unifying the Software Process Spectrum: International Software Process Workshop, SPW 2005, Beijing, China, May 25-27, 2005 Revised Selected Papers\n", "abstract": " This volume contains papers presented at SPW 2005, the Software Process Workshop held in Beijing, PR China, on May 25-27, 2005, and prepared for final publication. The theme of SPW2005 was \u0393\u00c7\u00a3Unifying the Software Process Spectrum.\u0393\u00c7\u00a5 Software process encompasses all the activities that aim at developing or evolving software products. The expanding role of software and information systems in the world has focused increasing attention on the need for assurances that software systems can be developed at acceptable speed and cost, on a predictable schedule, and in such a way that resulting systems are of acceptably high quality and can be evolved surely and rapidly as usage contexts change. This sharpened focus is creating new challenges and opportunities for software process technology. The increasing pace of software s-tem change requires more lightweight and adaptive processes, while the increasing mission criticality of software systems requires more process predictability and c-trol as well as more explicit attention to business or mission values. Emergent app-cation requirements create a need for ambiguity tolerance. Systems of systems and global development create needs for scalability and multi-collaborator, multi-culture concurrent coordination. COTS products provide powerful capabilities, but their v-dor-determined evolution places significant constraints on software definition, dev-opment, and evolution processes. The recognition of these needs has spawned a considerable amount of software process research across a broad spectrum.", "num_citations": "4\n", "authors": ["5"]}
{"title": "Evolving an experience base for software process research\n", "abstract": " Since 1996 the USC Center for Software Engineering has been accumulating a large amount of software process experience through many real-client project software engineering practices. Through the application of the Experience Factory approach, we have collected and evolved this experience into an experience base (eBASE) which has been leveraged successfully for empirically based software process research. Through eBASE we have realized tangible benefits in automating, organizational learning, and strategic advantages for software engineering research. We share our rationale for creating and evolving eBASE, give examples of how the eBASE has been used in recent process research, discuss current limitations and challenges with eBASE, and what we hope to do achieve in the future with it.", "num_citations": "4\n", "authors": ["5"]}
{"title": "Software process disruptors, opportunity areas, and strategies\n", "abstract": " The near future (5-10 years) of software processes will be largely driven by disruptive forces that require organizations to change their traditional ways of doing business. This report begins with a discussion of the major current and near-future disruptors in the software process area and how they interact. It then discusses major trends in terms of opportunity areas for dealing with various combinations of disruptors. Based on the opportunity areas, it then identifies some attractive future strategies that appear to have high payoff. Figure 1 provides an overview of the relations among the future disruptors, opportunity areas, and strategies.", "num_citations": "4\n", "authors": ["5"]}
{"title": "Second workshop on software quality\n", "abstract": " Software products are a critical and strategic asset in an organizations' business. They are becoming larger, more sophisticated and more complex. The challenge is to develop more complicated software products within the constraints of time and resources without the sacrifice of quality. Quality standards, methodologies and techniques have been continually proposed by researchers and used by software engineers in the industry. The Second Workshop on Software Quality aims to bring together academic, industrial and commercial communities interested in software quality topics to discuss the different technologies being defined and used in the software quality area.", "num_citations": "4\n", "authors": ["5"]}
{"title": "Value-based software metrics\n", "abstract": " Many software metrics, particularly dependability metrics, are usually presented in a value-neutral framework. For example, the primary metric for reliability, mean time between failures, is referenced to a one-size-fits-all (and often not explicitly defined) definition of \"failure\". This can lead to serious problems, as when operators' optimization on liveness produces user response times to queries of up to 2 weeks (an actual example). Clearly, unacceptable response time is a \"failure\" for some stakeholders that should be reflected in the metric definition. We present some initial thoughts and a candidate approach for addressing this issue.", "num_citations": "4\n", "authors": ["5"]}
{"title": "Determining software quality using COQUALMO\n", "abstract": " Cost, schedule, and quality comprise the three most important factors in software economics modeling. Unfortunately, these three factors are highly correlated, and consequently software project managers do not have a good parametric model they can trust to determine the effects of one on the other and hence make confident decisions during the software development process. We know that beyond the \u0393\u00c7\u00a3Quality is Free\u0393\u00c7\u00a5 point, it is difficult to increase the quality and reliability of a software product without increasing either the cost or schedule or both. Furthermore, we cannot compress the development schedule to reduce the time to market without having a negative impact on the operational reliability and quality of the software product and/or cost of development. However, software cost/schedule and quality/reliability estimation models can assist software project managers to make better decisions that impact the overall success of the software product. Recognizing the need for a model that aids managers in making difficult tradeoffs among cost/schedule/quality/reliability, we have developed COQUALMO, a quality model that extends the popular COCOMO II model (a software cost and schedule estimation model)(Boehm et al., 2000). COQUALMO helps managers determine the quality of the software product in terms of defect density that in turn is an early predictor of software reliability. We have organized the remainder of this chapter as follows:", "num_citations": "4\n", "authors": ["5"]}
{"title": "Tailoring a successful project-based course\n", "abstract": " The ultimate preparation for a career in software development is a project-based course in which students learn to work in teams on the development of useful software products for real clients. Published educational materials provide little support for the educator who wishes to teach such an ambitious course. We have embarked on a project whose goal is the development of materials which will enable educators to adapt Barry Boehm and Dan Port\u0393\u00c7\u00d6s USC CS577, one of the most successful such courses, to their special needs and resources, and to disseminate these materials in the form of a textbook, student and instructor manuals, a web-based course delivery framework, videotapes of USC course lectures, a CD archive of completed student projects, and public training in both adaptation and teaching of the course.Our starting point is the two-semester project-based Introduction to Software Engineering which\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["5"]}
{"title": "Point/Counterpoint\n", "abstract": " But a diet that is overbalanced toward theory is equally bad. Software engineering is fundamentally a contact sport. In software engineering as in rugby, no amount of theory about how to succeed in a Scrum is going to come close to real experience in the middle of a few Scrums. Furthermore, the theory that most current students get covers maybe 15 percent of the activities they encounter in practice. Most of it is based on a model of software engineering as a set-piece job of deriving and verifying code from a static set of requirements. This was a good model in the 1970s, but it\u0393\u00c7\u00d6s way out of date now.In the 1970s, software was a small part of most systems, and the requirements\u0393\u00c7\u00d6 stability meant that you could often separate concerns and just attack the problem of deriving code from software requirements in isolation. This has all changed now. From electronic services to", "num_citations": "4\n", "authors": ["5"]}
{"title": "Value-based feedback in software/it systems\n", "abstract": " Many software/IT feedback control systems (eg,\" earned value\" project management systems) focus on cost and technical progress, but not on business or mission value realization. This often leads to failures due to model clashes between the software/IT organization and the business or mission organization. This talk will discuss value-based feedback systems such as DMR's Benefits Realization Approach. It will show how these approaches avoid many of the model clashes, and how they can help explain some of the exceptions to\" laws of evolution\"(eg, Y2K spikes) in terms of business-value considerations.", "num_citations": "4\n", "authors": ["5"]}
{"title": "Modeling software defect introduction and removal\n", "abstract": " Cost, schedule and quality are highly correlated factors in software development. They basically form three sides of the same triangle. Beyond a certain point (the\" Quality is Free\" point), it is difficult to increase the quality without increasing either the cost or schedule or both for the software under development. Similarly, development schedule cannot be drastically compressed without hampering the quality of the software product and/or increasing the cost of development. Watts Humphrey, at the LA SPIN meeting in December'98, highlighted that\" Measuring Productivity without caring about Quality has no meaning\". Software estimation models can (and should) play an important role in facilitating the balance of cost/schedule and quality. Recognizing this important association, an attempt is being made to develop a quality model extension to COCOMO II; namely COQUALMO. An initial description of this model focusing on defect introduction was provided in [Chulani97a]. The model has ev...", "num_citations": "4\n", "authors": ["5"]}
{"title": "Calibration Approach and Results of the COCOMO II Post-Architecture Model\n", "abstract": " Calibration Approach and Results of the COCOMO II Post- Architecture Model Page 1 1 Calibration Approach and Results of the COCOMO II PostArchitecture Model Sunita Devnani-Chulani (sdevnani@sunset.usc.edu), Brad Clark, Barry Boehm, Bert Steece ISPA \u0393\u00c7\u00ff98 Copyright USC-CSE 1998 Page 2 Outline \u252c\u2593 Brief History of COCOMO x COCOMO II.1997 \u0393\u00c7\u00f4 Process \u0393\u00c7\u00f4 Prediction Accuracies \u0393\u00c7\u00f4 Comparison with COCOMO \u0393\u00c7\u00ff81 x Updates and Plans \u0393\u00c7\u00f4 Plans for Improving Prediction Accuracies \u0393\u00c7\u00f4 COCOMO II Research Aim \u0393\u00c7\u00f4 Effects of Process Maturity on Effort x Information Sources 2 Copyright USC-CSE 1998 Page 3 COnstructive COst MOdel (COCOMO) x COCOMO published since 1981 x Commercial implementations of COCOMO y CoCoPro, CB COCOMO, COCOMOID, COSTMODL, GECOMO Plus, SECOMO, etc. x Other models based on COCOMO y REVIC, Gulezian x COCOMO II y Research effort started in 1994 to \u0393\u00c7\u00d6\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["5"]}
{"title": "Helping students learn requirements engineering\n", "abstract": " Many software engineering courses (and methods) begin with an assumption that software requirements are presented to software engineers in a complete, consistent, feasible, testable, and traceable form, and that the software engineer's main job is to correctly transform the requirements into code. This is generally an unhealthy approach, as the requirements for virtually all significant software products are to some degrees unknown, unknowable, or the results of compromises requiring the software engineer's participation and expertise. In USC's first semester MS level software engineering core course, we have been experimenting with case study and role playing approaches to learning about software requirements engineering. The role playing approach involves a Theory W (win win) interpretation of software requirements as negotiated stakeholder win conditions. Students form three person teams to role play\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["5"]}
{"title": "Risk management practices: the six basic steps\n", "abstract": " Risk management practices | Software Risk Management ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksSoftware Risk ManagementRisk management practices: the six basic steps chapter Risk management practices: the six basic steps Share on Author: BW Boehm View Profile Authors Info & Affiliations Software Risk ManagementNovember 1989 Pages 115\u0393\u00c7\u00f4147 Published:01 November 1989 0citation 0 Downloads Metrics Total Citations0 Total Downloads0 Last 12 Months0 Last 6 weeks0 Get Citation Alerts New Citation Alert added! This alert has been successfully added and will be sent to: You will be notified whenever a . , .\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["5"]}
{"title": "ECSS-A prospective design tool for integrated information systems\n", "abstract": " A pre11nIin. ry version of Ecss is cwrently enrerlnq a field rent phase. In rhlch users outside Rand Can\" $0 and ex-perimnt with its basic crpabililies. and contribute to it $ further refinenmt. Con-curlenlly. Ye have bron analyzing tho major srrenqLhs and weaknesses of this version of sunmry of this rnrlysio (from Ref. 111. PCSS and p, rnninq rcflncments. BFlW is L). odd.", "num_citations": "4\n", "authors": ["5"]}
{"title": "The Professor and the Computer: 1985\n", "abstract": " The scenario reflects the authors belief that extrapolations into the future should point out not only the prospects but also the problems of implementing future technology.Descriptors:", "num_citations": "4\n", "authors": ["5"]}
{"title": "How features in ios app store reviews can predict developer responses\n", "abstract": " Until recently, communications regarding apps on the iOS App Store have been one-way from users to developers, with developers unable to respond to reviews directly. While studies have shown that responding to reviews improves an app's overall rating and user satisfaction, resource limitations make it so developers can usually only respond to some of the reviews. Although developers' response behavior has been studied, little is known about which features (aspects) of user reviews spur their responses. Motivated by these observations, we investigate a wide range of features that can be extracted from a user review and apply a random forest algorithm and the features it extracts to predict whether developers will respond to that review. We then determine the importance of these features in distinguishing reviews that receive a developer response from those that do not. Through a case study of three popular\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["5"]}
{"title": "Exploring the dependency relationships between software qualities\n", "abstract": " Software-intensive systems are expected to meet a variety of non-functional quality objectives essential for proper system functioning. However, there is still a lack of an effective estimation method for overall quality. Previous work has shown that it is possible to evaluate software systems' qualities based on issue summaries; however, this requires large amounts of manual effort to employ. There is a need for automated methods which can evaluate software systems as they evolve. Moreover, system qualities do not always change independently of each other. There are various dependencies which need to be taken into consideration when looking to make quality improvements. In this paper we look to explore the dependencies between bugs and the qualities they express. We have a fuzzy classifier that uses issue summaries to identify and summarize the quality of software with respect to maintainability, thus we\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["5"]}
{"title": "Process-driven incremental effort estimation\n", "abstract": " Effort estimation has shown its value in process decisions, such as feasibility analysis, resource allocation, risk mitigation, and project planning. In this paper, we propose an incremental effort estimation method that integrates phase-based effort estimation models to solve the concerns about software process compatibility and estimation accuracy, which one may often have when adopting effort estimation methods for project management. We define the process framework for incremental effort estimation in terms of the transition between the phase model that defines the analysis and design activities, the system models that specify a system's behavior and structure at different levels of detail, the sizing model that measures software functional size via transaction analysis, the Bayesian model that statistically models the effects that the size measurements have on project effort, and the phase-based effort estimation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["5"]}
{"title": "Rising rural body-mass index is the main driver of the global obesity epidemic in adults\n", "abstract": " Body-mass index (BMI) has increased steadily in most countries in parallel with a rise in the proportion of the population who live in cities 1,2 . This has led to a widely reported view that urbanization is one of the most important drivers of the global rise in obesity 3\u0393\u00c7\u00f46 . Here we use 2,009\u252c\u00e1population-based studies, with measurements of height and weight in more than 112\u252c\u00e1million adults, to report national, regional and global trends in mean\u252c\u00e1BMI segregated by place of residence (a rural or urban area) from 1985 to 2017. We show that, contrary to the dominant paradigm, more than 55% of the global rise in mean BMI from 1985 to 2017\u0393\u00c7\u00f6and more than 80% in\u252c\u00e1some low- and middle-income regions\u0393\u00c7\u00f6was due to increases in BMI in rural areas. This large contribution stems from the fact that, with the exception of women in sub-Saharan Africa, BMI is increasing at the same rate or faster in rural areas than in cities\u252c\u00e1in low- and middle-income regions. These trends have in turn resulted in a closing\u0393\u00c7\u00f6and in some countries reversal\u0393\u00c7\u00f6of the gap in BMI between urban and rural areas in low- and middle-income countries, especially for women. In high-income and industrialized countries, we noted a persistently higher rural BMI, especially for women. There is an urgent need for an integrated approach to rural nutrition that enhances financial and physical access to healthy foods, to avoid replacing the rural undernutrition disadvantage in poor countries with a more general malnutrition disadvantage that entails excessive consumption of low-quality calories.", "num_citations": "3\n", "authors": ["5"]}
{"title": "Calibrating use case points using bayesian analysis\n", "abstract": " Background: Use Case Points (UCPs) have been widely used to estimate software size for object-oriented projects. Yet, many research papers criticize the UCPs methodology for not being verified and validated with data, leading to inaccurate size estimates.Aims: This paper explores the use of Bayesian analysis to calibrate the use case complexity weights of the UCPs method to improve software size and project effort estimation accuracy.Method: Bayesian analysis is applied to integrate prior information (in this study, the weights defined by the UCPs method and suggested by other research papers) with parameter values suggested by multiple linear regression on the data. To validate the effectiveness of this approach, we run the Bayesian-inspired analysis on projects implemented by master's students at University of Southern California and a public dataset retrieved from PROMISE, and compared its\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["5"]}
{"title": "UMLx: a UML diagram analytic tool for software management decisions\n", "abstract": " A UML diagram analytic tool called UMLx is proposed, which automatically extracts information from UML diagrams to facilitate decision making in risk management, planning, resource allocation, and system design, based on a set of proposed metrics.", "num_citations": "3\n", "authors": ["5"]}
{"title": "Preliminary causal discovery results with software effort estimation data\n", "abstract": " Correlation does not imply causation. Though this is a well-known fact, most analyses depend on correlation as proof of relationships that are often treated as causal. Causal discovery, also referred to as causal model search, involves the application of statistical methods to identify causal relationships from conditional independences (and/or other statistical relationships) in the data. Though software cost estimation models use both domain knowledge and statistics, to date, there has yet to be a published report describing the evaluation of a software dataset using causal discovery. Two of the authors have previously used regression analysis to evaluate the effectiveness of the International Function Points User Group (IFPUG)'s and the Common Software Measurement International Consortium (COSMIC)'s functional size measurement methods for analyzing the Unified Code Count (UCC) 1's dataset of maintenance\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["5"]}
{"title": "Combatting Use Case Points\u0393\u00c7\u00d6 Limitations with COCOMO (R) II and Relative Difficulty\n", "abstract": " Software cost estimates become more accurate as more information becomes available, but are needed early for business case analyses, bids, and resource management. Use Case Points satisfy the ability to make software size estimates early in the lifecycle because they only require understanding how an actor will use the system. Though Use Case Points are easy to calculate, they might over-simplify a project's size and lead to inaccurate estimates. The Use Case Points method has also been heavily criticized for its technical and environmental factors, since they were not calibrated and verified with data. COCOMO(R) II possesses a rich knowledge base of factors that were calibrated and verified with data and expert judgment. Calculating and calibrating an effort model on Unified Code Count (UCC)'s project set using COCOMO(R) II's parameters and Use Case Points needed an additional relative difficulty\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["5"]}
{"title": "Software Project Risk and Opportunity Management\n", "abstract": " Risk is an uncertain event or condition that has a positive or negative effect on project objectives. Risk management includes the processes of planning, identification, analysis, resource planning, and controlling risk in a project. This chapter focuses on recent insights and approaches within risk management. A positive counterpart to risk management has emerged, called opportunity management. The duality between the two concepts is explained, and the fundamentals of risk\u0393\u00c7\u00f4opportunity management are discussed. Furthermore, risk and opportunity management methods, processes, and tools are presented.", "num_citations": "3\n", "authors": ["5"]}
{"title": "4.6. 3 Affordable Systems: Balancing the Capability, Schedule, Flexibility, and Technical Debt Tradespace\n", "abstract": " With the increasing demands for affordable system capabilities that can be provided quickly to the user community, developers must explore a variety of options for identifying \u0393\u00c7\u00a3satisficing\u0393\u00c7\u00a5 solutions. The system capability affordability tradespace must balance expedited systems engineering to reduce schedule and cost, encourage flexibility in architecture decisions to support future evolution of the system, and minimize technical debt that either results in later rework or adversely impacts future options. This paper shows how the University of Southern California (USC) Center for Systems and Software Engineering (CSSE) software and systems engineering cost models can be used in the analysis of this tradespace to show the range of options and the resulting consequences.", "num_citations": "3\n", "authors": ["5"]}
{"title": "An orthogonal framework for improving life cycle affordability\n", "abstract": " Many approaches to improving system affordability focus on one or two strategies (e,g., automation, outsourcing, repurposing, reuse, process maturity), and miss the opportunity for further improvements. Often, they focus on one phase (e.g., acquisition) at the expense of other factors that increase total ownership cost (TOC).Based on several related research projects, we have developed, applied, and evolved an orthogonal framework of strategies for improving Affordability. The framework includes:\u0393\u00c7\u00f3Get the best from people (Staffing, Teambuilding, Facilities, Kaizen)\u0393\u00c7\u00f3Make tasks more efficient (Tools, Work and Oversight Streamlining, Collaboration Technology)\u0393\u00c7\u00f3Eliminate tasks (Lean and Agile Methods, Automation, Model-Based Product Generation)\u0393\u00c7\u00f3Eliminate scrap and rework (Early Risk and Defect Elimination, Evidence-Based Decision Gates, Modularity around Sources of Change, Evolutionary Development, \u0393\u00c7\u00aa)\u0393\u00c7\u00f3\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["5"]}
{"title": "An Empirical Study of Requirements-to-Code Elaboration Factors\n", "abstract": " During a software development project\u0393\u00c7\u00d6s lifecycle, requirements are specified at multiple levels of detail eg objectives, shall statements, use cases, etc. The elaboration of requirements proceeds from one level of detail to the other till the final level\u0393\u00c7\u00f4lines of code\u0393\u00c7\u00f4is reached. This empirical study conducts a quantitative analysis of this hierarchical process of requirements elaboration. Multi-level requirements data from 25 small e-services projects is examined and the ratios between the numbers of requirements at consecutive levels are determined. Knowledge of these ratios\u0393\u00c7\u00f4called elaboration factors\u0393\u00c7\u00f4can play a crucial role in early software cost estimation. A case study demonstrating the utility of this approach in producing an early size estimate of a large commercial project is also presented.", "num_citations": "3\n", "authors": ["5"]}
{"title": "Adjusting Software Life-Cycle Anchorpoints: Lessons Learned in a System of Systems Context\n", "abstract": " Hardware Availability and Performance While focus of the event was on software development, any software review on FCS must come with an any software review on FCS must come with an accompanying understanding of hardware capability and delivery schedules. Tested and available hardware are necessary for any FCS operational software test.", "num_citations": "3\n", "authors": ["5"]}
{"title": "Software cost estimation for fractionated space systems\n", "abstract": " Fractionated Space Systems, as exemplified by Defense Advanced Research Projects Agency's Future Fast, Flexible, Fractionated, Free-Flying Spacecraft united by Information eXchange (DARPA\u0393\u00c7\u00d6s System F6), represent real challenges to software cost estimation. The concept is a traditional'monolithic'spacecraft is replaced by a cluster of wirelessly interconnected spacecraft modules to create a virtual satellite, delivering capability which is at least equivalent to the monolithic spacecraft. Concurrently, they significantly enhance flexibility and system robustness, and reduce risk throughout the mission life and the spacecraft development cycle. Such systems present real challenges to software cost estimation which arise from both the concept of a Directed System of Systems (DSOS) and the reduced risk which is primarily achievable only through the application of an Incremental Commitment Model (ICM). This paper\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["5"]}
{"title": "A process framework for system and software development governance\n", "abstract": " Effective system and software development governance involves clear definitions of a project's desired business objectives and associated roles, responsibilities, and authorities; concurrent determination of the system's objectives, constraints, priorities, and solutions with respect to business and technical feasibility; thorough definition of implementation plans; monitoring and control of project progress with respect to plans; and pro-active monitoring of the business and operational environment to adapt the plans to new challenges and opportunities.", "num_citations": "3\n", "authors": ["5"]}
{"title": "Nancy R. Mead: Making Requirements Prioritization a Priority\n", "abstract": " Nancy Mead has been expounding the virtues of requirements prioritization, especially in the area of information security, for over a decade. She advocates avoiding purely informal ad-hoc approaches and suggests a prioritization strategy based on both cost of implementing requirements and the expected value if implemented. Due to critical schedule and resource constraints, we have found that requirements prioritization is a key risk management practice for real project for real clients based courses. This work describes our experiences applying Nancy Meadpsilas requirement prioritization works within such courses and how we educate students on the importance of utilizing this for managing requirements implementation risk in student projects within three universities for both graduate and undergraduates.", "num_citations": "3\n", "authors": ["5"]}
{"title": "Improving process decisions in COTS\u0393\u00c7\u00c9based development via risk\u0393\u00c7\u00c9based prioritization\n", "abstract": " Good project planning requires the use of appropriate process models as well as effective decision support techniques. However, current software process models provide very little Commercial Off\u0393\u00c7\u00c9the\u0393\u00c7\u00c9Shelf (COTS)\u0393\u00c7\u00c9specific insight and guidance on helping COTS\u0393\u00c7\u00c9based application developers to make better decisions with respect to their particular project situations. This article presents a risk\u0393\u00c7\u00c9based prioritization approach that is used in the context of a COTS Process Decision Framework. This method is particularly useful in supporting many dominant decisions during a COTS\u0393\u00c7\u00c9based development (CBD) process, such as establishing COTS assessment criteria, scoping and sequencing development activities, prioritizing features to be implemented in incremental development, etc. In this way, the method not only provides a basis for optimal COTS selection but also helps focus limited development resources on\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["5"]}
{"title": "A software development environment for improving productivity\n", "abstract": " A major effort at improving productivity at TRW led to the creation of the software productivity project, or SPP, in 1981. The major thrust of this project is the establishment of a software development environment to support project activities; this environment is called the software productivity system, or SPS. It involves a set of strategies, including the work environment, the evaluation and procurement of hardware equipment, the provision for immediate access to computing resources through local area networks, the building of an integrated set of tools to support the software development life cycle and all project personnel, and a user support function to transfer new technology. All of these strategies are being accomplished incrementally. The current architecture is Vax-based and uses the Unix operating system, a wideband local network, and a set of software tools.", "num_citations": "3\n", "authors": ["5"]}
{"title": "Measuring Security Investment Benefit for COTS Based Systems-A Stakeholder Value Driven Approach\n", "abstract": " This paper presents the Threat Modeling method based on Attacking Path Analysis (T-MAP) which quantifies security threats by calculating the total severity weights of relevant attacking paths for Commercial Off The Shelf (COTS) based systems. Compared to existing approaches, T-MAP is sensitive to an organization\u0393\u00c7\u00d6s business value priorities and IT environment. It distills the technical details of thousands of relevant software vulnerabilities into management-friendly numbers at a high-level. In its initial usage in a large IT organization, T-MAP has demonstrated significant strength in prioritizing and estimating security investment effectiveness, as well as in evaluating the security performance of COTS systems. In the case study, we demonstrate the steps of using T-MAP to analyze the cost-effectiveness of how system patching, user account control and firewall can improve security. In addition, we introduce a software tool that automates the T-MAP.", "num_citations": "3\n", "authors": ["5"]}
{"title": "Providing incentives for spiral developments: An award fee plan\n", "abstract": " This plan provides candidate evaluation scales and weights for assessing relative contractor/subcontractor performance based on the seven factors. It also provides a set of candidate operating procedures for the subcontractor evaluation and award fee process. It is not a \u0393\u00c7\u00a3plug-and-play instrument.\u0393\u00c7\u00a5 Instead, the plan intends to provide buyers of systems with large software content with a tailorable contractual instrument that accommodates various sources of subcontract variation.", "num_citations": "3\n", "authors": ["5"]}
{"title": "Software Dependability Applications in Process Modeling\n", "abstract": " Software process modeling can be used to reason about strategies for attaining software dependability. The impact of different processes and technologies on dependability attributes can be evaluated through modeling and simulation. Strategies may have overlapping capabilities, and process modeling is useful for assessing mixed strategies. Dependability has many facets, and there is no single software dependability metric that fits all situations. A stakeholder value-based approach is useful for determining relevant dependability measures for different contexts. Analytical models and simulation techniques including continuous systems and discrete event modeling approaches can be applied to dependability. Continuous systems modeling is easier for aggregate analyses. Discrete event has some advantages for dependability applications because multiple attributes related to dependability measures can\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["5"]}
{"title": "Meeting the challenge of software engineering education for working professionals in the 21st century\n", "abstract": " Software engineering education for working professionals remains a challenge from the perspective of determining relevant content; identifying effective methods for delivery; and maintaining the focus and motivation of students. This panel brings together academic and industry professionals to share their perspectives and experiences. Anticipated points for discussion include: education/training delivery strategies, curriculum definition, certification challenges, marketing issues, collaboration strategies to engage industry sponsorship, value assessments for students and sponsoring organizations, and program success stories. This will be a highly interactive panel and the audience should come prepared to both ask and answer questions.", "num_citations": "3\n", "authors": ["5"]}
{"title": "Using Risk to Balance Agility and Discipline: A Quantitative Analysis\n", "abstract": " We have shown several qualitative analyses [2, 3] indicating that one can balance the risks of having too little project discipline with the risks of having too much project discipline, to find a \u0393\u00c7\u00a3sweet spot\u0393\u00c7\u00a5 operating point which minimizes the overall risk exposure for a given project. We have shown qualitatively that as a project\u0393\u00c7\u00d6s size and criticality increase, the sweet spot moves toward more project discipline, and vice versa.However, these results would have stronger credibility if shown to be true for a quantitative analysis backed up by a critical mass of data. Here we show the results of such a quantitative analysis, based on the cost estimating relationships in the COCOMO II cost estimation model and its calibration to 161 diverse project data points [1]. The projects in the COCOMO II database include management information systems, electronic services, telecommunications, middleware, engineering and science, command and control, and real time process control software projects. Their sizes range from 2.6 thousand lines of code (KLOC) to 1,300 KLOC, with 13 projects below 10 KLOC and 5 projects above 1000 KLOC.", "num_citations": "3\n", "authors": ["5"]}
{"title": "Reuse emphasized at next process workshop\n", "abstract": " We have described the key activities necessary for designing and analyzing an experiment in software engineering. We began by explaining how to choose an appropriate research technique to fit the goals of your project. In particular, we taught you how to state your hypothesis and determine how much control you need over the variables involved. If control is not possible, then a formal experiment is not possible; a case study may be a betteJ approach.Next, we explained the principles of formal experimentation. We listed the six stages of an experiment: conception, design, preparation, execution, analysis and dissemination. The design of an experiment was discussed in some detail. In particular, we pointed out that you must consider the need for replication, randomization and local control in any experiment that you plan to perform. We showed you how you can think of your design in terms of two types of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["5"]}
{"title": "Applying process programming to the spiral model\n", "abstract": " Applying process programming to the spiral model | Software Risk Management ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksSoftware Risk ManagementApplying process programming to the spiral model chapter Applying process programming to the spiral model Share on Authors: Barry William Boehm profile image BW Boehm View Profile , Frank C Belz profile image FC Belz View Profile Authors Info & Affiliations Publication: Software Risk ManagementNovember 1989 Pages 38\u0393\u00c7\u00f446 0citation 0 Downloads Metrics Total Citations0 Total Downloads0 Last 12 Months0 Last 6 weeks0 Get Citation Alerts New Citation Alert added.\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["5"]}
{"title": "The Open Channel\n", "abstract": " I compared these costs with an estimated $300 to $400 million Air Force expenditure for computer hard-ware in 1973, leading to an H/S cost ratio of 1: 3 or 1: 4. Details of this analysis are part of a more thorough and extensive 1974 report by David Fisher. 8 Fisher's results for the Department of Defense were comparable to the CCIP-85 results for the Air Force. His DoD estimates were $2.9 to $3.6 billion for software and $1.0 to $1.4 billion for hardware, giving an H/S cost ratio of roughly 1: 2.5Cragon's third assertion is that the cost of software is high, but less than the cost of hardware, and that any other interpretation of the available data is invalid. The analysis for the first assertion shows that, once we look at the real costs underlying the simplifying labels, we find that software costs are considerably higher than hardware costs over the en-tire US. But we shouldn't go from here to assert that the other interpretations of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["5"]}
{"title": "The high cost of software: causes and corrections\n", "abstract": " In any large scale data processing development the cost of software development is three to five times the cost of hardware. It is estimated that in 1980's this development cost will jump to ten times that of the expenditures predicted for the hardware of that era. If the Government or industry is to maintain a ceiling on this cost, research must be conducted on ways to reduce costs in the various phases of software development such as design, specification, implementation, etc. For purposes of this session,\" cost\" will encompass not only the resources necessary to develop the software or transfer it, but also costs associated with delays in both delivery and\" fixing,\" and resources wasted with software of poor quality. This session is an outgrowth of a tri-service/industry symposium entitled\" The High Cost of Software\" which was held at the Navy Post-Graduate Center, Monterey, California from September 17--19, 1973, in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["5"]}
{"title": "Learning Features that Predict Developer Responses for iOS App Store Reviews\n", "abstract": " Background: Which aspects of an iOS App Store user review motivate developers to respond? Numerous studies have been conducted to extract useful information from reviews, but limited effort has been expended to answer this question.Aims: This work aims to investigate the potential of using a machine learning algorithm and the features that can be extracted from user reviews to model developers' response behavior. Through this process, we want to uncover the learned relationship between these features and developer responses.Method: For our prediction, we run a random forest algorithm over the derived features. We then perform a feature importance analysis to understand the relative importance of each individual feature and groups thereof.Results: Through a case study of eight popular apps, we show patterns in developers' response behavior. Our results demonstrate not only that rating and review\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["5"]}
{"title": "On Building an Automatic Identification of Country-Specific Feature Requests in Mobile App Reviews: Possibilities and Challenges\n", "abstract": " Mobile app stores are available in over 150 countries, allowing users from all over the world to leave public reviews of downloaded apps. Previous studies have shown that such reviews can serve as sources of requirements and suggested that users from different countries have different needs and expectations regarding the same app. However, the tremendous quantity of reviews from multiple countries, as well as several other factors, complicates identifying country-specific app feature requests. In this work, we present a simple approach to address this through NLP-based analysis and discuss some of the challenges involved in using the NLP-based analysis for this task.", "num_citations": "2\n", "authors": ["5"]}
{"title": "Parallel Agile\u0393\u00c7\u00f4faster delivery, fewer defects, lower cost\n", "abstract": " \u252c\u2310 Springer Nature Switzerland AG 2020", "num_citations": "2\n", "authors": ["5"]}
{"title": "Large-Scale Parallel Development\n", "abstract": " In this chapter, you\u0393\u00c7\u00d6ll first learn how the ICSM CSFs were primarily derived from a set of successful large interactive TRW command and control software projects, which also satisfied and extended the CSFs to address forms of PA for very large systems. You\u0393\u00c7\u00d6ll then look at couple of examples. The first example is the three-version, million-line Command Center Processing and Display System\u0393\u00c7\u00f4Replacement (CCPDS-R) project. The second example is a series of even larger TRW command and control\u0393\u00c7\u00f4type projects that extended the set of CSFs and delivered highly successful results. Later in the chapter, we compare the PA approach and CSFs with other successful scalable Agile approaches, such as the speed, data, and ecosystems (SDE) approach (Bosch 2017) and the Scaled Agile Framework (SAFe; Leffingwell 2017).", "num_citations": "2\n", "authors": ["5"]}
{"title": "The parallel agile process: Applying parallel processing techniques to software engineering\n", "abstract": " For the last 4 years, we have been experimenting with the parallel agile (PA) approach. PA achieves significant schedule compression by leveraging parallelism; large teams of developers can independently and concurrently develop scenarios from initial concept through code. This paper summarizes our experience in defining and evolving PA by applying it to four representative emergent\u0393\u00c7\u00c9technology applications: location\u0393\u00c7\u00c9based advertising, picture sharing, bad driver reporting, and a VR/AR game project. In comparison with the mainstream architected agile process that we had been using on similar systems, the PA process has consistently achieved significant speedups in system development while simultaneously reducing defects. PA uses storyboards and prototypes to define both sunny\u0393\u00c7\u00c9day and rainy\u0393\u00c7\u00c9day scenarios, defines requirements for each use case, and decomposes each use case into a conceptual\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["5"]}
{"title": "Do consumers talk about the software in my product? an exploratory study of iot products on amazon\n", "abstract": " Consumer product reviews are an invaluable source of data because they contain a wide range of information that could help requirement engineers to meet user needs. Recent studies have shown that tweets about software applications and reviews on App Stores contain useful information, which enable a more responsive software requirements elicitation. However, all of these studies\u0393\u00c7\u00d6 subjects are merely software applications. Information on system software, such as embedded software, operating systems, and firmware, are overlooked, unless reviews of a product using them are investigated. Challenges in investigating these reviews could come from the fact that there is a huge volume of data available, as well as the fact that reviews of such products are diverse in nature, meaning that they may contain information mostly on hardware components or broadly on the product as a whole. Motivated by these observations, we conduct an exploratory study using a dataset of 7198 review sentences from 6 Internet of Things (IoT) products. Our qualitative analysis demonstrates that a sufficient quantity of software related information exists in these reviews. In addition, we investigate the performance of two supervised machine learning techniques (Support Vector Machines and Convolutional Neural Networks) for classification of information contained in the reviews. Our results suggest that, with a certain setup, these two techniques can be used to classify the information automatically with high precision and recall.", "num_citations": "2\n", "authors": ["5"]}
{"title": "Systems of systems thinking\n", "abstract": " As more of our systems interoperate in one or more independently evolving systems of systems and user needs focus more on \u0393\u00c7\u00a3new capabilities\u0393\u00c7\u00a5 rather than a \u0393\u00c7\u00a3new system,\u0393\u00c7\u00a5 so too do our systems thinking processes need to evolve to SoS thinking. SoS thinking builds upon systems thinking to consider the interactions of the SoS constituent systems with each other, along with the challenges of keeping multiple independently evolving SoSs compatibly evolving with each other and the external environment. This paper presents a framework for defining and guiding SoS thinking.", "num_citations": "2\n", "authors": ["5"]}
{"title": "A scalable and efficient approach for compiling and analyzing commit history\n", "abstract": " Background: Researchers oftentimes measure quality metrics only in the changed files when analyzing software evolution over commit-history. This approach is not suitable for compilation and using program analysis techniques that require byte-code. At the same time, compiling the whole software not only is costly but may also leave us with many uncompilable and unanalyzed revisions. Aims: We intend to demonstrate if analyzing changes in a module results in achieving a high compilation ratio and a better understanding of software quality evolution. Method: We conduct a large-scale multi-perspective empirical study on 37838 distinct revisions of the core module of 68 systems across Apache, Google, and Netflix to assess their compilability and identify when the software is uncompilable as a result of a developer's fault. We study the characteristics of uncompilable revisions and analyze compilable ones to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["5"]}
{"title": "Education and Social Development of T-Shaped Software Engineers\n", "abstract": " A T-shaped engineer is one who has deep knowledge and skills in a particular discipline, and also has a working knowledge of other disciplines important to the engineering projects they contribute to. Many computer science departments graduate mostly I-shaped students. They have strong knowledge and skills in computer science (CS), but little knowledge and skills in other disciplines such as economics, management sciences, human factors, or physical sciences important to their organization\u0393\u00c7\u00d6s projects. This paper summarizes my experiences with a large, diversified company, TRW, in addressing the challenge of broadening new I-shaped CS graduates to become full participants in TRW projects; learning about T-shaped engineers from Simon Ramo (the R in TRW); and deciding to early-retire from TRW to create a software engineering curriculum at USC that would produce more T-shaped MS graduates, and ultimately produce more T-shaped BS graduates.The paper proceeds to describe the resulting MS degree in CS, with specialization in Software Engineering, and particularly its real-client project course, in which teams of 6 students develop a software system satisfying the needs of a USC campus or neighborhood organization. This addresses some other shortfalls in undergraduate computer science students\u0393\u00c7\u00d6 education, in which they perform and are evaluated as individuals vs. being effective team members, learning about pure computer science topics such as software algorithms, data structures, programming languages, compilers, operating systems, database management systems, programming, software testing and debugging\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["5"]}
{"title": "Further Causal Search Analyses With UCC's Effort Estimation Data\n", "abstract": " Correlation does not imply causation. Though this is a well-known fact, most analyses depend on correlation as proof of relationships that are often treated as causal. Causal search, also referred to as causal discovery, involves the application of statistical methods to identify causal relationships using conditional independences (and/or other statistical relationships) within data. Though software cost estimation models use both domain knowledge and statistics, to date, there has yet to be a published report describing the evaluation of a software dataset using causal search. In a previous paper, the authors ran a PC causal search algorithm on Unified Code Count's (UCC's)  dataset of maintenance tasks and compared them to correlation test results. This paper builds on the previous paper to introduce causal discovery to software engineering research by exploring additional causal search algorithms (PC-Stable, fast greedy equivalent search [FGES], and fast adjacency skewness [FASK]) and comparing their results to the traditional multi-step regression analysis.", "num_citations": "2\n", "authors": ["5"]}
{"title": "Incremental Development Productivity Decline\n", "abstract": " Software production is on the critical path of increasingly many program abilities to deliver effective operational capabilities. This is due to the number, complexity, independence, interdependence, and software-intensiveness of their success-critical components and interfaces. The estimation parameters and knowledge bases of current software estimation tools are generally good for stable, standalone, single increment development. However, they do not fully account for the degrees of program and software dynamism, incrementality, coordination, complexity, and integration. These phenomena tend to decrease software productivity relative to the cost model estimates made for the individual software components and for the overall systems, but it is difficult to estimate by how much.Incremental software development generally involves either adding, modifying, or deleting parts of the code in the previous increments. This means that if a useful system is to be built, the maintenance that will have to go into previous increments will take away productivity from the later ones.", "num_citations": "2\n", "authors": ["5"]}
{"title": "An initial process decision table and a process evolution process\n", "abstract": " The Incremental Commitment Spiral Model (ICSM) presented in an ICSSP 2014 tutorial includes several decision milestones at which evidence of the feasibility of the proposed process is evaluated, and at which the stakeholders decide whether to proceed with it or to change course, based on the risk of proceeding with the proposed process. This generates a large number of potential processes, but we have found risk patterns that provide selection criteria for a set of common cases for at least the initial process.", "num_citations": "2\n", "authors": ["5"]}
{"title": "Improving software development tracking and estimation inside the cone of uncertainty\n", "abstract": " Software cost and schedule estimations are fundamental in software development projects as they determine the scopes and the resources required. With accurate estimations, the goals of project outcome can be assured within the available resources. However, developing accurate and realistic estimates require high level of experience, expertise, and historical data. Oftentimes, once the resources have been estimated, little is done to reduce the uncertainties in the estimations as the project progresses through its life cycle. To address this issue, we have developed the COTIPMO tool, an implementation of the COnstructive Team Improvement Process MOdel framework, to help automate the recalibration and estimation improvement processes. The tool allows software development teams to effectively track their development progress, assess the team\u0393\u00c7\u00d6s performance, and adjust the project estimates based on the assessment results. The COTIPMO tool has been used by 13 software engineering projects and the results are presented in this paper.", "num_citations": "2\n", "authors": ["5"]}
{"title": "Panel on the role of graduate software and systems engineering bodies of knowledge in formulating graduate software engineering curricula\n", "abstract": " The Software Engineering Body of Knowledge (SWEBOK), published in 2004, and now under revision, has influenced many software engineering graduate programs worldwide. In 2009, guidelines were published for graduate programs in software engineering (GSWE2009). GSWE2009, now sponsored by both the IEEE Computer Society and the Association for Computing Machinery, strongly rely on the SWEBOK but also recommends specific systems engineering knowledge for students to master. Today, an international team is creating a rigorous Systems Engineering Body of Knowledge (SEBoK) with the help of the IEEE Computer Society and the International Council on Systems Engineering and other professional societies. As it matures, the SEBoK should influence future versions of GSWE2009 and graduate program curricula worldwide. This panel will examine the influence of bodies of knowledge on both\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["5"]}
{"title": "Comparative analysis of requirements elaboration of an industrial product\n", "abstract": " Requirements for a software development project are gradually refined as more information becomes available. This process of requirements elaboration can be quantified using the appropriate set of metrics. This paper reports the results of an empirical study conducted to analyze the requirements elaboration of an industrial software process management tool-SoftPM-being used by more than 300 Chinese commercial software organizations. After adjusting for the effects of overlaps amongst different versions of SoftPM, multi-level requirements data are gathered and elaboration factors for each version are obtained. These elaboration data are compared with the data from a previous empirical study that analyzed requirements elaboration of a set of different small e-services projects. This comparison reveals that the elaboration factors of different SoftPM versions have much less variation confirming the intuition that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["5"]}
{"title": "Dangerous Differences: Crisis Management and Sino-American Naval Doctrines in the Taiwan Strait\n", "abstract": " In previous work I argue for a connection between different military doctrinal cultures and the failure of coercive diplomacy. Different military cultures in the United States and China affected signaling, communication, perception, and assessments of the balance of power. This led to failures of deterrence and coercion during the Korean War. In this paper, I extend this idea into two additional naval cases: the deterrence of conflict in the Taiwan Strait in 1950 and today. I argue that Chinese naval doctrine today is, for cultural and organizational reasons, distinct from that of the United States. I assess the potential dangers this poses to the conduct of military diplomacy in the Taiwan Strait, and offer proposals to overcome or limit these challenges.", "num_citations": "2\n", "authors": ["5"]}
{"title": "An Early Application generator and Other Recollections\n", "abstract": " In this recollection, I focus primarily on the experiences I had while developing an early application generator in the area of rocket flight mechanics. I begin by describing some of the events that led up to this work, and I end by describing some of the experiences I have had since.", "num_citations": "2\n", "authors": ["5"]}
{"title": "Putting systems to work: Processes for expanding system capabilities through system of systems acquisitions\n", "abstract": " Business and mission pressures to provide new complex system capabilities quickly are leading organizations to pursue the development of systems of systems. This allows us to get additional \u0393\u00c7\u00a3mileage\u0393\u00c7\u00a5 from our existing, but somewhat aging systems by putting them to work in a system of systems environment with other systems in order to achieve the new desired capabilities. Our experiences in helping to define, acquire, develop, and assess 21st century software-intensive system of systems (SISOS) have taught us that traditional 20th century acquisition and development processes do not work well on such systems. This article summarizes the characteristics of such systems, and indicates the major problem areas in using traditional processes on them. We also present new processes that we and others have been developing, applying, and evolving to address 21st century SISOS. These include extensions to the risk-driven spiral model to cover broad (many systems), deep (many supplier levels), and long (many increments) acquisitions needing rapid fielding, high assurance, adaptability to high change traffic, and complex interactions with evolving, often complex, Commercial Offthe-Shelf (COTS) products, legacy systems, and external systems.", "num_citations": "2\n", "authors": ["5"]}
{"title": "Workshop description of 4th workshop on software quality (WOSQ)\n", "abstract": " Cost, schedule and quality are highly correlated factors in software development. They basically form three sides of the same triangle. Beyond a certain point (the\" Quality is Free\" point), it is difficult to increase the quality without increasing either cost or schedule or both for the software under development. As products and applications mature, users expect higher quality products. They want IT organizations to be responsible and accountable for the quality claims made by the product marketing teams. In the last couple decades, much software engineering research has focussed on standards, methodologies and techniques for improving software quality, measuring software quality and software quality assurance. Most of this research is focused on the internal/development view of quality. More recent studies done in conjunction with the marketing groups have made attempts to understand the customer view of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["5"]}
{"title": "Educating students in value-based design and development\n", "abstract": " Summary form only given. Much of software engineering is taught and practiced in a value-neutral context, in which every requirement, use case, object, test case, and defect is equally important. Too often, students learn that some of their stakeholders' value considerations are more important than others by failing to consider this on the job and suffering the consequences. The recent book, Value-Based Software Engineering (S. Biffl et al., eds., Springer, 2005) sets out the agenda of the value-based software engineering community. It is to integrate value considerations into traditional software engineering principles and practices for use in software engineers' education and daily work. We have been pursuing this agenda in a research project called\" A Value-Based Science of Design\", within the NSF Science of Design program. This paper addresses the nature of\" value\" in a software engineering context; present\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["5"]}
{"title": "Mining and analyzing digital archive usage data to support collection development decisions\n", "abstract": " We demonstrate a collection development decision support tool that mines digital archive usage data. We want to better understand the University of Southern California (USC) Digital Archive's collection structure by analyzing the objects' characteristics, by analyzing the relationships between viewed objects, and by understanding usage trends over time. By relying on implicit patterns of usage data, such as co-retrievals, rather than explicit data, such as hit counts, we believe we can make more informed decisions about where to expend our resources", "num_citations": "2\n", "authors": ["5"]}
{"title": "Early Experiences in Software Economics\n", "abstract": " My first exposure to software economics came on my first day in the software business, in June 1955 at General Dynamics in San Diego. My supervisor took me on a walking tour through the computer, and ERA 1103, which occupied most of a large room. His most memorable comment was,                \u0393\u00c7\u00a3Now listen. We\u0393\u00c7\u00d6re paying this computer six hundred dollars an hour, and we\u0393\u00c7\u00d6re paying you two dollars an hour, and I want you to act accordingly.\u0393\u00c7\u00a5", "num_citations": "2\n", "authors": ["5"]}
{"title": "CORADMO a software cost estimation model for RAD projects\n", "abstract": " RAD (Rapid Application Development)\u0393\u00c7\u00f3 an application of any of a number of techniques or strategies to reduce software development cycle time", "num_citations": "2\n", "authors": ["5"]}
{"title": "MBASE/RUP Phase and Activity Distribution\n", "abstract": " A Management AA Inception phase management AAA Business case development AAB Elaboration phase release specifications AAC Elaboration phase WBS baselining AAD Software development plan AAE Inception phase project control and status assessments AB Elaboration phase management ABA Construction phase release specifications ABB Construction phase WBS baselining ABC Elaboration phase project control and status assessments AC Construction phase management ACA Deployment phase planning ACB Deployment phase WBS baselining ACC Construction phase project control and status assessments AD Transition phase management ADA Next generation planning ADB Transition phase project control and status assessments B Environment; C Requirements; D Design; E Implementation; F Assessment; G Deployment", "num_citations": "2\n", "authors": ["5"]}
{"title": "The Rosetta Stone: Making COCOMO 81 Files Work With COCOMO II\n", "abstract": " As part of our efforts to help COCOMO users, we, the COCOMO research team at the Center for Software Engineering at the University of Southern California (USC), have developed the Rosetta Stone for converting COCOMO 81 files to run using the new COCOMO II software cost estimating model. The Rosetta Stone is very important because it allows users to update estimates made with the earlier version of the model so that they can take full advantage of the many new features incorporated into the COCOMO II package. This paper describes both the Rosetta Stone and guidelines for making the job of conversion easy.", "num_citations": "2\n", "authors": ["5"]}
{"title": "Software reuse economics\n", "abstract": " Implementation of a powerful filter or waveform synthesis invention led to the design of a GSM standardized GMSK baseband processor which has a significantly reduced hardware complexity. A method for designing a filter by storing the possible output waveforms in a look-up table is presented. Based on bit comparisons, the filter controller selects a waveform to output. This method has the advantages of fast design turn-around time and uses very few resources. Using another filter invention, the hardware savings could be as much as 400% which will decrease power consumption. The design method is applied to a BTb= 0.5 Gaussian low pass filter with several sampling intervals and several quantization levels. In addition, this technique was applied to the Feher filter.", "num_citations": "2\n", "authors": ["5"]}
{"title": "Humans and process frameworks: some critical process elements\n", "abstract": " Successful engineering of complex software systems require humans to engage collaboratively in multiple critical process elements. This paper identifies those necessary process elements and defines WinWin, a collaborative process model that addresses the process elements. It briefly describes a process support system for the WinWin model.", "num_citations": "2\n", "authors": ["5"]}
{"title": "KBSE Tool for Megaprogramming\n", "abstract": " Research in Knowledge Based Software Engineering (KBSE)| the area of building intelligent systems that can automate the construction of software| has been intensi ed in the last ten years, and is now addressing a full range of software engineering activities Johnson 1993], trying to improve the quality and reduce the cost of software development.Megaprogramming Boehm 1992], the practice of software construction in a componentoriented fashion heavily based on software components reuse, has already been recognized as an important solution for the software crisis Radice 1988]. It is a powerful means of not only reducing software development costs in the long run, but also increasing software correctness and reliability among other main software quality factors. Thus, megaprogramming is a very strong candidate for KBSE tool support.", "num_citations": "2\n", "authors": ["5"]}
{"title": "A Software Development Environment for Improving Productivity\n", "abstract": " A major effort at improving productivity at TRW led to the creation of the software productivity project, or SPP, in 1981. The major thrust of this project is the establishment of a software development environment to support project activities; this environment is called the software productivity system, or SPS. It involves a set of strategies, including the work environment; the evaluation and procurement of hardware equipment; the provision for immediate access to computing resources through local area networks; the building of an integrated set of tools to support the software development life cycle and all project personnel; and a user support function to transfer new technology. All of these strategies are being accomplished incrementally. The current architecture is Vax-based and uses the Unix operating system, a wideband local network, and a set of software tools. This article* describes the steps that led to the creation of the SPP, summarizes the requirements analyses on which the SPS is based, describes the components which make up the SPS, and presents our conclusions.", "num_citations": "2\n", "authors": ["5"]}
{"title": "\u00b5\u00c5\u00c9\u0398\u00bd\u00ff\u03a6\u255c\u00bb\u03a3\u2557\u2562\u03c4\u00f6\u0192\u03a3\u2551\u00ba\u03c4\u00c4\u00e7 (\u03c4\u2557\u00a1)\n", "abstract": " \u03c3\u00a2\u00a2, \u00b5\u00c5\u00c9\u0398\u00bd\u00ff\u00b5\u00ee\u00eb\u03c4\u00e0\u00ba\u03a6\u255c\u00bb\u03a3\u2557\u2562\u03c4\u00f6\u0192\u03a3\u2551\u00ba\u03c4\u00c4\u00e7\u00b5\u00a3\u2551\u03a3\u255d\u00dc\u00b5\u00e1\u00e6\u03c4\u00dc\u00e4\u03c4\u2557\u00e4\u03c4\u2557\u00e7, \u03a3\u2555\u00ef\u0398\u00a5\u00f3\u03c3\u00bb\u2563\u00b5\u00c5\u00c9\u0398\u00bd\u00ff\u03a6\u255c\u00bb\u03a3\u2557\u2562\u03c4\u00f6\u0192\u03a3\u2551\u00ba\u03c4\u00c4\u00e7\u03c4\u00dc\u00e4\u03c3\u00e7\u00e1\u03a3\u2555\u00ac\u03a3\u2555\u2557\u03a6\u00aa\u00fc\u03c3\u00c5\u00bb\u0398\u00c7\u00eb\u00b5\u00fb\u2563\u0398\u00a5\u00f3\u03a6\u2510\u00a2\u03a6\u00ed\u00ee\u03a6\u00ab\u00bf\u03a6\u00ab\u2551: 1) \u00b5\u00ee\u00e6\u0398\u00c7\u00eb\u03c4\u2593\u255b\u03c3\u2563\u2593\u03a3\u2551\u2551\u03c3\u00e6\u00ff, 2) \u00b5\u00c5\u00c9\u0398\u00bd\u00ff\u0398\u00ff\u2562\u00b5\u00ab\u2561\u00b5\u00f2\u00ea\u03c4\u00c4\u00e7, 3) \u00b5\u2562\u00ea\u0398\u00d6\u00f1\u03a3\u2551\u2551\u03c3\u2556\u00d1\u0398\u00ff\u2562\u00b5\u00ab\u2561, 4) \u03c3\u00e7\u00c5\u03c3\u2591\u00e6\u0398\u00e7\u00ec\u03c3\u00f1\u00ec\u03c3\u00e8\u2502\u03c3\u00e8\u00bf, 5) \u03c3\u2557\u2551\u0398\u00c7\u00e1\u03c4\u00ab\u00c7\u03c3\u00ec\u00f2\u03a3\u2551\u00ba\u03c3\u00f4\u00fc, 6) \u0398\u00e7\u00ec\u03c4\u00f6\u00bf\u03a6\u255c\u00bb\u0398\u00e2\u00bf\u03a3\u2557\u2562. \u00b5\u00ee\u00e6\u0398\u00c7\u00eb\u03c4\u2593\u255b\u03c3\u2563\u2593\u03a3\u2551\u2551\u03c3\u00e6\u00ff\u03a6\u2592\u00ed\u00b5\u00a3\u2551\u03a3\u255d\u00dc\u00b5\u00e1\u00e6\u00b5\u00eb\u00c7\u03a6\u00ed\u00bf\u03a6\u2561\u00f1\u03c4\u00dc\u00e4\u0398\u00e9\u00fa\u00b5\u00e1\u2556, \u00b5\u00ee\u00e6\u0398\u00c7\u00eb\u03c4\u2593\u255b\u03c3\u2563\u2593\u03a3\u2551\u2551\u03c3\u00e6\u00ff\u00b5\u00a3\u00eb\u03a3\u2555\u00eb\u03a3\u2555\u00ac\u03a3\u2555\u2557\u03a6\u00aa\u00fc\u03c3\u00c5\u00bb\u0398\u00c7\u00eb\u0398\u00ed\u2563:", "num_citations": "2\n", "authors": ["5"]}
{"title": "Computer Performance Evaluation: Report of the 1973 NBS/ACM Workshop\n", "abstract": " The National Bureau of Standards1 was established by an act of Congress March 3, 1901. The Bureau's overall goal is to strengthen and advance the Nation's science and technology and facilitate their effective application for public benefit. To this end, the Bureau conducts research and provides:(1) a basis for the Nation's physical measurement system,(2) scientific and technological services for industry and government,(3) a technical basis for equity in trade, and (4) technical services to promote public safety. The Bureau consists of the Institute for Basic Standards, the Institute for Materials Research, the Institute for Applied Technology, the Institute for Computer Sciences and Technology, and the Office for Information Programs.THE INSTITUTE FOR BASIC STANDARDS provides the central basis within the United States of a complete and consistent system of physical measurement; coordinates that system with measurement systems of other nations; and furnishes essential services leading to accurate and uniform physical measurements throughout the Nation's scientific community, industry, and commerce. The Institute consists of a Center for Radiation Research, an Office of Measurement Services and the following divisions:", "num_citations": "2\n", "authors": ["5"]}
{"title": "Computer Performance Analysis: Objectives and Problems in Simulating Computers\n", "abstract": " The report describes a method for relating the issues and objectives of a simulation of a computer system that permits the simulation designer to anticipate possible difficulties. The issues that determine the designers approach are the amount of resources, degree of simulation code change, number of boundary changes, appropriateness of a cost model, importance of experimental design, level of detail, degree of accuracy, and amount and type of validation. The objectives of the simulation analysis have been reduced to three categories absolute projection, sensitivity analysis, and diagnostic investigation. A matrix is created to show the importance of each issue to each objective and to indicate some solutions.Descriptors:", "num_citations": "2\n", "authors": ["5"]}
{"title": "Information Processing Requirements for Future Air Force Command and Control Systems, and Some Implications for Software Research and Development\n", "abstract": " The report summarizes some results of a recent Air Force study on what should be cone in information processing now to meet the major requirements for Air Force command and control in the future.Descriptors:", "num_citations": "2\n", "authors": ["5"]}
{"title": "SIMULATION AIDS FOR DESIGNING INTEGRATED INFORMATION SYSTEMS. THE ECSS LANGUAGE\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u251c\u00actre utilis\u251c\u2310 dans le cadre d\u0393\u00c7\u00d6une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u251c\u2592alado antes, el contenido de este registro bibliogr\u251c\u00edfico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS\" O: 13:\\\" PanistOpenUrl\\\": 36:{s: 10:\\\"\\u0000*\\u0000openUrl\\\"; N; s: 6:\\\"\\u0000*\\u0000idc\\\"; N; s: 6:\\\"\\u0000*\\u0000fmt\\\"; s: 7:\\\" journal\\\"; s: 6:\\\"\\u0000*\\u0000doi\\\"; s: 0:\\\"\\\"; s: 6:\\\"\\u0000*\\u0000pii\\\"; s: 0:\\\"\\\"; s: 7:\\\"\\u0000*\\u0000pmid\\\"; s: 0:\\\"\\\"; s: 9:\\\"\\u0000*\\u0000atitle\\\"; s: 79:\\\" SIMULATION AIDS FOR DESIGNING INTEGRATED INFORMATION SYSTEMS. THE ECSS LANGUAGE\\\"; s: 9:\\\"\\u0000*\\u0000jtitle\\\"; s: 0:\\\"\\\"; s: 9:\\\"\\u0000*\\u0000stitle\\\"; s: 0:\\\"\\\"; s: 7:\\\"\\u0000*\\u0000date\\\"; s: 4:\\\" 1972\\\"; s: 9:\\\"\\u0000\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["5"]}
{"title": "Computing in South America: Some Observations and Policy Considerations for Aiding Technological Development\n", "abstract": " The document is concerned with the scope and diversity of South American computing trends, and of the similarities and differences in the ways in which the cultures of South America and that of the US are adapting to the onrush of computer technology. AuthorDescriptors:", "num_citations": "2\n", "authors": ["5"]}
{"title": "Developing interactive graphics systems for aerospace applications\n", "abstract": " Section 111 describes a higher-level programming system called POGO (Programmer-Oriented Graphics Operation) for creating interactive graphics interfaces by example instead of by programming. POGO has reduced our development time on Graphic ROCKET control displays by factors of from four to ten. It also provides a general capability for developing various cmon types of interactive graphics programs, with relatively little elapsed time, programming expertise, or graphics expertise.", "num_citations": "2\n", "authors": ["5"]}
{"title": "Systems engineering crosstalk\n", "abstract": " Article Submissions: We welcome articles of interest to the defense software community. Articles must be approved by the CROSSTALK editorial board prior to publication. Please follow the Author Guidelines, available at< www. stsc. hill. af. mil/crosstalk/xtlkguid. pdf>. CROSSTALK does not pay for submissions. Articles published in CROSSTALK remain the property of the authors and may be submitted to other publications. Reprints: Permission to reprint or post articles must be requested from the author or the copyright holder and coordinated with CROSSTALK.", "num_citations": "2\n", "authors": ["5"]}
{"title": "How Should Developers Respond to App Reviews? Features Predicting the Success of Developer Responses\n", "abstract": " Context: The Google Play Store allows app developers to respond to user reviews. Existing research shows that response strategies vary considerably. In addition, while responding to reviews can lead to several types of favorable outcomes, not every response leads to success, which we define as increased user ratings.Aims: This work has two objectives. The first is to investigate the potential to predict early whether a developer response to a review is likely to be successful. The second is to pinpoint how developers can increase the chance of their responses to achieve success.Method: We track changes in user reviews of the 1,600 top free apps over a ten-week period, and find that in 11,034 out of 228,274 one-to four-star reviews, the ratings increase after a response. We extract three groups of features, namely time, presentation and tone, from the responses given to these reviews. We apply the extreme\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["5"]}
{"title": "Process Implications of Executable Domain Models for Microservices Development\n", "abstract": " Microservice architecture has been recognized as an important enabler for continuous development of many cloud-based systems. Code generation has been tried in the tool chain of building microservices. However, most existing tools generally do not consider the risks from continuous development.", "num_citations": "1\n", "authors": ["5"]}
{"title": "Investigating the use of duration\u0393\u00c7\u00c9based windows and estimation by analogy for COCOMO\n", "abstract": " In model\u0393\u00c7\u00c9based software estimation, using the right training data is a key contributor for making accurate predictions, which is crucial for the success of software projects. This study investigates the use of duration\u0393\u00c7\u00c9based windows and estimation by analogy to calibrate COCOMO and assess their estimation performance. We compare these approaches as well as the use of all available historical data using the COCOMO data set of 341 projects and NASA data set of 93 projects. The results show that timing information exists in the data sets affecting estimation accuracy. Given sufficient data for calibration, using recently completed projects within short durations generates more accurate estimates than retaining all historical data or using k\u0393\u00c7\u00c9nearest neighbors based on estimation by analogy. More training data spanning a long period of time may not lead to improved estimation accuracy. This study offers evidence to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["5"]}
{"title": "Technical Debt Prioritization: A Search-Based Approach\n", "abstract": " Technical Debt (TD) prioritization is the process of deciding which TD items should be repaid first and which items can be endured until later releases. The goal of the process is to maximize the value of the TD repayment with limited resources. Unfortunately, researchers have indicated the scarcity of TD prioritization techniques and limitations in them. To address these limitations, we propose a novel search-based approach for prioritizing TD using a Multi-objective Evolutionary Algorithm (MOEA). The approach indicates which TD items should be repaid to maximize the value of a repayment activity within a specific cost constraint. An empirical evaluation that we performed on 40 Open-Source Software (OSS) systems demonstrated our approach's ability to improve the value of TD repayment by 1,796 over random search. Additionally, a user study that we conducted with developers confirmed the suitability of our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["5"]}
{"title": "Characterizing software maintainability in issue summaries using a fuzzy classifier\n", "abstract": " Despite the importance of software maintainability in the life cycle of software systems, accurate measurement remains difficult to achieve. Previous work has shown how bug reports can be classified by expressed quality concerns which can give insight into maintainability across domains and over time. However, the amount of manual effort required to produce such classifications limits its usage. In this paper, we build a fuzzy classifier with linguistic patterns to automatically map issue summaries into the seven subgroup SQ classifications provided in a software maintainability ontology. We investigate how long it takes to generate a stable set of rules and evaluate the performance of the rule set on both rule generating and nonrule generating projects. The results validate the generalizability of the fuzzy classifier in correctly and automatically identifying the subgroup SQ classifications from given issue summaries\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["5"]}
{"title": "Do users talk about the software in my product? Analyzing user reviews on IoT products\n", "abstract": " Consumer product reviews are an invaluable source of data because they contain a wide range of information that could help requirement engineers to meet user needs. Recent studies have shown that tweets about software applications and reviews on App Stores contain useful information, which enable a more responsive software requirements elicitation. However, all of these studies' subjects are merely software applications. Information on system software, such as embedded software, operating systems, and firmware, are overlooked, unless reviews of a product using them are investigated. Challenges in investigating these reviews could come from the fact that there is a huge volume of data available, as well as the fact that reviews of such products are diverse in nature, meaning that they may contain information mostly on hardware components or broadly on the product as a whole. Motivated by these observations, we conduct an exploratory study using a dataset of 7198 review sentences from 6 Internet of Things (IoT) products. Our qualitative analysis demonstrates that a sufficient quantity of software related information exists in these reviews. In addition, we investigate the performance of two supervised machine learning techniques (Support Vector Machines and Convolutional Neural Networks) for classification of information contained in the reviews. Our results suggest that, with a certain setup, these two techniques can be used to classify the information automatically with high precision and recall.", "num_citations": "1\n", "authors": ["5"]}
{"title": "The SERC 5-Year Technical Plan: Designing the Future of Systems Engineering Research\n", "abstract": " The Systems Engineering Research Center (SERC), a US University Affiliated Research Center, developed the 2014\u0393\u00c7\u00f42018 Technical Plan to provide the vehicle by which to align the SERC Vision and Research Strategy with the US Federal Government Sponsor\u0393\u00c7\u00d6s top research priorities. This paper summarizes the SERC Vision, the Sponsor\u0393\u00c7\u00d6s needs, and the SERC\u0393\u00c7\u00d6s response to these needs. It then describes the objectives, approach and content of the original five-year SERC Technical Plan, and provides an overview of the results. Emerging systems challenges are noted along with the approach that is being used to ad-dress them in the upcoming Five Year Technical Plan. Finally, this paper de-scribes the status of the new plan and some of the opportunities and challenges that it provides.", "num_citations": "1\n", "authors": ["5"]}
{"title": "Agile software development cost modeling for the US DoD\n", "abstract": " In this presentation, Dr. Boehm proposes a model for estimating the effort that will be needed to complete Agile software development projects. Previous estimation capabilities often require information such as function points, story points, or lines of code that are not available early for agile projects. For this work the team used initial and final Software Resource Data Reports from the Cost Assessment Data Enterprise or CADE (of the Office of the Secretary of Defense (OSD) Cost Assessment and Program Evaluation (CAPE) initiative) to tie actual effort and processes to initial estimates. The proposed Agile Effort Estimation Model uses a combination of three variables to provide estimates, including the number of software requirements at contract start, an initial estimate of peak staffing, and which \u0393\u00c7\u00a3super-domain\u0393\u00c7\u00a5 the project fits within.", "num_citations": "1\n", "authors": ["5"]}
{"title": "Evaluation of cross-project multitasking in software projects\n", "abstract": " It has been observed that multitasking can cause inefficient (or unproductive) work. Modern lean and agile practices in software engineering processes also acknowledge the problem and attempt to eliminate waste by limiting work in progress and using better team organization and work scheduling techniques. Existing research has studied multitasking and work interruptions on individuals, but very few of them have evaluated the effects of multitasking on the team or the whole organization. The goal of this study is to understand how multitasking and interruptions affect the cost of software projects. In this paper, we present a method for quantitative evaluation of the negative impact of cross-project multitasking in software development. The presented method can serve as a tool for better effort estimation as well as a metric for productivity evaluation in multiproject environments. The method was used to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["5"]}
{"title": "Avoiding Non-Technical Sources of Software Maintenance Technical Debt\n", "abstract": " Much of the emphasis in identifying a software maintenance project\u0393\u00c7\u00d6s technical debt involves analysis of the code to be maintained. Our recent analysis of the sources of software maintenance cost have identified the major sources of technical debt for non-developer maintenance organizations to originate from non-technical acquisition and development decisions and practices. As a way of identifying and avoiding these sources, we have developed an initial counterpart of the Technical Readiness Level framework, called the Software Maintenance Readiness Level (SMRL) framework. This paper summarizes the main non-technical sources of software maintenance technical debt, their root causes, a first-cut SMRL framework, and our early experiences in evaluating it.", "num_citations": "1\n", "authors": ["5"]}
{"title": "Improving productivity for projects with high turnover\n", "abstract": " Improving Productivity for Projects with High Turnover Page 1 Improving Productivity for Projects with High Turnover Anandi Hira University of Southern California October 13, 2015 Page 2 Outline | 2 \u0393\u00a3\u00baIntroduction \u0393\u00a3\u00baIDPD \u0393\u00a3\u00baUCC \u0393\u00a3\u00baMetrics \u0393\u00a3\u00baHypotheses \u0393\u00a3\u00baData Analysis Results \u0393\u00a3\u00baHypothesis 1 \u0393\u00a3\u00baHypothesis 2 \u0393\u00a3\u00baConclusion Outline Page 3 Introduction | 3 \u0393\u00a3\u00ba Incremental Development Productivity Decline \u0393\u00a3\u00ba Decrease in productivity percentage due to \u0393\u00a3\u00ba Increasing Integration and Testing Efforts \u0393\u00a3\u00ba Software Evolution \u0393\u00a3\u00ba Defects Resolution IDPD Page 4 Introduction | 4 \u0393\u00a3\u00ba Unified Code Count \u0393\u00a3\u00ba Source Line of Code (SLOC) metrics \u0393\u00a3\u00ba Logical SLOC, Cyclomatic Complexity \u0393\u00a3\u00ba 10% personnel continuity over 4 months \u0393\u00a3\u00ba Data collected from: \u0393\u00a3\u00ba Developed code \u0393\u00a3\u00ba Weekly Timesheets \u0393\u00a3\u00ba Test Case Documentation UCC Page 5 Introduction | 5 Metrics Page 6 Outline | 6 \u0393\u00a3\u00baIntroduction \u0393\u00a3\u00baIDPD \u0393\u00a3\u00baUCC \u0393\u00a3\u00baMetrics \u0393\u00a3\u00baHypotheses \u0393\u00a3\u00baData Results /\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["5"]}
{"title": "Combining software engineering education and empirical research via instrumented real-client team project courses\n", "abstract": " Real-client, team project courses provide excellent opportunities for performing empirical research in software engineering (SE). Compared to empirical research on large, multi-year SE projects, a course with several team projects per year is the SE research equivalent of the fruit fly in species evolution research. Although their predictive power for large-project SE is more suggestive than definitive, the research results generally provide useful contributions to human knowledge in the SE area.", "num_citations": "1\n", "authors": ["5"]}
{"title": "The Incremental Commitment Spiral Model (ICSM)\n", "abstract": " Figure 1 and Figure 2 show the differences between the original spiral model and the ICSM. The original spiral model was meant to say that after a project had identified its objectives, constraints, and alternative solution approaches (OC&As), it would evaluate the alternatives with respect to those objectives and constraints, and make a risk-driven determination of what to do next.If one alternative was clearly superior and feasible, the project could elaborate its lower-level OC&As into detailed requirements, designs, and plans, and proceed into a build to requirements approach. However, if there were uncertainties (which translate into risks), the project would address the risks before proceeding into development. If there were no risks about the planned system\u0393\u00c7\u00d6s infrastructure, scalability, etc., and the only risks were about the user interface look and feel, the project could use the available infrastructure and just\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["5"]}
{"title": "Skating to Where the Puck Is Going: Future Systems and Software Engineering Opportunities and Challenges\n", "abstract": " This paper provides an update and extension of a 2005 paper on The Future of Systems and Software Engineering Processes. Some of its challenges and opportunities are similar, such as the need to simultaneously achieve high levels of both agility and assurance. Others have emerged as increasingly important, such as the opportunities and challenges of dealing with smart systems involving ultralarge volumes of data; with multicore chips; with social networking services; and with cloud computing or software as a service. The paper is organized around eight relatively surprise-free trends and two \u0393\u00c7\u00a3wild cards\u0393\u00c7\u00a5 whose trends and implications are harder to foresee. The eight surprise-free trends are:                                     1.                                         Increasing emphasis on rapid development and adaptability;                                                                        2.                                         Increasing software criticality and need for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["5"]}
{"title": "Does the use of Fibonacci numbers in Planning Poker affect effort estimates?\n", "abstract": " ConclusionIf you are following normal agile practices and doing planning poker you are already on your way to improved agile estimates. Two things you can try to improve your estimates even more:", "num_citations": "1\n", "authors": ["5"]}
{"title": "What is the Return On Investment for Using Systems Engineering?\n", "abstract": " I hope that you find the article in this issue of INSIGHT as enlightening as the first three articles in the series were, and that you will be looking forward to the final article in the series. For more information on the process and ground rules for these articles, please refer to my previous article in the December 2009 issue of INSIGHT.", "num_citations": "1\n", "authors": ["5"]}
{"title": "Value of flexibility-Phase 1\n", "abstract": " This report provides findings from the research conducted under the Phase 1 of RT-18-Valuing Flexibility. The primary goal of this research project is to identify, develop and validate sound quantitative methods, processes, and tools that will enable DoD leadership and program managers to make a convincing case for investments in system flexibility when acquisition decisions are made. The research during this period focused on identifying current quantitative MPTs for valuing flexibility in DoD context, and delivering an initial capability to value investments that provide the flexibility to handle foreseeable sources of change, using easy-to-understand monetizable terms, such as the effect of the investments on total cost of ownership or return on investment. We conducted a critical evaluation of the theoretical foundations underlying current approaches, the dimensions of flexibility, measures of flexibility, value functions, and methods for incorporating flexibility both at the design phase and the operational phase, to identify strengths and weaknesses of each approach. During this period, we also explored the development of an analytical framework based on sound mathematical constructs. This report also presents our advances in defining and formalizing the value of flexibility and the underlying capability-need-value architecture that can be used by systems acquisition decision-makers.Descriptors:", "num_citations": "1\n", "authors": ["5"]}
{"title": "Development of 3-year roadmap to transform the discipline of systems engineering\n", "abstract": " As systems continue to grow in size and complexity, it has become clear that existing Systems Engineering (SE) methods, processes and tools are becoming increasingly inadequate. The SET project is intended to identify the gaps and bring about the necessary transformation in Systems Engineering to satisfy the needs of the complex system\u0393\u00c7\u00d6s life cycle. Accomplishing this transformation requires a fundamental rethinking of current SE practices. The SET project is focused on first principles and stripping away non-essential activities while being cognizant of recent trends in SE. A number of trends collectively accelerate this challenge. Growing system complexity and criticality raise vulnerability. The ascendancy of software as the preferred solution continues in the face of significant gaps in our ability to understand, validate and manage large evolving software ecosystems. The increasing speed of technological change, the rapid evolution of threats, and the decreasing schedules for development all lead to the sense that time itself is compressing. New systems envisioned by the defense and intelligence communities reflect, embrace and reinforce these trends.", "num_citations": "1\n", "authors": ["5"]}
{"title": "An Empirical Study of the Efficacy of COCOMO II Cost Drivers in Predicting a Project\u0393\u00c7\u00d6s Elaboration Profile\n", "abstract": " A project\u0393\u00c7\u00d6s elaboration profile consists of a set of ratios called elaboration factors that quantify the step-wise expansion of a project\u0393\u00c7\u00d6s requirements from very high-level business objectives to very low-level source lines of code. Knowledge of a project\u0393\u00c7\u00d6s elaboration profile can be extremely useful in deriving an early estimate of its size. The real challenge, however, is to predict the elaboration profile. Can the COCOMO II cost drivers be used for this purpose? This paper attempts to answer this question. It examines the elaboration profiles and COCOMO II cost driver ratings of 25 small real-client projects. The data collection process is thoroughly described and the cost drivers relevant at each stage of elaboration are identified. Relationships between elaboration factors and relevant cost drivers are analyzed using simple as well as multiple regression. The results indicate that there is no magical formula for predicting the various elaboration factors just from the values of COCOMO II cost drivers. This may be due to some confounding factors which are highlighted in this paper.", "num_citations": "1\n", "authors": ["5"]}
{"title": "Early Identification of SE-Related Program Risks: Opportunities for DoD Systems Engineering (SE) Transformation via SE Effectiveness Measures (EMs) and Evidence-Based Reviews\n", "abstract": " DoD programs need effective systems engineering SE to succeed. DoD program managers need early warning of any risks to achieving effective SE. This SERC project has synthesized analyses of DoD SE effectiveness risk sources into a lean framework and toolset for early identification of SE-related program risks. Three important points need to be made about these risks. They are generally not indicators of bad SE. Although SE can be done badly, more often the risks are consequences of inadequate program funding SE is the first victim of an underbudgeted program, of misguided contract provisions when a program manager is faced with the choice between allocating limited SE resources toward producing contract-incentivized functional specifications vs. addressing key performance parameter risks, the path of least resistance is to obey the contract, or of management temptations to show early progress on the easy parts while deferring the hard parts till later. Analyses have shown that unaddressed risk generally leads to serious budget and schedule overruns. Risks are not necessarily bad. If an early capability is needed, and the risky solution has been shown to be superior to the alternatives, accepting and focusing on mitigating the risk is generally better than waiting for a better alternative to show up. Unlike traditional schedule-based and event-based reviews, the SERC SE EM technology enables sponsors and performers to agree on the nature and use of more effective evidence-based reviews. These enable early detection of missing SE capabilities or personnel competencies with respect to a framework of Goals, Critical Success\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["5"]}
{"title": "Realistic Software Cost Estimation for Fractionated Space Systems\n", "abstract": " Fractionated Space Systems, as exemplified by Defense Advanced Research Projects Agency's Future Fast, Flexible, Fractionated, Free-Flying Spacecraft united by Information eXchange (DARPA\u0393\u00c7\u00d6s System F6), represent real challenges to software cost estimation. The concept is a traditional'monolithic'spacecraft is replaced by a cluster of wirelessly interconnected spacecraft modules to create a virtual satellite, delivering capability which is at least equivalent to the monolithic spacecraft. Concurrently, they significantly enhance flexibility and system robustness, and reduce risk throughout the mission life and the spacecraft development cycle. Such systems present real challenges to software cost estimation which arise from both the concept of a Directed System of Systems (DSOS) and the reduced risk which is primarily achievable only through the application of an Incremental Commitment Model (ICM). This paper\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["5"]}
{"title": "Applying Value-Based Software Process: An ERP Example.\n", "abstract": " Commercial organizations increasingly need software processes sensitive to business value, quick to apply, supportive of multi-stakeholder collaboration, and capable of early analysis for the cost-effectiveness of process instances. This paper presents experience in applying a lightweight synthesis of a Value-Based Software Quality Achievement process and an Object-Petri-Net-based process model to achieve a stakeholder win-win outcome for software quality achievement in an on-going ERP software project in China. The application results confirmed that 1) the Object-Petri-Net-based process model provided project managers with a synchronization and stabilization framework for process activities, successcritical stakeholders and their value propositions; 2) process visualization and simulation tools significantly increased management visibility and controllability for the success of the software project.", "num_citations": "1\n", "authors": ["5"]}
{"title": "An Empirical Study on MBASE and LeanMBASE\n", "abstract": " From 1998-2005, the successful Model-Based (Systems) Architecting and Software Engineering (MBASE) had been used as a set of guidelines for the keystone two-semester real-client team project graduate software engineering course sequence. However, to fit with small-sized and limited schedule projects, MBASE was trimmed to reduce the huge amount of efforts in documentation. Consequently, LeanMBASE, which is a light-weight software process framework that helps teams identify the high-value activities and helps balance the workload of a development, is being used in the software engineering course. This paper reports the comparison and improvement of the projects that use MBASE and LeanMBASE in terms of content, performance, and customer satisfaction.", "num_citations": "1\n", "authors": ["5"]}
{"title": "Software design and structuring\n", "abstract": " In 1974, the major leverage point in the software process is at the software design and structuring stage. This stage begins with a statement of software requirements. Properly done, the software design and structuring process can identify the weak spots and mismatches in the requirements statement that are the main sources of unresponsiveness of delivered software. Once these weak spots slip by the design phase, they will generally stay until during or after delivery, where only costly retrofits can make the software responsive to operational needs.The software design and structuring phase ends with a detailed specification of the code that is to be produced, a plan for testing the code, and a draft set of users\u0393\u00c7\u00d6 manuals describing how the product is going to be used. Software costs and problems result from allowing coding to begin on a piece of software before its design aspects have been thoroughly worked out, verified, and validated. Figure 1 illustrates this quite well. It summarizes an analysis of 220 types of software errors found during a large, generally good (on-cost, on-schedule delivery) TRW software project [19, 20]. The great majority of the types of errors found in testing the code had originated in the design phase. Even more significantly, the design errors are generally not found until later in the test process. Of the 54% of the error types that were typically not found until during or after the acceptance test phase, only 9% were coding errors. The other 45% were design errors.", "num_citations": "1\n", "authors": ["5"]}
{"title": "Planning Fundamentals\n", "abstract": " This chapter contains sections titled:   21 Project Management Success Tips   Requirements Management: The Search for Nirvana   Requirements Engineering as a Success Factor in Software Projects   The Secrets of Planning Success   The Slacker's Guide to Project Tracking    ]]>", "num_citations": "1\n", "authors": ["5"]}
{"title": "Assessment of process modeling tools to support the analysis of system of systems engineering activities\n", "abstract": " Many organizations are attempting to provide new system capabilities through the net-centric integration of existing systems into systems of systems (SoS). The engineering activities used to architect and develop these SoS are often referred to as SoS Engineering (SoSE). Recent reports are indicating that SoSE activities are considerably different from classical systems engineering (SE) activities. Other systems engineering experts believe that there is nothing really different with respect to systems engineering activities or component-based engineering in the SoS environment\u0393\u00c7\u00f6that there are only differences in scale and complexity. To better understand SoSE, studies are currently underway to evaluate the differences between classical SE and SoSE. This paper summarizes process areas to be investigated in the SE-SoSE comparison and then analyzes and evaluates several types of process modeling tools in order to identify a set of tools that can be used to capture classical SE and SoSE process characteristics for further comparison.", "num_citations": "1\n", "authors": ["5"]}
{"title": "Software Engineering Economics\n", "abstract": " This paper summarizes the current state of the art and recent trends in software engineering economics. It provides an overview of economic analysis techniques and their applicability to software engineering and management. It surveys the field of soft-ware cost estimation, including the major estimation techniques available and the state of the art in algorithmic cost models.", "num_citations": "1\n", "authors": ["5"]}
{"title": "Value-based feedback in software and information systems development\n", "abstract": " The role of feedback control in software and information system development has traditionally focused on a milestone plan to deliver a prespecified set of capabilities within a negotiated budget and schedule.Under the right conditions (capable people; realistic budgets and schedules), the traditional approach has been highly successful in project control. It has also created a legacy of project data that has been used to develop more accurate models for estimating future projects\u0393\u00c7\u00d6 budgets and schedules. These models have also been very helpful in supporting software capability/schedule/cost tradeoff analysis and in improving software development efficiency.", "num_citations": "1\n", "authors": ["5"]}
{"title": "Value-based quality processes and results\n", "abstract": " Cost effectiveness is one of the important issues for developing products in a life cycle. And review is a key activity that can detect defects from the early stage and fix them. This paper provides Value-based review techniques adding cost effectiveness into review processes and reports on an experiment on Value-based review. Through the experiment, the Value-based review is shown to have higher cost effectiveness in review processes.", "num_citations": "1\n", "authors": ["5"]}
{"title": "Using iDAVE to determine availability requirements\n", "abstract": " Different systems have different success-critical stakeholders. Even for the same system, these stakeholders may depend on it in different ways for different scenarios. Therefore a one-size-fits-all dependability metric is unachievable in practice. In order to cost-effectively achieve the stakeholders' desired dependability attribute requirements for a given project, we have to solve such problems as how to define an appropriate level for a particular dependability attribute and how much dependability investment is enough for a particular software/scenario class. However, the answers to those questions are traditionally difficult to obtain. This paper uses a hypothetical Lunar Biological Laboratory (LBL) as an example to illustrate how to use the iDAVE model to determine the appropriate levels of availability requirements for different software/scenario classes based on their different ROI profiles.", "num_citations": "1\n", "authors": ["5"]}
{"title": "Bayesian analysis of empirical software engineering cost models\n", "abstract": " To date many software engineering cost models have been developed to predict the cost, schedule, and quality of the software under development. But, the rapidly changing nature of software development has made it extremely difficult to develop empirical models that continue to yield high prediction accuracies. Software development costs continue to increase and practitioners continually express their concerns over their inability to accurately predict the costs involved. Thus, one of the most important objectives of the software engineering community has been to develop useful models that constructively explain the software development life-cycle and accurately predict the cost of developing a software product. To that end, many parametric software estimation models have evolved in the last two decades (25],[17],[26],[15],[28],[1],[2],[33],[7],[10],[22],[23]. Almost all of the above mentioned parametric models have been empirically calibrated to actual data from completed software projects. The most commonly used technique for empirical calibration has been the popular classical multiple regression approach. As discussed in this paper, the multiple regression approach imposes a few assumptions frequently violated by software engineering datasets. The source data is also generally imprecise in reporting size, effort, and cost-driver ratings, particularly across different organizations. This results in the development of inaccurate empirical models that don't perform very well when used for prediction. This paper illustrates the problems faced by the multiple regression approach during the calibration of one of the popular software engineering cost\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["5"]}
{"title": "Reasoning About the Value of Dependability: The iDave Model\n", "abstract": " In this paper, we present a framework for reasoning about the value of information processing dependability investments called the Information Dependability Attribute Value Enhancement (iDAVE) model. We describe the overall structure of iDAVE, and illustrate its use in determining the ROI of investments in dependability for a commercial order processing system. We conclude that dynamic and adaptive value-based dependability mechanisms such as iDAVE model will become increasingly important provided evidence that dependability attribute requirement levels tend to be more emergent than pre-specifiable.", "num_citations": "1\n", "authors": ["5"]}
{"title": "Using risk to balance agile and plan-driven methods\n", "abstract": " Methodologies such as Extreme Pro-gramming (XP), Scrum, and agile software development promise in-creased customer satisfaction, lower defect rates, faster development times, and a solution to rapidly changing requirements. Plan-driven approaches such as Cleanroom, the Personal Software Process, or methods based on the Capability Maturity Model promise predictability, stability, and high assurance. However, both agile and planned approaches have situation-dependent shortcomings that, if left unaddressed, can lead to project failure. The challenge is to balance the two approaches to take advantage of their strengths in a given situation while compensating for their weaknesses. We present a risk-based approach for structuring projects to incorporate both agile and plan-driven approaches in proportion to a project\u0393\u00c7\u00d6s needs. We drew this material from our book, Balancing Agility and Discipline: A Guide\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["5"]}
{"title": "Tutorial 1: Introducing software economics within SWE project courses\n", "abstract": " In 1996, USC switched its core two-semester software engineering course from a hypotheticalproject, homework-and-exam course based on the Bloom taxonomy of educational objectives (knowledge, comprehension, application, analysis, synthesis, evaluation). The revised course is a real-client team-project course based on the CRESST model of learning objectives (content understanding, problem solving, collaboration, communication, and self-regulation). We used the CRESST cognitive demands analysis to determine the necessary student skills required for applying software economics and the other major project activities, and have been refining the approach over the last four years of experience, including revised versions for one-semester undergraduate and graduate project course at Columbia. We have found that the most critical skill the students need is the ability to collaborate with their clients in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["5"]}
{"title": "Model-based (systems) architecting and software engineering (MBASE)\n", "abstract": " Many organizations face a constant problem of obtaining accurate, relevant information about complex, evolving systems. We see this with deployed hardware and software systems, for example. Such systems often have long lifetimes, while staff turns over frequently. New staff assigned to tasks such as maintenance and upgrades have difficulty obtaining the information that they need to perform their specific tasks. They waste time sifting through irrelevant information in voluminous documents, and the information that they find may be out of date. Tactical decision-makers in the military face similar problems, but on compressed time scales. Command staff members need to obtain focused views of an involving tactical situation, and their jobs can be hampered if the information that they receive is cluttered with irrelevant material or is out of date.", "num_citations": "1\n", "authors": ["5"]}
{"title": "What happened to integrated environments?(panel session)\n", "abstract": " 20 years ago, in 1979, a landmark community-wide process was launched to establish notional requirements for integrated software engineering environments. The resulting\" Stoneman\" document was published in February 1980. Bred of the software engineering research community and catalyzed by the Government Ada sponsor, this\" integrated environment movement\" branched out and was embraced widely in the software engineering community in the 1980's as a needed, achievable, centrist approach to accelerate the benefits of disciplined software engineering into mainstream practice. The CASE tool industry bloomed as products integrating lifecycle activities and artifacts emerged, and research evolved to environments integrated by support for emerging, maturing notions of software processes. Yet, at the end of the 1990's, this movement appears to have virtually died, and more and more production\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["5"]}
{"title": "4.5. 3 A Comparison Study in Software Requirements Negotiation\n", "abstract": " In a period of two years, two rather independent experiments were conducted at the University of Southern California. In 1995, 23 three\u0393\u00c7\u00c9person teams negotiated the requirements for a hypothetical library system. Then in 1996, 14 six\u0393\u00c7\u00c9person teams negotiated the requirements for real multimedia related library systems. A number of hypotheses were created to test how real software projects differ from hypothetical ones. Other hypotheses address differences in uniformity and repeatability. The results indicate that repeatability in 1996 was even harder to achieve then in 1995 (Egyed\u0393\u00c7\u00c9Boehm, 1996). Nevertheless, this paper presents some surprising commonalties between both years that indicate some areas of uniformity. In both years, the same overall development process (spiral model) was followed, the same negotiation tools (WinWin System) were used, and the same people were doing the analysis of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["5"]}
{"title": "A User's Guide For The Software Technology Economic Impact Model\n", "abstract": " The Software Technology Economic Impact Model was developed by IDA for use in analyzing the effect that improvements in software technology have on the total DoD development and maintenance costs of weapon systems. The model displays the costs savings that result from three major sources work avoidance, working smarter, and working faster. This users manual gives instructions for installing and starting the model on an IBM or IBM-compatible PC or an Apple Macintosh computer. It also describes how to modify the baseline parameters of the model and how to estimate both development and maintenance cost savings using the model. Illustrations included show examples of the screens the user will see when maneuvering through the model.Descriptors:", "num_citations": "1\n", "authors": ["5"]}
{"title": "Rapid prototyping, risk management, 2167, and the Ada process model\n", "abstract": " Rapid prototyping, risk management, 2167, and the Ada process model | Software Risk Management ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksSoftware Risk ManagementRapid prototyping, risk management, 2167, and the Ada process model chapter Rapid prototyping, risk management, 2167, and the Ada process model Share on Author: Barry William Boehm profile image BW Boehm View Profile Authors Info & Affiliations Publication: Software Risk ManagementNovember 1989 Pages 47\u0393\u00c7\u00f452 0citation 0 Downloads Metrics Total Citations0 Total Downloads0 Last 12 Months0 Last 6 weeks0 Get Citation Alerts New Citation \u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["5"]}
{"title": "Dealing with uncertainties, risk, and the value of information\n", "abstract": " Dealing with uncertainties, risk, and the value of information | Software Risk Management ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksSoftware Risk ManagementDealing with uncertainties, risk, and the value of information chapter Dealing with uncertainties, risk, and the value of information Share on Author: Barry William Boehm profile image BW Boehm View Profile Authors Info & Affiliations Publication: Software Risk ManagementNovember 1989 Pages 310\u0393\u00c7\u00f4318 0citation 0 Downloads Metrics Total Citations0 Total Downloads0 Last 12 Months0 Last 6 weeks0 Get Citation Alerts New Citation Alert added! This alert has been \u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["5"]}
{"title": "Statistical decision theory: the value of information\n", "abstract": " Statistical decision theory | Software Risk Management ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksSoftware Risk ManagementStatistical decision theory: the value of information chapter Statistical decision theory: the value of information Share on Author: Barry William Boehm profile image BW Boehm View Profile Authors Info & Affiliations Publication: Software Risk ManagementNovember 1989 Pages 319\u0393\u00c7\u00f4330 0citation 0 Downloads Metrics Total Citations0 Total Downloads0 Last 12 Months0 Last 6 weeks0 Get Citation Alerts New Citation Alert added! This alert has been successfully added and will be sent to: You will be a . \u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["5"]}
{"title": "Software Risk Managpnent\n", "abstract": " Software risk management (RM) is a relatively new discipline whose objectives are to identify, assess, and eliminate software risk items before they become either threats to successful software operation or major sources of expensive software rework. Rework costs generally comprise 40\u0393\u00c7\u00f450% of overall software development costs; typically, 80% of the rework costs are caused by the highest-risk 20% of the software problems encountered. Software RM provides a context and a ratio-male for software verification and validation", "num_citations": "1\n", "authors": ["5"]}
{"title": "Community Information Utilities: Conference Summary\n", "abstract": " The conference was organized into three sections services, design and management. The services group was charged with the responsibility of proposing what services should be offered on a prototype community information utility. The design group was expected to determine how to develop the technology to provide those services at reasonable cost. The management group was expected to propose organizational structures and financing arrangements appropriate for the kind of services and the kind of technical system proposed by the other two groups. Portions of this document are not fully legible.Descriptors:", "num_citations": "1\n", "authors": ["5"]}
{"title": "National Degree of Computerization: A Context for Evaluating Computer Education Policies in Developing Countries.\n", "abstract": " Developing countries should take immediate steps to avoid aome of the serious problems that are now facing the United States in regard to the pool of trained computer professionals. Problem areas which should be reconciled involve a diverse range of topics from general national policy to salary structures and conversions efforts. By using the hypothesis that the relative magnitude of most computing problems facing a country is a function of the degree of computerization (as measured by the number of computers per billion dollars of gross national product) the various stages of computer development can be detected. The evolution of computerization problems, particularly as they pertain to personnel, in advanced countries can be analyzed and suggestions can be made on the policies that developing countries should attempt or avoid. For example, policies concentrating on developing narrowly oriented computer specialists are likely to satisfy near-term needs, but will tend to backfire later as computer applications become a more pervasive part of. national society.(MC)", "num_citations": "1\n", "authors": ["5"]}
{"title": "Educational Information System Design; A Conceptual Framework.\n", "abstract": " A number of critical aspects of educational information system design are discussed. The success of any information system depends largely upon several key decisions that must be made early in the preliminary design phase. The paper attempts to (1) indicate the nature and some of the major implications of these key decisions and (2) provide a framework for rational decision making.(Author)", "num_citations": "1\n", "authors": ["5"]}
{"title": "Third-Generation Computer Trends\n", "abstract": " This report describes some of the prospects for third-generation computers, and also points out some of the problems and pitfalls. It discusses the problem area to which engineering managers can contribute most effectively the inadequate feedback in the process of developing and using computer systems. AuthorDescriptors:", "num_citations": "1\n", "authors": ["5"]}
{"title": "Curve fitting and editing via interactive graphics\n", "abstract": " The system described here allows a user to enter a curve into an IBM 360/40 computer via a RAND tablet [1], and interactively to specify various ways of fitting, editing and displaying the curve on an IBM 2250 scope (see Fig. 1). It was developed primarily as a tool to extend the analysis of multivariate function representation (described by Boehm [2]) from tabular methods to polynomial methods. We decided to use an interactive graphics approach for three main reasons:", "num_citations": "1\n", "authors": ["5"]}
{"title": "Prospects of a space-based cryogenic computer.\n", "abstract": " Cryogenic computers using superconductive switching elements currently promise an attractive capability for very large, fast computer memories, and also for associative memories. Their major drawback is the operations and maintenance problem of keeping the computer at a temperature near absolute zero. This memorandum suggests basing a cryogenic computer in outer space to simplify its maintenance. It presents a rough functional design of a space-based cryogenic computer system, and points out the major design problems. Although it is too early to be certain, the possibility of achieving an economically feasible system in about 20 years is fairly strong. Furthermore, its success is critically dependent on the amount of effort devoted to realizing the potential capabilities of cryogenic computers and to solution of the indicated design problems. AuthorDescriptors:", "num_citations": "1\n", "authors": ["5"]}