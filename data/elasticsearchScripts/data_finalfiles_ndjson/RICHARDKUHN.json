{"title": "Proposed NIST standard for role-based access control\n", "abstract": " In this article we propose a standard for role-based access control (RBAC). Although RBAC models have received broad support as a generalized approach to access control, and are well recognized for their many advantages in performing large-scale authorization management, no single authoritative definition of RBAC exists today. This lack of a widely accepted model results in uncertainty and confusion about RBAC's utility and meaning. The standard proposed here seeks to resolve this situation by unifying ideas from a base of frequently referenced RBAC models, commercial products, and research prototypes. It is intended to serve as a foundation for product development, evaluation, and procurement specification. Although RBAC continues to evolve as users, researchers, and vendors gain experience with its application, we feel the features and components proposed in this standard represent a\u00a0\u2026", "num_citations": "7282\n", "authors": ["300"]}
{"title": "Role-based access control\n", "abstract": " Role-based access control (RBAC) is a security mechanism that can greatly lower the cost and complexity of security administration for large networked applications. RBAC simplifies security administration by using roles, hierarchies, and constraints to organize privileges. This book explains these components of RBAC, as well as how to support and administer RBAC in a networked environment and how to integrate it with existing infrastructure.", "num_citations": "4198\n", "authors": ["300"]}
{"title": "The NIST model for role-based access control: towards a unified standard\n", "abstract": " This paper describes a unified model for role-based access control RBAC. RBAC is a proven technology for large-scale authorization. However, lack of a standard model results in uncertainty and confusion about its utility and meaning. The NIST model seeks to resolve this situation by unifying ideas from prior RBAC models, commercial products and research prototypes. It is intended to serve as a foundation for developing future standards. RBAC is a rich and open-ended technology which is evolving as users, researchers and vendors gain experience with it. The NIST model focuses on those aspects of RBAC for which consensus is available. It is organized into four levels of increasing functional capabilities called flat RBAC, hierarchical RBAC, constrained RBAC and symmetric RBAC. These levels are cumulative and each adds exactly one new requirement. An alternate approach comprising flat and hierarchical RBAC in an ordered sequence and two unordered features| constraints and symmetry| is also presented. The paper furthermore identifies important attributes of RBAC not included in the NIST model. Some are not suitable for inclusion in a consensus document. Others require further work and agreement before standardization is feasible.", "num_citations": "1414\n", "authors": ["300"]}
{"title": "Role-based access control (RBAC): Features and motivations\n", "abstract": " The central norion f Roke-Based Access Control fRBAC) is thar users do nor hare discretionary access to enterprise objects. Insted, zess permissions are administrztitely associated with roles, and tsers are administrztitely made merrtbers of approprire rokes. This icka greatiy simplifies management fatthorization while pro riding an opporttinity for great feribility in specifying and enforring enterprise-specifr protection policies Users can be made members foles as determined by their responsibilities and qualifications and can be easily ressigned from one role to another without modifying the underlying access structure. Rokes can be granted new pertissions as new applications and actions are incorporated, and permissions can be recked font rokes as", "num_citations": "1168\n", "authors": ["300"]}
{"title": "Guide to attribute based access control (abac) definition and considerations (draft)\n", "abstract": " This publication has been developed by NIST to further its statutory responsibilities under the Federal 2 Information Security Management Act (FISMA), Public Law (PL) 107-347. NIST is responsible for 3 developing information security standards and guidelines, including minimum requirements for Federal 4 information systems, but such standards and guidelines shall not apply to national security systems 5 without the express approval of appropriate Federal officials exercising policy authority over such 6 systems. This guideline is consistent with the requirements of the Office of Management and Budget 7 (OMB) Circular A-130, Section 8b (3), Securing Agency Information Systems, as analyzed in Circular A-8 130, Appendix IV: Analysis of Key Sections. Supplemental information is provided in Circular A-130, 9", "num_citations": "879\n", "authors": ["300"]}
{"title": "A role-based access control model and reference implementation within a corporate intranet\n", "abstract": " This paper describes NIST's enhanced RBAC model and our approach to  designing and implementing RBAC features for networked Web servers. The RBAC model formalized in this paper is based on the properties that were first described in Ferraiolo and Kuhn [1992] and Ferraiolo et al. [1995], with adjustments resulting from experience gained by prototype implementations, market analysis, and observations made by Jansen [1988] and Hoffman [1996]. The implementation of RBAC for the Web (RBAC/Web) provides an alternative to the conventional means of administering and enforcing authorization policy on a server-by-server basis. RBAC/Web provides administrators with a means of managing authorization data at the enterprise level, in a manner consistent with the current set of laws, regulations, and    practices.", "num_citations": "668\n", "authors": ["300"]}
{"title": "Adding attributes to role-based access control\n", "abstract": " Nat\u2019l Computer Security Conf., NSA/NIST, 1992, pp. 554-563; R. Sandhu et al.,\u201cRole-Based Access Control Models,\u201d Computer, 29 (2), 1996, pp. 38-47), also known as RBAC, provides a popular model for information security that helps reduce the complexity of security administration and supports review of permissions assigned to users. This feature is critical to organizations that must determine their risk exposure from employee IT system access.RBAC has frequently been criticized for the difficulty of setting up an initial role structure and for inflexibility in rapidly changing domains. A pure RBAC solution may provide inadequate support for dynamic attributes such as time of day, which might need to be considered when determining user permissions. To support dynamic attributes, particularly in large organizations, a \u201crole explosion\u201d can result in thousands of separate roles being fashioned for different collections of permissions. Recent interest in attribute-based access control (ABAC) suggests that attributes and rules could either replace RBAC or make it more simple and flexible.", "num_citations": "558\n", "authors": ["300"]}
{"title": "An investigation of the applicability of design of experiments to software testing\n", "abstract": " Approaches to software testing based on methods from the field of design of experiments have been advocated as a means of providing high coverage at relatively low cost. Tools to generate all pairs, or higher n-degree combinations, of input values have been developed and demonstrated in a few applications, but little empirical evidence is available to aid developers in evaluating the effectiveness of these tools for particular problems. We investigate error reports from two large open-source software projects, a browser and Web server, to provide preliminary answers to three questions: Is there a point of diminishing returns at which generating all n-degree combinations is nearly as effective as all n+1-degree combinations? What is the appropriate value of n for particular classes of software? Does this value differ for different types of software, and by how much? Our findings suggest that more than 95% of errors in\u00a0\u2026", "num_citations": "425\n", "authors": ["300"]}
{"title": "Attribute-based access control\n", "abstract": " Attribute-based access control (ABAC) is a flexible approach that can implement AC policies limited only by the computational language and the richness of the available attributes, making it ideal for many distributed or rapidly changing environments.", "num_citations": "400\n", "authors": ["300"]}
{"title": "IPOG: A general strategy for t-way software testing\n", "abstract": " Most existing work on t-way testing has focused on 2-way (or pairwise) testing, which aims to detect faults caused by interactions between any two parameters. However, faults can also be caused by interactions involving more than two parameters. In this paper, we generalize an existing strategy, called in-parameter-order (IPO), from pairwise testing to t-way testing. A major challenge of our generalization effort is dealing with the combinatorial growth in the number of combinations of parameter values. We describe a t-way testing tool, called FireEye, and discuss design decisions that are made to enable an efficient implementation of the generalized IPO strategy. We also report several experiments that are designed to evaluate the effectiveness of FireEye", "num_citations": "396\n", "authors": ["300"]}
{"title": "Implementation of role-based access control in multi-level secure systems\n", "abstract": " Role-based access control (RBAC) is implemented on an multi-level secure (MLS) system by establishing a relationship between privileges within the RBAC system and pairs of levels and compartments within the MLS system. The advantages provided by RBAC, that is, reducing the overall number of connections that must be maintained, and, for example, greatly simplifying the process required in response to a change of job status of individuals within an organization, are then realized without loss of the security provided by MLS.", "num_citations": "339\n", "authors": ["300"]}
{"title": "IPOG/IPOG\u2010D: efficient test generation for multi\u2010way combinatorial testing\n", "abstract": " This paper presents two strategies for multi\u2010way testing (i.e. t\u2010way testing with t>2). The first strategy generalizes an existing strategy, called in\u2010parameter\u2010order, from pairwise testing to multi\u2010way testing. This strategy requires all multi\u2010way combinations to be explicitly enumerated. When the number of multi\u2010way combinations is large, however, explicit enumeration can be prohibitive in terms of both the space for storing these combinations and the time needed to enumerate them. To alleviate this problem, the second strategy combines the first strategy with a recursive construction procedure to reduce the number of multi\u2010way combinations that have to be enumerated. Both strategies are deterministic, i.e. they always produce the same test set for the same system configuration. This paper reports a multi\u2010way testing tool called FireEye, and provides an analytic and experimental evaluation of the two strategies\u00a0\u2026", "num_citations": "333\n", "authors": ["300"]}
{"title": "Security considerations for voice over IP systems\n", "abstract": " Voice over Internet Protocol (VOIP) refers to the transmission of speech across data-style networks. This form of transmission is conceptually superior to conventional circuit switched communication in many ways. However, a plethora of security issues plagues the still evolving VOIP system. This report introduces VOIP, its security challenges, and potential countermeasures for VOIP vulnerabilities.", "num_citations": "326\n", "authors": ["300"]}
{"title": "Sources of failure in the public switched telephone network\n", "abstract": " What makes a distributed system reliable? A study of failures in the US public switched telephone network (PSTN) shows that human intervention is one key to this large system's reliability. Software is not the weak link in the PSTN system's dependability. Extensive use of built-in self-test and recovery mechanisms in major system components (switches) contributed to software dependability and are significant design features in the PSTN. The network's high dependability indicates that the trade-off between dependability gains and complexity introduced by built-in self-test and recovery mechanisms can be positive. Likewise, the tradeoff between complex interactions and the loose coupling of system components has been positive, permitting quick human intervention in most system failures and resulting in an extremely reliable system.", "num_citations": "269\n", "authors": ["300"]}
{"title": "Introduction to combinatorial testing\n", "abstract": " Combinatorial testing of software analyzes interactions among variables using a very small number of tests. This advanced approach has demonstrated success in providing strong, low-cost testing in real-world situations. Introduction to Combinatorial Testing presents a complete self-contained tutorial on advanced combinatorial testing methods for real-world software. The book introduces key concepts and procedures of combinatorial testing, explains how to use software tools for generating combinatorial tests, and shows how this approach can be integrated with existing practice. Detailed explanations and examples clarify how and why to use various techniques. Sections on cost and practical considerations describe tradeoffs and limitations that may impact resources or funding. While the authors introduce some of the theory and mathematics of combinatorial methods, readers can use the methods without in-depth knowledge of the underlying mathematics. Accessible to undergraduate students and researchers in computer science and engineering, this book illustrates the practical application of combinatorial methods in software testing. Giving pointers to freely available tools and offering resources on a supplementary website, the book encourages readers to apply these methods in their own testing projects.", "num_citations": "264\n", "authors": ["300"]}
{"title": "Assessment of access control systems\n", "abstract": " Adequate security of information and information systems is a fundamental management responsibility. Nearly all applications that deal with financial, privacy, safety, or defense include some form of access control. Access control is concerned with determining the allowed activities of legitimate users, mediating every attempt by a user to access a resource in the system. In some systems, complete access is granted after successful authentication of the user, but most systems require more sophisticated and complex control. In addition to the authentication mechanism (such as a password), access control is concerned with how authorizations are structured. In some cases, authorization may mirror the structure of the organization, while in others it may be based on the sensitivity level of various documents and the clearance level of the user accessing those documents. This publication explains some of the commonly used access control services available in information technology systems.Organizations planning to implement an access control system should consider three abstractions: access control policies, models, and mechanisms. Access control policies are highlevel requirements that specify how access is managed and who may access information under what circumstances. For instance, policies may pertain to resource usage within or across organizational units or may be based on need-to-know, competence, authority, obligation, or conflict-of-interest factors. At a high level, access control policies are enforced through a mechanism that translates a user\u2019s access request, often in terms of a structure that a system provides. An access\u00a0\u2026", "num_citations": "257\n", "authors": ["300"]}
{"title": "Mutual exclusion of roles as a means of implementing separation of duty in role-based access control systems\n", "abstract": " Role based access control (RBAC) is attracting increasing attention as a security mechanism for both commercial and many military systems. Much of RBAC is fundamentally different from multi-level security (MLS) systems, and the properties of RBAC systems have not been explored formally to the extent that MLS system properties have. This paper explores some aspects of mutual exclusion of roles as a means of implementing separation of duty policies, including a safety property for separation of duty; relationships between different types of exclusion rules; properties of mutual exclusion for roles; constraints on the role hierarchy introduced by mutual exclusion rules; and necessary and sufficient conditions for the safety property to hold. Results have implications for implementing separation of duty controls through mutual exclusion of roles, and for comparing mutual exclusion with other means of implementing\u00a0\u2026", "num_citations": "245\n", "authors": ["300"]}
{"title": "Failure modes in medical device software: an analysis of 15 years of recall data\n", "abstract": " Most complex systems today contain software, and systems failures activated by software faults can provide lessons for software development practices and software quality assurance. This paper presents an analysis of software-related failures of medical devices that caused no death or injury but led to recalls by the manufacturers. The analysis categorizes the failures by their symptoms and faults, and discusses methods of preventing and detecting faults in each category. The nature of the faults provides lessons about the value of generally accepted quality practices for prevention and detection methods applied prior to system release. It also provides some insight into the need for formal requirements specification and for improved testing of complex hardware-software systems.", "num_citations": "242\n", "authors": ["300"]}
{"title": "Practical combinatorial testing: Beyond pairwise\n", "abstract": " With new algorithms and tools, developers can apply high-strength combinatorial testing to detect elusive failures that occur only when multiple components interact. In pairwise testing, all possible pairs of parameter values are covered by at least one test, and good tools are available to generate arrays with the value pairs. In the past few years, advances in covering-array algorithms, integrated with model checking or other testing approaches, have made it practical to extend combinatorial testing beyond pairwise tests. The US National Institute of Standards and Technology (NIST) and the University of Texas, Arlington, are now distributing freely available methods and tools for constructing large t-way combination test sets (known as covering arrays), converting covering arrays into executable tests, and automatically generating test oracles using model checking (http://csrc.nist.gov/acts). In this review, we focus on\u00a0\u2026", "num_citations": "230\n", "authors": ["300"]}
{"title": "Fault classes and error detection capability of specification-based testing\n", "abstract": " Some varieties of specification-based testing rely upon methods for generating test cases from predicates in a software specification. These methods derive various test conditions from logic expressions, with the aim of detecting different types of faults. Some authors have presented empirical results on the ability of specification-based test generation methods to detect failures. This article describes a method for cokmputing the conditions that must be covered by a test set for the test set to guarantee detection of the particular fault class. It is shown that there is a coverage hierarchy to fault classes that is consistent with, and may therefore explain, experimental results on fault-based testing. The method is also shown to be effective for computing MCDC-adequate  tests.", "num_citations": "211\n", "authors": ["300"]}
{"title": "Security standards for the RFID market\n", "abstract": " As the RFID market expands, we'll see the continued proliferation of RFID tags built for highly specialized vertical markets, which means greater variety and the consequent need to ensure interoperability. A great deal of research and development is currently under way in the RFID security field to mitigate both known and postulated risks. Manufacturers; business managers, and RFID systems engineers continue to weigh the trade-offs between chip size, cost, functionality, interoperability, security and privacy with the bottom-line impact on business processes. Security features supporting data confidentiality, tag-to-reader authentication, optimized RF protocols, high-assurance readers, and secure system engineering principles should become available. Security and privacy in RFID tags aren't just technical issues; important policy questions arise as RFID tags join to create large sensor networks and bring us closer\u00a0\u2026", "num_citations": "210\n", "authors": ["300"]}
{"title": "Pseudo-exhaustive testing for software\n", "abstract": " Pseudo-exhaustive testing uses the empirical observation that, for broad classes of software, a fault is likely triggered by only a few variables interacting. The method takes advantage of two relatively recent advances in software engineering: algorithms for efficiently generating covering arrays to represent software interaction test suites, and automated generation of test oracles using model checking. An experiment with a module of the traffic collision avoidance system (TCAS) illustrates the approach testing pairwise through 6-way interactions. We also outline current and future work applying the test methodology to a large real-world application, the personal identity verification (PIV) smart card", "num_citations": "198\n", "authors": ["300"]}
{"title": "Practical combinatorial testing\n", "abstract": " Software implementation errors are one of the most significant contributors to information system security vulnerabilities, making software testing an essential part of system assurance. In 2003 NIST published a widely cited report which estimated that inadequate software testing costs the US economy $59.5 billion per year, even though 50% to 80% of development budgets go toward testing. Exhaustive testing\u2013testing all possible combinations of inputs and execution paths\u2013is impossible for real-world software, so high assurance software is tested using methods that require extensive staff time and thus have enormous cost. For less critical software, budget constraints often limit the amount of testing that can be accomplished, increasing the risk of residual errors that lead to system failures and security weaknesses.Combinatorial testing is a method that can reduce cost and increase the effectiveness of software testing for many applications. The key insight underlying this form of testing is that not every parameter contributes to every failure and most failures are caused by interactions between relatively few parameters. Empirical data gathered by NIST and others suggest that software failures are triggered by only a few variables interacting (6 or fewer). This finding has important implications for testing because it suggests that testing combinations of parameters can provide highly effective fault detection. Pairwise (2-way combinations) testing is sometimes used to obtain reasonably good results at low cost, but pairwise testing may miss 10% to 40% or more of system bugs, and is thus not sufficient for mission-critical software. Combinatorial testing\u00a0\u2026", "num_citations": "187\n", "authors": ["300"]}
{"title": "Refining the in-parameter-order strategy for constructing covering arrays\n", "abstract": " Covering arrays are structures for well-representing extremely large input spaces and are used to efficiently implement blackbox testing for software and hardware. This paper proposes refinements over the In-Parameter-Order strategy (for arbitrary t). When constructing homogeneous-alphabet covering arrays, these refinements reduce runtime in nearly all cases by a factor of more than 5 and in some cases by factors as large as 280. This trend is increasing with the number of columns in the covering array. Moreover, the resulting covering arrays are about 5% smaller. Consequently, this new algorithm has constructed many covering arrays that are the smallest in the literature. A heuristic variant of the algorithm sometimes produces comparably sized covering arrays while running significantly faster.", "num_citations": "178\n", "authors": ["300"]}
{"title": "Introduction to public key technology and the federal PKI infrastructure\n", "abstract": " Public Key Infrastructures PKIs can speed up and simplify delivery of products and services by providing electronic approaches to processes that historically have been paper based. These electronic solutions depend on data integrity and authenticity.Descriptors:", "num_citations": "172\n", "authors": ["300"]}
{"title": "Combinatorial software testing\n", "abstract": " Combinatorial testing can detect hard-to-find software faults more efficiently than manual test case selection methods. While the most basic form of combinatorial testing-pairwise-is well established, and adoption by software testing practitioners continues to increase, industry usage of these methods remains patchy at best. However, the additional training required is well worth the effort.", "num_citations": "168\n", "authors": ["300"]}
{"title": "Acts: A combinatorial test generation tool\n", "abstract": " In this paper, we introduce a combinatorial test generation research tool called Advanced Combinatorial Testing System (or ACTS). ACTS supports t-way combinatorial test generation with several advanced features such as mixed-strength test generation and constraint handling. To facilitate its use and integration with other tools, ACTS provides three types of external interface, including a graphic user interface, a command line interface, and an application programming interface. ACTS is a freely distributed research tool and has been downloaded by more than 1200 companies and organizations.", "num_citations": "149\n", "authors": ["300"]}
{"title": "Role based access control for the world wide web\n", "abstract": " One of the most challenging problems in managing large networked systems is the complexity of security administration. This is particularly true for organizations that are attempting to manage security in distributed multimedia environments such as those using world Wide Web (WWW) servers. Today, security administration is costly and prone to error because administrators usually specify access control lists for each user on the system individually.Role based access control (RBAC) is a technology that is attracting increasing attention, particularly for commercial applications, because of its potential for reducing the complexity and cost of security administration in large networked applications. This paper describes software components that provide RBAC for networked servers using WWW protocols. The RBAC components can be linked with commercially available web servers, and require no modi cation of the server software.", "num_citations": "149\n", "authors": ["300"]}
{"title": "Challenges in securing voice over IP\n", "abstract": " Although VoIP offers lower cost and greater flexibility, it can also introduce significant risks and vulnerabilities. This article explains the challenges of VoIP security and outlines steps for helping to secure an organization's VoIP network.", "num_citations": "135\n", "authors": ["300"]}
{"title": "Data loss prevention\n", "abstract": " In today's digital economy, data enters and leaves cyberspace at record rates. A typical enterprise sends and receives millions of email messages and downloads, saves, and transfers thousands of files via various channels on a daily basis. Enterprises also hold sensitive data that customers, business partners, regulators, and shareholders expect them to protect. Unfortunately, companies constantly fall victim to massive data loss, and high-profile data leakages involving sensitive personal and corporate data continue to appear (http://opensecurityfoundation. org). Data loss could substantially harm a company's competitiveness and reputation and could also invite lawsuits or regulatory consequences for lax security. Therefore, organizations should take measures to understand the sensitive data they hold, how it's controlled, and how to prevent it from being leaked or compromised.", "num_citations": "128\n", "authors": ["300"]}
{"title": "RBAC Standard rationale: comments on a critique of the ANSI standard on Role-Based Access Control\n", "abstract": " \"For original paper see Ninghui Li et al., vol. 5, no. 6, p.41, (2007)\". Some notion of roles for access control predates the research papers cited by the authors by at least a decade. Our work was designed to formalize RBAC and add features (such as hierarchies and constraints) to make it more useful to software developers and administrators. Extensive discussion of these and subsequent papers over many years led to the consensus standard for RBAC.", "num_citations": "120\n", "authors": ["300"]}
{"title": "An efficient algorithm for constraint handling in combinatorial test generation\n", "abstract": " Combinatorial testing has been shown to be a very effective testing strategy. An important problem in combinatorial testing is dealing with constraints, i.e., restrictions that must be satisfied in order for a test to be valid. In this paper, we present an efficient algorithm, called IPOG-C, for constraint handling in combinatorial testing. Algorithm IPOG-C modifies an existing combinatorial test generation algorithm called IPOG to support constraints. The major contribution of algorithm IPOG-C is that it includes three optimizations to improve the performance of constraint handling. These optimizations can be generalized to other combinatorial test generation algorithms. We implemented algorithm IPOG-C in a combinatorial test generation tool called ACTS. We report experimental results that demonstrate the effectiveness of algorithm IPOG-C. The three optimizations increased the performance by one or two orders of\u00a0\u2026", "num_citations": "92\n", "authors": ["300"]}
{"title": "Combinatorial methods for event sequence testing\n", "abstract": " Many software testing problems involve sequences of events. This paper applies combinatorial methods to testing problems that have n distinct events, where each event occurs exactly once. The methods described in this paper were motivated by testing needs for systems that may accept multiple communication or sensor connections and generate output to several communication links and other interfaces, where it is important to test the order in which connections occur. Although pair wise event order testing (both A followed by B and B followed by A) has been described, our algorithm ensures that any t events will be tested in every possible t-way order.", "num_citations": "92\n", "authors": ["300"]}
{"title": "Combinatorial testing of ACTS: A case study\n", "abstract": " In this paper we present a case study of applying combinatorial testing to test a combinatorial test generation tool called ACTS. The purpose of this study is two-fold. First, we want to gain experience and insights about how to apply combinatorial testing in practice. Second, we want to evaluate the effectiveness of combinatorial testing applied to a real-life system. ACTS has 24637 lines of uncommented code, and provides a command line interface and a fairly sophisticated graphic user interface. The main challenge of this study was to model the input space in terms of a set of parameters and values. Once the model was designed, we generated test cases using ACTS, which were then later used to test ACTS. The results of this study show that input space modeling can be a significant undertaking, and needs to be carefully managed. The results also show that combinatorial testing is effective in terms of achieving\u00a0\u2026", "num_citations": "86\n", "authors": ["300"]}
{"title": "Extensible access control markup language (XACML) and next generation access control (NGAC)\n", "abstract": " Extensible Access Control Markup Language (XACML) and Next Generation Access Control (NGAC) are very different attribute based access control standards with similar goals and objectives. An objective of both is to provide a standardized way for expressing and enforcing vastly diverse access control policies in support of various types of data services. The two standards differ with respect to the manner in which access control policies and attributes are specified and managed, and decisions are computed and enforced. This paper is presented as a consolidation and refinement of public draft NIST SP 800-178 [21], describing, and comparing these two standards.", "num_citations": "82\n", "authors": ["300"]}
{"title": "Ar+3 photodissociation and its mechanisms\n", "abstract": " The photodissociation spectrum of Ar+3 between 520 and 620 nm is reported. A broadband peaking near 520 nm is observed with a cross section of \u223c1.8\u00d710\u221216 cm2 at the peak, in agreement with the findings of Levinger et al. [J. Chem. Phys. 89, 71 (1988)]. However, in the present work, a shoulder whose prominence is highly temperature dependent is observed between 545 and 555 nm. The new results are discussed in terms of various electronic transitions and pathways to dissociation. The photodissociation mechanisms are studied by translational energy analysis of photofragmentation. Three photodissociation mechanisms that involve two electronic energy surfaces are proposed which account for the experimental findings.", "num_citations": "80\n", "authors": ["300"]}
{"title": "Recommendations of the national institute of standards and technology\n", "abstract": " This document outlines the security components and security guidelines needed to establish a secure Basic Input/Output System (BIOS) integrity measurement and reporting chain. Unauthorized modification of BIOS firmware constitutes a significant threat because of the BIOS\u2019s unique and privileged position within the PC architecture. The document focuses on two scenarios: detecting changes to the system BIOS code stored on the system flash, and detecting changes to the system BIOS configuration. The document is intended for hardware and software vendors that develop products that can support secure BIOS integrity measurement mechanisms, and may also be of use for organizations developing enterprise procurement or deployment strategies for these technologies.", "num_citations": "77\n", "authors": ["300"]}
{"title": "A survey of binary covering arrays\n", "abstract": " Binary covering arrays of strength  are 0\u20131 matrices having the property that for each  columns and each of the possible  sequences of  0's and 1's, there exists a row having that sequence in that set of  columns. Covering arrays are an important tool in certain applications, for example, in software testing. In these applications, the number of columns of the matrix is dictated by the application, and it is desirable to have a covering array with a small number of rows. Here we survey some of what is known about the existence of binary covering arrays and methods of producing them, including both explicit constructions and search techniques.", "num_citations": "66\n", "authors": ["300"]}
{"title": "Photodissociation of rare gas cluster ions: Ar+3\n", "abstract": " The photodissociation spectrum of the mass selected cluster ion Ar+3, produced by electron impact ionization of a supersonic cluster beam, has been measured for the first time. Measurements over the wavelength range 539 to 620 nm show the cross section to rise and peak near 545 nm and then decrease steadily throughout the longer wavelength range; the magnitude of the cross section is found to be smaller than the corresponding 2\u03a3+u\u21922\u03a3+g transition of Ar+2 in agreement with some theoretical predictions. Interestingly, Ar+ is the only detected photoproduct.", "num_citations": "65\n", "authors": ["300"]}
{"title": "A combinatorial testing strategy for concurrent programs\n", "abstract": " One approach to testing concurrent programs is called reachability testing, which derives test sequences automatically and on\u2010the\u2010fly, without constructing a static model. Existing reachability testing algorithms are exhaustive in that they are intended to exercise all possible synchronization sequences of a concurrent program with a given input. In this paper, we present a new testing strategy, called t\u2010way reachability testing, that adopts the dynamic framework of reachability testing but selectively exercises a subset of synchronization sequences. The selection of the synchronization sequences is based on a combinatorial testing strategy called t\u2010way testing. We present an algorithm that implements t\u2010way reachability testing, and report the results of several case studies that were conducted to evaluate its effectiveness. The results indicate that t\u2010way reachability testing can substantially reduce the number of\u00a0\u2026", "num_citations": "61\n", "authors": ["300"]}
{"title": "Role based access control on MLS systems without kernel changes\n", "abstract": " Role based access control (RBAC) is attracting increasing attention as a security mechanism for both commercial and many military systems. This paper shows how RBAC can be implemented using the mechanisms available on traditional multi-level security systems that implement information flow policies. The construction from MLS to RBAC systems is significant because it shows that the enormous investment in MLS systems can be leveraged to produce RBAC systems. The method requires no changes to the existing MLS system kernel and allows implementation of hierarchical RBAC entirely through site configuration options. A single trusted process is used to map privileges of RBAC roles to MLS labels. Access is then mediated by the MLS kernel. Where C is the number of categories and d the depth of the role hierarchy, the number of roles that can be controlled is approximately C/d d", "num_citations": "59\n", "authors": ["300"]}
{"title": "Combinatorial testing for software: An adaptation of design of experiments\n", "abstract": " Software has become increasingly ubiquitous in tools and methods used for science, engineering, medicine, commerce, and human interactions. Extensive testing is required to assure that software works correctly. Combinatorial testing is a versatile methodology which is useful in a broad range of situations to detect faults in software. It is based on the insight that while the behavior of a software system may be affected by a large number of factors, only a few factors are involved in a failure-inducing fault. We discuss the development of combinatorial testing for software as adaptation of design of experiment methods. Combinatorial testing began as pairwise testing in which first orthogonal arrays and then covering arrays were used to make sure that all pairs of the test settings were tested. Subsequent investigations of actual software failures showed that pairwise (2-way) testing may not always be sufficient and\u00a0\u2026", "num_citations": "55\n", "authors": ["300"]}
{"title": "A combinatorial approach to building navigation graphs for dynamic web applications\n", "abstract": " Modeling the navigation structure of a dynamic Web application is a challenging task because of the presence of dynamic pages. In particular, there are two problems to be dealt with: (1) the page explosion problem, i.e., the number of dynamic pages may be huge or even infinite; and (2) the request generation problem, i.e., many dynamic pages may not be reached unless appropriate user requests are supplied. As a user request typically consists of multiple parameter values, the request generation problem can be further divided into two problems: (1) How to select appropriate values for individual parameters? (2) How to effectively combine individual parameter values to generate requests? This paper presents a combinatorial approach to building a navigation graph. The novelty of our approach is two-fold. First, we use an abstraction scheme to control the page explosion problem. In this scheme, pages that are\u00a0\u2026", "num_citations": "54\n", "authors": ["300"]}
{"title": "Managing security: The security content automation protocol\n", "abstract": " Managing information systems security is an expensive and challenging task. Many different and complex software components- including firmware, operating systems, and applications-must be configured securely, patched when needed, and continuously monitored for security. Most organizations have an extensive set of security requirements. For commercial firms, such requirements are established through complex interactions of business goals, government regulations, and insurance requirements; for government organizations, security requirements are mandated. Meeting these requirements has been time consuming and error prone, because organizations have lacked standardized, automated ways of performing the tasks and reporting on results. To overcome these deficiencies and reduce security administration costs, the National Institute of Standards and Technology developed the security content\u00a0\u2026", "num_citations": "53\n", "authors": ["300"]}
{"title": "Composing and combining policies under the policy machine\n", "abstract": " As a major component of any host, or network operating system, access control mechanisms come in a wide variety of forms, each with their individual attributes, functions, methods for configuring policy, and a tight coupling to a class of policies. To afford generalized protection, NIST has initiated a project in pursuit of a standardized access control mechanism, referred to as the Policy Machine (PM) that requires changes only in its configuration in the enforcement of arbitrary and organization specific attribute-based access control policies. Included among the PM's enforceable policies are combinations of policy instances (eg, Role-Based Access Control and Multi-Level Security). In our effort to devise a generic access control mechanism, we construct the PM in terms of what we believe to be abstractions, properties and functions that are fundamental to policy configuration and enforcement. In its protection of objects\u00a0\u2026", "num_citations": "53\n", "authors": ["300"]}
{"title": "Combinatorial testing: Theory and practice\n", "abstract": " Combinatorial testing has rapidly gained favor among software testers in the past decade as improved algorithms have become available and practical success has been demonstrated. This chapter reviews the theory and application of this method, focusing particularly on research since 2010, with a brief background providing the rationale and development of combinatorial methods for software testing. Significant advances have occurred in algorithm performance, and the critical area of constraint representation and processing. In addition to these foundational topics, we take a look at advances in specialized areas including test suite prioritization, sequence testing, fault localization, the relationship between combinatorial testing and structural coverage, and approaches to very large testing problems.", "num_citations": "52\n", "authors": ["300"]}
{"title": "Combinatorial coverage measurement concepts and applications\n", "abstract": " Empirical data demonstrate the value of t-way coverage, but in some testing situations, it is not practical to use covering arrays. However any set of tests covers at least some proportion of t-way combinations. This paper describes a variety of measures of combinatorial coverage that can be used in evaluating aspects of t-way coverage of a test suite. We also provide lower bounds on t-way coverage of several widely-used testing strategies, and describe a tool that analyzes test suites using the measures discussed in the paper.", "num_citations": "52\n", "authors": ["300"]}
{"title": "Cryptographic hash standards: Where do we go from here?\n", "abstract": " Successful attacks against the two most commonly used cryptographic hash functions, MD5 and SHA-1, have triggered a kind of feeding frenzy in the cryptographic community. Many researchers are now working on hash function attacks, and we can expect new results in this area for the next several years. This article discusses the SHA-1 attack and the US National Institute of Standards and Technology's (NIST's) plans for SHA-1 and hash functions in general", "num_citations": "44\n", "authors": ["300"]}
{"title": "Study of BGP peering session attacks and their impacts on routing performance\n", "abstract": " We present a detailed study of the potential impact of border gateway protocol peering session attacks and the resulting exploitation of route flap damping (RFD) that cause network-wide routing disruptions. We consider canonical grid as well as down-sampled realistic autonomous system (AS) topologies and address the impact of various typical service provider routing policies. Our modeling focuses on three dimensions of routing performance sensitivity: 1) protocol aware attacks (e.g., tuned to RFD); 2) route selection policy; and 3) attack-region topology. Analytical results provide insights into the nature of the problem and potential impact of the attacks. Detailed packet-level simulation results complement the analytical models and provide many additional insights into specific protocol interactions and timing issues. Finally, we quantify the potential effect of the BGP graceful restart mechanism as a partial mitigation\u00a0\u2026", "num_citations": "43\n", "authors": ["300"]}
{"title": "An access control scheme for big data processing\n", "abstract": " Access Control (AC) systems are among the most critical of network security components. A system's privacy and security controls are more likely to be compromised due to the misconfiguration of access control policies rather than the failure of cryptographic primitives or protocols. This problem becomes increasingly severe as software systems become more and more complex, such as Big Data (BD) processing systems, which are deployed to manage a large amount of sensitive information and resources organized into a sophisticated BD processing cluster. Basically, BD access control requires the collaboration among cooperating processing domains to be protected as computing environments that consist of computing units under distributed AC managements. Many BD architecture designs were proposed to address BD challenges; however, most of them were focused on the processing capabilities of the\u00a0\u2026", "num_citations": "42\n", "authors": ["300"]}
{"title": "The role of rotational tunneling in the metastable decay of rare gas cluster ions\n", "abstract": " The decay of (Ar+3)*\u2192Ar+2+Ar is found to be metastable on a time scale longer than 40 \u03bcs. Theoretical considerations are presented for these slow metastable dissociation rates reported in this and other studies. From model calculations it is inferred that tunneling lifetimes from 10\u221210 to 10 s may be observed.", "num_citations": "42\n", "authors": ["300"]}
{"title": "Constraint handling in combinatorial test generation using forbidden tuples\n", "abstract": " Constraint handling is a challenging problem in combinatorial test generation. In general, there are two ways to handle constraints, i.e., constraint solving and forbidden tuples. In our earlier work, we proposed a constraint handling approach based on forbidden tuples for software product line systems consisting of only Boolean parameters. In this paper, we generalize this approach for general software systems that may consist of other types of parameter. The key idea of our approach is using the notion of minimum forbidden tuples to perform validity checks on both complete and partial tests. Furthermore, we propose an on-demand strategy that only generates minimum forbidden tuples for validity checks as they are encountered, instead of generating all of them up front. We implemented our generalized approach with and without the on-demand strategy in our combinatorial testing tool called ACTS. We performed\u00a0\u2026", "num_citations": "41\n", "authors": ["300"]}
{"title": "An empirical comparison of combinatorial and random testing\n", "abstract": " Some conflicting results have been reported on the comparison between t-way combinatorial testing and random testing. In this paper, we report a new study that applies t-way and random testing to the Siemens suite. In particular, we investigate the stability of the two techniques. We measure both code coverage and fault detection effectiveness. Each program in the Siemens suite has a number of faulty versions. In addition, mutation faults are used to better evaluate fault detection effectiveness in terms of both number and diversity of faults. The experimental results show that in most cases, t-way testing performed as good as or better than random testing. There are few cases where random testing performed better, but with a very small margin. Overall, the differences between the two techniques are not as significant as one would have probably expected. We discuss the practical implications of the results. We\u00a0\u2026", "num_citations": "39\n", "authors": ["300"]}
{"title": "Property verification for generic access control models\n", "abstract": " To formally and precisely capture the security properties that access control should adhere to, access control models are usually written to bridge the rather wide gap in abstraction between policies and mechanisms. In this paper, we propose a new general approach for property verification for access control models. The approach defines a standardized structure for access control models, providing for both property verification and automated generation of test cases. The approach expresses access control models in the specification language of a model checker and expresses generic access control properties in the property language. Then the approach uses the model checker to verify these properties for the access control models and generates test cases via combinatorial covering array for the system implementations of the models.", "num_citations": "38\n", "authors": ["300"]}
{"title": "Cost effective uses of formal methods in verification and validation\n", "abstract": " Formal methods offer the promise of significant improvements in verification and validation, and may be the only approach capable of demonstrating the absence of undesirable system behavior. But it is widely recognized that these methods are expensive, and their use has been limited largely to high-risk areas such as security and safety. This paper focuses on cost-effective applications of formal techniques in V&V, particularly recent developments such as automatic test generation and use of formal methods for analyzing requirements and conceptual models without a full-blown formal verification. We also discuss experience with requiring the use of formal techniques in standards for commercial software.", "num_citations": "37\n", "authors": ["300"]}
{"title": "Introducing combinatorial testing in a large organization\n", "abstract": " A two-year study of eight pilot projects to introduce combinatorial testing in a large aerospace corporation found that the new methods were practical, significantly lowered development costs, and improved test coverage by 20 to 50 percent.", "num_citations": "36\n", "authors": ["300"]}
{"title": "Formal representation and analysis of batch stock trading systems by logical Petri net workflows\n", "abstract": " This paper focuses on mitigating efficiently the problem of state explosion in Petri net models. A new modeling and analyzing method, logical Petri net workflows (LPNW), of the real-time systems with batch data process procedures is presented based on Petri net and workflow techniques. The use of LPNWs is illustrated by a useful example of a batch stock trading system. The properties and functional correctness of the modeled system are analyzed and verified formally on the basis of temporal logic. It has been sufficiently shown that this approach can avoid the problem of state explosion to a certain extent. Finally, further research work is proposed.", "num_citations": "36\n", "authors": ["300"]}
{"title": "Lessons from 342 medical device failures\n", "abstract": " Most complex systems today contain software, and systems failures activated by software faults can provide lessons for software development practices and software quality assurance. This paper presents an analysis of software-related failures of medical devices that caused no death or injury but led to recalls by the manufacturers. The analysis categorizes the failures by their symptoms and faults, and discusses methods of preventing and detecting faults in each category. The nature of the faults provides lessons about the value of generally accepted quality practices for prevention and detection methods applied prior to system release. It also provides some insight into the need for formal requirements specification and for improved testing of complex hardware-software systems.", "num_citations": "36\n", "authors": ["300"]}
{"title": "Challenges in securing the domain name system\n", "abstract": " Two main security threats exist for DNS in the context of query/response transactions. Attackers can spoof authoritative name servers responding to DNS queries and alter DNS responses in transit through man-in-the-middle attacks, and alter the DNS responses stored in caching name servers. The IETF has defined the digital signature-based DNSSEC for protecting DNS query/response transactions through a series of requests for comments.", "num_citations": "35\n", "authors": ["300"]}
{"title": "On the effective use of software standards in systems integration\n", "abstract": " A set of standards for an open systems environment is presented, and an approach to the use of these standards in systems integration is defined. In particular, the approach helps to deal with three aspects of software standards that affect systems integration: periodic revision, missing features that result in the use of proprietary system services, and imprecise, natural language specification. The architectural approach is consistent with the toolkit model of systems development that has been popularized by window systems, and takes advantage of the features provided by many window systems for building user-defined components.< >", "num_citations": "35\n", "authors": ["300"]}
{"title": "Finding bugs in cryptographic hash function implementations\n", "abstract": " Cryptographic hash functions are security-critical algorithms with many practical applications, notably in digital signatures. Developing an approach to test them can be particularly difficult, and bugs can remain unnoticed for many years. We revisit the National Institute of Standards and Technology hash function competition, which was used to develop the SHA-3 standard, and apply a new testing strategy to all available reference implementations. Motivated by the cryptographic properties that a hash function should satisfy, we develop four tests. The Bit-Contribution Test checks if changes in the message affect the hash value, and the Bit-Exclusion Test checks that changes beyond the last message bit leave the hash value unchanged. We develop the Update Test to verify that messages are processed correctly in chunks, and then use combinatorial testing methods to reduce the test set size by several orders of\u00a0\u2026", "num_citations": "34\n", "authors": ["300"]}
{"title": "A method for analyzing system state-space coverage within a t-wise testing framework\n", "abstract": " Inadequate state-space coverage of complex configurable systems during test phases is an area of concern for systems engineers. Determining the state-space coverage of a proposed or executed test suite traditionally involves qualitative assessment, rendering meaningful comparative analysis between tests for a given system or across multiple systems difficult. We propose a method for assessing state-space coverage of a test suite utilizing t-wise testing, a combinatorial technique borrowed from the software testing community which generalizes pair-wise testing. We refine traditional notions of a t-wise test suite to analyze the configuration coverage of a test plan. This provides a methodology and a set of metrics to assess both the level and the distribution of state-space coverage. We detail a proof-of-concept experiment using this partial t-wise coverage framework to analyze Integration and Test (I&T) data from\u00a0\u2026", "num_citations": "34\n", "authors": ["300"]}
{"title": "Applying combinatorial testing to the siemens suite\n", "abstract": " Combinatorial testing has attracted a lot of attention from both industry and academia. A number of reports suggest that combinatorial testing can be effective for practical applications. However, there are few systematic, controlled studies on the effectiveness of combinatorial testing. In particular, input parameter modeling is a key step in the combinatorial testing process. But most studies do not report the details of the modeling process. In this paper, we report an experiment that applies combinatorial testing to the Siemens suite. The Siemens suite has been used as a benchmark to evaluate the effectiveness of many testing techniques. Each program in the suite has a number of faulty versions. The effectiveness of combinatorial testing is measured in terms of the number of faulty versions that are detected. The experimental results show that combinatorial testing is effective in terms of detecting most of the faulty\u00a0\u2026", "num_citations": "32\n", "authors": ["300"]}
{"title": "Border gateway protocol security\n", "abstract": " Although not well known among everyday users, the Border Gateway Protocol (BGP) is one of the critical infrastructure protocols for the Internet. BGP is a routing protocol, whose purpose is to keep systems on the Internet up to date with information needed to receive and transmit traffic correctly. Sending and receiving email, viewing Web sites, and performing other Internet activities require the transmission of messages referred to as packets. Packets sent on the Internet contain source and destination addresses, much like paper mail sent in envelopes. But packets do not go directly from a user\u2019s computer to their destination. Many intermediate systems may be involved in the transmission, and because there are many paths from one point to another, not all packets follow the same path between source and destination. The systems that packets pass through from one point to another all need to know where to forward a packet, based on the destination address and information contained in a routing table. The routing table says, for example, that packets with a destination of A can be sent to system H, which will then forward the packets to their destination, possibly through other intermediate nodes.(Note that the terms \u201crouting table\u201d and \u201cforwarding table\u201d are often used interchangeably, although technically the forwarding table is used to determine where packets will be sent. More on the distinction between these tables can be found in Section 2.1.) Because the Internet changes continuously, as systems fail or are replaced or new systems are added, routing tables must be updated constantly. BGP is the protocol that serves this purpose for the\u00a0\u2026", "num_citations": "32\n", "authors": ["300"]}
{"title": "Security in open systems\n", "abstract": " The Public Switched Network (PSN) provides National Security and Emergency Preparedness (NS/EP) telecommunications. Service vendors, equipment manufacturers, and the federal government are concerned that vulnerabilities in the PSN could be exploited and result in disruptions or degradation of service. To address these threats, NIST is assisting the Office of the Manager, National Communications System (OMNCS), in the areas of computer and network security research and development. NIST is investigating the vulnerabilities and related security issues that result from the use of open systems platforms, ie, products based on open standards such as POSIX and OSI, in the telecommunications industry. This report is intended to provide information for the practicing programmer involved in development of telecommunications application software. In short, it provides answers to the question,\" How do I\u00a0\u2026", "num_citations": "29\n", "authors": ["300"]}
{"title": "Fault localization based on failure-inducing combinations\n", "abstract": " Combinatorial testing has been shown to be a very effective testing strategy. After a failure is detected, the next task is to identify the fault that causes the failure. In this paper, we present an approach to fault localization that leverages the result of combinatorial testing. Our approach is based on a notion called failure-inducing combinations. A combination is failure-inducing if it causes any test in which it appears to fail. Given a failure-inducing combination, our approach derives a group of tests that are likely to exercise similar traces but produce different outcomes. These tests are then analyzed to locate the faults. We conducted an experiment in which our approach was applied to the Siemens suite as well as the grep program from the SIR repository that has 10068 lines of code. The experimental results show that our approach can effectively and efficiently localize the faults in these programs.", "num_citations": "26\n", "authors": ["300"]}
{"title": "A combinatorial approach to detecting buffer overflow vulnerabilities\n", "abstract": " Buffer overflow vulnerabilities are program defects that can cause a buffer to overflow at runtime. Many security attacks exploit buffer overflow vulnerabilities to compromise critical data structures. In this paper, we present a black-box testing approach to detecting buffer overflow vulnerabilities. Our approach is motivated by a reflection on how buffer overflow vulnerabilities are exploited in practice. In most cases the attacker can influence the behavior of a target system only by controlling its external parameters. Therefore, launching a successful attack often amounts to a clever way of tweaking the values of external parameters. We simulate the process performed by the attacker, but in a more systematic manner. A novel aspect of our approach is that it adapts a general software testing technique called combinatorial testing to the domain of security testing. In particular, our approach exploits the fact that combinatorial\u00a0\u2026", "num_citations": "26\n", "authors": ["300"]}
{"title": "BEN: A combinatorial testing-based fault localization tool\n", "abstract": " We present a combinatorial testing-based fault localization tool called BEN. BEN takes as input three types of information, including the subject program, the source code, an input parameter model, and a combinatorial test set created based on the input parameter model. It is assumed that the combinatorial test set has already been executed, and thus the execution status of each test is known. The output of BEN is a ranking of statements in terms of their likelihood to be faulty. In the fault localization process, a small number of additional tests are generated by BEN and need to be executed by the user. In this paper, we present the major user scenarios and the highlevel design of BEN. BEN is implemented in Java and provides a graphical user interface that provides friendly access to the tool.", "num_citations": "25\n", "authors": ["300"]}
{"title": "Combinatorial testing\n", "abstract": " Software systems today are complex and have many possible configurations. Products released with inadequate testing can cause bodily harm, result in large economic losses or security breaches, and affect the quality of day-to-day life. Software testers have limited time and budgets, frequently making it impossible to exhaustively test software. Testers often intuitively test for defects that they anticipate while less foreseen defects are overlooked. Combinatorial testing can complement their tests by systematically covering t-way interactions. Research in combinatorial testing includes two major areas (1) algorithms that generate combinatorial test suites and (2) applications of combinatorial testing. The authors review these two topics in this chapter.", "num_citations": "25\n", "authors": ["300"]}
{"title": "Role-based access control for the Web\n", "abstract": " Establishing and maintaining a presence on the World Wide Web (Web), once a sideline for US industry, has become a key strategic aspect of marketing and sales. Many companies have demonstrated that a well designed Web site can have a positive effect on their profitability. Enabling customers to answer their own questions by clicking their way through Web pages, instead of dealing with operators and voice response systems, increases the efficiency of the customer interface. One of the most challenging problems in managing large networked systems is the complexity of security administration. This is particularly true for organizations that are attempting to manage security in distributed multimedia environments such as those using World Wide Web services. Today, security administration is costly and prone to error because administrators usually specify access control lists for each user on the system\u00a0\u2026", "num_citations": "25\n", "authors": ["300"]}
{"title": "An analysis of selected software safety standards\n", "abstract": " Standards, draft standards, and guidelines that provide requirements for the assurance of high integrity software are studied. The focus is on identifying the attributes necessary in such documents for providing reasonable assurance for high integrity software, and on identifying the relative strengths and weaknesses of the documents. The documents vary widely in their requirements and the precision with which the requirements are expressed. Security documents tend to have a narrow focus and to be more product oriented, while safety documents tend to be broad in scope and center primarily on the software development process. It is found that overall there is little relationship between the degree of risk and the rigor of applicable standards. Recommendations are provided for a base standard for the assurance of high integrity software.< >", "num_citations": "25\n", "authors": ["300"]}
{"title": "IEEE's Posix: making progress\n", "abstract": " The status of the interface standards within the IEEE Portable Operating System Interface (Posix) environment (the x in Posix denotes the Unix operating system origin of this effort) is discussed. The open system standards within the Posix activity are expected to resolve the portability and interoperability problems and to open the door to a major new industry of standard software components, or modules. From these components, users will be able to build and modify larger systems to suit their evolving needs, eliminating the need to produce several versions of an application program to accommodate operating systems with different file system structures and network interfaces. Details of the Posix Open Systems Environment (OSE) application program interfaces are examined to show how standards can be used in constructing portable software.< >", "num_citations": "25\n", "authors": ["300"]}
{"title": "An input space modeling methodology for combinatorial testing\n", "abstract": " The input space of a system must be modeled before combinatorial testing can be applied to this system. The effectiveness of combinatorial testing to a large extent depends on the quality of the input space model. In this paper we introduce an input space modeling methodology for combinatorial testing. The main idea is to consider the process of input space modeling as two steps, namely, input structure modeling and input parameter modeling. The first step tries to capture the structural relationship among different components in the input space. The second step tries to identify parameters, values, relations and constraints for individual components. We also suggest strategies about how to perform unit and integration testing based on the input space structure. We applied the proposed methodology to two real-life programs. We report our experience and results that demonstrate the effectiveness of the proposed\u00a0\u2026", "num_citations": "24\n", "authors": ["300"]}
{"title": "A technique for analyzing the effects of changes in formal specifications\n", "abstract": " Formal specifications are increasingly used in modeling software systems. An important aspect of a model is its value as an analytical tool to investigate the effect of changes. This paper defines the notion of predicate differences and shows how predicate differences may be used to analyze the effects of changes in formal specifications. Predicate differences have both theoretical and practical applications. As a theoretical tool, predicate differences may be used to define a meaning for the \u2018size\u201d of a change to a formal specification. Practical applications include analyzing the effect of design changes on a previously verified design; defining an affinity function for reusable software components; computing slices of formal specifications, similar to program slices; investigating the conditions under which invalid assumptions will render a system non-secure; and formalizing the database inference problem.", "num_citations": "24\n", "authors": ["300"]}
{"title": "Combinatorial Methods in Security Testing.\n", "abstract": " Many software security vulnerabilities result from the exploitation of ordinary coding flaws, rather than design or configuration errors. One study found that 64% of vulnerabilities are the result of such common bugs as missing or incorrect parameter checking, leaving the application open to common vulnerabilities including buffer overflows or SQL injection [1]. While this statistic may be discouraging, it also means that better functionality testing can have the additional benefit of significantly improved security.Testing that can reveal complex faults that occur only under rare conditions may be especially effective. Empirical data show that most failures are triggered by a single parameter value, or interactions between a small number of parameters, generally two to six, a relationship known as the interaction rule [2]. An example of a single-value fault might be a buffer overflow that occurs when the length of an input string exceeds a particular limit. Only a single condition must be true to trigger the fault: input length> buffer size. A 2-way fault is more complex, because two particular input values are needed to trigger the fault. One example is a search/replace function that only fails if both the search string and the replacement string are single characters. If one of the strings is longer than one character, the code does not fail, thus we refer to this as a 2-way fault. More generally, a t-way fault involves t such conditions.", "num_citations": "22\n", "authors": ["300"]}
{"title": "Vetting Mobile Apps\n", "abstract": " Billions of copies of apps for mobile devices have been purchased in recent years. With this growth, however, comes an increase in the spread of potentially dangerous security vulnerabilities. Because of an app's low cost and high proliferation, the threat of these vulnerabilities could be far greater than that of traditional computers. Thus, purchasing organizations or third-party labs should vet the apps before selling them, and consumers need to understand the risks of apps and the prospects for ensuring their security.", "num_citations": "22\n", "authors": ["300"]}
{"title": "NIST spectrally tunable lighting facility for color rendering and lighting experiments\n", "abstract": " The National Institute of Standards and Technology has developed a spectrally tunable lighting facility to allow state-of-the-art vision experiments on color rendering and lighting. The facility is composed of two cubicles, each lit by a spectrally tunable light source that contains 1800 high power light-emitting diodes comprising 22 color channels that are used to simulate traditional and new lighting spectra. Human observers can be completely immersed in the lighting environment and completely adapted to allow for the evaluation of the color rendering of objects, including human faces and skin tones, in a real-life setting.", "num_citations": "22\n", "authors": ["300"]}
{"title": "A comparison of attribute based access control (ABAC) standards for data service applications\n", "abstract": " Extensible Access Control Markup Language (XACML) and Next Generation Access Control (NGAC) are very different attribute based access control (ABAC) standards with similar goals and objectives. An objective of both is to provide a standardized way for expressing and enforcing vastly diverse access control policies on various types of data services. However, the two standards differ with respect to the manner in which access control policies are specified and implemented. This document describes XACML and NGAC, and then compares them with respect to five criteria. The goal of this publication is to help ABAC users and vendors make informed decisions when addressing future data service policy enforcement requirements.", "num_citations": "21\n", "authors": ["300"]}
{"title": "Combinatorial test generation for software product lines using minimum invalid tuples\n", "abstract": " A software product line is a set of software systems that share some common features. Several recent works have been reported that apply combinatorial testing, a very effective testing strategy, to software product lines. A unique challenge in these efforts is dealing with a potentially large number of constraints among different features. In this paper, we propose a novel constraint-handling strategy that uses minimum invalid tuples (MITs) as an alternative to traditional constraint solvers. Our approach systematically derives all MITs from a software product line, and uses them to quickly determine the validity of a test configuration during test generation. We implemented a test generation research tool called LOOKUP that integrates the proposed constraint-handling strategy with a general test generation algorithm called IPOG-C. Experimental results show that LOOKUP performs considerably better than two existing test\u00a0\u2026", "num_citations": "20\n", "authors": ["300"]}
{"title": "Sp 800-142. practical combinatorial testing\n", "abstract": " Combinatorial testing can help detect problems like this early in the testing life cycle. The key insight underlying t-way combinatorial testing is that not every parameter contributes to every fault and most faults are caused by interactions between a relatively small number of parameters. This publication provides a self-contained tutorial on using combinatorial testing for real-world software, including how to use it effectively for system and software assurance. It introduces the key concepts and methods, explains use of software tools for generating combinatorial tests (freely available on the NIST web site csrc.nist.gov/acts), and discusses advanced topics such as the use of formal models of software to determine the expected results for each set of test inputs. With each topic, a section on costs and practical considerations explains tradeoffs and limitations that may impact resources or funding. The material is accessible\u00a0\u2026", "num_citations": "20\n", "authors": ["300"]}
{"title": "A hybrid authentication protocol using quantum entanglement and symmetric cryptography\n", "abstract": " This paper presents a hybrid cryptographic protocol, using quantum and classical resources, for authentication and authorization in a network. One or more trusted servers distribute streams of entangled photons to individual resources that seek to communicate. It is assumed that each resource shares a previously distributed secret key with the trusted server, and that resources can communicate with the server using both classical and quantum channels. Resources do not share secret keys with each other, so that the key distribution problem for the network is reduced from O(n^2) to O(n). Some advantages of the protocol are that it avoids the requirement for timestamps used in classical protocols, guarantees that the trusted server cannot know the authentication key, can provide resistance to multiple photon splitting attacks and can be used with BB84 or other quantum key distribution protocols.", "num_citations": "20\n", "authors": ["300"]}
{"title": "Quantum computing and communication\n", "abstract": " A quantum computer, if built, will be to an ordinary computer as a hydrogen bomb is to gunpowder, at least for some types of computations. Today no quantum computer exists, beyond laboratory prototypes capable of solving only tiny problems, and many practical problems remain to be solved. Yet the theory of quantum computing has advanced significantly in the past decade, and is becoming a significant discipline in itself. This article explains the concepts and basic mathematics behind quantum computers and some of the promising approaches for building them. We also discuss quantum communication, an essential component of future quantum information processing, and quantum cryptography, widely expected to be the first practical application for quantum information technology.", "num_citations": "20\n", "authors": ["300"]}
{"title": "High integrity software standards and guidelines\n", "abstract": " This report presents results of a study of standards, draft standards, and guidelines (all of which will hereafter be referred to as documents) that provide requirements for the assurance of software in safety systems in nuclear power plants. The study focused on identifying the attributes necessary in a standard for providing reasonable assurance for software in nuclear systems. The study addressed some issues involved in demonstrating conformance to a standard. The documents vary widely in their requirements and the precision with which the requirements are expressed. Recommendations are provided for guidance addressing the assurance of high integrity software. It is recommended that a nuclear industry standard be developed based on the documents reviewed in this study with additional attention to the concerns identified in this report.", "num_citations": "20\n", "authors": ["300"]}
{"title": "Measuring and specifying combinatorial coverage of test input configurations\n", "abstract": " A key issue in testing is how many tests are needed for a required level of coverage or fault detection. Estimates are often based on error rates in initial testing, or on code coverage. For example, tests may be run until a desired level of statement or branch coverage is achieved. Combinatorial methods present an opportunity for a different approach to estimating required test set size, using characteristics of the test set. This paper describes methods for estimating the coverage of, and ability to detect, t-way interaction faults of a test set based on a covering array. We also develop a connection between (static) combinatorial coverage and (dynamic) code coverage, such that if a specific condition is satisfied, 100\u00a0% branch coverage is assured. Using these results, we propose practical recommendations for using combinatorial coverage in specifying test requirements, and for improving estimates of the fault\u00a0\u2026", "num_citations": "19\n", "authors": ["300"]}
{"title": "Random vs. combinatorial methods for discrete event simulation of a grid computer network\n", "abstract": " This study compared random and t-way combinatorial inputs of a network simulator, to determine if these two approaches produce significantly different deadlock detection for varying network configurations. Modeling deadlock detection is important for analyzing configuration changes that could inadvertently degrade network operations, or to determine modifications that could be made by attackers to deliberately induce deadlock. Discrete event simulation of a network may be conducted using random generation of inputs. In this study, we compare random with combinatorial generation of inputs. Combinatorial (or t-way) testing requires every combination of any t parameter values to be covered by at least one test. Combinatorial methods can be highly effective because empirical data suggest that nearly all failures involve the interaction of a small number of parameters (1 to 6). Thus, for example, if all deadlocks involve at most 5-way interactions between n parameters, then exhaustive testing of all n-way interactions adds no additional information that would not be obtained by testing all 5-way interactions. While the maximum degree of interaction between parameters involved in the deadlocks clearly cannot be known in advance, covering all t-way interactions may be more efficient than using random generation of inputs. In this study we tested this hypothesis for t= 2, 3, and 4 for deadlock detection in a network simulation. Achieving the same degree of coverage provided by 4-way tests would have required approximately 3.2 times as many random tests; thus combinatorial methods were more efficient for detecting deadlocks involving a higher\u00a0\u2026", "num_citations": "19\n", "authors": ["300"]}
{"title": "A source code analyzer for maintenance\n", "abstract": " This paper describes a tool that reads all C source files in a directory and produces information useful for program maintenance. The tool generates a call tree, a call matrix, and the transitive closure of the matrix, which shows indirect relationships between rou-tines. It computes some measures that may help estimate the complexity of the program being maintained, and also identifies subsystems (possibly nested) within the program. This paper describes the information pro-vided and shows how it saves time in under-standing the program to be modified, estimating the complexity of the change, and performing regression testing on the modified program. The tool is in the public domain and will be available through the National Technical Information Service (NTIS). tree, but if a program contains many routines the call tree may be spread across several pages of paper, making it awkward to analyze. The tool presented in this paper simplifies the analysis of these relationships. It also computes measures that may be useful as\" maintenance metrics\u201d, and identifies subsystems that are independent of other routines in the program. This paper gives examples of the information provided and shows how it can aid in understanding the pro gram to be modified, estimating the complexity of the change, and performing regression testing on the modified program.", "num_citations": "19\n", "authors": ["300"]}
{"title": "Internet of Things (IoT) Trust Concerns\n", "abstract": " The Internet of Things (IoT) refers to systems that involve computation, sensing, communication, 81 and actuation (as presented in NIST Special Publication (SP) 800-183). IoT involves the 82 connection between humans, non-human physical objects, and cyber objects, enabling monitoring, 83 automation, and decision making. The connection is complex and inherits a core set of trust 84 concerns, most of which have no current resolution This publication identifies 17 technical trust-85 related concerns for individuals and organizations before and after IoT adoption. The set of 86 concerns discussed here is necessarily incomplete given this rapidly changing industry, however 87 this publication should still leave readers with a broader understanding of the topic. This set was 88 derived from the six trustworthiness elements in NIST SP 800-183. And when possible, this 89 publication outlines recommendations for how to mitigate or reduce the effects of these IoT 90 concerns. It also recommends new areas of IoT research and study. This publication is intended 91 for a general information technology audience including managers, supervisors, technical staff, 92 and those involved in IoT policy decisions, governance, and procurement. 93", "num_citations": "18\n", "authors": ["300"]}
{"title": "Rethinking distributed ledger technology\n", "abstract": " Distributed ledger technology (DLT) offers new and unique advantages for information systems, but some of its features are not a good fit for many applications. We review the properties of DLT and show how two recently developed ideas can be used to retain its advantages while simplifying design.", "num_citations": "17\n", "authors": ["300"]}
{"title": "Surviving insecure it: Effective patch management\n", "abstract": " The amount of time to protect enterprise systems against potential vulnerability continues to shrink. Enterprises need an effective patch management mechanism to survive the insecure IT environment. Effective patch management is a systematic and repeatable patch distribution process which includes establishing timely and practical alerts, receiving notification of patches or discovering them, downloading patches and documentation, assessing and prioritizing vulnerabilities, performing testing, deploying patches, and auditing.", "num_citations": "17\n", "authors": ["300"]}
{"title": "Access control for emerging distributed systems\n", "abstract": " As big data, cloud computing, grid computing, and the Internet of Things reshape current data systems and practices, IT experts are keen to harness the power of distributed systems to boost security and prevent fraud. How can these systems' capabilities be used to improve processing without inflating risk?", "num_citations": "16\n", "authors": ["300"]}
{"title": "Verification and test methods for access control policies/models\n", "abstract": " Access control systems are among the most critical of computer security components. Faulty policies, misconfigurations, or flaws in software implementations can result in serious vulnerabilities. To formally and precisely capture the security properties that access control should adhere to, access control models are usually written, bridging the gap in abstraction between policies and mechanisms. Identifying discrepancies between policy specifications and their intended function is crucial because correct implementation and enforcement of policies by applications is based on the premise that the policy specifications are correct. As a result, policy specifications represented by models must undergo rigorous verification and validation through systematic verification and testing to ensure that the policy specifications truly encapsulate the desires of the policy authors. Verifying the conformance of access control policies and models is a non-trivial and critical task, and one important aspect of such verification is to formally check the inconsistency and incompleteness of the model and safety requirements of the policy, because an access control model and its implementation do not necessarily explicitly express the policy, which can also be implicitly embedded by mixing with direct access constraints or other access control models.", "num_citations": "16\n", "authors": ["300"]}
{"title": "What happened to software metrics?\n", "abstract": " In the 1980s, the software community was all \u2018a buzz\u2019with seemingly endless \u2018potential\u2019approaches for producing higher quality software. At the forefront of that was the field of software metrics, along with the corresponding testing techniques, tools, and process improvement schemes that relied on the software metrics. Later, there were also suggestions of legal remedies such as Uniform Computer Information Transactions Act (UCITA) and the licensing of software engineers as professional engineers. UCITA would have made software vendors liable for defective software and the licensing of software engineers would have allowed developers to be personally sued. Further, publications such as the Software Quality Journal were launched, and events such as the Annual Workshop on Software Metrics in Oregon were held for many years. Cyclomatic complexity, the Halstead metrics, Source lines of code (SLOC\u00a0\u2026", "num_citations": "16\n", "authors": ["300"]}
{"title": "Combinatorial coverage as an aspect of test quality\n", "abstract": " There are relatively few good methods for evaluating test set quality, after ensuring basic requirements-traceability. Structural coverage, mutation testing, and related methods can be used if source code is available, but these approaches may entail significant cost in time and resources. This paper introduces an alter native measure of test quality that is directly related to fault detection, simple to compute, and can be applied prior to execution of the system under test. As such, it provides an inexpensive complement to current approaches for evaluating test quality.", "num_citations": "16\n", "authors": ["300"]}
{"title": "Combinatorial testing\n", "abstract": " Software testing has always faced a seemingly intractable problem: for real-world programs, the number of possible input combinations can exceed the number of atoms in the ocean, so as a practical matter it is impossible to show through testing that the program works correctly for all inputs. Combinatorial testing offers a (partial) solution. Empirical data show that the number of variables involved in failures is small. Most failures are triggered by only one or two inputs, and the number of variables interacting tails off rapidly, a relationship called the interaction rule. Therefore, if we test input combinations for even small numbers of variables, we can provide very strong testing at low cost. As always, there is no \u201csilver bullet\u201d answer to the problem of software assurance, but combinatorial testing has grown rapidly because it works in the real world.This book introduces the reader to the practical application of combinatorial methods in software testing. Our goal is to provide sufficient depth that readers will be able to apply these methods in their own testing projects, with pointers to freely available tools. Included are detailed explanations of how and why to use various techniques, with examples that help clarify concepts in all chapters. Sets of exercises or questions and answers are also included with most chapters. The text is designed to be accessible to an undergraduate student of computer science or engineering, and includes an extensive set of references to papers that provide more depth on each topic. Many chapters introduce some of the theory and mathematics of combinatorial methods. While this material is needed for thorough knowledge of\u00a0\u2026", "num_citations": "16\n", "authors": ["300"]}
{"title": "Automated combinatorial test methods: Beyond pairwise testing\n", "abstract": " Pairwise testing has become a popular approach to software quality assurance because it often provides effective error detection at low cost. However, pairwise (2-way) coverage is not sufficient for assurance of mission-critical software. Combinatorial testing beyond pairwise is rarely used because good algorithms have not been available for complex combinations such as 3 way, 4-way, or more. In addition, significantly more tests are required for combinations beyond pairwise testing, and testers must determine expected results for each set of inputs. This article introduces new tools for automating the production of com plete test cases covering up to 6-way combinations.", "num_citations": "16\n", "authors": ["300"]}
{"title": "The computational complexity of enforceability validation for generic access control rules\n", "abstract": " In computer security, many researches have tackled on the possibility of a unified model of access control, which could enforce any access control policies within a single unified system. One issue that must be considered is the efficiency of such systems, i.e., what is the computational complexity for the enforce ability validation of access control rules of a system that is capable of implementing any access control policy? We investigate this question by arguing that two fundamental requirements exist for any such system: satisfiability of access rules and ensuring absence of deadlock among rules. We then show that both of these problems are NP-complete by using some basic computational theorems applied to the components of the generic access control process", "num_citations": "16\n", "authors": ["300"]}
{"title": "Practical application of formal methods in modeling and simulation\n", "abstract": " This paper provides an introduction to applying formal methods to modeling and simulation problems at reasonable cost. Two approaches are discussed. First, lightweight formal methods combine simplified specification approaches with automated analysis, making it possible to analyze requirements and designs early in the development cycle.", "num_citations": "16\n", "authors": ["300"]}
{"title": "Formal specification and verification of control software for cryptographic equipment\n", "abstract": " A description is given of the application of formal specification and verification methods to two microprocessor-based cryptographic devices: a 'smart token' system that controls access to a network of workstations, and a message authentication device implementing the ANSI X9.9 message authentication standard. Formal specification and verification were found to be practical, cost-effective tools for detecting potential security weaknesses, and helped to significantly strengthen the security of the access control system.< >", "num_citations": "16\n", "authors": ["300"]}
{"title": "Attribute-Based Access Control\n", "abstract": " This comprehensive new resource provides an introduction to fundamental Attribute Based Access Control (ABAC) models. This book provides valuable information for developing ABAC to improve information sharing within organizations while taking into consideration the planning, design, implementation, and operation. It explains the history and model of ABAC, related standards, verification and assurance, applications, as well as deployment challenges. Readers find authoritative insight into specialized topics including formal ABAC history, ABAC\u2019s relationship with other access control models, ABAC model validation and analysis, verification and testing, and deployment frameworks such as XACML. Next Generation Access Model (NGAC) is explained, along with attribute considerations in implementation. The book explores ABAC applications in SOA/workflow domains, ABAC architectures, and includes details on feature sets in commercial and open source products. This insightful resource presents a combination of technical and administrative information for models, standards, and products that will benefit researchers as well as implementers of ABAC systems in the field.", "num_citations": "15\n", "authors": ["300"]}
{"title": "Applying combinatorial testing to data mining algorithms\n", "abstract": " Data mining algorithms are used to analyze and discover useful information from data. This paper presents an experiment that applies Combinatorial Testing (CT) to five data mining algorithms implemented in an open-source data mining software called WEKA. For each algorithm, we first run the algorithm with 51 datasets to study the impact different datasets have on the test coverage. We select one dataset that achieves the highest branch coverage. Next we construct positive and negative combinatorial test sets of configuration options and execute each test set with the selected dataset. Test effectiveness is measured using branch and mutation coverage. Our results suggest that when testing data mining algorithms: (1) larger datasets do not necessarily achieve higher coverage than smaller datasets, (2) test coverage increases progressively slower as test strength increases, and (3) branch coverage correlates\u00a0\u2026", "num_citations": "15\n", "authors": ["300"]}
{"title": "Using combinatorial testing to build navigation graphs for dynamic web applications\n", "abstract": " Modelling a software system is often a challenging prerequisite to automatic test case generation. Modelling the navigation structure of a dynamic web application is particularly challenging because of the presence of a large number of pages that are created dynamically and the difficulty of reaching a dynamic page unless a set of appropriate input values are provided for the parameters. To address the first challenge, some form of abstraction is required to enable scalable modelling. For the second challenge, techniques are required to select appropriate input values for parameters and systematically combine them to reach new pages. This paper presents a combinatorial approach in building a navigation graph for dynamic web applications. The navigation graph can then be used to automatically generate test sequences for testing web applications. The novelty of our approach is twofold. First, we use an\u00a0\u2026", "num_citations": "15\n", "authors": ["300"]}
{"title": "Toward secure routing infrastructures\n", "abstract": " The protocols, data, and algorithms that compute paths through interconnected network devices are possibly the most vital, complex, and fragile components in the global information infrastructure. They are also the least protected. This article examines the current state of, and practical prospects for, security in IP routing infrastructures", "num_citations": "15\n", "authors": ["300"]}
{"title": "An Analysis of Vulnerability Trends, 2008-2016\n", "abstract": " Computer security has been a subject of serious study for at least 40 years, and a steady stream of innovations has improved our ability to protect networks and applications. But attackers have adapted and changed methods over the years as well. Where do we stand today in the battle between attackers and defenders? Are attackers gaining ground, as it often seems when reading press accounts of the latest data exposure? This analysis seeks to answer these questions using data from the US National Vulnerability Database (NVD), and to identify classes of vulnerabilities where improvements will be most cost effective.", "num_citations": "14\n", "authors": ["300"]}
{"title": "Cyber-physical system development environment for energy applications\n", "abstract": " Cyber-physical systems (CPS) are smart systems that include engineered interacting networks of physical and computational components. The tight integration of a wide range of heterogeneous components enables new functionality and quality of life improvements in critical infrastructures such as smart cities, intelligent buildings, and smart energy systems. One approach to study CPS uses both simulations and hardware-in-the-loop (HIL) to test the physical dynamics of hardware in a controlled environment. However, because CPS experiment design may involve domain experts from multiple disciplines who use different simulation tool suites, it can be a challenge to integrate the heterogeneous simulation languages and hardware interfaces into a single experiment. The National Institute of Standards and Technology (NIST) is working on the development of a universal CPS environment for federation (UCEF) that\u00a0\u2026", "num_citations": "13\n", "authors": ["300"]}
{"title": "Pseudo-exhaustive testing of attribute based access control rules\n", "abstract": " Access control typically requires translating policies or rules given in natural language into a form such as a programming language or decision table, which can be processed by an access control system. Once rules have been described in machine-processable form, testing is necessary to ensure that the rules are implemented correctly. This paper describes an approach based on combinatorial test methods for efficiently testing access control rules, using the structure of attribute based access control (ABAC) to detect a large class of faults without a conventional test oracle.", "num_citations": "13\n", "authors": ["300"]}
{"title": "Vulnerability Trends: Measuring Progress\n", "abstract": " We analyzed data from the National Vulnerability Database (NVD). Designed and operated by the National Institute of Standards and Technology (NIST) with support from the Department of Homeland Security, the NVD provides fine-grained search capabilities of all publicly reported software vulnerabilities since 1997-a total of 41,810 vulnerabilities for more than 20,000 products. Frequently, a single vulnerability can affect a large number of products-for example, when the fault occurs in a library function.", "num_citations": "13\n", "authors": ["300"]}
{"title": "Combinatorial and random testing effectiveness for a grid computer simulator\n", "abstract": " This paper compares the effectiveness of random and t-way combinatorial testing, where t= 2, 3, 4, for a grid computer network simulator. Previous investigations of random vs. combinatorial testing have reached conflicting results, with some showing more effective fault detection for combinatorial testing and others finding no significant difference between the two methods. In this paper, these two methods are compared for deadlock detection on a simulator with tests covering 2-way to 4-way combinations of configuration values, paired with an equal number of randomly generated tests. Random testing provided better results than pairwise (2-way) testing and there was no statistically significant difference between the methods for 3-way testing, but 4-way combinatorial tests detected more deadlocks than the same number of random tests. The paper reviews explanations for these results and implications for testing.", "num_citations": "13\n", "authors": ["300"]}
{"title": "It doesn\u2019t have to be like this: cybersecurity vulnerability trends\n", "abstract": " Given the large and impactful data breaches making headlines in recent years, Internet users naturally wonder: Why is this happening, and how much worse can it get? Here, the authors review trends in vulnerabilities, looking at earlier findings discussed in a previous installment of this column, as well as more recent trends. They also highlight prospects for the future.", "num_citations": "12\n", "authors": ["300"]}
{"title": "Optimizing ipog's vertical growth with constraints based on hypergraph coloring\n", "abstract": " In this paper, we present an optimization of IPOG's vertical growth phase in the presence of constraints. The vertical growth problem is modeled as a classical NP-hard graph problem called \"Minimum Vertex Coloring\". In the graph model, vertices are either missing tuples that are waiting to be colored or existing tests that are already colored in different colors at the initial state, edges/hyperedges are conflicts among vertices that cannot be put in a same test. After coloring, a group of vertices in same color can be transformed to exactly a valid test. Since the original IPOG algorithm uses an arbitrary order to cover missing tuples during vertical growth, in order to reduce the number of tests, we compute the Degree of Conflicts (DOC) for each tuple, and cover the tuples in the non-increasing order of DOC. We implement a new IPOG algorithm incorporating this optimization. The experimental results show that the new\u00a0\u2026", "num_citations": "12\n", "authors": ["300"]}
{"title": "Software testing [guest editors' introduction]\n", "abstract": " Ensuring software's reliability and effectiveness--particularly as its role in society becomes ubiquitous, and the platforms on which it operates continue to evolve--is increasingly critical, ever more challenging, and replete with moving targets.", "num_citations": "12\n", "authors": ["300"]}
{"title": "Efficient algorithms for t-way test sequence generation\n", "abstract": " Combinatorial testing has been shown to be a very effective testing strategy. Most work on combinatorial testing focuses on t-way test data generation, where each test is an unordered set of parameter values. In this paper, we study the problem of t-way test sequence generation, where each test is an ordered sequence of events. Using a general labeled transition system as the system model, we formally define the notion of t-way sequence coverage, and introduce an efficient algorithm to compute all valid t-way target sequences, i.e., sequences of t events that must be covered by at least one test sequence. We then report several algorithms to generate a set of test sequences that achieves the proposed t-way sequence coverage. These algorithms are developed as the result of a systematic exploration of the possible approaches to t-way test sequence generation, and are compared both analytically and\u00a0\u2026", "num_citations": "12\n", "authors": ["300"]}
{"title": "Evaluation of fault detection effectiveness for combinatorial and exhaustive selection of discretized test inputs\n", "abstract": " Testing components of web browsers and other graphical interface software can be extremely expensive because of the need for human review of screen appearance and interactive behavior. Combinatorial testing has been advocated as a method that provides strong fault detection with a small number of tests, although some authors have disputed its effectiveness. This paper compares the effectiveness of combinatorial test methods with exhaustive testing of discretized inputs for the Document Object Model Events standard. More than 36,000 tests\u2013all possible combinations of equivalence class values\u2013were reduced by more than a factor of 20 with an equivalent level of fault detection, suggesting that combinatorial testing is a cost-effective method of assurance for web-based interactive software.", "num_citations": "12\n", "authors": ["300"]}
{"title": "Finding interaction faults adaptively using distance-based strategies\n", "abstract": " Software systems are typically large and exhaustive testing of all possible input parameters is usually not feasible. Testers select tests that they anticipate may catch faults, yet many unanticipated faults may be overlooked. This work complements current testing methodologies by adaptively dispensing one-test-at-a-time, where each test is as \"distant\" as possible from previous tests. Two types of distance measures are explored: (1) distance defined in relation to combinations of parameter-values not previously tested together and (2) distance computed as the maximum minimal Hamming distance from previous tests. Experiments compare the effectiveness of these two types of distance-based tests and random tests. Experiments include simulations, as well as examination of instrumented data from an actual system, the Traffic Collision Avoidance System (TCAS). Results demonstrate that the two instantiations of\u00a0\u2026", "num_citations": "12\n", "authors": ["300"]}
{"title": "Combinatorial and MC/DC coverage levels of random testing\n", "abstract": " Software testing criteria differ in their effectiveness, the numbers of test cases required, and the processes of test generation. Specific criteria often are compared to random testing, and in some cases, random testing shows a surprisingly high level of effectiveness. One reason that this is the case is that any random test set has a specific level of coverage according to any coverage criterion. Numerical evaluation of coverage levels of random testing according to various coverage criteria is an interesting research task and is important in understanding the relationship between different testing approaches. In this paper, we performed an experimental evaluation of the coverage levels of random testing for two criteria: MC/DC and combinatorial t-way testing. Our experiments showed that, when the number of random test cases increased, a high level of coverage was reached rapidly, both for MC/DC and t-way. However\u00a0\u2026", "num_citations": "11\n", "authors": ["300"]}
{"title": "Implementing and managing policy rules in attribute based access control\n", "abstract": " Attribute Based Access Control (ABAC) is a popularapproach to enterprise-wide access control that provides flexibility suitable for today's dynamic distributed systems. ABAC controls access to objects by evaluating policy rules against the attributes of entities (subject and object),operations, and the environment relevant to a request, but great care must be taken in setting up and maintaining the access control rules that allow such flexible operations. This article summarizes important considerations in ABAC deployment first introduced in the Guide to Attribute Based Access Control [1].", "num_citations": "11\n", "authors": ["300"]}
{"title": "Vulnerability Hierarchies in Access Control Configurations\n", "abstract": " This paper applies methods for analyzing fault hierarchies to the analysis of relationships among vulnerabilities in misconfigured access control rule structures. Hierarchies have been discovered previously for faults in arbitrary logic formulae [11,10,9,21], such that a test for one class of fault is guaranteed to detect other fault classes subsumed by the one tested, but access control policies reveal more interesting hierarchies. These policies are normally composed of a set of rules of the form \u201cif [conditions] then [decision]\u201d, where [conditions] may include one or more terms or relational expressions connected by logic operators, and [decision] is often 2-valued (\u201cgrant\u201d or \u201cdeny\u201d), but may be n-valued. Rule sets configured for access control policies, while complex, often have regular structures or patterns that make it possible to identify generic vulnerability hierarchies for various rule structures such that an exploit for one\u00a0\u2026", "num_citations": "11\n", "authors": ["300"]}
{"title": "Practical interdomain routing security\n", "abstract": " This article reviews risks and vulnerabilities in interdomain routing and best practices that can have near-term benefits for routing security. It includes examples of routing failures and common attacks on routers, and countermeasures to reduce router vulnerabilities.", "num_citations": "11\n", "authors": ["300"]}
{"title": "Assessment of Access Control Systems.\n", "abstract": " Adequate security of information and information systems is a fundamental management responsibility. Nearly all applications that deal with financial, privacy, safety, or defense include some form of access control. Access control is concerned with determining the allowed activities of legitimate users, mediating every attempt by a user to access a resource in the system. In some systems, complete access is granted after successful authentication of the user, but most systems require more sophisticated and complex control. In addition to the authentication mechanism (such as a password), access control is concerned with how authorizations are structured. In some cases, authorization may mirror the structure of the organization, while in others it may be based on the sensitivity level of various documents and the clearance level of the user accessing those documents. This publication explains some of the commonly used access control services available in information technology systems.", "num_citations": "11\n", "authors": ["300"]}
{"title": "Toward credible IT testing and certification\n", "abstract": " Accessible, rigorous measurement and test methods are key to creating quality software and increasing IT market competition. At the US National Institute of Standards and Technology's Information Technology Laboratory, work focuses on establishing comprehensive certification capability for the IT industry. This article describes NIST's work and how it can be applied.", "num_citations": "11\n", "authors": ["300"]}
{"title": "Combinatorial methods for explainable AI\n", "abstract": " This short paper introduces an approach to producing explanations or justifications of decisions made by artificial intelligence and machine learning (AI/ML) systems, using methods derived from fault location in combinatorial testing. We use a conceptually simple scheme to make it easy to justify classification decisions: identifying combinations of features that are present in members of the identified class and absent or rare in non-members. The method has been implemented in a prototype tool, and examples of its application are given.", "num_citations": "10\n", "authors": ["300"]}
{"title": "Estimating t-way fault profile evolution during testing\n", "abstract": " Empirical studies have shown that most software interaction faults involve one or two variables interacting, with progressively fewer triggered by three or more, and no failure has been reported involving more than six variables interacting. This paper introduces a model for the origin of this distribution, evaluates model predictions against empirical data, and discusses implications for removal of interaction faults and reliability growth.", "num_citations": "10\n", "authors": ["300"]}
{"title": "CCM: A tool for measuring combinatorial coverage of system state space\n", "abstract": " This poster presents some measures of combinatorial coverage that can be helpful in estimating residual risk related to insufficient testing of rare interactions, and a tool for computing these measures.", "num_citations": "10\n", "authors": ["300"]}
{"title": "Pol\u00edticas de mercado de trabajo y pobreza rural en Am\u00e9rica Latina\n", "abstract": " Pol\u00edticas de mercado de trabajo y pobreza rural en Am\u00e9rica Latina FAO_logo home-icon English Espa\u00f1ol Fran\u00e7ais \u0627\u0644\u0639\u0631\u0628\u064a\u0629 \u4e2d\u6587 \u0420\u0443\u0441\u0441\u043a\u0438\u0439 home-icon Translate with Google Access the full text Link to PDF Lookup at Google Scholar google-logo Bibliographic information Language : English In AGRIS since : 2017 Start Page : diagrs., graphs, tables Publisher : FAO/CEPAL/OIT ISBN : 978-92-1-307179-1 All titles : \" Pol\u00edticas de mercado de trabajo y pobreza rural en Am\u00e9rica Latina \" \" Tomo II \" Save as: AGRIS_AP RIS EndNote(XML) Pol\u00edticas de mercado de trabajo y pobreza rural en Am\u00e9rica Latina Loading... Paper Written Paper Pol\u00edticas de mercado de trabajo y pobreza rural en Am\u00e9rica Latina [2012] Soto Baquero, F. Klein, E. CEPAL, Santiago (Chile) [Corporate Author] FAO, Santiago (Chile). Oficina Regional para Am\u00e9rica Latina y el Caribe [Corporate Author] OIT, Geneva (Switzerland) [Corporate Author] Access the -\u2026", "num_citations": "10\n", "authors": ["300"]}
{"title": "Vulnerabilities in Quantum Key Distribution Protocols\n", "abstract": " Recently proposed quantum key distribution protocols are shown to be vulnerable to a classic man-in-the-middle attack using entangled pairs created by Eve. It appears that the attack could be applied to any protocol that relies on manipulation and return of entangled qubits to create a shared key. The protocols that are cryptanalyzed in this paper were proven secure with respect to some eavesdropping approaches, and results reported here do not invalidate these proofs. Rather, they suggest that quantum cryptographic protocols, like conventional protocols, may be vulnerable to methods of attack that were not envisaged by their designers.", "num_citations": "10\n", "authors": ["300"]}
{"title": "Guide to attribute based access control (abac) definition and considerations [includes updates as of 02-25-2019]\n", "abstract": " This document provides Federal agencies with a definition of attribute based access control (ABAC). ABAC is a logical access control methodology where authorization to perform a set of operations is determined by evaluating attributes associated with the subject, object, requested operations, and, in some cases, environment conditions against policy, rules, or relationships that describe the allowable operations for a given set of attributes. This document also provides considerations for using ABAC to improve information sharing within organizations and between organizations while maintaining control of that information.[Supersedes SP 800-162 (January 2014): https://www. nist. gov/publications/guide-attribute-based-access-control-abac-definition-and-considerations]", "num_citations": "9\n", "authors": ["300"]}
{"title": "Testing IoT Systems\n", "abstract": " This article presents challenges and solutions to testing systems based on the underlying products and services commonly referred to as the Internet of \u2018things\u2019(IoT).", "num_citations": "9\n", "authors": ["300"]}
{"title": "Improving MC/DC and fault detection strength using combinatorial testing\n", "abstract": " Software, in many different fields and tasks, hasplayed a critical role and even replaced humans to improveefficiency and safety. However, catastrophic consequences can becaused by implementation bugs and design defects. Modifiedcondition/decision coverage (MC/DC), required by the FederalAviation Administration on Level A (the most safety criticalsystem), has been shown to be effective in detecting softwarebugs. However, generating tests to achieve high MC/DC can bevery expensive and time consuming. Recently, many studiesshowed that combinatorial testing (CT) could generate highqualitytest cases in a cost-effective way. Can CT generate testcases to achieve high MC/DC? In this paper, we conduct anempirical study on two real-life programs to evaluate theefficiency and effectiveness of using combinatorial testing toimprove MC/DC coverage achievement, as well as the faultdetection strength.", "num_citations": "9\n", "authors": ["300"]}
{"title": "Improving IPOG's vertical growth based on a graph coloring scheme\n", "abstract": " We show that the vertical growth phase of IPOG is optimal for t-way test generation when t = 2, but it is no longer optimal when t is greater than 2. We present an improvement that reduces the number of tests generated during vertical growth. The vertical growth problem is modeled as a classical NP-hard problem called \u201cMinimum Vertex Coloring\u201d. We adopted a greedy coloring algorithm to determine the order in which missing tuples are covered during vertical growth. We implemented a revised IPOG algorithm incorporating this improvement. The experimental results show that compared with the original IPOG algorithm, which uses an arbitrary order to cover missing tuples during vertical growth, the revised IPOG algorithm reduces the number of tests for many real-life systems.", "num_citations": "9\n", "authors": ["300"]}
{"title": "Introducing combinatorial testing in a large organization: Pilot project experience report\n", "abstract": " This poster gives an overview of the experience of eight pilot projects, over two years, applying combinatorial testing in Lockheed Martin (LM), one of the world's largest aerospace firms. Lockheed Martin and NIST developed a Co-operative Research and Development Agreement (CRADA) to evaluate effectiveness and areas of suitable application for combinatorial testing in a real-world industrial setting with complex software requirements. (One of the ways in which NIST conducts joint research with US industry is through CRADAs, which allow federal laboratories to work with US industry and provide flexibility in structuring projects, intellectual property rights, and in protecting industry proprietary information and research results).", "num_citations": "9\n", "authors": ["300"]}
{"title": "Understanding insecure IT: Practical risk assessment\n", "abstract": " Risk assessment involves gathering and evaluating risk information so that enterprise stakeholders can make mitigation decisions. Once we identify the risks, we can rank the probability of each one's occurrence and its impact on the organization. Some risks are more likely to occur than others, and different risks can affect an organization in different ways, so a practical risk assessment can help ensure that enterprises identify the most significant risks and determine the best actions for mitigating them.", "num_citations": "9\n", "authors": ["300"]}
{"title": "Introducing\" Insecure IT\"\n", "abstract": " This article introduces \"insecure IT\", a new department for IT Professional that will cover security weaknesses in IT systems, ranging from desktops to global e-commerce networks. This regular feature will offer ideas to improve IT security, both by looking at ways it can go wrong as well as by covering good practices.", "num_citations": "9\n", "authors": ["300"]}
{"title": "A quantum cryptographic protocol with detection of compromised server\n", "abstract": " This paper presents a hybrid cryptographic protocol, using quantum and classical resources, to generate a key for authentication and optionally for encryption in a network. One or more trusted servers distribute streams of entangled photons to individual resources that seek to communicate. An important class of cheating by a compromised server will be detected.", "num_citations": "9\n", "authors": ["300"]}
{"title": "CAGEN: A fast combinatorial test generation tool with support for constraints and higher-index arrays\n", "abstract": " In recent years, combinatorial testing methods have been successfully applied to test systems with a larger number of input parameters. Generating combinatorial test sets for such complex systems is a challenging task as it requires a lot of time and computing power. To tackle that issue, high performance tools are required. In this work, we present the combinatorial test set generation tool CAgen. It is capable of generating combinatorial test sets significantly faster than other state-of-the-art tools, such as ACTS, and contains various features such as constraint handling and higher-index arrays. It is highly compatible with other combinatorial testing tools and is available as CLI and Web-GUI. CAgen aims to make combinatorial testing more efficient and more accessible and user-friendly.", "num_citations": "8\n", "authors": ["300"]}
{"title": "Combinatorial testing of full text search in web applications\n", "abstract": " Database driven web applications are some of the most widely developed systems today. In this paper, we demonstrate use of combinatorial testing for testing database supported web applications, especially where full-text search is provided or many combinations of search options are utilized. We develop test-case selection techniques, where test strings are synthesized using characters or string fragments that may lead to system failure. We have applied our approach to the National Vulnerability Database (NVD) application and have discovered a number of \"corner-cases\" that had not been identified previously. We also present simple heuristics for isolating the fault causing factors that can lead to such system failures. The test method and input model described in this paper have immediate application to other systems that provide complex full text search.", "num_citations": "8\n", "authors": ["300"]}
{"title": "Access control policy verification\n", "abstract": " To ensure that an access control (AC) system is safe, there must be a reliable means to verify that the specified AC policy model conforms to the safety requirements and policy author's intentions. A general verification approach includes black-box and white-box testing, as well as the generation of sufficient test cases to check the correctness of model implementations.", "num_citations": "8\n", "authors": ["300"]}
{"title": "Tls cipher suites recommendations: A combinatorial coverage measurement approach\n", "abstract": " We present a coverage measurement for TLS cipher suites recommendations provided by various regulatory and intelligence organizations such as the IETF, Mozilla, ENISA, German BSI, and USA NSA. These cipher suites are measured and analyzed using a combinatorial approach, which was made feasible via developing the necessary input models. Besides shedding light on the coverage achieved by the proposed recommendations, we discuss implications towards aspects of test quality. One of them relates to the testing of a TLS implementation, where a system designer or tester should expand the TLS cipher suite registry and integrate the information back to the TLS implementation itself such that the (overall) testing effort is reduced.", "num_citations": "8\n", "authors": ["300"]}
{"title": "Evaluating the effectiveness of ben in localizing different types of software fault\n", "abstract": " Debugging or fault localization is one of the most challenging tasks during software development. Many tools have been developed to reduce the amount of effort and time software developers have to spend on fault localization. In this paper, we evaluate the effectiveness of a fault localization tool called BEN in localizing different types of software fault. Assuming that combinatorial testing has been performed on the subject program, BEN leverages the result obtained from combinatorial testing to perform fault localization. Our evaluation focuses on impact of three properties of software fault on the effectiveness of BEN. The three properties include accessibility, input value sensitivity and control flow sensitivity. A random test set-based approach is used to measure the three properties. The experimental results suggest that BEN is more effective, respectively, in localizing faults of lower accessibility, input value\u00a0\u2026", "num_citations": "8\n", "authors": ["300"]}
{"title": "Tower of covering arrays\n", "abstract": " Covering arrays are combinatorial objects that have several practical applications, specially in the design of experiments for software and hardware testing. A covering array of strength t and order v is an N\u00d7 k array over Z v with the property that every N\u00d7 t subarray covers all members of Z v t at least once. In this work we explore the construction of a Tower of Covering Arrays (TCA) as a way to produce covering arrays that improve or match some current upper bounds. A TCA of height h is a succession of h+ 1 covering arrays C 0, C 1,\u2026, C h in which for i= 1, 2,\u2026, h the covering array C i is one unit greater in the number of factors and the strength of the covering array C i\u2212 1; this way, if the covering array C 0 is of strength t and has k factors then the covering arrays C 1,\u2026, C h are of strength t+ 1,\u2026, t+ h and have k+ 1,\u2026, k+ h factors respectively. We note that the ratio between the number of rows of the last covering\u00a0\u2026", "num_citations": "8\n", "authors": ["300"]}
{"title": "A general conformance testing framework for IEEE 11073 PHD's communication model\n", "abstract": " ISO/IEEE 11073 Personal Health Data (IEEE 11073 PHD) is a set of standards that addresses the interoperability of personal healthcare devices. As an important part of IEEE 11073 PHD, ISO/IEEE 1107-20601 optimized exchange protocol (IEEE 11073-20601) defines how personal healthcare devices communicate with computing resources like PCs and set-top boxes. In this paper, we propose a general conformance testing framework for IEEE 11073-20601 protocol stack. This framework can be used to ensure that different implementations of the protocol stack conform to the specification and are thus able to interoperate with each other. We are developing a prototype research tool that applies the proposed framework to Antidote, an open-sourced IEEE 11073-20601 protocol stack. We report some preliminary testing results.", "num_citations": "8\n", "authors": ["300"]}
{"title": "Protecting wireless local area networks\n", "abstract": " To help organizations improve their WLAN security, the National Institute of Standards and Technology recently published Guidelines for Securing Wireless Local Area Networks (WLANs): Recommendations of the National Institute of Standards and Technology. Here, the authors summarize some of the recommendations.", "num_citations": "8\n", "authors": ["300"]}
{"title": "Role engineering: Methods and standards\n", "abstract": " Most of today's large firms use some form of role-based access control (RBAC) to support thousands of users and permission controls. Recognizing the need for some commonality among the various RBAC models, the National Institute of Standards and Technology proposed the NIST Model for RBAC in 2000. NIST is now working to update and enhance this standard.", "num_citations": "8\n", "authors": ["300"]}
{"title": "Advanced combinatorial test methods for system reliability\n", "abstract": " Every computer user is familiar with software bugs. Many seem to appear almost randomly, suggesting that the conditions triggering them must be complex, and some famous software bugs have been traced to highly unusual combinations of conditions. For example, the 1997 Mars Pathfinder mission began experiencing system resets at seemingly unpredictable times soon after it landed and began collecting data. Fortunately, engineers were able to deduce and correct the problem, which occurred only when (1) a particular type of data was being collected and (2) intermediate priority tasks exceeded a certain load, allowing a blocking condition that eventually triggered a reset.At 155,000 lines of code (not including the operating system), the Pathfinder program is small compared with commercial software: a Boeing 777 airliner flies on 6.5 million lines of code, the Microsoft Windows XP operating system is estimated at 40 million, and within the next two years the average new car may have more than 100 million lines of code in various subsystems. Ensuring correct operation of complex software is so difficult that more than half of a software development budget\u2013frequently tens of millions of dollars\u2013is normally devoted to testing, and even then errors often escape detection. A 2002 NIST-funded study by the Research Triangle Institute estimated an annual cost of inadequate software testing infrastructure at $22.2 to $59.5 billion for the US economy [1].", "num_citations": "8\n", "authors": ["300"]}
{"title": "Security and privacy landscape in emerging technologies\n", "abstract": " Recent events spawned a need for better communications of security systems, including industrial control systems and emergency management systems. This work is in initial phases and the author reports it here. In this final column for emerging standards and technologies, she also discusses the privacy and security challenges of Web 2.0 and globalization.", "num_citations": "8\n", "authors": ["300"]}
{"title": "How does combinatorial testing perform in the real world: an empirical study\n", "abstract": " Studies have shown that combinatorial testing (CT) can be effective for detecting faults in software systems. By focusing on the interactions between different factors of a system, CT shows its potential for detecting faults, especially those that can be revealed only by the specific combinations of values of multiple factors (multi-factor faults). However, is CT practical enough to be applied in the industry? Can it be more effective than other industry-favored techniques? Are there any challenges when applying CT in practice? These research questions remain in the context of industrial settings. In this paper, we present an empirical study of CT on five industrial systems with real faults. The details of the input space model (ISM) construction, such as factor identification and value assignment, are included. We compared the faults detected by CT with those detected by the in-house testing teams using other methods, and\u00a0\u2026", "num_citations": "7\n", "authors": ["300"]}
{"title": "Applying combinatorial testing to large-scale data processing at adobe\n", "abstract": " Adobe offers an analytics product as part of the Marketing Cloud software with which customers can track many details about users across various digital platforms. For the most part, customers define the amount and type of data to track. This high dimensionality makes validation difficult or intractable. Due to increasing attention from both industry and academia, combinatorial testing was investigated and applied to improve existing validation. In this paper, we report the practical application of combinatorial testing to the data collection, compression and processing components of the Adobe analytics product. Consequently, the effectiveness of combinatorial testing for this application is measured in terms of new defects found rather than detecting known defects from previous versions. The results of the application show that combinatorial testing is an effective way to improve validation for these components of Adobe\u00a0\u2026", "num_citations": "7\n", "authors": ["300"]}
{"title": "Internet of things (IoT) trust concerns (draft)\n", "abstract": " The Internet of Things (IoT) refers to systems that involve computation, sensing, communication, and actuation (as presented in NIST Special Publication (SP) 800-183). IoT involves the connection between humans, non-human physical objects, and cyber objects, enabling monitoring, automation, and decision making. The connection is complex and inherits a core set of trust concerns, most of which have no current resolution This publication identifies 17 technical trust-related concerns for individuals and organizations before and after IoT adoption. The set of concerns discussed here is necessarily incomplete given this rapidly changing industry, however this publication should still leave readers with a broader understanding of the topic. This set was derived from the six trustworthiness elements in NIST SP 800-183. And when possible, this publication outlines recommendations for how to mitigate or reduce the\u00a0\u2026", "num_citations": "7\n", "authors": ["300"]}
{"title": "Efficient methods for interoperability testing using event sequences\n", "abstract": " Many software testing problems involve sequences of events. The methods described in this paper were motivated by testing needs of mission critical systems that may accept multiple communication or sensor inputs and generate output to several communication links and other interfaces, where it is important to test the order in which events occur. Using combinatorial methods makes it possible to test sequences of events using significantly fewer tests than previous procedures.Descriptors:", "num_citations": "7\n", "authors": ["300"]}
{"title": "Enabling standards for nanomaterial characterization\n", "abstract": " A two-day international workshop was convened recently in order to scope out and address the urgent need for standards to accurately characterize the physico-chemical and biological properties of engineered nanomaterials. These standards are needed by industry and regulatory bodies in order to meet requirements for the production, application and lifecycle risk management of nanomaterial-based products ranging from cancer therapeutics to high-tech coatings and composites. The current deficiency in the availability of such standards, including both documentary and reference artifacts, is perceived as limiting the widespread adoption and implementation of nanoscale technologies. Herein is given a brief summary of that workshop, its findings and recommendations.", "num_citations": "7\n", "authors": ["300"]}
{"title": "An application of combinatorial methods for explainability in artificial intelligence and machine learning (draft)\n", "abstract": " This short paper introduces an approach to producing explanations or justifications of decisions made in some artificial intelligence and machine learning (AI/ML) systems, using methods derived from those for fault location in combinatorial testing. We show that validation and explainability issues are closely related to the problem of fault location in combinatorial testing, and that certain methods and tools developed for fault location can also be applied to this problem. This approach is particularly useful in classification problems, where the goal is to determine an object\u2019s membership in a set based on its characteristics. We use a conceptually simple scheme to make it easy to justify classification decisions: identifying combinations of features that are present in members of the identified class but absent or rare in non-members. The method has been implemented in a prototype tool called ComXAI, and examples of\u00a0\u2026", "num_citations": "6\n", "authors": ["300"]}
{"title": "Can reducing faults prevent vulnerabilities?\n", "abstract": " Most security vulnerabilities result from ordinary coding errors. What does this mean for the prospects of more secure software? Software vulnerabilities arise in different ways, but can roughly be grouped into three categories based on their source: design flaws, configuration errors, and ordinary bugs or coding errors.", "num_citations": "6\n", "authors": ["300"]}
{"title": "High resistance scaling from 10 k\u03a9 and QHR standards using a cryogenic current comparator\n", "abstract": " We describe a cryogenic current comparator (CCC) bridge for resistance scaling that provides improved measurement uncertainty over a range of resistance values from 100 kOmega to 1 GOmega. This CCC is designed for high resistance scaling based directly on a quantized Hall resistance (QHR) standard as well as comparisons of resistance ratios of 1, 10 and 100. The QHR-to-decade-value winding ratio offset is chosen so as to approximately cancel the offset produced by resistive decade windings.", "num_citations": "6\n", "authors": ["300"]}
{"title": "Simulations of noise-parameter verification using cascade with isolator or mismatched transmission line\n", "abstract": " Results are presented for simulations of a verification process for noise-parameter measurements. The verification process consists of first measuring separately both a passive device and the amplifier or transistor of interest (the device under test, or DUT) and then measuring the tandem configuration of passive device plus DUT. The results of the measurements of the tandem configuration are then compared to the predictions obtained by cascading the noise parameters and S-parameters of the two individual components. In order that the comparisons be meaningful, uncertainties are computed for both predictions and simulated measurements.", "num_citations": "6\n", "authors": ["300"]}
{"title": "The next generation of access control models: Do we need them and what should they be?\n", "abstract": " The next generation of access control models: Do we need them and what should they be? \u2014 Penn State Skip to main navigation Skip to search Skip to main content Penn State Logo Help & FAQ Home Researchers Research output Research Units Core Facilities Grants & Projects Prizes Activities Search by expertise, name or affiliation The next generation of access control models: Do we need them and what should they be? Ravi Sandhu, Elisa Bertino, Trent Ray Jaeger, Richard Kuhn, Carl Landwehr Computer Science and Engineering Institute for Computational and Data Sciences (ICDS) Research output: Chapter in Book/Report/Conference proceeding \u203a Conference contribution 2 Scopus citations Overview Fingerprint Original language English (US) Title of host publication Proceedings of Sixth ACM Symposium on Access Control Models and Technologies (SACMAT 2001) Number of pages 1 State Published - \u2026", "num_citations": "6\n", "authors": ["300"]}
{"title": "Future directions in role-based access control\n", "abstract": " RBAC efforts are underway at a number of organizations. Among the current efforts to develop new RBAC capabilities: l Major information technology companies, including IBM, ICL, Oracle, and Sybase are developing new RBAC features for their product lines. l Ravi Sandhu and associates at George Mason University and SE: TA Corporation are developing a formal framework for RBAC models. l The National Security Agency (NSA) is sponsoring work by the National Institute of Standards and Technology (NIST) and Virgil Gligor at the University of Maryland to model and implement RBAC on a policy-independent Mach microkernel-based operating system being developed by NSA called Synergy. l The implementation of RBAC using object-oriented software is being investigated at Odyssey Research, the University of Connecticut, and NIST. NIST is also initiating a project to implement RBAC on the World Wide\u00a0\u2026", "num_citations": "6\n", "authors": ["300"]}
{"title": "Open systems software standards in concurrent engineering\n", "abstract": " Publisher SummaryThis chapter presents an introduction to open systems, explains important open system standards, and discusses how to use open system products effectively in concurrent engineering. Products based on open systems standards\u2014particularly, the ISO Open System Interconnection (OSI), and IEEE POSIX\u2014are beginning to replace reliance on proprietary computing platforms. Engineers who develop products that incorporate embedded computers and software are thus faced with the challenge of understanding two technological developments that will change the way they do their jobs: concurrent engineering and open systems. Software development and integration are often the weak links in product development because of incompatibilities among embedded software components. A principal objective of open systems standards is to make systems integration easier. Open system standards\u00a0\u2026", "num_citations": "6\n", "authors": ["300"]}
{"title": "Input space coverage matters\n", "abstract": " Testing is the most commonly used approach for software assurance, yet it remains as much judgment and art as science. We suggest that structural coverage measures must be supplemented with measures of input space coverage, providing a means of verifying that an adequate input model has been defined.", "num_citations": "5\n", "authors": ["300"]}
{"title": "Systematic testing of post-quantum cryptographic implementations using metamorphic testing\n", "abstract": " Cryptographic algorithms are usually complex, and their code is highly compact. Moreover, there is often no test oracle to easily test some of these algorithms. Together these attributes make it extremely challenging to run tests and to discover bugs in them. Structural coverage based approaches such as statement or branch coverage are typically not very effective in discovering bugs in these types of programs. In this paper, we investigate the effectiveness of a systematic testing approach for discovering bugs in highly complex cryptographic algorithm implementations. In this work, we identify metamorphic relations based on the specifications of the algorithms, and design test cases such that a systematic coverage of the input space is achieved. Our results show that this approach is highly effective in discovering faults in complex cryptographic implementations.", "num_citations": "5\n", "authors": ["300"]}
{"title": "A data structure for integrity protection with erasure capability\n", "abstract": " This note describes a data structure, which can be referred to as a block matrix, that supports the ongoing addition of hash-linked records while also allowing the deletion of arbitrary records, preserving hashbased integrity assurance that other blocks are unchanged. The block matrix data structure may have utility for incorporation into applications requiring integrity protection that currently use permissioned blockchains. This capability could for example be useful in meeting privacy requirements such as the European Union General Data Protection Regulation (GDPR), which requires that organizations make it possible to delete all information related to a particular individual, at that person's request.", "num_citations": "5\n", "authors": ["300"]}
{"title": "Computer science education in 2018\n", "abstract": " Six senior computer science educators answer questions about the current state of computer science education, software engineering, and licensing software engineers.", "num_citations": "5\n", "authors": ["300"]}
{"title": "Applying higher strength combinatorial criteria to test prioritization: A case study\n", "abstract": " Faults in software systems often occur due to interactions between parameters. Several studies show that faults are caused by 2-way through 6-way interactions of parameters. In the context of test suite prioritization, we have studied prioritization by 2-way interwindow interaction coverage and found that this criteria is effective at finding faults quickly in the test execution cycle. However, since faults may be caused by interactions between more than 2 parameters, in this paper, we provide a greedy algorithm for test suite prioritization by n-way combinatorial coverage of inter-window interactions. While greedy algorithms that generate Combinatorial Interaction Test suites enumerate and track the coverage of all possible t-tuples and constraints, we have noticed that our user-session-based test suites often do not contain every possible t-tuple and we can take advantage of this in our algorithm by only storing t-tuples that\u00a0\u2026", "num_citations": "5\n", "authors": ["300"]}
{"title": "Practical combinatorial (t-way) methods for detecting complex faults in regression testing\n", "abstract": " Regression testing can be among the most challenging of software assurance tasks because program changes often introduce faults, including unexpected interactions among different parts of the code. Unanticipated interactions may also occur when software is modified for a new platform. Techniques such as pairwise testing are not sufficient for detecting these faults, because empirical evidence shows that some errors are triggered only by the interaction of three, four, or more parameters. However, new algorithms and tools make it possible to generate tests that cover complex combinations of values (2-way to 6-way), or to analyze existing test suites and automatically generate tests that provide combinatorial coverage. The key advantage of this approach is that it produces better testing using a fraction of the tests required by other methods.", "num_citations": "5\n", "authors": ["300"]}
{"title": "Fire pattern repeatability: a laboratory study on gypsum wallboard\n", "abstract": " In 2009, the National Research Council (US) published a report identifying the research needs of the forensic science community. In the field of fire investigation, one of the specific needs identified was research on the natural variability of burn patterns. The National Institute of Standards and Technology (NIST) is conducting a multi-year study, with the support of the National Institute of Justice (NIJ) and the NIST Office of Law Enforcement Standards (OLES), to examine the repeatability of burn patterns. The primary objective of the study is assessing the repeatability of burn patterns on gypsum board exposed to a range of source fires. This paper will provide the results from the characterization of the source fires and the results of the pre-flashover fire pattern repeatability experiments.Experiments were conducted with a natural gas fueled burner, a gasoline fueled pan fire, and polyurethane foam. The top surface of the burner or fuel was 0.30 m (1 ft) by 0.30 m (1 ft) with the top surface approximately 76 mm (3 in) above the floor. Replicate source fire experiments were conducted in an oxygen depletion calorimeter with each of the fuels, in order to examine the repeatability of the fires in terms of heat release rate. The flame movement and height for each fire was recorded with photographs and videos.", "num_citations": "5\n", "authors": ["300"]}
{"title": "An algorithm for generating very large covering arrays\n", "abstract": " Discussion: This procedure works because with many variables, each test case contains thousands of k-way combinations of variables. Another way of viewing the covering procedure is to think of each of the C=", "num_citations": "5\n", "authors": ["300"]}
{"title": "Security for Telecommuting and Broadband Communications: Recommendations of the National Institute of Standards and Technology\n", "abstract": " This document is intended to assist those responsible-users, system administrators, and management-for telecommuting security, by providing introductory information about broadband communication security and policy, security of home office systems, and considerations for system administrators in the central office. It addressees concepts relating to the selection, deployment, and management of broadband communications for a telecommuting user. This document is not intended to provide a mandatory framework for telecommuting or home office broadband communication environments, but rather to present suggested approaches to the topic.Descriptors:", "num_citations": "5\n", "authors": ["300"]}
{"title": "Security for telecommuting and broadband communications\n", "abstract": " This document has been developed by the National Institute of Standards and Technology (NIST) in furtherance of its statutory responsibilities under the Computer Security Act of 1987 and the Information Technology Management Reform Act of 1996, specifically 15 United States Code (USC) 278 g-3 (a)(5). This document is not a guideline within the meaning of 15 USC 278 g-3 (a)(3).", "num_citations": "5\n", "authors": ["300"]}
{"title": "Improving public switched network security in an open environment\n", "abstract": " Government studies have identified potential vulnerabilities in the public switched network that could be exploited by hostile users. As the telecommunications industry moves toward greater openness and more services are added, the potential for abuse is likely to increase. The US government's efforts to maintain the integrity of telecommunications services in this new open environment are summarized. The authors outline major government concerns and describe how federal agencies are responding. Key elements of the response include cooperation between government and industry, additional research and development, and an emphasis on building secure systems using open system software.< >", "num_citations": "5\n", "authors": ["300"]}
{"title": "Oracle-free match testing of a program using covering arrays and equivalence classes\n", "abstract": " A process for testing a program includes: receiving a variable comprising a plurality of input values; producing a plurality of equivalence classes for the input values; producing a representative value per equivalence class; producing, by a processor, a primary covering array comprising a plurality of primary vectors; producing a secondary covering array comprising a plurality of secondary vectors; providing the secondary vectors to the program; and producing a result vector comprising a plurality of result entries to test the program. A computer system for testing the program includes: a memory; and a processor, in communication with the memory, wherein the computer system is configured to perform the process for testing the program. A computer program product for testing the program includes: a non-transitory computer readable storage medium readable by a processor and storing program code for execution by\u00a0\u2026", "num_citations": "4\n", "authors": ["300"]}
{"title": "An approach to t-way test sequence generation with constraints\n", "abstract": " In this paper we address the problem of constraint handling in t-way test sequence generation. We develop a notation for specifying sequencing constraints and present a t-way test sequence generation that handles the constraints specified in this notation. We report a case study in which we applied our notation and test generation algorithm to a real-life communication protocol. Our experience indicates that our notation is intuitive to use and allows us to express important sequencing constraints for the protocol. However, the test generation algorithm takes a significant amount of time. This work is part of our larger effort to make t-way sequence testing practically useful.", "num_citations": "4\n", "authors": ["300"]}
{"title": "What proportion of vulnerabilities can be attributed to ordinary coding errors? poster\n", "abstract": " The analysis reported in this poster developed from questions that arose in discussions of the Reducing Software Vulnerabilities working group, sponsored by the White House Office of Science and Technology Policy in 2016 [1]. The key question we sought to address is the degree to which vulnerabilities arise from ordinary program errors, which may be detected in code reviews and functional testing, rather than post-release.", "num_citations": "4\n", "authors": ["300"]}
{"title": "Automated Combinatorial Testing for Software\n", "abstract": " The usual logic operators, plus temporal: A \u03c6-All: \u03c6 holds on all paths starting from the current state. E \u03c6-Exists: \u03c6 holds on some paths starting from the current state. G \u03c6-Globally: \u03c6 has to hold on the entire subsequent path. F \u03c6-Finally: \u03c6 eventually has to hold X \u03c6-Next: \u03c6 has to hold at the next state [others not listed] execution paths states on the execution paths", "num_citations": "4\n", "authors": ["300"]}
{"title": "Equivalence class verification and oracle-free testing using two-layer covering arrays\n", "abstract": " This short paper introduces a method for verifying equivalence classes for module/unit testing. This is achieved using a two-layer covering array, in which some or all values of a primary covering array represent equivalence classes. A second layer covering array of the equivalence class values is computed, and its values substituted for the equivalence class names in the primary array. It is shown that this method can detect certain classes of errors without a conventional test oracle, and an illustrative example is given.", "num_citations": "4\n", "authors": ["300"]}
{"title": "Estimating fault detection effectiveness\n", "abstract": " A t-way covering array can detect t-way faults, however they generally include other combinations beyond t-way as well. For example, a particular test set of all 5-way combinations is shown capable of detecting all seeded faults in a test program, despite the fact that it contains up to 9-way faults. This poster gives an overview of methods for estimating fault detection effectiveness of a test set based on combinatorial coverage for a class of software. Detection effectiveness depends on the distribution of t-way faults, which is not known. However based on past experience one could say for example the fraction of 1-way faults is F 1  = 60 %, 2- way faults F 2  = 25 % F 3  = 10 % and F 4  = 5 %. Such information could be used in determining the required strength t. It is shown that the fault detection effectiveness of a test set may be affected significantly by the t-way fault distribution, overall, simple coverage at each level of t\u00a0\u2026", "num_citations": "4\n", "authors": ["300"]}
{"title": "Combinatorial Methods in Testing\n", "abstract": " Chapter 1 in\" Introduction to Combinatorial Testing\" from CRC Press. Background material for Introduction to Combinatorial Testing from NIST SP 800-142 and other technical papers, which explain concepts of combinatorial testing, application to the Document Object Model, concepts and methods of combinatorial coverage measurement, and the history of combinatorial testing development from statistical design of experiments methods.", "num_citations": "4\n", "authors": ["300"]}
{"title": "Combinatorial coverage measurement\n", "abstract": " Combinatorial Coverage Measurement Page 1 Combinatorial Coverage Measurement Rick Kuhn NIST Page 2 Software Failure Analysis \u2022 We studied software failures in a variety of fields including 15 years of FDA medical device recall data \u2022 What causes software failures? \u2022 logic errors? \u2022 calculation errors? \u2022 inadequate input checking? \u2022 interaction faults? Etc. Interaction faults: eg, failure occurs if pressure < 10 (1-way <= all-values testing catches) pressure < 10 && volume>300 (2-way <= all-pairs testing catches) Example: Failure when \u201caltitude adjustment set on 0 meters and total flow volume set at delivery rate of less than 2.2 liters per minute.\u201d => 2-way interaction Page 3 Software Failure Internals How does an interaction fault manifest itself in code? Example: altitude_adj == 0 && volume < 2.2 (2-way interaction) if (altitude_adj == 0 ) { // do something if (volume < 2.2) { faulty code! BOOM! } else { good code, no } /!\u2026", "num_citations": "4\n", "authors": ["300"]}
{"title": "Simple tools to automate documentation\n", "abstract": " Simple tools to automate documentation | The Institute of Electrical and Electronics Engineers, Inc on Conference on software maintenance--1985 ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsThe Institute of Electrical and Electronics Engineers, Inc on Conference on software maintenance--1985Simple tools to automate documentation Article Simple tools to automate documentation Share on Authors: DR Kuhn View Profile , CG Hollis View Profile Authors Info & Affiliations Publication: The Institute of Electrical and Electronics Engineers, Inc on Conference on software maintenance--1985January 1986 Pages 203\u2013210 00 \u2026", "num_citations": "4\n", "authors": ["300"]}
{"title": "Browser fingerprinting using combinatorial sequence testing\n", "abstract": " In this paper, we report on the applicability of combinatorial sequence testing methods to the problem of fingerprinting browsers based on their behavior during a TLS handshake. We created an appropriate abstract model of the TLS handshake protocol and used it to map browser behavior to a feature vector and use them to derive a distinguisher. Using combinatorial methods, we created test sets consisting of TLS server-side messages as sequences that are sent to the client as server responses during the TLS handshake. Further, we evaluate our approach with a case study showing that combinatorial properties have an impact on browsers' behavior.", "num_citations": "3\n", "authors": ["300"]}
{"title": "Cyberthreats and Security\n", "abstract": " Successfully addressing the cybersecurity needs of new technologies is not an easy task, but advances in data analytics, forensics, threat modeling, and other techniques presented in this special issue on Cyberthreats and Security can help us meet the challenge.", "num_citations": "3\n", "authors": ["300"]}
{"title": "Combinatorial Testing for Cybersecurity and Reliability\n", "abstract": " This bulletin focuses on NIST's combinatorial testing work. Combinatorial testing is a proven method for more effective software testing at lower cost. The key insight underlying combinatorial testing's effectiveness resulted from a series of studies by NIST from 1999 to 2004. NIST research showed that most software bugs and failures are caused by one or two parameters, with progressively fewer by three or more.", "num_citations": "3\n", "authors": ["300"]}
{"title": "Computer Security\n", "abstract": " Computer Security Page 1 Computer Security Chapter 4 Access Control Prof. Junbeom Hur School of Computer Science and Engineering Chung-Ang University Page 2 Outline \u25cf Access control principles \u25cf Subjects, objects, and access rights \u25cf Discretionary access control \u25cf Example: Unix file access control \u25cf Role-based access control Page 3 Access Control ITU-T Recommendation X.800 defines access control as follows: \u201cThe prevention of unauthorized use of a resource, including the prevention of use of a resource in an unauthorized manner.\u201d Page 4 Access Control Principles RFC 2828 defines computer security as: \u201cMeasures that implement and assure security services in a computer system, particularly those that assure access control service\u201d \u25cf In a broad sense, all of computer security is concerned with access control \u25cf Access control implements \u25cf Security policy that specifies who or what may have \u2026", "num_citations": "3\n", "authors": ["300"]}
{"title": "Applying combinatorial testing to the siemens suite\n", "abstract": " Combinatorial testing has attracted a lot of attention from both industry and academia. A number of reports suggest that combinatorial testing can be effective for practical applications. However, there are few systematic, controlled studies on the effectiveness of combinatorial testing. In particular, input parameter modeling is a key step in the combinatorial testing process. But most studies do not report the details of the modeling process. In this paper, we report an experiment that applies combinatorial testing to the Siemens suite. The Siemens suite has been used as a benchmark to evaluate the effectiveness of many testing techniques. Each program in the suite has a number of faulty versions. The effectiveness of combinatorial testing is measured in terms of the number of faulty versions that are detected. The experimental results show that combinatorial testing is effective in terms of detecting most of the faulty versions with a small number of tests. In addition, we report the details of our modeling process, which we hope to shed some lights on this critical, yet often ignored step, in the combinatorial testing process.", "num_citations": "3\n", "authors": ["300"]}
{"title": "A small-sample, bi-directional scattering measurement system from 200-500 GHz\n", "abstract": " Beginning the fall of 2012, NIST will be providing scattering measurements for other government agencies. We present performance results of a bi-directional scattering measurement system in the 200-500 GHz range. The goal is to provide dense-spectrum, bi-directional reflectance distribution function (BRDF) of sample materials and small objects that can be propagated into detection models and used as standard materials to compare performance of various detection and imaging systems. Our system is built upon a commercial frequency-domain, vector network analyzer system. The system is designed to minimize drift due to movement and temperature changes. The initial data, presented here, of reflectance from a variety of standard targets and sample materials show operation from 200-500 GHz and highlight stability, repeatability, and dynamic range of the system.", "num_citations": "3\n", "authors": ["300"]}
{"title": "Measuring combinatorial coverage of system state-space for IV&V\n", "abstract": " This report describes some measures of combinatorial coverage that can be helpful in estimating this risk that we have applied to tests for spacecraft software but have general application to any combinatorial coverage problem. This method will be illustrated through a prior application to NASA spacecraft software, preliminary results on more recent NASA software, and non-NASA software.", "num_citations": "3\n", "authors": ["300"]}
{"title": "Comparison of texture in copper and aluminum thin films determined by XRD and EBSD\n", "abstract": " X-ray diffraction (XRD) and electron backscatter diffraction (EBSD) are commonly used to perform texture analysis of thin films. However, due to principle differences in data acquisition these techniques can yield disagreeing results. In this paper, we aim to highlight possible error sources with given examples for aluminum and copper thin films.", "num_citations": "3\n", "authors": ["300"]}
{"title": "Characterizing the risks and costs of BGP insecurity/security\n", "abstract": " We examine the performance of multimodal biometric authentication systems using state-of-the-art Commercial Off-the-Shelf (COTS) fingerprint and face biometric systems on a population approaching 1,000 individuals. Majority of prior studies of multimodal biometrics have been limited to relatively low accuracy non-COTS systems and populations of a few hundred users. Our work is the first to demonstrate that multimodal fingerprint and face biometric systems can achieve significant accuracy gains over either biometric alone, even when using highly accurate COTS systems on a relatively large-scale population. In addition to examining well-known multimodal methods, we introduce new methods of normalization and fusion that further improve the accuracy.", "num_citations": "3\n", "authors": ["300"]}
{"title": "Proposal for Fast-Tracking NIST Role-Based Access Control Standard\n", "abstract": " Background\u2022 Since 1995 vendors, users, and researchers have gathered on an annual basis to present papers and discuss issues related to RBAC, in a formal ACM workshop setting", "num_citations": "3\n", "authors": ["300"]}
{"title": "Software quality lessons from medical device failure data\n", "abstract": " Most complex systems today contain software, and systems failures activated by software faults can provide lessons for software development practices and software quality assurance. This report presents an analysis of 342 software-related failures of medical devices that caused no death or injury but led to recalls by the manufacturers. The analysis categorizes the failures by their symptoms and faults. Tables provide methods for preventing and detecting the faults. The nature of the faults provides lessons about the value of generally accepted quality practices for prevention and detection methods applied prior to system release. It also provides some insight into the need for formal requirements specification and for improved testing of complex hardware-software systems.", "num_citations": "3\n", "authors": ["300"]}
{"title": "Report on a Workshop on the Assurance of High Integrity Software\n", "abstract": " This paper provides information about the National Institute of Standards and Technology (NIST) effort to produce a comprehensive set of standards and guidelines for the assurance of high integrity software. In particular, the paper presents the results of a Workshop on the Assurance of High Integrity Software held at NIST on January 22-23, 1991.The NIST Workshop on the Assurance of High Integrity Software on January 22-23, 1991, involved a broad community of interested parties in the development of guidance for assuring high integrity software. The purpose of this, and future, workshops is to provide a forum for the discussion of technical issues and activities that NIST plans to undertake in the development of guidance. Participants in workshops will be asked to comment technical contributions as these evolve. on", "num_citations": "3\n", "authors": ["300"]}
{"title": "A new approach to the measurement of pulp consistency\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "3\n", "authors": ["300"]}
{"title": "Education standards for critical care nursing: conceptual framework.\n", "abstract": " AACN exists to promote the health and welfare of mankind by advancing the science and art of critical care nursing. One of AACN's long-range goals is to promote educational standards for critical care nursing. As a prelude to dissemination of AACN's Education Standards for Critical Care Nursing, this article presented the conceptual framework used to develop this set of standards.", "num_citations": "3\n", "authors": ["300"]}
{"title": "Mikrochemie\n", "abstract": " Mikroehemie~). 2. Qualitative und quantitative Analyse. Der Schwefelkohlenstoffnachweis yon N. Tischler~), der auf Bildung des braunen Cu-Di~ thyldithiocarbamats aus CS2, Di~ thylamin und Cu-Acetat beruht, wurde durch das DRP 216463 yore i9, I. 19087) und dureh T. Callans und JAR Hendersons s) Cu-Naehweis angeregt.--Reagenzien: a) LOsung yon I Vol.-~ o Di~ tthylamin in J~ thylalkohol, b) LOsung yon 0, 05 Gew.-~ o Cu-Acetat puriss, in abs. Alkohol.--Nachweis:~ ccm Dii~ thylaminlOsung und 5 Tropfen Cu-AeetatlOsung werden mit i ccm der ProbelOsung versetzt. Farblose LOsungen in Aceton, Chloroform,~ ther und Alkohol geben bei einer CS~-Konzentration yon 1: t00000 goldgelbe F/~ rbung, bei t: 500000 ausgesprochene Gelbf/irbung, bei t: 1000000 schwachen, aber merkliehen Farbton. In w~ Briger LOsung tritt bei gleicher Empfindliehkeit eine F~ llung auf.--I) imethylsulfid und; 4thylmereaptan\u00a0\u2026", "num_citations": "3\n", "authors": ["300"]}
{"title": "Understanding and fixing complex faults in embedded cyberphysical systems\n", "abstract": " Embedded systems are becoming ubiquitous companions in all our lives. This article reviews the terminology and modern understanding of complex anomalies and state-of-the-art debugging. It details sophisticated omniscient debugging and runtime verification and describes a novel technique to combine the benefits of those processes.", "num_citations": "2\n", "authors": ["300"]}
{"title": "Systematic software testing of critical embedded digital devices in nuclear power applications\n", "abstract": " While design assurance and testing methods for safety-critical systems have been widely researched and studied for years across a number of industry domains, there are few efforts reported in the literature on the actual application of software testing methods to nuclear power digital I&C systems or devices. We see this as a gap in the knowledge basis. The motivation for this research was to investigate the efficacy and challenges that arise when planning, automating and conducting systematic software testing on actual real-time embedded digital devices. In this paper, we present results on the application of a systematic testing methodology called Pseudo-Exhaustive testing. The systematic testing methods were applied at the unit and module integration levels of the software. The findings suggest that Pseudo Exhaustive testing supported by automated testing technology is an effective approach to testing real\u00a0\u2026", "num_citations": "2\n", "authors": ["300"]}
{"title": "Effectiveness of dataset reduction in testing machine learning algorithms\n", "abstract": " Many machine learning algorithms examine large amounts of data to discover insights from hidden patterns. Testing these algorithms can be expensive and time-consuming. There is a need to speed up the testing process, especially in an agile development process, where testing is frequently performed. One approach is to replace big datasets with smaller datasets produced by random sampling. In this paper, we report a set of experiments that are designed to evaluate the effectiveness of using reduced datasets produced by random sampling for testing machine learning algorithms. In our experiments, we use as subject programs four supervised learning algorithms from the Waikato Environment for Knowledge Analysis (WEKA). We identify five datasets from Kaggle.com to run with the four learning algorithms. For each dataset, we generate reduced datasets of different sizes using two random sampling\u00a0\u2026", "num_citations": "2\n", "authors": ["300"]}
{"title": "Leading-Edge Technologies\n", "abstract": " The articles in this special section focus on new and emerging leading edge technologies for information technology (IT) systems. It usually seems that the pace of change in computing is ever-increasing. A quasi-formal statement of this progress is of course Moore\u2019s Law. Originally, an empirical observation was that the number of transistors in an integrated circuit doubles roughly every two years; this observation is now often a general expression of the exponential rate of progress in information technology (IT). For the past few decades, IT progress has been primarily in the speed and capacity of processors and data storage, but qualitative changes are happening as well\u2014and these changes are moving rapidly into the consumer space.\u201cSelf-driving\u201d cars are the talk of the automotive industry, and intelligent assistants such as Amazon\u2019s Alexa and Apple\u2019s Siri are incorporated into everyday life in cell phones and\u00a0\u2026", "num_citations": "2\n", "authors": ["300"]}
{"title": "Detecting Vulnerabilities in Android Applications using Event Sequences\n", "abstract": " Sequence covering arrays have demonstrated their usefulness for finding software bugs that propagate via some sequence of events. However, the distribution of t-way event sequence failures has never been reported, and as a result, the practicality of using these methods is not fully known. In this paper, our analysis of the distribution of t-way interactions between events in event sequence bugs provides insight into the practicality and usefulness of this combinatorial testing method. From a developer's perspective, these methods can contribute to finding this particular class of bugs early in the software development process, saving the developers time and money without sacrificing effectiveness. However, an attacker may also leverage these techniques to discover previously undetected vulnerabilities as a means to exploit the system. This work involved analyzing hundreds of vulnerability reports, performing\u00a0\u2026", "num_citations": "2\n", "authors": ["300"]}
{"title": "Towards an automated unified framework to run applications for combinatorial interaction testing\n", "abstract": " Combinatorial interaction testing (CIT) is a well-known technique, but the industrial experience is needed to determine its effectiveness in different application domains. We present a case study introducing a unified framework for generating, executing and verifying CIT test suites, based on the open-source Avocado test framework. In addition, we present a new industrial case study to demonstrate the effectiveness of the framework. This evaluation showed that the new framework can generate, execute, and verify effective combinatorial interaction test suites for detecting configuration failures (invalid configurations) in a virtualization system.", "num_citations": "2\n", "authors": ["300"]}
{"title": "IoT Metrology\n", "abstract": " Measurement has its own field in science: metrology. Measurement can be used in determining what we currently have and what we can expect in the future based on what we have. The first is generally easier to measure. For example, we can count the coffee beans in a bag to determine what we have. But knowing whether those beans will result in a good cup of coffee once the beans are ground is a different question, unless there are almost no beans in the bag.Estimation and prediction naturally use numerical measures. Estimation tells you approximately what you have today with respect to a fixed environment and context. For example, a system might be estimated to be 99 percent reliable today. Note the key word is today, a point in time. In comparison, a prediction would say something like,\u201cBased on an estimate of 99 percent reliability today, we believe it will also be 99 percent reliable tomorrow, but after tomorrow, the reliability might change.\u201d Why? As time moves forward, components might wear out, thus reducing overall system reliability. And as time moves forward, the environment might change so that the system is under less stress, thus increasing the predicted reliability.", "num_citations": "2\n", "authors": ["300"]}
{"title": "Combinatorial Coverage Measurement of Test Vectors used in Cryptographic Algorithm Validation\n", "abstract": " Combinatorial Coverage Measurement of Test Vectors used in Cryptographic Algorithm Validation (Presentation) Page 1 Combinatorial Coverage Measurement of Test Vectors used in Cryptographic Algorithm Validation Dimitris Simos1, Stavros Mekesis1, D. Richard Kuhn2, Raghu N. Kacker2 SBA Research, Vienna, Austria; dsimos, smekesis@sba-research.org1 National Institute of Standards & Technology, Gaithersburg, MD, USA; kuhn, raghu.kacker@nist.gov2 Page 2 Overview \u2022 Measuring the combinatorial coverage of test vectors used in the Cryptographic Algorithm Validation Program (CAVP). \u2022 Using differential testing and golden model testing in tandem to identify incorrect behavior in the context of multiple AES implementations tested together. Page 3 CAVP \u2022 The Cryptographic Algorithm Validation Program (CAVP) encompasses validation testing for FIPS-approved and NIST recommended cryptographic . (\u2026", "num_citations": "2\n", "authors": ["300"]}
{"title": "General methods for access control policy verification (application paper)\n", "abstract": " Access control systems are among the most critical of computer security components. Faulty policies, misconfigurations, or flaws in software implementations can result in serious vulnerabilities. To formally and precisely capture the security properties that access control should adhere to, access control models are usually written, bridging the gap in abstraction between policies and mechanisms. Identifying discrepancies between policy specifications and their intended function is crucial because correct implementation and enforcement of policies by applications is based on the premise that the policy specifications are correct. As a result, policy specifications represented by models must undergo rigorous verification and validation through systematic verification and testing to ensure that the policy specifications truly encapsulate the desires of the policy authors. Verifying the conformance of access control policies\u00a0\u2026", "num_citations": "2\n", "authors": ["300"]}
{"title": "A rational foundation for software metrology\n", "abstract": " Much software research and practice involves ostensible measurements of software, yet little progress has been made on an SI-like metrological foundation for those measurements since the work of Gray, Hogan, et al. in 1996-2001. Given a physical object, one can determine physical properties using measurement principles and express measured values using standard quantities that have concrete realizations. In contrast, most software metrics are simple counts that are used as indicators of complex, abstract qualities. In this report we revisit software metrology from two directions: first, top down, to establish a theory of software measurement; second, bottom up, to identify specific purposes for which software measurements are needed, quantifiable properties of software, relevant units, and objects of measurement. Although there are structural obstacles to realizing the vision of software metrology that works like\u00a0\u2026", "num_citations": "2\n", "authors": ["300"]}
{"title": "Combinatorial Testing: Theory and Practice, Section 8.\n", "abstract": " Additional Section to PUB ID 918448. Combinatorial testing has rapidly gained favor among software testers in the past decade as improved algorithms have become available, and practical success has been demonstrated. This article reviews the theory and application of this method, focusing particularly on research since 2010, with a brief background providing the rationale and development of combinatorial methods for software testing. Significant advances have occurred in algorithm performance, and the critical area of constraint representation and processing. In addition to these foundational topics, we take a look at advances in specialized areas including test suite prioritization, sequence testing, fault localization, the relationship between combinatorial testing and structural coverage, and approaches to very large testing problems.", "num_citations": "2\n", "authors": ["300"]}
{"title": "Efficient verification of equivalence classes and simultaneous testing using two-layer covering arrays\n", "abstract": " This short paper introduces a method for verifying equivalence classes for module/unit testing. This is achieved using a two-layer covering array, in which some or all values of a primary covering array represent equivalence classes. A second layer covering array of the equivalence class values is computed, and its values substituted for the equivalence class names in the primary array. It is shown that this method can also detect certain classes of errors without a conventional test oracle, and an illustrative example is given.", "num_citations": "2\n", "authors": ["300"]}
{"title": "Introduction: Cybersecurity\n", "abstract": " Enterprise security, often considered a burden for system administrators and users alike, is one of the most rapidly evolving areas of IT. The articles in this issue can help IT professionals who want to be intelligent providers or consumers of secure products and services.", "num_citations": "2\n", "authors": ["300"]}
{"title": "SP 800-58. Security Considerations for Voice Over IP Systems\n", "abstract": " Voice over Internet Protocol (VOIP) refers to the transmission of speech across data-style networks. This form of transmission is conceptually superior to conventional circuit switched communication in many ways. However, a plethora of security issues are associated with still-evolving VOIP technology. This publication introduces VOIP, its security challenges, and potential countermeasures for VOIP vulnerabilities.", "num_citations": "2\n", "authors": ["300"]}
{"title": "PBX Vulnerability Analysis: Finding Holes in Your PBX Before Someone Else Does. Computer Security\n", "abstract": " This report presents a generic methodology for conducting an analysis of a Private Branch Exchange (PBX) in order to identify security vulnerabilities. The report focuses on digital-based PBXs and addresses the following areas for study: System Architecture; Hardware; Maintenance; Administrative Database/Software; User Features. The methods described in this report are designed to assist administrators in conducting this type of testing. Computer based telephony systems and new techniques such as voice over IP (VoIP) present an entirely new collection of vulnerabilities and are not addressed in this report. However, some of the evaluation methods described here may be applied to these systems as well.", "num_citations": "2\n", "authors": ["300"]}
{"title": "Fault classes and error detection in specification based testing\n", "abstract": " Specification based testing relies upon methods for generating test cases from predicates in a software specification.[1][2][3][4][5][6][7].", "num_citations": "2\n", "authors": ["300"]}
{"title": "Combinatorial Testing Metrics for Machine Learning\n", "abstract": " This paper defines a set difference metric for comparing machine learning (ML) datasets and proposes the difference between datasets be a function of combinatorial coverage. We illustrate its utility for evaluating and predicting performance of ML models. Identifying and measuring differences between datasets is of significant value for ML problems, where the accuracy of the model is heavily dependent on the degree to which training data are sufficiently representative of data encountered in application. The method is illustrated for transfer learning without retraining, the problem of predicting performance of a model trained on one dataset and applied to another.", "num_citations": "1\n", "authors": ["300"]}
{"title": "Combinatorially XSSing Web Application Firewalls\n", "abstract": " Cross-Site scripting (XSS) is a common class of vulnerabilities in the domain of web applications. As it re-mains prevalent despite continued efforts by practitioners and researchers, site operators often seek to protect their assets using web application firewalls (WAFs). These systems employ filtering mechanisms to intercept and reject requests that may be suitable to exploit XSS flaws and related vulnerabilities such as SQL injections. However, they generally do not offer complete protection and can often be bypassed using specifically crafted exploits. In this work, we evaluate the effectiveness of WAFs to detect XSS exploits. We develop an attack grammar and use a combinatorial testing approach to generate attack vectors. We compare our vectors with conventional counterparts and their ability to bypass different WAFs. Our results show that the vectors generated with combinatorial testing perform equal or better\u00a0\u2026", "num_citations": "1\n", "authors": ["300"]}
{"title": "A Combinatorial Approach to Testing Deep Neural Network-based Autonomous Driving Systems\n", "abstract": " Recent advancements in the field of deep learning have enabled its application in Autonomous Driving Systems (ADS). A Deep Neural Network (DNN) model is often used to perform tasks such as pedestrian detection, object detection, and steering control in ADS. Unfortunately, DNN models could exhibit incorrect or unexpected behavior in real-world scenarios. There is a need to rigorously test these models with real-world driving scenarios so that safety-critical bugs can be detected before their deployment in the real world.In this paper, we propose a combinatorial approach to testing DNN models. Our approach generates test images by applying a set of combinations of some basic image transformation operations to a seed image. First, we identify a set of valid transformation operations or simply transformations. Next, we design an input parameter model based on the valid transformations and generate a t-way\u00a0\u2026", "num_citations": "1\n", "authors": ["300"]}
{"title": "Factorials Experiments, Covering Arrays, and Combinatorial Testing\n", "abstract": " In the twenty-first century, our life will increasingly depend on software-based products and complex interconnected systems. Thus, the quality and security of software-based systems is a world-wide concern. Combinatorial testing is a versatile methodology for finding errors (bugs) and vulnerabilities in software-based systems. This paper offers a review of combinatorial testing. Combinatorial testing (CT) methods evolved from investigations which looked like factorial experiments (FE) with pass/fail outcomes. We will discuss the similarities and differences between FE and CT. Use of CT for detecting errors (bugs) in software-based systems has gained significant interest from the international software testing community. Many successful results have been reported from the use of CT to detect software errors in aerospace, automotive, defense, cybersecurity, electronic medical systems, and financial service industries\u00a0\u2026", "num_citations": "1\n", "authors": ["300"]}
{"title": "Aggregating atomic clocks for time-stamping\n", "abstract": " A timestamp is a critical component in many applications, such as proof of transaction ordering or analyzing algorithm performance. This paper reports on a method called Verified Timestamping (VT) that improves the standard timestamp protocol. VT was developed at the National Institute of Standards and Technology (NIST) for use in algorithms where timestamp accuracy is critical. VT is an aggregation of the outputs from various atomic clocks to create a Timestamping Authority (TsA). The motivation for this research effort included malicious delay issues in Networks of Things [NIST SP 800-183] as well as race conditions associated with the inclusion of new blocks into blockchains. This paper presents the TsA design and the results of VT, which indicate that atomic clock aggregation is not only possible, but a viable means to produce higher integrity timestamps at the ms level of performance. Tests showed that this\u00a0\u2026", "num_citations": "1\n", "authors": ["300"]}
{"title": "Low-Power Wide Area Networks (LPWAN) for Communications of Mobile Sensor Data\n", "abstract": " There are multiple options for communication of data to and from mobile sensors. For tracking systems, Global Navigation Satellite System (GNSS) is often used for localization and mobile-phone technologies are used for transmission of data. Low-power wide area networks (LPWAN) is a newer option for sensor networks including mobile sensors.", "num_citations": "1\n", "authors": ["300"]}
{"title": "Taking score on the success of blockchain, so far\n", "abstract": " DYLAN YAGA: It\u2019s difficult to do this as a blanket statement, since it\u2019s really on a per-implementation basis. For some, who have slow manual processes, these technologies could be faster and cheaper but not necessarily better since they ultimately achieve the same result. For some who do not have a lot of intermediaries to go through, they may be faster and better but not necessarily cheaper. The costs depend on the fees and agreements. For others, they could be better and cheaper, but if it uses a deliberately slow consensus mechanism, the technologies will be slower.ANGELOS STAVROU: They are faster and better but not necessarily cheaper. In general, we have to accept a higher cost to gain the benefits of the other two. Blockchain is not about being cheaper but accomplishing things in a better way among entities that do not trust each other or are even competing.", "num_citations": "1\n", "authors": ["300"]}
{"title": "Knowledge Extraction for Cryptographic Algorithm Validation Test Vectors by Means of Combinatorial Coverage Measurement\n", "abstract": " We present a combinatorial coverage measurement analysis for test vectors provided by the NIST Cryptographic Algorithm Validation Program (CAVP), and in particular for test vectors targeting the AES block ciphers for different key sizes and cryptographic modes of operation. These test vectors are measured and analyzed using a combinatorial approach, which was made feasible via developing the necessary input models. The extracted model from the test data in combination with combinatorial coverage measurements allows to extract information about the structure of the test vectors. Our analysis shows that some test sets do not achieve full combinatorial coverage. It is further discussed, how this retrieved knowledge could be used as a means of test quality analysis, by incorporating residual risk estimation techniques based on combinatorial methods, in order to assist the overall validation testing\u00a0\u2026", "num_citations": "1\n", "authors": ["300"]}
{"title": "Attribute Considerations for Access Control Systems\n", "abstract": " This document provides federal agencies with a guide for implementing attributes in access control systems. Attributes enable a logical access control methodology where authorization to perform a set of operations is determined by evaluating attributes associated with the subject, object, requested operations, and, in some cases, environment conditions against policy, rules, or relationships that describe the allowable operations for a given set of attributes. This document outlines factors which influence attributes that an authoritative body must address when standardizing an attribute system and proposes some notional implementation suggestions for consideration.", "num_citations": "1\n", "authors": ["300"]}
{"title": "Measuring Combinatorial Coverage at Adobe\n", "abstract": " Adobe offers an analytics product as part of the Marketing Cloud software with which customers can track many details about users across various digital platforms. For the most part, customers define the amount and type of data to track. In addition, customers can specify many feature combinations when reporting on this data. These features create high dimensionality that makes validation challenging for some of the most critical components of the Adobe Analytics product. One of these critical components is the reporting engine. This component has a validation framework often qualitatively considered within the engineering organization as highly effective. However, the effectiveness of this framework has never been quantitatively measured. Due to recent applications of combinatorial testing, the Analytics Tools team determined to use combinatorial coverage measurements (CCM) to evaluate the effectiveness of\u00a0\u2026", "num_citations": "1\n", "authors": ["300"]}
{"title": "Using Parameter Mapping to Avoid Forbidden Tuples in a Covering Array\n", "abstract": " This paper addresses an optimization problem that occurs when we try to remove from a covering array (CA) the rows that do not satisfy a given set of constraints. That is, how to minimize the number of rows to be removed? The key observation is that the columns of a CA can be swapped without affecting coverage. This makes it possible to explore different ways to map the parameters involved in the constraints to the columns in the CA, which further allows us to reduce the number of rows that must be removed. In order to find an optimal mapping, our approach maps one parameter at a time and employs a greedy algorithm that tries to minimize the number of rows to be removed at each step. We report several experiments in which we compared our approach to two other approaches, i.e., the identity-based approach, and the random approach. The results show that our approach can remove fewer rows than the\u00a0\u2026", "num_citations": "1\n", "authors": ["300"]}
{"title": "A Method-Level Test Generation Framework for Debugging Big Data Applications\n", "abstract": " When a failure occurs in a big data application, debugging with the original dataset can be difficult due to the large amount of data being processed. This paper introduces a framework for effectively generating method-level tests to facilitate debugging of big data applications. This is achieved by running a big data application with the original dataset and by recording the inputs to a small number of method executions, which we refer to as method-level tests, that preserve certain code coverage, e.g., edge coverage. The size of each method-level test is further reduced if needed, while maintaining code coverage. When debugging, a developer could inspect the execution of these method-level tests, instead of the entire program execution with the original dataset. We applied the framework to seven algorithms in the WEKA tool. The initial results show that in many cases a small number of method-level tests are\u00a0\u2026", "num_citations": "1\n", "authors": ["300"]}
{"title": "Pseudo-Exhaustive Verification of Rule Based Systems.\n", "abstract": " Rule-based systems are important in application domains such as artificial intelligence and business rule engines. When translated into an implementation, simple expressions in rules may map to a large body of code that requires testing. We show how rule-based systems may be tested efficiently, using combinatorial methods and a constraint solver in a test method that is pseudo-exhaustive, which we define as exhaustive testing of all combinations of variable values on which a decision is dependent. The method has been implemented in a tool that can be applied to testing and verification for a wide range of applications.", "num_citations": "1\n", "authors": ["300"]}
{"title": "A Data Structure for Integrity Protection with Erasure Capability (DRAFT)\n", "abstract": " This note describes a data structure, which can be referred to as a block matrix, that supports the ongoing addition of hash-linked records while also allowing the deletion of arbitrary records, preserving hashbased integrity assurance that other blocks are unchanged. The block matrix data structure may have utility for incorporation into applications requiring integrity protection that currently use permissioned blockchains. This capability could for example be useful in meeting privacy requirements such as the European Union General Data Protection Regulation (GDPR), which requires that organizations make it possible to delete all information related to a particular individual, at that person's request.", "num_citations": "1\n", "authors": ["300"]}
{"title": "Educating Next-Gen Computer Scientists\n", "abstract": " Six panelists (Phillip A. Laplante, Michael Lewis, Keith Miller, Jeff Offutt, Jon George Rokne, and Shiuhpyng Shieh) debate whether university computer science education is leading technology forward, or commercial technology demands are leaving these programs in the dust.", "num_citations": "1\n", "authors": ["300"]}
{"title": "A Model for T-Way Fault Profile Evolution during Testing\n", "abstract": " Empirical studies have shown that most software interaction faults involve one or two variables interacting, with progressively fewer triggered by three to six variables interacting. This paper introduces a model for the origin of this distribution. We start with two empirically reasonable assumptions regarding the distribution of branch conditions in code and the proportion of t-way combinations seen in testing, and show that the model closely reproduces empirical data on t-way fault distributions. Although the model was developed to explain the distribution of faults by t-way interaction strength, it is shown to reproduce the basic exponential reliability model as a special case at each level of interaction, t. The paper evaluates model predictions against empirical data, and discusses implications for detection and removal of interaction faults.", "num_citations": "1\n", "authors": ["300"]}
{"title": "IT Pro Conference on Information Systems Governance\n", "abstract": " Approximately 100 IT professionals participated in the 2014 IT Pro Conference on Information Systems Governance, held at the National Institute of Standards and Technology on 22 May 2014 (www.computer.org/itproconf). The conference was designed to bring together IT professionals from industry, government, and academia to explore new challenges in information systems.", "num_citations": "1\n", "authors": ["300"]}
{"title": "Introducing combinatorial testing in large organizations\n", "abstract": " \u2022 Results:\u201cOur initial estimate is that this method supported by the technology can save up to 20% of test planning/design costs if done early on a program while increasing test coverage by 20% to 50%.\u201d", "num_citations": "1\n", "authors": ["300"]}
{"title": "IT Risks [Guest editors' introduction]\n", "abstract": " Most IT professionals would agree that IT is good at identifying and managing risks--but is this really the case? Or has risk management become simply buzz word for us? This issue of IT Pro highlights some risk areas often overlooked. As IT professionals, we talk about risk management mitigations, write risk management plans, and use these terms in our testing strategies, but we often overlook risks in areas outside of normal project development.", "num_citations": "1\n", "authors": ["300"]}
{"title": "Firebrands Generated from Full-Scale Building Components and Structures Under and Applied Wind Field\n", "abstract": " Firebrand production from a real-scale structure under well-controlled laboratory conditions was investigated. The structure was fabricated using wood studs and oriented strand board (OSB). A sofa was placed inside the structure and ignited using a remotely controlled electric match. The door opening was sized to allow flashover to occur inside the structure. The entire structure was placed inside the Building Research Institute\u2019s (BRI) Fire Research Wind Tunnel Facility (FRWTF) in Japan to apply a wind field of 6 m/s onto the structure. As the structure burned, firebrands were collected using an array of water pans. The size and mass distributions of firebrands collected in this study were compared with firebrand generation data from actual full-scale structure burns, individual building component tests, and historical structure fire firebrand generation studies. The methodology presented here provides a framework to study firebrand generation from burning structures under well-controlled laboratory conditions while affording the ability to capture the important nature of realistic scales critical to the firebrand production process. The paper closes with recent data that considers the influence of siding treatments on firebrand generation from individual building components.", "num_citations": "1\n", "authors": ["300"]}
{"title": "Generating cell co-cultures by rapid cell adhesion on opposite sides of polyester membranes\n", "abstract": " A cell co-culture was generated on opposite sides of a permeable polyester membrane (PET) membrane by combining dielectrophoretic (DEP) and electrostatic forces to hold cells against gravity in a multilayer microfluidic device (Figure 1). HepG2 cells were trapped underneath the PET membrane (bottom channel) when DEP attractive forces were applied. Cells immediately adhered on the PET membrane by the electrostatic forces from a hybrid cell adhesive material (hCAM) deposited on the surface of the electrodes. NIH-3T3 cells adhered on the top channel via the hCAM deposited on that side of the membrane. Cells on both sides of the membrane remained attached and viable at least 24 h after the co-culture was generated.", "num_citations": "1\n", "authors": ["300"]}
{"title": "Authentication, Authorization, Access Control, and Privilege Management\n", "abstract": " Authentication is the process of verifying the identity or authority of a network or system user (which can be a human user, or a computer\u2010based process or device) through a secure means, such as digital signatures, passwords, tokens, or biometric features. Authorization, which takes place after authentication, refers to the privileges granted to an authenticated user who has requested access to services or resources. The concepts of authentication and authorization are interdependent; authorization to use a network or system resource frequently includes establishing the identity of the user requesting access or verifying that a trusted third party has certified that the user is entitled to the access requested. Privilege is a security attribute shared by users whose identities have been authenticated. Access control and privilege management begin with the administrative and mechanical process of defining, enabling, and\u00a0\u2026", "num_citations": "1\n", "authors": ["300"]}
{"title": "SP 800-54. Border Gateway Protocol Security\n", "abstract": " This document introduces the Border Gateway Protocol (BGP), explains its importance to the internet, and provides a set of best practices that can help in protecting BGP. Best practices described here are intended to be implementable on nearly all currently available BGP routers. While a number of enhanced protocols for BGP have been proposed, these generally require substantial changes to the protocol and may not interoperate with current BGP implementations. To improve the security of BGP routers, the recommendations listed below are introduced. While the recommendations can contribute to greatly improved BGP security, they are not a complete defense against all threats. Security administrators and decision makers should select and apply these methods based on their unique needs.", "num_citations": "1\n", "authors": ["300"]}
{"title": "CrossTalk: The Journal of Defense Software Engineering. Volume 19, Number 6\n", "abstract": " Social and Technical Reasons for Software Project Failures, by Capers Jones--Applying a careful program of risk analysis and risk abatement can lower the effects of the technical and social issues that handicap projects and lower the probability of major software disasters. What Weve Got Here Is... Failure to Communicate, by Alan C. Jost--The failure to communicate is the root problem of more program failures than we allow ourselves to believe. This article uses the famous line from the movie Cool Hand Luke, What weve got here is... failure to communicate, to illustrate communication failures and successes. Knowledge The Core Problem of Project Failure, by Timothy K. Perkins--This author contends that knowledge is the most common cause of project failures either project managers do not have enough of it, or they are not using the knowledge they do have correctly. Start With Simple Earned Value on All Your Projects, by Quentin W. Fleming and Joel M. Koppelman--The authors show that by only implementing 10 of the 32 American National Standards InstituteElectronic Industries Alliances criteria to all projects, Earned Value Management can be achieved. Statistical Methods Applied to EVM The Next Frontier, by Walt Lipke--Earned Value Management EVM has brought science to management projects, and this article describes the elements necessary for performing statistical analysis in association with EVM. Defining Short and Usable Processes, by Timothy G. Olson--This article describes common problems with process documentation, discusses best practices for defining short and usable processes and procedures, describes success\u00a0\u2026", "num_citations": "1\n", "authors": ["300"]}
{"title": "Security Standards for the RFID Market\n", "abstract": " Emerging Standards many ranchers now use implantable low frequency (LF) tags enclosed in glass capsules because they\u2019re less susceptible to attenuation from water and living tissue than ultrahigh frequency (UHF) tags. Such a tracking application requires RFID tags that can be securely attached or implanted in the animals, read through the skin from a relatively close distance to minimize multiple reads, and have limited onboard storage. In most cases, these requirements narrow the RFID tag selection down to a single product category\u2014an encapsulated, LF transponder that has limited read/write capability as specified by the ISO 11784 and 11785 standards. ISO 11784/5 transponders allow easy reprogramming of ID codes, however, and therefore provide insufficient security for applications such as tracking endangered species or high-value show animals. Standards for animal-tracking RFID tags that meet\u00a0\u2026", "num_citations": "1\n", "authors": ["300"]}
{"title": "Autonomous System Isolation under BGP Session Attacks with RFD Exploitation\n", "abstract": " There is a growing apprehension in the Internet community that there are potentially significant vulnerabilities in the deployed Border Gateway Protocol (BGP) routing system. Researchers speculate and debate the potential of targeted attacks to trigger large scale, potentially cascading, failures and persistent instability in the global routing system. To date, most modeling and analysis of BGP behavior under threatening scenarios has been limited to post mortem analysis of global routing exchanges during worm and virus attacks of Internet hosts; but these are not attacks focused on BGP. In this paper, we present results from our effort to conduct \u201cwhat if\u201d analyses of yet unseen attacks and to develop means to characterize the impact of various attacks on a distributed BGP routing system. In particular, we present a detailed study of the impact of BGP peering session attacks and the resulting exploitation of RFD that cause network-wide routing disruptions. Analytical results provide insights into the nature of the problem and impact of the attacks. Detailed packet level simulation results complement the analytical results and provide many useful insights as well. We also quantify the effect of BGP Graceful Restart mechanism on partial mitigation of the BGP vulnerability to peering session attacks.", "num_citations": "1\n", "authors": ["300"]}
{"title": "Securing Voice Over IP Networks\n", "abstract": " Voice over IP-the transmission of voice over traditional packet-switched IP networks-is one of the hottest trends in telecommunications. As with any new technology, VOIP introduces both opportunities and problems. Lower cost and greater flexibility are among the promises of VOIP for the enterprise, but security administrators will face significant challenges. Administrators may assume that since digitized voice travels in packets, they can simply plug VOIP components into their already-secured networks. Unfortunately, many of the tools used to safeguard today's computer networks, namely firewalls, Network Address Translation (NAT), and encryption, carry a hefty price when incorporated into a VOIP network. This paper introduces the security issues with VOIP and outlines steps that can be taken to operate a VOIP system securely.", "num_citations": "1\n", "authors": ["300"]}
{"title": "Panel: The next generation of acess control models (panel session) do we need them and what should they be?\n", "abstract": " Research on access control models was started in the 1960s and 1970s by the two thrusts of mandatory and discretionary access control. Mandatory access control (MAC) came from the military and national security arenas whereas discretionary access control (DAC) had its roots in academic and commercial research laboratories. These two thrusts were dominant through the 1970s and 1980s almost to exclusion of any other approach to access control models. In the 1990s we have seen a dramatic shift towards pragmatism. The dominant access-control model of the 1990s is role-based access control (RBAC). It is now understood that RBAC encompasses MAC and DAC as special cases and goes beyond them in providing a policy-neutral framework. This SACMAT meeting has evolved from a highly successful and productive series of ACM workshops on RBAC. This panel will address the basic question of\u00a0\u2026", "num_citations": "1\n", "authors": ["300"]}
{"title": "Converting System Failure Histories into Future Win Situations\n", "abstract": " Henry Petroski devotes an entire book to failures in engineering and lessons to be learned [1]. In his preface, he states\" the concept of failure-mechanical and structural failure in the context of this discussion-is central to understanding engineering, for engineering design has as its first and foremost objective the obviation of failure.\" He further states\" the lessons learned from\u2026 disasters can do more to advance engineering knowledge than all the successful machines and structures in the world.\" We extend Petroski's views from mechanical and structural engineering into the domain of software system failures. Lessons learned can affirm proposed software engineering principles or help define new ones. Several industries, including telecommunications, space, business, and defense, were early drivers of computer technology. Within these, more and more systems are controlled by, or dependent on, software today than in the early years. Examination of software-based failures from many domains yields insight about possible common causes of failures and the means to prevent them in the next system or, at the very least, to detect them before the system is released. The purpose is to reduce costs by finding and detecting problems, that is, faults in any of the system artifacts, before systems are recalled from multiple users.", "num_citations": "1\n", "authors": ["300"]}
{"title": "Security for Private Branch Exchange Systems\n", "abstract": " Unfortunately, the hospital\u2019s experience is not unique. Failure to secure a PBX system can result in exposing an organization to toll fraud, theft of proprietary, personal, and confidential information, loss of revenue, or legal entanglements. Depending on how the organization\u2019s network is configured and administered, information leading to intrusions of data networks may be compromised as well. A PBX is a sophisticated computer-based switch that can be thought of as essentially a small, in-house phone company for the organization that operates it. Protection of the PBX is thus a high priority. This bulletin introduces some of the vulnerabilities of PBX switches and describes some countermeasures that can be used to increase the security of your PBX. For a more detailed treatment of these issues, see NIST Special Publication (SP) 800-24, PBX Vulnerability Analysis (see http://csrc. nist. gov).", "num_citations": "1\n", "authors": ["300"]}
{"title": "Standards for high-integrity software\n", "abstract": " [en] This article describes a study that examines standards, draft standards, and guidelines (all of which will hereafter be referred to as documents) that provide requirements for the assurance of software in safety systems in nuclear power plants. The study focuses on identifying, for developers of standards, the elements to be addressed in a standard for providing reasonable assurance of software in safety systems in nuclear power plants. The documents vary widely in their requirements and the precision with which the requirements are expressed. Recommendations are outlined for guidance for the assurance of high-integrity software. 6 refs., 3 tabs", "num_citations": "1\n", "authors": ["300"]}
{"title": "Proceedings of the Workshop on High Integrity Software, Gaithersburg, MD, Jan. 22-23, 1991\n", "abstract": " Proceedings of the Workshop on High Integrity Software; Gaithersburg, MD; Jan. 22-23, 1991 | NIST Skip to main content US flag An official website of the United States government Here\u2019s how you know Here\u2019s how you know Official websites use .gov A .gov website belongs to an official government organization in the United States. Secure .gov websites use HTTPS A lock ( A locked padlock ) or https:// means you\u2019ve safely connected to the .gov website. Share sensitive information only on official, secure websites. National Institute of Standards and Technology Search NIST Search Menu Close Topics All Topics Advanced communications Artificial intelligence Bioscience Buildings and construction Chemistry Climate Cybersecurity Electronics Energy Environment Fire Forensic science Health Information technology Infrastructure Manufacturing Materials Mathematics and statistics Metrology Nanotechnology Neutron .\u2026", "num_citations": "1\n", "authors": ["300"]}
{"title": "Static Analysis Tools for Software Security certification\n", "abstract": " This paper describes a suite of tools used in evaluating software for security certification. The tools are currently being used on software for secure Electronic Funds Transfer, but could be applied to other applications as well.", "num_citations": "1\n", "authors": ["300"]}
{"title": "Lessons from 342 medical device failures\n", "abstract": " Most complex systems today contain software, and systems failures activated by software faults can provide lessons for software development practices and software quality assurance. This paper presents an analysis of softwarerelated failures of medical devices that caused no death or injury but led to recalls by the manufacturers. The analysis categorizes the failures by their symptoms and faults, and discusses methods of preventing and detecting faults in each category. The nature of the faults provides lessons about the value of generally accepted quality practices for prevention and detection methods applied prior to system release. It also provides some insight into the need for formal requirements specification and for improved testing of complex hardware-software systems.", "num_citations": "1\n", "authors": ["300"]}