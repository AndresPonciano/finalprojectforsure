{"title": "A systematic review of machine learning techniques for software fault prediction\n", "abstract": " BackgroundSoftware fault prediction is the process of developing models that can be used by the software practitioners in the early phases of software development life cycle for detecting faulty constructs such as modules or classes. There are various machine learning techniques used in the past for predicting faults.MethodIn this study we perform a systematic review of studies from January 1991 to October 2013 in the literature that use the machine learning techniques for software fault prediction. We assess the performance capability of the machine learning techniques in existing research for software fault prediction. We also compare the performance of the machine learning techniques with the statistical techniques and other machine learning techniques. Further the strengths and weaknesses of machine learning techniques are summarized.ResultsIn this paper we have identified 64 primary studies and seven\u00a0\u2026", "num_citations": "458\n", "authors": ["555"]}
{"title": "Empirical Study of Object-Oriented Metrics.\n", "abstract": " The increasing importance of software measurement has led to development of new software measures. Many metrics have been proposed related to various constructs like class, coupling, cohesion, inheritance, information hiding and polymorphism. But there is a little understanding of the empirical hypotheses and application of many of these measures. It is often difficult to determine which metric is more useful in which area. As a consequence, it is very difficult for project managers and practitioners to select measures for object-oriented systems. In this paper we investigate 22 metrics proposed by various researchers. The metrics are first defined and then explained using practical applications. They are applied on standard projects on the basis of which descriptive statistics, principal component analysis and correlation analysis is presented. Finally, a review of the empirical study concerning chosen metrics and subset of these measures that provide sufficient information is given and metrics providing overlapping information are excluded from the set.", "num_citations": "258\n", "authors": ["555"]}
{"title": "Empirical validation of object-oriented metrics for predicting fault proneness models\n", "abstract": " Empirical validation of software metrics used              to predict software quality attributes is important to ensure their practical relevance in software organizations. The aim of this work is to find the relation of object-oriented (OO) metrics with fault proneness at different severity levels of faults. For this purpose, different prediction models have been developed using regression and machine learning methods. We evaluate and compare the performance of these methods to find which method performs better at different severity levels of faults and empirically validate OO metrics given by Chidamber and Kemerer. The results of the empirical study are based on public domain NASA data set. The performance of the predicted models was evaluated using Receiver Operating Characteristic (ROC) analysis. The results show that the area under the curve (measured from the ROC analysis) of models predicted using\u00a0\u2026", "num_citations": "230\n", "authors": ["555"]}
{"title": "Empirical analysis for investigating the effect of object\u2010oriented metrics on fault proneness: a replicated case study\n", "abstract": " The importance of software measurement is increasing, leading to the development of new measurement techniques. Many metrics have been proposed related to the various object-oriented (OO) constructs like class, coupling, cohesion, inheritance, information hiding and polymorphism. The purpose of this article is to explore relationships between the existing design metrics and probability of fault detection in classes. The study described here is a replication of an analogous study conducted by Briand et al. The aim is to provide empirical evidence to draw the strong conclusions across studies. We used the data collected from Java applications for constructing a prediction model. Results of this study show that many metrics capture the same dimensions in the metric set, hence are based on comparable ideas and provides redundant information. It is shown that by using a subset of metrics prediction models can\u00a0\u2026", "num_citations": "169\n", "authors": ["555"]}
{"title": "Fault prediction using statistical and machine learning methods for improving software quality\n", "abstract": " An understanding of quality attributes is relevant for the software organization to deliver high software reliability. An empirical assessment of metrics to predict the quality attributes is essential in order to gain insight about the quality of software in the early phases of software development and to ensure corrective actions. In this paper, we predict a model to estimate fault proneness using Object Oriented CK metrics and QMOOD metrics. We apply one statistical method and six machine learning methods to predict the models. The proposed models are validated using dataset collected from Open Source software. The results are analyzed using Area Under the Curve (AUC) obtained from Receiver Operating Characteristics (ROC) analysis. The results show that the model predicted using the random forest and bagging methods outperformed all the other models. Hence, based on these results it is reasonable to claim that quality models have a significant relevance with Object Oriented metrics and that machine learning methods have a comparable performance with statistical methods.", "num_citations": "143\n", "authors": ["555"]}
{"title": "Empirical research in software engineering: concepts, analysis, and applications\n", "abstract": " Empirical research has become an essential component of software engineering research practice. Empirical research in software engineering\u2014including the concepts, analysis, and applications\u2014is all about designing, collecting, analyzing, assessing, and interpreting empirical data collected from software repositories using statistical and machine-learning techniques. Software practitioners and researchers can use the results obtained from these analyses to produce high quality, low cost, and maintainable software.Empirical software engineering involves planning, designing, analyzing, assessing, interpreting, and reporting results of validation of empirical data. There is a lack of understanding and level of uncertainty on the empirical procedures and practices in software engineering. The aim of this book is to present the empirical research processes, procedures, and practices that can be implemented in\u00a0\u2026", "num_citations": "110\n", "authors": ["555"]}
{"title": "Comparative analysis of statistical and machine learning methods for predicting faulty modules\n", "abstract": " The demand for development of good quality software has seen rapid growth in the last few years. This is leading to increase in the use of the machine learning methods for analyzing and assessing public domain data sets. These methods can be used in developing models for estimating software quality attributes such as fault proneness, maintenance effort, testing effort. Software fault prediction in the early phases of software development can help and guide software practitioners to focus the available testing resources on the weaker areas during the software development. This paper analyses and compares the statistical and six machine learning methods for fault prediction. These methods (Decision Tree, Artificial Neural Network, Cascade Correlation Network, Support Vector Machine, Group Method of Data Handling Method, and Gene Expression Programming) are empirically validated to find the relationship\u00a0\u2026", "num_citations": "86\n", "authors": ["555"]}
{"title": "Application of random forest in predicting fault-prone classes\n", "abstract": " There are available metrics for predicting fault prone classes, which may help software organizations for planning and performing testing activities. This may be possible due to proper allocation of resources on fault prone parts of the design and code of the software. Hence, importance and usefulness of such metrics is understandable, but empirical validation of these metrics is always a great challenge. Random forest (RF) algorithm has been successfully applied for solving regression and classification problems in many applications. This paper evaluates the capability of RF algorithm in predicting fault prone software classes using open source software. The results indicate that the prediction performance of random forest is good. However, similar types of studies are required to be carried out in order to establish the acceptability of the RF model.", "num_citations": "84\n", "authors": ["555"]}
{"title": "Techniques for text classification: Literature review and current trends.\n", "abstract": " Automated classification of text into predefined categories has always been considered as a vital method to manage and process a vast amount of documents in digital forms that are widespread and continuously increasing. This kind of web information, popularly known as the digital/electronic information is in the form of documents, conference material, publications, journals, editorials, web pages, e-mail etc. People largely access information from these online sources rather than being limited to archaic paper sources like books, magazines, newspapers etc. But the main problem is that this enormous information lacks organization which makes it difficult to manage. Text classification is recognized as one of the key techniques used for organizing such kind of digital data. In this paper we have studied the existing work in the area of text classification which will allow us to have a fair evaluation of the progress made in this field till date. We have investigated the papers to the best of our knowledge and have tried to summarize all existing information in a comprehensive and succinct manner. The studies have been summarized in a tabular form according to the publication year considering numerous key", "num_citations": "81\n", "authors": ["555"]}
{"title": "Software reuse metrics for object-oriented systems\n", "abstract": " The importance of software measurement is increasing leading to development of new measurement techniques. Reusing existing software components is a key feature in increasing software productivity. It is one of the key elements in object-oriented programming, which reduces the cost and increases the quality of the software. An important feature of C++ called templates support generic programming, which allows the programmer to develop reusable software modules such as functions, classes, etc. The need for software reusability metrics is particularly acute for an organization in order to measure the degree of generic programming included in the form of templates in code. This research addresses this need and introduces a new set of metrics for object-oriented software. Two metrics are proposed for measuring amount of genericty included in the code and then analytically evaluated against Weyuker's set\u00a0\u2026", "num_citations": "79\n", "authors": ["555"]}
{"title": "Investigation of relationship between object-oriented metrics and change proneness\n", "abstract": " Software is the heartbeat of modern day technology. In order to keep up with the pace of modern day expansion, change in any software is inevitable. Defects and enhancements are the two main reasons for a software change. The aim of this paper is to study the relationship between object oriented metrics and change proneness. Software prediction models based on these results can help us identify change prone classes of a software which would lead to more rigorous testing and better results. In the previous research, the use of machine learning methods for predicting faulty classes was found. However till date no study determines the effectiveness of machine learning methods for predicting change prone classes. Statistical and machine learning methods are two different techniques for software quality prediction. We evaluate and compare the performance of these machine learning methods with\u00a0\u2026", "num_citations": "72\n", "authors": ["555"]}
{"title": "Soft computing approaches for prediction of software maintenance effort\n", "abstract": " The relationship between object oriented metrics and software maintenance effort is complex and non-linear. Therefore, there is considerable research interest in development and application of sophisticated techniques which can be used to construct models for predicting software maintenance effort. The aim of this paper is to evaluate and compare the application of different soft computing techniques\u2013Artificial Neural Networks, Fuzzy Inference Systems and Adaptive Neuro-Fuzzy Inference Systems to construct models for prediction of Software Maintenance Effort. The maintenance effort data of two commercial software products is used in this study. The dependent variable in our study is maintenance effort. The independent variables are eight Object Oriented metrics. It is observed that soft computing techniques can be used for constructing accurate models for prediction of software maintenance effort and Adaptive Neuro Fuzzy Inference System technique gives the most accurate model.", "num_citations": "72\n", "authors": ["555"]}
{"title": "Comparative analysis of regression and machine learning methods for predicting fault proneness models\n", "abstract": " Demand for quality software has undergone rapid growth during the last few years. This is leading to increase in development of machine learning techniques for exploring datasets which can be used in constructing models for predicting quality attributes such as Decision Tree (DT), Support Vector Machine (SVM) and Artificial Neural Network (ANN). This paper examines and compares Logistic Regression (LR), ANN (model predicted in an analogous study using the same dataset), SVM and DT methods. These two methods are explored empirically to find the effect of object-oriented metrics given by Chidamber and Kemerer on the fault proneness of object-oriented system classes. Data collected from Java applications is used in the study. The performance of the methods was compared by Receiver Operating Characteristic (ROC) analysis. DT modelling showed 84.7% of correct classifications of faulty classes and\u00a0\u2026", "num_citations": "71\n", "authors": ["555"]}
{"title": "Software fault proneness prediction using support vector machines\n", "abstract": " Empirical validation of software metrics to predict quality using machine learning methods is important to ensure their practical relevance in the software organizations. In this paper, we build a Support Vector Machine (SVM) model to find the relationship between object-oriented metrics given by Chidamber and Kemerer and fault proneness. The proposed model is empirically evaluated using public domain KC1 NASA data set. The performance of the SVM method was evaluated by Receiver Operating Characteristic (ROC) analysis. Based on these results, it is reasonable to claim that such models could help for planning and performing testing by focusing resources on fault-prone parts of the design and code. Thus, the study shows that SVM method may also be used in constructing software quality models.", "num_citations": "62\n", "authors": ["555"]}
{"title": "Software effort prediction using statistical and machine learning methods\n", "abstract": " Accurate software effort estimation is an important part of software process. Effort is measured in terms of person months and duration. Both overestimation and underestimation of software effort may lead to risky consequences. Also, software project managers have to make estimates of how much a software development is going to cost. The dominant cost for any software is the cost of calculating effort. Thus, effort estimation is very crucial and there is always a need to improve its accuracy as much as possible. There are various effort estimation models, but it is difficult to determine which model gives more accurate estimation on which dataset. This paper empirically evaluates and compares the potential of Linear Regression, Artificial Neural Network, Decision Tree, Support Vector Machine and Bagging on software project dataset. The dataset is obtained from 499 projects. The results show that Mean Magnitude\u00a0\u2026", "num_citations": "60\n", "authors": ["555"]}
{"title": "Investigating effect of Design Metrics on Fault Proneness in Object-Oriented Systems.\n", "abstract": " Demand for quality software has undergone with rapid growth during the last few years. This is leading to an increase in the development of metrics for measuring the properties of software such as coupling, cohesion or inheritance that can be used in early quality assessments. Quality models that explore the relationship between these properties and quality attributes such as fault proneness, maintainability, effort or productivity are needed to use these metrics effectively. The goal of this work is to empirically explore the relationship between object-oriented design metrics and fault proneness of object-oriented system classes. The study used data collected from Java applications is containing 136 classes. We use a set of twenty-six design metrics in our work. Result of this study shows that many metrics are based on comparable ideas and provide redundant information. It is shown that by using a subset of metrics in the prediction models can be built to identify the faulty classes. The proposed model predicts faulty classes with more than 80% accuracy.", "num_citations": "60\n", "authors": ["555"]}
{"title": "Empirical validation of object-oriented metrics for predicting fault proneness at different severity levels using support vector machines\n", "abstract": " Empirical validation of software metrics to predict quality using machine learning methods is important to ensure their practical relevance in the software organizations. It would also be interesting to know the relationship between object-oriented metrics and fault proneness at different severity levels. In this paper, we build a Support vector machine (SVM) model to find the relationship between object-oriented metrics given by Chidamber and Kemerer and fault proneness, at different severity levels. The proposed models at different severity levels are empirically evaluated using public domain NASA data set. The performance of the SVM method was evaluated by receiver operating characteristic (ROC) analysis. Based on these results, it is reasonable to claim that such models could help for planning and performing testing by focusing resources on fault-prone parts of the design and code. The performance of\u00a0\u2026", "num_citations": "56\n", "authors": ["555"]}
{"title": "On the applicability of machine learning techniques for object oriented software fault prediction\n", "abstract": " Software testing is a critical and essential part of software development that consumes maximum resources and effort. The construction of models to predict faulty classes can help and guide the testing community in predicting faulty classes in early phases of software development. It is important to analyze and compare the predictive accuracy of machine learning classifiers. The aim of this paper is to find the relation of object oriented metrics and fault proneness of a class. We have used seven machine learning and one logistic regression method in order to predict faulty classes. The results of our work are based on data set obtained from open source software. The results show that the predictive accuracy of machine learning technique LogitBoost is highest with AUC of 0.806.", "num_citations": "54\n", "authors": ["555"]}
{"title": "An adequacy based test data generation technique using genetic algorithms\n", "abstract": " As the complexity of software is increasing, generating an effective test data has become a necessity. This necessity has increased the demand for techniques that can generate test data effectively. This paper proposes a test data generation technique based on adequacy based testing criteria. Adequacy based testing criteria uses the concept of mutation analysis to check the adequacy of test data. In general, mutation analysis is applied after the test data is generated. But, in this work, we propose a technique that applies mutation analysis at the time of test data generation only, rather than applying it after the test data has been generated. This saves significant amount of time (required to generate adequate test cases) as compared to the latter case as the total time in the latter case is the sum of the time to generate test data and the time to apply mutation analysis to the generated test data. We also use genetic algorithms that explore the complete domain of the program to provide near-global optimum solution. In this paper, we first define and explain the proposed technique. Then we validate the proposed technique using ten real time programs. The proposed technique is compared with path testing technique (that use reliability based testing criteria) for these ten programs. The results show that the adequacy based proposed technique is better than the reliability based path testing technique and there is a significant reduce in number of generated test cases and time taken to generate test cases.", "num_citations": "53\n", "authors": ["555"]}
{"title": "An empirical study for software change prediction using imbalanced data\n", "abstract": " Software change prediction is crucial in order to efficiently plan resource allocation during testing and maintenance phases of a software. Moreover, correct identification of change-prone classes in the early phases of software development life cycle helps in developing cost-effective, good quality and maintainable software. An effective software change prediction model should equally recognize change-prone and not change-prone classes with high accuracy. However, this is not the case as software practitioners often have to deal with imbalanced data sets where instances of one type of class is much higher than the other type. In such a scenario, the minority classes are not predicted with much accuracy leading to strategic losses. This study evaluates a number of techniques for handling imbalanced data sets using various data sampling methods and MetaCost learners on six open-source data sets. The\u00a0\u2026", "num_citations": "52\n", "authors": ["555"]}
{"title": "An empirical framework for defect prediction using machine learning techniques with Android software\n", "abstract": " ContextSoftware defect prediction is important for identification of defect-prone parts of a software. Defect prediction models can be developed using software metrics in combination with defect data for predicting defective classes. Various studies have been conducted to find the relationship between software metrics and defect proneness, but there are few studies that statistically determine the effectiveness of the results.ObjectiveThe main objectives of the study are (i) comparison of the machine-learning techniques using data sets obtained from popular open source software (ii) use of appropriate performance measures for measuring the performance of defect prediction models (iii) use of statistical tests for effective comparison of machine-learning techniques and (iv) validation of models over different releases of data sets.MethodIn this study we use object-oriented metrics for predicting defective classes using 18\u00a0\u2026", "num_citations": "52\n", "authors": ["555"]}
{"title": "A regression test selection and prioritization technique\n", "abstract": " Regression testing is a very costly process performed primarily as a software maintenance activity. It is the process of retesting the modified parts of the software and ensuring that no new errors have been introduced into previously tested source code due to these modifications. A regression test selection technique selects an appropriate number of test cases from a test suite that might expose a fault in the modified program. In this paper, we propose both a regression test selection and prioritization technique. We implemented our regression test selection technique and demonstrated in two case studies that our technique is effective regarding selecting and prioritizing test cases. The results show that our technique may significantly reduce the number of test cases and thus the cost and resources for performing regression testing on modified software.", "num_citations": "49\n", "authors": ["555"]}
{"title": "Software Design Metrics for Object-Oriented Software.\n", "abstract": " The importance of software measurement is increasing leading to development of new measurement techniques. As the development of object-oriented software is rising, more and more metrics are being defined for object-oriented languages. Many metrics have been proposed related to various object-oriented constructs like class, coupling, cohesion, inheritance, information hiding and polymorphism. The applicability of metrics developed by previous researchers is mostly limited to requirement, design and implementation phase. Exception handling is a desirable feature of software that leads to robust design and must be measured. This research addresses this need and introduces a new set of design metrics for object-oriented code. Two metrics are developed that measure the amount of robustness included in the code. The metrics are analytically evaluated against Weyuker\u2019s proposed set of nine axioms. These set of metrics are calculated and analyzed for standard projects and accordingly ways in which project managers can utilize these metrics are suggested.", "num_citations": "49\n", "authors": ["555"]}
{"title": "An empirical study to investigate oversampling methods for improving software defect prediction using imbalanced data\n", "abstract": " Software defect prediction is important to identify defects in the early phases of software development life cycle. This early identification and thereby removal of software defects is crucial to yield a cost-effective and good quality software product. Though, previous studies have successfully used machine learning techniques for software defect prediction, these techniques yield biased results when applied on imbalanced data sets. An imbalanced data set has non-uniform class distribution with very few instances of a specific class as compared to that of the other class. Use of imbalanced datasets leads to off-target predictions of the minority class, which is generally considered to be more important than the majority class. Thus, handling imbalanced data effectively is crucial for successful development of a competent defect prediction model. This study evaluates the effectiveness of machine learning classifiers for\u00a0\u2026", "num_citations": "44\n", "authors": ["555"]}
{"title": "Fault prediction considering threshold effects of object\u2010oriented metrics\n", "abstract": " Software product quality can be enhanced significantly if we have a good knowledge and understanding of the potential faults therein. This paper describes a study to build predictive models to identify parts of the software that have high probability of occurrence of fault. We have considered the effect of thresholds of object\u2010oriented metrics on fault proneness and built predictive models based on the threshold values of the metrics used. Prediction of fault prone classes in earlier phases of software development life cycle will help software developers in allocating the resources efficiently. In this paper, we have used a statistical model derived from logistic regression to calculate the threshold values of object oriented, Chidamber and Kemerer metrics. Thresholds help developers to alarm the classes that fall outside a specified risk level. In this way, using the threshold values, we can divide the classes into two levels of\u00a0\u2026", "num_citations": "39\n", "authors": ["555"]}
{"title": "Defect collection and reporting system for git based open source software\n", "abstract": " This paper describes Defect Collection and Reporting System which is an automated tool that aids in generating various reports to provide useful information regarding the defects which were present in a given version of a Git Version Control System based Open Source Software and were fixed in the subsequent version. The generated reports contain information such as the total number of defects, class wise and the corresponding values of different OO metrics for each class in the source code. Such kind of defect data can be readily used for defect prediction hypothesis pertaining to Git based Open Source Software. Some potential fields of application could be analysis and validation of the effect of a given metric suite on fault proneness; to predict fault proneness models for Defect Prediction, etc. In addition to the core functionalities and overall functioning of the above stated tool, this paper also gives an\u00a0\u2026", "num_citations": "39\n", "authors": ["555"]}
{"title": "An exploratory study for software change prediction in object-oriented systems using hybridized techniques\n", "abstract": " Variation in software requirements, technological upgrade and occurrence of defects necessitate change in software for its effective use. Early detection of those classes of a software which are prone to change is critical for software developers and project managers as it can aid in efficient resource allocation of limited resources. Moreover, change prone classes should be efficiently restructured and designed to prevent introduction of defects. Recently, use of search based techniques and their hybridized counter-parts have been advocated in the field of software engineering predictive modeling as these techniques help in identification of optimal solutions for a specific problem by testing the goodness of a number of possible solutions. In this paper, we propose a novel approach for change prediction using search-based techniques and hybridized techniques. Further, we address the following issues: (i) low\u00a0\u2026", "num_citations": "36\n", "authors": ["555"]}
{"title": "Investigating the effect of coupling metrics on fault proneness in object-oriented systems\n", "abstract": " Here the authors find the relationship of independent variables (coupling metrics) with dependent variable (fault proneness). Univariate LR analysis was done on 85 system classes. The results of univariate LR of coupling metrics for system classes are presented in Table 6. Table 7 presents statistics of coupling metrics for standard library classes. The table provides the coefficient (B), standard error (SE), statistical significance (sig), odds ratio (exp (B)), and R2 statistic for each measure.", "num_citations": "35\n", "authors": ["555"]}
{"title": "On the application of search-based techniques for software engineering predictive modeling: A systematic review and future directions\n", "abstract": " Software engineering predictive modeling involves construction of models, with the help of software metrics, for estimating quality attributes. Recently, the use of search-based techniques have gained importance as they help the developers and project-managers in the identification of optimal solutions for developing effective prediction models. In this paper, we perform a systematic review of 78 primary studies from January 1992 to December 2015 which analyze the predictive capability of search-based techniques for ascertaining four predominant software quality attributes, i.e., effort, defect proneness, maintainability and change proneness. The review analyses the effective use and application of search-based techniques by evaluating appropriate specifications of fitness functions, parameter settings, validation methods, accounting for their stochastic natures and the evaluation of developmental models with the\u00a0\u2026", "num_citations": "33\n", "authors": ["555"]}
{"title": "Heuristic search-based approach for automated test data generation: a survey\n", "abstract": " The complexity of software has been increasing in the past few years, and software testing as a most intensive factor is becoming more and more expensive. Testing costs often account for up to 50% of the total expense of software project development; hence any techniques leading to the automatic test data generation will have great potential to substantially reduce these costs. Existing approaches of automatic test data generation have achieved some success by using heuristic search-based approach, but they are not summarised. In this paper we presented a survey on heuristic search-based approach, i.e., genetic algorithm for automated test data generation. We summarise the work done by researchers those who have applied the concept of heuristic search-based approach for test data generation. The main objective of this paper is to acquire the concepts related to heuristic search-based approach for\u00a0\u2026", "num_citations": "33\n", "authors": ["555"]}
{"title": "Predicting testing effort using artificial neural network\n", "abstract": " The importance of software quality is becoming a motivating force for the development of techniques like Artificial Neural Network (ANN), which are being used for the design of prediction models of quality attributes. The purpose of this work is to examine the application of ANN for software quality prediction using Object-Oriented (OO) metrics. Testing effort has been predicted using ANN method and independent variables are OO metrics given by Chidamber and Kemerer. The public domain NASA data has been used to find the relationship between OO metrics and testing effort. The model has estimated testing effort within 35 percent of the actual effort in more than 72.54 percent of the classes, and with a MARE of 0.25. The results are quite interesting, however, more similar types of studies are required to be carried out with large data sets in order to establish the acceptability of the model.", "num_citations": "33\n", "authors": ["555"]}
{"title": "Prediction of defect severity by mining software project reports\n", "abstract": " With ever increasing demands from the software organizations, the rate of the defects being introduced in the software cannot be ignored. This has now become a serious cause of concern and must be dealt with seriously. Defects which creep into the software come with varying severity levels ranging from mild to catastrophic. The severity associated with each defect is the most critical aspect of the defect. In this paper, we intend to predict the models which will be used to assign an appropriate severity level (high, medium, low and very low) to the defects present in the defect reports. We have considered the defect reports from the public domain PITS dataset (PITS A, PITS C, PITS D and PITS E) which are being popularly used by NASA\u2019s engineers. Extraction of the relevant data from the defect reports is accomplished by using text mining techniques and thereafter model prediction is carried out by using\u00a0\u2026", "num_citations": "29\n", "authors": ["555"]}
{"title": "Prediction & assessment of change prone classes using statistical & machine learning techniques\n", "abstract": " Software today has become an inseparable part of our life. In order to achieve the ever demanding needs of customers, it has to rapidly evolve and include a number of changes. In this paper, our aim is to study the relationship of object oriented metrics with change proneness attribute of a class. Prediction models based on this study can help us in identifying change prone classes of a software. We can then focus our efforts on these change prone classes during testing to yield a better quality software. Previously, researchers have used statistical methods for predicting change prone classes. But machine learning methods are rarely used for identification of change prone classes. In our study, we evaluate and compare the performances of ten machine learning methods with the statistical method. This evaluation is based on two open source software systems developed in Java language. We also validated the developed prediction models using other software data set in the same domain (3D modelling). The performance of the predicted models was evaluated using receiver operating characteristic analysis. The results indicate that the machine learning methods are at par with the statistical method for prediction of change prone classes. Another analysis showed that the models constructed for a software can also be used to predict change prone nature of classes of another software in the same domain. This study would help developers in performing effective regression testing at low cost and effort. It will also help the developers to design an effective model that results in less change prone classes, hence better maintenance.", "num_citations": "29\n", "authors": ["555"]}
{"title": "Software defect prediction using neural networks\n", "abstract": " Defect severity assessment is the most crucial step in large industries and organizations where the complexity of the software is increasing at an exponential rate. Assigning the correct severity level to the defects encountered in large and complex software, would help the software practitioners to allocate their resources and plan for subsequent defect fixing activities. In order to accomplish this, we have developed a model based on text mining techniques that will be used to assign the severity level to each defect report based on the classification of existing reports done using the machine learning method namely, Radial Basis Function of neural network. The proposed model is validated using an open source NASA dataset available in the PITS database. Receiver Operating Characteristics (ROC) analysis is done to interpret the results obtained from model prediction by using the value of Area Under the Curve\u00a0\u2026", "num_citations": "29\n", "authors": ["555"]}
{"title": "Reliability modeling using particle swarm optimization\n", "abstract": " Software quality includes many attributes including reliability of a software. Prediction of reliability of a software in early phases of software development will enable software practitioners in developing robust and fault tolerant systems. The purpose of this paper is to predict software reliability, by estimating the parameters of Software Reliability Growth Models (SRGMs). SRGMs are the mathematical models which generally reflect the properties of the process of fault detection during testing. Particle Swarm Optimization (PSO) has been applied to several optimization problems and has showed good performance. PSO is a popular machine learning algorithm under the category of Swarm Intelligence. PSO is an evolutionary algorithm like Genetic Algorithm (GA). In this paper we propose the use of PSO algorithm to the SRGM parameter estimation problem, and then compare the results with those of GA. The\u00a0\u2026", "num_citations": "28\n", "authors": ["555"]}
{"title": "Object-oriented software engineering\n", "abstract": " This comprehensive and well-written book presents the fundamentals of object-oriented software engineering and discusses the recent technological developments in the field. It focuses on object-oriented software engineering in the context of an overall effort to present object-oriented concepts, techniques and models that can be applied in software estimation, analysis, design, testing and quality improvement. It applies unified modelling language notations to a series of examples with a real-life case study. The example-oriented approach followed in this book will help the readers in understanding and applying the concepts of object-oriented software engineering quickly and easily in various application domains. This book is designed for the undergraduate and postgraduate students of computer science and engineering, computer applications, and information technology. KEY FEATURES: Provides the foundation and important concepts of object-oriented paradigm. Presents traditional and object-oriented software development life cycle models with a special focus on Rational Unified Process model. Addresses important issues of improving software quality and measuring various object-oriented constructs using object-oriented metrics. Presents numerous diagrams to illustrate object-oriented software engineering models and concepts. Includes a large number of solved examples, chapter-end review questions and multiple choice questions along with their answers.", "num_citations": "28\n", "authors": ["555"]}
{"title": "Prediction of fault-prone software modules using statistical and machine learning methods\n", "abstract": " Demand for producing quality software has rapidly increased during the last few years. This is leading to increase in development of machine learning methods for exploring data sets, which can be used in constructing models for predicting quality attributes such as fault proneness, maintenance effort, testing effort, productivity and reliability. This paper examines and compares logistic regression and six machine learning methods (Artificial neural network, decision tree, support vector machine, cascade correlation network, group method of data handling polynomial method, gene expression programming). These methods are explored empirically to find the effect of static code metrics on the fault proneness of software modules. We use publicly available data set AR1 to analyze and compare the regression and machine learning methods in this study. The performance of the methods is compared by computing the area under the curve using Receiver Operating Characteristic (ROC) analysis. The results show that the area under the curve (measured from the ROC analysis) of model predicted using decision tree modeling is 0.865 and is a better model than the model predicted using regression and other machine learning methods. The study shows that the machine learning methods are useful in constructing software quality models.", "num_citations": "26\n", "authors": ["555"]}
{"title": "Automated classification of security requirements\n", "abstract": " Requirement engineers are not able to elicit and analyze the security requirements clearly, that are essential for the development of secure and reliable software. Proper identification of security requirements present in the Software Requirement Specification (SRS) document has been a problem being faced by the developers. As a result, they are not able to deliver the software free from threats and vulnerabilities. Thus, in this paper, we intend to mine the descriptions of security requirements present in the SRS document and thereafter develop the classification models. The security-based descriptions are analyzed using text mining techniques and are then classified into four types of security requirements viz. authentication-authorization, access control, cryptography-encryption and data integrity using J48 decision tree method. Corresponding to each type of security requirement, a prediction model has been\u00a0\u2026", "num_citations": "25\n", "authors": ["555"]}
{"title": "Software reliability prediction using machine learning techniques\n", "abstract": " Software Reliability is indispensable part of software quality and is one amongst the most inevitable aspect for evaluating quality of a software product. Software industry endures various challenges in developing highly reliable software. Application of machine learning (ML) techniques for software reliability prediction has shown meticulous and remarkable results. In this paper, we propose the use of ML techniques for software reliability prediction and evaluate them based on selected performance criteria. We have applied ML techniques including adaptive neuro fuzzy inference system (ANFIS), feed forward back propagation neural network, general regression neural network, support vector machines, multilayer perceptron, Bagging, cascading forward back propagation neural network, instance based learning, linear regression, M5P, reduced error pruning tree, M5Rules to predict the software reliability\u00a0\u2026", "num_citations": "22\n", "authors": ["555"]}
{"title": "An empirical comparison of machine learning techniques for software defect prediction\n", "abstract": " Software systems are exposed to various types of defects. The timely identification of defective classes is essential in early phases of software development to reduce the cost of testing the software. This will guide the software practitioners and researchers for planning of the proper allocation of testing resources. Software metrics can be used in conjunction with defect data to develop models for predicting defective classes. There have been various machine learning techniques proposed in the literature for analyzing complex relationships and extracting useful information from problems in less time. However, more studies comparing these techniques are needed to provide evidence so that confidence is established on the performance of one technique over the other. In this paper we address four issues (i) comparison of the machine learning techniques over unpopular used data sets (ii) use of inappropriate\u00a0\u2026", "num_citations": "19\n", "authors": ["555"]}
{"title": "A defect prediction model for open source software\n", "abstract": " Defect prediction models are significantly beneficial for software systems, where testing experts need to focus their attention and resources on problematic areas in the software under development. In this paper we find the relation between object oriented metrics and fault proneness using logistic regression method. The results are analyzed using open source software. The performance of the predicted models is evaluated using Receiver Operating Characteristic (ROC) analysis. The results show that Area under Curve (calculated by ROC analysis) of the predicted model is 0.829.", "num_citations": "18\n", "authors": ["555"]}
{"title": "Particle swarm optimization-based ensemble learning for software change prediction\n", "abstract": " ContextVarious researchers have successfully established the association between Object-Oriented metrics and change prone nature of a class. However, they actively continue to explore effective classifiers for developing efficient change prediction models. Recent developments have ascertained that ensemble methodology can be used to improve the prediction performance of individual classifiers.ObjectiveThis study proposes four strategies of ensemble learning to predict change prone classes by combining seven individual Particle Swarm Optimization (PSO) based classifiers as constituents of ensembles and aggregating them using weighted voting.MethodThe weights allocated to individual classifiers are based on their accuracy and their ability to correctly predict \u201chard instances\u201d i.e. classes which are frequently misclassified by a majority of classifiers. Each individual PSO based classifier uses a different\u00a0\u2026", "num_citations": "16\n", "authors": ["555"]}
{"title": "Application of support vector machine to predict fault prone classes\n", "abstract": " Empirical validation of software metrics to predict quality using machine learning methods is important to ensure their practical relevance in the software organizations. It would also be interesting to know the relationship between object-oriented metrics and fault proneness. In this paper, we build a Support Vector Machine (SVM) model to find the relation-ship between object-oriented metrics given by Chidamber and Kemerer and fault proneness. The proposed model is empirically evaluated using open source software. The performance of the SVM method was evaluated by Receiver Operating Characteristic (ROC) analysis. Based on these results, it is reasonable to claim that such models could help for planning and performing testing by focusing resources on fault- prone parts of the design and code. Thus, the study shows that SVM method may also be used in constructing software quality models. However\u00a0\u2026", "num_citations": "16\n", "authors": ["555"]}
{"title": "Mining the impact of object oriented metrics for change prediction using machine learning and search-based techniques\n", "abstract": " Change in a software is crucial to incorporate defect correction and continuous evolution of requirements and technology. Thus, development of quality models to predict the change proneness attribute of a software is important to effectively utilize and plan the finite resources during maintenance and testing phase of a software. In the current scenario, a variety of techniques like the statistical techniques, the Machine Learning (ML) techniques and the Search-based techniques (SBT) are available to develop models to predict software quality attributes. In this work, we assess the performance of ten machine learning and search-based techniques using data collected from three open source software. We first develop a change prediction model using one data set and then we perform inter-project validation using two other data sets in order to obtain unbiased and generalized results. The results of the study indicate\u00a0\u2026", "num_citations": "15\n", "authors": ["555"]}
{"title": "On the applicability of evolutionary computation for software defect prediction\n", "abstract": " Removal of defects is the key in ensuring long-term error free operation of a software system. Although improvements in the software testing process has resulted in better coverage, it is evident that some parts of a software system tend to be more defect prone than the other parts and identification of these parts can greatly benefit the software practitioners in order to deliver high quality maintainable software products. A defect prediction model is built by training a learner using the software metrics. These models can later be used to predict defective classes in a software system. Many studies have been conducted in the past for predicting defective classes in the early phases of the software development. However, the evolutionary computation techniques have not yet been explored for predicting defective classes. The nature of evolutionary computation techniques makes them better suited to the software\u00a0\u2026", "num_citations": "15\n", "authors": ["555"]}
{"title": "Test suite optimization using mutated artificial bee colony\n", "abstract": " Software test suite optimization is one of the most important issue in software testing as testing consumes a lot of time in executing redundant test cases. In this paper, we have proposed and implemented a new approach for test suite optimization, namely, Mutated Artificial Bee Colony. Artificial Bee colony algorithm combines local search carried out by employed and onlooker bees with global search managed by scouts and gives optimal results. To further improve the global search capabilities of ABC algorithm, Mutation function of Genetic Algorithm is permuted with Onlooker bee and Scout bee, giving four different approaches for test suite optimization that are, Simple ABC, ABC with mutation at onlooker bee, ABC with mutation at Scout bee and ABC with mutation at both onlooker bee and scout bee. All approaches will be compared on the basis of runtime and number of iterations. Based on the experimental results, it has been verified and validates the proposed algorithm. The proposed algorithm would be beneficial for the software testers for selecting a minimal test suite for testing.", "num_citations": "15\n", "authors": ["555"]}
{"title": "Predicting software fault proneness model using neural network\n", "abstract": " Importance of construction of models for predicting software quality attributes is increasing leading to usage of artificial intelligence techniques such as Artificial Neural Network (ANN). The goal of this paper is to empirically compare traditional strategies such as Logistic Regression (LR) and ANN to assess software quality. The study used data collected from public domain NASA data set. We find the effect of software metrics on fault proneness. The fault proneness models were predicted using LR regression and ANN methods. The performance of the two methods was compared by Receiver Operating Characteristic (ROC) analysis. The areas under the ROC curves are 0.78 and 0.745 for the LR and ANN model, respectively. The predicted model shows that software metrics are related to fault proneness. The models predict faulty classes with more than 70 percent accuracy. The study showed that ANN\u00a0\u2026", "num_citations": "15\n", "authors": ["555"]}
{"title": "Empirical comparison of machine learning algorithms for bug prediction in open source software\n", "abstract": " Bug tracking and analysis truly remains one of the most active areas of software engineering research. Bug tracking results may be employed by the software practitioners of large software projects effectively. The cost of detecting and correcting the defect becomes exponentially higher as we go from requirement analysis to the maintenance phase, where defects might even lead to loss of lives. Software metrics in conjunction with defect data can serve as basis for developing predictive models. Open source projects which encompass contributions from millions of people provide capacious dataset for testing. There have been diverse machine learning techniques proposed in the literature for analyzing complex relationships and extracting useful information from problems using optimal resources and time. However, more extensive research comparing these techniques is needed to establish superiority of one\u00a0\u2026", "num_citations": "14\n", "authors": ["555"]}
{"title": "Software fault prediction for object oriented systems: a literature review\n", "abstract": " There always has been a demand to produce efficient and high quality software. There are various object oriented metrics that measure various properties of the software like coupling, cohesion, inheritance etc. which affect the software to a large extent. These metrics can be used in predicting important quality attributes such as fault proneness, maintainability, effort, productivity and reliability. Early prediction of fault proneness will help us to focus on testing resources and use them only on the classes which are predicted to be fault-prone. Thus, this will help in early phases of software development to give a measurement of quality assessment. This paper provides the review of the previous studies which are related to software metrics and the fault proneness. In other words, it reviews several journals and conference papers on software fault prediction. There is large number of software metrics proposed in the\u00a0\u2026", "num_citations": "14\n", "authors": ["555"]}
{"title": "Application of machine learning methods for software effort prediction\n", "abstract": " Software effort estimation is an important area in the field of software engineering. If the software development effort is over estimated it may lead to tight time schedules and thus quality and testing of software may be compromised. In contrast, if the software development effort is underestimated it may lead to over allocation of man power and resource. There are many models proposed in the literature for estimating software effort. In this paper, we analyze machine learning methods in order to develop models to predict software development effort we used Maxwell data consisting 63 projects. The results show that linear regression, MSP and M5Rules are effective methods for predicting software development effort.", "num_citations": "14\n", "authors": ["555"]}
{"title": "Dynamic selection of fitness function for software change prediction using particle swarm optimization\n", "abstract": " ContextOver the past few years, researchers have been actively searching for an effective classifier which correctly predicts change prone classes. Though, few researchers have ascertained the predictive capability of search-based algorithms in this domain, their effectiveness is highly dependent on the selection of an optimum fitness function. The criteria for selecting one fitness function over the other is the improved predictive capability of the developed model on the entire dataset. However, it may be the case that various subsets of instances of a dataset may give best results with a different fitness function.ObjectiveThe aim of this study is to choose the best fitness function for each instance rather than the entire dataset so as to create models which correctly ascertain the change prone nature of majority of instances. Therefore, we propose a novel framework for the adaptive selection of a dynamic optimum fitness\u00a0\u2026", "num_citations": "13\n", "authors": ["555"]}
{"title": "Cross project change prediction using open source projects\n", "abstract": " Predicting the changes in the next release of software, during the early phases of software development is gaining wide importance. Such a prediction helps in allocating the resources appropriately and thus, reduces costs associated with software maintenance. But predicting the changes using the historical data (data of past releases) of the software is not always possible due to unavailability of data. Thus, it would be highly advantageous if we can train the model using the data from other projects rather than the same project. In this paper, we have performed cross project predictions using 12 datasets obtained from three open source Apache projects, Abdera, POI and Rave. In the study, cross project predictions include both the inter-project (different projects) and inter-version (different versions of same projects) predictions. For cross project predictions, we investigated whether the characteristics of the datasets\u00a0\u2026", "num_citations": "13\n", "authors": ["555"]}
{"title": "Severity assessment of software defect reports using text classification\n", "abstract": " Defect severity assessment is essential in order to allocate testing resources and effectively plan testing activities. In this paper, we use text classification techniques to predict and assess the severity of defects. The results are based on defect description of issue requirements obtained from NASA project. We have used Support Vector Machine technique to predict defect severity from issue reports.", "num_citations": "13\n", "authors": ["555"]}
{"title": "Empirical validation of web metrics for improving the quality of web page\n", "abstract": " Web page metrics is one of the key elements in measuring various attributes of web site. Metrics gives the concrete values to the attributes of web sites which may be used to compare different web pages. The web pages can be compared based on the page size, information quality, screen coverage, content coverage etc. Internet and website are emerging media and service avenue requiring improvements in their quality for better customer services for wider user base and for the betterment of human kind. E-business is emerging and websites are not just medium for communication, but they are also the products for providing services. Measurement is the key issue for survival of any organization Therefore to measure and evaluate the websites for quality and for better understanding, the key issues related to website engineering is very important. In this paper we collect data from webby awards data (2007-2010) and classify the websites into good sites and bad sites on the basics of the assessed metrics. To achieve this aim we investigate 15 metrics proposed by various researchers. We present the findings of quantitative analysis of web page attributes and how these attributes are calculated. The result of this paper can be used in quantitative studies in web site designing. The metrics captured in the predicted model can be used to predict the goodness of website design.", "num_citations": "13\n", "authors": ["555"]}
{"title": "Prediction of software quality model using gene expression programming\n", "abstract": " There has been number of measurement techniques proposed in the literature. These metrics can be used in assessing quality of software products, thereby controlling costs and schedules. The empirical validation of object-oriented (OO) metrics is essential to ensure their practical relevance in industrial settings. In this paper, we empirically validate OO metrics given by Chidamber and Kemerer for their ability to predict software quality in terms of fault proneness. In order to analyze these metrics we use gene expression programming (GEP). Here, we explore the ability of OO metrics using defect data for open source software. Further, we develop a software quality metric and suggest ways in which software professional may use this metric for process improvement. We conclude that GEP can be used in detecting fault prone classes. We also conclude that the proposed metric may be effectively used by\u00a0\u2026", "num_citations": "13\n", "authors": ["555"]}
{"title": "Software change prediction: a literature review\n", "abstract": " In industrial organisations, software products are quite large and complex, consisting of a number of classes. Thus, it is not possible to test all the products with a finite number of resources. Hence, it would be beneficial if we could predict in advance some of the attributes associated with the classes such as change proneness, defect proneness, maintenance effort, etc. In this paper, we have dealt with one of the quality attributes, i.e., change proneness. Changes in the software are unavoidable and thus, early prediction of change proneness will help the developers to focus the limited resources on the classes which are predicted to be change-prone. We have conducted a systematic review which evaluates all the available important studies relevant to the area of change proneness. This will help us to identify gaps in the current technology and discuss possible new directions of research in the areas related to\u00a0\u2026", "num_citations": "12\n", "authors": ["555"]}
{"title": "A new metric for predicting software change using gene expression programming\n", "abstract": " Software metrics help in determining the quality of a software product. They can be used for continuous inspection of a software to assist software developers in improving its quality. We can also use metrics to develop quality models which predict important quality attributes like change proneness. Determination of change prone classes in an Object-Oriented software will help software developers to focus their time and resources on the weak portions of the software. In this paper, we validate the Chidamber and Kemerer metric suite for building an efficient software quality model which predict change prone classes with the help of Gene Expression Programming. The model is developed using an open source software. We further propose a new metric which can be used for identifying change prone classes in the early phases of software development life cycle. The proposed metric is validated on another open\u00a0\u2026", "num_citations": "12\n", "authors": ["555"]}
{"title": "Inter project validation for change proneness prediction using object-oriented metrics\n", "abstract": " Developing prediction models for determining change prone classes of a software is a significant upcoming research area. Such models help usin effective and efficient resource utilization during maintenance phase. A crucial step in developing these prediction models is the use of training data of that project. Thus training data of a project dictates its prediction model. This dependency leads to difficulties in applying the prediction model to other projects. The training data of a project, consists of metrics data as well as change statistics. Although computation of metrics data is easy, but collection of change statistics is complex. In this paper we attempt to reuse the generated prediction model of one project and validate it on another project. For the purpose of our evaluation, we have used two open source projects written in Java language. The performance of the predicted models was evaluated using Receiver\u00a0\u2026", "num_citations": "12\n", "authors": ["555"]}
{"title": "Comparison of search based techniques for automated test data generation\n", "abstract": " One of the essential parts of the software development process is software testing as it ensures the delivery of a good quality and reliable software. Various techniques and algorithms have been developed to carry out the testing process. This paper deals with utility of the nature based algorithms namely Genetic Algorithm, Ant Colony Optimization algorithm and Artificial Bee Colony algorithm in automatic generation of optimized test suite for a given set of programs. The performance of algorithms is evaluated using various factors such as number of paths covered, number of iterations, number of test cases produced and time taken for generation of test suite. The results of performance analysis concluded that Artificial Bee Colony algorithm is more efficient as compared to other mentioned algorithms and can be employed for optimized test suite generation for various complex programs or software.", "num_citations": "11\n", "authors": ["555"]}
{"title": "Application of adaptive neuro-fuzzy inference system for predicting software change proneness\n", "abstract": " In this paper, we model the relationship between object-oriented metrics and software change proneness. We use adaptive neuro-fuzzy inference system (ANFIS) to calculate the change proneness for the two commercial open source software systems. The performance of ANFIS is compared with other techniques like bagging, logistic regression and decision trees. We use the area under receiver operating characteristic (ROC) curve to determine the effectiveness of the model. The present analysis shows that of all the techniques investigated, ANFIS gives the best results for both the software systems. We also calculate the sensitivity and specificity for each technique and use it as a measure to evaluate the model effectiveness. The aim of the study is to know the change prone classes in the early phases of software development so as to plan the allocation of testing resources effectively and thus improve software\u00a0\u2026", "num_citations": "11\n", "authors": ["555"]}
{"title": "Alternative methods to rank the impact of object oriented metrics in fault prediction modeling using neural networks\n", "abstract": " The aim of this paper is to rank the impact of Object Oriented (OO) metrics in fault prediction modeling using Artificial Neural Networks (ANNs). Past studies on empirical validation of object oriented metrics as fault predictors using ANNs have focused on the predictive quality of neural networks versus standard statistical techniques. In this empirical study we turn our attention to the capability of ANNs in ranking the impact of these explanatory metrics on fault proneness. In ANNs data analysis approach, there is no clear method of ranking the impact of individual metrics. Five ANN based techniques are studied which rank object oriented metrics in predicting fault proneness of classes. These techniques are i) overall connection weights method ii) Garson\u2019s method iii) The partial derivatives methods iv) The Input Perturb method v) the classical stepwise methods. We develop and evaluate different prediction models based on the ranking of the metrics by the individual techniques. The models based on overall connection weights and partial derivatives methods have been found to be most accurate.", "num_citations": "11\n", "authors": ["555"]}
{"title": "Analyzing machine learning techniques for fault prediction using web applications\n", "abstract": " Web applications are indispensable in the software industry and continuously evolve either meeting a newer criteria and/or including new functionalities. However, despite assuring quality via testing, what hinders a straightforward development is the presence of defects. Several factors contribute to defects and are often minimized at high expense in terms of man-hours. Thus, detection of fault proneness in early phases of software development is important. Therefore, a fault prediction model for identifying fault-prone classes in a web application is highly desired. In this work, we compare 14 machine learning techniques to analyse the relationship between object oriented metrics and fault prediction in web applications. The study is carried out using various releases of Apache Click and Apache Rave datasets. En-route to the predictive analysis, the input basis set for each release is first optimized using filter based correlation feature selection (CFS) method. It is found that the LCOM3, WMC, NPM and DAM metrics are the most significant predictors. The statistical analysis of these metrics also finds good conformity with the CFS evaluation and affirms the role of these metrics in the defect prediction of web applications. The overall predictive ability of different fault prediction models is first ranked using Friedman technique and then statistically compared using Nemenyi post-hoc analysis. The results not only upholds the predictive capability of machine learning models for faulty classes using web applications, but also finds that ensemble algorithms are most appropriate for defect prediction in Apache datasets. Further, we also derive a consensus\u00a0\u2026", "num_citations": "10\n", "authors": ["555"]}
{"title": "Quantitative evaluation of web metrics for automatic genre classification of web pages\n", "abstract": " An additional dimension that facilitate a swift and relevant response from a web search engine is to introduce a genre class for each web page. The web genre classification distinguishes between pages by means of their features such as functionality, style, presentation layout, form and meta-content rather than on content. In this work, nineteen web metrics are identified according to the lexical, structural and functionality attributes of the web page rather than topic. The study is carried out to determine which of these attributes (lexical, structural and functionality) or its combinations, are significant for the development of web genre classification model. Also, we investigate the best web genre prediction model using parametric (Logistic Regression), non-parametric (Decision Tree) and ensemble (Bagging, Boosting) machine learning algorithms. We built forty-two genre classification models to classify web\u00a0\u2026", "num_citations": "9\n", "authors": ["555"]}
{"title": "An exploratory study for predicting maintenance effort using hybridized techniques\n", "abstract": " Software maintenance effort prediction is one of the very costly and challenging affair in the process of software development. Early detection of changes in software are also necessary as it helps the software developers and project managers to allocate resources in an efficient manner. It is very critical for the project managers and software developers to detect changes in software in the earlier phases of software development so that the portions of software that are more prone to changes can be restructured and redesigned. Various statistical and machine learning based models are available for maintenance effort prediction of these objects oriented systems. In this paper, we propose a novel approach for maintenance effort prediction using hybridized (ie, combining search-based techniques with machine learning alternatives) techniques. Specifically, we will address these research issues:(i) low repeatability of\u00a0\u2026", "num_citations": "9\n", "authors": ["555"]}
{"title": "Predicting change using software metrics: A review\n", "abstract": " Software change prediction deals with identifying the classes that are prone to changes during the early phases of software development life cycle. Prediction of change prone classes leads to higher quality, maintainable software with low cost. This study reports a systematic review of change prediction studies published in journals and conference proceedings. This review will help researchers and practitioners to examine the previous studies from different viewpoints: metrics, data analysis techniques, datasets, and experimental results perspectives. Besides this, the research questions formulated in the review allow us to identify gaps in the current technology. The key findings of the review are: (i) less use of method level metrics, machine learning methods and commercial datasets; (ii) inappropriate use of performance measures and statistical tests; (iii) lack of use of feature reduction techniques; (iv) lack of risk\u00a0\u2026", "num_citations": "9\n", "authors": ["555"]}
{"title": "CMS tool: calculating defect and change data from software project repositories\n", "abstract": " Defect and change prediction is a very important activity in software development. Predicting erroneous classes of the system early in the software development life cycle will enable early identification of risky classes in the initial phases. This will assist software practitioners in designing and developing software systems of better quality with focused resources and hence take necessary corrective design actions. In this work we describe a framework to develop and calculate the defect fixes and changes made during various versions of a software system. We develop a tool, Configuration Management System (CMS), which uses log files obtained from a Concurrent Versioning System (CVS) repository in order to collect the number of defects from each class. The tool also calculates the number of changes made during each version of the software. This tool will also assist software practitioners and researchers in\u00a0\u2026", "num_citations": "9\n", "authors": ["555"]}
{"title": "A neuro-fuzzy classifier for website quality prediction\n", "abstract": " To improve the quality of websites, it is necessary to continually assess and evaluate web metrics and subsequently make improvements. In this research, we have computed nine quantitative web measures for each website using an automated Web Metrics Analyzer tool developed in JAVA programming language. The website quality prediction models are developed utilizing ANFIS-Subtractive clustering and ANFIS-FCM based FIS models, to classify the quality of website as good or bad. The models are validated using 10 cross validation on a collection of web pages of Pixel Awards web metrics collected through the tool. The results are analyzed using Area Under Curve obtained from Receiver Operating Characteristic (ROC) analysis. The results showed that both ANFIS-Subtractive and ANFIS-FCM have acceptable performance in terms of specificity and sensitivity. In addition, ANFIS-Subtractive and ANFIS\u00a0\u2026", "num_citations": "9\n", "authors": ["555"]}
{"title": "Development of a framework for test case prioritization using genetic algorithm\n", "abstract": " Software Testing is a time and effort consuming part of the software development life cycle. Retesting a software application during the maintenance phase, with the entire test suite and additional test cases for the modifications in the software, within budget and time, is a challenge for software testers. Test Case Prioritization is used to overcome this problem by prioritizing the test cases in order to maximize certain testing objectives like fault detection rate, statement coverage, etc. In this paper, we propose a framework for test case prioritization that emphasizes a new metric, APBCm (modified Average Percentage of Block Coverage). This metric evaluates the rate of code coverage by incorporating knowledge about the significance of blocks of code in the form of weights. We have used this metric as fitness evaluation function in a Genetic Algorithm in order to evaluate the effectiveness of a test case sequence. We\u00a0\u2026", "num_citations": "9\n", "authors": ["555"]}
{"title": "Assessment of defect prediction models using machine learning techniques for object-oriented systems\n", "abstract": " Software development is an essential field today. The advancement in software systems leads to risk of them being exposed to defects. It is important to predict the defects well in advance in order to help the researchers and developers to build cost effective and reliable software. Defect prediction models extract information about the software from its past releases and predict the occurrence of defects in future releases. A number of Machine Learning (ML) algorithms proposed and used in the literature to efficiently develop defect prediction models. What is required is the comparison of these ML techniques to quantify the advantage in performance of using a particular technique over another. This study scrutinizes and compares the performances of 17 ML techniques on the selected datasets to find the ML technique which gives the best performance for determining defect prone classes in an Object-Oriented(OO\u00a0\u2026", "num_citations": "8\n", "authors": ["555"]}
{"title": "A comparative study of models for predicting fault proneness in object-oriented systems\n", "abstract": " Demand for quality software has undergone rapid growth during the last few years. This is leading to an increase in development of metrics for measuring the properties of software such as coupling, cohesion or inheritance that can be used in early quality assessments. Quality models that explore the relationship between these properties and quality attributes such as fault proneness, maintainability, effort or productivity are needed to use these metrics effectively. This study reflects the relevance of quality models to industrial practices and the maturity of research in developing these models. In this paper we summarise the results of empirical studies done so far to assess the applicability of fault proneness models across object-oriented software. We perform a systematic study of these to identify general conclusions drawn from them. This work recommends the research methodology that should be followed to\u00a0\u2026", "num_citations": "8\n", "authors": ["555"]}
{"title": "Software change prediction: A systematic review and future guidelines\n", "abstract": " EN Background: The importance of Software Change Prediction (SCP) has been emphasized by several studies. Numerous prediction models in literature claim to effectively predict change-prone classes in software products. These models help software managers in optimizing resource usage and in developing good quality, easily maintainable products. Aim: There is an urgent need to compare and assess these numerous SCP models in order to evaluate their effectiveness. Moreover, one also needs to assess the advancements and pitfalls in the domain of SCP to guide researchers and practitioners. Method: In order to fulfill the above stated aims, we conduct an extensive literature review of 38 primary SCP studies from January 2000 to June 2019. Results: The review analyzes the different set of predictors, experimental settings, data analysis techniques, statistical tests and the threats involved in the studies, which develop SCP models. Conclusion: Besides, the review also provides future guidelines to researchers in the SCP domain, some of which include exploring methods for dealing with imbalanced training data, evaluation of search-based algorithms and ensemble of algorithms for SCP amongst others.", "num_citations": "7\n", "authors": ["555"]}
{"title": "An extensive analysis of search-based techniques for predicting defective classes\n", "abstract": " In spite of constant planning, effective documentation and proper implementation of a software during its life cycle, many defects still occur. Various empirical studies have found that prediction models developed using software metrics can be used to predict these defects. Researchers have advocated the use of search-based techniques and their hybridized versions in literature for developing software quality prediction models. This study conducts an extensive comparison of 20 search-based techniques, 16 hybridized techniques and 17 machine-learning techniques amongst each other, to develop software defect prediction models using 17 data sets. The comparison framework used in the study is efficient as it (i) deals with the stochastic nature of the techniques (ii) provides a fair comparison amongst the techniques (iii) promotes repeatability of the study and (iv) statistically validates the results. The results of the\u00a0\u2026", "num_citations": "7\n", "authors": ["555"]}
{"title": "Prediction of change prone classes using evolution-based and object-oriented metrics\n", "abstract": " Determination of change prone classes is crucial in providing guidance to software practitioners for efficient allocation of limited resources and to develop favorable quality software products with optimum costs. Previous literature studies have proposed successful use of design metrics to predict classes which are more prone to change in an Object-Oriented (OO) software. However, the use of evolution-based metrics suite, which quantifies history of changes in a software, release by release should also be evaluated for effective prediction of change prone classes. Evolution-based metrics are representative of evolution characteristics of a class over all its previous releases and are important in order to understand progression and change-prone nature of a class. This study evaluates the use of evolution-based metrics when used in conjunction with OO metrics for prediction of classes which are change prone in\u00a0\u2026", "num_citations": "7\n", "authors": ["555"]}
{"title": "A defect tracking tool for open source software\n", "abstract": " Defect reporting and correction is one of the most crucial part in any phase of software development. This is a very costly activity. A lot of time, effort and resources can be saved if the defects can be predicted beforehand, using a suitable training data set. For this, the generation of defect reports which reports the classes that are defective and computes the required software metrics is required. This process should be fully automated. Various data analysis research techniques and algorithms can be applied on this produced data set and this information can be very helpful for the developers and the organization as a whole for the prediction of bugs for future purposes and for analysis of the quality of the software being produced. The various issues addressed in this paper are (1) selection of suitable projects as input for the tool, (2) a mechanism to trace the whole project code history and identifying the classes\u00a0\u2026", "num_citations": "7\n", "authors": ["555"]}
{"title": "The ability of search-based algorithms to predict change-prone classes\n", "abstract": " After analyzing the models predicted on the Eclipse data set, the PMD data set, and the SubSonic data set using the ten-fold cross-validation method, the authors found that the best results on the three data sets were given by the GAJnt method on the Eclipse data set, the SUCS method on the PMD data set, and the FH-GBML method on the SubSonic data set. Hence, the predictive performance of the GAJnt, SUCS, and FH-GBML methods was competitive and even better as compared to the LR, SVM, MLP, and C4. 5 methods....", "num_citations": "7\n", "authors": ["555"]}
{"title": "Examining the effectiveness of machine learning algorithms for prediction of change prone classes\n", "abstract": " Managing change in the early stages of a software development life cycle is an effective strategy for developing a good quality software at low costs. In order to manage change, we use software quality models which can efficiently predict change prone classes and hence guide developers in appropriate distribution of limited resources. This study examines the effectiveness of ten machine learning algorithms for developing such software quality models on three object-oriented software data sets. We also compare the performance of machine learning algorithms with the widely used logistic regression technique and statistically rank various algwith the help of Friedman test.", "num_citations": "7\n", "authors": ["555"]}
{"title": "Software change prediction using voting particle swarm optimization based ensemble classifier\n", "abstract": " Prediction of change prone classes of a software has become an important area of research where the search for the best classifier still continues. While searching for an effective classifier, it needs to be ascertained whether an ensemble of classifier is better than its corresponding constituent classifiers. In this work, we propose four voting ensemble of classifiers, where a group of classifiers learn together and are used to create a single prediction model. The set of Particle Swarm Optimization (PSO) based classifiers are created based on five different fitness functions. Then the voting method is used to combine the predictions of these multiple classifiers so that the resultant model has improved accuracy. This proposal is based on the premises that while using a search-based algorithm for classification tasks, it is crucial to combine various classifiers based on different fitness function. The results of the study are\u00a0\u2026", "num_citations": "6\n", "authors": ["555"]}
{"title": "Analyzing and assessing the security-related defects\n", "abstract": " The use of the Internet has become an integral part of everyone's life. Due to this, the introduction of virus and other malicious crackers is increasing everyday. This in turn leads to the introduction of defects which adversely affect the security. Thus, protecting vital information in this cyber world is not an easy task. We need to deal with security related defects to ensure failure free and smooth functioning of the software. Thus, in this paper, we intend to study and analyze various aspects of security-related defects by analyzing the defect reports available in various open-source software repositories. Besides this, prediction models can also be constructed which can be used by researchers and practitioners to predict various aspects of security - related defects. Such prediction models are especially beneficial for large-scale systems, where testing experts need to focus their attention and resources to the problem areas\u00a0\u2026", "num_citations": "6\n", "authors": ["555"]}
{"title": "Mining defect reports for predicting software maintenance effort\n", "abstract": " Software Maintenance is the crucial phase of software development lifecycle, which begins once the software has been deployed at the customer's site. It is a very broad activity and includes almost everything that is done to change the software if required, to keep it operational after its delivery at the customer's end. A lot of maintenance effort is required to change the software after it is in operation. Therefore, predicting the effort and cost associated with the maintenance activities such as correcting and fixing the defects has become one of the key issues that need to be analyzed for effective resource allocation and decision-making. In view of this issue, we have developed a model based on text mining techniques using the statistical method namely, Multi-nominal Multivariate Logistic Regression (MMLR). We apply text mining techniques to identify the relevant attributes from defect reports and relate these relevant\u00a0\u2026", "num_citations": "6\n", "authors": ["555"]}
{"title": "Search based techniques for software fault prediction: current trends and future directions\n", "abstract": " The effective allocation of the resources is crucial and essential in the testing phase of the software development life cycle so that the weak areas in the software can be verified and validated efficiently. The prediction of fault prone classes in the early phases of software development can help software developers to focus the limited available resources on those portions of software, which are more prone to fault. Recently, the search based techniques have been successfully applied in the software engineering domain. In this study, we analyze the position of search based techniques for use in software fault prediction by collecting relevant studies from the literature which were conducted during the period January 1991 to October 2013. We further summarize current trends by assessing the performance capability of the search based techniques in the existing research and suggest future directions.", "num_citations": "6\n", "authors": ["555"]}
{"title": "Test case prioritization using genetic algorithm\n", "abstract": " Software is built by human so it cannot be perfect. So in order to make sure that developed software does not do any unintended thing we have to test every software before launching it in the operational world. Software testing is the major part of software development lifecycle. Testing involves identifying the test cases which can find the errors in the program. Exhaustive testing is not a good idea to follow. It is very difficult and time consuming to perform. In this paper a technique has been proposed to do prioritize test cases according to their capability of finding errors. One which is more likely to find the errors has been assigned a higher priority and the one which is less likely to find the errors in the program has been assigned low priority. It is recommended to execute the test cases according their priority to find the errors.", "num_citations": "6\n", "authors": ["555"]}
{"title": "Ischemic stricture of Roux-en-Y intestinal loop and recurrent cholangitis.\n", "abstract": " The commonest complication of hepaticojejunostomy for the management of biliary strictures is recurrent cholangitis. We report a 54-year-old man who underwent choledochojejunostomy after choledochal cyst excision, and later developed ischemic stricture of the Roux-en-Y loop intestinal loop and recurrent cholangitis. The stricturous intestinal loop was excised with re-anastomosis with new Roux-en-Y loop, with uneventful recovery.", "num_citations": "6\n", "authors": ["555"]}
{"title": "A study on software defect prediction using feature extraction techniques\n", "abstract": " Identification and elimination of defects in software is time and resource-consuming activity. The maintenance of a defective software system is burdensome. Software defect prediction (SDP) at an early stage of the Software Development Life Cycle (SDLC) results in quality software and reduces its development cost. In this study, a comparison is performed on nine open-source softwaresystems written in Java from PROMISE Repository using four mostly used feature extraction techniques such as Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), Kernel-based Principal Component Analysis (K-PCA) and Autoencoders with Support Vector Machine (SVM) as base machine learning classifier. The model validation is performed using a ten-fold cross-validation method and the efficiency of the model is evaluated using accuracy and ROCAUC. The results of this study indicate that Autoencoders is\u00a0\u2026", "num_citations": "5\n", "authors": ["555"]}
{"title": "On the application of cross-project validation for predicting maintainability of open source software using machine learning techniques\n", "abstract": " Design and development of models to predict software maintenance effort is an impending research area as these models help to predict maintenance effort of software system at earlier stages of its development. The predictions of these models help in allocation of limited resources in an optimal way in the test and maintenance phases of software development. Although numeral software maintainability prediction models have been successfully developed in the past using machine learning (ML) and statistical techniques but there is always threat to generalizability of result have prevailed, as these models are validated on the same data set on which they are trained. This study endeavors to improve generalizability of the software maintainability prediction by cross-project validation where prediction model developed on one software project is validated against the other project. To meet our objective we have\u00a0\u2026", "num_citations": "5\n", "authors": ["555"]}
{"title": "Threats to validity in search-based predictive modelling for software engineering\n", "abstract": " A number of studies in the literature have developed effective models to address prediction tasks related to a software product such as estimating its development effort, or its change/defect proneness. These predictions are critical as they help in identifying weak areas of a software product and thus guide software project managers in effective allocation of project resources to these weak parts. Such practices assure good quality software products. Recently, the use of search-based approaches (SBAs) for developing software prediction models (SPMs) has been successfully explored by a number of researchers. However, in order to develop effective and practical SPMs it is imperative to analyse various sources of threats. This study extensively reviews 93 primary studies, which use SBAs for developing SPMs of four commonly used software attributes (effort, defect-proneness, maintainability and change-proneness\u00a0\u2026", "num_citations": "5\n", "authors": ["555"]}
{"title": "Tool to handle imbalancing problem in software defect prediction using oversampling methods\n", "abstract": " Data imbalancing is becoming a common problem to tackle in different fields like, defect prediction, change prediction, oil spills, medical diagnose etc. Various methods have been developed to handle imbalanced datasets in order to improve accuracy of the prediction models. Many studies have been carried out in the field of defect prediction for imbalanced datasets but most of them uses SMOTE oversampling method to handle the imbalanced data problem. There are many other oversampling methods which help to deal with imbalancing problem and are still unexplored particularly in the field of software defect prediction. This study develops a tool by implementing three of those unexplored oversampling methods namely ADASYN, SPIDER and Safe-Level-SMOTE. Furthermore, we analyze their performance in comparison to traditional method SMOTE. The performance of oversampling methods is evaluated by\u00a0\u2026", "num_citations": "5\n", "authors": ["555"]}
{"title": "Identifying threshold values of an open source software using Receiver Operating Characteristics curve (ROC)\n", "abstract": " Software metrics are widely used to predict the classes in a software that are comparatively more change prone than others. Developers construct various prediction models which are used for the prediction of change prone classes. Thus, focussed attention can be laid on such classes leading to saving of lots of resources in terms of money, manpower and cost. But construction and usage of the prediction models using metrics is not always efficient and practical for developers. Thus, the alternative approach can be to identify certain alarming values of metrics above which a class is said to be at high risk. These alarming values are known as thresholds of the metrics. The main focus of this study is to identify metric threshold values using receiver operating characteristic curves. We have used threshold values to identify change prone classes (or classes that may lead to some risk in future) so that developers can\u00a0\u2026", "num_citations": "5\n", "authors": ["555"]}
{"title": "Predicting Software Maintenance effort using neural networks\n", "abstract": " Software Maintenance is an important phase of software development lifecycle, which starts once the software has been deployed at the customer's end. A lot of maintenance effort is required to change the software after it is in operation. Therefore, predicting the effort and cost associated with the maintenance activities such as correcting and fixing the defects has become one of the key issues that need to be analyzed for effective resource allocation and decision-making. In view of this issue, we have developed a model based on text mining techniques using machine learning method namely, Radial Basis Function of neural network. We apply text mining techniques to identify the relevant attributes from defect reports and relate these relevant attributes to software maintenance effort prediction. The proposed model is validated using `Browser' application package of Android Operating System. Receiver Operating\u00a0\u2026", "num_citations": "5\n", "authors": ["555"]}
{"title": "Prediction of change prone classes using threshold methodology\n", "abstract": " The widespread use of software metrics to predict various quality attributes is evident. In this study we have used metrics to identify change prone parts of software so that developers can pay focused attention on such classes. Various metric models are constructed using machine learning and statistical techniques, which can be used for predicting change prone parts of the software. However, training these models is a time consuming task and hence, these models cannot be used on a daily basis to predict change proneness. In this paper, an alternative approach is used which is based on calculating thresholds of metrics. Thresholds are defined as alarming values above which a class is considered to be risky or change prone and hence, needs careful attention. A statistical approach is used to calculate threshold values of open source software, Freemind 0.9. 0. To examine the applicability of threshold values, they are validated on the different releases of Freemind as well as on a similar nature project, Frinika. The results demonstrate the effectiveness of the methodology in identification of the threshold values.", "num_citations": "5\n", "authors": ["555"]}
{"title": "Analyzing software change in open source projects using artificial immune system algorithms\n", "abstract": " Development of software change prediction models, based on the change histories of a software, are valuable for early identification of change prone classes. Classification of these change prone classes is vital to yield competent use of limited resources in an organization. This paper validates Artificial Immune System (AIS) algorithms for development of change prediction models using six open source data sets. It also compares the performance of AIS algorithms with other machine learning and statistical algorithms. The results of the study indicate, that the models developed, are effective means of predicting change prone classes in the future versions of the software. However, AIS algorithms do not perform better that machine learning and other statistical algorithms. The study provides conclusive results about the capabilities of AIS algorithms and reports whether there are any significant differences in the\u00a0\u2026", "num_citations": "5\n", "authors": ["555"]}
{"title": "Comparative analysis of J48 with statistical and machine learning methods in predicting fault-prone classes using object-oriented systems\n", "abstract": " There are available metrics for predicting fault prone classes, which May help software organizations for planning and performing testing activities. This May be possible due to proper allocation of resources on fault prone parts of the design and code of the software. Hence, importance and usefulness of such metrics is understandable, but empirical validation of these metrics is always a great challenge. J48 decision tree algorithm has been successfully applied for solving classification problems in many applications. This paper evaluates the capability of algorithm and compares its performance with nine statistical and machine learning methods in predicting fault prone software classes using publicly available NASA data set. The results indicate that the prediction performance of J48 is generally better than other statistical and machine learning models. However, similar types of studies are required to be\u00a0\u2026", "num_citations": "5\n", "authors": ["555"]}
{"title": "Handling imbalanced data using ensemble learning in software defect prediction\n", "abstract": " With the ever growing software industry, software defect prediction is one of the key ingredients in recipe of producing good quality software. Defects uncovered well in time helps in saving resources in terms of time, effort and money. However imbalanced nature of software data may hamper the resultant performance of models leading to incorrect interpretations of results. This problem has dragged attention of researchers and many solutions are proposed to overcome the effect of this problem. This paper aims to provide empirical comparison of software defect prediction models developed by using various boosting based ensemble methods on three open source JAVA projects. Four ensemble methods incorporate resampling techniques within them. Performances of models obtained are evaluated using stable metrics like Balance, G-Mean and AUC. Results show that use of resampling techniques before\u00a0\u2026", "num_citations": "4\n", "authors": ["555"]}
{"title": "Cross project defect prediction for open source software\n", "abstract": " Software defect prediction is the process of identification of defects early in the life cycle so as to optimize the testing resources and reduce maintenance efforts. Defect prediction works well if sufficient amount of data is available to train the prediction model. However, not always this is the case. For example, when the software is the first release or the company has not maintained significant data. In such cases, cross project defect prediction may identify the defective classes. In this work, we have studied the feasibility of cross project defect prediction and empirically validated the same. We conducted our experiments on 12 open source datasets. The prediction model is built using 12 software metrics. After studying the various train test combinations, we found that cross project defect prediction was feasible in 35 out of 132 cases. The success of prediction is determined via precision, recall and AUC of the\u00a0\u2026", "num_citations": "4\n", "authors": ["555"]}
{"title": "Assessment of machine learning algorithms for determining defective classes in an object-oriented software\n", "abstract": " Software defect prediction is a well renowned field of software engineering. Determination of defective classes early in the lifecycle of a software product helps software practitioners in effective allocation of resources. More resources are allocated to probable defective classes so that defects can be removed in the initial phases of the software product. Such a practice would lead to a good quality software product. Although, hundreds of defect prediction models have been developed and validated by researchers, there is still a need to develop and evaluate more models to draw generalized conclusions. Literature studies have found Machine Learning (ML) algorithms to be effective classifiers in this domain. Thus, this study evaluates four ML algorithms on data collected from seven open source software projects for developing software defect prediction models. The results indicate superior performance of the\u00a0\u2026", "num_citations": "4\n", "authors": ["555"]}
{"title": "An automated tool for generating change report from open-source software\n", "abstract": " Classes in object-oriented software systems are continuously subject to change. Change prediction is a very important activity in software development. Change data consists of the number of lines of codes added, deleted and modified for each common class between any two versions of a software system. It is important to develop tools to calculate change data and object-oriented metrics that will assist software practitioners in identifying change prone classes in early stages of the software development life cycle. In this paper, we develop a tool, Change Report Generator (CRG) to generate the change report from software source codes of various versions of open-source software. We also extend this tool to automate object-oriented metrics calculation from the source codes of software systems. The generated files store the total number of changes class wise and corresponding values of different object-oriented\u00a0\u2026", "num_citations": "4\n", "authors": ["555"]}
{"title": "Delhi\n", "abstract": " The relationship between object oriented metrics and software maintenance effort is complex and non-linear. Therefore, there is considerable research interest in development and application of sophisticated techniques which can be used to construct models for predicting software maintenance effort. The aim of this paper is to evaluate and compare the application of different soft computing techniques\u2013Artificial Neural Networks, Fuzzy Inference Systems and Adaptive Neuro-Fuzzy Inference Systems to construct models for prediction of Software Maintenance Effort. The maintenance effort data of two commercial software products is used in this study. The dependent variable in our study is maintenance effort. The independent variables are eight Object Oriented metrics. It is observed that soft computing techniques can be used for constructing accurate models for prediction of software maintenance effort and Adaptive Neuro Fuzzy Inference System technique gives the most accurate model.", "num_citations": "4\n", "authors": ["555"]}
{"title": "Software defect prediction using Binary Particle Swarm Optimization with Binary Cross Entropy as the fitness function\n", "abstract": " Software is a consequential asset because concrete software is needed in virtually every industry, in every business, and for every function. It becomes more paramount as time goes on\u2013if something breaks within your application portfolio and expeditious, efficient, and efficacious fine-tune needs to transpire as anon as possible. Therefore, recognizing the faults in the early phase of the software development lifecycle is essential for both, diminishing the cost in terms of efforts and money Similarly, It is important to find out features which could be redundant or features which are highly correlated with each other, as it could largely affect the model's learning process. This analysis refers to the use of crossover Artificial Neural Network (ANN) and Binary Particle Swarm Optimization (BPSO) with Binary Cross-Entropy (BCE) loss for the fitness function. The conclusion of proposed paper is to provide the significance and\u00a0\u2026", "num_citations": "3\n", "authors": ["555"]}
{"title": "Analysis of evolutionary algorithms to improve software defect prediction\n", "abstract": " Defect prediction of software is necessary to determine defective parts of software. Defect prediction models are elaborated with the help of software metrics when combined with defective data to predict the classes that are defective. In this paper we have used datasets that statistically resolve the relationship among software metrics and defect vulnerability. The main intent of this paper are 1) Feature selection for defect prediction using proposed evolutionary algorithm 2) Comparing machine learning techniques 3) Use of precision and recall as performance measure for defect prediction 4) 10- fold validation is performed on every model. In this discourse, we predict defective class using 5 machine learning techniques and 2 evolutionary techniques for feature selection. In this work, we have applied evolutionary algorithms for feature selection suitable for each of the classification techniques applied on five open\u00a0\u2026", "num_citations": "3\n", "authors": ["555"]}
{"title": "Analysis of Software Project Reports for Defect Prediction Using KNN\n", "abstract": " Defect severity assessment is highly essential for the software practitioners so that they can focus their attention and resources on the defects having a higher priority than the other defects. This would directly impact resource allocation and planning of subsequent defect fixing activities. In this paper, we intend to predict a model which will be used to assign a severity level to each of the defect found during testing. The model is based on text mining and machine learning technique. We have used KNN machine learning method to predict the model employed on an open source NASA dataset available in the PITS database. Area Under the Curve (AUC) obtained from Receiver Operating Characteristics (ROC) analysis is used as the performance measure to validate and analyze the results. The obtained results show that the performance of KNN technique is exceptionally well in predicting the defects corresponding to top 100 words for all the severity levels. Its performance is less for top 5 words, better for top 25 words and still better for top 50 words. Hence, with these results, it is reasonable to claim that the performance of KNN is dependent on the number of words selected as independent features. As the number of words increases, the performance of KNN also gets better. Apart from this, it has been noted that KNN method works best for medium severity defects as compared to the other severity defects.", "num_citations": "3\n", "authors": ["555"]}
{"title": "Prediction of High-, Medium-, and Low-Severity Faults Using Software Metrics\n", "abstract": " In this work, the author analyzes the performance of the fault proneness predictions using static code metrics such as lines of code (Halstead 1977; McCabe 1976), with particular focus on how accurately these metrics predict fault proneness at different fault severity levels. The analysis of these metrics is essential due to their wide applicability in many software projects.", "num_citations": "3\n", "authors": ["555"]}
{"title": "Assessment of Code Smells for Predicting Class Change Proneness\n", "abstract": " The authors developed a tool to analyze Java source code for the presence of code smells. While the tool is capable of estimating 17 code smells, they only use 13 of them in their study. The tool is designed such that the users have the freedom to set their own thresholds for the metrics and statistics used for estimating smells. The tool takes as input the raw. java files as well as. class files.", "num_citations": "3\n", "authors": ["555"]}
{"title": "Empirical Validation of Object-Oriented Metrics to Predict Fault Proneness Using Open Source Software\n", "abstract": " The dependent variable in this study is fault proneness. The goal of this study is to empirically explore the relationship between OO metrics and fault proneness at the class level. Fault proneness is defined as the probability of fault detection in a class (Briand,[DaIy], and Wust 2000). The authors use LR and ANN methods to predict the probability of fault proneness. Their dependent variable will be predicted based on the faults found during software development.", "num_citations": "3\n", "authors": ["555"]}
{"title": "Severity Prediction of Software Vulnerabilities Using Textual Data\n", "abstract": " Nowadays, all essential activities carried out by society are dependent on software systems. These systems with time have become more and more complex. Increase in complexity leads to introduction of vulnerabilities in the software system. Vulnerabilities are being reported every year and are increasing exponentially with time. These vulnerabilities required to be patched and fixed before getting exploited. To achieve this goal, textual data available on vulnerabilities is needed to be exploited to retrieve valuable information from it. Therefore, in this paper, we aim to develop a prediction model which will take textual description of Apache Tomcat vulnerabilities as input and will predict the severity of the vulnerabilities. Text mining techniques and machine learning algorithms are used to perform this task. The huge volume of textual data available on vulnerabilities has to be prioritized so that more severe\u00a0\u2026", "num_citations": "2\n", "authors": ["555"]}
{"title": "A Systematic Review on Application of Deep Learning Techniques for Software Quality Predictive Modeling\n", "abstract": " Software quality prediction is the process of evaluating the software developed for various metrics like defect prediction, bug localisation, effort estimation etc. To evaluate these metrics a myriad of techniques have been developed in the literature, from manual assessment to application of machine learning and statistical testing. These methodologies, however, had lower accuracy in determining SQPMs due to their inability to model the complex relationships in the training data. With the wide emergence of deep learning, not only has the accuracy of the pre-existing models enhanced, but it has also opened doors for new metrics that could be evaluated and automated. This study performs a systematic literature review of research papers published from January 1990 to January 2019 that used deep learning to evaluate software quality prediction metrics (SQPM). The paper identifies 20 primary studies and 7\u00a0\u2026", "num_citations": "2\n", "authors": ["555"]}
{"title": "Cross-domain ambiguity detection using linear transformation of word embedding spaces\n", "abstract": " The requirements engineering process is a crucial stage of the software development life cycle. It involves various stakeholders from different professional backgrounds, particularly in the requirements elicitation phase. Each stakeholder carries distinct domain knowledge, causing them to differently interpret certain words, leading to cross-domain ambiguity. This can result in misunderstanding amongst them and jeopardize the entire project. This paper proposes a natural language processing approach to find potentially ambiguous words for a given set of domains. The idea is to apply linear transformations on word embedding models trained on different domain corpora, to bring them into a unified embedding space. The approach then finds words with divergent embeddings as they signify a variation in the meaning across the domains. It can help a requirements analyst in preventing misunderstandings during elicitation interviews and meetings by defining a set of potentially ambiguous terms in advance. The paper also discusses certain problems with the existing approaches and discusses how the proposed approach resolves them.", "num_citations": "2\n", "authors": ["555"]}
{"title": "Estimating the threshold of software metrics for web applications\n", "abstract": " Estimating thresholds for software metrics is a key step towards assigning a quality index. In defect prediction, two approaches are widely used those based on statistics and, that which uses rigorous mathematical models. Although significant insights have been surmised, a general consensus on their results is still far from generalizations. In these perspectives, we attempt to check whether there exists any relationship between the two approaches. An empirical investigation is carried out in this work to study the relationship between estimated threshold values calculated at various risk levels using Bender\u2019s approach and measures of central tendency using the Apache Click web application. The effect of these different threshold estimates on the performance of the developed defect prediction models is also studied and validated using different releases of the dataset. We find that the threshold indicator\u00a0\u2026", "num_citations": "2\n", "authors": ["555"]}
{"title": "Analysis of Refactoring Effect on Software Quality of Object-Oriented Systems\n", "abstract": " Software industry primarily recognizes the significance of high quality, robust, reliable, and maintainable software. The industry always demands for efficient solutions that can improve the quality of software. Refactoring is one such potential solution; however, literature shows varied results of the application of refactoring techniques on software quality attributes. There are number of refactoring techniques that still needs to be empirically validated. This paper focuses on analyzing the effect of four unexplored refactoring techniques on different software quality attributes like coupling, cohesion, complexity, inheritance, reusability, and testability on object-oriented softwares. Impact analysis is performed by calculating Chidamber-Kemerer (CK)\u00a0metrics of the projects, both before and after applying the refactoring techniques, and the results are statistically validated. Empirical analysis of results revealed that\u00a0\u2026", "num_citations": "2\n", "authors": ["555"]}
{"title": "Parameter Tuning on Software Defect Prediction Using Differential Evolution & Simulated Annealing\n", "abstract": " Machine Learning algorithms are used in Software Engineering to predict defects. These defect predictors are powerful in comparison to manual methods. They are also pretty simple to grasp and use. But an important thing, which is often ignored is the tuning of these defect predictors to optimize their performance. We try to find simple and easy to implement methods for tuning the defect predictors and also compare the performances of these methods. We ran Differential Evolution and Simulated Annealing as optimizers using different datasets from open-source JAVA systems to explore the tuning space. Finally, we tested the tunings and compared the results obtained from both the methods. We found that tuning improved the performance in majority of cases. It was also found that not all optimisation algorithms used for tuning produced the same results. As (1) there is significant improvement in performance after\u00a0\u2026", "num_citations": "2\n", "authors": ["555"]}
{"title": "Bug localization in software using NSGA-II\n", "abstract": " Finding bugs in a software is a cumbersome and tedious task. When a new bug is reported, the developers find it challenging to replicate the unexpected behavior of the software, in order to fix the original fault. In this paper, an automated model is presented to find and sort the classes present in the source code according to their proneness of containing a bug, depending upon the bug reports. The model uses a text mining approach and recommends a list of classes based upon the lexical similarity between bug reports and the API descriptions, and also the changes previously recommended during bug fixing. To maximize the similarity index and at the same time reduce the number of classes recommended, a Non-dominant Sorting Genetic Algorithm (NSGA-II) is employed. This model was evaluated on three java based open source applications and it is observed that the model created using multi-objective\u00a0\u2026", "num_citations": "2\n", "authors": ["555"]}
{"title": "Investigation of various data analysis techniques to identify change prone parts of an open source software\n", "abstract": " Identifying and examining the change-prone parts of the software is gaining wide importance in the field of software engineering. This would help software practitioners to cautiously assign the resources for testing and maintenance. Software metrics can be used for constructing various classification models which allow timely identification of change prone classes. There have been various machine learning classification models proposed in the literature. However, due to varying results across studies, more research needs to be done to increase the confidence in the results and provide a valuable conclusion. In this paper, we have used a number of data analysis techniques (14 machine learning techniques and a statistical technique) to construct change prediction models and performed statistical testing to compare the performance of these models. The application of a large number of techniques will\u00a0\u2026", "num_citations": "2\n", "authors": ["555"]}
{"title": "Common threats to software quality predictive modeling studies using search-based techniques\n", "abstract": " Development of Software Quality Predictive Models (SQPM) is an important research area as it helps in effective use of project resources and assures a good quality software product. A number of studies in literature have developed successful SQPM using search-based techniques which are meta-heuristic in nature. However, in order to perform a successful empirical study which develops SQPM using search-based techniques, it is essential to consider various probable sources of threats to the empirical study so that the developed models are realistic and efficient. This study reviews and analyzes 33 empirical studies in literature which have successfully used search-based techniques for prediction of two common software quality attributes i.e. fault-proneness and change-proneness in order to comprehensively present various probable threats to such studies. The study also proposes remedial actions to mitigate\u00a0\u2026", "num_citations": "2\n", "authors": ["555"]}
{"title": "Applicability of inter project validation for determination of change prone classes\n", "abstract": " The research in the field of defect and change proneness prediction of software has gained a lot of momentum over the past few years. Indeed, effective prediction models can help software practitioners in detecting the change prone modules of a software, allowing them to optimize the resources used for software testing. However, the development of the prediction models used to determine change prone classes are dependent on the availability of historical data from the concerned software. This can pose a challenge in the development of effective change prediction models. The aim of this paper is to address this limitation by using the data from models based on similar projects to predict the change prone classes of the concerned software. This inter project technique can facilitate the development of generalized models which can be used to ascertain change prone classes for multiple software projects. It would also lead to optimization of critical time and resources in the testing and maintenance phases. This work evaluates the effectiveness of statistical and machine learning techniques for developing such models using receiver operating characteristic analysis. The observations of the study indicate varied results for the different techniques used.", "num_citations": "2\n", "authors": ["555"]}
{"title": "Empirical Validation of an Efficient Test Data Generation Algorithm Based on Adequacy based Testing Criteria\n", "abstract": " The length and the complexity of the software are rising day by day. This rising complexity has increased the demand for techniques that can generate test data effectively. Test data generation techniques selects from the input domain of the program, those input values that satisfies a pre-defined testing criteria. In this paper, we propose a new test data generation algorithm. Our algorithm generates test data using adequacy based testing criteria that aims to generate an adequate test data set by using the concept of mutation analysis. In general, mutation analysis is applied after the test data is generated. But, our algorithm applies mutation analysis at the time of generating test data only rather than applying it after the generation of test data. Incorporation of mutation analysis at the time of test data generation leads to the generation of test data that is itself adequate and hence we need not check for its adequacy after\u00a0\u2026", "num_citations": "2\n", "authors": ["555"]}
{"title": "A Measurement Model of Reusability for Evaluating Component\n", "abstract": " Several current measurement methods for evaluating reusability of reusable component are analyzed, then a relationship of component reusability and its complexity is presented according to the two usual measurement methods of component complexity. Furthermore, a quantity method that appraises the reusability of reusable component is given.", "num_citations": "2\n", "authors": ["555"]}
{"title": "An empirical study to investigate the impact of data resampling techniques on the performance of class maintainability prediction models\n", "abstract": " With the increasing complexity of the software systems nowadays, the trend has been shifted to object-oriented (OO) development. The classes are the central construct in an OO software that are expected to be of utmost quality and high maintainability. The maintainability of a class is the probability that a class can be effortlessly modifiable in the maintenance phase. Unfortunately, it is very tough to determine the maintainability of a class with confidence before the release of the software. However, maintainability can be predicted with the help of internal quality attributes (viz. complexity, cohesion coupling, inheritance, etc.). The researchers in the literature have studied the relation amongst the internal quality attributes and class maintainability. Many class maintainability prediction models have been developed in the past with the help of internal quality attributes. Effective prediction models are vital to forecast class\u00a0\u2026", "num_citations": "1\n", "authors": ["555"]}
{"title": "Handwriting Recognition for Medical Prescriptions using a CNN-Bi-LSTM Model\n", "abstract": " It is commonly seen that it is tough to read the handwritten text from medical prescriptions. It is mostly due to the different style of handwriting and the use of Latin abbreviations for medical terms which is usually unknown to the general public. This can make it difficult for both patients and even pharmacists to read the prescription, which can have negative or even fatal consequences if read incorrectly. This paper demonstrates the use of a CNN-Bi-LSTM model along with Connectionist Temporal Classification. The prescribed model consists of three components, the convolutional layers for feature extraction, the Bi-LSTM network for making predictions for each frame of the context vector and the final decoding to translate each character in the recognized sequence by LSTM layers into an alphabetic character using the CTC loss function. A linear layer is added after the bi-LSTM layer to compute the final probabilities\u00a0\u2026", "num_citations": "1\n", "authors": ["555"]}
{"title": "Predicting Software Defects for Object-Oriented Software Using Search-based Techniques\n", "abstract": " Development without any defect is unsubstantial. Timely detection of software defects favors the proper resource utilization saving time, effort and money. With the increasing size and complexity of software, demand for accurate and efficient prediction models is increasing. Recently, search-based techniques (SBTs) have fascinated many researchers for Software Defect Prediction (SDP). The goal of this study is to conduct an empirical evaluation to assess the applicability of SBTs for predicting software defects in object-oriented (OO) softwares. In this study, 16 SBTs are exploited to build defect prediction models for 13 OO software projects. Stable performance measures\u00a0\u2014 GMean, Balance and Receiver Operating Characteristic-Area Under Curve (ROC-AUC) are employed to probe into the predictive capability of developed models, taking into consideration the imbalanced nature of software datasets. Proper\u00a0\u2026", "num_citations": "1\n", "authors": ["555"]}
{"title": "Application of Particle Swarm Optimization for Software Defect Prediction Using Object Oriented Metrics\n", "abstract": " With the growing number of software applications being developed for every small challenge, the importance of devising efficient software defect prediction models is imperative. Over the years, various machine learning techniques have been utilized to develop defect prediction model and have managed to achieve good results. In all defect prediction models, the task of correcting imbalanced data and feature selection has been of great significance. In this paper we have tried to analyze the working of the oversampling technique SMOTE along with feature selection using Particle Swarm Optimization on Object Oriented metrics. The selected features were then used to train the datasets one of the most popular classification techniques-Support Vector Machine to predict defects. The four datasets used for this study are of different Apache applications whose source code was obtained from open-source platforms and\u00a0\u2026", "num_citations": "1\n", "authors": ["555"]}
{"title": "An empirical study on predictability of software maintainability using imbalanced data\n", "abstract": " In software engineering predictive modeling, early prediction of software modules or classes that possess high maintainability effort is a challenging task. Many prediction models are constructed to predict the maintainability of software classes or modules by applying various machine learning (ML) techniques. If the software modules or classes need\u00a0high maintainability, effort would be reduced\u00a0in a dataset, and\u00a0there would be imbalanced data to train the model. The imbalanced datasets make\u00a0ML techniques bias their predictions towards low maintainability effort or majority classes, and minority class instances get discarded as noise by the machine learning (ML) techniques. In this direction, this paper presents empirical work to improve the performance of software maintainability prediction (SMP) models developed with ML techniques using imbalanced data. For developing the models, the imbalanced\u00a0\u2026", "num_citations": "1\n", "authors": ["555"]}
{"title": "A systematic literature review on empirical studies towards prediction of software maintainability\n", "abstract": " Software maintainability prediction in the earlier stages of software development involves the construction of models for the accurate estimation of maintenance effort. This guides the software practitioners to manage the resources optimally. This study aims at systematically reviewing the prediction models from January 1990 to October 2019 for predicting software maintainability. We analyze the effectiveness of these models according to various aspects. To meet the goal of the research, we have identified 36 research papers. On investigating these papers, we found that various machine learning (ML), statistical (ST), and hybridized (HB) techniques have been applied to develop prediction models to predict software maintainability. The significant finding of this review is that the overall performance of ML-based models is better than that of ST models. The use of HB techniques for prediction of software\u00a0\u2026", "num_citations": "1\n", "authors": ["555"]}
{"title": "Using Ensembles for Class-Imbalance Problem to Predict Maintainability of Open Source Software\n", "abstract": " To facilitate software maintenance and save the maintenance cost, numerous machine learning (ML) techniques have been studied to predict the maintainability of software modules or classes. An abundant amount of effort has been put by the research community to develop software maintainability prediction (SMP) models by relating software metrics to the maintainability of modules or classes. When software classes demanding the high maintainability effort (HME) are less as compared to the low maintainability effort (LME) classes, the situation leads to imbalanced datasets for training the SMP models. The imbalanced class distribution in SMP datasets could be a dilemma for various ML techniques because, in the case of an imbalanced dataset, minority class instances are either misclassified by the ML techniques or get discarded as noise. The recent development in predictive modeling has ascertained that\u00a0\u2026", "num_citations": "1\n", "authors": ["555"]}
{"title": "Transfer Learning Code Vectorizer based Machine Learning Models for Software Defect Prediction\n", "abstract": " Software development life cycle comprises of planning, design, implementation, testing and eventually, deployment. Software defect prediction can be used in the initial stages of the development life cycle for identifying defective modules. Researchers have devised various methods that can be used for effective software defect prediction. The prediction of the presence of defects or bugs in a software module can facilitate the testing process as it would enable developers and testers to allocate their time and resources on modules that are prone to defects. Transfer learning can be used for transferring knowledge obtained from one domain into the other. In this paper, we propose Transfer Learning Code Vectorizer, a novel method that derives features from the text of the software source code itself and uses those features for defect prediction. We focus on the software code and convert it into vectors using a pre\u00a0\u2026", "num_citations": "1\n", "authors": ["555"]}
{"title": "Exploiting bad-smells and object-oriented characteristics to prioritize classes for refactoring\n", "abstract": " Bad-smell indicates code-design flaws and poor software-quality that weaken software design and inversely affects software development. It also works as a catalyst for bugs and failures in the software system. Refactoring methods are used by software practitioners as corrective actions for bad-smells. The problem relies in the fact that there are over seventy refactoring methods available in literature and multiple refactoring methods can be used to nullify the effect of a particular bad-smell. So, it becomes very difficult to apply refactoring on complete source-code and almost impossible if software size is dramatically large. Thus, there arises a need for prioritizing classes in some way. This study aims at applying refactoring solution to only severely affected classes to improve the overall software quality. We proposed a framework that detects a small subset of classes from the entire source-code instantly require\u00a0\u2026", "num_citations": "1\n", "authors": ["555"]}
{"title": "Prediction of employee performance using machine learning techniques\n", "abstract": " Any business's success depends on its employees. Businesses that realize this are concerned about employee output and productivity. Productivity has a compounding effect at the different levels in the workplace, meaning that high productivity at a lower level of organization paves way for higher productivity at the higher levels of the organization. Hence, analysis of performance of employees in any organization is the need of the hour. Performance of an employee cannot be attributed to any fixed quality. Different people have different skill sets and different behavioral characteristics. Thus, performance analysis requires data to be gathered from all walks of life.", "num_citations": "1\n", "authors": ["555"]}
{"title": "Analyzing the Effectiveness of Machine Learning Algorithms for Determining Faulty Classes: A Comparative Analysis\n", "abstract": " The quality of the software can be improved by determining its faulty portions in the initial phases of the lifecycle of a software product. There are various machine learning algorithms proposed in literature studies that can be used to predict faulty classes. The machine learning algorithms determine faulty classes by using object oriented metrics as predictors. These models will allow the developers to predict faulty classes and concentrate the constraint resources in testing these weaker portions of the software. This study evaluates and compares the predictive capability of six machine learning algorithms amongst themselves and with logistic regression, a statistical algorithm for determining faulty portions of a software. The results are validated using seven open source software.", "num_citations": "1\n", "authors": ["555"]}
{"title": "Empirical assessment of feature selection techniques in defect prediction models using web applications\n", "abstract": " In order to minimize the over-fitting and related factors that are caused by the high dimensionality of the input data in software defect prediction, the attributes are often optimized using various feature selection techniques. However, the comparative performance of these selection techniques in combination with machine learning algorithms remains largely unexplored using web applications. In this work, we investigate the best possible combination of feature selection technique with machine learning algorithms, with the sample space chosen from open source Apache Click and Rave data sets. Our results are based on 945 defect prediction models derived from parametric, non-parametric and ensemble-based machine learning algorithms, for which the metrics are derived from the various filter and threshold-based ranking techniques. Friedman and Nemenyi post-hoc statistical tests are adopted to identify the\u00a0\u2026", "num_citations": "1\n", "authors": ["555"]}
{"title": "Software quality predictive modeling: An effective assessment of experimental data\n", "abstract": " A major problem faced by software project managers is to develop good quality software products within tight schedules and budget constraints [1]. Predictive modeling, in the context of software engineering relates to construction of models for estimation of software quality attributes such as defect-proneness, maintainability and effort amongst others. For developing such models, software metrics act as predictor variables as they signify various design characteristics of a software such as coupling, cohesion, inheritance and polymorphism. A number of techniques such as statistical and machine learning are available for developing predictive models.", "num_citations": "1\n", "authors": ["555"]}
{"title": "Special issue on search-based techniques and their hybridizations in software engineering\n", "abstract": " Special issue on search-based techniques and their hybridizations in software engineering | Computer Languages, Systems and Structures ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Computer Languages, Systems and Structures Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsComputer Languages, Systems and StructuresVol. , No. P2Special issue on search-based techniques and their hybridizations in software engineering research-article Special issue on search-based techniques and their hybridizations in software engineering Share on Author: Ruchika Malhotra profile image Ruchika Malhotra View Profile Authors Info & \u2026", "num_citations": "1\n", "authors": ["555"]}
{"title": "Predicting Software Maintenance Effort by Mining Software Project Reports Using Inter-Version Validation\n", "abstract": " Changes in the software are unavoidable due to an ever changing dynamic and active environment wherein expectations and requirements of the users tend to change rapidly. As a result, software needs to upgrade itself from its previous version to the next version in order to meet expectations of the user. The upgradation of the software is in terms of total number of Lines of Code (LOC) that might have been inserted, deleted or modified in moving from one version of software to the next. These changes are maintained in the change reports which constitute of the defect ID and defect description. Defect description describes the cause of defect which might have occurred in the previous version of the software due to which either new LOC needs to be inserted or existing LOC need to be deleted or modified. A lot of effort is required to correct the defects identified in software at the maintenance phase i.e., when\u00a0\u2026", "num_citations": "1\n", "authors": ["555"]}
{"title": "Automatic test data generator: A tool based on search-based techniques\n", "abstract": " Software Testing is the most time consuming activity in the software development lifecycle. It is impossible to test everything. Hence, several automated test data generation techniques have been introduced in recent times in order to reduce the effort spent during testing. Search based techniques have been found to be more efficient than normal or random testing. In this paper, we propose to demonstrate the designing framework, implementation and explore the capabilities of a tool to aid in the generation of test data. Our tool is based on generating the optimal set of test cases based on the user defined coverage criteria. We have implemented the system in C++ language and have restricted ourselves to the use of command line interface. We provide the path as well as the test cases generated to the tester making his work of testing a lot easier.", "num_citations": "1\n", "authors": ["555"]}
{"title": "Towards formalizing adaptive software services\n", "abstract": " More and more complex, distributed and software-Intensive systems are built using independently developed services. Due to various reasons, such as changes in the execution environment, these systems may need to adapt their behavior. Although, adaptation at the system level has been extensively studied, developing adaptive services to start-with has not received any significant attention. This paper describes a framework for formalizing the concept of adaptation at the service level, leading to the \"service adaptation by construction\" approach. Hence, the proposed work will help software developers in identifying the important adaptation categories at the service level.", "num_citations": "1\n", "authors": ["555"]}
{"title": "A web metric collection and reporting system\n", "abstract": " The web genre classification distinguishes between pages by means of their features such as style, presentation layout, etc rather than on the topic; improving search results returned to the user by providing genre class of a web page apart from topic. Hence, if a user is able to specify the genre of search like Help, FAQ, Wikipedia etc, chances of getting results in accordance to his interest are high. The classification of web pages into genre is a challenging task as the information is semi-structured, heterogeneous and dynamic. Therefore it is required to find appropriate features which describes web page in the context of genre to increase the genre classification and accuracy of the search result.", "num_citations": "1\n", "authors": ["555"]}
{"title": "Study & Analysis For Software Test-Data Generation using Genetic Algorithm for a Use Case\n", "abstract": " Genetic_Algorithms (GAs) have been considered to automate the generation of test-data for developed software in an efficient & quick manner for the various domain developed software. Genetic_Algorithms (GAs) are adaptive search techniques that improves the software testing technique to improve the testing-automation problems when traditional methods are considered too complex and time consuming. The concept of genetic_algorithms is used to optimize the automated generation of test-cases with the testing output effectiveness. A use case is studied by using Genetic Algorithms (GAs) based testing tool which generates the tests cases in an automated fashion. It is discussed that Genetic_Algorithms (GAs) are used to generate test cases automatically & efficiently using Genetic_Algorithms (GAs) based test generator. The major benefit in developed software program testing is its automation and easy to get results along with the quality output. This paper discusses that Genetic_Algorithms (GAs) are useful in reducing the long testing time (TT) by generating automatic test-cases using Genetic_Algorithms (GAs) based test generator.", "num_citations": "1\n", "authors": ["555"]}
{"title": "Predicting software change in an open source software using machine learning algorithms\n", "abstract": " Due to various reasons such as ever increasing demands of the customer or change in the environment or detection of a bug, changes are incorporated in a software. This results in multiple versions or evolving nature of a software. Identification of parts of a software that are more prone to changes than others is one of the important activities. Identifying change prone classes will help developers to take focused and timely preventive actions on the classes of the software with similar characteristics in the future releases. In this paper, we have studied the relationship between various object oriented (OO) metrics and change proneness. We collected a set of OO metrics and change data of each class that appeared in two versions of an open source dataset, 'Java TreeView', i.e., version 1.1.6 and version 1.0.3. Besides this, we have also predicted various models that can be used to identify change prone classes, using\u00a0\u2026", "num_citations": "1\n", "authors": ["555"]}
{"title": "Quantitative Assessment of Risks Considering Threshold Effects of Object-Oriented Metrics Using Open Source Software\n", "abstract": " Object-oriented metrics are being widely used to estimate and assess the quality of the software. Quantitative risk assessment is essential for any software product. An effective approach for quantitative risk assessment is to investigate the threshold effects of the concerned risk factors in the software. Threshold can be defined as a point above which risk is high; below that point, risk is within the required limit.", "num_citations": "1\n", "authors": ["555"]}
{"title": "Comparative Analysis of Random Forests with Statistical and Machine Learning Methods in Predicting Fault-Prone Classes\n", "abstract": " There are available metrics for predicting fault prone classes, which may help software organizations for planning and performing testing activities. This may be possible due to proper allocation of resources on fault prone parts of the design and code of the software. Hence, importance and usefulness of such metrics is understandable, but empirical validation of these metrics is always a great challenge. Random Forest (RF) algorithm has been successfully applied for solving regression and classification problems in many applications. In this work, the authors predict faulty classes/modules using object oriented metrics and static code metrics. This chapter evaluates the capability of RF algorithm and compares its performance with nine statistical and machine learning methods in predicting fault prone software classes. The authors applied RF on six case studies based on open source, commercial software and\u00a0\u2026", "num_citations": "1\n", "authors": ["555"]}
{"title": "Empirical Validation of Object-Oriented Metrics Using Discriminant Analysis for Object-Oriented Systems\n", "abstract": " The authors found the NOC metric to be insignificant (as all classes were predicted to be nonfaulty) with respect to LSF and HSF in this DA analysis. The NOC metric, however, was found to be inversely related to fault proneness with respect to MSF and USF. The results of DT and ANN also showed lower values for sensitivity of NOC for all severities of faults. Braind,[Wust], and Lounis (2001),[Gyimothy], Forenc, and Siket (2005), and [Tang], Kao, and [M.", "num_citations": "1\n", "authors": ["555"]}
{"title": "Emprical investigation to find the Effect of Design Metrics on Fault Proneness\n", "abstract": " Importance of quality software is increasing leading to development of sophisticated techniques for exploring data sets, which can be used in constructing models for predicting quality attributes. The goal of this paper is to empirically compare regression and machine learning technique to assess software quality. The study used data collected from public domain NASA data set. We find the effect of software metric on fault proneness. The fault proneness models were predicted using logistic regression and decision tree methods. The performance of the two methods was compared by Receiver Operating Characteristic (ROC) analysis. The predicted model shows that metrics are related to fault proneness. The models predict faulty classes with more than 70 percent accuracy. The study showed that machine learning method is useful in constructing software quality models. Based on these results, it is reasonable to\u00a0\u2026", "num_citations": "1\n", "authors": ["555"]}
{"title": "Improving logistic regression predictions of software quality using principal component analysis\n", "abstract": " Assessment of object-oriented systems requires robust modeling techniques. Multivariate models build to predict fault proneness can be misleading if the metrics are highly correlated Khoshgaftaar et al. propose using principal component analysis to avoid such problems. The advantages of using this technique have not been empirically demonstrated especially for objectoriented systems. Our study illustrates the use of principal component analysis to predict software quality models. We empirically explore relationship between object-oriented metrics given by Chidamber and Kamerer and Fault proneness. This study used data collected from Java applications. The study used logistic regression analysis for predicting fault prone classes. The results show that principal component analysis can improve predictive quality of software model.", "num_citations": "1\n", "authors": ["555"]}