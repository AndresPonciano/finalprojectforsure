{"title": "Introducci\u00f3n a la Miner\u00eda de Datos\n", "abstract": " El manual discurre apoy\u00e1ndose en numerosos ejemplos pr\u00e1cticos y utilizando herramientas de miner\u00eda de datos como SPSS, Clementine o WEKA, ilustrando cada t\u00e9cnica con las diferentes implementaciones que de ella proporciona cada sistema. Para facilitar la asimilaci\u00f3n de conceptos, el libro se estructura en seis partes bien diferenciadas, que se pueden seguir con varios itinerarios. Tabla de Contenido: PARTE I. Introducci\u00f3n Cap 1. Introducci\u00f3n a la Miner\u00eda de Datos Cap 2. El proceso de Extracci\u00f3n de Conocimiento PARTE II. Preparaci\u00f3n de datos Cap 3. Recopilaci\u00f3n. Almacenes de datos Cap 4. Limpieza y Transformaci\u00f3n de datos PARTE III. T\u00e9cnicas de miner\u00eda de datos Cap 6. El problema de la extracci\u00f3n de patrones Cap 7. M\u00e9todos estad\u00edsticos Cap 8. Reglas de asociaci\u00f3n y dependencias Cap 9. M\u00e9todos basados en casos, en densidad o distancia Cap 10. M\u00e9todos bayesianos Cap 11. \u00e1rboles de decisi\u00f3n y sistemas de aprendizaje de Reglas Cap 12. M\u00e9todos relacionales y otros m\u00e9todos declarativos Cap 13. Redes neuronales artificiales Cap 14. M\u00e9todos basados en n\u00facleo y m\u00e1quinas de soporte vectorial Cap 15. M\u00e9todos estoc\u00e1sticos PARTE IV. E PARTE I. INTRODUCCI\u00d3N Cap 1. Introducci\u00f3n a la Miner\u00eda de Datos Cap 2. El proceso de Extracci\u00f3n de Conocimiento PARTE II. PREPARACI\u00d3N DE DATOS Cap 3. Recopilaci\u00f3n. Almacenes de datos Cap 4. Limpieza y Transformaci\u00f3n de datos PARTE III. T\u00c9CNICAS DE MINER\u00cdA DE DATOS Cap 6. El problema de la extracci\u00f3n de patrones Cap 7. M\u00e9todos estad\u00edsticos Cap 8. Reglas de asociaci\u00f3n y dependencias Cap 9. M\u00e9todos basados en casos, en densidad o distancia\u00a0\u2026", "num_citations": "1038\n", "authors": ["1701"]}
{"title": "An experimental comparison of performance measures for classification\n", "abstract": " Performance metrics in classification are fundamental in assessing the quality of learning methods and learned models. However, many different measures have been defined in the literature with the aim of making better choices in general or for a specific application area. Choices made by one metric are claimed to be different from choices made by other metrics. In this work, we analyse experimentally the behaviour of 18 different performance metrics in several scenarios, identifying clusters and relationships between measures. We also perform a sensitivity analysis for all of them in terms of several traits: class threshold choice, separability/ranking quality, calibration performance and sensitivity to changes in prior class distribution. From the definitions and experiments, we make a comprehensive analysis of the relationships between metrics, and a taxonomy and arrangement of them according to the previous\u00a0\u2026", "num_citations": "669\n", "authors": ["1701"]}
{"title": "Learning decision trees using the area under the ROC curve\n", "abstract": " ROC analysis is increasingly being recognised as an important tool for evaluation and comparison of classifiers when the operating characteristics (ie class distribution and cost parameters) are not known at training time. Usually, each classifier is characterised by its estimated true and false positive rates and is represented by a single point in the ROC diagram. In this paper, we show how a single decision tree can represent a set of classifiers by choosing different labellings of its leaves, or equivalently, an ordering on the leaves. In this setting, rather than estimating the accuracy of a single tree, it makes more sense to use the area under the ROC curve (AUC) as a quality metric. We also propose a novel splitting criterion which chooses the split with the highest local AUC. To the best of our knowledge, this is the first probabilistic splitting criterion that is not based on weighted average impurity. We present experiments suggesting that the AUC splitting criterion leads to trees with equal or better AUC value, without sacrificing accuracy if a single labelling is chosen.", "num_citations": "398\n", "authors": ["1701"]}
{"title": "A coherent interpretation of AUC as a measure of aggregated classification performance\n", "abstract": " The area under the ROC curve (AUC), a well-known measure of ranking performance, is also often used as a measure of classification performance, aggregating over decision thresholds as well as class and cost skews. However, David Hand has recently argued that AUC is fundamentally incoherent as a measure of aggregated classifier performance and proposed an alternative measure (Hand, 2009). Specifically, Hand derives a linear relationship between AUC and expected minimum loss, where the expectation is taken over a distribution of the misclassification cost parameter that depends on the model under consideration. Replacing this distribution with a Beta (2, 2) distribution, Hand derives his alternative measure H. In this paper we offer an alternative, coherent interpretation of AUC as linearly related to expected loss. We use a distribution over cost parameter and a distribution over data points, both uniform and hence model-independent. Should one wish to consider only optimal thresholds, we demonstrate that a simple and more intuitive alternative to Hand's H measure is already available in the form of the area under the cost curve.", "num_citations": "226\n", "authors": ["1701"]}
{"title": "Volume under the ROC surface for multi-class problems\n", "abstract": " Operating Characteristic (ROC) analysis has been successfully applied to classifier problems with two classes. The Area Under the ROC Curve (AUC) has been elected as a better way to evaluate classifiers than predictive accuracy or error and has also recently used for evaluating probability estimators. However, the extension of the Area Under the ROC Curve for more than two classes has not been addressed to date, because of the complexity and elusiveness of its precise definition. Some approximations to the real AUC are used without an exact appraisal of their quality. In this paper, we present the real extension to the Area Under the ROC Curve in the form of the Volume Under the ROC Surface (VUS), showing how to compute the polytope that corresponds to the absence of classifiers (given only by the trivial classifiers), to the best classifier and to whatever set of classifiers. We compare the real VUS\u00a0\u2026", "num_citations": "223\n", "authors": ["1701"]}
{"title": "A unified view of performance metrics: Translating threshold choice into expected classification loss\n", "abstract": " Many performance metrics have been introduced in the literature for the evaluation of classification performance, each of them with different origins and areas of application. These metrics include accuracy, unweighted accuracy, the area under the ROC curve or the ROC convex hull, the mean absolute error and the Brier score or mean squared error (with its decomposition into refinement and calibration). One way of understanding the relations among these metrics is by means of variable operating conditions (in the form of misclassification costs and/or class distributions). Thus, a metric may correspond to some expected loss over different operating conditions. One dimension for the analysis has been the distribution for this range of operating conditions, leading to some important connections in the area of proper scoring rules. We demonstrate in this paper that there is an equally important dimension which has so far received much less attention in the analysis of performance metrics. This dimension is given by the decision rule, which is typically implemented as a threshold choice method when using scoring models. In this paper, we explore many old and new threshold choice methods: fixed, score-uniform, score-driven, rate-driven and optimal, among others. By calculating the expected loss obtained with these threshold choice methods for a uniform range of operating conditions we give clear interpretations of the 0-1 loss, the absolute error, the Brier score, the AUC and the refinement loss respectively. Our analysis provides a comprehensive view of performance metrics as well as a systematic approach to loss minimisation which can be\u00a0\u2026", "num_citations": "207\n", "authors": ["1701"]}
{"title": "Improving the AUC of probabilistic estimation trees\n", "abstract": " In this work we investigate several issues in order to improve the performance of probabilistic estimation trees (PETs). First, we derive a new probability smoothing that takes into account the class distributions of all the nodes from the root to each leaf. Secondly, we introduce or adapt some new splitting criteria aimed at improving probability estimates rather than improving classification accuracy, and compare them with other accuracy-aimed splitting criteria. Thirdly, we analyse the effect of pruning methods and we choose a cardinality-based pruning, which is able to significantly reduce the size of the trees without degrading the quality of the estimates. The quality of probability estimates of these three issues is evaluated by the 1-vs-1 multi-class extension of the Area Under the ROC Curve (AUC) measure, which is becoming widespread for evaluating probability estimators, ranking of predictions in particular.", "num_citations": "107\n", "authors": ["1701"]}
{"title": "Quantification via probability estimators\n", "abstract": " Quantification is the name given to a novel machine learning task which deals with correctly estimating the number of elements of one class in a set of examples. The output of a quantifier is a real value, since training instances are the same as a classification problem, a natural approach is to train a classifier and to derive a quantifier from it. Some previous works have shown that just classifying the instances and counting the examples belonging to the class of interest classify count typically yields bad quantifiers, especially when the class distribution may vary between training and test. Hence, adjusted versions of classify count have been developed by using modified thresholds. However, previous works have explicitly discarded (without a deep analysis) any possible approach based on the probability estimations of the classifier. In this paper, we present a method based on averaging the probability estimations of\u00a0\u2026", "num_citations": "105\n", "authors": ["1701"]}
{"title": "Delegating classifiers\n", "abstract": " A sensible use of classifiers must be based on the estimated reliability of their predictions. A cautious classifier would delegate the difficult or uncertain predictions to other, possibly more specialised, classifiers. In this paper we analyse and develop this idea of delegating classifiers in a systematic way. First, we design a two-step scenario where a first classifier chooses which examples to classify and delegates the difficult examples to train a second classifier. Secondly, we present an iterated scenario involving an arbitrary number of chained classifiers. We compare these scenarios to classical ensemble methods, such as bagging and boosting. We show experimentally that our approach is not far behind these methods in terms of accuracy, but with several advantages:(i) improved efficiency, since each classifier learns from fewer examples than the previous one;(ii) improved comprehensibility, since each\u00a0\u2026", "num_citations": "102\n", "authors": ["1701"]}
{"title": "Brier curves: a new cost-based visualisation of classifier performance\n", "abstract": " It is often necessary to evaluate classifier performance over a range of operating conditions, rather than as a point estimate. This is typically assessed through the construction of'curves' over a'space', visualising how one or two performance metrics vary with the operating condition. For binary classifiers in particular, cost space is a natural way of showing this range of performance, visualising loss against operating condition. However, the curves which have been traditionally drawn in cost space, known as cost curves, show the optimal loss, and hence assume knowledge of the optimal decision threshold for a given operating condition. Clearly, this leads to an optimistic assessment of classifier performance. In this paper we propose a more natural way of visualising classifier performance in cost space, which is to plot probabilistic loss on the y-axis, ie, the loss arising from the probability estimates. This new curve provides new ways of understanding classifier performance and new tools to compare classifiers. In addition, we show that the area under this curve is exactly the Brier score, one of the most popular performance metrics for probabilistic classifiers.", "num_citations": "93\n", "authors": ["1701"]}
{"title": "CRISP-DM twenty years later: From data mining processes to data science trajectories\n", "abstract": " CRISP-DM (CRoss-Industry Standard Process for Data Mining) has its origins in the second half of the nineties and is thus about two decades old. According to many surveys and user polls it is still thede factostandard for developing data mining and knowledge discovery projects. However, undoubtedly the field has moved on considerably in twenty years, with data science now the leading term being favoured over data mining. In this paper we investigate whether, and in what contexts, CRISP-DM is still fit for purpose for data science projects. We argue that if the project is goal-directed and process-driven the process model view still largely holds. On the other hand, when data science projects become more exploratory the paths that the project can take become more varied, and a more flexible model is called for. We suggest what the outlines of such a trajectory-based model might look like and how it can be used\u00a0\u2026", "num_citations": "60\n", "authors": ["1701"]}
{"title": "Incremental learning of functional logic programs\n", "abstract": " In this work, we consider the extension of the Inductive Functional Logic Programming (IFLP) framework in order to learn functions in an incremental way. In general, incremental learning is necessary when the number of examples is infinite, very large or presented one by one. We have performed this extension in the FLIP system, an implementation of the IFLP framework. Several examples of programs which have been induced indicate that our extension pays off in practice. An experimental study of some parameters which affect this efficiency is performed and some applications for programming practice are illustrated, especially small classification problems and data-mining of semi-structured data.", "num_citations": "56\n", "authors": ["1701"]}
{"title": "Cautious Classifiers.\n", "abstract": " The evaluation and use of classifiers is based on the idea that a classifier is defined as a complete function from instances to classes. Even when probabilistic classifiers are used, these are ultimately converted into categorical classifiers that must choose one class (with more or less confidence) from a set of classes. Evaluation metrics such as accuracy/error, global cost, precision, recall, f-score, specificity, sensitivity, effectiveness, macro-average, logloss, MSE or the Area Under the ROC Curve (AUC) are usually defined for \u201ccomplete\u201d classifiers. In this paper we pursue the usefulness and evaluation of \u201ccautious\u201d or \u201cpartial\u201d classifiers. A cautious classifier adds an extra class \u201cunknown\u201d to the set of the original classes. This \u201cunknown\u201d class represents the cases where the prediction is uncertain or not reliable. Now, in a cost-insensitive context, accuracy and error will not be directly related but indirectly, through the coverage index. We develop new measures, efficacy and capacity, which find a compromise between reducing the number of misclassified data (error) and reducing the number of unclassified data (abstention). Inspired by ROC analysis we introduce several techniques to choose from a set of cautious classifiers. For probabilistic classifiers we define a discretisation method for converting them into cautious classifiers by using a \u201ccaution window\u201d. We develop new response graphs to show the way in which different classifiers behave according to the size of the window and the class bias. In a cost-sensitive context, cost matrices and confusion matrices can be directly extended to account for this new class. Moreover, we extend ROC\u00a0\u2026", "num_citations": "52\n", "authors": ["1701"]}
{"title": "Decision support for data mining\n", "abstract": " In this chapter we give an introduction to ROC (\u2018receiver operating characteristics\u2019) analysis and its applications to data mining. We argue that ROC analysis provides decision support for data mining in several ways. For model selection, ROC analysis establishes a method to determine the optimal model once the operating characteristics for the model deployment context are known. We also show how ROC analysis can aid in constructing and refining models in the modeling stage.", "num_citations": "52\n", "authors": ["1701"]}
{"title": "Modifying ROC curves to incorporate predicted probabilities\n", "abstract": " The area under the ROC curve (AUC) is becoming a popular measure for the evaluation of classifiers, even more than other more classical measures, such as error/accuracy, logloss/entropy or precision. The AUC measure is specifically adequate to evaluate in two-class problems how well a model ranks a set of examples according to the probability assigned to the positive class. One shortcoming of AUC is that it ignores the probability values, and it only takes the order into account. On the other hand, logloss or MSE are alternative measures, but they only consider how well the probabilities are calibrated, and not its order. In this paper we introduce a new probabilistic version of AUC, called pAUC. This measure evaluates ranking performance, but also takes the magnitude of the probabilities into account. Secondly, we present a method for visualising a pROC curve such that the area under this curve corresponds to pAUC.", "num_citations": "51\n", "authors": ["1701"]}
{"title": "Improving performance of multiclass classification by inducing class hierarchies\n", "abstract": " In the last decades, one issue that has received a lot of attention in classification problems is how to obtain better classifications. This problem becomes even more complicated when the number of classes is high. In this multiclass scenario, it is assumed that the class labels are independent of each other, and thus, most techniques and methods proposed to improve the performance of the classifiers rely on it. An alternative way to address the multiclass problem is to hierarchically distribute the classes in a collection of multiclass subproblems by reducing the number of classes involved in each local subproblem. In this paper, we propose a new method for inducing a class hierarchy from the confusion matrix of a multiclass classifier. Then, we use the class hierarchy to learn a tree-like hierarchy of classifiers for solving the original multiclass problem in a similar way as the top-down hierarchical classification approach\u00a0\u2026", "num_citations": "50\n", "authors": ["1701"]}
{"title": "On the effect of calibration in classifier combination\n", "abstract": " A general approach to classifier combination considers each model as a probabilistic classifier which outputs a class membership posterior probability. In this general scenario, it is not only the quality and diversity of the models which are relevant, but the level of calibration of their estimated probabilities as well. In this paper, we study the role of calibration before and after classifier combination, focusing on evaluation measures such as MSE and AUC, which better account for good probability estimation than other evaluation measures. We present a series of findings that allow us to recommend several layouts for the use of calibration in classifier combination. We also empirically analyse a new non-monotonic calibration method that obtains better results for classifier combination than other monotonic calibration methods.", "num_citations": "48\n", "authors": ["1701"]}
{"title": "Calibration of machine learning models\n", "abstract": " The evaluation of machine learning models is a crucial step before their application because it is essential to assess how well a model will behave for every single case. In many real applications, not only is it important to know the \u201ctotal\u201d or the \u201caverage\u201d error of the model, it is also important to know how this error is distributed and how well confidence or probability estimations are made. Many current machine learning techniques are good in overall results but have a bad distribution assessment of the error. For these cases, calibration techniques have been developed as postprocessing techniques in order to improve the probability estimation or the error distribution of an existing model. This chapter presents the most common calibration techniques and calibration measures. Both classification and regression are covered, and a taxonomy of calibration techniques is established. Special attention is given to\u00a0\u2026", "num_citations": "43\n", "authors": ["1701"]}
{"title": "An\u00e1lisis del rendimiento acad\u00e9mico en los estudios de inform\u00e1tica de la Universidad Polit\u00e9cnica de Valencia aplicando t\u00e9cnicas de miner\u00eda de datos\n", "abstract": " En este trabajo presentamos un an\u00e1lisis del rendimiento acad\u00e9mico de los alumnos de nuevo ingreso en la titulaci\u00f3n de Ingenier\u00eda T\u00e9cnica en Inform\u00e1tica de Sistemas de la Universidad Polit\u00e9cnica de Valencia (UPV) a lo largo de tres cursos, aunque tambi\u00e9n se ha trabajado con las titulaciones de Ingenier\u00eda T\u00e9cnica en Inform\u00e1tica de Gesti\u00f3n y de Ingenier\u00eda Inform\u00e1tica. Este an\u00e1lisis relaciona el rendimiento con las caracter\u00edsticas socioecon\u00f3micas y acad\u00e9micas de los alumnos, que se obtienen en el momento de su matr\u00edcula, y que se recogen en la base de datos de la universidad. Hemos definido un indicador del rendimiento para cada alumno, teniendo en cuenta las calificaciones obtenidas y las convocatorias utilizadas.Para el estudio utilizamos t\u00e9cnicas de miner\u00eda de datos, que pretenden determinar qu\u00e9 nivel de condicionamiento existe entre dicho rendimiento y caracter\u00edsticas como el nivel de conocimientos de entrada del alumno, su contexto geogr\u00e1fico y sociocultural, etc\u2026 Esto proporciona una herramienta importante para la acci\u00f3n tutorial, que puede apoyarse en las predicciones de los modelos que se obtienen para encauzar sus recomendaciones y encuadrar las expectativas y el esfuerzo necesario para cada alumno, l\u00f3gicamente dentro de la cautela habitual a la hora de tratar modelos inferidos a partir de datos.", "num_citations": "43\n", "authors": ["1701"]}
{"title": "From ensemble methods to comprehensible models\n", "abstract": " Ensemble methods improve accuracy by combining the predictions of a set of different hypotheses. However, there are two important shortcomings associated with ensemble methods. Huge amounts of memory are required to store a set of multiple hypotheses and, more importantly, comprehensibility of a single hypothesis is lost. In this work, we devise a new method to extract one single solution from a hypothesis ensemble without using extra data, based on two main ideas: the selected solution must be similar, semantically, to the combined solution, and this similarity is evaluated through the use of a random dataset. We have implemented the method using shared ensembles, because it allows for an exponential number of potential base hypotheses. We include several experiments showing that the new method selects a single hypothesis with an accuracy which is reasonably close to the combined\u00a0\u2026", "num_citations": "43\n", "authors": ["1701"]}
{"title": "An improved model selection heuristic for AUC\n", "abstract": " The area under the ROC curve (AUC) has been widely used to measure ranking performance for binary classification tasks. AUC only employs the classifier\u2019s scores to rank the test instances; thus, it ignores other valuable information conveyed by the scores, such as sensitivity to small differences in the score values However, as such differences are inevitable across samples, ignoring them may lead to overfitting the validation set when selecting models with high AUC. This problem is tackled in this paper. On the basis of ranks as well as scores, we introduce a new metric called scored AUC (sAUC), which is the area under the sROC curve. The latter measures how quickly AUC deteriorates if positive scores are decreased. We study the interpretation and statistical properties of sAUC. Experimental results on UCI data sets convincingly demonstrate the effectiveness of the new metric for classifier evaluation\u00a0\u2026", "num_citations": "38\n", "authors": ["1701"]}
{"title": "ROC curves in cost space\n", "abstract": " ROC curves and cost curves are two popular ways of visualising classifier performance, finding appropriate thresholds according to the operating condition, and deriving useful aggregated measures such as the area under the ROC curve (AUC) or the area under the optimal cost curve. In this paper we present new findings and connections between ROC space and cost space. In particular, we show that ROC curves can be transferred to cost space by means of a very natural threshold choice method, which sets the decision threshold such that the proportion of positive predictions equals the operating condition. We call these new curves rate-driven curves, and we demonstrate that the expected loss as measured by the area under these curves is linearly related to AUC. We show that the rate-driven curves are the genuine equivalent of ROC curves in cost space, establishing a point-point rather than a point\u00a0\u2026", "num_citations": "37\n", "authors": ["1701"]}
{"title": "Wind-sensitive interpolation of urban air pollution forecasts\n", "abstract": " People living in urban areas are exposed to outdoor air pollution. Air contamination is linked to numerous premature and pre-native deaths each year. Urban air pollution is estimated to cost approximately 2% of GDP in developed countries and 5% in developing countries. Some works reckon that vehicle emissions produce over 90% of air pollution in cities in these countries. This paper presents some results in predicting and interpolating real-time urban air pollution forecasts for the city of Valencia in Spain. Although many cities provide air quality data, in many cases, this information is presented with significant delays (three hours for the city of Valencia) and it is limited to the area where the measurement stations are located. We compare several regression models able to predict the levels of four different pollutants (NO, NO2, SO2, O3) in six different locations of the city. Wind strength and direction is a key feature\u00a0\u2026", "num_citations": "23\n", "authors": ["1701"]}
{"title": "Simple mimetic classifiers\n", "abstract": " The combination of classifiers is a powerful tool to improve the accuracy of classifiers, by using the prediction of multiple models and combining them. Many practical and useful combination techniques work by using the output of several classifiers as the input of a second layer classifier. The problem of this and other multi-classifier approaches is that huge amounts of memory are required to store a set of multiple classifiers and, more importantly, the comprehensibility of a single classifier is lost and no knowledge or insight can be acquired from the model. In order to overcome these limitations, in this work we analyse the idea of \u201cmimicking\u201d the semantics of an ensemble of classifiers. More precisely, we use the combination of classifiers for labelling an invented random dataset, and then, we use this artificially labelled dataset to re-train one single model. This model has the following advantages: it is almost\u00a0\u2026", "num_citations": "23\n", "authors": ["1701"]}
{"title": "A computational analysis of general intelligence tests for evaluating cognitive development\n", "abstract": " The progression in several cognitive tests for the same subjects at different ages provides valuable information about their cognitive development. One question that has caught recent interest is whether the same approach can be used to assess the cognitive development of artificial systems. In particular, can we assess whether the \u2018fluid\u2019 or \u2018crystallised\u2019 intelligence of an artificial cognitive system is changing during its cognitive development as a result of acquiring more concepts? In this paper, we address several IQ tests problems (odd-one-out problems, Raven\u2019s Progressive Matrices and Thurstone\u2019s letter series) with a general learning system that is not particularly designed on purpose to solve intelligence tests. The goal is to better understand the role of the basic cognitive operational constructs (such as identity, difference, order, counting, logic, etc.) that are needed to solve these intelligence test problems and\u00a0\u2026", "num_citations": "20\n", "authors": ["1701"]}
{"title": "Probabilistic class hierarchies for multiclass classification\n", "abstract": " The improvement in the performance of classifiers has been the focus of attention of many researchers over the last few decades. Obtaining accurate predictions becomes more complicated as the number of classes increases. Most families of classification techniques generate models that define decision boundaries trying to separate the classes as well as possible. As an alternative, in this paper, we propose to hierarchically decompose the original multiclass problem by reducing the number of classes involved in each local subproblem. This is done by deriving a similarity matrix from the misclassification errors given by a first classifier that is learned for this, and then, using the similarity matrix to build a tree-like hierarchy of specialized classifiers. Then, we present two approaches to solve the multiclass problem: the first one traverses the tree of classifiers in a top-down manner similar to the way some hierarchical\u00a0\u2026", "num_citations": "19\n", "authors": ["1701"]}
{"title": "Aggregative quantification for regression\n", "abstract": " The problem of estimating the class distribution (or prevalence) for a new unlabelled dataset (from a possibly different distribution) is a very common problem which has been addressed in one way or another in the past decades. This problem has been recently reconsidered as a new task in data mining, renamed quantification when the estimation is performed as an aggregation (and possible adjustment) of a single-instance supervised model (e.g., a classifier). However, the study of quantification has been limited to classification, while it is clear that this problem also appears, perhaps even more frequently, with other predictive problems, such as regression. In this case, the goal is to determine a distribution or an aggregated indicator of the output variable for a new unlabelled dataset. In this paper, we introduce a comprehensive new taxonomy of quantification tasks, distinguishing between the estimation\u00a0\u2026", "num_citations": "18\n", "authors": ["1701"]}
{"title": "Web Categorisation Using Distance-Based Decision Trees1\n", "abstract": " In Web classification, web pages are assigned to pre-defined categories mainly according to their content (content mining). However, the structure of the web site might provide extra information about their category (structure mining). Traditionally, both approaches have been applied separately, or are dealt with techniques that do not generate a model, such as Bayesian techniques. Unfortunately, in some classification contexts, a comprehensible model becomes crucial. Thus, it would be interesting to apply rule-based techniques (rule learning, decision tree learning) for the web categorisation task. In this paper we outline how our general-purpose learning algorithm, the so called distance based decision tree learning algorithm (DBDT), could be used in web categorisation scenarios. This algorithm differs from traditional ones in the sense that the splitting criterion is defined by means of metric conditions (\u201cis nearer\u00a0\u2026", "num_citations": "18\n", "authors": ["1701"]}
{"title": "Decision trees for ranking: Effect of new smoothing methods, new splitting criteria and simple pruning methods\n", "abstract": " In this work we investigate several issues in order to improve the performance of probabilistic estimation trees (PETs). First, we derive a new probability smoothing that takes into account the class distributions of all the nodes from the root to each leaf. This enhances probability estimations with respect to other previous approaches without smoothing or with Laplace correction. Secondly, we introduce or adapt some new splitting criteria aimed at improving probability estimations rather than improving classification accuracy, and compare them with other accuracy-aimed splitting criteria. Thirdly, we analyse the effect of pruning methods and we choose a cardinality-based pruning, which is able to significantly reduce the size of the trees without degrading the quality of the estimations. The quality of probability estimations of these three issues is evaluated by the 1-vs-1 multi-class extension of the Area Under the ROC Curve (AUC) measure, which is becoming widespread for evaluating probability estimators, ranking of predictions in particular.", "num_citations": "18\n", "authors": ["1701"]}
{"title": "Knowledge acquisition with forgetting: an incremental and developmental setting\n", "abstract": " Identifying the balance between remembering and forgetting is the key to abstraction in the human brain and, therefore, the creation of memories and knowledge. We present an incremental, lifelong view of knowledge acquisition which tries to improve task after task by determining what to keep, consolidate and forget, overcoming the stability\u2013plasticity dilemma. Our framework can combine any rule-based inductive engine (which learns new rules) with a deductive engine (which derives a coverage graph for all rules) and integrate them into a lifelong learner. We rate rules by introducing several metrics through the first adaptation, to our knowledge, of the minimum message length (MML) principle to a coverage graph, a hierarchical assessment structure which handles evidence and rules in a unified way. The metrics are used to forget some of the worst rules and also to consolidate those selected rules that are\u00a0\u2026", "num_citations": "15\n", "authors": ["1701"]}
{"title": "Specialised Tools for Automating Data Mining for Hospital Management\n", "abstract": " This paper presents a research project which is directed to the partial automation of Data Mining (DM) in hospital information systems (HIS). We concentrate on hospital management applications and information systems such as emergencies and ward management, human resources (services, night duties, etc.), physical resources (beds, intervention theatres, etc.), etc. We have realised how the business objectives are usually the same across several hospitals and so is the information which is gathered in several HIS (even using different DBMS). This means that although the models extracted highly differ between hospitals, data mining processes are highly similar across different hospitals. We argue how a tool can be constructed in such a way that it automates many DM processes and that can be ported to other hospitals which could benefit more quickly of a first DM experience.Our work plan covers all the stages in the process of Knowledge Discovery from Databases (KDD): data cleansing, extraction and integration from the HIS and external data, construction of tasks and minable views, model generation, and finally a module to carry out and interpret their predictions. We also consider a module to perform simulations and to integrate the models extracted by the previous modules with other decision support systems as well as model monitoring.", "num_citations": "15\n", "authors": ["1701"]}
{"title": "Similarity-binning averaging: a generalisation of binning calibration\n", "abstract": " In this paper we revisit the problem of classifier calibration, motivated by the issue that existing calibration methods ignore the problem attributes (i.e., they are univariate). We propose a new calibration method inspired in binning-based methods in which the calibrated probabilities are obtained from k instances from a dataset. Bins are constructed by including the k-most similar instances, considering not only estimated probabilities but also the original attributes. This method has been tested wrt. two calibration measures, including a comparison with other traditional calibration methods. The results show that the new method outperforms the most commonly used calibration methods.", "num_citations": "14\n", "authors": ["1701"]}
{"title": "Airvlc: An application for real-time forecasting urban air pollution\n", "abstract": " This paper presents Airvlc, an application for producing real-time urban air pollution forecasts for the city of Valencia in Spain. Although many cities provide air quality data, in many cases, this information is presented with significant delays (three hours for the city of Valencia) and it is limited to the area where the measurement stations are located. The application employs regression models able to predict the levels of four different pollutants (CO, NO, PM2. 5, NO2) in three different locations of the city. These models are trained using features that represent traffic intensity, persistence of pollutants and meteorological parameters such as wind speed and temperature. We compare different learning techniques to get the better performance in the prediction of pollutants. According to our experiments, ensembles of decision trees (Random Forest) outperforms the rest of methods in almost all of our tests. Airvlc incorporates the best regression models and, by a distance-weighted combination of the predictions, is able to generate a real-time pollution map of the city of Valencia. The application also includes a warning system for sending notifications to users when a nearby risk pollution concentration is detected.", "num_citations": "13\n", "authors": ["1701"]}
{"title": "Learning with configurable operators and rl-based heuristics\n", "abstract": " In this paper, we push forward the idea of machine learning systems for which the operators can be modified and finetuned for each problem. This allows us to propose a learning paradigm where users can write (or adapt) their operators, according to the problem, data representation and the way the information should be navigated. To achieve this goal, data instances, background knowledge, rules, programs and operators are all written in the same functional language, Erlang. Since changing operators affect how the search space needs to be explored, heuristics are learnt as a result of a decision process based on reinforcement learning where each action is defined as a choice of operator and rule. As a result, the architecture can be seen as a \u2018system for writing machine learning systems\u2019 or to explore new operators.", "num_citations": "13\n", "authors": ["1701"]}
{"title": "Bagging decision multi-trees\n", "abstract": " Ensemble methods improve accuracy by combining the predictions of a set of different hypotheses. A well-known method for generating hypothesis ensembles is Bagging. One of the main drawbacks of ensemble methods in general, and Bagging in particular, is the huge amount of computational resources required to learn, store, and apply the set of models. Another problem is that even using the bootstrap technique, many simple models are similar, so limiting the ensemble diversity. In this work, we investigate an optimization technique based on sharing the common parts of the models from an ensemble formed by decision trees in order to minimize both problems. Concretely, we employ a structure called decision multi-tree which can contain simultaneously a set of decision trees and hence consider just once the \u201drepeated\u201d parts. A thorough experimental evaluation is included to show that the proposed\u00a0\u2026", "num_citations": "13\n", "authors": ["1701"]}
{"title": "The teaching size: computable teachers and learners for universal languages\n", "abstract": " The theoretical hardness of machine teaching has usually been analyzed for a range of concept languages under several variants of the teaching dimension: the minimum number of examples that a teacher needs to figure out so that the learner identifies the concept. However, for languages where concepts have structure (and hence size), such as Turing-complete languages, a low teaching dimension can be achieved at the cost of using very large examples, which are hard to process by the learner. In this paper we introduce the teaching size, a more intuitive way of assessing the theoretical feasibility of teaching concepts for structured languages. In the most general case of universal languages, we show that focusing on the total size of a witness set rather than its cardinality, we can teach all total functions that are computable within some fixed time bound. We complement the theoretical results with a\u00a0\u2026", "num_citations": "12\n", "authors": ["1701"]}
{"title": "Using negotiable features for prescription problems\n", "abstract": " Data mining is usually concerned on the construction of accurate models from data, which are usually applied to well-defined problems that can be clearly isolated and formulated independently from other problems. Although much computational effort is devoted for their training and statistical evaluation, model deployment can also represent a scientific problem, when several data mining models have to be used together, constraints appear on their application, or they have to be included in decision processes based on different rules, equations and constraints. In this paper we address the problem of combining several data mining models for objects and individuals in a common scenario, where not only we can affect decisions as the result of a change in one or more data mining models, but we have to solve several optimisation problems, such as choosing one or more inputs to get the best overall result\u00a0\u2026", "num_citations": "12\n", "authors": ["1701"]}
{"title": "Distance based generalisation\n", "abstract": " Many distance-based methods in machine learning are able to identify similar cases or prototypes from which decisions can be made. The explanation given is usually based on expressions such as \u201cbecause case a is similar to case b\u201d. However, a more general or meaningful pattern, such as \u201cbecause case a has properties x and y (as b has)\u201d is usually more difficult to find. Even in this case, the connection of this pattern with the original distance-based method is generally unclear, or even inconsistent. In this paper, we study the connection between the concept of distance (or similarity) and the concept of generalisation. More precisely, we define several conditions which, in our view, a sensible distance-based generalisation must have. From that, we are able to tell whether a generalisation operator for a pattern representation language is consistent with the metric space defined by the underlying distance\u00a0\u2026", "num_citations": "12\n", "authors": ["1701"]}
{"title": "Zipf\u2019s and benford\u2019s laws in twitter hashtags\n", "abstract": " Social networks have transformed communication dramatically in recent years through the rise of new platforms and the development of a new language of communication. This landscape requires new forms to describe and predict the behaviour of users in networks. This paper presents an analysis of the frequency distribution of hashtag popularity in Twitter conversations. Our objective is to determine if these frequency distribution follow some well-known frequency distribution that many real-life sets of numerical data satisfy. In particular, we study the similarity of frequency distribution of hashtag popularity with respect to Zipf\u2019s law, an empirical law referring to the phenomenon that many types of data in social sciences can be approximated with a Zipfian distribution. Additionally, we also analyse Benford\u2019s law, is a special case of Zipf\u2019s law, a common pattern about the frequency distribution of leading digits. In order to compute correctly the frequency distribution of hashtag popularity, we need to correct many spelling errors that Twitter\u2019s users introduce. For this purpose we introduce a new filter to correct hashtag mistake based on string distances. The experiments obtained employing datasets of Twitter streams generated under controlled conditions show that Benford\u2019s law and Zipf\u2019s law can be used to model hashtag frequency distribution.", "num_citations": "11\n", "authors": ["1701"]}
{"title": "Semi-supervised clustering algorithms for grouping scientific articles\n", "abstract": " Creating sessions in scientific conferences consists in grouping papers with common topics taking into account the size restrictions imposed by the conference schedule. Therefore, this problem can be considered as semi-supervised clustering of documents based on their content. This paper aims to propose modifications in traditional clustering algorithms to incorporate size constraints in each cluster. Specifically, two new algorithms are proposed to semi-supervised clustering, based on: binary integer linear programming with cannot-link constraints and a variation of the K-Medoids algorithm, respectively. The applicability of the proposed semi-supervised clustering methods is illustrated by addressing the problem of automatic configuration of conference schedules by clustering articles by similarity. We include experiments, applying the new techniques, over real conferences datasets: ICMLA-2014, AAAI-2013 and\u00a0\u2026", "num_citations": "10\n", "authors": ["1701"]}
{"title": "Strategies in E-Business: Positioning and Social Networking in Online Markets\n", "abstract": " \u00a9 Springer Science+ Business Media New York 2014", "num_citations": "10\n", "authors": ["1701"]}
{"title": "Similarity functions for structured data. An application to decision trees\n", "abstract": " Learning from structured data is becoming increasingly important. Besides the well-known approaches which deal directly with complex data representations (inductive logic programming and multi-relational data mining), new techniques have been recently proposed by upgrading propositional learning algorithms. Focusing on distance-based methods, these techniques are extended by incorporating similarity functions defined over structured domains, for instance a k-NN algorithm solving a graph classification problem. Since a measure between objects is the essential component for this kind of methods, this paper starts with a description of some of the recent similarity functions defined over common structured data (lists, sets, terms, etc.). However, many of the most common classification techniques, such as decision tree learning, are not distance-based methods or cannot be directly adapted to be so (as kernel methods and neural networks have been adapted). In this work, we extend decision trees to use any kind of similarity function. The method is inspired by \u201ccentre splitting\u201d, which constructs decision trees by defining splits based on the distance to two or more centroids. We include an experimental analysis with both propositional data and complex data. Apart from the advantages of the new proposed method, it can be used as an example of how other partition-based methods can be adapted to deal with distances and, hence, with structured data.", "num_citations": "10\n", "authors": ["1701"]}
{"title": "Beam search extraction and forgetting strategies on shared ensembles\n", "abstract": " Ensemble methods improve accuracy by combining the predictions of a set of different hypotheses. However, there is an important shortcoming associated with ensemble methods. Huge amounts of memory are required to store a set of multiple hypotheses. In this work, we have devised an ensemble method that partially solves this problem. The key point is that components share their common parts. We employ a multi-tree, which is a structure that can simultaneously contain an ensemble of decision trees but has the advantage that decision trees share some conditions. To construct this multi-tree, we define an algorithm based on a beam search with several extraction criteria and with several forgetting policies for the suspended nodes. Finally, we compare the behaviour of this ensemble method with some well-known methods for generating hypothesis ensembles.", "num_citations": "10\n", "authors": ["1701"]}
{"title": "Casp-dm: Context aware standard process for data mining\n", "abstract": " We propose an extension of the Cross Industry Standard Process for Data Mining (CRISPDM) which addresses specific challenges of machine learning and data mining for context and model reuse handling. This new general context-aware process model is mapped with CRISP-DM reference model proposing some new or enhanced outputs.", "num_citations": "9\n", "authors": ["1701"]}
{"title": "Hierarchical distance-based conceptual clustering\n", "abstract": " In this work we analyse the relation between hierarchical distance-based clustering and the concepts that can be obtained from the hierarchy by generalisation. Many inconsistencies may arise, because the distance and the conceptual generalisation operator are usually incompatible. To overcome this, we propose an algorithm which integrates distance-based and conceptual clustering. The new dendrograms can show when an element has been integrated to the cluster because it is near in the metric space or because it is covered by the concept. In this way, the new clustering can differ from the original one but the metric traceability is clear. We introduce three different levels of agreement between the clustering hierarchy obtained from the linkage distance and the new hierarchy, and we define properties these generalisation operators should satisfy in order to produce distance-consistent dendrograms.", "num_citations": "9\n", "authors": ["1701"]}
{"title": "Introducci\u00f3n a la miner\u00eda de datos\n", "abstract": " Paso 1. Recolecci\u00f3n de datos. El primer paso en un proyecto de Miner\u00eda de Datos es la recolecci\u00f3n de datos. Los datos de una organizaci\u00f3n habitualmente est\u00e1n almacenados en muchos sitios: Bases de Datos, Hojas de C\u00e1lculo, Archivos Planos, Bit\u00e1coras, etc. Paso 2. Depuraci\u00f3n y transformaci\u00f3n de los datos. Es el paso m\u00e1s importante de un proyecto de Miner\u00eda de Datos. Tiene como objetivo eliminar el ruido y la informaci\u00f3n irrelevante. El proceso de transformaci\u00f3n consiste en modificar los datos originales en diferentes formatos en t\u00e9rminos de tipos de datos y valores.", "num_citations": "9\n", "authors": ["1701"]}
{"title": "An integrated distance for atoms\n", "abstract": " In this work, we introduce a new distance function for data representations based on first-order logic (atoms, to be more precise) which integrates the main advantages of the distances that have been previously presented in the literature. Basically, our distance simultaneously takes into account some relevant aspects, concerning atom-based presentations, such as the position where the differences between two atoms occur (context sensitivity), their complexity (size of these differences) and how many times each difference occur (the number of repetitions). Although the distance is defined for first-order atoms, it is valid for any programming language with the underlying notion of unification. Consequently, many functional and logic programming languages can also use this distance.", "num_citations": "8\n", "authors": ["1701"]}
{"title": "ROC Analysis in Artificial Intelligence\n", "abstract": " Artificial Intelligence, ROCAI-2004. The workshop was held as part of the Sixteenth European Conference on Artificial Intelligence, ECAI\u20192004, in Valencia (Spain) on August 22, 2004. Receiver Operating Characteristic Analysis (ROC Analysis) is a powerful tool for cost/benefit analysis in decision making. Widely used in psychology and medicine for many decades, it has been introduced relatively recently in several areas of artificial intelligence: machine learning, multiagent systems, intelligent decision support and expert systems. In this context, ROC analysis provides techniques to select possibly optimal models and to discard suboptimal ones independently from (and prior to specifying) the cost context or the class distribution. Furthermore, the Area Under the ROC Curve (AUC) has been shown to be a better evaluation measure than accuracy in contexts with variable misclassification costs and/or imbalanced\u00a0\u2026", "num_citations": "8\n", "authors": ["1701"]}
{"title": "Shared ensemble learning using multi-trees\n", "abstract": " Decision tree learning is a machine learning technique that allows accurate and comprehensible models to be generated. Accuracy can be improved by ensemble methods which combine the predictions of a set of different trees. However, a large amount of resources is necessary to generate the ensemble. In this paper, we introduce a new ensemble method that minimises the usage of resources by sharing the common parts of the components of the ensemble. For this purpose, we learn a decision multi-tree instead of a decision tree. We call this newapproac h shared ensembles. The use of a multi-tree produces an exponential number of hypotheses to be combined, which provides better results than boosting/bagging. We performed several experiments, showing that the technique allows us to obtain accurate models and improves the use of resources with respect to classical ensemble methods.", "num_citations": "8\n", "authors": ["1701"]}
{"title": "Induction of decision multi-trees using levin search\n", "abstract": " In this paper, we present a method for generating very expressiv e decision trees over a functional logic language. The generation of the tree folio ws a short-to-long search which is guided by the MDL principle. Once a solution is found, the construction of the tree goes on in order to obtain more solutions ordered as well by description length. The result is a multi-tree which is populated taking into consideration computational resources according to a Levin search. Some experiments show that the method pays off in practice.", "num_citations": "8\n", "authors": ["1701"]}
{"title": "Learning mdl-guided decision trees for constructor-based languages\n", "abstract": " In this paper, we present a method for generating very expressive decision trees based on several partitions over a functional logic language. First, trees are generated top-down and partitions are selected according to the MDL principle. Secondly, not only is a decision tree obtained, but multiple decision trees can be guided as well by the MDL principle. This means that the overall search space is scoured in a short-to-long fashion, thus allowing for a better use of computational resources. The most", "num_citations": "8\n", "authors": ["1701"]}
{"title": "Learning functional logic classification concepts from databases\n", "abstract": " In this paper we address the possibilities, advantages and shortcomings of addressing different data-mining problems with the Inductive Functional Logic Programming (IFLP) paradigm. As a functional extension of the Inductive Logic Programming (ILP) approach, IFLP has all the advantages of the latter but the potential of a more natural representation language for classification, clustering and functional dependencies problems. Two issues are extremely important for successfully tackling these problems: incremental learning to handle large volumes of data and a consistent and flexible classes distribution evaluation to select among many possible hypotheses. We illustrate how these features are included in the IFLP paradigm and show some results with our system FLIP.", "num_citations": "8\n", "authors": ["1701"]}
{"title": "Fairness and missing values\n", "abstract": " The causes underlying unfair decision making are complex, being internalised in different ways by decision makers, other actors dealing with data and models, and ultimately by the individuals being affected by these decisions. One frequent manifestation of all these latent causes arises in the form of missing values: protected groups are more reluctant to give information that could be used against them, delicate information for some groups can be erased by human operators, or data acquisition may simply be less complete and systematic for minority groups. As a result, missing values and bias in data are two phenomena that are tightly coupled. However, most recent techniques, libraries and experimental results dealing with fairness in machine learning have simply ignored missing data. In this paper, we claim that fairness research should not miss the opportunity to deal properly with missing data. To support this claim, (1) we analyse the sources of missing data and bias, and we map the common causes, (2) we find that rows containing missing values are usually fairer than the rest, which should not be treated as the uncomfortable ugly data that different techniques and libraries get rid of at the first occasion, and (3) we study the trade-off between performance and fairness when the rows with missing values are used (either because the technique deals with them directly or by imputation methods). We end the paper with a series of recommended procedures about what to do with missing data when aiming for fair decision making.", "num_citations": "7\n", "authors": ["1701"]}
{"title": "SMILES: A multi-purpose learning system\n", "abstract": " A machine learning system is useful for extracting models from data that can be used for many applications such as data analysis, decision support or data mining. SMILES is a machine learning system that integrates many different features from other machine learning techniques and paradigms, and more importantly, it presents several innovations in almost all of these features, such as ensemble methods, cost-sensitive learning, and the generation of a comprehensible model from an ensemble. This paper contains a short description of the main features of the system as well as some experimental results.", "num_citations": "7\n", "authors": ["1701"]}
{"title": "Identifying dominant models when the noise context is known\n", "abstract": " In many machine learning applications the quality of features during deployment is worse than during training, as more effort and care is usually invested for the purpose of training a good model. In this paper, we consider the situation when this degradation (or noise) depends on a context whose value can be known during deployment. In this case, we can anticipate how a model will behave during deployment for different noise contexts. We introduce context plots for this, with the error on the y-axis and the level of noise on the x-axis. From these plots we can determine the dominance regions for a set of models and discard those models that are not optimal for any noise context. We perform some experiments on several classification and regression problems that show that the dominance regions can be well identified, leading to better decisions than those if the best model (without noise) is used.", "num_citations": "6\n", "authors": ["1701"]}
{"title": "On the definition of a general learning system with user-defined operators\n", "abstract": " In this paper, we push forward the idea of machine learning systems whose operators can be modified and fine-tuned for each problem. This allows us to propose a learning paradigm where users can write (or adapt) their operators, according to the problem, data representation and the way the information should be navigated. To achieve this goal, data instances, background knowledge, rules, programs and operators are all written in the same functional language, Erlang. Since changing operators affect how the search space needs to be explored, heuristics are learnt as a result of a decision process based on reinforcement learning where each action is defined as a choice of operator and rule. As a result, the architecture can be seen as a 'system for writing machine learning systems' or to explore new operators where the policy reuse (as a kind of transfer learning) is allowed. States and actions are represented in a Q matrix which is actually a table, from which a supervised model is learnt. This makes it possible to have a more flexible mapping between old and new problems, since we work with an abstraction of rules and actions. We include some examples sharing reuse and the application of the system gErl to IQ problems. In order to evaluate gErl, we will test it against some structured problems: a selection of IQ test tasks and some experiments on some structured prediction problems (list patterns).", "num_citations": "6\n", "authors": ["1701"]}
{"title": "An instantiation of hierarchical distance-based conceptual clustering for propositional learning\n", "abstract": " In this work we analyse the relationship between distance and generalisation operators for real numbers, nominal data and tuples in the context of hierarchical distance-based conceptual clustering (HDCC). HDCC is a general approach to conceptual clustering that extends the traditional algorithm for hierarchical clustering by producing conceptual generalisations of the discovered clusters. This makes it possible to combine the flexibility of changing distances for several clustering problems and the advantage of having concepts which are crucial for tasks as summarisation and descriptive data mining in general. In this work we propose a set of generalisation operators and distances for the data types mentioned before and we analyse the properties by them satisfied on the basis of three different levels of agreement between the clustering hierarchy obtained from the linkage distance and the hierarchy\u00a0\u2026", "num_citations": "6\n", "authors": ["1701"]}
{"title": "Joint cutoff probabilistic estimation using simulation: A mailing campaign application\n", "abstract": " Frequently, organisations have to face complex situations where decision making is difficult. In these scenarios, several related decisions must be made at a time, which are also bounded by constraints (e.g. inventory/stock limitations, costs, limited resources, time schedules, etc). In this paper, we present a new method to make a good global decision when we have such a complex environment with several local interwoven data mining models. In these situations, the best local cutoff for each model is not usually the best cutoff in global terms. We use simulation with Petri nets to obtain better cutoffs for the data mining models. We apply our approach to a frequent problem in customer relationship management (CRM), more specifically, a direct-marketing campaign design where several alternative products have to be offered to the same house list of customers and with usual inventory limitations. We\u00a0\u2026", "num_citations": "6\n", "authors": ["1701"]}
{"title": "Estimating the class probability threshold without training data\n", "abstract": " In this paper we analyse three different techniques to establish an optimal-cost class threshold when training data is not available. One technique is directly derived from the definition of cost, a second one is derived from a ranking of estimated probabilities and the third one is based on ROC analysis. We analyse the approaches theoretically and experimentally, applied to the adaptation of existing models. The results show that the techniques we present are better for reducing the overall cost than the classical approaches (eg oversampling) and show that cost contextualisation can be performed with good results when no data is available.", "num_citations": "6\n", "authors": ["1701"]}
{"title": "The 1st workshop on ROC analysis in artificial intelligence (ROCAI-2004)\n", "abstract": " This short report includes a summary of the presentations and discussions held during the ROCAI-2004 workshop, as well as the workshop conclusions and the future agenda. ROCAI-2004 was held in Valencia, on August the 22nd, as part of the 16th European Conference on Artificial Intelligence, ECAI-2004, in Valencia, Spain.", "num_citations": "6\n", "authors": ["1701"]}
{"title": "General-purpose declarative inductive programming with domain-specific background knowledge for data wrangling automation\n", "abstract": " Given one or two examples, humans are good at understanding how to solve a problem independently of its domain, because they are able to detect what the problem is and to choose the appropriate background knowledge according to the context. For instance, presented with the string \"8/17/2017\" to be transformed to \"17th of August of 2017\", humans will process this in two steps: (1) they recognise that it is a date and (2) they map the date to the 17th of August of 2017. Inductive Programming (IP) aims at learning declarative (functional or logic) programs from examples. Two key advantages of IP are the use of background knowledge and the ability to synthesise programs from a few input/output examples (as humans do). In this paper we propose to use IP as a means for automating repetitive data manipulation tasks, frequently presented during the process of {\\em data wrangling} in many data manipulation problems. Here we show that with the use of general-purpose declarative (programming) languages jointly with generic IP systems and the definition of domain-specific knowledge, many specific data wrangling problems from different application domains can be automatically solved from very few examples. We also propose an integrated benchmark for data wrangling, which we share publicly for the community.", "num_citations": "5\n", "authors": ["1701"]}
{"title": "airVLC: An application for visualizing wind-sensitive interpolation of urban air pollution forecasts\n", "abstract": " Air pollution has been identified as a major source of health problems for people living in cities. In this sense, it is important to identify the areas of the city that present high levels of pollutants in order to avoid them. airVLC is an application for predicting and interpolating real-time urban air pollution forecasts for the city of Valencia (Spain). We compare different regression models in order to predict the levels of four pollutants (NO, NO 2 , SO 2 , O 3 ) in the six measurement stations of the city. Since wind is a key feature in the dispersion of the pollution, we study different techniques to incorporate this factor in the models. Finally, we are able to interpolate forecasts all around the city. For this goal, we propose a new interpolation method that takes wind direction into account, improving well-known methods like IDW or Kriging. By using these pollution estimates, we are able to generate real-time pollution maps of the city\u00a0\u2026", "num_citations": "5\n", "authors": ["1701"]}
{"title": "Binarised regression tasks: methods and evaluation metrics\n", "abstract": " Some supervised tasks are presented with a numerical output but decisions have to be made in a discrete, binarised, way, according to a particular cutoff. This binarised regression task is a very common situation that requires its own analysis, different from regression and classification\u2014and ordinal regression. We first investigate the application cases in terms of the information about the distribution and range of the cutoffs and distinguish six possible scenarios, some of which are more common than others. Next, we study two basic approaches: the retraining approach, which discretises the training set whenever the cutoff is available and learns a new classifier from it, and the reframing approach, which learns a regression model and sets the cutoff when this is available during deployment. In order to assess the binarised regression task, we introduce context plots featuring error against cutoff. Two special\u00a0\u2026", "num_citations": "5\n", "authors": ["1701"]}
{"title": "Automated Data Transformation with Inductive Programming and Dynamic Background Knowledge\n", "abstract": " Data quality is vital for machine learning and data science. Despite the growing amount of data preparation tools, the most tedious data wrangling activities and feature manipulation are still partially resistant to automation because they depend heavily on domain information. For example, if the strings \u201c17th of August of 2017\u201d and \u201c2017-08-17\u201d are to be formatted into \u201c08/17/2017\u201d to be correctly recognised by a data science tool, this is generally processed in two phases:(1) they are recognised as dates and (2) some conversions are applied specific to the date domain. The dates manipulating processes, however, are very distinct from those for manipulating addresses or phone numbers. This needs enormous quantities of background knowledge, which generally becomes a bottleneck as domain and format diversity grows. We assist to relieve this issue by using inductive programming (IP) with dynamic background knowledge (BK) fuelled by a machine learning meta-model that chooses the domain, the primitives (or both) from some descriptive features of the data wrangling problem (see Figure 1). We demonstrate this for the automation of data transformation and we evaluate the approach on an integrated benchmark for data wrangling, which we share publicly for the community.Acknowledgments. This research was supported by the EU (FEDER) and the Spanish MINECO (RTI2018-094403-B-C32), Universitat Polit\u00e8cnica de Val\u00e8ncia (PAID-06-18), and the Generalitat Valenciana (PROMETEO/2019/098 and BEST/2018/027). L. Contreras-Ochando was also supported by the Spanish MECD (FPU15/03219). J. Hern\u00e1ndez-Orallo is also funded\u00a0\u2026", "num_citations": "5\n", "authors": ["1701"]}
{"title": "General-purpose inductive programming for data wrangling automation\n", "abstract": " Data acquisition, integration, transformation, cleansing and other highly tedious tasks take a large proportion of data science projects. These routine tasks are tedious basically because they are repetitive and, hence, automatable. As a consequence, progress in the automation of this process can lead to a dramatic reduction of the cost and duration of data science projects. Recently, Inductive Programming (IP) has shown a large potential as a paradigm for addressing this automation. This short paper elaborates on the recent success of induction using domain-specific languages (DSLs) for the automation of data wrangling process and advocating for the use of inductive programming over general-purpose declarative languages (GPDLs) using domain-specific background knowledge (DSBKs).", "num_citations": "5\n", "authors": ["1701"]}
{"title": "Identifying the sport activity of GPS tracks\n", "abstract": " The wide propagation of devices, such as mobile phones, that include a global positioning system (GPS) sensor has popularised the storing of geographic information for different kinds of activities, many of them recreational, such as sport. Extracting and learning knowledge from GPS data can provide useful geographic information that can be used for the design of novel applications. In this paper we address the problem of identifying the sport from a GPS track that is recorded during a sport session. For that purpose, we store 8500 GPS tracks from ten different kinds of sports. We extract twelve features that are able to represent the activity that was recorded in a GPS track. From these features several models are induced by diverse machine learning classification techniques. We study the problem from two different perspectives: flat classification, i.e, models classify the track in one of the ten possible sport types; and\u00a0\u2026", "num_citations": "5\n", "authors": ["1701"]}
{"title": "Bridging the gap between distance and generalization\n", "abstract": " Distance\u2010based and generalization\u2010based methods are two families of artificial intelligence techniques that have been successfully used over a wide range of real\u2010world problems. In the first case, general algorithms can be applied to any data representation by just changing the distance. The metric space sets the search and learning space, which is generally instance\u2010oriented. In the second case, models can be obtained for a given pattern language, which can be comprehensible. The generality\u2010ordered space sets the search and learning space, which is generally model\u2010oriented. However, the concepts of distance and generalization clash in many different ways, especially when knowledge representation is complex (e.g., structured data). This work establishes a framework where these two fields can be integrated in a consistent way. We introduce the concept of distance\u2010based generalization, which connects\u00a0\u2026", "num_citations": "5\n", "authors": ["1701"]}
{"title": "Threshold choice methods: The missing link\n", "abstract": " Many performance metrics have been introduced for the evaluation of classification performance, with different origins and niches of application: accuracy, macro-accuracy, area under the ROC curve, the ROC convex hull, the absolute error, and the Brier score (with its decomposition into refinement and calibration). One way of understanding the relation among some of these metrics is the use of variable operating conditions (either in the form of misclassification costs or class proportions). Thus, a metric may correspond to some expected loss over a range of operating conditions. One dimension for the analysis has been precisely the distribution we take for this range of operating conditions, leading to some important connections in the area of proper scoring rules. However, we show that there is another dimension which has not received attention in the analysis of performance metrics. This new dimension is given by the decision rule, which is typically implemented as a threshold choice method when using scoring models. In this paper, we explore many old and new threshold choice methods: fixed, score-uniform, score-driven, rate-driven and optimal, among others. By calculating the loss of these methods for a uniform range of operating conditions we get the 0-1 loss, the absolute error, the Brier score (mean squared error), the AUC and the refinement loss respectively. This provides a comprehensive view of performance metrics as well as a systematic approach to loss minimisation, namely: take a model, apply several threshold choice methods consistent with the information which is (and will be) available about the operating condition, and\u00a0\u2026", "num_citations": "5\n", "authors": ["1701"]}
{"title": "Minimal distance-based generalisation operators for first-order objects\n", "abstract": " Distance-based methods have been a successful family of machine learning techniques since the inception of the discipline. Basically, the classification or clustering of a new individual is determined by the distance to one or more prototypes. From a comprehensibility point of view, this is not especially problematic in propositional learning where prototypes can be regarded as a good generalisation (pattern) of a group of elements. However, for scenarios with structured data, this is no longer the case. In recent work, we developed a framework to determine whether a pattern computed by a generalisation operator is consistent w.r.t. a distance. In this way, we can determine which patterns can provide a good representation of a group of individuals belonging to a metric space. In this work, we apply this framework to analyse and define minimal distance-based generalisation operators (mg operators) for first\u00a0\u2026", "num_citations": "5\n", "authors": ["1701"]}
{"title": "Re-designing cost-sensitive decision tree learning\n", "abstract": " In this work we present several innovations in the generation of comprehensible models, by the use of a newly devised structure for the learning of multiple decision trees. In particular, we are able to obtain a set of models (a hypothesis ensemble) fi'om which we can construct a locally combined model or select the best one (the archetype of the ensemble).", "num_citations": "5\n", "authors": ["1701"]}
{"title": "Domain specific induction for data wrangling automation\n", "abstract": " Increasingly often, the available data we are analysing is messy, diverse, unstructured and incomplete, which makes its analysis more difficult. Thus, a data wrangling process is usually needed before most machine learning applications can be applied. This process aims at cleaning, transforming and combining data in order to render it in an appropriate format. Unfortunately, this process is mostly a manual and very time-consuming. In this paper we first present an approach to semi-automate some common transformations that appear in the data wrangling process, which is based on a general purpose inductive programming tool that is extended with domain-specific background knowledge. Next, we illustrate a web-based tool that allows users to provide a set of inputs and one or more examples of outputs, in such a way that a pattern is found that is applied to the rest of examples automatically by the tool.", "num_citations": "4\n", "authors": ["1701"]}
{"title": "Bike rental and weather data across dozens of cities\n", "abstract": " Effectiveness of an urban bike rental service depends on how well-planned are the layout of stations and the balancing activities which take bikes from full stations to empty stations. Layout and balancing decisions require the analysis and forecast of bike rental demand. To this date we are not aware of any public bike rental datasets which would cover many cities to facilitate largescale analyses of demand. We provide a dataset of bike rental station status logs from 3584 stations in 27 cities of 11 countries, over the period of 7 months. The dataset is accompanied with weather information from the same cities over the same period. We have performed some example analyses to demonstrate that the dataset provides a rich source for many types of analyses, including analyses about the weekly profiles, station dependencies and relationships between weather and demand. We plan to provide updates to this dataset on a regular basis.", "num_citations": "4\n", "authors": ["1701"]}
{"title": "A knowledge growth and consolidation framework for lifelong machine learning systems\n", "abstract": " A more effective vision of machine learning systems entails tools that are able to improve task after task and to reuse the patterns and knowledge that are acquired previously for future tasks. This incremental, long-life view of machine learning goes beyond most of state-of-the-art machine learning techniques that learn throw-away models. In this paper we present a long-life knowledge acquisition, evaluation and consolidation framework that is designed to work with any rule-based machine learning or inductive inference engine and integrate it into a long-life learner. In order to do that we work over the graph of working memory rules and introduce several topological metrics over it from which we derive an oblivion criterion to drop useless rules from working memory and a consolidation process to promote the rules to the knowledge base. We evaluate the framework on a series of tasks in a chess rule learning domain.", "num_citations": "4\n", "authors": ["1701"]}
{"title": "Policy reuse in a general learning framework\n", "abstract": " The reuse of knowledge which has been acquired in previous learning processes in order to improve or accelerate the learning of future tasks is an appealing idea. The knowledge transferred between tasks can be viewed as a bias in the learning of the target using the information learned in the source task", "num_citations": "4\n", "authors": ["1701"]}
{"title": "Applying distances between terms to both flat and hierarchical data\n", "abstract": " Terms are the basis for functional and logic programming representations. In turn, functional and logic programming can be used for knowledge representation in a variety of applications (knowledgebased systems, data mining, etc.). Distances between terms provide a very useful tool to compare terms and arrange the search space in many of these applications. However, distances between terms have special features which have precluded them from being used for other datatypes, such as hierarchical data or propositional data. In this paper, we explore the use of distances between terms in different scenarios: propositional data using the names of the attributes to construct the term tree (hierarchy), deriving the term tree by using attribute similarity and, finally, functional data representing hierarchies. In order to do this, we perform transformations from the original data representation to XML, thus allowing the use of term distances to directly handle objects of different degrees of \u2018hierarchisation\u2019, from flat data to fully hierarchical data.", "num_citations": "4\n", "authors": ["1701"]}
{"title": "Newton trees\n", "abstract": " This paper presents Newton trees, a redefinition of probability estimation trees (PET) based on a stochastic understanding of decision trees that follows the principle of attraction (relating mass and distance through the Inverse Square Law). The structure, application and the graphical representation of Newton trees provide a way to make their stochastically driven predictions compatible with user\u2019s intelligibility, so preserving one of the most desirable features of decision trees, comprehensibility. Unlike almost all existing decision tree learning methods, which use different kinds of partitions depending on the attribute datatype, the construction of prototypes and the derivation of probabilities from distances are identical for every datatype (nominal and numerical, but also structured). We present a way of graphically representing the original stochastic probability estimation trees using a user-friendly gravitation\u00a0\u2026", "num_citations": "4\n", "authors": ["1701"]}
{"title": "On the relationship between distance and generalisation\n", "abstract": " The most common metric spaces (eg the real numbers using the absolute difference for distance) normally have some properties (eg completeness) upon which new useful concepts and operations can be established (eg to compute the limit of a sequence). Therefore, if we aim to define a distance-based generalisation operator, we could wonder whether the metric spaces we are working with should satisfy certain conditions.In this line, we propose the following reasoning: given two objects, x and y, we notice that a concept z is more general than x and y if z somehow collects the common features of x and y. Additionally, z is likely to represent other elements besides x and y. For instance, imagine that x is a regular pentagon and y is an equilateral triangle. Then, z could be the set of all the of regular polygon with n sides. It is helpful to see that many generalisations hide a transformation which can be used to gradually convert x into y and vice-versa. Namely, the pentagon of our example can be transformed into a regular triangle by means of two basic steps. First, we convert the pentagon into a square (the square is a particular case of z) by removing one of its sides and then we do the same over the square. Methods which have to perform a search in a metric space to find a good generalisation would benefit from this property. From this observation, it seems that the metric spaces we are interested in have to preserve some kind of \u201ccontinuity\u201d in order to express proper generalisations. Let us see that both concepts (\u201ccontinuity\u201d and \u201cgradual transformations\u201d) are related.", "num_citations": "4\n", "authors": ["1701"]}
{"title": "A survey of (pseudo-distance) functions for structured-data\n", "abstract": " Learning from structured data is becoming increasingly important. Besides the well-known approaches which deals directly with complex data representation (inductive logic programming and multi relational data mining), recently new techniques have been proposed by upgrading propositional learning algorithms. Focusing on distance-based methods, they are extended by incorporating similarity functions defined over structured domains, for instance a k-NN algorithm solving a graph classification problem. Since a measure between objects is the essential component for this kind of methods, this paper consists of a brief survey about some of the recent similarity functions defined over common structured data (lists, sets, terms, etc.).", "num_citations": "4\n", "authors": ["1701"]}
{"title": "Cost-sensitive diagnosis of declarative programs\n", "abstract": " Diagnosis methods in debugging aim at detecting bugs of a program, either by comparing it with a correct specification or by the help of an oracle (typically, the user herself). Debugging techniques for declarative programs usually exploit the semantical properties of programs (and specifications) and generally try to detect one or more \u201cbuggy\u201d rules. In this way, rules are split apart in an absolute way: either they are correct or not. However, in many situations, not every error has the same consequences, an issue that is ignored by classical debugging frameworks. In this paper, we generalise debugging by considering a cost function, i.e. a function that assigns different cost values to each kind of error and different benefit values to each kind of correct response. The problem is now redefined as assigning a real-valued probability and cost to each rule, by considering each rule more or less \u201cguilty\u201d of the overall error\u00a0\u2026", "num_citations": "4\n", "authors": ["1701"]}
{"title": "Multi-dimensional roc analysis with decision trees\n", "abstract": " In this paper, we review the fundamental aspects of cost-sensitive learning and ROC analysis, highlighting the limitations of current approaches for problems with more than two classes and proposing some extensions and alternatives. In particular we address these problems for decision tree learners, although most of the results can be extended to other classifier systems. Among the new proposals included in this paper, we highlight two alternative representations for ROC curves that can be used for multidimensional problems, and new measures for evaluating cost matrix irregularity. We also discuss the problems of extending previous theoretical results on the efficient re-assignment of classes of a two-class rule-based model to multi-class problems and we outline some directions for covering a multidimensional ROC space.", "num_citations": "4\n", "authors": ["1701"]}
{"title": "Missing the missing values: The ugly duckling of fairness in machine learning\n", "abstract": " Nowadays, there is an increasing concern in machine learning about the causes underlying unfair decision making, that is, algorithmic decisions discriminating some groups over others, especially with groups that are defined over protected attributes, such as gender, race and nationality. Missing values are one frequent manifestation of all these latent causes: protected groups are more reluctant to give information that could be used against them, sensitive information for some groups can be erased by human operators, or data acquisition may simply be less complete and systematic for minority groups. However, most recent techniques, libraries and experimental results dealing with fairness in machine learning have simply ignored missing data. In this paper, we present the first comprehensive analysis of the relation between missing values and algorithmic fairness for machine learning: (1) we analyse the\u00a0\u2026", "num_citations": "3\n", "authors": ["1701"]}
{"title": "Automating common data science matrix transformations\n", "abstract": " Programming languages such as R or Python are commonplace in data science projects. However, transforming data is usually tricky and the composition of the right primitives (using the appropriate libraries) to get the most elegant code transformation is not always easy. In this paper, we present the first system that is able to automatically synthesise program snippets in R given an input data matrix and an output matrix, partially filled by the user representing the required transformation. We use the type information given by the dimensions of the matrix primitives (and other constraints) to reduce the combinatorial explosion of primitive compositions. We test the performance of our approach with a set of artificial data and real examples from Stack Overflow questions.", "num_citations": "3\n", "authors": ["1701"]}
{"title": "Identifying the machine learning family from black-box models\n", "abstract": " We address the novel question of determining which kind of machine learning model is behind the predictions when we interact with a black-box model. This may allow us to identify families of techniques whose models exhibit similar vulnerabilities and strengths. In our method, we first consider how an adversary can systematically query a given black-box model (oracle) to label an artificially-generated dataset. This labelled dataset is then used for training different surrogate models (each one trying to imitate the oracle\u2019s behaviour). The method has two different approaches. First, we assume that the family of the surrogate model that achieves the maximum Kappa metric against the oracle labels corresponds to the family of the oracle model. The other approach, based on machine learning, consists in learning a meta-model that is able to predict the model family of a new black-box model. We compare\u00a0\u2026", "num_citations": "3\n", "authors": ["1701"]}
{"title": "Modelling machine learning models\n", "abstract": " Machine learning (ML) models make decisions for governments, companies, and individuals. Accordingly, there is the increasing concern of not having a rich explanatory and predictive account of the behaviour of these ML models relative to the users\u2019 interests (goals) and (pre-)conceptions (ontologies). We argue that the recent research trends in finding better characterisations of what a ML model does are leading to the view of ML models as complex behavioural systems. A good explanation for a model should depend on how well it describes the behaviour of the model in simpler, more comprehensible, or more understandable terms according to a given context. Consequently, we claim that a more contextual abstraction is necessary (as is done in system theory and psychology), which is very much like building a subjective mind modelling problem. We bring some research evidence of how this partial\u00a0\u2026", "num_citations": "3\n", "authors": ["1701"]}
{"title": "Model integration in data mining: from local to global decisions\n", "abstract": " Machine Learning is a research area that provides algorithms and techniques that are capable of learning automatically from past experience. These techniques are essential in the area of Knowledge Discovery from Databases (KDD), whose central stage is typically referred to as Data Mining. The KDD process can be seen as the learning of a model from previous data (model generation) and the application of this model to new data (model deployment). Model deployment is very important, because people and, very especially, organisations make decisions depending on the results of the models. Usually, each model is learned independently from the others, trying to obtain the best (local) result. However, when several models have to be used together, some of them can depend on each other (eg, outputs of a model are inputs of other models) and constraints appear on their application. In this scenario, the best local decision for each individual problem could not give the best global result, or the result could be invalid if it does not fulfill the problem constraints.Customer Relationship Management (CRM) is an area that has originated real application problems where data mining and (global) optimisation need to be combined. For example, prescription problems deal about distinguishing or ranking the products to be offered to each customer (or simetrically, selecting the customers to whom we should make an offer). These areas (KDD, CRM) are lacking tools for a more holistic view of the problems and a better model integration according to their interdependencies and the global and local constraints. The classical application of data mining to\u00a0\u2026", "num_citations": "3\n", "authors": ["1701"]}
{"title": "Agrupamiento conceptual jer\u00e1rquico basado en distancias\n", "abstract": " En este trabajo de investigaci\u00f3n analizamos la relaci\u00f3n existente entre el agrupamiento jer\u00e1rquico basado en distancia y los conceptos que se pueden inducir por generalizaci\u00f3n a partir de una jerarqu\u00eda, mostrando que pueden surgir diversas inconsistencias a partir de incompatibilidades entre las distancias subyacentes y los operadores de generalizaci\u00f3n empleados. En este contexto, hemos definido un marco te\u00f3rico gen\u00e9rico en el cual, por un lado, hemos propuesto un nuevo algoritmo de agrupamiento en el que integramos el agrupamiento jer\u00e1rquico basado en distancias y el agrupamiento conceptual, permitiendo observar en las nuevas jerarqu\u00edas obtenidas si un elemento ha sido integrado a un grupo por la distancia de enlazado o porque se encuentra cubierto por el concepto asociado al grupo. Por otro lado, hemos definido tres niveles diferentes de consistencia entre los operadores de generalizaci\u00f3n y las distancias a partir de la similitud existente entre las nuevas jerarqu\u00edas conseguidas con nuestro algoritmo y las correspondientes obtenidas por el algoritmo tradicional jer\u00e1rquico. Actualmente, nos encontramos trabajando en el an\u00e1lisis de diversas instanciaciones del marco te\u00f3rico gen\u00e9rico antes mencionado.", "num_citations": "3\n", "authors": ["1701"]}
{"title": "Defining inductive operators using distances over lists\n", "abstract": " Instance-based learning is one of the most widely-used paradigms in the field of automatic induction. Several reasons back its popularity, among them, we must stand out its capability to cope with different data representations: these methods are designed on the basis of a similarity principle (similar examples should share similar properties) which makes them easily adaptable to different datatypes via redefining the similarity (distance) function. In this sense, multiple distances and similarity functions can be found in the literature.However, the most notorious downside when speaking of distance-based or similarity-based methods concerns the low expressivity of the models (if any) these methods learn. Decisions are made from expressions such as \u201cexample x is more similar or nearer to example y then\u201d which results in little practical knowledge, very specially when structured data is involved. However, in many application areas we require patterns to describe the similarities of the data. In [Estruch 2008], we have addressed and formalised the problem of turning distance-based methods outputs into comprehensible and consistent patterns. In this work, we first overview our framework and then instantiated it for the case of data represented by lists of symbols.", "num_citations": "3\n", "authors": ["1701"]}
{"title": "Some results about generalisations of graphs embedded in metric spaces\n", "abstract": " Distances and similarity functions between structured datatypes, such as graphs, have been widely employed in machine learning since they are able to identify similar cases or prototypes from which decisions can be made. In these distance-based methods the justification of the labelling of a new case a is usually based on expressions such as \u201clabel (a)= label (b) because case a is similar to case b\u201d. However, a more general or meaningful pattern, such as \u201cbecause case a has properties x and y (as b has)\u201d is usually more difficult to find. Furthermore, the connection of this pattern with the original distance-based method might be unclear, or even inconsistent. The relationship between the concept of distance (or similarity), generalisation and pattern languages has been studied in a general way in [5]. In this paper we analyse the particular case of graphs embedded in metric spaces. We also study how to characterise consistent generalisation operators for one given metric space of graphs. Then, we analyse some properties related to the pattern language and the metric space employed.", "num_citations": "3\n", "authors": ["1701"]}
{"title": "Extracci\u00f3n Autom\u00e1tica de Conocimiento en Bases de Datos e Ingenier\u00eda del Software\n", "abstract": " 1.2. T\u00e9cnicas de Aprendizaje Autom\u00e1tico. 1.3. Evaluaci\u00f3n de Hip\u00f3tesis 1.4. Inducci\u00f3n Declarativa y \u00c1rboles de Decisi\u00f3n", "num_citations": "3\n", "authors": ["1701"]}
{"title": "Aprendizaje autom\u00e1tico de programas l\u00f3gico-funcionales\n", "abstract": " En este trabajo se presenta un sistema para el aprendizaje de programas l\u00f3gico-funcionales a partir de ejemplos y de conocimiento previo.", "num_citations": "3\n", "authors": ["1701"]}
{"title": "BK-ADAPT: Dynamic background knowledge for automating data transformation\n", "abstract": " An enormous effort is usually devoted to data wrangling, the tedious process of cleaning, transforming and combining data, such that it is ready for modelling, visualisation or aggregation. Data transformation and formatting is one common task in data wrangling, which is performed by humans in two steps:(1) they recognise the specific domain of data (dates, phones, addresses, etc.) and (2) they apply conversions that are specific to that domain. However, the mechanisms to manipulate one specific domain can be unique and highly different from other domains. In this paper we present BK-ADAPT, a system that uses inductive programming (IP) with a dynamic background knowledge (BK) generated by a machine learning meta-model that selects the domain and/or the primitives from several descriptive features of the data wrangling problem. To show the performance of our method, we have created a web-based tool that allows users to provide a set of inputs and one or more examples of outputs, in such a way that the rest of examples are automatically transformed by the tool.", "num_citations": "2\n", "authors": ["1701"]}
{"title": "Setting decision thresholds when operating conditions are uncertain\n", "abstract": " The quality of the decisions made by a machine learning model depends on the data and the operating conditions during deployment. Often, operating conditions such as class distribution and misclassification costs have changed during the time since the model was trained and evaluated. When deploying a binary classifier that outputs scores, once we know the new class distribution and the new cost ratio between false positives and false negatives, there are several methods in the literature to help us choose an appropriate threshold for the classifier\u2019s scores. However, on many occasions, the information that we have about this operating condition is uncertain. Previous work has considered ranges or distributions of operating conditions during deployment, with expected costs being calculated for ranges or intervals, but still the decision for each point is made as if the operating condition were certain. The\u00a0\u2026", "num_citations": "2\n", "authors": ["1701"]}
{"title": "A dataset of attributes from papers of a machine learning conference\n", "abstract": " In this work, we present a dataset which provides information on the scientific program of a set conferences of Machine Learning. Data were extracted from the IEEE Xplore Digital Library and the official web site of the International Conference on Machine Learning Applications (ICMLA). We include data of four different editions (from 2014 to 2017). Web scrapping techniques were used to mine the data contained in these web sites. The dataset covers 448 papers presented in the conference and every paper contains 6 attributes including information about the thematic session in which they were presented in the conference. The dataset is hosted in the Mendeley Dataset Repository.", "num_citations": "2\n", "authors": ["1701"]}
{"title": "Adapting Hierarchical Multiclass Classification to changes in the target concept\n", "abstract": " Machine learning models often need to be adapted to new contexts, for instance, to deal with situations where the target concept changes. In hierarchical classification, the modularity and flexibility of learning techniques allows us to deal directly with changes in the learning problem by readapting the structure of the model, instead of having to retrain the model from the scratch. In this work, we propose a method for adapting hierarchical models to changes in the target classes. We experimentally evaluate our method over different datasets. The results show that our novel approach improves the original model, and compared to the retraining approach, it performs quite competitive while it implies a significantly smaller computational cost.", "num_citations": "2\n", "authors": ["1701"]}
{"title": "Low-level Event Detection System for Minimally-Invasive Surgery Training\n", "abstract": " We present an event detection system in a laparoscopic surgery domain, as part of a more ambitious supervision by observation project. The system, which only requires the incorporation of two cameras in a laparoscopic training box, integrates several computer vision and machine learning techniques to detect the states and movements of the elements involved in the exercise. We compare the states detected by the system with the hand-labelled ground truth, using an exercise of the domain as example. We show that the system is able to detect the events accurately.", "num_citations": "2\n", "authors": ["1701"]}
{"title": "Cycling network projects: A decision-making aid approach\n", "abstract": " Effcient and clean urban mobility is a key factor in quality of life and sustainability of towns and cities. Traditionally, cities have focused on cars and other fuel-based vehicles as transport means. However, several problems are directly linked to massive car use, particularly in terms of air pollution and traffc congestion. Several works reckon that vehicle emissions produce over 90% of air pollution. One way to reduce the  use  of  fuel-based  vehicles  (and  thus  the  emission  of  pollutants)  is to create effcient, easily accessible and secure bike lane networks which, as many studies show, promote cycling as a major mean of conveyance. In this regard, this paper presents an approach to design and calculate bike lane networks based on the use of open data about the historical use  of  a  urban  bike  rental  services.  Concretely,  we  model  this  task  as a  network  design  problem  (NDP)  and  we  study  four  di erent  optimisation  strategies  to  solve  it.  We  test  these  methods  using  data  of  the city of Valencia (Spain). Our experiments conclude that an optimisation approach based on genetic programming obtains the best performance. The proposed method can be easily used to improve or extend bike lane networks based on historic bike use data in other cities.", "num_citations": "2\n", "authors": ["1701"]}
{"title": "On the coherence of AUC\n", "abstract": " The area under the ROC curve (AUC) is a well-known measure of ranking performance, and is also often used as a measure of classification performance, aggregating over decision thresholds as well as class and cost skews. However, David Hand has recently argued that AUC is fundamentally incoherent as a measure of aggregated classifier performance and proposed an alternative measure [5]. Specifically, Hand derives a linear relationship between AUC and expected minimum loss, where the expectation is taken over a distribution of the misclassification cost parameter that depends on the model under consideration. Replacing this distribution with a Beta (2, 2) distribution, Hand derives his alternative measure H. In this paper we offer an alternative, coherent interpretation of AUC as linearly related to expected loss. We use a distribution over cost parameters and a distribution over data points, both uniform and hence modelindependent. Should one wish to consider only optimal thresholds, we demonstrate that a simple and more intuitive alternative to Hand\u2019s H measure is already available in the form of the area under the cost curve.", "num_citations": "2\n", "authors": ["1701"]}
{"title": "Data mining strategies for CRM negotiation prescription problems\n", "abstract": " In some data mining problems, there are some input features that can be freely modified at prediction time. Examples happen in retailing, prescription or control (prices, warranties, medicine doses, delivery times, temperatures, etc.). If a traditional model is learned, many possible values for the special attribute will have to be tried to attain the maximum profit. In this paper, we exploit the relationship between these modifiable (or negotiable) input features and the output to (1) change the problem presentation, possibly turning a classification problem into a regression problem, and (2) maximise profits and derive negotiation strategies. We illustrate our proposal with a paradigmatic Customer Relationship Management (CRM) problem: maximising the profit of a retailing operation where the price is the negotiable input feature. Different negotiation strategies have been experimentally tested to estimate optimal\u00a0\u2026", "num_citations": "2\n", "authors": ["1701"]}
{"title": "Generalisation Operators for Lists Embedded in a Metric Space\n", "abstract": " In some application areas, similarities and distances are used to calculate how similar two objects are in order to use these measurements to find related objects, to cluster a set of objects, to make classifications or to perform an approximate search guided by the distance. In many other application areas, we require patterns to describe similarities in the data. These patterns are usually constructed through generalisation (or specialisation) operators. For every data structure, we can define distances. In fact, we may find different distances for sets, lists, atoms, numbers, ontologies, web pages, etc. We can also define pattern languages and use generalisation operators over them. However, for many data structures, distances and generalisation operators are not consistent. For instance, for lists (or sequences), edit distances are not consistent with regular languages, since, for a regular pattern such as *a, the\u00a0\u2026", "num_citations": "2\n", "authors": ["1701"]}
{"title": "An experimental comparison of classification performance metrics\n", "abstract": " Performance metrics in classi\ufb01cation are fundamental to assess the quality of learning methods and learned models. However, many different measures havq been de\ufb01ned and choices made by one metric are different from choices madq by other metrics. In this work we analyse experimentally the behaviour of 16l different performance metrics in di_\ufb01erent scenarios, identifying different clustersl and relationships between measures.", "num_citations": "2\n", "authors": ["1701"]}
{"title": "Multi\u2010paradigm learning of declarative models\n", "abstract": " This paper abstracts the contents of the PhD dissertation which has been recently defended by the author. Machine learning is the area of computer science that is concerned with the question of how to construct computer programs that automatically improve with experience. Recently, there have been important advances in the theoretical foundations of this field. At the same time, many successful applications have been developed: systems for extracting information from databases (data mining), applications to support decisions in medicine, telephone fraud and network intrusion detection, prediction of natural disasters, email filtering, document classification, and many others. This thesis introduces novel supervised learning methods that produce accurate and comprehensible models from past experiences which minimise the costs of generation and the costs of application.", "num_citations": "2\n", "authors": ["1701"]}
{"title": "AUTOMAT [R] IX: learning simple matrix pipelines\n", "abstract": " Matrices are a very common way of representing and working with data in data science and artificial intelligence. Writing a small snippet of code to make a simple matrix transformation is frequently frustrating, especially for those people without an extensive programming expertise. We present AUTOMAT [R] IX, a system that is able to induce R program snippets from a single (and possibly partial) matrix transformation example provided by the user. Our learning algorithm is able to induce the correct matrix pipeline snippet by composing primitives from a library. Because of the intractable search space\u2014exponential on the size of the library and the number of primitives to be combined in the snippet, we speed up the process with (1) a typed system that excludes all combinations of primitives with inconsistent mapping between input and output matrix dimensions, and (2) a probabilistic model to estimate the probability\u00a0\u2026", "num_citations": "1\n", "authors": ["1701"]}
{"title": "Learning alternative ways of performing a task\n", "abstract": " A common way of learning to perform a task is to observe how it is carried out by experts. However, it is well known that for most tasks there is no unique way to perform them. This is especially noticeable the more complex the task is because factors such as the skill or the know-how of the expert may well affect the way she solves the task. In addition, learning from experts also suffers of having a small set of training examples generally coming from several experts (since experts are usually a limited and expensive resource), being all of them positive examples (i.e. examples that represent successful executions of the task). Traditional machine learning techniques are not useful in such scenarios, as they require extensive training data. Starting from very few executions of the task presented as activity sequences, we introduce a novel inductive approach for learning multiple models, with each one representing an\u00a0\u2026", "num_citations": "1\n", "authors": ["1701"]}
{"title": "Family and prejudice: A behavioural taxonomy of machine learning techniques\n", "abstract": " One classical way of characterising the rich range of machine learning techniques is by defining \u2018families\u2019, according to their formulation and learning strategy (eg, neural networks, Bayesian methods, etc.). However, this taxonomy of learning techniques does not consider the extent to which models built with techniques from the same or different family agree on their outputs, especially when their predictions have to extrapolate in sparse zones where insufficient training data was available. In this paper we present a new taxonomy of machine learning techniques for classification, where families are clustered according to their degree of (dis) agreement in behaviour considering both dense and sparse zones, using Cohen\u2019s kappa statistic. To this end, we use a representative collection of datasets and learning techniques. We finally validate the taxonomy by performing a number of experiments for technique selection\u00a0\u2026", "num_citations": "1\n", "authors": ["1701"]}
{"title": "SALER: a data science solution to detect and prevent corruption in public administration\n", "abstract": " In this paper, we introduce SALER, an ongoing project developed by the Universitat Polit\u00e8cnica de Val\u00e8ncia (Spain) which aims at detecting and preventing bad practices and fraud in public administration. The main contribution of the project is the development of a data science-based solution to systematically assist managing authorities to increase the effectiveness and efficiency when analysing fraud and corruption cases. The tool combines descriptive and predictive machine learning models with advanced statistics and visualisations. In this regard, we define a number of specific requirements in terms of questions and data analyses, as well as risk indicators and other anomaly patterns. Each of these requirements will materialize in specific visualisations, reports and dashboards included in the final solution. Several internal and external data sources are analysed and assessed to explore possible\u00a0\u2026", "num_citations": "1\n", "authors": ["1701"]}
{"title": "General-purpose Declarative Inductive Programming with Domain-Specific Background Knowledge for Data Wrangling Automation.\n", "abstract": " Given one or two examples, humans are good at understanding how to solve a problem independently of its domain, because they are able to detect what the problem is and to choose the appropriate background knowledge according to the context. For instance, presented with the string\" 8/17/2017\" to be transformed to\" 17th of August of 2017\", humans will process this in two steps:(1) they recognise that it is a date and (2) they map the date to the 17th of August of 2017. Inductive Programming (IP) aims at learning declarative (functional or logic) programs from examples. Two key advantages of IP are the use of background knowledge and the ability to synthesise programs from a few input/output examples (as humans do). In this paper we propose to use IP as a means for automating repetitive data manipulation tasks, frequently presented during the process of {\\em data wrangling} in many data manipulation\u00a0\u2026", "num_citations": "1\n", "authors": ["1701"]}
{"title": "Logging Data Scientists: Collecting Evidence for Data Science Automation\n", "abstract": " If we really want to automate data science we need to know how data scientists behave. In other words, we have to apply data science to data scientists. However, it seems very difficult to track all the activities a data scientist (or a data science team) is doing. Indeed, apart from a few surveys (about the tools and times they devote to every stage of the whole process), there is a lack of evidence about what data scientists really do and the decisions and actions they take, especially at a high granularity level. The introduction of data mining tools in the past two decades, such as SPSS Clementine (then IBM Modeler), Weka KnowledgeFlow, SAS Enterprise Miner, RapidMiner and many others that followed, made it possible, for the first time, to incorporate most of a data mining process into the same tool. However, logging the actions of the users had to be done locally, with the difficulty of obtaining a relative good number of expert experiences. Collaborative or competitive platforms such as Kaggle or Github can also be a source of data, but it is difficult to extract information about sequential workflow or the particular actions that have to be taken for all the stages of a data science project. This is aggravated by the recent \u201cback to programming\u201d trend, where the products of data scientists in these platforms are programs (usually in R or python), and not a sequence of actions over a structured set of possibilities. In fact, some tools that try to automate the process are based on the \u201cknowledge, experience and best practices\u201d of data scientists, such as DataRobot, but not based on the evidence of real logs at a high granularity level.Things are different for cloud\u00a0\u2026", "num_citations": "1\n", "authors": ["1701"]}
{"title": "Forgetting and consolidation for incremental and cumulative knowledge acquisition systems\n", "abstract": " The application of cognitive mechanisms to support knowledge acquisition is, from our point of view, crucial for making the resulting models coherent, efficient, credible, easy to use and understandable. In particular, there are two characteristic features of intelligence that are essential for knowledge development: forgetting and consolidation. Both plays an important role in knowledge bases and learning systems to avoid possible information overflow and redundancy, and in order to preserve and strengthen important or frequently used rules and remove (or forget) useless ones. We present an incremental, long-life view of knowledge acquisition which tries to improve task after task by determining what to keep, what to consolidate and what to forget, overcoming The Stability-Plasticity dilemma. In order to do that, we rate rules by introducing several metrics through the first adaptation, to our knowledge, of the Minimum Message Length (MML) principle to a coverage graph, a hierarchical assessment structure which treats evidence and rules in a unified way. The metrics are not only used to forget some of the worst rules, but also to set a consolidation process to promote those selected rules to the knowledge base, which is also mirrored by a demotion system. We evaluate the framework with a series of tasks in a chess rule learning domain.", "num_citations": "1\n", "authors": ["1701"]}
{"title": "An adaptive probabilistic classification method for dynamic class hierarchies\n", "abstract": " Many classification tasks involve a large number of categories which are ordered in a hierarchy. In hierarchical classification a model learns to distinguish between the classes by using the structure of the hierarchy. This paper investigates a hierarchical classification framework in that the class hierarchies can dynamically change from learning to deployment time. We propose a new probabilistic method that uses the hierarchy information in order to determine the predicted class. This fact makes our approach able to handle changes in the hierarchy by adapting the decisions of the model to the new class structure. To evaluate our method in this dynamic scenario, we define an evaluation metric based on the class hierarchy. We experimentally analyse the performance of our approach over a collection of datasets using different classification techniques for learning the model. The dynamic scenario defined for the experiments simulate the change of hierarchy by a process that allows us to extract different taxonomies from data. The results show that the performance of the hierarchical probabilistic model over the new hierarchy is competitive.", "num_citations": "1\n", "authors": ["1701"]}
{"title": "2014 13th International Conference on Machine Learning and Applications\n", "abstract": " [Title page iii] - IEEE Conference Publication IEEE.org IEEE Xplore IEEE-SA IEEE Spectrum More Sites Create Account Personal Sign In Personal Sign In For IEEE to continue sending you helpful information on our products and services, please consent to our updated Privacy Policy. I have read and accepted the IEEE Privacy Policy. Accept & Sign In Email Address Password Sign In Forgot Password? [Title page iii] Abstract: Presents the title page of the proceedings record. Published in: 2014 13th International Conference on Machine Learning and Applications Article #: Date of Conference: 3-6 Dec. 2014 Date Added to IEEE Xplore: 09 February 2015 ISBN Information: Electronic ISBN: 978-1-4799-7415-3 INSPEC Accession Number: Persistent Link: https://xplorestaging.ieee.org/servlet/opac?punumber=7031352 More \u00bb Publisher: IEEE IEEE Account Change Username/Password Update Address Purchase Details \u2026", "num_citations": "1\n", "authors": ["1701"]}
{"title": "Coverage graph metrics consolidation and oblivion mechanisms for lifelong machine learning\n", "abstract": " A more effective vision of machine learning systems entails tools that are able to improve task after task and that are able to reuse the patterns and knowledge that are acquired in previous tasks for future tasks. This incremental, long-life view of machine learning goes beyond most of state-of-the-art machine learning techniques that learn throw-away models. In this paper we present a long-life knowledge acquisition, evaluation and consolidation framework that is designed to work with any rule-based machine learning or inductive inference engine and integrate it into a long-life learner. In order to do that we create graph of rules for working memory and introduce several topological metrics over it that summarise the usefulness, validity and compactness of the rules. From these metrics we derive an oblivion criterion to drop useless rules from working memory and a consolidation process to promote the rules to the knowledge base. We evaluate the framework on a series of tasks in a chess rule learning domain and show how the knowledge topology evolves under the framework.", "num_citations": "1\n", "authors": ["1701"]}
{"title": "An Instantiation for Sequences of Hierarchical Distance-based Conceptual Clustering\n", "abstract": " In this work, we present an instantiation of our framework for Hierarchical Distance-based Conceptual Clustering (HDCC) using sequences, a particular kind of structured data. We analyse the relationship between distances and generalisation operators for sequences in the context of HDCC. HDCC is a general approach to conceptual clustering that extends the traditional algorithm for hierarchical clustering by producing conceptual generalisations of the discovered clusters. Since the approach is general, it allows to combine the flexibility of changing distances for different data types at the same time that we take advantage of the interpretability offered by the obtained concepts, which is central for descriptive data mining tasks. We propose here different generalisation operators for sequences and analyse how they work together with the edit and linkage distances in HDCC. This analysis is carried out based on three different properties for generalisation operators and three different levels of agreement between the clustering hierarchy obtained from the linkage distance and the hierarchy obtained by using generalisation operators.", "num_citations": "1\n", "authors": ["1701"]}
{"title": "Negotiation with Price-dependent Probability Models.\n", "abstract": " Negotiation and agreement generally require models of the peers who are involved in the negotiation. One typical area where negotiation takes place is in selling and retailing, which is also known as Customer Relationship Management (CRM). Customers and products are usually modelled using previous retailing experiences with similar or dissimilar customers and products. Machine learning is typically used to construct these models, which can be used to design mailing campaigns, to recommend new products, to do cross-selling, etc. Many CRM problems can already be solved through rankers, recommender systems, etc., provided that there are good models of customer and product behaviours available. A related but more general problem is when models are used to negotiate with one or more features of the product (or, less frequently, the customer) such as prices, bonuses, warranties, etc. Additionally, if it is possible to make several bids until an agreement is reached, methods must be devised so that the maximum profit is obtained by the seller. In this work, we present a taxonomy of CRM problems, from which we distinguish those that have already been solved and those whose solutions are still pending. Then, we extend classical purchase probability rankings to the notion of profit probability curves (price-dependent distributions), and we propose a simple negotiation solution for these cases.", "num_citations": "1\n", "authors": ["1701"]}
{"title": "Feature Dependent Models\n", "abstract": " In some data mining problems, there are some input features that can be freely modified at prediction time. Examples happen in retailing, prescription or control (prices, warranties, medicine doses, delivery times, temperatures,...). If we learn a traditional model, we will have to try many possible values for the special attribute to attain a maximum benefit. Additionally, in general, some rules are not considered between these so-called \u201cnegotiable\u201d features and the output. In this paper, we exploit these rules to (1) generate more examples and learn better models,(2) change the problem presentation, possibly turning a classification problem into a regression problem, and (3) maximise benefits and derive negotiation strategies. We illustrate our proposal with a paradigmatic problem: maximising a profit of a retailing operation where the price is the negotiable feature. We have experimentally tested different strategies to estimate optimal prices, showing that strategies based on negotiable features get higher benefits.", "num_citations": "1\n", "authors": ["1701"]}
{"title": "Una herramienta CASE para la mejora de la ense\u00f1anza de la ingenier\u00eda del software\n", "abstract": " Aunque ha habido avances importantes en la ingenier\u00eda del software en los \u00faltimos a\u00f1os, todav\u00eda no se ha percibido los efectos de la madurez de la materia en una mejora de la calidad del software desarrollado actualmente. El motivo de este estancamiento es debido principalmente a que en la mayor\u00eda de casos todav\u00eda se sigue desarrollando software de forma artesanal sin seguir una metodolog\u00eda de desarrollo de forma disciplinada y por la falta de herramientas que faciliten el seguimiento de estas metodolog\u00edas guiando a los desarrolladores en cada una de las fases de este proceso. Es por lo tanto muy importante enfatizar en los estudiantes de carreras relacionadas con la inform\u00e1tica la importancia de la aplicaci\u00f3n de metodolog\u00edas de desarrollo de software y de herramientas CASE adecuadas, de manera que en un futuro, cuando los estudiantes se integren en equipos de producci\u00f3n de software en las empresas, se pueda mejorar esta situaci\u00f3n.", "num_citations": "1\n", "authors": ["1701"]}
{"title": "Applying MLL to ILP\n", "abstract": " In Inductive Logic Programming (ILP), since logic is a complete (universal) language, infinitely many possible hypotheses are compatible (hence plausible) given the evidence. An intrinsic way of selecting the most convenient hypothesis from the set of possible theories is not only useful for model selection but it is also useful for guiding the search in the hypotheses space, as some ILP systems have done in the past. One selection/search criterion is to apply Occam\u2019s razor, ie to first select/try the simplest hypotheses which cover the evidence. In order to do this, it is necessary to measure how simple a theory is. The Minimum Message Length (MML) principle is based on information theory and it reflects Occam\u2019s razor philosophy. In this paper we present a MML method for costing both logic programs and sets of facts according to the theory. Our scheme has a solid foundation and avoids the drawbacks of previous coding schemes in ILP,", "num_citations": "1\n", "authors": ["1701"]}
{"title": "On the specificity of distance-based generalisation operators\n", "abstract": " Learning methods based on distances are widely used to deal with structured information, since several distance functions can be found for the most common sorts of data. In these algorithms the justification of the labelling of a new case is normally guided by a pattern expressing the similarity to a prototype. Other patterns based on the structure of the data (eg one saying \u201call the lists headed by the item i\u201d) could be more interesting but some requirements must be taken into account [2]. Among them, it is convenient to know how specific a pattern is. Here, we briefly present a general framework where the concept of specificity is expressed in terms of the distance and, hence, can be defined for every sort of data embedded in a metric space. In this line, it can be shown that Plotkin\u2019s lgg is a specific case of minimal generalisation.", "num_citations": "1\n", "authors": ["1701"]}
{"title": "SMILES v. 2.3\u2014A Multi-purpose Learning System\n", "abstract": " In this paper we describe SMILES, a Stunning Multi-purpose Integrated LEarning System. A machine learning system is useful for extracting models from data and hence it can be used for many applications such as data analysis, decision support or data mining. SMILES is a machine learning system that integrates many different features from other machine learning techniques and paradigms and, more importantly, it presents several innovations in almost all of these features. This report contains four major parts: a description about the system architecture, a user\u2019s manual, a more advanced section on how to take the most of the system and, finally, some brief programmer\u2019s guidelines. A complete table of all the options provided in the system is also included.", "num_citations": "1\n", "authors": ["1701"]}
{"title": "Accelerating learning by reusing abstract policies in gErl\n", "abstract": " In this paper, we present gErl [7], a general learning system for which the operators can be modified and finetuned for each problem, and its intrinsic ability to transfer learning between related tasks (as q values or models). gErl, as an Erlang-coded general learning system, allows the users write (or adapt) their own operators, according to the problem, data representation and the way the information should be navigated. Since changing operators affect how the search space needs to be explored, heuristics are learnt as a result of a decision process based on reinforcement learning where each action is defined as a choice of operator and rule. The knowledge acquired solving an specific task may be reused to accelerate the learning of new more large related tasks due to the abstraction of states and actions.", "num_citations": "1\n", "authors": ["1701"]}
{"title": "Un Sistema para la Extracci\u00f3n de Conocimiento en Bioinform\u00e1tica\n", "abstract": " El \u00e1rea del aprendizaje autom\u00e1tico, en especial el aprendizaje autom\u00e1tico inductivo, ha perseguido durante d\u00e9cadas la generaci\u00f3n autom\u00e1tica de modelos generales a partir de conjuntos de datos particulares. As\u00ed, el uso de t\u00e9cnicas de aprendizaje autom\u00e1tico para la extracci\u00f3n de conocimiento se ha convertido en una pr\u00e1ctica frecuente y ha servido de impulso y de caracter\u00edstica diferenciadora de los nuevos sistemas de miner\u00eda de datos frente a los tradicionales sistemas de an\u00e1lisis de datos. En particular, las t\u00e9cnicas de aprendizaje autom\u00e1tico se han mostrado especialmente adecuadas en \u00e1reas caracterizadas por grandes vol\u00famenes de datos pero con pocos modelos te\u00f3ricos, como es el caso del diagn\u00f3stico m\u00e9dico o la biolog\u00eda molecular. Otra de las ventajas esgrimidas a favor de esta aproximaci\u00f3n es que los modelos generados pueden f\u00e1cilmente ser adaptados ante cambios de entorno (las hip\u00f3tesis\u00a0\u2026", "num_citations": "1\n", "authors": ["1701"]}