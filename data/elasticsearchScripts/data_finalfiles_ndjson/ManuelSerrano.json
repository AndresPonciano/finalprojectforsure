{"title": "A data quality in use model for big data\n", "abstract": " Beyond the hype of Big Data, something within business intelligence projects is indeed changing. This is mainly because Big Data is not only about data, but also about a complete conceptual and technological stack including raw and processed data, storage, ways of managing data, processing and analytics. A challenge that becomes even trickier is the management of the quality of the data in Big Data environments. More than ever before the need for assessing the Quality-in-Use gains importance since the real contribution\u2013business value\u2013of data can be only estimated in its context of use. Although there exists different Data Quality models for assessing the quality of regular data, none of them has been adapted to Big Data. To fill this gap, we propose the \u201c3As Data Quality-in-Use model\u201d, which is composed of three Data Quality characteristics for assessing the levels of Data Quality-in-Use in Big Data projects\u00a0\u2026", "num_citations": "171\n", "authors": ["1214"]}
{"title": "Metrics for data warehouse conceptual models understandability\n", "abstract": " Due to the principal role of Data warehouses (DW) in making strategy decisions, data warehouse quality is crucial for organizations. Therefore, we should use methods, models, techniques and tools to help us in designing and maintaining high quality DWs. In the last years, there have been several approaches to design DWs from the conceptual, logical and physical perspectives. However, from our point of view, none of them provides a set of empirically validated metrics (objective indicators) to help the designer in accomplishing an outstanding model that guarantees the quality of the DW. In this paper, we firstly summarise the set of metrics we have defined to measure the understandability (a quality subcharacteristic) of conceptual models for DWs, and present their theoretical validation to assure their correct definition. Then, we focus on deeply describing the empirical validation process we have carried out\u00a0\u2026", "num_citations": "111\n", "authors": ["1214"]}
{"title": "Towards Data Warehouse Quality Metrics.\n", "abstract": " Organizations are adopting datawarehouses to manage information efficiently as \u201cthe\u201d main organizational asset. It is essential that we can assure the information quality of the data warehouse, as it became the main tool for strategic decisions. Information quality depends on presentation quality and the data warehouse quality. This last includes the multidimensional model quality. In the last years different authors have proposed some useful guidelines to design multidimensional models, however more objective indicators are needed to help designers and managers to develop quality datawarehouses. In this paper a first proposal of metrics for multidimensional model quality is shown together with their formal validation.", "num_citations": "97\n", "authors": ["1214"]}
{"title": "Managing software process measurement: A metamodel-based approach\n", "abstract": " The evaluation of software processes is nowadays a very important issue due to the growing interest of software companies in the improvement of the productivity and quality of delivered products. Software measurement plays a fundamental role here. Given the great diversity of entities which are candidates for measurement in the software process improvement context (process models, projects, resources, products) this measurement must be performed in a consistent and integrated way. This will facilitate the making of decisions in process improvement. In this paper, a proposal for the integrated management of the software measurement is presented. The goal is to provide companies with a generic and flexible environment for software measurement which facilitates and establishes the basis for a common and effective measurement process and which is not restricted to only one kind of software entity or to a\u00a0\u2026", "num_citations": "90\n", "authors": ["1214"]}
{"title": "Main issues in big data security\n", "abstract": " Data is currently one of the most important assets for companies in every field. The continuous growth in the importance and volume of data has created a new problem: it cannot be handled by traditional analysis techniques. This problem was, therefore, solved through the creation of a new paradigm: Big Data. However, Big Data originated new issues related not only to the volume or the variety of the data, but also to data security and privacy. In order to obtain a full perspective of the problem, we decided to carry out an investigation with the objective of highlighting the main issues regarding Big Data security, and also the solutions proposed by the scientific community to solve them. In this paper, we explain the results obtained after applying a systematic mapping study to security in the Big Data ecosystem. It is almost impossible to carry out detailed research into the entire topic of security, and the outcome of this research is, therefore, a big picture of the main problems related to security in a Big Data system, along with the principal solutions to them proposed by the research community. View Full-Text", "num_citations": "65\n", "authors": ["1214"]}
{"title": "Empirical validation of metrics for conceptual models of data warehouses\n", "abstract": " Data warehouses (DW), based on the multidimensional modeling, provide companies with huge historical information for the decision making process. As these DW\u2019s are crucial for companies in making decisions, their quality is absolutely critical. One of the main issues that influences their quality lays on the models (conceptual, logical and physical) we use to design them. In the last years, there have been several approaches to design DW\u2019s from the conceptual, logical and physical perspectives. However, from our point of view, there is a lack of more objective indicators (metrics) to guide the designer in accomplishing an outstanding model that allows us to guarantee the quality of these DW\u2019s. In this paper, we present a set of metrics to measure the quality of conceptual models for DW\u2019s. We have validated them through an empirical experiment performed by expert designers in DW\u2019s. Our experiment\u00a0\u2026", "num_citations": "59\n", "authors": ["1214"]}
{"title": "A data quality in use model for big data\n", "abstract": " Organizations are nowadays immersed in the Big Data Era. Beyond the hype of the concept of Big Data, it is true that something in the way of doing business is really changing. Although some challenges keep being the same as for regular data, with big data, the focus has changed. The reason is due to Big Data is not only data, but also a complete framework including data themselves, storage, formats, and ways of provisioning, processing and analytics. A challenge that becomes even trickier is the one concerning to the management of the quality of big data. More than ever the need for assessing the quality-in-use of big datasets gains importance since the real contribution \u2013 business value- of a dataset to a business can be only estimated in its context of use. Although there exists different data quality models to assess the quality of data there still lacks of a quality-in-use model adapted to big data. To fill\u00a0\u2026", "num_citations": "49\n", "authors": ["1214"]}
{"title": "Validating metrics for data warehouses\n", "abstract": " Organisations are adopting data warehouses to manage information efficiently as the main organisational asset. This success of data warehouses (DW) can be explained because a data warehouse is a set of data and technologies aimed at enabling the executives, managers and analysts to make better and faster decisions. Due to the principal role of data warehouses in taking strategic decisions, quality is fundamental. One of the most important factors that affects the quality of the final system is its design. Although in recent years different authors have proposed some useful guidelines to design a data warehouse, more objective indicators are needed to help designers and managers to develop quality data warehouses. A set of metrics for data warehouse models is presented, and an empirical validation is carried out in order to prove practically their usefulness as quality indicators.", "num_citations": "44\n", "authors": ["1214"]}
{"title": "Experimental validation of multidimensional data models metrics\n", "abstract": " Multidimensional data models are playing an increasingly prominent role in support of day-to-day business decisions. Due to their significance in taking strategic decisions it is fundamental to assure its quality. Although there are some useful guidelines proposals for designing multidimensional data models, objective indicators (metrics) are needed to help designers and managers to develop quality multidimensional data models. In this paper we present two metrics (number of fact tables, NFT and number of dimensional tables, NDT) we have defined for multidimensional data models and an experiment developed in order to validate them as quality indicators. As a result of this experiment it seems that the number of fact tables can be considered as a solid quality indicator of a multidimensional data model.", "num_citations": "39\n", "authors": ["1214"]}
{"title": "A set of quality indicators and their corresponding metrics for conceptual models of data warehouses\n", "abstract": " The quality of Data Warehouses is absolutely relevant for organizations in the decision making process. The sooner we can deal with quality metrics (i.e. conceptual modelling), the more willing we are in achieving a data warehouse (DW) of a high quality. From our point of view, there is a lack of more objective indicators (metrics) to guide the designer in accomplishing an outstanding model that allows us to guarantee the quality of these data warehouses. However, in some cases, the goals and purposes of the proposed metrics are not very clear on their own. Lately, quality indicators have been proposed to properly define the goals of a measurement process and group quality measures in a coherent way. In this paper, we present a framework to design metrics in which each metric is part of a quality indicator we wish to measure. In this way, our method allows us to define metrics (theoretically validated\u00a0\u2026", "num_citations": "38\n", "authors": ["1214"]}
{"title": "An experimental replication with data warehouse metrics\n", "abstract": " Data warehouses are large repositories that integrate data from several sources for analysis and decision support. Data warehouse quality is crucial, because a bad data warehouse design may lead to the rejection of the decision support system or may result in non-productive decisions. In the last years, we have been working on the definition and validation of software metrics in order to assure data warehouse quality. Some of the metrics are adapted directly from previous ones defined for relational databases, and others are specific for data warehouses. In this paper, we present part of the empirical work we have developed in order to know if the proposed metrics can be used as indicators of data warehouse quality. Previously, we have developed an experiment and its replication, and in this paper, we present the second replication we have made with the purpose of assessing data warehouse maintainability\u00a0\u2026", "num_citations": "29\n", "authors": ["1214"]}
{"title": "Specification of security constraint in UML\n", "abstract": " In recent years various initiatives for solving the problem of security in databases have arisen. However, all of them have been only partial solutions which resolved isolated problems. Consequently, a global solution to the problem has not been reached yet. We believe that a methodological approach in which security is taken into consideration from the earliest stages of the development process of the databases is the best strategy. The authors present a language for specifying security constraints in models of classes, which is one step to a complete methodology for the development of secure multilevel databases. This language is called OSCL (Object Security Constraint Language). OSCL is based on the constraint language OCL (Object Constraint Language), which is used by the current standard of modeling, UML (Unified Modeling Language).", "num_citations": "26\n", "authors": ["1214"]}
{"title": "The Talavera Manifesto for Quantum Software Engineering and Programming.\n", "abstract": " This paper presents the Talavera Manifesto for quantum software engineering and programming. This manifesto collects some principles and commitments about the quantum software engineering and programming field, as well as some calls for action. This is the result of the discussion and different viewpoints of academia and industry practitioners who joined at the first International Workshop on QuANtum SoftWare Engineering & pRogramming (QANSWER).", "num_citations": "25\n", "authors": ["1214"]}
{"title": "Un m\u00e9todo para la definici\u00f3n de m\u00e9tricas de software\n", "abstract": " Existe un gran n\u00famero de medidas para capturar atributos de los procesos y productos software. Tradicionalmente estas medidas se han realizado confiando en la sabidur\u00eda de los expertos, y si bien esta experiencia es importante, puede no ser suficiente. En la actualidad muchas de las m\u00e9tricas propuestas han fracasado, debido generalmente a que carec\u00edan de atributos necesarios para ser v\u00e1lidas y \u00fatiles para el prop\u00f3sito para el que fueron creadas. La creaci\u00f3n de una m\u00e9trica debe seguir un proceso metodol\u00f3gico que permita obtener m\u00e9tricas adecuadas a nuestros prop\u00f3sitos. En el presente trabajo se presenta un m\u00e9todo de creaci\u00f3n de m\u00e9tricas de software compuesto por cinco etapas principales y que intenta integrar todos los aspectos que deben tenerse en cuenta en el proceso y que pretende evitar los problemas asociados a la creaci\u00f3n de las m\u00e9tricas.", "num_citations": "25\n", "authors": ["1214"]}
{"title": "Measures to get better quality databases\n", "abstract": " Due to the growing complexity of information systems, continuous attention to and assessment of the quality of databases, which are the essential core of information systems, it is necessary to produce quality information systems. In a typical database design a conceptual schema which specifies the requirements of the database is first built. Even more conceptual schemas determine what information can be represented by an information system, so their quality can have a significant impact on the quality of the database which is ultimately implemented. Unfortunately, most of the work regarding conceptual schemas quality merely lists properties, without giving quantitative measures that assess the quality of such models in an objective way. In this work, we will propose a set of metrics for measuring the complexity of the well known Entity Relationship schemas, which will allow database designers to measure\u00a0\u2026", "num_citations": "16\n", "authors": ["1214"]}
{"title": "Towards a service architecture for master data exchange based on ISO 8000 with support to process large datasets\n", "abstract": " During the execution of business processes involving various organizations, Master Data is usually shared and exchanged. It is necessary to keep appropriate levels of quality in these Master Data in order to prevent problems in the business processes. Organizations can be benefitted from having information about the level of quality of master data along with the master data to support decision about the usage of data in business processes is to include information about the level of quality alongside the Master Data. ISO 8000-1x0 specifies how to add this information to the master data messages. From the clauses stated in the various part of standard we developed a reference architecture, enhanced with big data technologies to better support the management of large datasetsThe main contribution of this paper is a service architecture for Master Data Exchange supporting the requirements stated by the different\u00a0\u2026", "num_citations": "15\n", "authors": ["1214"]}
{"title": "Toward a quantum software engineering\n", "abstract": " Nowadays, we are at the dawn of a new age, the quantum era. Quantum computing is no longer a dream; it is a reality that needs to be adopted. But this new technology is taking its first steps, so we still do not have models, standards, or methods to help us in the creation of new systems and the migration of current ones. Given the current state of quantum computing, we need to go back to the path software engineering took in the last century to achieve the new golden age for quantum software engineering.", "num_citations": "14\n", "authors": ["1214"]}
{"title": "Metrics for data warehouse quality\n", "abstract": " This chapter proposes a set of metrics to assess data warehouse quality. A set of data warehouse metrics is presented, and the formal and empirical validations that have been done with them. As we consider that information is the main organizational asset, one of our primary duties should be assuring its quality. Although some interesting guidelines have been proposed for designing \u201cgood\u201d data models for data warehouses, more objective indicators are needed. Metrics are a useful objective mechanism for improving the quality of software products and also for determining the best ways to help professionals and researchers. In this way, our goal is to elaborate a set of metrics for measuring data warehouse quality which can help designers in choosing the best option among more than one alternative design.", "num_citations": "14\n", "authors": ["1214"]}
{"title": "Empowering global software development with business intelligence\n", "abstract": " Context: Global Software Development (GSD) allows companies to take advantage of talent spread across the world. Most research has been focused on the development aspect. However, little if any attention has been paid to the management of GSD projects. Studies report a lack of adequate support for management\u2019s decisions made during software development, further accentuated in GSD since information is scattered throughout multiple factories, stored in different formats and standards.Objective: This paper aims to improve GSD management by proposing a systematic method for adapting Business Intelligence techniques to software development environments. This would enhance the visibility of the development process and enable software managers to make informed decisions regarding how to proceed with GSD projects.Method: A combination of formal goal-modeling frameworks and data modeling\u00a0\u2026", "num_citations": "13\n", "authors": ["1214"]}
{"title": "Medidas para estimar el rendimiento y capacidad de los procesos software de conformidad con el est\u00e1ndar ISO/IEC 15504-5: 2006\n", "abstract": " Actualmente es importante tener un conjunto de medidas para medir las mejoras introducidas por esfuerzos de mejora de procesos de software y que en muchas ocasiones estas mejoras se miden a trav\u00e9s de procesos informales y subjetivos basados en la percepci\u00f3n de los empleados y/o auditores. En este trabajo se presenta un conjunto de medidas para medir el rendimiento1 y la capacidad de los procesos software basados en el est\u00e1ndar internacional ISO/IEC 15504. Este conjunto de medidas tienen como objetivo disminuir la subjetividad de las personas al hacer la medici\u00f3n de procesos, de tal manera que permita realizar la evaluaci\u00f3n de manera m\u00e1s formal y objetiva.", "num_citations": "13\n", "authors": ["1214"]}
{"title": "Software modernization to embrace quantum technology\n", "abstract": " Quantum Computing is becoming an increasingly mature area, with a simultaneous escalation of investment in many sectors. Quantum technology will revolutionize all the engineering fields. For example, companies will need to add quantum computing progressively to some or all of their daily operations. It is clear that all existing classical information systems cannot be done away with. Rather than that occurring, it is expected that some quantum algorithms will be added, so that they can work alongside classical information systems. There has been no systematic solution offered to deal with this challenge so far. This research proposes a software modernization approach (model-driven reengineering) designed to restructure classical systems to work in conjunction with quantum systems, thereby providing target environments that combine both of these computational paradigms. The approach proposed is\u00a0\u2026", "num_citations": "12\n", "authors": ["1214"]}
{"title": "Secure development of big data ecosystems\n", "abstract": " A Big Data environment is a powerful and complex ecosystem that helps companies extract important information from data to make the best business and strategic decisions. In this context, due to the quantity, variety, and sensitivity of the data managed by these systems, as well as the heterogeneity of the technologies involved, privacy and security especially become crucial issues. However, ensuring these concerns in Big Data environments is not a trivial issue, and it cannot be treated from a partial or isolated perspective. It must be carried out through a holistic approach, starting from the definition of requirements and policies, and being present in any relevant activity of its development and deployment. Therefore, in this paper, we propose a methodological approach for integrating security and privacy in Big Data development based on main standards and common practices. In this way, we have defined a\u00a0\u2026", "num_citations": "9\n", "authors": ["1214"]}
{"title": "Tailoring Data Quality Models Using Social Network Preferences\n", "abstract": " To succeed in their tasks, users need to manage data with the most adequate quality levels possible according to specific data quality models. Typically, data quality assessment consists of calculating a synthesizing value by means of a weighted average of values and weights associated with each data quality dimension of the data quality model. We shall study not only the overall perception of the level of importance for the set of users carrying out similar tasks, but also the different issues that can influence the selection of the data quality dimensions for the model. The core contribution of this paper is a framework for representing and managing data quality models using social networks. The framework includes a proposal for a data model for social networks centered on data quality (DQSN), and an extensible set of operators based on soft-computing theories for corresponding operations.", "num_citations": "9\n", "authors": ["1214"]}
{"title": "I8K| DQ-BigData: I8K architecture extension for data quality in big data\n", "abstract": " During the execution of business processes  involving various organizations, Master Data is usually shared and exchanged. It is necessary to keep appropriate levels of quality in these Master Data, in order to prevent defects and failures in the business processes. A way to support the decision about the usage of data in business processes is to include information about the level of quality alongside the Master Data. ISO/TS 8000 parts 100 to 140, may support the provision of this kind of information in a usable manner. Specifically I8K, a reference implementation from academic sources of the aforementioned standard parts (ISO/TS 8000:100-140), may be used for this objective. Regrettably, I8K is not aimed to support the assessment of large Master Data volumes and does not reach the required efficiency in Big Data surroundings. This paper describe an extension of I8K to resolve those problems of\u00a0\u2026", "num_citations": "8\n", "authors": ["1214"]}
{"title": "Towards a Metrics Suite for Conceptual Models of Datawarehouses.\n", "abstract": " Nowadays most organizations have incorporated datawarehouses as one of their principal assets for the efficient management of information. It is vital to be able to guarantee the quality of the information that is stored in the datawarehouses given that they have become the principal tool for strategic decision making. The quality of the information depends on the quality of its presentation and the quality of the datawarehouse. The latter includes the quality of the multidimensional model, at a conceptual, logical, and physical level. Over recent years we have proposed and validated several metrics for the evaluation of the complexity of the multidimensional star model (at a logical level). In this article we present an initial proposal of metrics for the multidimensional model at a conceptual level and for their theoretical validation.", "num_citations": "7\n", "authors": ["1214"]}
{"title": "Improving incident response in big data ecosystems by using blockchain technologies\n", "abstract": " Big data ecosystems are increasingly important for the daily activities of any type of company. They are decisive elements in the organization, so any malfunction of this environment can have a great impact on the normal functioning of the company; security is therefore a crucial aspect of this type of ecosystem. When approaching security in big data as an issue, it must be considered not only during the creation and implementation of the big data ecosystem, but also throughout its entire lifecycle, including operation, and especially when managing and responding to incidents that occur. To this end, this paper proposes an incident response process supported by a private blockchain network that allows the recording of the different events and incidents that occur in the big data ecosystem. The use of blockchain enables the security of the stored data to be improved, increasing its immutability and traceability. In addition, the stored records can help manage incidents and anticipate them, thereby minimizing the costs of investigating their causes; that facilitates forensic readiness. This proposal integrates with previous research work, seeking to improve the security of big data by creating a process of secure analysis, design, and implementation, supported by a security reference architecture that serves as a guide in defining the different elements of this type of ecosystem. Moreover, this paper presents a case study in which the proposal is being implemented by using big data and blockchain technologies, such as Apache Spark or Hyperledger Fabric. View Full-Text", "num_citations": "6\n", "authors": ["1214"]}
{"title": "La Experimentaci\u00f3n en la docencia de Ingenier\u00eda del Software\n", "abstract": " Los estudios emp\u00edricos en Ingenier\u00eda del Software son fundamentales para la validaci\u00f3n de diversos m\u00e9todos, t\u00e9cnicas, herramientas, etc., y los alumnos juegan un papel fundamental a la hora de llevarlos a cabo. Estos estudios no permiten obtener beneficios centrados exclusivamente en los aspectos de investigaci\u00f3n, sino que es muy importante considerar tambi\u00e9n sus beneficios en la docencia. En este art\u00edculo se estudia la aplicaci\u00f3n de experimentos controlados en cursos de Ingenier\u00eda de Software, destacando los beneficios que estos estudios aportan a los alumnos ya los docentes e investigadores que los llevan a cabo. Adem\u00e1s, se presentan los resultados obtenidos en la realizaci\u00f3n de tres experimentos en cursos de ingenier\u00eda del software. Como consecuencia de llevar a cabo estos experimentos se han obtenido importantes beneficios pedag\u00f3gicos.", "num_citations": "6\n", "authors": ["1214"]}
{"title": "Materiales multimedia para la orientaci\u00f3n profesional\n", "abstract": " Nos ha tocado vivir en una sociedad en profundo cambio, en la que el reto de los procesos educativos y formativos habr\u00e1 de pasar\u2013en una sociedad como la nuestra, tecnol\u00f3gicamente avanzada, donde la transformaci\u00f3n se est\u00e1 produciendo en todos los \u00f3rdenes de la vida\u2013, por no perder de vista lo fundamental, la integraci\u00f3n de los individuos que la componen en la complejidad de los procesos que se establecen, desde una concepci\u00f3n humanista, donde la persona es el referente principal, alrededor del cual deben girar todos los procesos, sean estos productivo-econ\u00f3micos, participativo-culturales o sociales en general (Bermejo, 2002) 1.", "num_citations": "6\n", "authors": ["1214"]}
{"title": "Measuring Oracle database schemas\n", "abstract": " Software measurement is an effective means to manage software development and maintenance projects. In the past decades a huge amount of software metrics has been proposed, but primarily focused on programs. Metrics for databases have been neglected, mainly because databases have developed a secondary role in Information Systems (IS) infrastructure until a few years ago. But nowadays, databases are the core of IS, influencing considerably their complexity. This paper proposes a metrics suite for measuring relational database complexity, and apply them to the ORACLE Dictionary. IMACS/IEEE CSCC'99 Proceedings, Pages: 7101-7107", "num_citations": "6\n", "authors": ["1214"]}
{"title": "Visualisation environment for global software development management\n", "abstract": " Global software development (GSD) involves new challenges that need to be addressed when project managers have to make significant decisions such as task allocation, resource assignments, etc. Visualisation techniques can be useful as regards helping managers to process complex information and interpret the data shown. The main goal of this study is to describe a visualisation environment with which to support the decision-making process in GSD contexts. The environment contains a set of visualisation metaphors that can be organised in a hierarchical manner and which show both the information related to GSD projects and subprojects and the organisational context of companies and their corresponding factories. The DESGLOSA tool has been developed to support the visualisation of measures and indicators in GSD. The tool has been applied in one of the INDRA Company's GSD projects and two\u00a0\u2026", "num_citations": "5\n", "authors": ["1214"]}
{"title": "Estimating the performance and capacity of software processes according to ISO/IEC 15504\n", "abstract": " At the moment there is no set of metrics which measures the improvements brought in by efforts to make software processes better. It is often the case that these improvements are measured using informal and subjective processes based on the perception of employees and/or auditors. Bearing all this in mind, this work presents a set of measurements for gauging the performance and capability of software processes, based on the international standard ISO/IEC 15504. This set of metrics aims to lower the level of subjectivity of people when measuring the processes. A more objective and hence more formal evaluation is thus achieved.", "num_citations": "5\n", "authors": ["1214"]}
{"title": "A security pattern for key-value NoSQL database authorization\n", "abstract": " Numerous authorization models have been proposed for relational databases. On the other hand, several NoSQL databases used in Big Data applications use a new model appropriate to their requirements for structure, speed, and large amount of data. This model protects each individual cell in key-value databases by labeling them with authorization rights following a Role-Based Access Control model or similar. We present here a pattern to describe this model as it exists in several Big Data systems.", "num_citations": "4\n", "authors": ["1214"]}
{"title": "MANTICA: Una Herramienta de M\u00e9tricas para Modelos de Datos\n", "abstract": " La creciente demanda de sistemas de informaci\u00f3n (SI) de calidad ha hecho de la calidad un factor de discriminaci\u00f3n entre productos. Dado el rol fundamental que juegan los datos en un SI creemos necesario enfocar la evaluaci\u00f3n de su calidad centr\u00e1ndonos en la calidad de los modelos de datos. El modelo de datos conceptual es la base para todo el trabajo de dise\u00f1o posterior, y es un factor determinante en la calidad del dise\u00f1o del sistema global. Por lo tanto creemos que es fundamental poder evaluar la calidad de los modelos de datos, para contribuir al desarrollo SI de calidad. En este trabajo proponemos un conjunto m\u00e9tricas que permiten evaluar la calidad de los diagramas entidad interrelaci\u00f3n (ER). Pero el objetivo principal de este art\u00edculo no es la definici\u00f3n de m\u00e9tricas, sino mostrar el dise\u00f1o de una herramienta gen\u00e9rica de m\u00e9tricas, MANTICA. El dise\u00f1o gen\u00e9rico de esta herramienta hace que sea muy flexible y f\u00e1cilmente extensible par medir cualquier modelo de datos.", "num_citations": "4\n", "authors": ["1214"]}
{"title": "Empirical studies in software engineering courses: Some pedagogical experiences\n", "abstract": " Empirical studies in software engineering are essential for the validation of different methods, techniques, tools, etc. Students play a fundamental role in carrying these studies out successfully and, as a consequence, most experiments connected with software engineering are conducted in academia. Benefits which are concerned exclusively with aspects of research are not the only ones to come from studies of this kind: it is very important also to consider benefits from a teaching point of view. Therefore, when experiments are conducted in academia, they must be planned not only to obtain insights into research but also to help students who participate as experimental subjects.", "num_citations": "3\n", "authors": ["1214"]}
{"title": "M\u00e9tricas de calidad para almacenes de datos\n", "abstract": " M\u00e9tricas de calidad para almacenes de datos - Dialnet Ir al contenido Dialnet Buscar Revistas Tesis Congresos Ayuda M\u00e9tricas de calidad para almacenes de datos Autores: MA Serrano, C. Pascual, Coral Calero Mu\u00f1oz, Mario G. Piattini Velthuis Localizaci\u00f3n: JISBD 2001. Jornadas de ingenier\u00eda del software y bases de datos: 21 y 23 de noviembre de 2001. Almagro (Ciudad Real) / Arantza Illarramendi Echave ( ed. lit. ), \u00d3scar D\u00edaz Garc\u00eda ( ed. lit. ), Mario G. Piattini Velthuis ( ed. lit. ), 2001, ISBN 84-699-6275-2, p\u00e1gs. 537-548 Idioma: espa\u00f1ol Texto completo no disponible (Saber m\u00e1s ...) Fundaci\u00f3n Dialnet Acceso de usuarios registrados Imagen de identificaci\u00f3n Identificarse \u00bfOlvid\u00f3 su contrase\u00f1a? \u00bfEs nuevo? Reg\u00edstrese Ventajas de registrarse Dialnet Plus M\u00e1s informaci\u00f3n sobre Dialnet Plus Opciones de compartir Facebook Twitter Opciones de entorno Sugerencia / Errata \u00a9 2001-2021 Fundaci\u00f3n Dialnet \u00b7 \u2026", "num_citations": "3\n", "authors": ["1214"]}
{"title": "Lecciones aprendidas tras varias convocatorias de un programa para el fortalecimiento y mejora de las competencias de empleabilidad de los graduados en inform\u00e1tica\n", "abstract": " Este art\u00edculo describe la implantaci\u00f3n real de un convenio llamado convenio FORTE, que persigue el fortalecimiento de las competencias de empleabilidad de los graduados en Ingenier\u00eda Inform\u00e1tica. El objetivo del convenio es acercar los graduados a las empresas mediante un enfoque orientado a las necesidades espec\u00edficas de las compa\u00f1\u00edas participantes. El art\u00edculo describe los resultados y pasos seguidos durante dos convocatorias (2014 y 2015) y c\u00f3mo se han ido llevando las diferentes tareas, acuerdos, selecci\u00f3n y preparaci\u00f3n tanto de alumnos, profesores y empresas para la realizaci\u00f3n de los convenios FORTE y cu\u00e1l ha sido la aceptaci\u00f3n por parte de todos los implicados", "num_citations": "2\n", "authors": ["1214"]}
{"title": "Proyecto profESIonal\u00edzate: Mejorando la empleabilidad a trav\u00e9s de la mejora de las competencias profesionales\n", "abstract": " Este art\u00edculo describe el proyecto de innovaci\u00f3n docente profESIonal\u00edzate, cuyo objetivo es el de establecer un plan transversal para fortalecer las competencias profesionales de los alumnos del Grado en Ingenier\u00eda en Inform\u00e1tica y del M\u00e1ster en Ingenier\u00eda en Inform\u00e1tica. El elemento central del proyecto es el convenio FORTE que se firmar\u00e1 con importantes empresas del sector.", "num_citations": "2\n", "authors": ["1214"]}
{"title": "Visual Basic .NET\n", "abstract": " Un objeto es una agrupaci\u00f3n de c\u00f3digo, compuesta de propiedades (atributos) y m\u00e9todos, que pueden ser manipulados como una entidad independiente. Las propiedades definen los datos o informaci\u00f3n del objeto, permitiendo consultar o modificar su estado; mientras que los m\u00e9todos son rutinas que definen su comportamiento.", "num_citations": "2\n", "authors": ["1214"]}
{"title": "Introducci\u00f3n a los SARI\n", "abstract": " Introducci\u00f3n a los SARI Page 1 1 Introducci\u00f3n a los SARI Manuel Serrano ES Inform\u00e1tica CR \u2013 UCLM Manuel.Serrano@uclm.es http://alarcos.inf-cr.uclm.es/per/mserrano/ Page 2 2 \u00cdndice 1. Objetivos 2. Conceptos b\u00e1sicos 3. Evaluaci\u00f3n de la recuperaci\u00f3n 4. Perspectiva hist\u00f3rica Page 3 3 Objetivos \u220e Concepto de Almacenamiento y Recuperaci\u00f3n de la Informaci\u00f3n como proceso \u220e M\u00e1quina Memex del Dr. Vannervar Bush \u220e Par\u00e1metros fundamentales en el m\u00e9todo de almacenar la informaci\u00f3n: \u220e Tipo de Informaci\u00f3n: Num\u00e9rica, textos en lenguaje natural, sonidos, fotos,... \u220e M\u00e9todos de recuperaci\u00f3n: Lenguaje formal no ambiguo, lenguaje formal ambiguo, m\u00e9todos exploratorios,... Page 4 4 Definici\u00f3n de recuperaci\u00f3n de informaci\u00f3n \u220e Baeza \u2013 Yates [1999]: Parte de la inform\u00e1tica que estudia la recuperaci\u00f3n de la informaci\u00f3n (no datos) de una colecci\u00f3n de documentos escritos. Los documentos recuperados \u2026", "num_citations": "2\n", "authors": ["1214"]}
{"title": "Metrics for Datawarehouses Conceptual Models\n", "abstract": " Datawarehouses have become the most important trend in business information technology and represent one of the most interesting areas within the database industry (Chaudhuri and Dayal, 1997) as they provide relevant and precise information enabling the improvement of strategic decisions (Jarke et al., 2000) and as such the quality of the information that they contain must be guaranteed (English, 1996). In fact, a lack of quality can", "num_citations": "2\n", "authors": ["1214"]}
{"title": "The advisability of using packages in data warehouse design\n", "abstract": " Data warehouses are large data repositories integrating data from several sources that support decision making. Although, traditionally, data warehouses have been designed using the \u2018well-known\u2019star schema, some design methodologies have come into existence in recent times. These new methodologies have not only focused on logical design: they also propose performing a conceptual modeling using UML. At present, it is widely accepted that modeling using packages simplifies the management and understanding of the designs. Until now, however, this statement has not been empirically proved in the data warehouse field. In this paper, we present an empirical study whose aim is to check whether using packages in designing data warehouses makes them more understandable.", "num_citations": "2\n", "authors": ["1214"]}
{"title": "BlockBD: a security pattern to incorporate blockchain in big data ecosystems\n", "abstract": " Big Data is changing the perspective on how to obtain valuable information from data stored by organizations of all kinds. By using these insights, companies can make better decisions and thus achieve their business goals. However, each new technology can create new security problems, and Big Data is no exception. One of the major security issues in a Big Data ecosystem is what level of trust in data and data sources stakeholders can have: without reliable data, the results of data analysis lose value. In this paper, we propose a security pattern to improve traceability and veracity of data through the use of Blockchain technologies. In this pattern, Blockchain will be used as a distributed ledger where all operations performed on the data will be registered. Therefore, the veracity of the data will increase, as will the confidence in the insights obtained from the analysis. The purpose of this paper is to help Chief\u00a0\u2026", "num_citations": "1\n", "authors": ["1214"]}
{"title": "Marisma-BiDa: Entorno Integrado de An\u00e1lisis y Gesti\u00f3n de Riesgos en Big Data\n", "abstract": " Marisma-BiDa: Entorno Integrado de An\u00e1lisis y Gesti\u00f3n de Riesgos en Big Data - Dialnet Ir al contenido Dialnet Buscar Revistas Tesis Congresos Ayuda Marisma-BiDa: Entorno Integrado de An\u00e1lisis y Gesti\u00f3n de Riesgos en Big Data Autores: Julio Moreno, Luis E. S\u00e1nchez, Antonio Santos Olmo, David G. Rosad, Manuel A. Serrano, Eduardo Fern\u00e1ndez-Medina Pat\u00f3n Localizaci\u00f3n: Actas de las Cuartas Jornadas Nacionales de Investigaci\u00f3n en Ciberseguridad / Urko Zurutuza Ortega ( aut. ), Mikel Iturbe Urretxa ( aut. ), Enaitz Ezpeleta ( aut. ), I\u00f1aki Garitano Garitano ( aut. ), 2018, ISBN 9788409026975, p\u00e1gs. 159-165 Idioma: espa\u00f1ol Enlaces Texto Completo Libro (pdf) Fundaci\u00f3n Dialnet Acceso de usuarios registrados Imagen de identificaci\u00f3n Identificarse \u00bfOlvid\u00f3 su contrase\u00f1a? \u00bfEs nuevo? Reg\u00edstrese Ventajas de registrarse Dialnet Plus M\u00e1s informaci\u00f3n sobre Dialnet Plus Opciones de compartir Facebook \u2026", "num_citations": "1\n", "authors": ["1214"]}
{"title": "Propuesta de m\u00e9tricas para almacenes de datos a nivel conceptual.\n", "abstract": " En la actualidad la mayor parte de las organizaciones han incorporado los almacenes de datos (datawarehouses) para gestionar la informaci\u00f3n, como uno de sus activos principales, de una forma eficiente. Es fundamental poder asegurar la calidad de la informaci\u00f3n que se encuentra dentro de los almacenes de datos, debido a que se han convertido en la principal herramienta para la toma de decisiones estrat\u00e9gicas. La calidad de la informaci\u00f3n depende de la calidad de su representaci\u00f3n y de la calidad del almac\u00e9n de datos. Este \u00faltimo aspecto, incluye la calidad del modelo multidimensional, tanto a nivel conceptual, como l\u00f3gico y f\u00edsico. En los \u00faltimos a\u00f1os hemos propuesto y validado diversas m\u00e9tricas para evaluar la complejidad del modelo multidimensional en estrella (a nivel l\u00f3gico). En este art\u00edculo presentamos una primera propuesta de m\u00e9tricas para el modelo multidimensional a nivel conceptual y su validaci\u00f3n formal.", "num_citations": "1\n", "authors": ["1214"]}