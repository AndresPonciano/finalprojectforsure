{"title": "Software architecture for control of highly parallel computer systems\n", "abstract": " A computer software architecture for controlling a highly parallel computer system comprises several layers of abstraction. The first layer is an abstract physical machine which contains a set of abstract physical processors. This layer may be considered as a microkernel. The next layer includes virtual machines and virtual processors. A virtual machine comprises a virtual address space and a set of virtual processors that are connected in a virtual topology. Virtual machines are mapped onto abstract physical machines with each virtual processor mapped onto an abstract physical processor. The third layer of abstraction defines threads. Threads are lightweight processes that run on virtual processors. In a preferred embodiment the abstract physical machines, abstract physical processors, virtual machines, virtual processors, thread groups, and threads are all first class objects.", "num_citations": "314\n", "authors": ["294"]}
{"title": "Improving duplicate elimination in storage systems\n", "abstract": " Minimizing the amount of data that must be stored and managed is a key goal for any storage architecture that purports to be scalable. One way to achieve this goal is to avoid maintaining duplicate copies of the same data. Eliminating redundant data at the source by not writing data which has already been stored not only reduces storage overheads, but can also improve bandwidth utilization. For these reasons, in the face of today's exponentially growing data volumes, redundant data elimination techniques have assumed critical significance in the design of modern storage systems.Intelligent object partitioning techniques identify data that is new when objects are updated, and transfer only these chunks to a storage server. In this article, we propose a new object partitioning technique, called fingerdiff, that improves upon existing schemes in several important respects. Most notably, fingerdiff dynamically chooses a\u00a0\u2026", "num_citations": "238\n", "authors": ["294"]}
{"title": "Safe futures for Java\n", "abstract": " A future is a simple and elegant abstraction that allows concurrency to be expressed often through a relatively small rewrite of a sequential program. In the absence of side-effects, futures serve as benign annotations that mark potentially concurrent regions of code. Unfortunately, when computation relies heavily on mutation as is the case in Java, its meaning is less clear, and much of its intended simplicity lost. This paper explores the definition and implementation of safe futures for Java. One can think of safe futures as truly transparent annotations on method calls, which designate opportunities for concurrency. Serial programs can be made concurrent simply by replacing standard method calls with future invocations. Most significantly, even though some parts of the program are executed concurrently and may indeed operate on shared data, the semblance of serial execution is nonetheless preserved. Thus\u00a0\u2026", "num_citations": "217\n", "authors": ["294"]}
{"title": "Distributed agent software system and method having enhanced process mobility and communication in a computer network\n", "abstract": " A distributed software system and method are provided for use with a plurality of potentially heterogeneous computer machines connected as a network. The system may comprise at least one agent comprising a protection domain, wherein the protection domain of the at least one agent resides on at least two of the plurality of computer machines. A plurality of objects is contained within the protection domain of the at least one agent, a first object residing on a first of the at least two computer machines and a second object residing on a second of the at least two computer machines. The objects are selectively movable among the at least two computer machines by a programmer of the system. The first object on the first computer machine may access the second object on the second computer machine in a location-transparent or network-transparent manner; that is, without knowledge of the physical address of the\u00a0\u2026", "num_citations": "170\n", "authors": ["294"]}
{"title": "Transmission of higher-order objects across a network of heterogeneous machines\n", "abstract": " The system comprises a collection of address spaces within which potentially many concurrent lightweight perceptible threads may execute. The address space is uniformly distributed among different nodes in a network of heterogeneous machines. Address spaces are first-class and may be generated dynamically. Threads within an address space may communicate with one another via shared memory; communication between address spaces takes place using explicit message-passing.", "num_citations": "169\n", "authors": ["294"]}
{"title": "A unified treatment of flow analysis in higher-order languages\n", "abstract": " We describe a framework for flow analysis in higher-order languages. It is both a synthesis and extension of earlier work in this area, most notably [20, 22]", "num_citations": "153\n", "authors": ["294"]}
{"title": "Static specification inference using predicate mining\n", "abstract": " The reliability and correctness of complex software systems can be significantly enhanced through well-defined specifications that dictate the use of various units of abstraction (e.g., modules, or procedures). Often times, however, specifications are either missing, imprecise, or simply too complex to encode within a signature, necessitating specification inference. The process of inferring specifications from complex software systems forms the focus of this paper. We describe a static inference mechanism for identifying the preconditions that must hold whenever a procedure is called. These preconditions may reflect both data flow properties (e.g., whenever p is called, variable x must be non-null) as well as control-flow properties (e.g., every call to p must bepreceded by a call to q). We derive these preconditions using a ninter-procedural path-sensitive dataflow analysis that gathers predicates at each program point\u00a0\u2026", "num_citations": "137\n", "authors": ["294"]}
{"title": "Path-sensitive inference of function precedence protocols\n", "abstract": " Function precedence protocols define ordering relations among function calls in a program. In some instances, precedence protocols are well-understood (e.g., a call to pthread_mutex_init must always be present on all program paths before a call to pthread_mutex_lock). Oftentimes, however, these protocols are neither well- documented, nor easily derived. As a result, protocol violations can lead to subtle errors that are difficult to identify and correct. In this paper, we present CHRONICLER, a tool that applies scalable inter-procedural path-sensitive static analysis to automatically infer accurate function precedence protocols. Chronicler computes precedence relations based on a program's control-flow structure, integrates these relations into a repository, and analyzes them using sequence mining techniques to generate a collection of feasible precedence protocols. Deviations from these protocols found in the\u00a0\u2026", "num_citations": "136\n", "authors": ["294"]}
{"title": "Transactional monitors for concurrent objects\n", "abstract": " Transactional monitors are proposed as an alternative to monitors based on mutual-exclusion synchronization for object-oriented programming languages. Transactional monitors have execution semantics similar to mutual-exclusion monitors but implement monitors as lightweight transactions that can be executed concurrently (or in parallel on multiprocessors). They alleviate many of the constraints that inhibit construction of transparently scalable and robust applications.               We undertake a detailed study of alternative implementation schemes for transactional monitors. These different schemes are tailored to different concurrent access patterns, and permit a given application to use potentially different implementations in different contexts.               We also examine the performance and scalability of these alternative approaches in the context of the Jikes Research Virtual Machine, a state\u00a0\u2026", "num_citations": "122\n", "authors": ["294"]}
{"title": "Search with probabilistic guarantees in unstructured peer-to-peer networks\n", "abstract": " Search is a fundamental service in peer-to-peer (P2P) networks. However, despite numerous research efforts, efficient algorithms for guaranteed location of shared content in unstructured P2P networks are yet to be devised. In this paper, the authors presented a simple but highly effective protocol for object location that gives probabilistic guarantees of finding even rare objects independently of the network topology. The protocol relies on randomized techniques for replication of objects (or their references) and for query propagation. The authors proved analytically, and demonstrated experimentally, that this scheme provides high probabilistic guarantees of success, while incurring minimal overhead. The performance of this scheme was quantified in terms of network messages, probability of success, and response time. The robustness of this protocol was also evaluated in the presence of node failures (departures\u00a0\u2026", "num_citations": "112\n", "authors": ["294"]}
{"title": "Higher-order distributed objects\n", "abstract": " We describe a distributed implementation of Scheme that permits efficient transmission of higher-order objects such as closures and continuations. The integration of distributed communication facilities within a higher-order programming language engenders a number of new abstractions and paradigms for distributed computing. Among these are user-specified load-balancing and migration policies for threads, incrementally linked distributed computations, and parameterized client-server applications. To our knowledge, this is the first distributed dialect of Scheme (or a related language) that addresses lightweight communication abstractions for higher-order objects.", "num_citations": "105\n", "authors": ["294"]}
{"title": "Declarative programming over eventually consistent data stores\n", "abstract": " User-facing online services utilize geo-distributed data stores to minimize latency and tolerate partial failures, with the intention of providing a fast, always-on experience. However, geo-distribution does not come for free; application developers have to contend with weak consistency behaviors, and the lack of abstractions to composably construct high-level replicated data types, necessitating the need for complex application logic and invariably exposing inconsistencies to the user. Some commercial distributed data stores and several academic proposals provide a lattice of consistency levels, with stronger consistency guarantees incurring increased latency and throughput costs. However, correctly assigning the right consistency level for an operation requires subtle reasoning and is often an error-prone task. In this paper, we present QUELEA, a declarative programming model for eventually consistent data stores\u00a0\u2026", "num_citations": "97\n", "authors": ["294"]}
{"title": "Preemptible atomic regions for real-time Java\n", "abstract": " We present a new concurrency control abstraction for real-time systems called preemptible atomic regions (PARs). PARs a transactional mechanism that improves upon lock-based mutual exclusion in several ways. First, and foremost, PARs provide strong correctness guarantees. Any sequence of operations declared atomic will not suffer interference from other threads, even in the presence of programmer errors. In spite of this, PARs can be preempted by high priority tasks; this is essential to the minimization of blocking times. We have implemented PARs in a uniprocessor real-time Java virtual machine and evaluated their utility on a number of programs. The results suggest that programs that use PARs, depending on their semantics, can run faster and experience less jitter than those that use locks", "num_citations": "96\n", "authors": ["294"]}
{"title": "Flow-directed inlining\n", "abstract": " A flow-directed inlining strategy uses information derived from control-flow analysis to specialize and inline procedures for functional and object-oriented languages. Since it uses control-flow analysis to identify candidate call sites, flow-directed inlining can inline procedures whose relationships to their call sites are not apparent. For instance, procedures defined in other modules, passed as arguments, returned as values, or extracted from data structures can all be inlined. Flow-directed inlining specializes procedures for particular call sites, and can selectively inline a particular procedure at some call sites but not at others. Finally, flow-directed inlining encourages modular implementations: control-flow analysis, inlining, and post-inlining optimizations are all orthogonal components. Results from a prototype implementation indicate that this strategy effectively reduces procedure call overhead and leads to significant\u00a0\u2026", "num_citations": "86\n", "authors": ["294"]}
{"title": "Polymorphic splitting: An effective polyvariant flow analysis\n", "abstract": " This article describes a general-purpose program analysis that  computes global control-flow and data-flow information for  higher-order, call-by-value languages.  The analysis employs a  novel form of polyvariance called polymorhic splitting that uses  let-expressions as syntactic clues to gain precision. The information  derived from the analysis is used both to eliminate run-time checks and to inline procedure. The analysis and optimizations have been  applied to a suite of Scheme programs. Experimental results obtained  from the prototype implementation indicate that the analysis is  extremely precise and has reasonable cost. Compared to monovariant flow analyses such as 0CFA, or analyses based on type inference such as  soft typing, the analysis eliminates significantly more    run-time checks. Run-time check elimination and inlining together typically yield a 20 to 40% performance improvement for the\u00a0\u2026", "num_citations": "84\n", "authors": ["294"]}
{"title": "Effective flow analysis for avoiding run-time checks\n", "abstract": " This paper describes a general purpose program analysis that computes global control-flow and data-flow information for higher-order, call-by-value programs. This information can be used to drive global program optimizations such as inlining and run-time check elimination, as well as optimizations like constant folding and loop invariant code motion that are typically based on special-purpose local analyses.             The analysis employs a novel approximation technique called polymorphic splitting that uses let-expressions as syntactic clues to gain precision. Polymorphic splitting borrows ideas from Hindley-Milner polymorphic type inference systems to create an analog to polymorphism for flow analysis.             Experimental results derived from an implementation of the analysis for Scheme indicate that the analysis is extremely precise and has reasonable cost. In particular, it eliminates significantly more\u00a0\u2026", "num_citations": "79\n", "authors": ["294"]}
{"title": "Single and loving it: Must-alias analysis for higher-order languages\n", "abstract": " In standard control-flow analyses for higher-order languages, a single abstract binding for a variable represents a set of exact bindings, and a single abstract reference cell represents a set of exact reference cells. While such analyses provide useful may-alias information, they are unable to answer mustalias questions about variables and cells, as these questions ask about equality of specific bindings and references. In this paper, we present a novel program analysis for higher-order languages that answers must-alias questions. At every program point, the analysis associates with each variable and abstract cell a cardinality, which is either single or multiple. If variable x is single at program point p, then all bindings for x in the heap reachable from the environment at p hold the same value. If abstract cell r is single at p, then at most one exact cell corresponding to r is reachable from the environment at p. Must-alias\u00a0\u2026", "num_citations": "75\n", "authors": ["294"]}
{"title": "Distributed uniform sampling in unstructured peer-to-peer networks\n", "abstract": " Uniform sampling in networks is at the core of a wide variety of randomized algorithms. Random sampling can be performed by modeling the system as an undirected graph with associated transition probabilities and defining a corresponding Markov chain (MC). A random walk of prescribed minimum length, performed on this graph, yields a stationary distribution, and the corresponding random sample. This sample, however, is not uniform when network nodes have a non-uniform degree distribution. This poses a significant practical challenge since typical large scale real-world unstructured networks tend to have non-uniform degree distributions, e.g., power-law degree distribution in unstructured peer-to-peer networks. In this paper, we present a distributed algorithm that enables efficient uniform sampling in large unstructured non-uniform networks. Specifically, we prescribe necessary conditions for uniform\u00a0\u2026", "num_citations": "70\n", "authors": ["294"]}
{"title": "Flow-directed closure conversion for typed languages\n", "abstract": " This paper presents a new closure conversion algorithm for simply-typed languages. We have have implemented the algorithm as part of MLton, a whole-program compiler for Standard ML (SML). MLton first applies all functors and eliminates polymorphism by code duplication to produce a simply-typed program. MLton then performs closure conversion to produce a first-order, simply-typed program. In contrast to typical functional language implementations, MLton performs most optimizations on the first-order language, after closure conversion. There are two notable contributions of our work:                                         1.                                             The translation uses a general flow-analysis framework which includes OCFA. The types in the target language fully capture the results of the analysis. MLton uses the analysis to insert coercions to translate between different representations of a closure to preserve type\u00a0\u2026", "num_citations": "70\n", "authors": ["294"]}
{"title": "Semantics-aware trace analysis\n", "abstract": " As computer systems continue to become more powerful and complex, so do programs. High-level abstractions introduced to deal with complexity in large programs, while simplifying human reasoning, can often obfuscate salient program properties gleaned from automated source-level analysis through subtle (often non-local) interactions. Consequently, understanding the effects of program changes and whether these changes violate intended protocols become difficult to infer. Refactorings, and feature additions, modifications, or removals can introduce hard-to-catch bugs that often go undetected until many revisions later. To address these issues, this paper presents a novel dynamic program analysis that builds a semantic view of program executions. These views reflect program abstractions and aspects; however, views are not simply projections of execution traces, but are linked to each other to capture\u00a0\u2026", "num_citations": "66\n", "authors": ["294"]}
{"title": "Efficient tag detection in RFID systems\n", "abstract": " Recent technological advances have motivated large-scale deployment of RFID systems. However, a number of critical design issues relating to efficient detection of tags remain unresolved. In this paper, we address three important problems associated with tag detection in RFID systems: (i) accurately detecting RFID tags in the presence of reader interference (reader collision avoidance problem); (ii) eliminating redundant tag reports by multiple readers (optimal tag reporting problem); and (iii) minimizing redundant reports from multiple readers by identifying a minimal set of readers that cover all tags present in the system (optimal tag coverage problem). The underlying difficulties associated with these problems arise from the lack of collision detection mechanisms, the potential inability of RFID readers to relay packets generated by other readers, and severe resource constraints on RFID tags. In this paper we\u00a0\u2026", "num_citations": "65\n", "authors": ["294"]}
{"title": "A transactional object calculus\n", "abstract": " A transaction defines a locus of computation that satisfies important concurrency and failure properties. These so-called ACID properties provide strong serialization guarantees that allow us to reason about concurrent and distributed programs in terms of higher-level units of computation (e.g., transactions) rather than lower-level data structures (e.g., mutual-exclusion locks). This paper presents a framework for specifying the semantics of a transactional facility integrated within a host programming language. The TFJ calculus, an object calculus derived from Featherweight Java, supports nested and multi-threaded transactions. We give a semantics to TFJ that is parametrized by the definition of the transactional mechanism that permits the study of different transaction models. We give two instantiations: one that defines transactions in terms of a versioning-based optimistic concurrency model, and the other which\u00a0\u2026", "num_citations": "65\n", "authors": ["294"]}
{"title": "Asynchronous algorithms in MapReduce\n", "abstract": " Asynchronous algorithms have been demonstrated to improve scalability of a variety of applications in parallel environments. Their distributed adaptations have received relatively less attention, particularly in the context of conventional execution environments and associated overheads. One such framework, MapReduce, has emerged as a commonly used programming framework for large-scale distributed environments. While the MapReduce programming model has proved to be effective for data-parallel applications, significant questions relating to its performance and application scope remain unresolved. The strict synchronization between map and reduce phases limits expression of asynchrony and hence, does not readily support asynchronous algorithms. This paper investigates the notion of partial synchronizations in iterative MapReduce applications to overcome global synchronization overheads. The\u00a0\u2026", "num_citations": "64\n", "authors": ["294"]}
{"title": "Macroprogramming heterogeneous sensor networks using cosmos\n", "abstract": " In this paper, we present COSMOS, a novel architecture for macroprogramming heterogeneous sensor network systems. Macroprogramming specifies aggregate system behavior, as opposed to device-specific programs that code distributed behavior using explicit messaging. COSMOS is comprised of a macroprogramming language, mPL, and an operating system, mOS. mPL macroprograms are statically verifiable compositions of reusable user-specified, or system supported functional components. The mOS node/network operating system provides component management and a lean execution environment for mPL programs in heterogeneous resource-constrained sensor networks. It provides runtime application instantiation, with over-the-air reprogramming of the network. COSMOS facilitates composition of complex real-world applications that are robust, scalable and adaptive in dynamic data-driven sensor\u00a0\u2026", "num_citations": "63\n", "authors": ["294"]}
{"title": "Unstructured peer-to-peer networks for sharing processor cycles\n", "abstract": " Motivated by the needs and success of projects such as SETI@home and genome@home, we propose an architecture for a sustainable large-scale peer-to-peer environment for distributed cycle sharing among Internet hosts. Such networks are characterized by highly dynamic state due to high arrival and departure rates. This makes it difficult to build and maintain structured networks and to use state-based resource allocation techniques. We build our system to work in an environment similar to current file-sharing networks such as Gnutella and Freenet. In doing so, we are able to leverage vast network resources while providing resilience to random failures, low network overhead, and an open architecture for resource brokering. This paper describes the underlying analytical and algorithmic substrates based on randomization for job distribution, replication, monitoring, aggregation and oblivious resource sharing\u00a0\u2026", "num_citations": "56\n", "authors": ["294"]}
{"title": "Transparently reconciling transactions with locking for java synchronization\n", "abstract": " Concurrent data accesses in high-level languages like Java and C# are typically mediated using mutual-exclusion locks. Threads use locks to guard the operations performed while the lock is held, so that the lock\u2019s guarded operations can never be interleaved with operations of other threads that are guarded by the same lock. This way both atomicity and isolation properties of a thread\u2019s guarded operations are enforced. Recent proposals recognize that these properties can also be enforced by concurrency control protocols that avoid well-known problems associated with locking, by transplanting notions of transactions found in database systems to a programming language context. While higher-level than locks, software transactions incur significant implementation overhead. This overhead cannot be easily masked when there is little contention on the operations being guarded.               We show how\u00a0\u2026", "num_citations": "54\n", "authors": ["294"]}
{"title": "An inductive synthesis framework for verifiable reinforcement learning\n", "abstract": " Despite the tremendous advances that have been made in the last decade on developing useful machine-learning applications, their wider adoption has been hindered by the lack of strong assurance guarantees that can be made about their behavior. In this paper, we consider how formal verification techniques developed for traditional software systems can be repurposed for verification of reinforcement learning-enabled ones, a particularly important class of machine learning systems. Rather than enforcing safety by examining and altering the structure of a complex neural network implementation, our technique uses blackbox methods to synthesizes deterministic programs, simpler, more interpretable, approximations of the network that can nonetheless guarantee desired safety properties are preserved, even when the network is deployed in unanticipated or previously unobserved environments. Our\u00a0\u2026", "num_citations": "53\n", "authors": ["294"]}
{"title": "A foundation for an efficient multi-threaded Scheme system\n", "abstract": " We have built a parallel dialect of Scheme called STING that differs from its contemporaries in a number of important respects. STING is intended to be used as an operating system substrate for modern parallel programming languages.", "num_citations": "52\n", "authors": ["294"]}
{"title": "A data-driven CHC solver\n", "abstract": " We present a data-driven technique to solve Constrained Horn Clauses (CHCs) that encode verification conditions of programs containing unconstrained loops and recursions. Our CHC solver neither constrains the search space from which a predicate's components are inferred (e.g., by constraining the number of variables or the values of coefficients used to specify an invariant), nor fixes the shape of the predicate itself (e.g., by bounding the number and kind of logical connectives). Instead, our approach is based on a novel machine learning-inspired tool chain that synthesizes CHC solutions in terms of arbitrary Boolean combinations of unrestricted atomic predicates. A CEGAR-based verification loop inside the solver progressively samples representative positive and negative data from recursive CHCs, which is fed to the machine learning tool chain. Our solver is implemented as an LLVM pass in the SeaHorn\u00a0\u2026", "num_citations": "47\n", "authors": ["294"]}
{"title": "A uniform transactional execution environment for Java\n", "abstract": " Transactional memory (TM) has recently emerged as an effective tool for extracting fine-grain parallelism from declarative critical sections. In order to make STM systems practical, significant effort has been made to integrate transactions into existing programming languages. Unfortunately, existing approaches fail to provide a simple implementation that permits lock-based and transaction-based abstractions to coexist seamlessly. Because of the fundamental semantic differences between locks and transactions, legacy applications or libraries written using locks can not be transparently used within atomic regions. To address these shortcomings, we implement a uniform transactional execution environment for Java programs in which transactions can be integrated with more traditional concurrency control constructs. Programmers can run arbitrary programs that utilize traditional mutual-exclusion-based\u00a0\u2026", "num_citations": "46\n", "authors": ["294"]}
{"title": "Sieve: A tool for automatically detecting variations across program versions\n", "abstract": " Software systems often undergo many revisions during their lifetime as new features are added, bugs repaired, abstractions simplified and refactored, and performance improved. When a revision, even a minor one, does occur, the changes it induces must be tested to ensure that invariants assumed in the original version are not violated unintentionally. In order to avoid testing components that are unchanged across revisions, impact analysis is often used to identify code blocks or functions that are affected by a change. In this paper, we present a novel solution to this general problem that uses dynamic programming on instrumented traces of different program binaries to identify longest common subsequences in strings generated by these traces. Our formulation allows us to perform impact analysis and also to detect the smallest set of locations within the functions where the effect of the changes actually manifests\u00a0\u2026", "num_citations": "46\n", "authors": ["294"]}
{"title": "Programming linguistics\n", "abstract": " \u201cProgramming linguistics studies the design of programming languages and the relationships among them. Its particular goal is to discover the patterns, the relationships and the common antecedents that can make a complicated field rational and graspable\u201d(p. 1). This good book contains a lot of useful information. The preface says it is intended as \u201ca text for courses in the design and evolution of programming languages.\u201d It fulfills these objectives well. This book is organized primarily by language, but includes some significant sections on concepts. The authors provide 74 exercises, but no answers. The bibliography has 137 entries but is not annotated. The book is suitable as a text, reference, or survey. This book covers (at least to some degree) Ada, ALGOL60, ALGOL 68, APL, CLU, COBOL, FORTRAN, LISP, Modula, Pascal, PL/I, Prolog, SIMULA-67, SMALLTALK, CSP, Linda, Miranda, Occam, and Scheme. C is\u00a0\u2026", "num_citations": "46\n", "authors": ["294"]}
{"title": "A semantic framework for designer transactions\n", "abstract": " A transaction defines a locus of computation that satisfies important concurrency and failure properties; these so-called ACID properties provide strong serialization guarantees that allow us to reason about concurrent and distributed programs in terms of higher-level units of computation (e.g., transactions) rather than lower-level data structures (e.g., mutual-exclusion locks). This paper presents a framework for specifying the semantics of a transactional facility integrated within a host programming language. The TFJ calculus supports nested and multi-threaded transactions. We give a semantics to TFJ that is parameterized by the definition of the transactional mechanism that permits the study of different transaction models.", "num_citations": "44\n", "authors": ["294"]}
{"title": "A customizable substrate for concurrent languages\n", "abstract": " We describe an approach to implementing a wide-range of concurrency paradigms in high-level (symbolic) programming languages. The focus of our discussion is STING, a dialect of Scheme, that supports lightweight threads of control and virtual processors as first-class objects. Given the significant degree to which the behavior of these objects may be customized, we can easily express a variety of concurrency paradigms and linguistic structures within a common framework without loss of efficiency. Unlike parallel systems that rely on operating system services for managing concurrency, STING implements concurrency management entirely in terms of Scheme objects and procedures. It, therefore, permits users to optimize the runtime behavior of their applications without requiring knowledge of the underlying runtime system. This paper concentrates on (a) the implications of the design for building asynchronous\u00a0\u2026", "num_citations": "43\n", "authors": ["294"]}
{"title": "Modular reasoning for deterministic parallelism\n", "abstract": " Weaving a concurrency control protocol into a program is difficult and error-prone. One way to alleviate this burden is deterministic parallelism. In this well-studied approach to parallelisation, a sequential program is annotated with sections that can execute concurrently, with automatically injected control constructs used to ensure observable behaviour consistent with the original program. This paper examines the formal specification and verification of these constructs. Our high-level specification defines the conditions necessary for correct execution; these conditions reflect program dependencies necessary to ensure deterministic behaviour. We connect the high-level specification used by clients of the library with the low-level library implementation, to prove that a client's requirements for determinism are enforced. Significantly, we can reason about program and library correctness without breaking abstraction\u00a0\u2026", "num_citations": "41\n", "authors": ["294"]}
{"title": "Synthesizing racy tests\n", "abstract": " Subtle concurrency errors in multithreaded libraries that arise because of incorrect or inadequate synchronization are often difficult to pinpoint precisely using only static techniques. On the other hand, the effectiveness of dynamic race detectors is critically dependent on multithreaded test suites whose execution can be used to identify and trigger races. Usually, such multithreaded tests need to invoke a specific combination of methods with objects involved in the invocations being shared appropriately to expose a race. Without a priori knowledge of the race, construction of such tests can be challenging. In this paper, we present a lightweight and scalable technique for synthesizing precisely these kinds of tests. Given a multithreaded library and a sequential test suite, we describe a fully automated analysis that examines sequential execution traces, and produces as its output a concurrent client program that drives\u00a0\u2026", "num_citations": "40\n", "authors": ["294"]}
{"title": "Environments as first class objects\n", "abstract": " We describe a programming language called Symmetric Lisp that treats environments as first-class objects. Symmetric Lisp allows programmers to write expressions that evaluate to environments, and to create and denote variables and constants of type environment as well. One consequence is that the roles filled in other languages by a variety of limited, special purpose environment forms like records, structures, closures, modules, classes and abstract data types are filled instead by a single versatile and powerful structure. In addition to being its fundamental structuring tool, environments also serve as the basic functional object in the language. Because the elements of an environment are evaluated in parallel, Symmetric Lisp is a parallel programming language; because they may be assembled dynamically as well as statically, Symmetric Lisp accommodates an unusually flexible and simple (parallel\u00a0\u2026", "num_citations": "39\n", "authors": ["294"]}
{"title": "Analyzing stores and references in a parallel symbolic language\n", "abstract": " We describe an analysis of a parallel language in which processes communicate via first-class mutable shared locations. The sequential core of the language defines a higher-order strict functional language with list data structures. The parallel extensions permit processes and shared locations to be dynamically created; synchronization among processes occurs exclusively via shared locations.The analysis is defined by an abstract interpretation on this language. The interpretation is efficient and useful, facilitating a number of important optimizations related to synchronization, processor/thread mapping, and storage management.", "num_citations": "37\n", "authors": ["294"]}
{"title": "MultiMLton: A multicore-aware runtime for standard ML\n", "abstract": " MultiMLton is an extension of the MLton compiler and runtime system that targets scalable, multicore architectures. It provides specific support for ACML, a derivative of Concurrent ML that allows for the construction of composable asynchronous events. To effectively manage asynchrony, we require the runtime to efficiently handle potentially large numbers of lightweight, short-lived threads, many of which are created specifically to deal with the implicit concurrency introduced by asynchronous events. Scalability demands also dictate that the runtime minimize global coordination. MultiMLton therefore implements a split-heap memory manager that allows mutators and collectors running on different cores to operate mostly independently. More significantly, MultiMLton exploits the premise that there is a surfeit of available concurrency in ACML programs to realize a new collector design that completely eliminates the\u00a0\u2026", "num_citations": "36\n", "authors": ["294"]}
{"title": "One stack to run them all\n", "abstract": " We present a reduction from a concurrent real-time program with priority preemptive scheduling to a sequential program that has the same set of behaviors. Whereas many static analyses of concurrent programs are undecidable, our reduction enables the application of any sequential program analysis to be applied to a concurrent real-time program with priority preemptive scheduling.", "num_citations": "36\n", "authors": ["294"]}
{"title": "Locality in structured peer-to-peer networks\n", "abstract": " Distributed hash tables (DHTs), used in a number of structured peer-to-peer (P2P) systems, provide efficient mechanisms for resource placement and location. A key distinguishing feature of current DHT systems, such as Chord, Pastry, CAN and Tapestry, is the way they handle locality in the underlying network. Topology-based node identifier assignment, proximity routing, and proximity neighbor selection are examples of heuristics used to minimize message delays in the underlying network. While these heuristics are sometimes effective, they all rely on a single global overlay that may install the key of a popular object at a node far from most of the nodes accessing it. Furthermore, a response to a lookup message does not contain any locality information about the nodes holding a copy of the object. We address these issues in Plethora, a novel two-level overlay P2P network. A local overlay in Plethora acts as a\u00a0\u2026", "num_citations": "35\n", "authors": ["294"]}
{"title": "Type-directed flow analysis for typed intermediate languages\n", "abstract": " Flow analysis is especially valuable for optimizing functional languages because control-flow information is not syntactically apparent in higher-order programs. Flow analyses typically operate on untyped languages. However, recent compilers for typed functional languages such as ML and Haskell use a typed intermediate language to expose data representations for optimization. This paper presents a polyvariant flow analysis framework for the predicative subset of system F, a common basis for typed intermediate languages. Analyses in this framework can take advantage of types to analyze programs more precisely. We study a specific analysis called S RT that uses types to control polyvariance. We prove that S RT respects types: whenever it assigns abstract value   to a variable and the type system assigns type \u03c3 to the same variable, then   , where [ \u00b7 ] denotes a set of values. S RT does not\u00a0\u2026", "num_citations": "35\n", "authors": ["294"]}
{"title": "PHALANX: a graph-theoretic framework for test case prioritization\n", "abstract": " Test case prioritization for regression testing can be performed using different metrics (eg, statement coverage, path coverage) depending on the application context. Employing different metrics requires different prioritization schemes (eg, maximum coverage, dissimilar paths covered). This results in significant algorithmic and implementation complexity in the testing process associated with various metrics and prioritization schemes. In this paper, we present a novel approach to the test case prioritization problem that addresses this limitation. We devise a framework, Phalanx, that identifies two distinct aspects of the problem. The first relates to metrics that define ordering relations among test cases; the second defines mechanisms that implement these metrics on test suites. We abstract the information into a test-case dissimilarity graph--a weighted graph in which nodes specify test cases and weighted edges specify\u00a0\u2026", "num_citations": "33\n", "authors": ["294"]}
{"title": "Compositional and lightweight dependent type inference for ML\n", "abstract": " We consider the problem of inferring expressive safety properties of higher-order functional programs using first-order decision procedures. Our approach encodes higher-order features into first-order logic formula whose solution can be derived using a lightweight counterexample guided refinement loop. To do so, we extract initial verification conditions from dependent typing rules derived by a syntactic scan of the program. Subsequent type-checking and type-refinement phases infer and propagate specifications of higher order functions, which are treated as uninterpreted first-order constructs, via subtyping chains. Our technique provides several benefits not found in existing systems: (1) it enables compositional verification and inference of useful safety properties for functional programs; (2) additionally provides counterexamples that serve as witnesses of unsound assertions: (3) does not entail a\u00a0\u2026", "num_citations": "31\n", "authors": ["294"]}
{"title": "Transparent communication for distributed objects in Java\n", "abstract": " We describe a native-code implementation of Java that supports distributed objects. In order to foster the correctness of distributed programs, remote access is syntactically and semantically indistinguishable from local access. This transparency is provided by the runtime system through the implicit generation of remote references to an object when it is passed as an argument or returned from a remote method call. Consistency is achieved through the use of a distributed (and thus scalable) global addressing scheme. Experiments show that application performance is a function of data layout, access algorithm, and local workload. For distributed applications, such as distributed databases, these factors may not be known statically, suggesting the importance of runtime support.", "num_citations": "31\n", "authors": ["294"]}
{"title": "Stabilizers: a modular checkpointing abstraction for concurrent functional programs\n", "abstract": " Transient faults that arise in large-scale software systems can often be repaired by re-executing the code in which they occur. Ascribing a meaningful semantics for safe re-execution in multi-threaded code is not obvious, however. For a thread to correctly rexecute a region of code, it must ensure that all other threads which have witnessed its unwanted effects within that region are also reverted to a meaningful earlier state. If not done properly, data inconsistencies and other undesirable behavior may result. however, automatically determining what constitutes a consistent global checkpoint is not straightforward since thread interactions are a dynamic property of the program.In this paper, we present a safe and efficient checkpointing mechanism for Concurrent ML (CML) that can be used to recover from transient faults. We introduce a new linguistic abstraction called stabilizers that permits the specification of per\u00a0\u2026", "num_citations": "30\n", "authors": ["294"]}
{"title": "Metalevel building blocks for modular systems\n", "abstract": " The formal definition of any namespace device found in a programming language can be given in terms of transformations on a semantic environment. It is worthwhile, therefore, to consider the implications of incorporating environments as bona fide data objects in a programming system. In this article, we propose a treatment of environments and the mechanism by which they are reified and manipulated, that addresses these concerns. The language described below (Rascal) permits environments to be reified into data structures, and data structures to be reflected into environments, but gives users great flexibility to constrain the extent and scope of these processes. We argue that the techniques and operators developed define a cohesive basis for building large-scale modular systems using reflective programming techniques.", "num_citations": "30\n", "authors": ["294"]}
{"title": "Safe replication through bounded concurrency verification\n", "abstract": " High-level data types are often associated with semantic invariants that must be preserved by any correct implementation. While having implementations enforce strong guarantees such as linearizability or serializability can often be used to prevent invariant violations in concurrent settings, such mechanisms are impractical in geo-distributed replicated environments, the platform of choice for many scalable Web services. To achieve high-availability essential to this domain, these environments admit various forms of weak consistency that do not guarantee all replicas have a consistent view of an application's state. Consequently, they often admit difficult-to-understand anomalous behaviors that violate a data type's invariants, but which are extremely challenging, even for experts, to understand and debug.   In this paper, we propose a novel programming framework for replicated data types (RDTs) equipped with an\u00a0\u2026", "num_citations": "28\n", "authors": ["294"]}
{"title": "Automatically learning shape specifications\n", "abstract": " This paper presents a novel automated procedure for discovering expressive shape specifications for sophisticated functional data structures. Our approach extracts potential shape predicates based on the definition of constructors of arbitrary user-defined inductive data types, and combines these predicates within an expressive first-order specification language using a lightweight data-driven learning procedure. Notably, this technique requires no programmer annotations, and is equipped with a type-based decision procedure to verify the correctness of discovered specifications. Experimental results indicate that our implementation is both efficient and effective, capable of automatically synthesizing sophisticated shape specifications over a range of complex data types, going well beyond the scope of existing solutions.", "num_citations": "28\n", "authors": ["294"]}
{"title": "Customization of first-class tuple-spaces in a higher-order language\n", "abstract": " A distributed data structure is an object which permits many producers to augment or modify its contents, and many consumers simultaneously to access its component elements. Synchronization is implicit in data structure access: a process that requests an element which has not yet been generated blocks until a producer creates it.               In this paper, we describe a parallel programming language (called TS) whose fundamental communication device is a significant generalization of the tuple-space distributed data structure found in the Linda coordination language[6]. Our sequential base language is a dialect of Scheme[19].               Beyond the fact that TS is derived by incorporating a tuple-space coordination language into a higher-order computation language (i.e., Scheme), TS differs from other tuple-space languages in two important ways:                                         Tuple-spaces are first-class objects\u00a0\u2026", "num_citations": "28\n", "authors": ["294"]}
{"title": "Learning refinement types\n", "abstract": " We propose the integration of a random test generation system (capable of discovering program bugs) and a refinement type system (capable of expressing and verifying program invariants), for higher-order functional programs, using a novel lightweight learning algorithm as an effective intermediary between the two. Our approach is based on the well-understood intuition that useful, but difficult to infer, program properties can often be observed from concrete program states generated by tests; these properties act as likely invariants, which if used to refine simple types, can have their validity checked by a refinement type checker. We describe an implementation of our technique for a variety of benchmarks written in ML, and demonstrate its effectiveness in inferring and proving useful invariants for programs that express complex higher-order control and dataflow.", "num_citations": "26\n", "authors": ["294"]}
{"title": "Alone together: compositional reasoning and inference for weak isolation\n", "abstract": " Serializability is a well-understood correctness criterion that simplifies reasoning about the behavior of concurrent transactions by ensuring they are isolated from each other while they execute. However, enforcing serializable isolation comes at a steep cost in performance because it necessarily restricts opportunities to exploit concurrency even when such opportunities would not violate application-specific invariants. As a result, database systems in practice support, and often encourage, developers to implement transactions using weaker alternatives. These alternatives break the strong isolation guarantees offered by serializable transactions to permit greater concurrency. Unfortunately, the semantics of weak isolation is poorly understood, and usually explained only informally in terms of low-level implementation artifacts. Consequently, verifying high-level correctness properties in such environments remains a\u00a0\u2026", "num_citations": "25\n", "authors": ["294"]}
{"title": "Composable asynchronous events\n", "abstract": " Although asynchronous communication is an important feature of many concurrent systems, building composable abstractions that leverage asynchrony is challenging. This is because an asynchronous operation necessarily involves two distinct threads of control -- the thread that initiates the operation, and the thread that discharges it. Existing attempts to marry composability with asynchrony either entail sacrificing performance (by limiting the degree of asynchrony permitted), or modularity (by forcing natural abstraction boundaries to be broken). In this paper, we present the design and rationale for asynchronous events, an abstraction that enables composable construction of complex asynchronous protocols without sacrificing the benefits of abstraction or performance. Asynchronous events are realized in the context of Concurrent ML's first-class event abstraction. We discuss the definition of a number of useful\u00a0\u2026", "num_citations": "25\n", "authors": ["294"]}
{"title": "Automated detection of serializability violations under weak consistency\n", "abstract": " While a number of weak consistency mechanisms have been developed in recent years to improve performance and ensure availability in distributed, replicated systems, ensuring correctness of transactional applications running on top of such systems remains a difficult and important problem. Serializability is a well-understood correctness criterion for transactional programs; understanding whether applications are serializable when executed in a weakly-consistent environment, however remains a challenging exercise. In this work, we combine the dependency graph-based characterization of serializability and the framework of abstract executions to develop a fully automated approach for statically finding bounded serializability violations under \\emph{any} weak consistency model. We reduce the problem of serializability to satisfiability of a formula in First-Order Logic, which allows us to harness the power of existing SMT solvers. We provide rules to automatically construct the FOL encoding from programs written in SQL (allowing loops and conditionals) and the consistency specification written as a formula in FOL. In addition to detecting bounded serializability violations, we also provide two orthogonal schemes to reason about unbounded executions by providing sufficient conditions (in the form of FOL formulae) whose satisfiability would imply the absence of anomalies in any arbitrary execution. We have applied the proposed technique on TPC-C, a real world database program with complex application logic, and were able to discover anomalies under Parallel Snapshot Isolation, and verify serializability for unbounded executions under\u00a0\u2026", "num_citations": "24\n", "authors": ["294"]}
{"title": "Flexible access control for JavaScript\n", "abstract": " Providing security guarantees for systems built out of untrusted components requires the ability to define and enforce access control policies over untrusted code. In Web 2.0 applications, JavaScript code from different origins is often combined on a single page, leading to well-known vulnerabilities. We present a security infrastructure which allows users and content providers to specify access control policies over subsets of a JavaScript program by leveraging the concept of delimited histories with revocation. We implement our proposal in WebKit and evaluate it with three policies on 50 widely used websites with no changes to their JavaScript code and report performance overheads and violations.", "num_citations": "23\n", "authors": ["294"]}
{"title": "Randomized leader election\n", "abstract": " We present an efficient randomized algorithm for leader election in large-scale distributed systems. The proposed algorithm is optimal in message complexity (O(n) for a set of n total processes), has round complexity logarithmic in the number of processes in the system, and provides high probabilistic guarantees on the election of a unique leader. The algorithm relies on a balls and bins abstraction and works in two phases. The main novelty of the work is in the first phase where the number of contending processes is reduced in a controlled manner. Probabilistic quorums are used to determine a winner in the second phase. We discuss, in detail, the synchronous version of the algorithm, provide extensions to an asynchronous version and examine the impact of failures.", "num_citations": "23\n", "authors": ["294"]}
{"title": "TS/Scheme: Distributed data structures in Lisp\n", "abstract": " Many parallel dialects of Lisp express concurrency via a lightweightprocess constructor such as future [6], and model process communication via dataftow constraints that exist between a future instance and its consumers. These dataflow constraints may be enforced implicitly via strict operations which are passed a future as an argument (eg, as in MultiLisp [6] or MuI-T [11]), or explicitly via a primitive walt operation on futures (eg, touch in MultiLisp, or the future-wait procedure in QLisp [5]). In either case, task communication is tightly-coupled with task creation: tasks 1 can only initiate requests for the value of an object if the identity of the object is known. Because of this constraint, multiple tasks cannot collectively contribute to the construction of a shared data object without using other explicit (low-level) synchronization primitives such as locks or monitor-like abstractions (eg, qlambda [4] or exlambda [8]). Thus, while\u00a0\u2026", "num_citations": "23\n", "authors": ["294"]}
{"title": "Preemption-based avoidance of priority inversion for Java\n", "abstract": " Priority inversion occurs in concurrent programs when low-priority threads hold shared resources needed by some high-priority thread, causing them to block indefinitely. Shared resources are usually guarded by low-level synchronization primitives such as mutual-exclusion locks, semaphores, or monitors. There are two existing solutions to priority inversion. The first, establishing high-level scheduling invariants over synchronization primitives to eliminate priority inversion a priori, is difficult in practice and undecidable in general. Alternatively, run-time avoidance mechanisms such as priority inheritance still force high-priority threads to wait until desired resources are released. We describe a novel compiler and run-time solution to the problem of priority inversion, along with experimental evaluation of its effectiveness. Our approach allows preemption of any thread holding a resource needed by higher-priority\u00a0\u2026", "num_citations": "22\n", "authors": ["294"]}
{"title": "In search of a simple visual vocabulary\n", "abstract": " Visual languages are more complex than we would like. We introduce a small but powerful visual vocabulary for a visual programming environment that is simple, yet expressive enough to represent the structure of programs and program executions. This vocabulary is not based on any existing textual language. It was designed for the purpose of visually representing and understanding programs and their executions.", "num_citations": "22\n", "authors": ["294"]}
{"title": "A relational framework for higher-order shape analysis\n", "abstract": " We propose the integration of a relational specification framework within a dependent type system capable of verifying complex invariants over the shapes of algebraic datatypes. Our approach is based on the observation that structural properties of such datatypes can often be naturally expressed as inductively-defined relations over the recursive structure evident in their definitions. By interpreting constructor applications (abstractly) in a relational domain, we can define expressive relational abstractions for a variety of complex data structures, whose structural and shape invariants can be automatically verified. Our specification language also allows for definitions of parametricrelations for polymorphic data types that enable highly composable specifications and naturally generalizes to higher-order polymorphic functions. We describe an algorithm that translates relational specifications into a decidable fragment of\u00a0\u2026", "num_citations": "21\n", "authors": ["294"]}
{"title": "Partial memoization of concurrency and communication\n", "abstract": " Memoization is a well-known optimization technique used to eliminate redundant calls for pure functions. If a call to a function f with argument v yields result r, a subsequent call to f with v can be immediately reduced to r without the need to re-evaluate f's body. Understanding memoization in the presence of concurrency and communication is significantly more challenging. For example, if f communicates with other threads, it is not sufficient to simply record its input/output behavior; we must also track inter-thread dependencies induced by these communication actions. Subsequent calls to f can be elided only if we can identify an interleaving of actions from these call-sites that lead to states in which these dependencies are satisfied. Similar issues arise if f spawns additional threads. In this paper, we consider the memoization problem for a higher-order concurrent language whose threads may communicate through\u00a0\u2026", "num_citations": "21\n", "authors": ["294"]}
{"title": "Optimistic concurrency semantics for transactions in coordination languages\n", "abstract": " There has been significant recent interest in exploring the role of coordination languages as middleware for distributed systems. These languages provide operations that allow processes to dynamically and atomically access and manipulate collections of shared data. The need to impose discipline on the manner in which these operations occur becomes paramount if we wish to reason about correctness in the presence of increased program complexity. Transactions provide strong serialization guarantees that allow us to reason about programs in terms of higher-level units of abstraction rather than lower-level data structures. In this paper, we explore the role of an optimistic transactional facility for a Linda-like coordination language. We provide a\u00a0semantics for a transactional coordination calculus and state a\u00a0soundness result for this semantics. Our use of an optimistic concurrency protocol distinguishes\u00a0\u2026", "num_citations": "21\n", "authors": ["294"]}
{"title": "Poling: SMT Aided Linearizability Proofs\n", "abstract": " Proofs of linearizability of concurrent data structures generally rely on identifying linearization points to establish a simulation argument between the implementation and the specification. However, for many linearizable data structure operations, the linearization points may not correspond to their internal static code locations; for example, they might reside in the code of another concurrent operation. To overcome this limitation, we identify important program patterns that expose such instances, and describe a tool (Poling) that automatically verifies the linearizability of implementations that conform to these patterns.", "num_citations": "19\n", "authors": ["294"]}
{"title": "Eliminating read barriers through procrastination and cleanliness\n", "abstract": " Managed languages typically use read barriers to interpret forwarding pointers introduced to keep track of copied objects. For example, in a multicore environment with thread-local heaps and a global, shared heap, an object initially allocated on a local heap may be copied to a shared heap if it becomes the source of a store operation whose target location resides on the shared heap. As part of the copy operation, a forwarding pointer may be established in the original object to point to the copied object. This level of indirection avoids the need to update all of the references to the object that has been copied.", "num_citations": "19\n", "authors": ["294"]}
{"title": "Enhancing locality in structured peer-to-peer networks\n", "abstract": " Distributed hash tables (DHTs), used in a number of structured peer-to-peer systems, provide efficient mechanisms for resource location. A key distinguishing feature of current DHT systems such as Chord, Pastry, and Tapestry is the way they handle locality in the underlying network. Topology-based node identifier assignment, proximity routing, and proximity neighbor selection are examples of heuristics used to minimize message delays in the underlying network. While these heuristics are sometimes effective, they rely on a single global overlay that may install the key of a popular object at a node far from most of the nodes accessing it. Furthermore, a response to a lookup does not contain any locality information about the nodes holding a copy of the object. We address these issues by proposing a novel two-level overlay peer-to-peer architecture. In our architecture, local overlays act as locality-aware caches for\u00a0\u2026", "num_citations": "19\n", "authors": ["294"]}
{"title": "Mergeable replicated data types\n", "abstract": " Programming geo-replicated distributed systems is challenging given the complexity of reasoning about different evolving states on different replicas. Existing approaches to this problem impose significant burden on application developers to consider the effect of how operations performed on one replica are witnessed and applied on others. To alleviate these challenges, we present a fundamentally different approach to programming in the presence of replicated state. Our insight is based on the use of invertible relational specifications of an inductively-defined data type as a mechanism to capture salient aspects of the data type relevant to how its different instances can be safely merged in a replicated environment. Importantly, because these specifications only address a data type's (static) structural properties, their formulation does not require exposing low-level system-level details concerning asynchrony\u00a0\u2026", "num_citations": "17\n", "authors": ["294"]}
{"title": "Lightweight asynchrony using parasitic threads\n", "abstract": " Message-passing is an attractive thread coordination mechanism because it cleanly delineates points in an execution when threads communicate, and unifies synchronization and communication: a sender is allowed to proceed only when a receiver willing to accept the data being sent is available and vice versa. To enable greater performance, however, asynchronous or non-blocking extensions are usually provided that allow senders and receivers to proceed even if a matching partner is unavailable. Lightweight threads with synchronous message-passing can be used to encapsulate asynchronous message-passing operations, although such implementations have greater thread management costs that can negatively impact scalability and performance.", "num_citations": "17\n", "authors": ["294"]}
{"title": "Building Resource Adaptive Software Systems (BRASS) Objectives and System Evaluation\n", "abstract": " As modern software systems continue inexorably to increase in complexity and capability, users have become accustomed to periodic cycles of updating and upgrading to avoid obsolescence---if at some cost in terms of frustration. In the case of the U.S. military, having access to well-functioning software systems and underlying content is critical to national security, but updates are no less problematic than among civilian users and often demand considerable time and expense. To address these challenges, DARPA has announced a new four-year research project to investigate the fundamental computational and algorithmic requirements necessary for software systems and data to remain robust and functional in excess of 100 years. The Building Resource Adaptive Software Systems, or BRASS, program seeks to realize foundational advances in the design and implementation of long-lived software systems that\u00a0\u2026", "num_citations": "16\n", "authors": ["294"]}
{"title": "Verifying custom synchronization constructs using higher-order separation logic\n", "abstract": " Synchronization constructs lie at the heart of any reliable concurrent program. Many such constructs are standard (e.g., locks, queues, stacks, and hash-tables). However, many concurrent applications require custom synchronization constructs with special-purpose behavior. These constructs present a significant challenge for verification. Like standard constructs, they rely on subtle racy behavior, but unlike standard constructs, they may not have well-understood abstract interfaces. As they are custom built, such constructs are also far more likely to be unreliable. This article examines the formal specification and verification of custom synchronization constructs. Our target is a library of channels used in automated parallelization to enforce sequential behavior between program statements. Our high-level specification captures the conditions necessary for correct execution; these conditions reflect program\u00a0\u2026", "num_citations": "16\n", "authors": ["294"]}
{"title": "Lightweight checkpointing for concurrent ML\n", "abstract": " Transient faults that arise in large-scale software systems can often be repaired by reexecuting the code in which they occur. Ascribing a meaningful semantics for safe reexecution in multithreaded code is not obvious, however. For a thread to reexecute correctly a region of code, it must ensure that all other threads that have witnessed its unwanted effects within that region are also reverted to a meaningful earlier state. If not done properly, data inconsistencies and other undesirable behavior might result. However, automatically determining what constitutes a consistent global checkpoint is not straightforward because thread interactions are a dynamic property of the program. In this paper, we present a safe and efficient checkpointing mechanism for Concurrent ML (CML) that can be used to recover from transient faults. We introduce a new linguistic abstraction, called stabilizers, that permits the specification of per\u00a0\u2026", "num_citations": "16\n", "authors": ["294"]}
{"title": "Communication-passing style for coordination languages\n", "abstract": " Coordination languages for parallel and distributed systems specify mechanisms for creating tasks and communicating data among them. These languages typically assume that (a) once a task begins execution on some processor, it will remain resident on that processor throughout its lifetime, and (b) communicating shared data among tasks is through some form of message-passing and data migration. In this paper, we investigate an alternative approach to understanding coordination. Communication-passing style (CmPS) refers to a coordination semantics in which data communication is always undertaken by migrating the continuation of the task requiring the data to the processor where the data resides.             Communication-passing style is closely related to continuation-passing style (CPS), a useful transformation for compiling functional languages. Just as CPS eliminates implicit call-return\u00a0\u2026", "num_citations": "16\n", "authors": ["294"]}
{"title": "Development of a hand-held real-time decision support aid for critical care nursing\n", "abstract": " In the current health care environment, nurse clinicians must work \"faster and smarter\" making complex decisions on almost a continual basis. Evidence-based knowledge and standardized guides, such as clinical algorithms, can support clinical nursing decisions, however; effective real-time access is limited. This paper outlines research addressing this problem. In this research, current clinical knowledge is delivered to the clinician via an off-the-shelf handheld computer using wireless access to a central server and data repository. Innovative minimal-set database, data mining and knowledge discovery algorithms using a combination of case based and rule based learning with added confidence measures permitting bi-directional (forward and backwards) inferencing based on individual client data are developed and presented for the hand held device. The technology provides real-time decision support for the\u00a0\u2026", "num_citations": "15\n", "authors": ["294"]}
{"title": "Compiling Java to a typed lambda-calculus: A preliminary report\n", "abstract": " A typical compiler for Java translates source code into machine-independent byte code. The byte code may be either interpreted by a Java Virtual Machine, or further compiled to native code by a just-in-time compiler. The byte code architecture provides platform independence at the cost of execution speed. When Java is used as a tool for writing applets--small ultra-portable programs that migrate across the web on demand--this tradeoff is justified. However, as Java gains acceptance as a mainstream programming language, performance rather than platform independence becomes a prominent issue. To obtain highperformance code for less mobile applications, we are developing an optimizing compiler for Java that bypasses byte code, and, just like optimizing compilers for C or Fortran, translates Java directly to native code. Our approach to building an optimizing compiler for Java has two novel aspects: we use\u00a0\u2026", "num_citations": "15\n", "authors": ["294"]}
{"title": "Proof-directed parallelization synthesis by separation logic\n", "abstract": " We present an analysis which takes as its input a sequential program, augmented with annotations indicating potential parallelization opportunities, and a sequential proof, written in separation logic, and produces a correctly synchronized parallelized program and proof of that program. Unlike previous work, ours is not a simple independence analysis that admits parallelization only when threads do not interfere; rather, we insert synchronization to preserve dependencies in the sequential program that might be violated by a na\u00efve translation. Separation logic allows us to parallelize fine-grained patterns of resource usage, moving beyond straightforward points-to analysis. The sequential proof need only represent shape properties, meaning we can handle complex algorithms without verifying every aspect of their behavior. Our analysis works by using the sequential proof to discover dependencies between different\u00a0\u2026", "num_citations": "14\n", "authors": ["294"]}
{"title": "Flattening tuples in an SSA intermediate representation\n", "abstract": " For functional programs, unboxing aggregate data structures such as tuples removes memory indirections and frees dead components of the decoupled structures. To explore the consequences of such optimizations in a whole-program compiler, this paper presents a tuple flattening transformation and a framework that allows the formal study and comparison of different flattening schemes.               We present our transformation over functional SSA, a simply-typed, monomorphic language and show that the transformation is type-safe. The flattening algorithm defined by our transformation has been incorporated into MLton, a whole-program, optimizing compiler for SML. Experimental results indicate that aggressive tuple flattening can lead to substantial improvements in runtime performance, a reduction in code size, and a decrease in total allocation without a significant increase in compilation time.", "num_citations": "14\n", "authors": ["294"]}
{"title": "Randomized protocols for duplicate elimination in peer-to-peer storage systems\n", "abstract": " Distributed peer-to-peer systems rely on voluntary participation of peers to effectively manage a storage pool. In such systems, data is generally replicated for performance and availability. If the storage associated with replication is not monitored and provisioned, the underlying benefits may not be realized. Resource constraints, performance scalability, and availability present diverse considerations. Availability and performance scalability, in terms of response time, are improved by aggressive replication, whereas resource constraints limit total storage in the network. Identification and elimination of redundant data pose fundamental problems for such systems. In this paper, we present a novel and efficient solution that addresses availability and scalability with respect to management of redundant data. Specifically, we address the problem of duplicate elimination in the context of systems connected over an\u00a0\u2026", "num_citations": "14\n", "authors": ["294"]}
{"title": "A programming language supporting first-class parallel environments\n", "abstract": " This thesis presents a new programming model called the symmetric model in which the representation of programs is identical to the representation of data to specify a computation, one defines a data structure. This data structure possesses the semantics of a first-class naming environment-it defines a scope and can be used to affect the evaluation environment of other expressions. We present a new programming language called Symmetric Lisp based on the symmetric model. Program structures in Symmetric Lisp are considered non-strict a programs components may be examined even as its other elements continue to evaluate. The first part of the thesis investigates the interaction of non-strictness with first-class naming environments. The second part of the thesis discuss the compilation and implementation of Symmetric Lisp. We present an extended type-inference system for first-class environments that can be used to infer the proper evaluation environment of identifiers found within the scope of environment-yielding expressions. We also present a translation of Symmetric Lisp into a high-level data flow language.Descriptors:", "num_citations": "14\n", "authors": ["294"]}
{"title": "Resource-sensitive synchronization inference by abduction\n", "abstract": " We present an analysis which takes as its input a sequential program, augmented with annotations indicating potential parallelization opportunities, and a sequential proof, written in separation logic, and produces a correctly-synchronized parallelized program and proof of that program. Unlike previous work, ours is not an independence analysis; we insert synchronization constructs to preserve relevant dependencies found in the sequential program that may otherwise be violated by a naive translation. Separation logic allows us to parallelize fine-grained patterns of resource-usage, moving beyond straightforward points-to analysis. Our analysis works by using the sequential proof to discover dependencies between different parts of the program. It leverages these discovered dependencies to guide the insertion of synchronization primitives into the parallelized program, and to ensure that the resulting parallelized\u00a0\u2026", "num_citations": "13\n", "authors": ["294"]}
{"title": "Exceptionally safe futures\n", "abstract": " A future is a well-known programming construct used to introduce concurrency to sequential programs. Computations annotated as futures are executed asynchronously and run concurrently with their continuations. Typically, futures are not transparent annotations: a program with futures need not produce the same result as the sequential program from which it was derived. Safe futures guarantee a future-annotated program produce the same result as its sequential counterpart. Ensuring safety is especially challenging in the presence of constructs such as exceptions that permit the expression of non-local control-flow. For example, a future may raise an exception whose handler is in its continuation. To ensure safety, we must guarantee the continuation does not discard this handler regardless of the continuation\u2019s own internal control-flow (e.g. exceptions it raises or futures it spawns). In this paper, we\u00a0\u2026", "num_citations": "13\n", "authors": ["294"]}
{"title": "A domain-specific compiler theory based framework for automated reaction network generation\n", "abstract": " Catalytic chemical reaction networks are often very complicated because of the numerous species and reactions involved. Hence, automating the network generation process is necessary as it is quite labor intensive and error prone to write down all the reactions manually. We present an automated integrated framework for reaction network generation based on domain-specific compiler theory using a knowledge base of chemistry rules. The chemistry rules represent basic reaction mechanisms that the reactants can undergo. The system's domain-specific compiler takes the rules and initial reactants as inputs, parses the rule text, generates the intermediate representation, and finally produces the reaction network by interpreting the intermediate representation. We chose the Abstract Syntax Tree (AST) as the intermediate representation because of its transparency and ease of search. The system executes the AST\u00a0\u2026", "num_citations": "12\n", "authors": ["294"]}
{"title": "Plethora: An efficient wide-area storage system\n", "abstract": " Trends in conventional storage infrastructure motivate the development of foundational technologies for building a wide-area read-write storage repository capable of providing a single image of a distributed storage resource. The overarching design goals of such an infrastructure include client performance, global resource utilization, system scalability (providing a single logical view of larger resource and user pools) and application scalability (enabling single applications with large resource requirements). Such a storage infrastructure forms the basis for second generation data-grid efforts underlying massive data handling in high-energy physics, nanosciences, and bioinformatics, among others.               This paper describes some of the foundational technologies underlying such a repository, Plethora, for semi-static peer-to-peer (P2P) networks implemented on a wide-area Internet testbed. In contrast to\u00a0\u2026", "num_citations": "12\n", "authors": ["294"]}
{"title": "Continuation-based transformations for coordination languages\n", "abstract": " Coordination languages for parallel and distributed systems specify mechanisms for creating tasks and communicating data among them. These languages typically assume that (a) once a task begins execution on some processor, it will remain resident on that processor throughout its lifetime, and (b) communicating shared data among tasks is through some form of message-passing and data migration. In this paper, we investigate an alternative approach to understanding coordination. Communication-passing style (CmPS) refers to a coordination semantics in which data communication is always undertaken by migrating the continuation of the task requiring the data to the processor where the data resides.Communication-passing style is closely related to continuation-passing style (CPS), a useful transformation for compiling functional languages. Just as CPS eliminates implicit call-return sequences, CmPS\u00a0\u2026", "num_citations": "12\n", "authors": ["294"]}
{"title": "Art: Abstraction Refinement-Guided Training for Provably Correct Neural Networks.\n", "abstract": " Artificial Neural Networks (ANNs) have demonstrated remarkable utility in various challenging machine learning applications. While formally verified properties of their behaviors are highly desired, they have proven notoriously difficult to derive and enforce. Existing approaches typically formulate this problem as a post facto analysis process. In this paper, we present a novel learning framework that ensures such formal guarantees are enforced by construction. Our technique enables training provably correct networks with respect to a broad class of safety properties, a capability that goes well-beyond existing approaches, without compromising much accuracy. Our key insight is that we can integrate an optimization-based abstraction refinement loop into the learning process and operate over dynamically constructed partitions of the input space that considers accuracy and safety objectives synergistically. The refinement procedure iteratively splits the input space from which training data is drawn, guided by the efficacy with which such partitions enable safety verification. We have implemented our approach in a tool (ART) and applied it to enforce general safety properties on unmanned aviator collision avoidance system ACAS Xu dataset and the Collision Detection dataset. Importantly, we empirically demonstrate that realizing safety does not come at the price of much accuracy. Our methodology demonstrates that an abstraction refinement methodology provides a meaningful pathway for building both accurate and correct machine learning networks.", "num_citations": "11\n", "authors": ["294"]}
{"title": "Revocation techniques for Java concurrency\n", "abstract": " This paper proposes two approaches to managing concurrency in Java using a guarded region abstraction. Both approaches use revocation of such regions\u2014the ability to undo their effects automatically and transparently. These new techniques alleviate many of the constraints that inhibit construction of transparently scalable and robust concurrent applications. The first solution, revocable monitors, augments existing mutual exclusion monitors with the ability to dynamically resolve priority inversion and deadlock, by reverting program execution to a consistent state when such situations are detected, while preserving Java semantics. The second technique, transactional monitors, extends the functionality of revocable monitors by implementing guarded regions as lightweight transactions that can be executed concurrently (or in parallel on multiprocessor platforms). The presentation includes discussion of design\u00a0\u2026", "num_citations": "11\n", "authors": ["294"]}
{"title": "Query protocols for highly resilient peer-to-peer networks\n", "abstract": " We prol'e via a rigorous stochaSlie anlllysts that any queJJ', regardless o (type,\"'ill be, sUI: tessruIly'scni \u00abd wlih high probability. Furtbu, we show thDt for a Dc'twork\"'ilb N nodlCS\", Ihc hop complexity o (the prolocolls O (/oSN) with high probahility. We wso ddine bAndwidth complellity, 11 measure orcongesUon at: lily node, and prol'C that 11 is O (log'3. N) lrilb higb probabIlity. We provide Ddelnlled simubtion of the system lind show that It conforms clllSely 10 our IhtDn: lit\" aJ guarantees.", "num_citations": "11\n", "authors": ["294"]}
{"title": "Optimizing Analysis for First-Class Tuple-Spaces\n", "abstract": " This paper considers the design and optimization of a simple asynchronous parallel language that uses rst-class tuple-spaces as its main communication and process creation device. Our proposed kernel language di ers from other tuple-space languages insofar tuple-spaces are treated as true rst-class objects. Moreover, we develop a formal framework for constructing an optimizing preprocessor for such a language. The semantic analysis is based on an inference engine that statically computes the set of tuples (and their structural attributes) that can occupy any given tuple-space. The inference system is non-trivial insofar as it operates in the presence of higher-order functions and non-at data structures (eg, lists). The result of the inference procedure can be used to customize the representation of tuple-space objects.", "num_citations": "11\n", "authors": ["294"]}
{"title": "TransMR: Data-Centric Programming Beyond Data Parallelism.\n", "abstract": " MapReduce and related data-centric programming models have proven to be effective for a variety of large-scale distributed computations, in particular, those that manifest data parallelism. The fault-tolerance model underlying these programming environments relies on deterministic replay, which makes data-sharing (side-effects) across computations harder to support. This significantly limits the application scope of MapReduce and related models. This paper:(i) investigates data sharing (side-effects) in programming models operating on distributed key-value stores, specifically, the inconsistencies between the fault recovery mechanisms in execution and storage layers;(ii) defines semantics for a novel programming model, TransMR (Transactional MapReduce), which addresses these inconsistencies; and (iii) demonstrates broad application scope and enhanced performance through data-sharing across computations for a prototype implementation of the proposed semantics.", "num_citations": "10\n", "authors": ["294"]}
{"title": "The design rationale for Multi-MLton\n", "abstract": " Multi-MLtonis a compiler and runtime environment that targets scalable multicore platforms. It combines new language abstractions and associated compiler analyses for expressing and implementing various kinds of fine-grained parallelism (safe futures, speculation, transactions, etc.), along with a sophisticated runtime system tuned to efficiently handle large numbers of lightweight threads.Multi-MLton defines a programming model in which threads primarily communicate via message-passing. It differs from other message-passing systems insofar as the abstractions it provides permit (a) the expression of isolation of communication effects among groups of communicating threads;(b) composable speculative actions that are message-passing aware;(c) the construction of asynchronous events that seamlessly integrate abstract asynchronous communication protocols with abstract CML-style events, and (d) deterministic concurrency within threads to enable the extraction of additional parallelism when feasible and profitable. These abstractions are supported by a combination of compiletime analyses and specialized runtime structures to enable efficient execution on scalable multicore and manycore platforms.", "num_citations": "10\n", "authors": ["294"]}
{"title": "Relaxed synchronization and eager scheduling in mapreduce\n", "abstract": " MapReduce has rapidly emerged as a programming paradigm for large-scale distributed environments. While the underlying programming model based on maps and reduces has been shown to be effective in specific domains, significant questions relating to performance and application scope remain unresolved. This paper targets questions of performance through relaxed semantics of underlying map and reduce constructs in iterative MapReduce applications like eigenspectra/pagerank and clustering (k-means). Specifically, it investigates the notion of partial synchronizations combined with eager scheduling to overcome global synchronization overheads associated with reduce operations. It demonstrates the use of these partial synchronizations on two complex problems, illustrative of much larger classes of problems.The following specific contributions are reported in the paper:(i) partial synchronizations combined with eager scheduling is capable of yielding significant performance improvements,(ii) diverse classes of applications (on sparse graphs), and data analysis (clustering) can be efficiently computed in MapReduce on hundreds of distributed hosts, and (iii) a general API for partial synchronizations and eager map scheduling holds significant performance potential for other applications with highly irregular data and computation profiles.", "num_citations": "10\n", "authors": ["294"]}
{"title": "Uniformity of environment and computation in MAP\n", "abstract": " Visual programming languages use graphics to visualize and aid in the understanding of programs. We describe the graphical techniques used in the MAP environment to visualize the workspace, data, programs and program executions. Meta information is one approach to supplementing the basic functionality provided in the visual programming environment. We introduce meta-commands, commands to control visual characteristics of program objects, and describe the MAP meta-commands for color, folding, render and transparency. We show how these meta-commands can be used to affect graphical representation of programs and their executions and as a way to optimize the recording of program executions, and present examples of how a programmer can use these techniques to build, execute and analyze programs.", "num_citations": "10\n", "authors": ["294"]}
{"title": "A concurrent abstract interpreter\n", "abstract": " Abstract interpretation [6] has been long regarded as a promising optimization and analysis technique for high-level languages. In this article, we describe an implementation of aconcurrent abstract interpreter. The interpreter evaluates programs written in an expressive parallel language that supports dynamic process creation, first-class locations, list data structures and higher-order procedures. Synchronization in the input language is mediated via first-class shared locations. The analysis computes intra- and inter-threadcontrol anddataflow information. The interpreter is implemented on top of Sting [12], a multi-threaded dialect of Scheme that serves as a high-level operating system for modern programming languages.", "num_citations": "10\n", "authors": ["294"]}
{"title": "Dynamic modules in higher-order languages\n", "abstract": " Providing programmers the ability to construct meaningful abstractions to help manage complexity is a serious language design issue. Many languages define a module system that can be used to specify distinct namespaces, and build user-defined data abstractions; however, few languages support dynamic modules, i.e., modules which are true first-class objects. We define a module semantics for a dialect of Scheme called Rascal. Modules are defined in terms of reified environments, and are first-class objects which may be dynamically created, freely assigned, used as arguments to procedures, etc. If defined naively, however, implementing modules using environments can entail the capture of unwanted bindings, leading to potentially severe violations of lexical abstraction and locality. We address these concerns by giving users great flexibility to manipulate environments, and to constrain the extent and scope\u00a0\u2026", "num_citations": "10\n", "authors": ["294"]}
{"title": "CLOTHO: directed test generation for weakly consistent database systems\n", "abstract": " Relational database applications are notoriously difficult to test and debug. Concurrent execution of database transactions may violate complex structural invariants that constraint how changes to the contents of one (shared) table affect the contents of another. Simplifying the underlying concurrency model is one way to ameliorate the difficulty of understanding how concurrent accesses and updates can affect database state with respect to these sophisticated properties. Enforcing serializable execution of all transactions achieves this simplification, but it comes at a significant price in performance, especially at scale, where database state is often replicated to improve latency and availability.   To address these challenges, this paper presents a novel testing framework for detecting serializability violations in (SQL) database-backed Java applications executing on weakly-consistent storage systems. We manifest our\u00a0\u2026", "num_citations": "9\n", "authors": ["294"]}
{"title": "Distributed uniform sampling in real-world networks\n", "abstract": " Uniform sampling in networks is at the core of a wide variety of randomized algorithms. Random sampling can be peJjonned by modeling the system as a graph with associated transition probabilities and defining a corresponding Markov chain (Me). A random walk ofprescribed minimum length, pelfonned on this graph, yields a stationary dim'ibution, and the corresponding random sample. This sample, however, is not uniform when network nodes have a nonuniform degree distribution. This poses a significant practical challenge since typical large scale, real-world, unstructured networks tend to have non-uniform degree distributions, eg. power-law degree distribution iJl unstructured peer-to-peer networks.In this paper we present a distributed algorithm that enables efficient un (form sampling in large real-world networks. Specifically, we prescribe necessary conditions for uniform sampling in such networks and\u00a0\u2026", "num_citations": "9\n", "authors": ["294"]}
{"title": "Parallelism, persistence and meta-cleanliness in the symmetric Lisp interpreter\n", "abstract": " Symmetric Lisp is a programming language designed around first-class environments, where an environment is a dictionary that associates names with definitions or values. In this paper we describe the logical structure of the Symmetric Lisp interpreter. In other interpreted languages, the interpreter is a virtual machine that evaluates user input on the basis of its own internal state. The Symmetric Lisp interpreter, on the other hand, is a simple finite-state machine with no internal state. Its role is to attach user input to whatever environment the user has specified; such environments are transparent objects created by, maintained by and fully accessible to the user. The interpreter's semantics are secondary to the semantics of environments in Symmetric Lisp: it is the environment-object to which an expression is attached, not the interpreter, that controls the evaluation of expressions.This arrangement has several\u00a0\u2026", "num_citations": "9\n", "authors": ["294"]}
{"title": "Semantic indexing in structured peer-to-peer networks\n", "abstract": " The past few years have seen tremendous advances in distributed storage infrastructure. Unstructured and structured overlay networks have been successfully used in a variety of applications, ranging from file-sharing to scientific data repositories. While unstructured networks benefit from low maintenance overhead, the associated search costs are high. On the other hand, structured networks have higher maintenance overheads, but facilitate bounded time search of installed keywords. When dealing with typical data sets, though, it is infeasible to install every possible search term as a keyword into the structured overlay.State-of-the art semantic indexing techniques have been successfully integrated into peer-to-peer (P2P) systems using semantic overlays. However, exiting approaches are based on the premise that the fundamental ingredient of semantic indexing, a semantic basis for the underlying data, is\u00a0\u2026", "num_citations": "8\n", "authors": ["294"]}
{"title": "A Coherent and Managed Runtime for ML on the SCC.\n", "abstract": " Intel\u2019s Single-Chip Cloud Computer (SCC) is a many-core architecture which stands out due to its complete lack of cache-coherence and the presence of fast, on-die interconnect for inter-core messaging. Cache-coherence, if required, must be implemented in software. Moreover, the amount of shared memory available on the SCC is very limited, requiring stringent management of resources even in the presence of software cachecoherence.In this paper, we present a series of techniques to provide the ML programmer a cache-coherent view of memory, while effectively utilizing both private and shared memory. To that end, we introduces a new, type-guided garbage collection scheme that effectively exploits SCC\u2019s memory hierarchy, attempts to reduce the use of shared memory in favor of message passing buffers, and provides a efficient, coherent global address space. Experimental results over a variety of benchmarks show that more than 99% of the memory requests can be potentially cached. These techniques are realized in MultiMLton, a scalable extension of MLton Standard ML compiler and runtime system on the SCC.", "num_citations": "7\n", "authors": ["294"]}
{"title": "Path-sensitive analysis using edge strings\n", "abstract": " Path sensitivity improves the quality of static analysis by avoiding approximative merging of dataflow facts collected along distinct program paths. Because full path sensitivity has prohibitive cost, it is worthwhile to consider hybrid approaches that provide path sensitivity on selected subsets of paths. In this paper, we consider such a technique based on an edge string, a compact abstraction of a set of static program paths. The edge string es=[e1, e2,..., ek], where each ei is an edge label found in a program\u2019s control-flow graph, is used to disambiguate dataflow facts that manifest only on paths in which es occurs as a subsequence. The length of es dictates the tradeoff between precision and analysis cost. Loosely speaking, edge strings are a path-sensitive analog to the notion of call-strings exploited by context-sensitive analyses. We present a formalization of edge strings and discuss optimizations that incorporate\u00a0\u2026", "num_citations": "7\n", "authors": ["294"]}
{"title": "Modular Checkpointing for Atomicity\n", "abstract": " Transient faults that arise in large-scale software systems can often be repaired by re-executing the code in which they occur. Ascribing a meaningful semantics for safe re-execution in multi-threaded code is not obvious, however. For a thread to correctly re-execute a region of code, it must ensure that all other threads which have witnessed its unwanted effects within that region are also reverted to a meaningful earlier state. If not done properly, data inconsistencies and other undesirable behavior may result. However, automatically determining what constitutes a consistent global checkpoint is not straightforward since thread interactions are a dynamic property of the program.In this paper, we present a safe and efficient checkpointing mechanism for Concurrent ML (CML) that can be used to recover from transient faults. We introduce a new linguistic abstraction called stabilizers that permits the specification of per\u00a0\u2026", "num_citations": "7\n", "authors": ["294"]}
{"title": "An IP address based caching scheme for peer-to-peer networks\n", "abstract": " Distributed hash tables (DHTs), used in a number of current peer-to-peer systems, provide efficient mechanisms for resource location. Systems such as Chord, Pastry, CAN, and Tapestry provide strong guarantees that queries in the overlay network can be resolved in a bounded number of overlay hops, while preserving load balance among the peers. A key distinction in these systems is the way they handle locality in the underlying network. Topology-based node identifier assignment, proximity routing, and proximity neighbor selection are examples of heuristics used to minimize message delays in the underlying network. We investigate the use of source IP addresses to enhance locality in overlay networks based on DHTs. We first show that a naive use of source IP address potentially leads to severe resource imbalance due to nonuniformity of peers over the IP space. We then present an effective caching\u00a0\u2026", "num_citations": "7\n", "authors": ["294"]}
{"title": "Expressing fine-grained parallelism using concurrent data structures\n", "abstract": " A major criticism of concurrent data structures has been that efficient implementations are difficult to construct. To some extent, these criticisms have been valid particularly when considered in the context of fine-grained concurrency. Previous implementations of tuple-space languages, for example, have by and large ignored issues of runtime scheduling and storage management, and have not fully addressed the implications of using semantic-based compile-time analysis for building optimal representations of tuple-space structures.             This paper has focused on both concerns. Type analysis is used to infer structural properties of concurrent data structures. Generating efficient representations for tuple-spaces becomes tractable once their type structure is derived. A runtime kernel that permits deferred evaluation of thread objects makes it possible for programs to generate many fine-grained processes\u00a0\u2026", "num_citations": "7\n", "authors": ["294"]}
{"title": "Automated parameterized verification of crdts\n", "abstract": " Maintaining multiple replicas of data is crucial to achieving scalability, availability and low latency in distributed applications. Conflict-free Replicated Data Types (CRDTs) are important building blocks in this domain because they are designed to operate correctly under the myriad behaviors possible in a weakly-consistent distributed setting. Because of the possibility of concurrent updates to the same object at different replicas, and the absence of any ordering guarantees on these updates, convergence is an important correctness criterion for CRDTs. This property asserts that two replicas which receive the same set of updates (in any order) must nonetheless converge to the same state. One way to prove that operations on a CRDT converge is to show that they commute since commutative actions by definition behave the same regardless of the order in which they execute. In this paper, we present a\u00a0\u2026", "num_citations": "6\n", "authors": ["294"]}
{"title": "Cooking the books: Formalizing JMM implementation recipes\n", "abstract": " The Java Memory Model (JMM) is intended to characterize the meaning of concurrent Java programs. Because of the model's complexity, however, its definition cannot be easily transplanted within an optimizing Java compiler, even though an important rationale for its design was to ensure Java compiler optimizations are not unduly hampered because of the language's concurrency features. In response, Lea's JSR-133 Cookbook for Compiler Writers, an informal guide to realizing the principles underlying the JMM on different (relaxed-memory) platforms was developed. The goal of the cookbook is to give compiler writers a relatively simple, yet reasonably efficient, set of reordering-based recipes that satisfy JMM constraints. In this paper, we present the first formalization of the cookbook, providing a semantic basis upon which the relationship between the recipes defined by the cookbook and the guarantees enforced by the JMM can be rigorously established. Notably, one artifact of our investigation is that the rules defined by the cookbook for compiling Java onto Power are inconsistent with the requirements of the JMM, a surprising result, and one which justifies our belief in the need for formally provable definitions to reason about sophisticated (and racy) concurrency patterns in Java, and their implementation on modern-day relaxed-memory hardware. Our formalization enables simulation arguments between an architecture-independent intermediate representation of the kind suggested by Lea with machine abstractions for Power and x86. Moreover, we provide fixes for cookbook recipes that are inconsistent with the behaviors admitted by the\u00a0\u2026", "num_citations": "6\n", "authors": ["294"]}
{"title": "Speculative n-way barriers\n", "abstract": " Speculative execution is an important technique that has historically been used to extract concurrency from sequential programs. While techniques to support speculation work well when computations perform relatively simple actions (eg, reads and writes to known locations), understanding speculation for multi-threaded programs in which threads may communicate and synchronize through multiple shared references is significantly more challenging, and is the focus of this paper.", "num_citations": "6\n", "authors": ["294"]}
{"title": "High-level abstractions for efficient concurrent systems\n", "abstract": " Parallel symbolic algorithms exhibit characteristics that make their efficient implementation on current multiprocessor platforms difficult: data is generated dynamically and, often have irregular shape and density, data sets typically consist of objects of many different types and structure, the natural unit of concurrency is often much smaller than can be efficiently supported on stock hardware, efficient scheduling, migration and load-balancing strategies vary widely among different algorithms, and sensible decomposition of the program into parallel threads of control often cannot be achieved by mere examination of the source text.Many of these challenges are also faced by implementors of multi-threaded operating systems and kernels [2, 23, 29]. In both cases, the utility of an implementation is determined by how well it supports a diverse range of applications in terms of performance and programmability.", "num_citations": "6\n", "authors": ["294"]}
{"title": "Reflective building blocks for modular systems\n", "abstract": " The formal de nition of any namespace device found in a programming language can be given in terms of transformations on a semantic environment. It is therefore worthwhile to consider the implications of incorporating environments as bona de data objects in a programming system.Because of their expressive power, environments can be easily abused. Reifying an environment can entail the capture of unwanted bindings, leading to potentially severe violations of lexical abstraction and locality. Re ecting a data structure into an environment may cause useful program transformations which rely on static scoping (eg,-conversion) to be no longer applicable. Proposals that have heretofore been suggested for manipulating environments as data objects, however, provide no mechanism to constrain the e ect (or extent) of the rei cation or re ection process.", "num_citations": "6\n", "authors": ["294"]}
{"title": "Building resource adaptive software systems\n", "abstract": " The Defense Advanced Research Projects Agency (DARPA) Building Resource Adaptive Software Systems (BRASS) program is an ambitious effort to improve the resilience and longevity of complex software systems. Its vision seeks a principled integration of adaptive reasoning into all aspects of the software design cycle.", "num_citations": "5\n", "authors": ["294"]}
{"title": "Memoizing multi-threaded transactions\n", "abstract": " There has been much recent interest in using transactions to simplify concurrent programming, improve scalability, and increase performance. When a transaction must abort due to a serializability violation, deadlock, or resource exhaustion, its effects are revoked, and the transaction re-executed. For long-lived transactions, however, the cost of aborts and re-execution can be prohibitive. To ensure performance, programmers are often forced to reason about transaction lifetimes and interactions while structuring their code, defeating the simplicity transactions purport to provide.One way to reduce the overheads of re-executing a failed transaction is to avoid re-executing those operations that were unaffected by the violation (s) that induced the abort. Memoization is one way to capitalize on re-execution savings, especially if violations are not pervasive. Within a transaction, if a procedure p is applied with argument v, and the transaction subsequently aborts, p need only be re-evaluated if its argument when the transaction is retried is different from v. In this paper, we consider the memoization problem for transactions in the context of Concurrent ML (CML)[20]. Our design supports multi-threaded transactions which allow internal communication through synchronous channel-based communication. The challenge to memoization in the context is ensuring that communication actions performed by memoized procedures in the original (aborted) execution can be satisfied when the transaction is retried. We validate the effectiveness of our approach using STMBench7 [9], a customizable transaction benchmark. Our results indicate that memoization for CML\u00a0\u2026", "num_citations": "5\n", "authors": ["294"]}
{"title": "Dynamic state restoration using versioning exceptions\n", "abstract": " We explore the semantics and analysis of a new kind of control structure called a versioning exception that ensures the state of the program, at the point when an exception handler is invoked, reflects the program state at the point when the handler is installed. Versioning exceptions provide a transaction-like versioning semantics to the code protected by a handler: modifications performed within the dynamic context of the corresponding handler are versioned, and committed to the store only if the computation completes normally. Similar to the role of backtracking in logic programming, this facility allows unwanted effects of computations to be discarded when exceptional or undesirable conditions are detected.               We define a novel points-to analysis to efficiently track changes to the store within handler-protected scopes. The role of the analysis is to facilitate optimizations that minimize the number of\u00a0\u2026", "num_citations": "5\n", "authors": ["294"]}
{"title": "Concurrency analysis for Java\n", "abstract": " Concurrency is an integral feature of Java. While there has been recent research [CGS+99,BH99,WR99,Bla99] on devising analyses to eliminate the overhead imposed by synchronization, these analyses do not explicitly track multiple threads of control, nor do they appear particularly well-suited to facilitate other concurrency-related optimizations that may be applicable in a parallel or distributed environment.               In this paper, we develop a novel program analysis for Java, which explicitly incorporates an abstract (semantic) notion of threads. Our analysis framework is distinguished from related efforts in three important respects:                                          1                                             It employs a whole-program flow analysis adapted from a simple sequential analysis framework that formally defines a notion of an abstract thread of control. Our initial approximation defines for each thread the set of objects\u00a0\u2026", "num_citations": "5\n", "authors": ["294"]}
{"title": "Mobile and distributed agents in mobidget\n", "abstract": " The paper briefly presents the design rationale for Mobidget, an agent based distributed programming language, intended for highly mobile applications. Mobidget provides two abstractions to foster mobility in heterogeneous computing environments: agents serve as distributed protection domains, and cliques act as the basic unit of mobility. Migration does not impact the semantics for intra-agent references, which are completely transparent to the programmer even though an agent's contents may be physically distributed.", "num_citations": "5\n", "authors": ["294"]}
{"title": "Virtual topologies: A new concurrency abstraction for high-level parallel languages\n", "abstract": " We present a new concurrency abstraction and implementation technique for high-level (symbolic) parallel languages that allows significant programmer control over load-balancing and mapping of fine-grained lightweight threads. Central to our proposal is the notion of a virtual topology. A virtual topology defines a relation over a collection of virtual processors, and a mapping of those processors to a set of physical processors; processor topologies configured as trees, graphs, butterflies, and meshes are some well-known examples. A virtual topology need not have any correlation with a physical one; it is intended to capture the interconnection structure best suited for a given algorithm. We consider a virtual processor to be an abstraction that defines scheduling, migration and load-balancing policies for the threads it executes. Thus, virtual topologies are intended to provide a simple, expressive and efficient\u00a0\u2026", "num_citations": "5\n", "authors": ["294"]}
{"title": "Locality abstractions for parallel and distributed computing\n", "abstract": " Temporal and spatial locality are significant concerns in the design and implementation of any realistic parallel or distributed computing system. Temporal locality is concerned with relations among objects that share similar lifetimes and birth dates; spatial locality is concerned with relations among objects that share information. Exploiting temporal locality can lead to improved memory behavior; exploiting spatial locality can lead to improved communication behavior. Linguistic, compiler, and runtime support for locality issues is especially important for unstructured symbolic computations in which lifetimes and sharing properties of objects are not readily apparent.             Language abstractions for spatial and temporal locality include mechanisms for grouping related threads of control, allowing programs flexibility to map computations onto virtual processors, reusing dynamic contexts efficiently, and permitting\u00a0\u2026", "num_citations": "5\n", "authors": ["294"]}
{"title": "Version Control Is for Your Data Too\n", "abstract": " Programmers regularly use distributed version control systems (DVCS) such as Git to facilitate collaborative software development. The primary purpose of a DVCS is to maintain integrity of source code in the presence of concurrent, possibly conflicting edits from collaborators. In addition to safely merging concurrent non-conflicting edits, a DVCS extensively tracks source code provenance to help programmers contextualize and resolve conflicts. Provenance also facilitates debugging by letting programmers see diffs between versions and quickly find those edits that introduced the offending conflict (eg, via git blame). In this paper, we posit that analogous workflows to collaborative software development also arise in distributed software execution; we argue that the characteristics that make a DVCS an ideal fit for the former also make it an ideal fit for the latter. Building on this observation, we propose a distributed programming model, called carmot that views distributed shared state as an entity evolving in time, manifested as a sequence of persistent versions, and relies on an explicitly defined merge semantics to reconcile concurrent conflicting versions. We show examples demonstrating how carmot simplifies distributed programming, while also enabling novel workflows integral to modern applications such as blockchains. We also describe a prototype implementation of carmot that we use to evaluate its practicality.", "num_citations": "4\n", "authors": ["294"]}
{"title": "Dependent array type inference from tests\n", "abstract": " We present a type-based program analysis capable of inferring expressive invariants over array programs. Our system combines dependent types with two additional key elements. First, we associate dependent types with effects and precisely track effectful array updates, yielding a sound flow-sensitive dependent type system that can capture invariants associated with side-effecting array programs. Second, without imposing an annotation burden for quantified invariants on array indices, we automatically infer useful array invariants by initially guessing very coarse invariant templates, using test suites to exercise the functionality of the program to faithfully instantiate these templates with more precise (likely) invariants. These inferred invariants are subsequently encoded as dependent types for validation. Experimental results demonstrate the utility of our approach, with respect to both expressivity of the\u00a0\u2026", "num_citations": "4\n", "authors": ["294"]}
{"title": "Dynamic aspects for runtime fault determination and recovery\n", "abstract": " One of the most promising applications of aspect oriented programming (AOP) is the area of fault tolerance and recovery. In traditional programming languages, error handling code must be closely interwoven with program logic. AOP allows the programmer to take a more modular approach - error handling code can be woven into the code by expressing it as an aspect. One major impediment to handling error code in this way is that while errors are a dynamic, runtime property, most research on AOP has focused on static properties. In this paper, we propose a method for handling a variety of run-time faults as dynamic aspects. First, we separate fault handling into two different notions: fault determination, or the discovery of faults within a program, and fault recovery, or the logic used to recover from a fault. Our position is that fault determination should be expressed as dynamic aspects. We propose a system, called\u00a0\u2026", "num_citations": "4\n", "authors": ["294"]}
{"title": "Stochastic analysis of a fault-tolerant and bandwidth-efficient P2P network\n", "abstract": " We present the design and analysis of a fault-tolerant and bandwidth-efficient Peer-to-Peer (P2P) scheme (called as WARP). WARP employs a simple and natural fault-tolerant mechanism to manage the dynamic nature of node arrivals and departures by allowing multiple physical nodes to service data mapped to a single node in the overlay. WARP\u2019s overlay network is asymmetric (built using an underlying tree topology) which naturally allows search queries of two different types\u2014centralized and distributed\u2014to be handled in an uniform way. Centralized queries go to specific chosen nodes in the network and are useful in the context of popular data while distributed queries can be directed to any node in the network. We prove via a stochastic analysis that any query, regardless of type, will be successfully serviced with high probability (whp). Specifically, we show that for a network with N nodes on the average, the hop complexity of any search is O (log N) whp, while the degree of each node is bounded by O (log3 N) whp When many search queries happen at the same time, it is important to avoid congestion at any one node. WARP achieves this by introducing small-world edges to the overlay which reduces the bandwidth complexity (the worst case expected number of queries that go through any node). We prove that this complexity is bounded by O (log3 N) whp We also generalize our results when the P2P network is built using other underlying topologies such as hypercubes or rings. Thus our scheme can be used to \u201cconvert\u201d any static topology into a dynamic fault-tolerant network.", "num_citations": "4\n", "authors": ["294"]}
{"title": "Scalable Synthesis of Verified Controllers in Deep Reinforcement Learning\n", "abstract": " There has been significant recent interest in devising verification techniques for learning-enabled controllers (LECs) that manage safety-critical systems. Given the opacity and lack of interpretability of the neural policies that govern the behavior of such controllers, many existing approaches enforce safety properties through the use of shields, a dynamic monitoring and repair mechanism that ensures a LEC does not emit actions that would violate desired safety conditions. These methods, however, have shown to have significant scalability limitations because verification costs grow as problem dimensionality and objective complexity increase. In this paper, we propose a new automated verification pipeline capable of synthesizing high-quality safety shields even when the problem domain involves hundreds of dimensions, or when the desired objective involves stochastic perturbations, liveness considerations, and other complex non-functional properties. Our key insight involves separating safety verification from neural controller, using pre-computed verified safety shields to constrain neural controller training which does not only focus on safety. Experimental results over a range of realistic high-dimensional deep RL benchmarks demonstrate the effectiveness of our approach.", "num_citations": "3\n", "authors": ["294"]}
{"title": "Robustness to Adversarial Attacks in Learning-Enabled Controllers\n", "abstract": " Learning-enabled controllers used in cyber-physical systems (CPS) are known to be susceptible to adversarial attacks. Such attacks manifest as perturbations to the states generated by the controller's environment in response to its actions. We consider state perturbations that encompass a wide variety of adversarial attacks and describe an attack scheme for discovering adversarial states. To be useful, these attacks need to be natural, yielding states in which the controller can be reasonably expected to generate a meaningful response. We consider shield-based defenses as a means to improve controller robustness in the face of such perturbations. Our defense strategy allows us to treat the controller and environment as black-boxes with unknown dynamics. We provide a two-stage approach to construct this defense and show its effectiveness through a range of experiments on realistic continuous control domains such as the navigation control-loop of an F16 aircraft and the motion control system of humanoid robots.", "num_citations": "3\n", "authors": ["294"]}
{"title": "Mergeable types\n", "abstract": " Distributed applications often eschew strong consistency and replicate data asynchronously to improve availability and fault tolerance. However, programming under eventual consistency is significantly more complex and often leads to onerous programming model where inconsistencies must be handled explicitly. We introduce vml, a programming model that extends ML datatypes with mergeabilitya la version control systems with the ability to define and compose distributed ML computations around such data. Our OCaml implementation instantiates mergeable types on Irmin, a distributed contentaddressible store to enable composable and highly-available distributed applications.", "num_citations": "3\n", "authors": ["294"]}
{"title": "Weaving Atomicity Through Dynamic Dependence Tracking\n", "abstract": " Programmability is the key hurdle towards effectively utilizing next-generation high-performance computing systems. Current trends in CMP processor design point to the emergence of many-core architectures, in which a single chip can support tens to potentially hundreds of cores. Systems constructed by aggregating these processors can enable parallel execution of thousands of threads. Transactional memory (TM) has been the subject of significant interest in both academia and industry because it offers a compelling alternative to existing concurrency control abstractions, making it especially well-suited for programming applications on scalable multi-core platforms. TM abstractions permit logically concurrent access to shared regions of code, but ensure through some combination of hardware, compiler, and runtime support that such accesses do not violate intended serializability invariants. By doing so\u00a0\u2026", "num_citations": "3\n", "authors": ["294"]}
{"title": "Aspect-Based Introspection and Change Analysis for Evolving Programs.\n", "abstract": " Challenges arise in discovering, managing, and testing the impact of changes made to evolving software. These challenges are magnified in software systems that evolve while running, because the new functionality is piece-wise introduced into a live program with prior state produced by earlier component versions. If new functionality introduced into a live system induces bugs, it can be extremely difficult to analyze exactly which differences led to the incorrect behavior. In order to help programmers plan for evolution, understand the impact of specific evolutionary steps, and to diagnose evolution gone wrong, herein we propose combining the benefits of Aspect-Oriented Programming and reflection with impact analysis techniques from the OO and software engineering disciplines. We contribute a tool that assists with the deployment of new code to evolving software that gives insight as to precisely the behavioral changes between the new code and the code it is replacing within the running system. We conclude by considering the challenges of implementing and deploying such a tool and outline our plans for future research and evaluation.", "num_citations": "3\n", "authors": ["294"]}
{"title": "Trace-based memory aliasing across program versions\n", "abstract": " One of the major costs of software development is associated with testing and validation of successive versions of software systems. An important problem encountered in testing and validation is memory aliasing, which involves correlation of variables across program versions. This is useful to ensure that existing invariants are preserved in newer versions and to match program execution histories. Recent work in this area has focused on trace-based techniques to better isolate affected regions. A variation of this general approach considers memory operations to generate more refined impact sets. The utility of such an approach eventually relies on the ability to effectively recognize aliases.               In this paper, we address the general memory aliasing problem and present a probabilistic trace-based technique for correlating memory locations across execution traces, and associated variables in program\u00a0\u2026", "num_citations": "3\n", "authors": ["294"]}
{"title": "A systematic approach for automated reaction network generation\n", "abstract": " In this work, we propose a systematic approach to gather reaction mechanism knowledge and automatically generate reaction networks based on this knowledge. Ontologies are created to model all related information and knowledge. Ontologies for molecule patterns, as well as elementary reaction operations have been created. The reasoning capability provided for an ontology is used to classify molecules or fragments to predefined molecule patterns. A reaction mechanism is modeled as a set of individuals of the defined ontology. The semantic consistency between the elementary steps in reaction mechanism is also validated using the reasoning capability. An execution engine has been developed to automatically generate reaction network, given the reaction mechanisms and molecules. The reaction mechanism knowledge stored in the system can also be easily reused to create new reaction mechnisms.", "num_citations": "3\n", "authors": ["294"]}
{"title": "Flow-directed closure conversion for typed languages (Extended Summary)\n", "abstract": " This paper presents a novel closure conversion strategy which is part of MLton, a wholeprogram compiler for Standard ML. Unlike other implementations, MLton performs closureconversion at an early stage to translate programs to a simply-typed rst-order intermediate language upon which most optimizations are performed. Closure conversion is guided by a global control-ow analysis. Like previous work on defunctionalization, the translation implements closures as elements of datatypes, and uses dispatches at certain call-sites to select the appropriate function to call. However, our use of control-ow analysis leads to a signi cantly improved translation.", "num_citations": "3\n", "authors": ["294"]}
{"title": "Semantics, Specification, and Bounded Verification of Concurrent Libraries in Replicated Systems\n", "abstract": " Geo-replicated systems provide a number of desirable properties such as globally low latency, high availability, scalability, and built-in fault tolerance. Unfortunately, programming correct applications on top of such systems has proven to be very challenging, in large part because of the weak consistency guarantees they offer. These complexities are exacerbated when we try to adapt existing highly-performant concurrent libraries developed for shared-memory environments to this setting. The use of these libraries, developed with performance and scalability in mind, is highly desirable. But, identifying a suitable notion of correctness to check their validity under a weakly consistent execution model has not been well-studied, in large part because it is problematic to na\u00efvely transplant criteria such as linearizability that has a useful interpretation in a shared-memory context to a distributed one where the cost of\u00a0\u2026", "num_citations": "2\n", "authors": ["294"]}
{"title": "Representation without Taxation: A Uniform, Low-Overhead, and High-Level Interface to Eventually Consistent Key-Value Stores.\n", "abstract": " Geo-distributed web applications often favor high availability over strong consistency. In response to this bias, modern-day replicated data stores often eschew sequential consistency in favor of weaker eventual consistency (EC) data semantics. While most operations supported by a typical web application can be engineered, with sufficient care, to function under EC, there are oftentimes critical operations that require stronger consistency guarantees. A few off-the-shelf eventually consistent key-value stores offer tunable consistency levels to address the need for varying consistency guarantees. However, these consistency levels often have poorly-defined ad hoc semantics that is usually too low-level from the perspective of an application to relate their guarantees to invariants that must be respected by the application. Moreover, these guarantees are often defined in way that is strongly influenced by a specific implementation of the data store. While such low-level implementation-dependent solutions do not readily cater to the high-level requirements of an application, relying on ill-defined guarantees additionally complicates the already hard task of reasoning about application semantics under eventual consistency. In this paper, we describe Quelea, a declarative programming model for eventually consistent data stores. A novel aspect of Quelea is that it abstracts the actual implementation of the data store via highlevel programming and system-level models that are agnostic to a specific implementation of the data store. By doing so, Quelea frees application programmers from having to reason about their application in terms of low-level\u00a0\u2026", "num_citations": "2\n", "authors": ["294"]}
{"title": "\u211e CML: A Prescription for Safely Relaxing Synchrony\n", "abstract": " A functional programming discipline, combined with abstractions like Concurrent ML (CML)\u2019s first-class synchronous events, offers an attractive programming model for concurrency. In high-latency distributed environments, like the cloud, however, the high communication latencies incurred by synchronous communication can compromise performance. While switching to an explicitly asynchronous communication model may reclaim some of these costs, program structure and understanding also becomes more complex. To ease the challenge of migrating concurrent applications to distributed cloud environments, we have built an extension of the MultiMLton compiler and runtime that implements CML communication asynchronously, but guarantees that the resulting execution is faithful to the synchronous semantics of CML. We formalize the conditions under which this equivalence holds, and present an\u00a0\u2026", "num_citations": "2\n", "authors": ["294"]}
{"title": "Isolating determinism in multi-threaded programs\n", "abstract": " Futures are a program abstraction that express a simple form of fork-join parallelism. The expression future (e)  declares that e can be evaluated concurrently with the future\u2019s continuation. Safe-futures provide additional deterministic guarantees, ensuring that all data dependencies found in the original (non-future annotated) version are respected. In this paper, we present a dynamic analysis for enforcing determinism of safe-futures in an ML-like language with dynamic thread creation and first-class references. Our analysis tracks the interaction between futures (and their continuations) with other explicitly defined threads of control, and enforces an isolation property that prevents the effects of a continuation from being witnessed by its future, indirectly through their interactions with other threads. Our analysis is defined via a lightweight capability-based dependence tracking mechanism that serves as a\u00a0\u2026", "num_citations": "2\n", "authors": ["294"]}
{"title": "Transactional Support in MapReduce for Speculative Parallelism\n", "abstract": " MapReduce has emerged as a popular programming model for large-scale distributed computing. Its framework enforces strict synchronization between successive map and reduce phases and limited data-sharing within a phase. Use of key-value based persistent storage with MapReduce presents intriguing opportunities and challenges. These challenges relate primarily to semantic inconsistencies arising from the different fault-tolerant mechanisms employed by the execution environment and the underlying storage medium. We define formal transactional semantics for MapReduce over reliable key-value stores. With minimal performance overhead and no increase in program complexity, our solutions support broad classes of distributed applications hitherto infeasible in MapReduce.Specifically, this paper (i) motivates the use of key-value stores as the underlying storage for MapReduce,(ii) defines\u00a0\u2026", "num_citations": "2\n", "authors": ["294"]}
{"title": "A Simple Churn Tolerant Structured Peer-to-Peer Scheme\n", "abstract": " We present a simple and general scheme to build a churn (fault)-tolerant structured Peer-to-Peer (P2P) network. Our scheme shows how to \u201cconvert\u201d a static network into a dynamic distributed hash table (DHT)-based P2P network such that all the good properties of the static network are guaranteed with high probability. Applying our scheme to a cube-connected cycles network, for example, yields a O (log N) degree network, in which every search succeeds in O (log N) hops whp, using O (log N) messages, where N is the expected stable network size. Our scheme has an O (log N) storage overhead (the number of nodes responsible for servicing a data item) and an O (log N) overhead (messages and time) per insertion and no overhead for deletions. All these bounds are essentially optimal.", "num_citations": "2\n", "authors": ["294"]}
{"title": "Scalable Data Collection in Dense Wireless Sensor Networks\n", "abstract": " Applications of sensor networks often necessitate fine-grained data collection, requiring dense deployment of sensors, with associated high data rates. Such deployment and application scenarios impose significant constraints on aggregate network data rate, resource utilization, and robustness. Effective protocols for supporting such data transfers are critical for dense sensing applications. These protocols often rely on spatio-temporal correlations in sensor data to achieve in-network data compression. The message complexity of these schemes is generally lower bounded by n, for a network with n sensors, since correlation is not collocated with sensing. Consequently, as the number of nodes and network density increase, these protocols become increasingly inefficient. We present here a novel protocol, called SNP, for fine-grained data collection, which requires approximately O (n-R) messages, where R, a\u00a0\u2026", "num_citations": "2\n", "authors": ["294"]}
{"title": "Fingerdiff: Improved Duplicate Elimination in Storage Systems.\n", "abstract": " Minimizing the amount of data that must be stored and managed is a key goal for any storage architecture that purports to be scalable. One way to achieve this goal is to avoid maintaining duplicate copies of the same data. Eliminating redundant data at the source by not writing data which has already been stored, not only reduces storage overheads, but can also improve bandwidth utilization. For these reasons, in the face of today\u2019s exponentially growing data volumes, redundant data elimination techniques have assumed critical significance in the design of modern storage systems.Intelligent object partitioning techniques identify data that are new when objects are updated, and transfer only those chunks to a storage server. In this paper, we propose a new object partitioning technique, called fingerdiff, that improves upon existing schemes in several important respects. Most notably fingerdiff dynamically chooses a partitioning strategy for a data object based on its similarities with previously stored objects in order to improve storage and bandwidth utilization. We present a detailed evaluation of fingerdiff, and other existing object partitioning schemes, using a set of real-world workloads. We show that for these workloads, the duplicate elimination strategies employed by fingerdiff improve storage utilization on average by 25%, and bandwidth utilization on average by 40% over comparable techniques.", "num_citations": "2\n", "authors": ["294"]}
{"title": "Plethora: a locality enhancing peer-to-peer network\n", "abstract": " Distributed hash tables (DHTs), used in a number of structured peer-to-peer systems provide efficient mechanisms for resource location. A key distinguishing feature of current DHT systems like Chord [15], Pastry [12], and Tapestry [19] is the way they handle locality in the underlying network. Topology-based node identifier assignment, proximity routing, and proximity neighbor selection are examples of heuristics used to minimize message delays in the underlying network. While these heuristics are sometimes effective, they all rely on a single global overlay that may install the key of a popular object at a node far from most of the nodes accessing it. Furthermore, a response to a lookup message does not contain any locality information about the nodes holding a copy of the object. We address these issues by defining Plethora, a novel two-level overlay peer-to-peer network. A local overlay in Plethora acts as a locality-aware cache for the global overlay, grouping nodes close together in the underlying network. Local overlays are constructed by exploiting the structure of the Internet as Autonomous Systems. We present a detailed experimental study that demonstrates the practicality of the system, and shows performance gains in response time of upto 60% compared to a single global overlay. We also present efficient distributed algorithms for maintaining local overlays in the presence of node arrivals and departures.", "num_citations": "2\n", "authors": ["294"]}
{"title": "Efficient randomized search algorithms in unstructured Peer-to-Peer networks\n", "abstract": " Searching for objects in unstructured peerlo-peer (P2P) networks is an important problem, and one that has received recent attention. In this paper, we present a simple, elegant, yet highly effective technique for object location (including rare objects). Our scheme installs object references at a known numhu of randomly selected peers. A query to the object is routed to a prede-Icnnined number of random peers, selected independently of (he installation procedure. The high probability of a non-empty intersection between these two sets forms the basis for our search mechanism. We prove analyticnUy, and demonstrate experimentally, that our scheme provides high probabilistic guarantees of success, while incurring minimal overhead.EITedive realiz. ation of the approach builds on a number of recent results on generating random walks, and efficiently estimating network size for unstructured networks. The presence\u00a0\u2026", "num_citations": "2\n", "authors": ["294"]}
{"title": "On the interaction between mobile processes and objects\n", "abstract": " Java's remote method invocation mechanism provides a number of features that extended the functionality of traditional client/server-based distributed systems. However, there are a number of characteristics of the language that influence its utility as a vehicle in which to express lightweight mobile processes. Among these are its highly imperative sequential core, the close coupling of control and state as a consequence of its object model, and the fact that remote method calls are not properly tail-recursive. These features impact the likelihood that Java can easily support process and object mobility for programs which exhibit complex communication and distribution patterns.", "num_citations": "2\n", "authors": ["294"]}
{"title": "Coercion as a Metaphor for Computatiion.\n", "abstract": " The idea of coercion {taking objects of one type and transforming them into objects of another {is not a new one, and has been an important feature of language design since the advent of Fortran. This paper considers a generalization of coercion that permits structured transformations between program and data structures. The nature of these coercions goes signi cantly beyond what is found in most modern programming languages.Our intention is to develop a programming model that permits the expression of a wide-range of super cially-diverse modularity constructs within a simple and uni ed framework. We base the design of this model on the observation that a variety of program structures found in modern programming languages are represented fundamentally in terms of an environment. Given suitable transformations that map the environment representation of a program structure into a data object, we can enable the programmer to gain explicit control over his naming environment.", "num_citations": "2\n", "authors": ["294"]}
{"title": "syncope: Automatic Enforcement of Distributed Consistency Guarantees\n", "abstract": " Designing reliable and highly available distributed applications typically requires data to be replicated over geo-distributed stores. But, such architectures force application developers to make an undesirable tradeoff between ease of reasoning, possible when replicated data is required to be strongly consistent, and performance, possible when such guarantees are weakened. Unfortunately, undesirable behaviors may arise under weak consistency that can violate application correctness, forcing designers to either implement ad-hoc mechanisms to avoid these anomalies, or choose to run applications using stronger levels of consistency than necessary. The former approach introduces unwanted complexity, while the latter sacrifices performance. In this paper, we describe a lightweight runtime verification system that relieves developers from having to make such tradeoffs. Instead, our approach leverages\u00a0\u2026", "num_citations": "1\n", "authors": ["294"]}
{"title": "Path-aware static program analyses for specification mining\n", "abstract": " In this chapter, we address issues pertaining to improving software reliability by exploring static program analysis methodologies that permit the efficient mining of useful and pertinent specifications. Our techniques help answer the following two important questions:(1) What are the specifications and invariants a program execution is expected to follow?(2) When specifications are not followed, what are the root causes for failure? The reason these questions are important is because well-defined specifications can significantly enhance the reliability and correctness of complex software systems. When available, they can be used to verify correctness of libraries and device drivers [4, 9, 25, 44], enable modular reuse [35], and guide testing mechanisms toward bugs [15, 21]. When specifications are provided by the user, type systems [13, 18, 19], model checking [9, 25], typestate interpretation [24, 29], and other related\u00a0\u2026", "num_citations": "1\n", "authors": ["294"]}
{"title": "Flexible access control policies with delimited histories and revocation\n", "abstract": " Providing security guarantees for software systems built out of untrusted components requires the ability to enforce finegrained access control policies. This is evident in Web 2.0 applications where JavaScript code from different origins is often combined on a single page, leading to well-known vulnerabilities. We present a security infrastructure which allows users and content providers to specify access control policies over subsets of JavaScript execution traces and reversion to a safe state if a violation is detected. The proposal is evaluated in the context of a production browser where security principals are based on the browser\u2019s same origin policy. Simple security policies can be shown to prevent real attacks without imposing drastic restrictions on legacy applications. We have evaluated our infrastructure with two non-trivial policies on 50 of the Alexa top websites with no changes to the legacy JavaScript code\u00a0\u2026", "num_citations": "1\n", "authors": ["294"]}
{"title": "Scalable data collection in sensor networks\n", "abstract": " Dense sensor deployments impose significant constraints on aggregate network data rate and resource utilization. Effective protocols for such data transfers rely on spatio-temporal correlations in sensor data for in-network data compression. The message complexity of these schemes is generally lower bounded by n, for a network with n sensors, since correlation is not collocated with sensing. Consequently, as the number of nodes and network density increase, these protocols become increasingly inefficient. We present here a novel protocol, called SNP, for fine-grained data collection, which requires approximately O(n\u2009\u2212\u2009R) messages, where R, a measure of redundancy in sensed data generally increases with density. SNP uses spatio-temporal correlations to near-optimally compress data at the source, reducing network traffic and power consumption. We present a comprehensive information\u00a0\u2026", "num_citations": "1\n", "authors": ["294"]}
{"title": "Building verifiable sensing applications through temporal logic specification\n", "abstract": " Sensing is at the core of virtually every DDDAS application. Sensing applications typically involve distributed communication and coordination over large self-organized networks of heterogeneous devices with severe resource constraints. As a consequence, developers must explicitly deal with low-level details, making programming time-consuming and error-prone. To reduce this burden, current sensor network programming languages espouse a model that relies on packaged reusable components to implement relevant pieces of a distributed communication infrastructure. Unfortunately, programmers are often forced to understand the mechanisms used by these implementations in order to optimize resource utilization and performance, and to ensure application requirements are met. To address these issues, we propose a novel and high-level programming model that directly exposes control over sensor\u00a0\u2026", "num_citations": "1\n", "authors": ["294"]}
{"title": "Building verifiable sensing applications through temporal logic specification\n", "abstract": " Sensing is at the core of virtually every DDDAS application. Sensing applications typically involve distributed communication and coordination over large self-organized networks of heterogeneous devices with severe resource constraints. As a consequence, developers must explicitly deal with low-level details, making programming time-consuming and error-prone. To reduce this burden, current sensor network programming languages espouse a model that relies on packaged reusable components to implement relevant pieces of a distributed communication infrastructure. Unfortunately, programmers are often forced to understand the mechanisms used by these implementations in order to optimize resource utilization and performance, and to ensure application requirements are met. To address these issues, we propose a novel and high-level programming model that directly exposes control over sensor network behavior using temporal logic specifications, in conjunction with a set of system state abstractions to specify, generate, and automatically validate resource and communication behavior for sensor network applications. TLA+(the temporal logic of actions) is used as the underlying specification language to express global state abstractions as well as user invariants. We develop a synthesis engine that utilizes TLC (a temporal logic model-checker) to generate detailed actions so that user-provided behavioral properties can be satisfied, guaranteeing program correctness. The synthesis engine generates specifications in TLA+, which are compiled down to sensor node primitive actions. We illustrate our model using a detailed\u00a0\u2026", "num_citations": "1\n", "authors": ["294"]}
{"title": "Formal specification of the ML basis system\n", "abstract": " This document formally specifies the ML Basis system (MLB) in MLton used to program in the large. The system has been designed to be a natural extension of Standard ML, and the specification is given in the style of The Definition of Standard ML [MTHM97](henceforeth, the Definition). This section adopts (often silently) abbreviations, conventions, definitions, and notation from the Definition.", "num_citations": "1\n", "authors": ["294"]}
{"title": "Stabilizers: Safe Lightweight Check-pointing for Concurrent Programs\n", "abstract": " A checkpoint is a mechanism that allows program execution to be restarted from a previously saved state. Checkpoints can be used in conjunction with exception handling abstractions to recover from exceptional or erroneous events, to support debugging or replay mechanisms, or to facilitate algorithms that rely on speculative evaluation. While relatively straightforward in a sequential setting, for example through the capture and application of continuations, it is less clear how 6 ascribea meaningful semantics for lightweight and safe check~ oints in the Dresence of concurrency. For a thread to correctly resume execution fmm a saved checkpoint, it must ensure that all other threads which have witnessed its unwanted effects after the checkpoint was established are also reverted to a meaningful earlier state. If this is not done, data inconsistencies and other undesirable behavior may result. However, automatically\u00a0\u2026", "num_citations": "1\n", "authors": ["294"]}
{"title": "Transactional lock-free objects for real-time Java\n", "abstract": " Priority inversion is an important concern in providing robust synchronization in real-time systems. When a highpriority task attempts to acquire a lock held by a low priority task, it is often necessary to momentarily resume the execution of the low priority task so as to allow it to leave the critical region safely, ensuring that shared resources are not in an inconsistent state. Once these resources are properly released, the high priority task can proceed. In pathological cases, the priority of several threads may have to be increased, and the high priority tasks can experience unbounded delays.An alternative approach would record the original values of shared objects whenever they are modified, restoring them if the executing thread is interrupted by a higher-priority one. This approach thus treats the critical section as a lightweight transaction. This paper presents an extension to the Real-time Specification for Java with transactional lock-free (TLF) objects. Atomic methods of TLF-objects can be accessed concurrently without risking priority inversion. The semantics of our transactions are such that a high-priority thread will always succeed when trying to enter an atomic section. The time to enter is bounded by the number of locations updated within the atomic section. Experimental results undertaken in the context of Ovm, a virtual machine framework for Java that implements the Real-Time Specification for Java, indicates that transactional lock-free objects can improve the responsiveness of high priority threads compared to priority-inheritance based approaches at the cost of a reduction throughput.", "num_citations": "1\n", "authors": ["294"]}
{"title": "Compiling functional languages with flow analysis\n", "abstract": " We argue for the use of aggressive interprocedural flow analysis to guide optimizations for higher-order languages such as Scheme [Clinger and Rees 1991] and ML [Milner et al. 1990]. Functional languages provide abstraction through firstclass procedures and abstract datatypes. Without sophisticated interprocedural analysis, compilers must make overly conservative assumptions about how abstractions are used in the program. These assumptions often lead to poor performance. We present a strategy for building high-performance implementations that relaxes these assumptions and provides a framework upon which useful optimizations can be built. We describe two experiments that provide some evidence that this approach is viable.", "num_citations": "1\n", "authors": ["294"]}
{"title": "k-Bounded Points-To Analysis\n", "abstract": " This paper explores new points-to analysis techniques whose accuracy and cost are biased towards discovering small pointsto sets. Our motivation is based on the intuition that for most practical purposes, consumers of a points-to analysis are more likely to exploit small points-to sets over large ones. By not expending resources on computing information that is unlikely to be of value to its clients, these techniques deliver improved performance and scalability. We describe both demand-driven and exhaustive k-bounded subset-based points-to analyses for Java where k is a clientsupplied constant. For both analyses, we examine fieldbased and field-sensitive variants. The demand-driven strategy implements a bounded breadth-first search over the transpose of a program\u2019s flow graph. The exhaustive analysis bounds the size of points-to sets as it propagates constraints. Experimental results over a range of\u00a0\u2026", "num_citations": "1\n", "authors": ["294"]}