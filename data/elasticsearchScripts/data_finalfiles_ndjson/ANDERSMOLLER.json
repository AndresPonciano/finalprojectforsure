{"title": "Precise analysis of string expressions\n", "abstract": " We perform static analysis of Java programs to answer a simple question: which values may occur as results of string expressions? The answers are summarized for each expression by a regular language that is guaranteed to contain all possible values. We present several applications of this analysis, including statically checking the syntax of dynamically generated expressions, such as SQL queries. Our analysis constructs flow graphs from class files and generates a context-free grammar with a nonterminal for each string expression. The language of this grammar is then widened into a regular language through a variant of an algorithm previously used for speech recognition. The collection of resulting regular languages is compactly represented as a special kind of multi-level automaton from which individual answers may be extracted. If a program error is detected, examples of invalid strings are\u00a0\u2026", "num_citations": "578\n", "authors": ["292"]}
{"title": "Type analysis for JavaScript\n", "abstract": " JavaScript is the main scripting language for Web browsers, and it is essential to modern Web applications. Programmers have started using it for writing complex applications, but there is still little tool support available during development.               We present a static program analysis infrastructure that can infer detailed and sound type information for JavaScript programs using abstract interpretation. The analysis is designed to support the full language as defined in the ECMAScript standard, including its peculiar object model and all built-in functions. The analysis results can be used to detect common programming errors \u2013 or rather, prove their absence, and for producing type information for program comprehension.               Preliminary experiments conducted on real-life JavaScript code indicate that the approach is promising regarding analysis precision on small and medium size programs, which\u00a0\u2026", "num_citations": "424\n", "authors": ["292"]}
{"title": "The pointer assertion logic engine\n", "abstract": " We present a new framework for verifying partial specifications of programs in order to catch type and memory errors and check data structure invariants. Our technique can verify a large class of data structures, namely all those that can be expressed as graph types. Earlier versions were restricted to simple special cases such as lists or trees. Even so, our current implementation is as fast as the previous specialized tools. Programs are annotated with partial specifications expressed in Pointer Assertion Logic, a new notation for expressing properties of the program store. We work in the logical tradition by encoding the programs and partial specifications as formulas in monadic second-order logic. Validity of these formulas is checked by the MONA tool, which also can provide explicit counterexamples to invalid formulas. To make verification decidable, the technique requires explicit loop and function call invariants. In\u00a0\u2026", "num_citations": "363\n", "authors": ["292"]}
{"title": "In defense of soundiness: a manifesto\n", "abstract": " Soundy is the new sound.", "num_citations": "242\n", "authors": ["292"]}
{"title": "Mona version 1.4: User manual\n", "abstract": " It has been known since 1960 that the class of regular languages1 is linked to decidability questions in formal logics. In particular, WS1S (Weak monadic Second-order theory of 1Successor) is decidable2 through the automaton-logic connection, which can be simply stated: the set of satisfying interpretations of a subformula is represented by a finite-state automaton [B\u00fcc60b, Elg61]. WS1S thus acts as a notation for regular languages, just as regular expressions do.", "num_citations": "225\n", "authors": ["292"]}
{"title": "MONA implementation secrets\n", "abstract": " The MONA tool provides an implementation of automaton-based decision procedures for the logics WS1S and WS2S. It has been used for numerous applications, and it is remarkably efficient in practice, even though it faces a theoretically non-elementary worst-case complexity. The implementation has matured over a period of six years. Compared to the first naive version, the present tool is faster by several orders of magnitude. This speedup is obtained from many different contributions working on all levels of the compilation and execution of formulas. We present an overview of MONA and a selection of implementation \"secrets\" that have been discovered and tested over the years, including formula reductions, DAGification, guided tree automata, three-valued logic, eager minimization, BDD-based automata representations, and cache-conscious data structures. We describe these techniques and quantify their\u00a0\u2026", "num_citations": "197\n", "authors": ["292"]}
{"title": "An introduction to XML and Web Technologies\n", "abstract": " This thoroughly class tested text and online tutorial gives a complete introduction to the essentials of the XML standard. It will teach students how to apply web technologies to develop XML based web applications. Through the book, the student will build applications that work together to construct interesting and workable web applications.", "num_citations": "164\n", "authors": ["292"]}
{"title": "Extending Java for high-level Web service construction\n", "abstract": " We incorporate innovations from the  project into the Java language to provide high-level features for Web service programming. The resulting language, JWIG, contains an advanced session model and a flexible mechanism for dynamic construction of XML documents, in particular XHTML. To support program development we provide a suite of program analyses that at compile time verify for a given program that no runtime errors can occur while building documents or receiving form input, and that all documents being shown are valid according to the document type definition for XHTML 1.0.We compare JWIG with Servlets and JSP which are widely used Web service development platforms. Our implementation and evaluation of JWIG indicate that the language extensions can simplify the program structure and that the analyses are sufficiently fast and precise to be practically useful.", "num_citations": "156\n", "authors": ["292"]}
{"title": "Modeling the HTML DOM and browser API in static analysis of JavaScript web applications\n", "abstract": " Developers of JavaScript web applications have little tool support for catching errors early in development. In comparison, an abundance of tools exist for statically typed languages, including sophisticated integrated development environments and specialized static analyses. Transferring such technologies to the domain of JavaScript web applications is challenging. In this paper, we discuss the challenges, which include the dynamic aspects of JavaScript and the complex interactions between JavaScript, HTML, and the browser. From this, we present the first static analysis that is capable of reasoning about the flow of control and data in modern JavaScript applications that interact with the HTML DOM and browser API.", "num_citations": "145\n", "authors": ["292"]}
{"title": "DSD: A schema language for XML\n", "abstract": " XML (eXtensible Markup Language) is a linear syntax for trees, which has gathered a remarkable amount of interest in industry. The acceptance of XML opens new venues for the application of formal methods such as specification of abstract syntax tree sets and tree transformations.", "num_citations": "131\n", "authors": ["292"]}
{"title": "Remedying the Eval that Men Do\n", "abstract": " A range of static analysis tools and techniques have been developed in recent years with the aim of helping JavaScript web application programmers produce code that is more robust, safe, and efficient. However, as shown in a previous large-scale study, many web applications use the JavaScript eval function to dynamically construct code from text strings in ways that obstruct existing static analyses. As a consequence, the analyses either fail to reason about the web applications or produce unsound or useless results.", "num_citations": "127\n", "authors": ["292"]}
{"title": "Determinacy in Static Analysis of jQuery\n", "abstract": " Static analysis for JavaScript can potentially help programmers find errors early during development. Although much progress has been made on analysis techniques, a major obstacle is the prevalence of libraries, in particular jQuery, which apply programming patterns that have detrimental consequences on the analysis precision and performance. Previous work on dynamic determinacy analysis has demonstrated how information about program expressions that always resolve to a fixed value in some call context may lead to significant scalability improvements of static analysis for such code. We present a static dataflow analysis for JavaScript that infers and exploits determinacy information on-the-fly, to enable analysis of some of the most complex parts of jQuery. The analysis combines selective context and path sensitivity, constant propagation, and branch pruning, based on a systematic investigation of the\u00a0\u2026", "num_citations": "114\n", "authors": ["292"]}
{"title": "Static analysis of XML transformations in Java\n", "abstract": " XML documents generated dynamically by programs are typically represented as text strings or DOM trees. This is a low-level approach for several reasons: 1) traversing and modifying such structures can be tedious and error prone, 2) although schema languages, e.g., DTD, allow classes of XML documents to be defined, there are generally no automatic mechanisms for statically checking that a program transforms from one class to another as intended. We introduce XACT, a high-level approach for Java using XML templates as a first-class data type with operations for manipulating XML values based on XPath. In addition to an efficient runtime representation, the data type permits static type checking using DTD schemas as types. By specifying schemes for the input and output of a program, our analysis algorithm will statically verify that valid input data is always transformed into valid output data and that the\u00a0\u2026", "num_citations": "112\n", "authors": ["292"]}
{"title": "Mona 1. x: new techniques for WS1S and WS2S\n", "abstract": " In this note, we present the first version of the MONA tool to be released in its entirety. The tool now offers decision procedures for both WS1S and WS2S and a completely rewritten front-end. Here, we present some of our techniques, which make calculations couched in WS1S run up to five times faster than with our pre-release tool based on M2L(Str). This suggests that WS1S\u2014with its better semantic properties\u2014is preferable to M2L(Str).", "num_citations": "109\n", "authors": ["292"]}
{"title": "Systematic Execution of Android Test Suites in Adverse Conditions\n", "abstract": " Event-driven applications, such as, mobile apps, are difficult to test thoroughly. The application programmers often put significant effort into writing end-to-end test suites. Even though such tests often have high coverage of the source code, we find that they often focus on the expected behavior, not on occurrences of unusual events. On the other hand, automated testing tools may be capable of exploring the state space more systematically, but this is mostly without knowledge of the intended behavior of the individual applications. As a consequence, many programming errors remain unnoticed until they are encountered by the users. We propose a new methodology for testing by leveraging existing test suites such that each test case is systematically exposed to adverse conditions where certain unexpected events may interfere with the execution. In this way, we explore the interesting execution paths and take\u00a0\u2026", "num_citations": "100\n", "authors": ["292"]}
{"title": "Powerforms: Declarative client-side form field validation\n", "abstract": " All uses of HTML forms may benefit from validation of the specified input field values. Simple validation matches individual values against specified formats, while more advanced validation may involve interdependencies of form fields. There is currently no standard for specifying or implementing such validation. Today, CGI programmers often use Perl libraries for simple server-side validation or program customized JavaScript solutions for client-side validation. We present PowerForms, which is an add-on to HTML forms that allows a purely declarative specification of input formats and sophisticated interdependencies of form fields. While our work may be seen as inspiration for a future extension of HTML, it is also available for CGI programmers today through a preprocessor that translates a PowerForms document into a combination of standard HTML and JavaScript that works on all combinations of\u00a0\u2026", "num_citations": "94\n", "authors": ["292"]}
{"title": "Dual syntax for XML languages\n", "abstract": " XML is successful as a machine processable data interchange format, but it is often too verbose for human use. For this reason, many XML languages permit an alternative more legible non-XML syntax. XSLT stylesheets are often used to convert from the XML syntax to the alternative syntax; however, such transformations are not reversible since no general tool exists to automatically parse the alternative syntax back into XML.We present XSugar, which makes it possible to manage dual syntax for XML languages. An XSugar specification is built around a context-free grammar that unifies the two syntaxes of a language. Given such a specification, the XSugar tool can translate from alternative syntax to XML and vice versa. Moreover, the tool statically checks that the transformations are reversible and that all XML documents generated from the alternative syntax are valid according to a given XML schema.", "num_citations": "87\n", "authors": ["292"]}
{"title": "Interprocedural analysis with lazy propagation\n", "abstract": " We propose lazy propagation as a technique for flow- and context-sensitive interprocedural analysis of programs with objects and first-class functions where transfer functions may not be distributive. The technique is described formally as a systematic modification of a variant of the monotone framework and its theoretical properties are shown. It is implemented in a type analysis tool for JavaScript where it results in a significant improvement in performance.", "num_citations": "81\n", "authors": ["292"]}
{"title": "Analyzing ambiguity of context-free grammars\n", "abstract": " It has been known since 1962 that the ambiguity problem for context-free grammars is undecidable. Ambiguity in context-free grammars is a recurring problem in language design and parser generation, as well as in applications where grammars are used as models of real-world physical structures.We observe that there is a simple linguistic characterization of the grammar ambiguity problem, and we show how to exploit this by presenting an ambiguity analysis framework based on conservative language approximations. As a concrete example, we propose a technique based on local regular approximations and grammar unfoldings. We evaluate the analysis using grammars that occur in RNA analysis in bioinformatics, and we demonstrate that it is sufficiently precise and efficient to be practically useful.", "num_citations": "76\n", "authors": ["292"]}
{"title": "Tool-supported refactoring for JavaScript\n", "abstract": " Refactoring is a popular technique for improving the structure of existing programs while maintaining their behavior. For statically typed programming languages such as Java, a wide variety of refactorings have been described, and tool support for performing refactorings and ensuring their correctness is widely available in modern IDEs. For the JavaScript programming language, however, existing refactoring tools are less mature and often unable to ensure that program behavior is preserved. Refactoring algorithms that have been developed for statically typed languages are not applicable to JavaScript because of its dynamic nature. We propose a framework for specifying and implementing JavaScript refactorings based on pointer analysis. We describe novel refactorings motivated by best practice recommendations for JavaScript programming, and demonstrate how they can be described concisely in terms of\u00a0\u2026", "num_citations": "75\n", "authors": ["292"]}
{"title": "Static validation of dynamically generated HTML\n", "abstract": " We describe a static analysis of< bigwig> programs that efficiently decides if all dynamically computed XHTML documents presented to the client will validate according to the official DTD. We employ two data-flow analyses to construct a graph summarizing the possible documents. This graph is subsequently analyzed to determine validity of those documents. By evaluating the technique on a number of realistic benchmarks, we demonstrate that it is sufficiently fast and precise to be practically useful.", "num_citations": "65\n", "authors": ["292"]}
{"title": "The design space of type checkers for XML transformation languages\n", "abstract": " We survey work on statically type checking XML transformations, covering a wide range of notations and ambitions. The concept of type may vary from idealizations of DTD to full-blown XML Schema or even more expressive formalisms. The notion of transformation may vary from clean and simple transductions to domain-specific languages or integration of XML in general-purpose programming languages. Type annotations can be either explicit or implicit, and type checking ranges from exact decidability to pragmatic approximations.               We characterize and evaluate existing tools in this design space, including a recent result of the authors providing practical type checking of full unannotated XSLT 1.0 stylesheets given general DTDs that describe the input and output languages.", "num_citations": "60\n", "authors": ["292"]}
{"title": "Checking Correctness of TypeScript Interfaces for JavaScript Libraries\n", "abstract": " The TypeScript programming language adds optional types to JavaScript, with support for interaction with existing JavaScript libraries via interface declarations. Such declarations have been written for hundreds of libraries, but they can be difficult to write and often contain errors, which may affect the type checking and misguide code completion for the application code in IDEs.", "num_citations": "58\n", "authors": ["292"]}
{"title": "Static Program Analysis\n", "abstract": " Static program analysis is the art of reasoning about the behavior of computer programs without actually running them. This is useful not only in optimizing compilers for producing efficient code but also for automatic error detection and other tools that can help programmers. A static program analyzer is a program that reasons about the behavior of other programs. For anyone interested in programming, what can be more fun than writing programs that analyze programs?As known from Turing and Rice, all nontrivial properties of the behavior of programs written in common programming languages are mathematically undecidable. This means that automated reasoning of software generally must involve approximation. It is also well known that testing, ie concretely running programs and inspecting the output, may reveal errors but generally cannot show their absence. In contrast, static program analysis can\u2013with the right kind of approximations\u2013check all possible executions of the programs and provide guarantees about their properties. One of the key challenges when developing such analyses is how to ensure high precision and efficiency to be practically useful. For example, nobody will use an analysis designed for bug finding if it reports many false positives or if it is too slow to fit into real-world software development processes.", "num_citations": "50\n", "authors": ["292"]}
{"title": "Static analysis for Java Servlets and JSP\n", "abstract": " We present an approach for statically reasoning about the behavior of Web applications that are developed using Java Servlets and JSP. Specifically, we attack the problems of guaranteeing that all output is well-formed and valid XML and ensuring consistency of XHTML form fields and session state. Our approach builds on a collection of program analysis techniques developed earlier in the JWIG and Xact projects, combined with work on balanced context-free grammars. Together, this provides the necessary foundation concerning reasoning about output streams and application control flow.", "num_citations": "49\n", "authors": ["292"]}
{"title": "Semi-Automatic Rename Refactoring for JavaScript\n", "abstract": " Modern IDEs support automated refactoring for many programming languages, but support for JavaScript is still primitive. To perform renaming, which is one of the fundamental refactorings, there is often no practical alternative to simple syntactic search-and-replace. Although more sophisticated alternatives have been developed, they are limited by whole-program assumptions and poor scalability. We propose a technique for semi-automatic refactoring for JavaScript, with a focus on renaming. Unlike traditional refactoring algorithms, semi-automatic refactoring works by a combination of static analysis and interaction with the programmer. With this pragmatic approach, we can provide scalable and effective refactoring support for real-world code, including libraries and incomplete applications. Through a series of experiments that estimate how much manual effort our technique demands from the programmer, we\u00a0\u2026", "num_citations": "47\n", "authors": ["292"]}
{"title": "Type Regression Testing to Detect Breaking Changes in Node.js Libraries\n", "abstract": " The npm repository contains JavaScript libraries that are used by millions of software developers. Its semantic versioning system relies on the ability to distinguish between breaking and non-breaking changes when libraries are updated. However, the dynamic nature of JavaScript often causes unintended breaking changes to be detected too late, which undermines the robustness of the applications. We present a novel technique, type regression testing, to automatically determine whether an update of a library implementation affects the types of its public interface, according to how the library is being used by other npm packages. By leveraging available test suites of clients, type regression testing uses a dynamic analysis to learn models of the library interface. Comparing the models before and after an update effectively amplifies the existing tests by revealing changes that may affect the clients. Experimental results on 12 widely used libraries show that the technique can identify type-related breaking changes with high accuracy. It fully automatically classifies at least 90% of the updates correctly as either major or as minor or patch, and it detects 26 breaking changes among the minor and patch updates.", "num_citations": "39\n", "authors": ["292"]}
{"title": "The DSD schema language\n", "abstract": " XML (Extensible Markup Language), a linear syntax for trees, has gathered a remarkable amount of interest in industry. The acceptance of XML opens new venues for the application of formal methods such as specification of abstract syntax tree sets and tree transformations.               A user domain may be specified as a set of trees. For example, XHTML is a user domain corresponding to a set of XML documents that make sense as hypertext. A notation for defining such a set of XML trees is called a schema language. We believe that a useful schema notation must identify most of the syntactic requirements present in the user domains, and yet be sufficiently simple and easy to understand both by the schema authors and the users. Furthermore, it must allow efficient parsing and be modular and extensible to support reuse and evolution of descriptions.               In the present paper, we give a tutorial\u00a0\u2026", "num_citations": "38\n", "authors": ["292"]}
{"title": "Static validation of XSL Transformations\n", "abstract": " XSL Transformations (XSLT) is a programming language for defining transformations among XML languages. The structure of these languages is formally described by schemas, for example using DTD or XML Schema, which allows individual documents to be validated. However, existing XSLT tools offer no static guarantees that, under the assumption that the input is valid relative to the input schema, the output of the transformation is valid relative to the output schema. We present a validation technique for XSLT based on the XML graph formalism introduced in the static analysis of JWIG Web services and XACT XML transformations. Being able to provide static guarantees, we can detect a large class of errors in an XSLT stylesheet at the time it is written instead of later when it has been deployed, and thereby provide benefits similar to those of static type checkers for modern programming languages. Our analysis\u00a0\u2026", "num_citations": "35\n", "authors": ["292"]}
{"title": "Document Structure Description 1.0\n", "abstract": " Document Structure Description 1.0 is a complete specification of a new XML notation for describing classes of XML documents. The notation is designed to be a simple tool based on familiar concepts. DSDs provide more flexible and precise structural descriptions than possible with DTDs or the current XML Schema proposal. A DSD generates a CSS-like default mechanism independent of formatting models. Finally, it allows an extension mechanism so that DSDs may be updated with new structural concepts.", "num_citations": "35\n", "authors": ["292"]}
{"title": "Compile-time debugging of C programs working on trees\n", "abstract": " We exhibit a technique for automatically verifying the safety of simple C programs working on tree-shaped data structures. We do not consider the complete behavior of programs, but only attempt to verify that they respect the shape and integrity of the store. A verified program is guaranteed to preserve the tree-shapes of data structures, to avoid pointer errors such as NULL dereferences, leaking memory, and dangling references, and furthermore to satisfy assertions specified in a specialized store logic.               A program is transformed into a single formula in WSRT, an extension of WS2S that is decided by the MONA tool. This technique is complete for loop-free code, but for loops and recursive functions we rely on Hoare-style invariants. A default well-formedness invariant is supplied and can be strengthened as needed by programmer annotations. If a program fails to verify, a counterexample in the form of\u00a0\u2026", "num_citations": "35\n", "authors": ["292"]}
{"title": "Systematic Approaches for Increasing Soundness and Precision of Static Analyzers\n", "abstract": " Building static analyzers for modern programming languages is difficult. Often soundness is a requirement, perhaps with some well-defined exceptions, and precision must be adequate for producing useful results on realistic input programs. Formally proving such properties of a complex static analysis implementation is rarely an option in practice, which raises the challenge of how to identify causes and importance of soundness and precision problems.", "num_citations": "34\n", "authors": ["292"]}
{"title": "Static analysis for dynamic XML\n", "abstract": " We describe the summary graph lattice for dataflow analysis of programs that dynamically construct XML documents. Summary graphs have successfully been used to provide static guarantees in the JWIG language for programming interactive Web services. In particular, the JWIG compiler is able to check validity of dynamically generated XHTML documents and to type check dynamic form data. In this paper we present summary graphs and indicate their applicability for various scenarios. We also show that summary graphs have exactly the same expressive power as the regular expression types from XDuce, but that the extra structure in summary graphs makes them more suitable for certain program analyses.", "num_citations": "34\n", "authors": ["292"]}
{"title": "Automated Detection of Client-State Manipulation Vulnerabilities\n", "abstract": " Web application programmers must be aware of a wide range of potential security risks. Although the most common pitfalls are well described and categorized in the literature, it remains a challenging task to ensure that all guidelines are followed. For this reason, it is desirable to construct automated tools that can assist the programmers in the application development process by detecting weaknesses. Many vulnerabilities are related to Web application code that stores references to application state in the generated HTML documents to work around the statelessness of the HTTP protocol. In this article, we show that such client-state manipulation vulnerabilities are amenable to tool-supported detection. We present a static analysis for the widely used frameworks Java Servlets, JSP, and Struts. Given a Web application archive as input, the analysis identifies occurrences of client state and infers the information flow\u00a0\u2026", "num_citations": "30\n", "authors": ["292"]}
{"title": "A runtime system for interactive Web services\n", "abstract": " Interactive Web services are increasingly replacing traditional static Web pages. Producing Web services seems to require a tremendous amount of laborious low-level coding due to the primitive nature of CGI programming. We present ideas for an improved runtime system for interactive Web services built on top of CGI running on virtually every combination of browser and HTTP/CGI server. The runtime system has been implemented and used extensively in <bigwig>, a tool for producing interactive Web services.", "num_citations": "29\n", "authors": ["292"]}
{"title": "Precision-Guided Context Sensitivity for Pointer Analysis\n", "abstract": " Context sensitivity is an essential technique for ensuring high precision in Java pointer analyses. It has been observed that applying context sensitivity partially, only on a select subset of the methods, can improve the balance between analysis precision and speed. However, existing techniques are based on heuristics that do not provide much insight into what characterizes this method subset. In this work, we present a more principled approach for identifying precision-critical methods, based on general patterns of value flows that explain where most of the imprecision arises in context-insensitive pointer analysis. Accordingly, we provide an efficient algorithm to recognize these flow patterns in a given program and exploit them to yield good tradeoffs between analysis precision and speed.   Our experimental results on standard benchmark and real-world programs show that a pointer analysis that applies context\u00a0\u2026", "num_citations": "28\n", "authors": ["292"]}
{"title": "Scalability-First Pointer Analysis with Self-Tuning Context-Sensitivity\n", "abstract": " Context-sensitivity is important in pointer analysis to ensure high precision, but existing techniques suffer from unpredictable scalability. Many variants of context-sensitivity exist, and it is difficult to choose one that leads to reasonable analysis time and obtains high precision, without running the analysis multiple times.", "num_citations": "28\n", "authors": ["292"]}
{"title": "A runtime system for XML transformations in Java\n", "abstract": " We show that it is possible to extend a general-purpose programming language with a convenient high-level data-type for manipulating XML documents while permitting (1) precise static analysis for guaranteeing validity of the constructed XML documents relative to the given DTD schemas, and (2) a runtime system where the operations can be performed efficiently. The system, named Xact, is based on a notion of immutable XML templates and uses XPath for deconstructing documents. A companion paper presents the program analysis; this paper focuses on the efficient runtime representation.", "num_citations": "25\n", "authors": ["292"]}
{"title": "QuickChecking Static Analysis Properties\n", "abstract": " A static analysis can check programs for potential errors. A natural question that arises is therefore: who checks the checker? Researchers have given this question varying attention, ranging from basic testing techniques, informal monotonicity arguments, thorough pen\u2010and\u2010paper soundness proofs, to verified fixed point checking. In this paper, we demonstrate how quickchecking can be useful to test a range of static analysis properties with limited effort. We show how to check a range of algebraic lattice properties, to help ensure that an implementation follows the formal specification of a lattice. Moreover, we offer a number of generic, type\u2010safe combinators to check transfer functions and operators on lattices, to help ensure that these are, eg, monotone, strict, or invariant. We substantiate our claims by quickchecking a type analysis for the Lua programming language.", "num_citations": "24\n", "authors": ["292"]}
{"title": "Sparse Dataflow Analysis with Pointers and Reachability\n", "abstract": " Many static analyzers exploit sparseness techniques to reduce the amount of information being propagated and stored during analysis. Although several variations are described in the literature, no existing technique is suitable for analyzing JavaScript code. In this paper, we point out the need for a sparse analysis framework that supports pointers and reachability.We present such a framework, which uses static single assignment form for heap addresses and computes def-use information on-the-fly.We also show that essential information about dominating definitions can be maintained efficiently using quadtrees. The framework is presented as a systematic modification of a traditional dataflow analysis algorithm.             Our experimental results demonstrate the effectiveness of the technique for a suite of JavaScript programs. By also comparing the performance with an idealized staged approach that\u00a0\u2026", "num_citations": "24\n", "authors": ["292"]}
{"title": "XML graphs in program analysis\n", "abstract": " XML graphs have shown to be a simple and effective formalism for representing sets of XML documents in program analysis. It has evolved through a six year period with variants tailored for a range of applications. We present a unified definition, outline the key properties including validation of XML graphs against different XML schema languages, and provide a software package that enables others to make use of these ideas. We also survey the use of XML graphs for program analysis with four very different languages: Xact (XML in Java), Java Servlets (Web application programming), XSugar (transformations between XML and non-XML data), and XSLT (stylesheets for transforming XML documents).", "num_citations": "24\n", "authors": ["292"]}
{"title": "Type Checking with XML Schema in XACT.\n", "abstract": " We show how to extend the program analysis technique used in the Xact system to support XML Schema as type formalism. Moreover, we introduce optional type annotations to improve modularity of the type checking. The resulting system supports a flexible style of programming XML transformations and provides static guarantees of validity of the generated XML data.", "num_citations": "23\n", "authors": ["292"]}
{"title": "Document structure description 2.0\n", "abstract": " A schema language for XML provides a notation for defining classes of XML documents by describing syntactic requirements for their structure and contents. This document contains the specification of the Document Structure Description 2.0 (DSD2) schema language for XML. The specification describes the syntax and semantics of DSD2 and the relation between DSD2 schemas and instance documents.", "num_citations": "22\n", "authors": ["292"]}
{"title": "Language-based caching of dynamically generated HTML\n", "abstract": " Increasingly, HTML documents are dynamically generated by interactive Web services. To ensure that the client is presented with the newest versions of such documents it is customary to disable client caching causing a seemingly inevitable performance penalty. In the  system, dynamic HTML documents are composed of higher-order templates that are plugged together to construct complete documents. We show how to exploit this feature to provide an automatic fine-grained caching of document templates, based on the service source code. A  service transmits not the full HTML document but instead a compact JavaScript recipe for a client-side construction of the document based on a static collection of fragments that can be cached by the browser in the usual manner. We compare our approach with related techniques and demonstrate on a number of realistic benchmarks that the size of the transmitted\u00a0\u2026", "num_citations": "21\n", "authors": ["292"]}
{"title": "Server Interface Descriptions for Automated Testing of JavaScript Web Applications\n", "abstract": " Automated testing of JavaScript web applications is complicated by the communication with servers. Specifically, it is difficult to test the JavaScript code in isolation from the server code and database contents. We present a practical solution to this problem. First, we demonstrate that formal server interface descriptions are useful in automated testing of JavaScript web applications for separating the concerns of the client and the server. Second, to support the construction of server interface descriptions for existing applications, we introduce an effective inference technique that learns communication patterns from sample data.", "num_citations": "19\n", "authors": ["292"]}
{"title": "Contracts for cooperation between Web service programmers and HTML designers\n", "abstract": " We propose a system based on XML templates and formalized contracts allowing a flexible separation of concerns. The contracts act as interfaces between the programmers and the HTML designers and permit tool support for statically checking that both parties fulfill their obligations. This ensures that (1) programmers and HTML designers work more independently focusing on their own expertises,(2) the Web service implementation is better structured and thus easier to develop and maintain,(3) it is guaranteed that only valid HTML is sent to the clients even though it is constructed dynamically,(4) the programmer uses the XML templates consistently, and (5) the form input fields being sent to the client always match the code receiving those values. Additionally, we describe tools that aid in the construction and management of contracts and XML templates.", "num_citations": "18\n", "authors": ["292"]}
{"title": "Inference and Evolution of TypeScript Declaration Files\n", "abstract": " TypeScript is a typed extension of JavaScript that has become widely used. More than 2000 JavaScript libraries now have publicly available TypeScript declaration files, which allows the libraries to be used when programming TypeScript applications. Such declaration files are written manually, however, and they are often lagging behind the continuous development of the libraries, thereby hindering their usability. The existing tool tscheck is capable of detecting mismatches between the libraries and their declaration files, but it is less suitable when creating and evolving declaration files. In this work we present the tools tsinfer and tsevolve that are designed to assist the construction of new TypeScript declaration files and support the co-evolution of the declaration files as the underlying JavaScript libraries evolve. Our experimental results involving major libraries demonstrate that tsinfer and tsevolve are superior to\u00a0\u2026", "num_citations": "17\n", "authors": ["292"]}
{"title": "Type Test Scripts for TypeScript Testing\n", "abstract": " TypeScript applications often use untyped JavaScript libraries. To support static type checking of such applications, the typed APIs of the libraries are expressed as separate declaration files. This raises the challenge of checking that the declaration files are correct with respect to the library implementations. Previous work has shown that mismatches are frequent and cause TypeScript's type checker to misguide the programmers by rejecting correct applications and accepting incorrect ones.   This paper shows how feedback-directed random testing, which is an automated testing technique that has mostly been used for testing Java libraries, can be adapted to effectively detect such type mismatches. Given a JavaScript library with a TypeScript declaration file, our tool TSTEST generates a \"type test script\", which is an application that interacts with the library and tests that it behaves according to the type declarations\u00a0\u2026", "num_citations": "16\n", "authors": ["292"]}
{"title": "Extracting Taint Specifications for JavaScript Libraries\n", "abstract": " Modern JavaScript applications extensively depend on third-party libraries. Especially for the Node. js platform, vulnerabilities can have severe consequences to the security of applications, resulting in, eg, cross-site scripting and command injection attacks. Existing static analysis tools that have been developed to automatically detect such issues are either too coarse-grained, looking only at package dependency structure while ignoring dataflow, or rely on manually written taint specifications for the most popular libraries to ensure analysis scalability.", "num_citations": "15\n", "authors": ["292"]}
{"title": "Static Analysis with Demand-Driven Value Refinement\n", "abstract": " Static analysis tools for JavaScript must strike a delicate balance, achieving the level of precision required by the most complex features of target programs without incurring prohibitively high analysis time. For example, reasoning about dynamic property accesses sometimes requires precise relational information connecting the object, the dynamically-computed property name, and the property value. Even a minor precision loss at such critical program locations can result in a proliferation of spurious dataflow that renders the analysis results useless.   We present a technique by which a conventional non-relational static dataflow analysis can be combined soundly with a value refinement mechanism to increase precision on demand at critical locations. Crucially, our technique is able to incorporate relational information from the value refinement mechanism into the non-relational domain of the dataflow analysis\u00a0\u2026", "num_citations": "14\n", "authors": ["292"]}
{"title": "Model-Based Testing of Breaking Changes in Node.js Libraries\n", "abstract": " Semantic versioning is widely used by library developers to indicate whether updates contain changes that may break existing clients. Especially for dynamic languages like JavaScript, using semantic versioning correctly is known to be difficult, which often causes program failures and makes client developers reluctant to switch to new library versions.", "num_citations": "13\n", "authors": ["292"]}
{"title": "Practical Initialization Race Detection for JavaScript Web Applications\n", "abstract": " Event races are a common source of subtle errors in JavaScript web applications. Several automated tools for detecting event races have been developed, but experiments show that their accuracy is generally quite low. We present a new approach that focuses on three categories of event race errors that often appear during the initialization phase of web applications: form-input-overwritten errors, late-event-handler-registration errors, and access-before-definition errors. The approach is based on a dynamic analysis that uses a combination of adverse and approximate execution. Among the strengths of the approach are that it does not require browser modifications, expensive model checking, or static analysis.   In an evaluation on 100 widely used websites, our tool InitRacer reports 1085 initialization races, while providing informative explanations of their causes and effects. A manual study of 218 of these reports\u00a0\u2026", "num_citations": "12\n", "authors": ["292"]}
{"title": "Systematic Black-Box Analysis of Collaborative Web Applications\n", "abstract": " Web applications, such as collaborative editors that allow multiple clients to concurrently interact on a shared resource, are difficult to implement correctly. Existing techniques for analyzing concurrent software do not scale to such complex systems or do not consider multiple interacting clients. This paper presents Simian, the first fully automated technique for systematically analyzing multi-client web applications.   Naively exploring all possible interactions between a set of clients of such applications is practically infeasible. Simian obtains scalability for real-world applications by using a two-phase black-box approach. The application code remains unknown to the analysis and is first explored systematically using a single client to infer potential conflicts between client events triggered in a specific context. The second phase synthesizes multi-client interactions targeted at triggering misbehavior that may result from\u00a0\u2026", "num_citations": "10\n", "authors": ["292"]}
{"title": "Refactoring towards the good parts of JavaScript\n", "abstract": " JavaScript is one of the most widely used programming languages of the present day. While its flexibility is treasured by proponents, its lack of language support for encapsulation is an obstacle to writing maintainable programs. We propose refactorings for improving modularity, and discuss challenges arising in their implementation.", "num_citations": "10\n", "authors": ["292"]}
{"title": "Reasonably-Most-General Clients for JavaScript Library Analysis\n", "abstract": " A well-known approach to statically analyze libraries without having access to their client code is to model all possible clients abstractly using a most-general client. In dynamic languages, however, a most-general client would be too general: it may interact with the library in ways that are not intended by the library developer and are not realistic in actual clients, resulting in useless analysis results. In this work, we explore the concept of a reasonably-most-general client, in the context of a new static analysis tool REAGENT that aims to detect errors in TypeScript declaration files for JavaScript libraries. By incorporating different variations of reasonably-most-general clients into an existing static analyzer for JavaScript, we use REAGENT to study how different assumptions of client behavior affect the analysis results. We also show how REAGENT is able to find type errors in real-world TypeScript declaration files, and\u00a0\u2026", "num_citations": "9\n", "authors": ["292"]}
{"title": "Message safety in Dart\n", "abstract": " Unlike traditional static type checking, the type system in the Dart programming language is unsound by design, even for fully annotated programs. The rationale has been that this allows compile-time detection of likely errors and enables code completion in integrated development environments, without being restrictive on programmers.Despite unsoundness, judicious use of type annotations can ensure useful properties of the runtime behavior of Dart programs. We present a formal model of a core of Dart with a focus on its type system, which allows us to elucidate the causes of unsoundness. Our main contribution is a characterization of message-safe programs and a theorem stating that such programs will never encounter \u2018message-not-understood\u2019 errors at runtime. Message safety is less restrictive than traditional type soundness, and we argue that it forms a natural intermediate point between dynamically\u00a0\u2026", "num_citations": "9\n", "authors": ["292"]}
{"title": "Type Safety Analysis for Dart\n", "abstract": " Optional typing is traditionally viewed as a compromise between static and dynamic type checking, where code without type annotations is not checked until runtime. We demonstrate that optional type annotations in Dart programs can be integrated into a flow analysis to provide static type safety guarantees both for annotated and non-annotated parts of the code. We explore two approaches: one that uses type annotations for filtering, and one that uses them as specifications. What makes this particularly challenging for Dart is that its type system is unsound even for fully annotated code. Experimental results show that the technique is remarkably effective, even without context sensitivity: 99.3% of all property lookup operations are reported type safe in a collection of benchmark programs.", "num_citations": "8\n", "authors": ["292"]}
{"title": "The Big Manual for the Java String Analyzer\n", "abstract": " The Java String Analyzer, called JSA, is a tool for performing static analysis of Java programs. Its purpose is to predict the possible values of string expressions in an attempt to validate the correctness of a program. The analysis is based on the technique described by Christensen, M\u00f8ller and Schwartzbach [1]. The JSA home page at http://www. brics. dk/JSA/provides a more detailed reference of the classes than found in this manual, including Javadoc documentation. This manual describes version 2.1 of JSA. It is intended both for developers of tools that build on top of JSA and for those extending JSA with new functionality.As input, the tool takes the. class files to analyze, which we call the application classes. In the application classes, one or more string expressions are selected as hotspots. For each hotspot, the tool produces a finite-state automaton whose language contains all possible Unicode strings that the hotspot might evaluate to at runtime. This is a sound approximation: the output automaton may accept more strings than may actually occur, but not the opposite.", "num_citations": "8\n", "authors": ["292"]}
{"title": "JWIG User Manual\n", "abstract": " The JWIG programming language is a Java-based high-level language for development of interactive Web services. It contains an advanced session model, a flexible mechanism for dynamic construction of XML documents, in particular XHTML, and a powerful API for simplifying use of the HTTP protocol and many other aspects of Web service programming. To support program development, JWIG provides a unique suite of highly specialized program analyses that at compile time verify for a given program that no runtime errors can occur while building documents or receiving form input, and that all documents being shown are valid according to the document type definition for XHTML 1.0 [6]. The main goal of the JWIG project is to simplify development of complex Web services, compared to alternatives, such as, Servlets, JSP, ASP, and PHP. JWIG is a descendant of the< bigwig> research language.", "num_citations": "8\n", "authors": ["292"]}
{"title": "Detecting Locations in JavaScript Programs Affected by Breaking Library Changes\n", "abstract": " JavaScript libraries are widely used and evolve rapidly. When adapting client code to non-backwards compatible changes in libraries, a major challenge is how to locate affected API uses in client code, which is currently a difficult manual task. In this paper we address this challenge by introducing a simple pattern language for expressing API access points and a pattern-matching tool based on lightweight static analysis.   Experimental evaluation on 15 popular npm packages shows that typical breaking changes are easy to express as patterns. Running the static analysis on 265 clients of these packages shows that it is accurate and efficient: it reveals usages of breaking APIs with only 14% false positives and no false negatives, and takes less than a second per client on average. In addition, the analysis is able to report its confidence, which makes it easier to identify the false positives. These results suggest that\u00a0\u2026", "num_citations": "7\n", "authors": ["292"]}
{"title": "Value Partitioning: A Lightweight Approach to Relational Static Analysis for JavaScript\n", "abstract": " In static analysis of modern JavaScript libraries, relational analysis at key locations is critical to provide sound and useful results. Prior work addresses this challenge by the use of various forms of trace partitioning and syntactic patterns, which is fragile and does not scale well, or by incorporating complex backwards analysis. In this paper, we propose a new lightweight variant of trace partitioning named value partitioning that refines individual abstract values instead of entire abstract states. We describe how this approach can effectively capture important relational properties involving dynamic property accesses, functions with free variables, and predicate functions. Furthermore, we extend an existing JavaScript analyzer with value partitioning and demonstrate experimentally that it is a simple, precise, and efficient alternative to the existing approaches for analyzing widely used JavaScript libraries.", "num_citations": "7\n", "authors": ["292"]}
{"title": "A Principled Approach to Selective Context Sensitivity for Pointer Analysis\n", "abstract": " Context sensitivity is an essential technique for ensuring high precision in static analyses. It has been observed that applying context sensitivity partially, only on a select subset of the methods, can improve the balance between analysis precision and speed. However, existing techniques are based on heuristics that do not provide much insight into what characterizes this method subset. In this work, we present a more principled approach for identifying precision-critical methods, based on general patterns of value flows that explain where most of the imprecision arises in context-insensitive pointer analysis. Using this theoretical foundation, we present an efficient algorithm, Zipper, to recognize these flow patterns in a given program and employ context sensitivity accordingly. We also present a variant, Zipper e, that additionally takes into account which methods are disproportionally costly to analyze with context\u00a0\u2026", "num_citations": "7\n", "authors": ["292"]}
{"title": "Analyzing Test Completeness for Dynamic Languages\n", "abstract": " In dynamically typed programming languages, type errors can occur at runtime. Executing the test suites that often accompany programs may provide some confidence about absence of such errors, but generally without any guarantee. We present a program analysis that can check whether a test suite has sufficient coverage to prove a given type-related property, which is particularly challenging for program code with overloading and value dependent types. The analysis achieves a synergy between scalable static analysis and dynamic analysis that goes beyond what can be accomplished by the static analysis alone. Additionally, the analysis provides a new coverage adequacy metric for the completeness of a test suite regarding a family of type-related properties. Based on an implementation for Dart, we demonstrate how such a hybrid static/dynamic program analysis can be used for measuring the quality of a\u00a0\u2026", "num_citations": "7\n", "authors": ["292"]}
{"title": "Technical perspective: WebAssembly: a quiet revolution of the web\n", "abstract": " Technical perspective: WebAssembly: a quiet revolution of the web Page 1 106 COMMUNICATIONS OF THE ACM | DECEMBER 2018 | VOL. 61 | NO. 12 sign. This approach also demonstrates the power of formal techniques and mechanized language semantics: the essential type safety properties now have machine-checked proofs, which guarantees a solid foundation for the running software. WebAssembly is now supported by all the modern browsers, and it has been embraced by a wide range of software companies. The primary goal of WebAssembly is (currently) not to replace JavaScript, but to complement it by making it easier to develop computationally demanding Web applications, such as games, software for audio/video processing, virtual reality systems, and CAD tools, as well as to port desktop applications to the Web. This is just the first step. The initial focus of the WebAssembly team has been on from /\u2026", "num_citations": "6\n", "authors": ["292"]}
{"title": "NodeRacer: Event Race Detection for Node.js Applications\n", "abstract": " The Node.js platform empowers a huge number of software systems programmed with JavaScript. Node.js employs an asynchronous execution model where event handlers are scheduled nondeterministically, and unexpected races between event handlers often cause malfunctions. Existing techniques for detecting such event races require complex modifications of the Node.js internals, or target only certain kinds of races. This paper presents a new approach, called NODERACER, that detects event races in Node.js applications by selectively postponing events, guided by happens-before relations. The technique is implemented entirely with code instrumentation, without modifications of the Node.js system. Our experimental results give evidence that NODERACER finds event race errors with higher probability than a state-of-the-art fuzzer, and that the use of happens-before relations helps avoiding false positives\u00a0\u2026", "num_citations": "5\n", "authors": ["292"]}
{"title": "Type Unsoundness in Practice: An Empirical Study of Dart\n", "abstract": " The type system in the Dart programming language is deliberately designed to be unsound: for a number of reasons, it may happen that a program encounters type errors at runtime although the static type checker reports no warnings. According to the language designers, this ensures a pragmatic balance between the ability to catch bugs statically and allowing a flexible programming style without burdening the programmer with a lot of spurious type warnings. In this work, we attempt to experimentally validate these design choices. Through an empirical evaluation based on open source programs written in Dart totaling 2.4 M LOC, we explore how alternative, more sound choices affect the type warnings being produced. Our results show that some, but not all, sources of unsoundness can be justified. In particular, we find that unsoundness caused by bivariant function subtyping and method overriding does not\u00a0\u2026", "num_citations": "5\n", "authors": ["292"]}
{"title": "JWIG: Yet another framework for maintainable and secure web applications\n", "abstract": " Although numerous frameworks for web application programming have been developed in recent years, writing web applications remains a challenging task. Guided by a collection of classical design principles, we propose yet another framework. It is based on a simple but flexible server-oriented architecture that coherently supports general aspects of modern web applications, including dynamic XML construction, session management, data persistence, caching, and authentication, but it also simplifies programming of server-push communication and integration of XHTML-based applications and XML-based web services. The resulting framework provides a novel foundation for developing maintainable and secure web applications.", "num_citations": "4\n", "authors": ["292"]}
{"title": "Program Verification with Monadic Second-Order Logic & Languages for Web Service Development\n", "abstract": " Domain-specific formal languages are an essential part of computer science, combining theory and practice. Such languages are characterized by being tailor-made for specific application domains and thereby providing expressiveness on high abstraction levels and allowing specialized analysis and verification techniques. This dissertation describes two projects, each exploring one particular instance of such languages: monadic second-order logic and its application to program verification, and programming languages for construction of interactive Web services. Both program verification and Web service development are areas of programming language research that have received increased attention during the last years. We first show how the logic Weak monadic Second-order Logic on Strings and Trees can be implemented efficiently despite an intractable theoretical worst-case complexity. Among several other applications, this implementation forms the basis of a verification technique for imperative programs that perform data-type operations using pointers. To achieve this, the basic logic is extended with layers of language abstractions. Also, a language for expressing data structures and operations along with correctness specifications is designed. Using Hoare logic, programs are split into loop-free fragments which can be encoded in the logic. The technique is described for recursive data types and later extended to the whole class of graph types. As an example application, we verify correctness properties of an implementation of the insert procedure for red-black search trees.We then show how Web service development can benefit\u00a0\u2026", "num_citations": "3\n", "authors": ["292"]}
{"title": "Verifying Programs that Manipulate Pointers (Invited Talk)\n", "abstract": " Verifying Programs that Manipulate Pointers Page 1 Anders M\u00f8ller University of Aarhus http://www.brics.dk/~amoeller/talks/infinity.pdf Verifying Programs that Manipulate Pointers Page 2 Verifying Programs that Manipulate Pointers 2 Analyzing the Heap Heap: an untidy pile or mass of things [Cambridge Dictionary] \u2022 This is how most program analyses view the heap \u2022 because pointers are notoriously difficult to reason about \u2022 but they are an important part of most programming languages\u2026 Page 3 Verifying Programs that Manipulate Pointers 3 Example: Reversing a Linked List struct Node { struct Node *n; int data; } Node *reverse(Node *x) { Node *y, *t; y = NULL; while (x != NULL) { t = y; y = x; x = x->n; y->n = t; } return y; } Assume that the input is an acyclic list, argue that \u2022 there are no null pointer dereferences \u2022 no elements are lost \u2022 the output is an acyclic list \u2022 the output is the reverse of the input \u2022 no other parts of 4 (\u2026", "num_citations": "2\n", "authors": ["292"]}
{"title": "Modular Call Graph Construction for Security Scanning of Node.js Applications\n", "abstract": " Most of the code in typical Node. js applications comes from third-party libraries that consist of a large number of interdependent modules. Because of the dynamic features of JavaScript, it is difficult to obtain detailed information about the module dependencies, which is vital for reasoning about the potential consequences of security vulnerabilities in libraries, and for many other software development tasks. The underlying challenge is how to construct precise call graphs that capture the connectivity between functions in the modules.", "num_citations": "1\n", "authors": ["292"]}
{"title": "Semantic Patches for Adaptation of JavaScript Programs to Evolving Libraries\n", "abstract": " JavaScript libraries are often updated and sometimes breaking changes are introduced in the process, resulting in the client developers having to adapt their code to the changes. In addition to locating the affected parts of their code, the client developers must apply suitable patches, which is a tedious, error-prone, and entirely manual process.To reduce the manual effort, we present JSFIX. Given a collection of semantic patches, which are formalized descriptions of the breaking changes, the tool detects the locations affected by breaking changes and then transforms those parts of the code to become compatible with the new library version. JSFIX relies on an existing static analysis to approximate the set of affected locations, and an interactive process where the user answers questions about the client code to filter away false positives.An evaluation involving 12 popular JavaScript libraries and 203 clients shows\u00a0\u2026", "num_citations": "1\n", "authors": ["292"]}
{"title": "Eliminating Abstraction Overhead of Java Stream Pipelines using Ahead-of-Time Program Optimization\n", "abstract": " Java 8 introduced streams that allow developers to work with collections of data using functional-style operations. Streams are often used in pipelines of operations for processing the data elements, which leads to concise and elegant program code. However, the declarative data processing style comes at a cost. Compared to processing the data with traditional imperative language mechanisms, constructing stream pipelines requires extra heap objects and virtual method calls, which often results in significant run-time overheads.   In this work we investigate how to mitigate these overheads to enable processing data in the declarative style without sacrificing performance. We argue that ahead-of-time bytecode-to-bytecode transformation is a suitable approach to optimization of stream pipelines, and we present a static analysis that is designed to guide such transformations. Experimental results show a significant\u00a0\u2026", "num_citations": "1\n", "authors": ["292"]}
{"title": "Managing Gradual Typing with Message-Safety in Dart\n", "abstract": " This paper establishes a notion of message-safe programs as a natural intermediate point between dynamically typed and statically typed Dart programs. Unlike traditional static type checking, the type system in the Dart programming language is unsound by design. The rationale has been that this allows compile-time detection of likely errors and enables code completion in integrated development environments, without being restrictive on programmers. We show that, despite unsoundness, judicious use of type annotations can ensure useful properties of the runtime behavior of Dart programs. This supports evolution from a dynamically typed program to a strictly statically typed form.We present a formal model of Dart that elucidates how a core of the language and its standard type system works. This allows us to characterize message-safe programs and present a theorem stating that such programs will never cause \u2018message not understood\u2019errors, which is generally not guaranteed for Dart programs that pass the standard type checker. The formal model has been expressed in Coq.", "num_citations": "1\n", "authors": ["292"]}
{"title": "Static analysis for event-based XML processing\n", "abstract": " Event-based processing of XML data\u2013as exemplified by the popular SAX framework\u2013is a powerful alternative to using W3C\u2019s DOM or similar tree-based APIs. The event-based approach is particularly superior when processing large XML documents in a streaming fashion with minimal memory consumption. This paper discusses challenges and presents some considerations for creating program analyses for SAX applications. In particular, we consider the problem of statically guaranteeing that a given SAX application always produces only well-formed and valid XML output.", "num_citations": "1\n", "authors": ["292"]}