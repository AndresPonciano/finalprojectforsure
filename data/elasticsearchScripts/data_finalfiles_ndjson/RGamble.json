{"title": "The impact of component architectures on interoperability\n", "abstract": " Component interoperability has become an important concern as companies migrate legacy systems, integrate COTS products, and assemble modules from disparate sources into a single application. While middleware is available for this purpose, it often does not form a complete bridge between components and may be inflexible as the application evolves. What is needed is the explicit design information that will forecast a more accurate, evolvable, and less costly integration solution implementation.Emerging research has shown that interoperability problems can be traced to the software architecture of the components and integrated application. Furthermore, the solutions generated for these problems are guided by an implicit understanding of software architecture. Current technology does not fully identify what must be made explicit about software architecture to aid in comparison of the architectures and\u00a0\u2026", "num_citations": "74\n", "authors": ["1404"]}
{"title": "Towards a taxonomy of architecture integration strategies\n", "abstract": " By detecting interoperability problems at the architectural style level, properties of an integrated system can be defined early in the design process. There are many integration strategies, but their definitions do not provide the developer with a foundation to select a strategy for its appropriateness in implementing a correct solution. In this paper, we discuss our preliminary findings from architectural style integration analysis. We form a partial integration taxonomy that shows the relationship between existing integration strategies and the integration elements defined at the architectural level. 1.1 Keywords", "num_citations": "73\n", "authors": ["1404"]}
{"title": "Secagreement: Advancing security risk calculations in cloud services\n", "abstract": " By choosing to use cloud services, organizations seek to reduce costs and maximize efficiency. For mission critical systems that must satisfy security constraints, this push to the cloud introduces risks associated with cloud service providers not implementing organizationally selected security controls or policies. As internal system details are abstracted away as part of the cloud architecture, the organization must rely on contractual obligations embedded in service level agreements (SLAs) to assess service offerings. Current SLAs focus on quality of service metrics and lack the semantics needed to express security constraints that could be used to measure risk. We create a framework, called SecAgreement (SecAg), that extends the current SLA negotiation standard, WS-Agreement, to allow security metrics to be expressed on service description terms and service level objectives. The framework enables cloud\u00a0\u2026", "num_citations": "57\n", "authors": ["1404"]}
{"title": "Introducing replaceability into web service composition\n", "abstract": " By discovering and reusing relevant web services, an organization can select and compose those services that most closely meet its business and Quality of Service (QoS) needs. As the number of available web services increases, selecting the best fit services for a given task becomes more challenging. QoS attributes play a significant role in the selection process by directing service composition constraints to a workflow plan that has the best QoS values. Two major problems arise at runtime when undesirable events necessitate the need to reselect services and replan the service bindings. First, if the reselection process consumes additional time, it can impact a temporal QoS constraint. Second, the newly generated composition might not comply with other QoS constraints imposed on the plan. This paper proposes an approach to composing web services that both performs reselection and avoids the violation of\u00a0\u2026", "num_citations": "49\n", "authors": ["1404"]}
{"title": "Methodologies for the development of knowledge-based systems, 1982\u20132002\n", "abstract": " Knowledge-based systems have often been criticised for the limited theoretical base upon which they are constructed. This view asserts that systems are often developed in an ad hoc, individual way that leads to unmaintainable, unreliable and non-rigorous systems. The last decade, however, has seen an increased effort to produce methodologies to counter this view as well as continued research into validation and verification techniques. This paper presents a brief discussion of some of the important research in knowledge-based system life cycles and development methods. Methodologies are considered and are discussed in light of two sets of quality assurance criteria.", "num_citations": "42\n", "authors": ["1404"]}
{"title": "Formal derivation of rule-based programs\n", "abstract": " It is shown that a combination of specification and program refinement may be applied to deriving efficient concurrent rule-based programs. Specification refinement is used to generate an initial rule-based program that is refined into a program which is highly concurrent and efficient. This program derivation strategy is divided into two major tasks. The first task relies on specification refinement. Techniques similar to those employed in the derivation of UNITY programs are used to produce a correct rule-based program having a static knowledge base. The second task involves program refinement and is specific to the development of concurrent rule-based programs. It relies heavily on the availability of a computational model, such as Swarm, that has the ability to dynamically restructure the knowledge base. The ways in which a Swarm program can be translated to OPS5 specifically, given some restrictions, while\u00a0\u2026", "num_citations": "42\n", "authors": ["1404"]}
{"title": "Monoliths to mashups: Increasing opportunistic assets\n", "abstract": " Opportunities are available resources that yield desired results. Their suitability depends on who seizes the opportunity and the context for its use. Opportunistic development relies on the availability of reusable software components to produce hybrid applications that opportunistically join such components to meet immediate functional or content needs. Availability and connectivity are key qualities of an opportunity. Situational assessment determines when the best available, most deployable opportunities exist within time and resource constraints.", "num_citations": "33\n", "authors": ["1404"]}
{"title": "Assessing individual performance in Agile undergraduate software engineering teams\n", "abstract": " The Agile Software Development (ASD) process is at the forefront of rapid product development driven by changing customer requirements and a trusted, self-organizing development team. Scrum has become a viable model of ASD focusing on determining immediate deliverables and structuring short timelines, called Sprints, for designing, implementing, and providing them for testing by the customer. While these practices are being adopted by organizations, there is significant difficulty in scaling them to the classroom. Once in place, it is a complex task to evaluate individual student performance based solely on the product outcome and Sprint grade. Thus, there is limited opportunity to catch performance problems that may lead to missing deliverable deadlines or decreasing team trust. In this paper, we impose ASD using Scrum on a senior software projects course in Computer Science. Using a collaborative\u00a0\u2026", "num_citations": "32\n", "authors": ["1404"]}
{"title": "Information flow control for stream processing in clouds\n", "abstract": " In the near future, clouds will provide situational monitoring services using streaming data. Examples of such services include health monitoring, stock market monitoring, shopping cart monitoring, and emergency control and threat management. Offering such services require securely processing data streams generated by multiple, possibly competing and/or complementing, organizations. Processing of data streams also should not cause any overt or covert leakage of information across organizations. We propose an information flow control model adapted from the Chinese Wall policy that can be used to protect against sensitive data disclosure. We propose architectures that are suitable for securely and efficiently processing streaming information belonging to different organizations. We discuss how performance can be further improved by sharing the processing of multiple queries. We demonstrate the feasibility\u00a0\u2026", "num_citations": "32\n", "authors": ["1404"]}
{"title": "CyberPhishing: a game-based platform for phishing awareness testing\n", "abstract": " Phishing attacks sap billions of dollars annually from unsuspecting individuals while compromising individual privacy. Companies and privacy advocates seek ways to better educate the populace against such attacks. Current approaches examining phishing include test-based techniques that ask subjects to classify content as phishing or not and inthe- wild techniques that directly observe subject behavior through distribution of faked phishing attacks. Both approaches have issues. Test-based techniques produce less reliable data since subjects may adjust their behavior with the expectation of seeing phishing stimuli, while in-the-wild studies can put subjects at risk through lack of consent or exposure of data. This paper examines a third approach that seeks to incorporate game-based learning techniques to combine the realism of in-thewild approaches with the training features of testing approaches. We propose\u00a0\u2026", "num_citations": "31\n", "authors": ["1404"]}
{"title": "Practical DoS attacks on embedded networks in commercial vehicles\n", "abstract": " The Controller Area Network (CAN) protocol has become the primary choice for in-vehicle communications for passenger cars and commercial vehicles. However, it is possible for malicious adversaries to cause major damage by exploiting flaws in the CAN protocol design or implementation. Researchers have shown that an attacker can remotely inject malicious messages into the CAN network in order to disrupt or alter normal vehicle behavior. Some of these attacks can lead to catastrophic consequences for both the vehicle and the driver. Although there are several defense techniques against CAN based attacks, attack surfaces like physically and remotely controllable Electronic Control Units (ECUs) can be used to launch attacks on protocols running on top of the CAN network, such as the SAE J1939 protocol. Commercial vehicles adhere to the SAE J1939 standards that make use of the CAN protocol\u00a0\u2026", "num_citations": "30\n", "authors": ["1404"]}
{"title": "A tiered strategy for auditing in the cloud\n", "abstract": " In this paper, we outline a tiered approach to auditing information in the cloud. The approach provides perspectives on auditable events that may include compositions of independently formed audit trails. Filtering and reasoning over the audit trails can manifest potential security vulnerabilities and performance attributes as desired by stakeholders.", "num_citations": "30\n", "authors": ["1404"]}
{"title": "A notation for problematic architecture interactions\n", "abstract": " The progression of component-based software engineering (CBSE) is essential to the rapid, cost-effective development of complex software systems. Given the choice of well-tested components, CBSE affords reusability and increases reliability. However, applications developed according to this practice can often suffer from difficult maintenance and control, problems that stem from improper or inadequate integrate solutions. Avoiding such unfortunate results requires knowledge of what causes the interoperability problems in the first place. The time for this assessment is during application design. In this paper, we define problematic architecture interactions using a simple notation with extendable properties. Furthermore, we delineate a multi-phase process for pre-integration analysis that relies on this notation. Through this effort, potential problematic architecture interactions can be illuminated and used to form\u00a0\u2026", "num_citations": "29\n", "authors": ["1404"]}
{"title": "Understanding the architectural characteristics behind middleware choices\n", "abstract": " Integrating heterogeneous components to form a distributed system can manifest difficult interoperability problems among the components. Contributing to this problem is a lack of understanding of the underlying software architecture of the components, and, possibly, the distributed architecture in which they participate. Middleware to resolve these problems may be difficult to choose and implement, often resulting in integration solutions that are not complete or evolvable. This paper discusses software architectural characteristics that underlie choices for integration solutions to interoperability problems in distributed component architectures. The 4+ 1 view model of architecture is used to represent the characteristics at their appropriate level of abstraction. We use two distinct case studies to illustrate how comparisons among characteristic values of participating components in the integrated system could have predicted interoperability problems.", "num_citations": "28\n", "authors": ["1404"]}
{"title": "Secu Wear: An open source, multi-component hardware/software platform for exploring wearable security\n", "abstract": " Wearables are the next big development in the mobile internet of things. Operating in a body area network around a smartphone user they serve a variety of commercial, medical, and personal uses. Whether used for fitness tracking, mobile health monitoring, or as remote controllers, wearable devices can include sensors that collect a variety of data and actuators that provide hap tic feedback and unique user interfaces for controlling software and hardware. Wearables are typically wireless and use Bluetooth LE (low energy) to transmit data to a waiting smartphone app. Frequently, apps forward this data onward to online web servers for tracking. Security and privacy concerns abound when wearables capture sensitive data or provide critical functionality. This paper develops a platform, called SecuWear, for conducting wearable security research, collecting data, and identifying vulnerabilities in hardware and\u00a0\u2026", "num_citations": "24\n", "authors": ["1404"]}
{"title": "DDoS attacks in service clouds\n", "abstract": " The scalability and dynamic configuration of service clouds can be susceptible to Distributed Denial of Service (DDoS) attacks. The attack on web services causes a performance decrease in the cloud applications or can shut them down. Additionally, due to the high distribution of the service cloud components, finding the original attacking service becomes a far more complex task. This paper advocates a DDoS attack detection approach for service clouds and develops efficient algorithms to resolve the originating service for the attack. The detection approach is composed of four levels such that each level detects symptoms of DDoS attacks from its local data. The detection results of all levels are corroborated to confirm the victim and attacking services. We evaluate our proposed solution by using a random dataset. The results indicate that it is a promising solution to mitigate the DDoS attack in the service cloud.", "num_citations": "24\n", "authors": ["1404"]}
{"title": "Risk propagation of security SLAs in the cloud\n", "abstract": " For organizations with mission critical systems, moving data or functionality to the cloud introduces a risk of additional exposed vulnerabilities associated with cloud service providers not implementing organizationally selected security controls. When internal system details are abstracted away as part of the cloud architecture, the organization must rely on contractual obligations embedded in service level agreements (SLAs) to assess service offerings for security risk. Whenever an SLA is formed, the level of risk incurred is based on how well the offered service terms meet the organizational security demands. In the cloud, additional SLAs between third party cloud service providers are formed to federate cloud resources, effectively distributing organizational risk among the various providers involved in the negotiated federations or service compositions. At runtime, whenever a cloud or service violates its SLA with\u00a0\u2026", "num_citations": "21\n", "authors": ["1404"]}
{"title": "Identifying evolvability for integration\n", "abstract": " The seamless integration of commercial-off-the-shelf (COTS) components offers many benefits associated with reuse. Even with successful composite applications, unexpected interoperability conflicts can arise when COTS products are upgraded, new components are needed, and the application requirements change. Recent approaches to integration follow pattern-based design principles to construct integration architecture for the composite application. This integration architecture provides a design perspective for addressing the problematic interactions among components within the application environment. However, little attention has been paid to the evolvability of these architectures and their embedded functionality. In this paper, we discuss the need for design traceability based on the history of interoperability conflicts and resolution decisions that comprise the integration architecture. Additionally\u00a0\u2026", "num_citations": "20\n", "authors": ["1404"]}
{"title": "Building a compliance vocabulary to embed security controls in cloud slas\n", "abstract": " Mission critical information systems must be certified against a set of security controls to mitigate potential security incidents. Cloud service providers must in turn employ adequate security measures that conform to security controls expected by the organizational information systems they host. Since service implementation details are abstracted away by the cloud, organizations can only rely on service level agreements (SLAs) to assess the compliance of cloud security properties and processes. Various representation schema allow SLAs to embed service security terms, but are disconnected from documents regulating security controls. This paper demonstrates an extensible solution for building a compliance vocabulary that associates SLA terms with security controls. The terms allow services to express which security controls they comply with and enable at-a-glance comparison of security service offerings so\u00a0\u2026", "num_citations": "19\n", "authors": ["1404"]}
{"title": "Patterns of conflict among software components\n", "abstract": " Integrating a system of disparate components to form a single application is still a daunting, high risk task, especially for components with heterogeneous communication expectations. It benefits integration to know explicitly the interoperability conflicts that can arise based on the current application design and the components being considered. However, there is no consistent representation of identified conflicts that also defines strategies to resolve them. Instead, developers use prior experience which may have consequential gaps. In this paper, we formulate a common representation for six major interoperability conflicts that arise through discrepancies or direct mismatches among architectural properties of interacting components. We use an Extender-Translator-Controller (ETC) classification scheme to describe the conflict resolution strategies. Detailing these associations as patterns provides insight into\u00a0\u2026", "num_citations": "19\n", "authors": ["1404"]}
{"title": "System development using the integrating component architectures process\n", "abstract": " The purpose of this paper is to present a software development process that considers interoperability problems associated with integrating in-house systems with COTS products. The Integrating Component Architectures Process (ICAP) outlines a process that includes predicting interoperability conflicts among independent components, COTS included, based on architectural differences. Previously defined integration elements, along with a taxonomy of integration solutions based on these elements, direct movement through the process to an integration architecture for implementation. The ICAP predominantly falls into the elaboration phase of the Rational Unified Process, providing greater detail of the issues surrounding interoperability. We believe that such a development process will allow easier and more cost-effective maintenance and migration of legacy systems, as well as the use of COTS products in system development.", "num_citations": "19\n", "authors": ["1404"]}
{"title": "Developing a platform to evaluate and assess the security of wearable devices\n", "abstract": " Operating in a body area network around a smartphone user, wearables serve a variety of commercial, medical and personal uses. Depending on a certain smartphone application, a wearable can capture sensitive data about the user and provide critical, possibly life-or-death, functionality. When using wearables, security problems might occur on hardware/software of wearables, connected phone apps or web services devices, or Bluetooth channels used for communication. This paper develops an open source platform called SecuWear for identifying vulnerabilities in these areas and facilitating wearable security research to mitigate them. SecuWear supports the creation, evaluation, and analysis of security vulnerability tests on actual hardwares. Extending earlier results, this paper includes an empirical evaluation that demonstrates proof of concept attacks on commercial wearable devices and shows how\u00a0\u2026", "num_citations": "17\n", "authors": ["1404"]}
{"title": "Formal Verification of Pure Production System Programs.\n", "abstract": " Reliability, defined as the guarantee that a program satisfies its specifications, is an important aspect of many applications for which rule-based programs are suited. Executing rule-based programs on a series of test cases does not guarantee correct behavior in all possible test cases. To show a program is reliable, it is desirable to construct formal specifications for the program and to prove that it obeys those specifications. This paper presents an assertional approach to the verification of a class of rule-based programs characterized by the absence of conflict resolution. The proof logic needed for verification is already in use by researchers in concurrent programming. The approach involves expressing the program in a language called Swarm, and its specifications as assertions over the Swarm program. Among models that employ rule-based notation, Swarm is the first to have an axiomatic proof logic. A brief review of Swarm and its proof logic is given, along with an illustration of the formal verification method used on a simple rule-based program.", "num_citations": "17\n", "authors": ["1404"]}
{"title": "Application of the heuristic-systematic model to computer code trustworthiness: The influence of reputation and transparency\n", "abstract": " Computer programs (code) are integral to the functions of current society. Yet, little is known about why programmers trust code they did not create. The current paper applied the heuristic-systematic model (HSM) of information processing to perceptions of code trust for reuse. The studies explored transparency (readability and organization) and reputation (source) as factors that influenced trust perceptions and time spent reviewing code using professional programmers. Source and readability manipulations led to higher trustworthiness assessments in the first study. Organization had nonlinear effects on trustworthiness. A three-way interaction including time was also found. The second online study largely replicated the first study\u2019s main and interaction effects for trustworthiness, but the main effects on time were not significant. Our findings suggest the relationships of transparency on trustworthiness are not as\u00a0\u2026", "num_citations": "16\n", "authors": ["1404"]}
{"title": "Toward formalizing service integration glue code\n", "abstract": " Application integrations employ interacting software components and services. Components - distributed, black-box, processing elements - may have distinct communication styles. Services can be used to augment the interaction of components to deliver on a common task. Unfortunately, there exist scenarios where components and services do not interact congenially. At issue is the service design that, even with standards in place, may ignore important aspects of component communication expectations. Thus, a complete integration is not achieved without implementing ad hoc glue code external to the design strategy. Introducing glue code, without design traceability, limits control, dynamism, and upgrades to components and services. We specify templates for framing glue code as integration completion functions (ICFs). Uniform designs are given using multiple modeling techniques to foster compilation of an\u00a0\u2026", "num_citations": "16\n", "authors": ["1404"]}
{"title": "A patterned approach for linking knowledge-based systems to external resources\n", "abstract": " Knowledge-based systems (KBSs) have been developed and used in industry and government as assistance systems, voting partner systems, and embedded applications. As web-based systems change the face of software implementations, these closed, internal KBSs need to be integrated into multicomponent applications that provide updated and extensible services. Therefore, KBSs must be adapted to an environment in which data and control are exchanged with external processes and resources; complementing other participating systems or using them to refine its own results. This integration can be a daunting task. If improperly done, it can result in an inefficient and unmanageable composite application. One approach to simplifying this task is the use of architectural patterns for integration. These patterns are assembled from functional entities that resolve component interoperability conflicts. In this paper\u00a0\u2026", "num_citations": "16\n", "authors": ["1404"]}
{"title": "Entailment for specification refinement\n", "abstract": " Specification refinement is part of formal program derivation, a method by which software is directly constructed from a provably correct specification. Because program derivation is an intensive manual exercise used for critical software systems, an automated approach would allow it to be viable for many other types of software systems. The goal of this research is to determine if genetic programming (GP) can be used to automate the specification refinement process. The initial steps toward this goal are to show that a well {known proof logic for program derivation can be encoded such that a GP {based system can infer sentences in the logic for proof of a particular sentence. The results are promising and indicate that GP can be useful in aiding program derivation.", "num_citations": "16\n", "authors": ["1404"]}
{"title": "Applying formal verification methods to rule-based programs\n", "abstract": " Applying formal verification methods to rule-based programs | International Journal of Expert Systems ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search International Journal of Expert Systems Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsInternational Journal of Expert SystemsVol. , No. Applying formal verification methods to rule-based programs article Applying formal verification methods to rule-based programs Share on Authors: Rose F Gamble profile image Rose F. Gamble View Profile , Gruia-Catalin Roman profile image Gruia-Catalin Roman View Profile , Harold Conrad Cunningham profile image H. Conrad Cunningham View Profile \u2026", "num_citations": "16\n", "authors": ["1404"]}
{"title": "Classifying and detecting anomalies in hybrid knowledge-based systems1\n", "abstract": " The increasing need for hybrid Knowledge-Based Systems (KBS) that accommodate more complex applications has led to the need for new verification concerns that are more specific to the hybrid representation using objects and rule-based inference. Traditionally, verification of expert systems has focused solely on rule-based inference systems. Hybrid KBSs present additional verification problems not found in traditional rule-based systems. This paper is an investigation into the anomalies that may be present in a hybrid representation that warrant detection for the verification of the KBS. Many anomalies are due to the interaction of the component parts of the hybrid KBS. For example, subsumption anomalies arise due to an interaction between inheritance of objects and rule-based inference. In this paper, we extend the context of subsumption anomalies and introduce additional types of anomalies that may be\u00a0\u2026", "num_citations": "15\n", "authors": ["1404"]}
{"title": "Using meta-knowledge within a multilevel framework for KBS development\n", "abstract": " This paper describes a multilevel development life cycle of representation refinement for knowledge-based systems that incorporates meta-knowledge at each level. The methodology uses formal techniques in the specification of the domain knowledge, the cognitive aspects and the representation. The methodology provides the knowledge engineer with a dynamic perspective of the system which can be used in conjunction with the static aspects found in the representation abstractions. To provide perspective, the paper details the refinement of one of the levels called the intermediate level, in which an implementation-independent representation is created by the use of a knowledge filter.", "num_citations": "15\n", "authors": ["1404"]}
{"title": "Automating preference and change in workflows\n", "abstract": " Workflow languages can compose and sequence Web service invocations to achieve meaningful task results. Updating workflows when services and task goals change is difficult, and automating the change at runtime requires workflow goals and service preferences to be stated, organized, and controlled. Changing the preferences should be immediately reflected in executing workflows. While there are many applications that could benefit from this dynamic reconfiguration, managing such preferences can be daunting. We introduce NeWT, a Next-generation Workflow Toolkit that combines workflow editing with utilities for introducing and managing change. NeWT allows the novel input of goal preferences to use workflows that accomplish the same task but in different ways to be interchangeable according to the current goal. It accepts that multiple, competing Web services will be available and should be\u00a0\u2026", "num_citations": "14\n", "authors": ["1404"]}
{"title": "Classifying interoperability conflicts\n", "abstract": " A common path for application development is to pick the COTS or legacy products, choose a middleware product, and determine what additional functionality is needed to make it all work. While this may seem the most expedient and least costly way to develop an integrated application, unexpected interoperability conflicts can surface after implementation, deployment and/or evolution of any of the participating components. An interoperability conflict is any factor inhibiting communication of control or data among components. Current research has shown that interoperability conflicts can be traced to the software architecture of the components and integrated application, making this level of abstraction a suitable domain for conflict description. In this paper, we describe and substantiate a set of architecture-based conflicts that embody the predominant interoperability problems found in software integrations.", "num_citations": "14\n", "authors": ["1404"]}
{"title": "The opportunity for formal models of integration\n", "abstract": " Major industrial and governmental efforts in information sharing and integration require building or migrating applications using heterogeneous component systems. This style of software development enjoys the benefits of reusability, adaptability, and evolvability. However, as with most component integration attempts, interoperability problems arise. Although there are several strategies that currently exist for the integration of systems, many suffer from informality and are tightly coupled to particular domains and products. More importantly, interoperability problem detection and integration solution design show promise as areas where formal methods can be applied. In this paper, we discuss why the opportunity to use formal models of integration should not be overlooked. We present several criteria for formal modeling to be useful and usable for information and process integration. We illustrate our approach to integration modeling and discuss the expansion efforts needed to satisfy the criteria discussed.", "num_citations": "14\n", "authors": ["1404"]}
{"title": "How system architectures impede interoperability\n", "abstract": " Interoperability problems arise when complex software systems are constructed by integrating distinct, and often heterogeneous, components. By performing interoperability analysis on the software architecture design of the system and its components, potential incompatibilities can be anticipated early in the design process. In this paper, we focus on an application\u2019s structural requirements as reflected in values of particular architectural characteristics, describing how they can be incompatible with individual component system properties. Keywords", "num_citations": "14\n", "authors": ["1404"]}
{"title": "Rule-based systems formalized within a software architectural style\n", "abstract": " This article considers the utilization of architectural styles in the formal design of knowledge-based systems. The formal model of a style is an approach to systems modeling that allows software developers to understand and prove properties about the system design in terms of its components, connectors, configurations, and constraints. This allows commonality of design to be easily understood and captured, leading to a better understanding of the role that an architectural abstraction would have in another complex system, embedded context, or system integration. In this article, a formal rule-based architectural style is presented in detail using the Z notation. The benefits of depicting the rule-based system as an architectural style include reusability, understandability, and the allowance for formal software analysis and integration techniques. The ability to define the rule-based architectural style in this way\u00a0\u2026", "num_citations": "14\n", "authors": ["1404"]}
{"title": "SEREBRO: facilitating student project team collaboration\n", "abstract": " In this demonstration, we show SEREBRO, a lightweight courseware developed for student team collaboration in a software engineering class. SEREBRO couples an idea forum with software project management tools to maintain cohesive interaction between team discussion and resulting work products, such as tasking, documentation, and version control. SEREBRO has been used consecutively for two years of software engineering classes. Student input and experiments on student use in these classes has directed SERBRO to its current functionality.", "num_citations": "13\n", "authors": ["1404"]}
{"title": "Semantic hierarchies for extracting, modeling, and connecting compliance requirements in information security control standards\n", "abstract": " Companies and government organizations are increasingly compelled, if not required by law, to ensure that their information systems will comply with various federal and industry regulatory standards, such as the NIST Special Publication on Security Controls for Federal Information Systems (NIST SP-800-53), or the Common Criteria (ISO 15408-2). Such organizations operate business or mission critical systems where a lack of or lapse in security protections translates to serious confidentiality, integrity, and availability risks that, if exploited, could result in information disclosure, loss of money, or, at worst, loss of life. To mitigate these risks and ensure that their information systems meet regulatory standards, organizations must be able to (a) contextualize regulatory documents in a way that extracts the relevant technical implications for their systems, (b) formally represent their systems and demonstrate that\u00a0\u2026", "num_citations": "12\n", "authors": ["1404"]}
{"title": "Towards a Cyber Assurance Testbed for Heavy Vehicle Electronic Controls\n", "abstract": " Cyber assurance of heavy trucks is a major concern with new designs as well as with supporting legacy systems. Many cyber security experts and analysts are used to working with traditional information technology (IT) networks and are familiar with a set of technologies that may not be directly useful in the commercial vehicle sector. To help connect security researchers to heavy trucks, a remotely accessible testbed has been prototyped for experimentation with security methodologies and techniques to evaluate and improve on existing technologies, as well as developing domain-specific technologies. The testbed relies on embedded Linux-based node controllers that can simulate the sensor inputs to various heavy vehicle electronic control units (ECUs). The node controller also monitors and affects the flow of network information between the ECUs and the vehicle communications backbone. For example, a node controller acts as a clone that generates analog wheel speed sensor data while at the same time monitors or controls the network traffic on the J1939 and J1708 networks. The architecture and functions of the node controllers are detailed. Sample interaction with the testbed is illustrated, along with a discussion of the challenges of running remote experiments. Incorporating high fidelity hardware in the testbed enables security researchers to advance the state of the art in hardening heavy vehicle ECUs against cyber-attacks. How the testbed can be used for security research is presented along with an example of its use in evaluating seed/key exchange strength and in intrusion detection systems (IDSs).", "num_citations": "12\n", "authors": ["1404"]}
{"title": "Predicting individual performance in student project teams\n", "abstract": " Due to the critical role of communication in project teams, capturing and analyzing developer design notes and conversations for use as performance predictors is becoming increasing important as software development processes become more asynchronous. Current prediction methods require human Subject Matter Experts (SME) to laboriously examine and rank user content along various categories such as participation and the information they express. SEREBRO is an integrated courseware tool that captures social and development artifacts automatically and provides real time rewards, in the form of badges and titles, indicating a user's progress towards predefined goals using a variety of automated assessment measures. The tool allows for instructor visualization, involvement, and feedback in the ongoing projects and provides avenues for the instructor to adapt or adjust project scope or individual role\u00a0\u2026", "num_citations": "12\n", "authors": ["1404"]}
{"title": "Blackboard systems formalized within a software architectural style\n", "abstract": " This paper presents a formal model of the blackboard system within the context of a software architectural style, i.e., components, connectors, and configurations. We present a refinement of a generic controller component to perform a choice among the knowledge source that can execute against the blackboard. In addition, we demonstrate the benefits of depicting the blackboard system as an architectural style, such as reusability, understandability, and the ability to use formal software analysis and integration techniques. We discuss our formal model of the blackboard software architecture in comparison with previous formalizations of blackboard systems to show that the generic architectural style can be specialized to meet the criteria of serial, parallel, and distributed blackboard systems.", "num_citations": "12\n", "authors": ["1404"]}
{"title": "A methodology to incorporateformal methods in hybrid KBS verification\n", "abstract": " Thereis an increasing need for hybrid knowledge based systems(KBS) that accommodate more complex AI applications. Weconsider a hybrid KBS to be one that combinesobject-oriented, frame-based, and rule-based paradigms.Building hybrid KBSs requires formal methods forverification due to the possible interactions betweenobjects, inheritance, methods, and rules. The necessarycomplex verification for this type of KBS can be achievedwith formal specification and derivation because theyprovide unambiguous specifications and proof mechanisms.However, the complete use of formal methods is often tootime consuming and hence, impractical. Therefore, it isnecessary to find a balance between using formal methods andtraditional KBS techniques for development and verification.This paper presents a practical methodology to constructverifiable hybrid KBSs from formal specifications thatfollows traditional\u00a0\u2026", "num_citations": "12\n", "authors": ["1404"]}
{"title": "Self-Adaptation Strategies to Maintain Security Assurance Cases\n", "abstract": " Information system security certification involves guaranteeing that mechanisms are deployed to comply with selected security controls, such as those in the NIST SP800-53, at acceptable levels of confidence and risk. When a system can self-adapt at runtime, it may alter its functional behavior to address a defect or anomaly. This functional change can impact associated security controls, potentially making the adapted system vulnerable to security threats. Performing security control assurance adaptation along with functional adaptation would allow both compliance confidence and risk analysis to accompany functional adaptation analysis. The need for this dual assessment implies security control compliance should be expressed such that an adaptation can be reflected as part of its compliance status. In this paper, we represent security controls and their deployed mechanisms in terms of security assurance cases\u00a0\u2026", "num_citations": "11\n", "authors": ["1404"]}
{"title": "Developing a security meta-language framework\n", "abstract": " Service-oriented architectures (SOAs) with web services have become commonplace in business and government application development. One reason that web services should facilitate application implementation and deployment is their use of standards to provide clear descriptions of service expectations. However, when reliance on these standards is mandatory, such as in the case of guaranteeing the SOA meets specific security and information assurance constraints, design and development difficulties arise due to the magnitude of standards available, their cross referencing, and dependencies. This paper introduces a framework to provide the foundation for a security meta-language (SML) that models the security relevant portions of the standards for their consistent, comprehensive, and correct application. The goal of the framework is for security constraints and the SOA application domain to filter the\u00a0\u2026", "num_citations": "11\n", "authors": ["1404"]}
{"title": "Reconfiguring workflows of web services\n", "abstract": " Workflow reconfiguration traditionally involves modifying workflow specifications to adapt to changing architectural conditions. Causes include the introduction of new services or the alteration of goals. Dynamic reconfiguration is currently achieved in workflow specifications employing Web services using techniques that modify endpoint bindings and control structures. Abstract specification of service endpoints delay the point at which the endpoint for a Web service is bound, and modifications to control structures can allow for a variety of complex workflows to be specified. These approaches work with limited changes. This paper defines an improved process of dynamically reconfiguring Web service workflows. Our double loop approach utilizes companion meta-data specifications and reconfiguration plans that are associated with workflow specifications. The approach maps external change requests to workflow\u00a0\u2026", "num_citations": "11\n", "authors": ["1404"]}
{"title": "Assessing the Risk of an Adaptation using Prior Compliance Verification\n", "abstract": " Autonomous systems must respond to large amounts of streaming information. They also must comply with critical properties to maintain behavior guarantees. Compliance is especially important when a system selfadapts to perform a repair, improve performance, or modify decisions. There remain significant challenges assessing the risk of adaptations that are dynamically configured at runtime with respect to critical property compliance. Assuming compliance verification was performed for the originally deployed system, the proof process holds valuable meta-data about the variables and conditions that impact reusing the proof on the adapted system. We express this meta-data as a verification workflow using Colored Petri Nets. As dynamic adaptations are configured, the Petri Nets produce alert tokens suggesting the potential proof reuse impact of an adaptation. Alert tokens hold risk values for use in a utility function to determine the least risky adaptations. We illustrate the modeling and risk assessment using a case study.", "num_citations": "10\n", "authors": ["1404"]}
{"title": "The SEREBRO project: Fostering creativity through collaboration and rewards\n", "abstract": " Software Engineering is a highly creative endeavor that challenges Computer Science (CS) students to establish an innovative vision and to craft an outstanding product. Curriculum standards for CS education typically lack creative approaches to Software Engineering, focusing on technological solutions rather than innovative design. Accountability for and contribution to creative initiatives are therefore not part of grading methods in typical Software Engineering courses. In this paper, we introduce a unique framework to foster creativity within an asynchronous, interactive, and graphical environment that tracks the team\u2019s product understanding through the phases of the Rational Unified Process for software engineering. We incorporate a layered, multiagent system to apportion rewards for creative contributions that correspond to theories of creativity based on external motivation.", "num_citations": "10\n", "authors": ["1404"]}
{"title": "Transforming rule-based programs: from the sequential to the parallel\n", "abstract": " Conflict resolution is a form of global control used in production systems to achieve an efficient sequential execution of a rule-based program. This type of control is not used in parallel production system models [6, 13]. Instead, only those programs are executed which make no assumptions regarding conflict resolution. Therefore, the initial sequential rule-based programs are either executed in parallel without their conflict resolution strategy, which normally results in incorrect behavior, or the programs are transformed in an ad hoc manner to execute on a particular parallel production system model. As a result, these programs do not exhibit the parallelism hoped for [10, 13].", "num_citations": "10\n", "authors": ["1404"]}
{"title": "Self-adapting workflow reconfiguration\n", "abstract": " Because web services are highly interoperable, they are capable of providing uniform access to underlying technologies, allowing developers to choose between competing services. Workflow languages, such as BPEL, compose and sequence Web service invocations resulting in meaningful, and sometimes, repeated tasks. Their prevalence means there may be multiple Web services that perform the same operation with some better than others depending on the situation. Their potential for being unavailable at critical workflow execution times forces a reliance on such redundant services. One remedy for unavailability and situational awareness constraints is using quality of service factors and user-directed preferences to assign priorities to workflows and services to perform run-time replacement. In this paper we describe a novel approach to self-adapting workflow reconfiguration. We discuss the implementation\u00a0\u2026", "num_citations": "9\n", "authors": ["1404"]}
{"title": "Dynamically changing workflows of web services\n", "abstract": " Workflow reconfiguration traditionally modifies only workflow definitions. Incorporating dynamism in Web service workflows should also adapt instance execution as services change availability. Commercial workflow engines lack mechanisms to adapt instances except where instances deploy with all possible workflow paths, to achieve pseudo-dynamism. This error prone method has the potential for unsound specifications and still does not allow runtime modifications. We perform workflow reconfiguration through an inspection-feedback loop that decouples services specifications and priorities that can change BPEL workflows from their actual execution. When a change occurs, such as service unavailability, immediate adaptation of the workflow instance takes place. To guarantee proper reconfiguration, we formally specify the architecture, interactions, and change directives, according to a natural separation of\u00a0\u2026", "num_citations": "9\n", "authors": ["1404"]}
{"title": "Reasoning about hybrid system of systems designs\n", "abstract": " Constructing a complex system-of-systems (SoS) involves integrating two or more components. When integrations overcome isolating mechanisms inherent to heterogeneous components, a SoS is a software hybrid. SoS designers are challenged to create a viable hybrid that reuses significant value from autonomous, component systems while allowing for new, SoS-wide properties to emerge and be reliably maintained. Thus, a SoS is a super-system integrating many designs, yet not identical to any of them. Formalisms beyond architecture definition languages for mismatch are needed to express SoS designs to terms of the properties and structures that promote the determination of the cause of hybrid failure and its resolution. This paper extends a formal specification language to SoS designs within a paradigm based on software speciation, where software systems experience divergent evolution under various\u00a0\u2026", "num_citations": "9\n", "authors": ["1404"]}
{"title": "Toward Identifying The Impact Of COTS Evolution On Integrated Systems\n", "abstract": " Evolution is inevitable when dealing with current software systems. Often it stems from user requests, developer needs, advancements in technology, and component upgrades. COTS products are especially susceptible to evolution, including radical changes, due to the need to attain and keep a broad customer base. Sometimes this evolution is embraced as customers see added functionality or performance in the upgraded product. Other times, the product has changed so drastically that customers may be wary of adopting it because of unknown bugs, missing functionality, or lack of backward compatibility. Problems created by component evolution are magnified when a COTS product is part of a system built from independent, heterogeneous components. Often an integration solution or middleware is used to bridge the interaction among components. When one component is modified or upgraded in a manner that alters its interaction properties, it can affect the way the middleware performs. Indeed, this is a weighty issue for developers because these changes can require that an expensive integration effort be redone. The reason is that evolution can generate additional component integration issues, while, at the same time, making others obsolete.How the impact of a component upgrade on interoperability is handled determines the complexity, and ultimately the cost, of changes to the existing middleware. Simple reinsertion of the component may, in fact, work if the evolution is not drastic. However, trial and error is not the best approach, because it will not provide any clues as to what integration solution changes to make when they are\u00a0\u2026", "num_citations": "9\n", "authors": ["1404"]}
{"title": "Developing a Mechanism to Study Code Trustworthiness\n", "abstract": " When software code is acquired from a third party or version control repository, programmers assign a level of trust to the code. This trust prompts them to use the code as-is, make minor changes, or rewrite it, which can increase costs and delay deployment. This paper discusses types of degradations to code based on readability and organization expectations and how to present that code as part of a study on programmer trust. Degradations were applied to sixteen of eighteen Java classes that were labeled as acquired from reputable or unknown sources. In a pilot study, participants were asked to determine a level of trustworthiness and whether they would use the code without changes. The results of the pilot study are presented to provide a baseline for the continuance of the study to a larger set of participants and to make adjustments to the presentation environment to improve user experience.", "num_citations": "8\n", "authors": ["1404"]}
{"title": "Architecting web service attack detection handlers\n", "abstract": " There is a wealth of research on web service attack types and different techniques to mitigate them. However, there is little discussion on reusable methods for implementing these known techniques. In this paper, we introduce two handler architectures that can be reused to implement a broad set of known attack countermeasures. While structurally similar, the architectures differ in the information they require for attack detection, in the needed changes to or restructuring of the message and its content, and in their invocation order among other handlers deployed on the application server and used by the web service. We present the handler architecture designs and how they address the specific web service attack types. We discuss the benefits of their attachment to the Web service. Also, we cover their implementation and deployment details on a JBoss application server and provide a case study to document the\u00a0\u2026", "num_citations": "8\n", "authors": ["1404"]}
{"title": "Analyzing the role of tags as lightweight traceability links\n", "abstract": " Tagging offers a traceability mechanism for software development by connecting artifacts in a meaningful way. Our integrated courseware, SEREBRO, provides a framework of tools that capture conversation and artifact creation and modification throughout the software development lifecycle by student team members developing non-trivial software products in a Software Engineering course. Using a data driven approach, we investigate the use of lightweight tagging mechanisms applied by student software project teams and present some preliminary results of this investigation.", "num_citations": "8\n", "authors": ["1404"]}
{"title": "Evaluating Security Assurance Case Adaptation\n", "abstract": " Security certification processes for information systems involve expressing security controls as functional and non-functional requirements, monitoring deployed mechanisms that satisfy the requirements, and measuring the degree of confidence in system compliance. With the potential for systems to perform runtime self-adaptation, functional changes to remedy system performance may impact security control compliance. This impact can extend throughout a network of related controls causing significant degradation to the system\u2019s overall compliance status. We represent security controls as security assurance cases and implement them in XML for management and evaluation. The approach maps security controls to softgoals, introducing achievement weights to the assurance case structure as the foundation for determining security softgoal satisficing levels. Potential adaptations adjust the achievement weights to produce different satisficing levels. We show how the levels can be propagated within the network of related controls to assess the overall security control compliance of a potential adaptation.", "num_citations": "7\n", "authors": ["1404"]}
{"title": "The influence of commenting validity, placement, and style on perceptions of computer code trustworthiness: a heuristic-systematic processing approach\n", "abstract": " Computer programs are a ubiquitous part of modern society, yet little is known about the psychological processes that underlie reviewing code. We applied the heuristic-systematic model (HSM) to investigate the influence of computer code comments on perceptions of code trustworthiness. The study explored the influence of validity, placement, and style of comments in code on trustworthiness perceptions and time spent on code. Results indicated valid comments led to higher trust assessments and more time spent on the code. Properly placed comments led to lower trust assessments and had a marginal effect on time spent on code; however, the effect was no longer significant after controlling for effects of the source code. Low style comments led to marginally higher trustworthiness assessments, but high style comments led to longer time spent on the code. Several interactions were also found. Our findings\u00a0\u2026", "num_citations": "7\n", "authors": ["1404"]}
{"title": "MTL Robustness for Path Planning with A\n", "abstract": " Maintaining the safety of an autonomous drone while it executes a mission is a primary concern in presence of fixed and mobile enemies. Path planning using A* fails to deliver a feasible, safe plan when a drone has resource limitations in such environments. Enhancing A* with constraint optimization techniques may improve outcomes, but significantly increases path determination time. We define Robust A*(RA*) that introduces the use of a safety margin to maximize the robustness of the drone to meet mission requirements while managing resource restrictions. We rely on a theory of robustness based on Metric Temporal Logic (MTL) as applied to offline verification and online control of hybrid systems. By satisfying the predefined MTL constraints, RA* dynamically defines a safety margin between the drone and an enemy, while constraining the margin size given the drone\u2019s resources. The safety margin creates a robust neighborhood around the dynamically generated path. The robust neighborhood holds all valid trajectories within the current world state. When the world state changes, RA* first examines the robust neighborhood to find a valid trajectory before initiating the path re-planning. We evaluate RA* using the Rassim simulator. The results show that the algorithm generates faster and safer paths than the classical A* in the presence of moving enemies. 1.", "num_citations": "7\n", "authors": ["1404"]}
{"title": "Toward evaluating the impact of self-adaptation on security control certification\n", "abstract": " Certifying security controls is required for information systems that are either federally maintained or maintained by a US government contractor. As described in the NIST SP800-53, certified and accredited information systems are deployed with an acceptable security threat risk. Self-adaptive information systems that allow functional and decision-making changes to be dynamically configured at runtime may violate security controls increasing the risk of security threat to the system. Methods are needed to formalize the process of certification for security controls by expressing and verifying the functional and non-functional requirements to determine what risks are introduced through self-adaptation. We formally express the existence and behavior requirements of the mechanisms needed to guarantee the security controls' effectiveness using audit controls on program example. To reason over the risk of security\u00a0\u2026", "num_citations": "7\n", "authors": ["1404"]}
{"title": "Embedding Verification Concerns in Self-Adaptive System Code\n", "abstract": " For a self-adaptive system, adaptive plans deployed at runtime should comply with critical requirements. The ability to assess plans is especially useful when the system operates for long periods without intervention. Dynamic compliance re-verification consumes enormous resources that may not be available. Plus, in many cases, re-verification of all requirements is unnecessary because the adaptive plan does not impact the associated state variables. If a plan can be configured dynamically from predefined parts, one method is to pre-check all possible plan combinations to determine if compliance could be violated. Unfortunately, this approach disallows runtime formulation of new functionality or new functionality integrations for self-adaptation. Thus, these new products will not be fully vetted prior to system deployment. However, if the deployed system has been verified to comply with critical requirements, then a\u00a0\u2026", "num_citations": "7\n", "authors": ["1404"]}
{"title": "Outcomes of emotional content from agile team forum posts\n", "abstract": " Social media and discussion centric collaborative tools have become an integral part of software development team communication. Accessible by mobile apps and alongside development tools, these tools provide a medium for conversation and enable teams to share project artifacts including commits, documents, and models. As with other forms of human communication, team members naturally emote during software development, resulting in affective post content expressing emotions such as urgency, frustration, and commendation. The effects of affective interaction on team performance, while studied in the literature, are still not well understood, particularly for student teams. This paper adds to the exploration of affect, by examining the effect of emotional post content on project performance metrics among teams of students in a software engineering course. The data set consists of over thirteen hundred\u00a0\u2026", "num_citations": "7\n", "authors": ["1404"]}
{"title": "Diagnosing Vulnerability Patterns in Cloud Audit Logs\n", "abstract": " A service cloud architecture that allows web service compositions to answer complex requests improves the accessibility and flexibility of web services from different vendors. However, security issues exist in the service cloud, including both vulnerabilities of traditional web service communications and new issues brought by inter-cloud communications. Cloud-wide auditing to uncover security issues is a complex task due to the large scale and decentralized structure of the cloud environment. Existing security standards, protocols and auditing mechanisms can provide audit logs, but in most cases, these logs cannot pinpoint type, location, and impact of threats. Given a cloud architecture that specifies the scope of audit logs and a definition of the expected auditable events in the cloud providing evidence of potential threats, we define Vulnerability Diagnostic Trees (VDTs) to formally manifest vulnerability\u00a0\u2026", "num_citations": "7\n", "authors": ["1404"]}
{"title": "Auditing requirements for implementing the chinese wall model in the service cloud\n", "abstract": " The service cloud model allows for the composition of services into an application that can respond to tenant requests. The composition of services, which may originate with different vendors, results in a service chain that supports end-to-end round trip messaging. Thus, the service cloud model must support provisioning services for the request without incurring a conflict of interest (COI) in their message exchange among vendors. Service vendors must disclose their COI classes for storage and analysis by the cloud because as services are provisioned to an application, additional conflict classes may be added, preventing the service from future compositions to avoid COI. In this paper, we present a strategy to centrally store and monitor COI classes for services in a service chain using principles of the Chinese Wall Model. We introduce a Security Monitoring Database (SMDB) that audits and monitors the COI\u00a0\u2026", "num_citations": "7\n", "authors": ["1404"]}
{"title": "An architecture for cross-cloud auditing\n", "abstract": " Auditing message exchange in a cloud involves logging interactions between services that are dynamically composed to satisfy a client's request to the cloud. Additional cloud management services are needed to facilitate audit record capture of events occurring within the end-to-end round trip messaging of the composition and to analyze the resulting audit assets for security anomalies. When an external cloud, possibly in a federation, has a service needed for the task, audit assets must be conveyed back to the originating cloud for disclosure of communications and resource accesses. Otherwise, the external cloud may hide any potential vulnerabilities related to information tainting, message attacks, and resource misuse since these security risks cannot be directly assessed. In this paper, we underscore the importance of designing cross-cloud communications to retain security audit information to assess\u00a0\u2026", "num_citations": "7\n", "authors": ["1404"]}
{"title": "Isolation in design reuse\n", "abstract": " Software composition relies heavily on the ability to reuse software within the context of a complex target system. When components are built or sourced from third party suppliers, some form of design material is reused that embodies a variety of artifacts that differ in both granularity and abstraction. The more concrete the design material, particularly if it is in the form of a fully realized reusable component, the more likely it has evolved from a distinct supplier development path that will cause interoperability problems in the composite design. Unfortunately, recognizing integration problems occurs late in a typical design process, often disrupting the process of choosing alternative components. We express the design reuse process using a concept map whose nodes represent sources of design material. Guidance for consolidating and analyzing reuse alternatives is shown via design moves between concepts\u00a0\u2026", "num_citations": "7\n", "authors": ["1404"]}
{"title": "A framework for interaction in software development training\n", "abstract": " Complex application development should be an integral part of a capstone software engineering course for undergraduates. In the same respect, graduate students in computer science should have practical knowledge of project management. In this paper, we introduce FIST (a Framework for Interaction in Software development Training), which allows students to experience real-world project structures and goals. FIST was devised to establish a corporate-style interaction between the instructor, managers, software project teams, and customers.There are multiple courses involved in FIST. The first course is a combined senior-level/graduate course in software design and specification. In this fall semester course, instruction is given on the Unified Process and the inception phase of the project is begun. In the spring semester, senior computer science students take the capstone software engineering projects course, while graduate students enroll in project management. FIST structures these two courses in an innovative manner. Namely, graduate students perform all the duties of middle and upper management for the senior projects.", "num_citations": "7\n", "authors": ["1404"]}
{"title": "Using AI for Knowledge Management and Business Process Reengineering: Papers from the 1998 Workshop\n", "abstract": " Issues of knowledge management and business process reengineering, such as workflow management, are among the most important topics to business in today's information technology environment. Unfortunately, to date there has been only limited use of AI to address these issues. The purpose of this workshop was to establish foundations for the use of AI in these emerging areas of corporate interest. The workshop sought papers and panels to discuss these issues and the AI techniques being embedded and/or applied to resolve and facilitate them.Previous research in this area has extended many AI approaches to this domain, including case-based reasoning, constraint-based approaches, expert systems and intelligent agents. The workshop attempted to examine the trade-offs between these approaches and new approaches brought forward. One focus of the workshop was on the problems of knowledge and information flow, control, and distribution with respect to knowledge management and business process reengineering.", "num_citations": "7\n", "authors": ["1404"]}
{"title": "The Influence of Personality on Code Reuse\n", "abstract": " The ubiquity and necessity of computer software requires programmers to reuse extant code to keep up with increasing software demands. Researchers have started to investigate the underlying psychological processes and the programmer characteristics affecting code reuse. The present study investigated the role of programmer personality (propensity to trust, suspicion propensity) on willingness to reuse code. Programmers were recruited through Amazon\u2019s Mechanical Turk. Programmers completed propensity to trust and suspicion personality inventories and were subsequently presented with 18 pieces of computer code containing transparency and reputation manipulations. The results demonstrated that propensity to trust did not influence willingness to reuse code. However, facets of suspicion propensity did affect reuse willingness. Programmers lower in trait mal-intent perceptions and higher in cognitive activity were more likely to report they would reuse code. Implications and applications are discussed.", "num_citations": "6\n", "authors": ["1404"]}
{"title": "Toward Increasing Collaboration Awareness in Software Engineering Teams\n", "abstract": " This Research Full Paper investigates collaborative personality traits in undergraduate software engineering teams. Online tools, such as Slack.com, provide team engagement and project management. While metrics can be defined for team and individual performance, it is difficult to measure collaboration and its impacts. Specifically, the forms of collaboration that lead to a successful software product should have associated metrics that correlate with individual performance, peer assessments, and project outcomes. Given the difficulty of assembling teams that best exemplify collaborative personality traits, it may be more beneficial for team members to recognize these traits so that their positive aspects can be exploited toward a successful product outcome. We employ IBM Watson TM  Personality Insights service to analyze team Slack.com posts collected from forty students across ten teams and two semesters of\u00a0\u2026", "num_citations": "6\n", "authors": ["1404"]}
{"title": "Securing Wearables through the Creation of a Personal Fog\n", "abstract": " Increased reliance on wearables using Bluetooth requires additional security and privacy measures to protect these devices and personal data, regardless of device vendor. Most wearables lack the ability to monitor their communication connections and protect personal data without assistance. Attackers can force wearables to disconnect from base stations. When a wearable loses its connection to its base station, an attacker can connect to the wearable to steal stored personal data or await reconnection to the base station to eavesdrop on communications. If the base station inadvertently disconnects from the cloud serving a security-aware app, it would be unable to respond to a rapid change in the security of its current environment. We design a personal fog incorporating wearables, a base station, and the cloud that allows the wearable to be situationally aware and manage inter-and intra-fog communications, given local personal fogs with the same app.", "num_citations": "6\n", "authors": ["1404"]}
{"title": "Toward Predicting Secure Environments for Wearable Devices\n", "abstract": " Wearable devices have become more common for the average consumer. As devices need to operate with low power, many devices use simplified security measures to secure the data during transmission. While Bluetooth, the primary method of communication, includes certain security measures as part of the format, they are insufficient to fully secure the connection and the data transmitted. Users must be made aware of the potential security threats to the information communicated by the wearable, as well as be empowered and engaged to protect it. In this paper, we propose a method of identifying insecure environments through crowdsourced data, allowing wearable consumers to deploy an application on their base system (e.g., a smart phone) that alerts when in the presence of a security threat. We examine two different machine learning methods for classifying the environment and interacting with the users, as well as evaluating the potential uses for both algorithms.", "num_citations": "6\n", "authors": ["1404"]}
{"title": "Toward Increasing Awareness of Suspicious Content through Game Play\n", "abstract": " Phishing, elicitation, and impersonation techniques are performed using multiple forms, targeting content specific to the delivery modality, such as email, social media, and general browser communications. Education to increase awareness is one mechanism to combat phishing. Average email and internet users are less attentive to media warnings and training materials provided by employers than they are in interactive environments. In this paper, we overview a game concept that immerses users in a role play challenge where they must send email, use social media, and browse the web and determine whether content received within these modalities is trustworthy or not. The game, built as a Javascript framework, simulates phishing scams, measures trust and suspicion levels, and individualizes training for users. The game architecture employs components that facilitate dynamic content generation in each of\u00a0\u2026", "num_citations": "6\n", "authors": ["1404"]}
{"title": "A design and verification framework for service composition in the cloud\n", "abstract": " The service cloud model allows hosted services to be dynamically provisioned and composed as part of larger, more complex cloud applications. Compatibility of interaction and quality of service are important to provisioning similar services available in the cloud to a client request. Auditing individual services, the composition and its outcome, and the overall cloud resources, used for monetary assessment or to ensure critical operations, also provide properties for reasoning over service and composition capabilities. Security policies and potential violations pose a threat to the composition since sensitive data may be leaked if information flow control guarantees cannot be proven. Service engineering lacks design principles and an expression infrastructure for formal representation and reasoning within a service cloud model. Reasoning over service compositions requires a formal language that can express multiple\u00a0\u2026", "num_citations": "6\n", "authors": ["1404"]}
{"title": "Security policy foundations in context UNITY\n", "abstract": " Security certification includes assessing an information system to verify its compliance with diverse, pre-selected security controls. The goal of certification is to identify where controls are implemented correctly and where they are violated, creating potential vulnerability risks. Certification complexity is magnified in software composed of systems of systems where there are limited formal methodologies to express management policies, given a set of security control properties, and verify them against the interaction of the participating components and their individual security policy implementations. In this paper, we extend Context UNITY, a formal, distributed, and context aware coordination language to support policy controls. The new language features enforce security controls and provide a means to declare policy specifics in a manner similar to declaring variable types. We use these features in a specification to\u00a0\u2026", "num_citations": "6\n", "authors": ["1404"]}
{"title": "Adapting rewards to encourage creativity\n", "abstract": " Creativity drives innovation and improves the quality of products, problem solving skills, information technology solutions and entrepreneurship. Curriculum standards for CS education typically lack the emphasis and coverage to promote or encourage creative approaches to Software Engineering, focusing on technological solutions rather than innovative design. As a result, creativity is not directly rewarded. This chapter discusses the development of a multi-agent system to apportion rewards for creative contributions to collaborative and group problem solving among students in a software projects course. Encouraging creativity in a classroom team environment, especially for software development, needs a collaboration framework that combines idea management with a motivating reward system. Their multi-agent reward system works directly with the idea capture and visualization portion of SEREBRO, a Web\u00a0\u2026", "num_citations": "6\n", "authors": ["1404"]}
{"title": "Security Controls Applied to Web Service Architectures.\n", "abstract": " Security certification assesses the security posture of a software system to verify its compliance with diverse, pre-specified security controls identified by guidelines from NIST and the US Department of Defense. Service-oriented architectures (SOA) are difficult to certify because they require compliance verification over a mix of local, global, and interaction criteria dictated by the policies of the participating services and SOA governance. Web services further contribute to this difficulty because they lack direct methods to express security controls. Besides being understandable, the method of expression should indicate potential problems complying with chosen services. This paper presents a method for configuring of web service standards to enforce security requirements on service interaction specification documents within a SOA. The outcome serves as a mechanism to direct the population of constraints derived from security controls within standards specification documents, such as WS-Policy. We focus on security controls for auditing and how these can be enforced in an SOA. We introduce a reusable architecture to notate the comparison of security controls across services.", "num_citations": "6\n", "authors": ["1404"]}
{"title": "Measuring Creativity in Software Development.\n", "abstract": " Creativity involves choosing to direct resources toward developing novel ideas. Information technology development, including software engineering, requires creative discourse among team members to design and implement a novel, competitive product that meets usability, performance, and functional requirements set by the customer. In this paper, we present results that correlate metrics of creative collaboration with successful software product development in a Senior Software Projects class that is a capstone course in accredited Computer Science programs. An idea management and reward system, called SEREBRO, provides measurement opportunities to develop metrics of fluency, flexibility, originality, elaboration, and overall creativity. These metrics incorporate multiple perspectives and sources of information into the measurement of creativity software design. The idea management portion of SEREBRO is a Web application that allows team members to initiate asynchronous, creative discourse through the use of threads. Participants are rewarded for brainstorming activities that start new threads for creative discourse and spinning new ideas from existing ones.", "num_citations": "6\n", "authors": ["1404"]}
{"title": "The Impact of Certification Criteria on Integrated COTS-Based Systems\n", "abstract": " While COTS products can be made secure and reliable within a individual domains, they may introduce security vulnerabilities when integrated with other components due to different security expectations. These problematic interactions within an integrated system can be hidden among the multiple, contributing policy types. Furthermore, security certification criteria governing the integrated system can introduce conflicts with local component policies. Security policies and certification criteria lack a common representation. Security policies use various formats and levels of granularity without comparable attributes. Certification criteria are often text-based checklists. We outline a policy configuration model to represent security policies in a format which can manifest conflicting properties across policy specifications. The model defines security policies according to fundamental attributes of property assertions\u00a0\u2026", "num_citations": "6\n", "authors": ["1404"]}
{"title": "Interaction Partnering Criteria for COTS Components\n", "abstract": " Commercial-off-the-Shelf (COTS) software provides a choice of products to streamline enterprise applications. COTS software integration can introduce security vulnerabilities due to mismatches between security constraints coupled with inadequate knowledge of interaction requirements. Though a component can be validated against its stand-alone functional and security requirements, two aspects of the validation for its integration are missing. First, no straightforward process exists to guide the developer in identifying integration-induced security risks. Second, interaction properties contributing security risks are not part of COTS product evaluation. In the former case, a process is needed to take advantage of selection criteria. In the latter case, interaction partnering criteria-criteria indicating how closely related the security constraints of two potentially communicating components are-must be defined. We examine these issues by defining initial interaction partnering criteria and exploring there use in a security profile for COTS components. 1.", "num_citations": "6\n", "authors": ["1404"]}
{"title": "Conflict Patterns: Toward Identifying Suitable Middleware.\n", "abstract": " Architectural patterns aid developers in resolving coarse-grained integration problems among components. These patterns are assembled from functionality slices that resolve various communication problems between applications. However, little attention has been paid to how interoperability problems and their resolution are embodied in these patterns. Mapping these problems to specific functionality promises insight into composing integration architectures by illuminating the consistent, high-level solutions that resolve individual conflicts. The objective of this paper is to describe patterns of interoperability conflicts along with their typical resolution in an effort to present reusable solutions for the design of integration architectures. To this end we present the Extender-Controller-Translator (ETC1) model, and detail its use in a pattern to resolve the problem of data inconsistency.", "num_citations": "6\n", "authors": ["1404"]}
{"title": "Adaptive Coordination to Complete Mission Goals\n", "abstract": " Autonomous systems have become incredibly common, with autonomous vehicles and drones dictating major research trends. Coordination of autonomous vehicles is one of these trends. With multiple different, likely proprietary, systems all needing to communicate and accomplish a task as a unit, there is a need for each individual autonomous system to be capable of entering or leaving the unit, either because of a failure or the need to perform a different task. Thus, each device has a local goal it is trying to complete and a global goal that needs to be completed as part of the unit. Given environmental changes, the systems must adapt by determining how they can satisfy their local goals and self-integrate into the unit's goal when needed or when it is consistent with a local goal. In this paper, we examine self-integrating policies as part of satisfying a global goal when local goals also reside in an autonomous\u00a0\u2026", "num_citations": "5\n", "authors": ["1404"]}
{"title": "Individual Differences in Trust in Code: The Moderating Effects of Personality on the Trustworthiness-Trust Relationship\n", "abstract": " The daily use of technology has made people ever more reliant on software. It is important these software systems are produced in a manner that is both efficient and secure. In this context, psychological trust of software is a pertinent aspect of research. The present study explored the relationship of trustworthiness ratings, propensity to trust, and trait suspicion on software reuse. In addition, we explored personality as a moderator of the trustworthiness-reuse relationship, as hypothesized in the interpersonal trust literature [1]. We recruited participants from Amazon\u2019s Mechanical Turk and requested they assess classes of Java code. Analyses revealed trait suspicion influenced decisions to reuse code and moderated the trustworthiness-trust relationship. A dual-process model of information processing was adopted for interpretation of these effects. Implications include contributions to research and theory on\u00a0\u2026", "num_citations": "5\n", "authors": ["1404"]}
{"title": "Examining Collaboration among Student Teams relying on Web Applications to Coordinate Software Development\n", "abstract": " Training students in software engineering should attempt to mimic industry practices. Thus, student teams develop non-trivial software products, which includes interacting with collaborative tools deployed as web applications. The interaction may be mechanistic or organic, and occur for different durations. Collaboration studies tightly control these factors, relying on manual activity logging, very specific software requirements, surveys and interviews. Since these tools allow simultaneous interaction and capture revision histories, collaboration may be more objectively measured. This paper investigates social media conversations, revision histories, and commit logs from undergraduate student teams performing software development. The objective is to examine how this form of data could be translated into collaborative activities and whether the same performance relationships are achieved in a class setting. A small pilot study shows that the translation methodology did not produce the exact relationships from other studies, but it does shed light on a team\u2019s perception of collaborators.", "num_citations": "5\n", "authors": ["1404"]}
{"title": "Imposing security awareness on wearables\n", "abstract": " Bluetooth reliant devices are increasingly proliferating into various industry and consumer sectors as part of a burgeoning wearable market that adds convenience and awareness to everyday life. Relying primarily on a constantly changing hop pattern to reduce data sniffing during transmission, wearable devices routinely disconnect and reconnect with their base station (typically a cell phone), causing a connection repair each time. These connection repairs allow an adversary to determine what local wearable devices are communicating to what base stations. In addition, data transmitted to a base station as part of a wearable app may be forwarded onward to an awaiting web API even if the base station is in an insecure environment (eg a public Wi-Fi). In this paper, we introduce an approach to increase the security and privacy associated with using wearable devices by imposing transmission changes given\u00a0\u2026", "num_citations": "5\n", "authors": ["1404"]}
{"title": "Apriori Prediction of Phishing Victimization Based on Structural Content Factors\n", "abstract": " Sending malicious content to users for obtaining personnel, financial, or intellectual property has become a multibillion dollar criminal enterprise. Malicious content comes in multiple forms including emails, social media posts, and phishing websites. User training initiatives seek to minimize the impact of malicious content through improved vigilance. Training works best when tailored to specific user deficiencies. However, tailoring training requires understanding how malicious content victimizes users. This paper creates a predictive method that analyzes structural content design factors on malicious phishing content to derive a metric that can predict the likelihood of users being victimized, ie getting phished. The design factors examined are developed from an analysis of over 300 pieces of content from email, social media and websites. In addition to predicting how likely a user is to be phished, our method pinpoints specific deficiencies to enable targeted user training. To evaluate the efficacy of our method, its predictive power, and its usefulness for identifying deficiencies, we conducted two experiments involving a combined 80 subjects and over 5000 individual trust decisions in a game-based simulation platform. The results from these experiments, their analysis, and the implications of the findings are presented as part of the evaluation.", "num_citations": "5\n", "authors": ["1404"]}
{"title": "Gauging influence in software development teams\n", "abstract": " Agile software development teams are generally small, focused groups that require highly motivated members operating in a high-trust environment. Through interactive communication and collaborative work, team members can influence project outcomes both directly and indirectly. One method to examine influence is in terms of communication and control flow among team members when sharing a communication medium and artifact development tools. There currently exist several approaches for characterizing team communication using social network analysis. However, these approaches require modification to be applicable for measuring control flow influence in small teams. In this paper, we discuss the methodology and results of an influence study in which we analyze data generated by student teams in a capstone Software Engineering course. We develop three metrics to measure direct and indirect\u00a0\u2026", "num_citations": "5\n", "authors": ["1404"]}
{"title": "Combining coordination with usage policies to improve mission scheduling resilience\n", "abstract": " Intelligence, surveillance, and reconnaissance (ISR) assets must coordinate to maximize their availability, timeliness, and accuracy of information in a fast-paced, everchanging, and often adversarial environment. The distributed constraint optimization problem (DCOP) is a canonical formulation for representing coordination problems found in the planning and collection phases of global integrated ISR operations. However, ISR assets are often owned by different organizations and have ISR usage policies that dictate additional constraints based on organizational needs. The distributed, agent based system presented in this paper seeks to add resilience to ISR missions by combining DCOP techniques to optimize the scheduling of ISR assets while using policy negotiation to inform the assets' usage policies in order to increase the number of tasks performed by the system. We overview the system architecture, the\u00a0\u2026", "num_citations": "5\n", "authors": ["1404"]}
{"title": "Embedding a Distributed Auditing Mechanism in the Service Cloud\n", "abstract": " The Cloud Security Alliance identified the \"notorious nine\" threats for cloud computing. The range of these threats across the cloud indicates that centralized prevention and detection would be highly inefficient, potentially reporting incidents to tenants well after they occur and are difficult to mitigate. This paper presents an auditing framework for the service cloud that distributes logging, monitoring, and reporting at the local service level, at the application or session level that can involve multiple tenant services, and at the cloud level where corroboration and verification of threats takes place. To verify the forensic coverage of the framework, a set of CAPEC attack patterns are investigated to match attack evidence gathering and mitigation techniques with the proposed distributed detection and mitigation levels of the framework.", "num_citations": "5\n", "authors": ["1404"]}
{"title": "Forming a Security Certification Enclave for Service-Oriented Architectures\n", "abstract": " Security issues with Web services have slowed their adoption for deployment of critical services in the enterprise. Maintaining security in Web service architectures is especially difficult because of their open, standards based interfaces. Yet many organizations are moving to this technology and are faced with the challenge of certifying their environments as secure. Unique challenges exist with the combination of Web service authentication, network security vulnerabilities, incompatible security-mechanisms, open publication of interface definitions, and automated discovery of services. Certification processes mandate the need for a security certification boundary given identified vulnerabilities. In this paper, we review Web service security vulnerabilities and outline guidelines to form an enclave. The expressed guidelines are specific to certifying a service-oriented architecture implemented with Web services", "num_citations": "5\n", "authors": ["1404"]}
{"title": "Understanding services for integration management\n", "abstract": " With the advent of web services, service-oriented architectures (SOAs), which promise interoperable communication in an application integration, are now primarily web-based. Due to the high degree of encapsulation and heterogeneity among commercial-off-the-shelf (COTS) components, their use is limited within these SOAs. This is because the definitions of web-based services do not differentiate between component services and integration functionality nor has this integration functionality been classified specifically for COTS components. Understanding what inhibits COTS components from participating in a SOA is essential to enabling their integration. In this paper, we identify and define common enablement services that facilitate service-oriented integrations in which COTS components participate as integration management services. Software architecture can express integrated system design by\u00a0\u2026", "num_citations": "5\n", "authors": ["1404"]}
{"title": "Architecture integration elements\n", "abstract": " When reusing popular and well-tested systems, unexpected interoperability problems, which can be disastrous, are often encountered during implementation. It is clear from research and empirical studies that these problems can be detected at an architectural level of abstraction. The best approach to resolving these problems is a solution at the same architectural level. This abstract analysis makes resolving the interoperability problem a design decision instead of an implementation fix. However, it is not clear how to formulate an architecturally-based integration solution. In this paper we define integration elements that can resolve interoperability problems found by examining the architecture of the participating software components. We demonstrate, through the use of a taxonomy, that published integration strategies comprise these elements. By describing the strategies in this way, the taxonomy facilitates the transition from the design issues regarding interoperability to their implementation. The taxonomy and the elements are part of the Integrating Component Architectures Process (ICAP).", "num_citations": "5\n", "authors": ["1404"]}
{"title": "Eliminating Redundancy, Conflict, and Incompleteness from Knowledge-Based Systems\n", "abstract": " The reliability of knowledge-based systems (KBSs) has been the subject of a great deal of recent research. Much of this research focuses on the verification of KBSs, specifically to eliminate redundant rules, conflicting rules, and to ensure that the KBS is complete. Often, verification is approached by testing the structural properties of the KBS after the rules have been defined. In this paper we take a different approach and show how the process of formal program derivation from software engineering can be applied to KBSs. The use of these techniques eliminates the need for post-development verification because program derivation guarantees that the KBS will not contain redundant rules, conflicting rules, or be incomplete. As such, verification concerns are addressed during development.", "num_citations": "5\n", "authors": ["1404"]}
{"title": "Task Allocation in Uncertain Environments using a QuadTree and Flow network\n", "abstract": " Systems of multiple UAVs have been used for surveillance and reconnaissance operations for the past few decades. One of the most challenging problems with deploying multiple UAVs with different capabilities is how to individually assign them to a set of tasks in such a way that optimizes the overall mission goal subject to a set of resource constraints. In this paper, we consider the problem of task collaboration and coordination between a team of UAVs using  a priori  reconnaissance about the task and risk distributions. We present an intuitive approach to allocating tasks with centralized and decentralized techniques using a Quad-Tree and K-Partite graph. The Quad-Tree formally models the reconnaissance using its quadrants as potential task locations. It is used to compute the utilities for assigning each UAV to the quadrants, taking into account the cost of reaching the quadrant centroid and eliminating the risk\u00a0\u2026", "num_citations": "4\n", "authors": ["1404"]}
{"title": "Extracting security control requirements\n", "abstract": " Expressing security controls as functional requirements aids in the verification step of security certification and accreditation. Distributed, multi-component systems or systems of systems (SoSs) are difficult to verify because the security controls must be understood in functional terms with respect to their local effects on components, their global effects on the SoS, and their effect on component information exchange. In this paper, we define a process to formulate functional requirements from security controls with SoSs as the target. The process starts by extracting model elements associated with assets, functions, organization variables, and external influences. These models are composed across a set of security controls and normalized to maintain consistency and remove redundancies. We apply the models to SoSs to provide essential details to their specification in functional requirements. The objective is to\u00a0\u2026", "num_citations": "4\n", "authors": ["1404"]}
{"title": "Using Petri nets to detect access control violations in a system of systems\n", "abstract": " System of systems are comprised of multiple, interacting components that may have independent access control policies.. Each component in the system of systems has its own access control domain. Certain interactions among the software components within the system of systems imply the need to configure of inter-domain mappings of access privileges. These mappings require formal, scalable scrutiny to uncover potential violations of confidentiality and availability within a single component. In this paper, we introduce the Conflict Petri Net (ConPN) to analyze interdomain access control mappings for security violations in system of systems. We show formally that ConPN finds all potential conflicts related to role inheritance, Separation of Duty (SoD), cardinality, and temporal restrictions in Role-Based Access Control (RBAC). We generate the ConPN graphically, analyze the policy in motion, and present the conflict outcomes visually and textually.", "num_citations": "4\n", "authors": ["1404"]}
{"title": "Migrating application integrations\n", "abstract": " The internal functionality of middleware is highly variable and thus, well-constructed integrations are difficult to perform without understanding the architectural style of the middleware and the adaptive connections needed make components in an application integration \u201cmiddleware-aware.\u201d In this paper, we use IBM\u2019s WebSphere\u00ae MQ to implement two different architectural styles of integration: request/reply and publish/subscribe. The middleware supports both approaches by using different configurations of controlling, routing, and translating functionality within the connectors. By explicitly describing the component connectors attached to the middleware, we discuss the trade-offs that exist between centralized solutions in which the middleware is responsible for the majority of the integration functionality and localized solutions, in which application connectors are responsible for integration to the largest\u00a0\u2026", "num_citations": "4\n", "authors": ["1404"]}
{"title": "Designing secure integration architectures\n", "abstract": " Security has become a paramount concern due to dramatic advances of network technologies and a wide variety of new business opportunities. These advances have also brought the need for integration of computers systems to the surface, mainly for real-time, information sharing. As these systems are network-based, COTS products are predominantly used in these types of integration efforts. Since security is still a relatively new concern, it is often addressed as an afterthought in software development. Unfortunately, to ensure a high degree of security, it is imperative to address the concerns in a principled manner. Software architectures provide a unique opportunity to assess and structure the security as part of integration solution design. In this paper, we describe an approach to constructing secure integration architectures\u2014architectural solutions to component interoperability that both satisfy known\u00a0\u2026", "num_citations": "4\n", "authors": ["1404"]}
{"title": "Using XML for an Architecture Interaction Conspectus\n", "abstract": " Architectural descriptions of component style and behavior provide powerful predictive capabilities for interoperability conflicts. In this paper, we describe architectural indicators for a first pass interoperability assessment. These indicators form a set that is minimal and realizable, thus having the potential for automated forecasting. We encapsulate these indicators into a XML architecture interaction conspectus (XMLAIC) that fronts each independent software system. XML provides an open, standards-based way to implement the conspectus, affording industry-wide interoperability assessment for both pre-and post-component purchase.", "num_citations": "4\n", "authors": ["1404"]}
{"title": "Adapting Compliance of Security Requirements in Multi-Tenant Applications\n", "abstract": " Multi-tenancy in cloud-based applications helps cloud providers improve their Quality of Service (QoS) and reduce service customization and maintenance time. This result is achieved by sharing resources among many tenants, which can be in the form of applications composed of multiple web services. However, distinct tenants may impose different security requirements on their perspective of the application. Thus, the application must comply with the individual tenant requirements while assuring compatibility across all tenants using the application. This assurance during runtime remains a challenge, especially if tenants are allowed to alter their security posture dynamically. Such self-adaptation within cloud tenants can help to shift security compliance tasks from the static design time to runtime. Security requirements have relied on human intervention or complex models to change or integrate the requirements\u00a0\u2026", "num_citations": "3\n", "authors": ["1404"]}
{"title": "Suspicion, Trust, and Automation\n", "abstract": " This final report provides an overview of research efforts performed under the Suspicion, Trust, and Automation research portfolio as well as detailed sections from one of several studies in three distinct research efforts. The first section examines predictors of state-level IT suspicion. The second section summarizes a conceptual demonstration and prototype development of a non-contact functional near-infrared spectroscopy fNIRS device. The final section focuses on a software engineers trust in reusable software code, specifically readability and organization.Descriptors:", "num_citations": "3\n", "authors": ["1404"]}
{"title": "Measuring the Potential for Victimization in Malicious Content\n", "abstract": " Sending malicious content to users for obtaining personnel, financial, or intellectual property has become a multi-billion dollar criminal enterprise. This content is primarily presented in the form of emails, social media posts, and phishing websites. User training initiatives seek to minimize the impact of malicious content through improved vigilance. Training works best when tailored to specific user deficiencies. However, tailoring training requires understanding how malicious content victimizes users. In this paper, we link a set of malicious content design factors, in the form of degradations and sophistications, to their potential to form a victimization prediction metric. The design factors examined are developed from an analysis of over 100 pieces of content from email, social media and websites. We conducted an experiment using a sample of the content and a game-based simulation platform to evaluate the efficacy of\u00a0\u2026", "num_citations": "3\n", "authors": ["1404"]}
{"title": "Collaborative learning in software development teams\n", "abstract": " Recently Web 2.0 has emerged as a framework to study collaborative learning. Assessing learning in team projects is onemechanism used to improve teaching methodologies and tool support. Web 2.0 technologies enable automated assessmentcapabilities, leading to both rapid and incremental feedback. Such feedback can catch problems in time for pedagogicadjustment, to better guide students toward reaching learning objectives. Our courseware, SEREBRO, couples a social, tagging enabled, idea network with a range of modular toolkits, such as wikis, feeds and project management tools into aWeb 2.0 environment for collaborating teams. In this paper, we first refine a set of published learning indicators intocommunication patterns that are facilitated in SEREBRO. We apply these indicators to student software development teamdiscussions regarding their collaborative activities. We show how the refined patterns, captured by SEREBRO's Web 2.0 modules, are catalysts to the learning process involved in software development.", "num_citations": "3\n", "authors": ["1404"]}
{"title": "Moving toward \u201creality\u201d in team selection for software engineering\n", "abstract": " College students have been bombarded with reality shows. Coupled with extensive video gaming, these weird tasks and challenges have become commonplace for vicarious thrill seekers. Place these same students in a software engineering class where pedagogical norms include process understanding, project design methods, and implementation guidelines, and the class trends toward low-energy and minimal student effort, even with the most state-of-the-art projects. Our approach is to re-energize students using a new mode of competition. The goal is to allow the students to compete for role, team, and project choice. The difficulty in the approach is shaping competitions within the confines of the CS curriculum while maintaining accreditation standards, appropriately grading ldquorealityrdquo challenges, and uniformly configuring teams without ldquooustingrdquo anyone. We introduce the reality software\u00a0\u2026", "num_citations": "3\n", "authors": ["1404"]}
{"title": "Merging integration solutions for architecture and security mismatch\n", "abstract": " Integrating COTS products into a composite application can reduce development effort and associated costs. A major drawback comes from interoperability problems that hinder the seamless integration of components. Two types of problems are prominent: architecture mismatch and security mismatch. Because of their distinct properties, each problem is currently analyzed separately. The results are integration solutions that are constructed in isolation. Combining these solutions can yield another set of problems if their functionality is conflicting, duplicated, or overly complex. It is imperative to address these issues in component based software development. In this paper, we depict the architectural differences among components, their security access control policies, and the integration solutions that result from independent analysis. This is the first step toward including architectural interoperability issues\u00a0\u2026", "num_citations": "3\n", "authors": ["1404"]}
{"title": "Seeking concurrency in rule-based programming\n", "abstract": " Thk paper describes a~ ormal approach for developing concurrent rule-based programs. Specification refinement is used to generate an initial version of the program. Program refinement is then applied to produce a highly concurrent and efficient version of the same program. Tectilques for deriving concurrent programs through either specification or program refinement have been described in previous literature. The main contribution of this paper consists of extending the applicability of these techniques to a broad class of rule-based programs. To the best of our knowledge, thk is the first time formal derivation is employed in the context of rule-based programming.", "num_citations": "3\n", "authors": ["1404"]}
{"title": "Predictive Path Planning Algorithm Using Kalman Filters and MTL Robustness\n", "abstract": " In order to preserve an unmanned autonomous vehicle's (UAV) safety in a dynamic obstacles environment, several technologies must be utilized including mobile obstacle prediction, path planning, and real-time obstacle avoidance. In this paper, we develop a path planning and monitoring approach where metric temporal logic (MTL) and predictive MTL (P-MTL) are used to specify the desired behavior of the UAV and specify its environment. We rely on a theory of robustness based on MTL as applied to offline verification and online control of hybrid systems to augment our previous path planning algorithm. During the path execution, a Kalman Filter is utilized to predict the motion model for the observed mobile obstacles. Then, the monitoring algorithm uses the prediction model to logically and probabilistically reason about the P-MTL formulas of the current trajectory. By predicting the obstacle's path, the MTL\u00a0\u2026", "num_citations": "2\n", "authors": ["1404"]}
{"title": "Mitigating service impersonation attacks in clouds\n", "abstract": " Providing security for interacting cloud services requires more than user authentication with passwords or digital certificates and confidentiality in data transmission. Existing data protection mechanisms have previously failed in preventing data theft attacks perpetrated by an insider to the cloud provider or impersonators. In this paper, we focus on the service cloud model, which facilitates the composition and communication among web services owned by different cloud vendors. We augment a detection approach for impersonation attacks with additional analyses to improve the security of communicating web services hosted in the cloud. A statistical model generates a normal behavior profile for individual services and groups of services based on their business tasks. The detection approach monitors the behavior of each service and identifies anomalies as a potential impersonation attack if it deviates significantly\u00a0\u2026", "num_citations": "2\n", "authors": ["1404"]}
{"title": "Stream Processing with Secure Information Flow Constraints\n", "abstract": " In the near future, clouds will provide situational monitoring services such as health monitoring, stock market monitoring, shopping cart monitoring, and emergency control and threat management. Offering such services requires securely processing data streams generated by multiple, possibly competing and/or complementing, organizations, such that there is no overt or covert leakage of sensitive information. We demonstrate how an information flow control model adapted from the Chinese Wall policy can be used to protect against sensitive data disclosure in data stream management system. We also develop a language based on Continuous Query Language that can be used to express information flow constraints in stream processing and provide its formal semantics.", "num_citations": "2\n", "authors": ["1404"]}
{"title": "Enforcing the Chinese wall model for tenant conflict of interest in the service cloud\n", "abstract": " The service cloud provisions the services from different vendors into an application that can respond to tenant requests. Thus, the service cloud model must support provisioning services for the request without incurring a Conflict of Interest (COI) in their message exchange among vendors. Service vendors must disclose their COI classes for storage and analysis by the cloud because as services are provisioned to an application, additional conflict classes may be added, preventing the service from future compositions to avoid COI. In this paper, we present a strategy to centrally store and monitor COI classes for services using principles of the Chinese wall model. We introduce a Security Monitoring Database (SMDB) that encompasses the repositories and resources to audit the COI classes as they are assigned to hosted services. We describe an algorithm to prevent COI before provisioning services and\u00a0\u2026", "num_citations": "2\n", "authors": ["1404"]}
{"title": "Client-side rendering of collaborative idea networks\n", "abstract": " When distributed teams brainstorm and discuss ideas toward solving a non-trivial problem, it is essential to maintain the temporal and content connections of the discussion, identify to ideas for response, and track conversations to determine emerging problem solutions in order to achieve greater benefit from the collaboration. Our Web 2.0 technology visualizes these connections, identifications, and emerging solutions in the form of a network graph display of connected idea nodes representing posts whose internal content can be viewed by hovering over the node of interest. A user can respond to a post by clicking on it and using the provided post template. Current methods of server-side graph rendering consume too much time when the graph reaches a certain size, limiting user interaction because of refresh delays of the user and other team member posts. Since the objective of the forum is to be nearly as\u00a0\u2026", "num_citations": "2\n", "authors": ["1404"]}
{"title": "FACT: A fusion architecture with contract templates for semantic and syntactic integration\n", "abstract": " Linking components with end-user requests for processing is problematic when there are fundamental language differences between component specifications and how individual users state their needs. Appropriate components may not exist, the users may not know if a component exists until a one matching their requirements is generated, or the users may adjust their requirements. For complex systems governed by a community of interest, we introduce a Fusion Architecture coupled with Contract Templates (FACT). The Fusion Architecture assists with syntactic and semantic unification of user directives. Contract Templates provide a standardized mechanism to collect heterogeneous systems within domains of interest. Based upon specific analysis of each component, Contract Templates attach connectors to generate integrated systems to which queries can be directed. A case study demonstrates how FACT\u00a0\u2026", "num_citations": "2\n", "authors": ["1404"]}
{"title": "Isolating Mechanisms in COTS-based Systems\n", "abstract": " Software composition relies on the interaction of software components, either new or sourced from COTS software vendors. Successful component interaction assumes interoperability, which requires the sharing of common characteristics. Shared characteristics make \"like\" components integrate more easily but also create isolating mechanisms that prevent or inhibit interaction with components that lack such characteristics. Designing systems from COTS components requires understanding their inherent isolating mechanisms. Most approaches only examine composed systems from the perspective of their integration properties. Ignoring or suppressing isolating mechanisms can lead to a partially composed system. Problems may surface anywhere in the system lifecycle including late binding situations at runtime. We examine how shared properties both facilitate membership in a population of like components; yet\u00a0\u2026", "num_citations": "2\n", "authors": ["1404"]}
{"title": "Elevating interaction requirements for web service composition\n", "abstract": " Web services are increasingly utilized to create integrated applications from existing components. However, incompatible interaction expectations between Web service interfaces and/or non-Web service interfaces participating in the overall application can inhibit the integration. Frequently, these conflicts are resolved on a case by case basis. Advantages can be gained by understanding integration conflict resolution as formal interaction requirements. This approach enables evaluation of recurring integration conflicts during requirements analysis (rather than as a testing step), thus providing a more complete resolution that is abstracted from specific runtime interaction instances. 1.", "num_citations": "2\n", "authors": ["1404"]}
{"title": "November 2005\n", "abstract": " Message from the Program Co-Chairs Page 1 ix Message from the Program Co-Chairs elcome to the Proceedings for the Fifth Annual IEEE International Conference on COTS-Based Software Systems (ICCBSS\u201906) held at Orlando, Florida, USA. These proceedings \u2014 containing contributed papers documenting current research and relevant experience, workshops, and a tutorial \u2014 provide a good understanding of both the current state of the art and the current state of practice in developing, acquiring, deploying, and sustaining COTS-based software systems. We believe that you will find them to be a very useful source of insightful, timely information. This year\u2019s submissions comprised a truly international scope, with a balance of papers from academia and industry. Authors from Europe, North and South America, Africa, and Asia submitted their latest results on a diverse range of COTS-related issues. A total of 18 \u2026", "num_citations": "2\n", "authors": ["1404"]}
{"title": "Understanding solution architecture concerns\n", "abstract": " Modern software system development includes the use of off-the-shelf components in the form of third-party software (TPS). Vendors of TPS products play an increasingly important role in the systems that incorporate their offerings. Modeling these vendors' motivations, in the form of architectural concerns and viewpoints, provides an additional tool for customers evaluating TPS. Such viewpoint-based analysis of vendor-oriented solution architectures can provide valuable insights to customers at the time of TPS acquisition as well during the overall lifetime of the system incorporating TPS.", "num_citations": "2\n", "authors": ["1404"]}
{"title": "Dynamism Gloom and Doom?\n", "abstract": " In the areas of application integration, mobility, and agent technology, dynamism is an increasingly desirable quality. However, deploying dynamism is problematic due to constraints on and limitations of the applications and middleware used in such distributed systems. In order to realize dynamism in these contexts, it must be well defined and it must work with available features in middleware. The application components, objects, and agents involved must themselves be equipped to perform dynamically. In this paper, we outline issues for incorporating dynamism into application integrations that rely on middleware. 1", "num_citations": "2\n", "authors": ["1404"]}
{"title": "Defining Change Management Properties for Component Interoperability Assessment.\n", "abstract": " In this paper, we leverage software architecture analysis techniques to codify change management properties and to examine their affect on the functional integration of software components. Software architecture provides a means to predict and analyze the potential for interoperability problems among interacting components. However, the properties addressed in traditional software architecture analysis are limited to the functions for a component\u2019s exchange of control and data. We define architectural abstractions for change management that can be used to extend integration analysis. We exemplify how these properties contribute to the architectural conflicts between interacting components, as well as influence the conflict resolution. The goal is to extend component-based systems analysis to include change management concerns so that clearer designs of dynamic, distributed systems emerge.", "num_citations": "2\n", "authors": ["1404"]}
{"title": "Application of Artificial Intelligence to Reservoir Characterization-An Interdisciplinary Approach\n", "abstract": " The primary goal of this project is to develop a user-friendly computer program to integrate geological and engineering information using Artificial Intelligence (AI) methodology. The project is restricted to fluvially dominated deltaic environments. The static information used in constructing the reservoir description includes well core and log data. Using the well core and the log data, the program identifies the marker beds, and the type of sand facies, and in turn, develops correlation's between wells. Using the correlation's and sand facies, the program is able to generate multiple realizations of sand facies and petrophysical properties at interwell locations using geostatistical techniques. The generated petrophysical properties are used as input in the next step where the production data are honored. By adjusting the petrophysical properties, the match between the simulated and the observed production rates is obtained.", "num_citations": "2\n", "authors": ["1404"]}
{"title": "Using KBS verification techniques to demonstrate the existence of rule anomalies in ADBs\n", "abstract": " As the field of verification and validation for knowledge-based systems (KBSs) has matured, much information, technology, and theory has become available. Though not all of the problems with respect to KBSs have been solved, many have been identified with solutions that can be used in an analogous manner in situations where the application is not necessarily a traditional KBS. As one example, the \u201cactive\u201d component in an active database (ADB) consists of rules that execute as a result of database accesses and updates. In this paper, we demonstrate that anomalies found to impact the correctness of a KBS can also exist in ADBs. We first compare the rule structure of a KBS with the rule structures of various ADBs. To show their existence, we convert the rule syntax of the ADBs into a consistent format for analysis and anomaly detection. Once converted, we apply KBS verification techniques to isolate these\u00a0\u2026", "num_citations": "2\n", "authors": ["1404"]}
{"title": "Observing Team Collaboration Personality Traits in Undergraduate Software Development Projects\n", "abstract": " Team collaboration is an important aspect of software development. When translated to an undergraduate software engineering class, determining if the team is exhibiting positive collaboration toward successful milestone completion means knowing what actions to reward and when to intervene.  Personality traits reflect a person\u2019s tendency toward collaborative behavior. However, it remains a challenge to determine if collaborative traits are effective predictors of team project success. In addition, it is unclear if the traits should be measured at the individual or team level.  In this paper we examine team member collaborative personality traits and observe their appearance and relationship to grades at each of three product milestones during an undergraduate software engineering course.  We use IBM Watson\u2122 Personality Insights service to process online team conversations. The observed patterns indicate which traits are found in well-performing teams and show how trait manifestation can change during the course of the project.", "num_citations": "1\n", "authors": ["1404"]}
{"title": "Gait-Based Identification Using Wearables in the Personal Fog\n", "abstract": " Wearables are becoming more computationally powerful, with increased sensing and control capabilities, creating a need for accurate user authentication. Greater control and power allow wearables to become part of a personal fog system, but introduces new attack vectors. An attacker that steals a wearable can gain access to stored personal data on the wearable. However, the new computational power can also be employed to safeguard use through more secure authentication. The wearables themselves can now perform authentication. In this paper, we use gait identification for increased authentication when potentially harmful commands are requested. We show how the relying on the processing and storage inherent in the personal fog allows distributed storage of information about the gait of the wearer and the ability to fully process this data for user authentication locally at the edge. While gait-based authentication has been examined before, we show an additional, low-power method of verification for wearables.", "num_citations": "1\n", "authors": ["1404"]}
{"title": "Employing the SI Network Model to Evaluate Network Propagation in Bluetooth MANETs\n", "abstract": " In any network application, devices must reliably send information to and receive information from other devices on the network. This reliability has proven to be challenging with Bluetooth mobile ad-hoc networks, or MANETs, because the topology of the network can change unpredictably. To provide reliable network propagation, applications must account for the geophysical constraints unique to Bluetooth MANETs, factors such as the geolocation of the devices in the network, limited communication range, and discovery latency. This has led to the development of communication protocols to overcome these challenges. However, no protocol can guarantee reliable network propagation across all network conditions. In this paper, we define a SI model that can predict the network coverage of a Bluetooth MANET with predefined network conditions. The SI model is a weighted, fully-connected, directed graph which\u00a0\u2026", "num_citations": "1\n", "authors": ["1404"]}
{"title": "Analysis of End-to-End SOA Security Protocols with Mobile Devices\n", "abstract": " Service Oriented Architecture (SOA) is an architectural style that provides agility to align technical solutions to a modular business Web Services (WS) that are well decoupled from their consumers. This agility is extended to the Cloud model. To achieve a high level of security and a degree of decoupling, SOA encourages the use of standardized transport schemes such as SOAP/HTTP(s) with WS family of standards specifications (commonly referred to as WS-* (WS-star)) to ease the interoperability complexity and security concerns in enterprise networks, which have medium/high bandwidth and reliable/wired networks. However, these protocol standards are ill suited for mobile devices due to their limited computational capabilities, low bandwidth, and intermittent connectivity. In this paper, we present an analysis of WS-* standards, classifying and discussing their inter-dependencies to provide a basis for\u00a0\u2026", "num_citations": "1\n", "authors": ["1404"]}
{"title": "Reasoning about policy noncompliance\n", "abstract": " In this paper, we introduce a tuple notation for noncompliance that represents certification problems when meeting security controls in distributed, multi-component software systems. The security controls are adopted from NIST SP800-53 and DoD 8500.2 documents. We derive tuples from component policies and interactions, along with the risks associated with violating the security controls. Tuples can be clustered from different perspectives, reasoned about to target the cause and strength of noncompliance. They can also be mapped to specific security concerns and weaknesses in the multi-component architecture.", "num_citations": "1\n", "authors": ["1404"]}
{"title": "Using A-Federations for On-Demand Crisis Situation Response\n", "abstract": " Dynamic, service-based systems offer powerful resources for knowledge and information processing in a crisis situation. These systems need efficient strategies to integrate diverse services under the stress of an emergency to capture, organize, and disseminate appropriate information. These strategies require a combination of service discovery,'mashing', and orchestration given workflow task templates that can be instantiated and deployed on demand. In this paper, we introduce the A-Federation reference architecture for on-demand emergency management systems (ODEMS). The A-Federation provides the foundation for incorporating these strategies that consider type, availability, and requirements of services as historically understood by users in similar crisis events.", "num_citations": "1\n", "authors": ["1404"]}
{"title": "Establishing Connectors as Integration Services\n", "abstract": " A \"pure\" service-oriented architecture (SOA) is an architectural style with a loose-coupling of services (components) that interact via message passing (connectors) through the standards based and WSDL defined service interfaces regardless of the interacting components\u2019 heterogeneity.", "num_citations": "1\n", "authors": ["1404"]}
{"title": "A theoretical basis for the assessment of rule-based system reliability\n", "abstract": " Determining the reliability of KBS has become an important research area due to the application of KBSs to areas where misplaced confidence can cause large monetary losses or even loss of life. Developers need a set of criteria to evaluate KBS\u2019s reliability, ie reliability criteria. Using arguments from philosophy of science, we define three criteria, thus providing a theoretical basis for judging the reliability of a KBS. Previous researchers have argued for or against specific criteria, however there is little agreement on definitions and groupings of these criteria (Nazareth and Kennedy 1993). The lack of agreement is, in part, due to the lack of a theoretical foundation underpinning KBS reliability. A theoretical viewpoint provides a basis for understanding and generalizing results. Without a theory to explain results, findings may be idiosyncratic to a particular system. Worse, findings that may not be idiosyncratic are easier\u00a0\u2026", "num_citations": "1\n", "authors": ["1404"]}
{"title": "Exploiting inheritance in modeling architectural abstractions\n", "abstract": " Formal (mathematical) modeling of a specification provides an unambiguous representation that allows for rigorous analysis and reasoning over properties. Architecture descriptions define an abstract representation of a system component that is amenable to formal modeling. Using formal modeling within this context provides a basis for integrated system descriptions and analysis, as well as a basis for guaranteeing properties of applications. Missing from the formal modeling of architectures, however, is the notion of inheritance.Representations that include inheritance could provide for a taxonomy of architecture abstractions that model generic classes that can be inherited by more specific classes. Such a taxonomy would include reusable templates and abstract properties that can be inherited to relieve some of the burden of repeated specification and proof. Further benefits can be achieved when designing an integrated system comprising multiple architecture descriptions. By modeling an integrated system at a high level of abstraction, it is easier to initially derive and analyze properties. If the low level, more detailed system models can inherit the high-level specification, then the result is a savings in time and effort, along with a reduction in error (Hasler et al., 1998).", "num_citations": "1\n", "authors": ["1404"]}
{"title": "Application of knowledge based systems to virtual organizations\n", "abstract": " Virtual organizations are networked organizations that bind together teams of people that meet and work together through the use of information technology. Such organizations may bring together hundred or perhaps thousands of separate organizational entities. Virtual organizations rely on and traffic in shared information so that the many entities they manage may coordinate their activities effectively and efficiently. Virtual enterprises supported by such organizations have the benefit of the shared expertise and resources of all the contributing members. In this paper, we discuss a prototype knowledge based system to advise companies considering the creation of or membership in a virtual organization.", "num_citations": "1\n", "authors": ["1404"]}
{"title": "Using software architecture to formally derive synchronous knowledge based systems\n", "abstract": " Knowledge based systems have matured enough in recent years that increased efficiency and reliability are important issues for their continued use. It has been previously shown that the application of formal methods to constructing knowledge based system can improve reliability. Research has turned to concurrency in the form of parallel production system shells as a way to increase execution speed. However, the typical use of formal program derivation does not consider aspects of concurrency or efficiency until the specification design is nearly complete. In this paper, we illustrate the formal development of a synchronous knowledge based system using methods in which the software architecture of the eventual implementation is exploited early in the specification definition process. This architecture contains information on aspects of concurrency and efficiency in rule processing. This approach is in contrast to\u00a0\u2026", "num_citations": "1\n", "authors": ["1404"]}
{"title": "Integrating a formal specification course with a software projects course via an editing tool\n", "abstract": " This paper reports on a two-course sequence for undergraduate students that provides them with an intensive course in formal specification methods and a traditional software design course. The specification course provides an appreciation for the use of rigorous specification methods within the software lifecycle. The manual nature of developing a formal specification provides the appropriate justification for the projects in the software design course in which the students create useful tools to aid the development of a formal specification. Within the project course, the students developed the first prototype of a graphical editor for building Z specifications. We discuss the building of the editing tool and its integration into the software engineering curriculum.", "num_citations": "1\n", "authors": ["1404"]}