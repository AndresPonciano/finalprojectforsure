{"title": "Test case reduction using data mining technique\n", "abstract": " Software testing is a process of ratifying the functionality of software. It is a crucial area which consumes a great deal of time and cost. The time spent on testing is mainly concerned with testing large numbers of unreliable test cases. The authors' goal is to reduce the numbers and offer more reliable test cases, which can be achieved using certain selection techniques to choose a subset of existing test cases. The main goal of test case selection is to identify a subset of the test cases that are capable of satisfying the requirements as well as exposing most of the existing faults. The state of practice among test case selection heuristics is cyclomatic complexity and code coverage. The authors used clustering algorithm which is a data mining approach to reduce the number of test cases. Their approach was able to obtain 93 unique effective test cases out a total of 504.", "num_citations": "13\n", "authors": ["1980"]}
{"title": "Model checking based classification technique for wireless sensor networks\n", "abstract": " Recently, many data mining techniques have been applied to analyze and interpret the huge volume of data collected from wireless sensor networks. Such techniques, especially classification and clustering, have been used to relate raw data and assign a class label (a useful interpretation) to the set of attributes values received from the sensors' nodes. However, building a classifier, such as decision tree, is a cost process in terms of energy consumption due to the large size of the resultant tree. In this article, we propose a model-checking based classification method that relies on cutting-off parts of the decision tree while keeping the performance fixed. The pruning process aims to reduce the size of the tree and, thus, reduce the amount of the energy needed to maintain the classifier. Results have shown energy reduction between 10\u201315% compared with a nonpruned decision tree.", "num_citations": "13\n", "authors": ["1980"]}
{"title": "Dispersion\u2013based prediction framework for estimating missing values in wireless sensor networks\n", "abstract": " Wireless Sensor Networks (WSNs) have attracted many researchers in the past few years due to their applicability for a wide\u2013range of applications. WSNs rely on unreliable sensing schemes in which a sensor might lose some data due to the inherent characteristics of such networks. Estimating missing values that cope with other collected ones is crucial for some applications. In this paper, we introduced a framework dedicated to predicting missing values in WSNs. The key idea is to estimate missing values according to the natural spread (i.e. dispersion) of the guilty sensors. The framework considers cases in which distance and time play a significant role in estimating missing values. Thus, accurate values might be generated as compared to state\u2013of\u2013the\u2013art central tendency measurements such as mean, median, mode, and midrange.", "num_citations": "13\n", "authors": ["1980"]}
{"title": "Model-based testing of distributed systems\n", "abstract": " This paper provides an overview of Model-Based Testing (MBT) and its activities. A classification of MBT based on different criteria is also presented. Furthermore, several difficulties of MBT are highlighted in this paper. A survey that provides a detailed description of how MBT is effective in testing different quality attributes of distributed systems such as security, performance, reliability, and correctness is given. A comparison between different MBT tools based on the classification is also given at the end of this paper.", "num_citations": "13\n", "authors": ["1980"]}
{"title": "Evaluating Maintainability of Android Applications\n", "abstract": " Android applications are considered the most popular and desirable applications due to their friendly interface, various categories and available options, and supporting most of the available hardware in the market. Complexity to understand, design, develop, implement, and test android applications have been raised. Maintainability is a very important quality attribute that we should consider it seriously. However, it is one of the most difficult and costly attributes that can be achieved. The software metrics are used to predict and estimate the software maintainability value. There are several metrics and formulas that are used to measure and estimate the maintainability value. These metrics and formulas are derived from the different usages of the maintainability. This paper focus on analyzing and measuring the maintainability of Android mobile applications using Object Oriented metrics and Android Metrics. The\u00a0\u2026", "num_citations": "11\n", "authors": ["1980"]}
{"title": "Mutation operators for JADE mobile agent systems\n", "abstract": " Mobile Agent System (MAS) is a distributed software system responsible for supporting and managing mobile agents. It is suitable to develop many applications for mobile computing. Testing and debugging MASs is hard to do, due the complex execution of MAS. In this paper we are interested in using mutation analysis to evaluate, compare and improve quality for MASs. There are many mutation operators in the literature. However, they are insufficient for MASs. This paper we extract 26 mutation operators for JADE MASs. Those mutation operators are categorized into four levels based on the specific fault features of JADE Mobile Agent System.", "num_citations": "11\n", "authors": ["1980"]}
{"title": "Test Case Reduction using Data Mining Classifier Techniques\n", "abstract": " The main purpose of test case reduction is to decrease the number of test cases in order to minimize the time and cost of executing them. We used the data mining approach, mainly because of its ability to extract patterns of test cases that are invisible. In this paper, we use two data mining classifiers, Na\u00efve based and J48, in order to classify a set of test cases and identify which test case is redundant or irredundant. The results show the applicability of data mining classification in removing the number of redundant test cases.", "num_citations": "10\n", "authors": ["1980"]}
{"title": "A survey of using model-based testing to improve quality attributes in distributed systems\n", "abstract": " This paper provides a detailed survey of how Model- Based Testing (MBT) has been used for testing different quality attributes of distributed systems such as security, performance, reliability, and correctness. For this purpose, three additional criteria are added to the classification. These criteria are: the purpose of testing, the test case paradigm, and the type of conformance checking. A comparison between different MBT tools based on the classification is also given.", "num_citations": "10\n", "authors": ["1980"]}
{"title": "Mining software repositories for adaptive change commits using machine learning techniques\n", "abstract": " ContextVersion Control Systems, such as Subversion, are standard repositories that preserve all of the maintenance changes undertaken to source code artifacts during the evolution of a software system. The documented data of the version history are organized as commits; however, these commits do not keep a tag that would identify the purpose of the relevant undertaken change of a commit, thus, there is rarely enough detail to clearly direct developers to the changes associated with a specific type of maintenance.ObjectiveThis work examines the version histories of an open source system to automatically classify version commits into one of two categories, namely adaptive commits and non-adaptive commits.MethodWe collected the commits from the version history of three open source systems, then we obtained eight different code change metrics related to, for example, the number of changed statements\u00a0\u2026", "num_citations": "9\n", "authors": ["1980"]}
{"title": "Evaluating the understandability of android applications\n", "abstract": " Understandability is one of the major quality attributes used to measure the understandability of object oriented software and Android applications based on certain metrics. It is very important in most software development life cycles because misunderstanding any of the steps of the software development life cycle will lead to a poor-quality product. Mobile applications, like any software, need to be maintained and evolved over time, so understanding them is essential to their maintainability, reliability, quality and reusability. In this research, the authors discuss the importance of understandability in Android applications and determine the most influential metrics by using Metrics Reloaded tool to measure the metrics values. Then, based on these values, they develop a formula to measure the understandability for Android applications. The results show that the understandability of Android application can be measured\u00a0\u2026", "num_citations": "9\n", "authors": ["1980"]}
{"title": "Selecting a standard set of attributes for cost estimation of software projects\n", "abstract": " The aim of the software engineering is to enhance projects that produce the needed results within limited schedule and budget. So that, software effort estimation becomes a valuable manner since it limits the problems of overestimate and underestimate for the software. Software cost estimation is the process of predicting the effort required to develop a software system. There are many estimation models over the last decade, and in this paper, we use six public cost estimation data sets that we obtained from promise repository. We perform regression analysis over these data sets and perform a feature selection in order to get the most effective attribute to the effort. Finally, we analyze and compare the results obtained from each data set to build a framework for the standard set of metrics that we suggest each cost estimation data set must contain.", "num_citations": "8\n", "authors": ["1980"]}
{"title": "Run-time conformance checking of mobile and distributed systems using executable models\n", "abstract": " This paper describes an approach for conformance testing of mobile and distributed systems. The approach is based on kiltera---a novel, high-level language supporting the description and execution of models of concurrent, mobile, distributed, and timed computation. In our approach, a kiltera model of the system is constructed from a high-level model which describes system behavior using, eg, a suitable UML profile. Check points are identified in the implementation under test (IUT) and the kiltera model and both are instrumented appropriately at these check points. During execution, relevant information flows from the IUT to the kiltera model which signals any non-conformance detected. Unique features of our approach include the support for mobility, distribution, time, dynamic creation and deletion of agents, and distributed monitoring. We describe the approach and a prototype implementation using a running\u00a0\u2026", "num_citations": "8\n", "authors": ["1980"]}
{"title": "Data Mining Tools Evaluation Based on their Quality Attributes\n", "abstract": " As a result of the rapid evolution of open source software, the entire software industry has been grown up in a way that made it harder to improve the software quality or even keep it as it is without any shortages in the quality level. However, data mining tools are considered as a good example of open source software, which has a big similarity between them such as in goals, results and maybe in the data set which are used in work, but for sure they are different in terms of quality. Quality attributes such as maintainability, reusability and fault-proneness are important factors in any process. In addition, they can tell which tool is appropriate for specific jobs. In this paper the quality of five open source data mining products (Weka 3.9, Rapid Miner, Knime, Apache Mahout and Keel) is investigated by studying their quality metrics and attribute, to help the software developers and researchers decide and select the best data mining tools that meet their desired needs. By applying a predefined maintainability index, ApacheMahout tool has been found the most maintainable. Also, Keel had the most reusable components based on the reusability index formula used in this paper. As a result of this research, a formula (according to a previous equation) has been developed, based on object oriented metrics, to measure the fault-proneness of the selected data mining tools.", "num_citations": "6\n", "authors": ["1980"]}
{"title": "Regression Test-Selection Technique Using Component Model Based Modification: Code to Test Traceability\n", "abstract": " Regression testing is a safeguarding procedure to validate and verify adapted software, and guarantee that no errors have emerged. However, regression testing is very costly when testers need to re-execute all the test cases against the modified software. This paper proposes a new approach in regression test selection domain. The approach is based on metamodels (test models and structured models) to decrease the number of test cases to be used in the regression testing process. The approach has been evaluated using three Java applications. To measure the effectiveness of the proposed approach, we compare the results using the re-test to all approaches. The results have shown that our approach reduces the size of test suite without negative impact on the effectiveness of the fault detection.", "num_citations": "6\n", "authors": ["1980"]}
{"title": "Evaluate and Improve GUI Testing Coverage Automatically\n", "abstract": " Evaluating coverage criteria is a core subject in software testing. Coverage can be evaluated based on several concerns such as: code, requirements, syntax, paths, decisions, graph, etc. The focus of this research is in evaluating the Graphical User Interface (GUI) testing coverage. In this research, several GUI model based methods are proposed and evaluated for the goal of automatically testing user interfaces and evaluate the testing coverage. The user interfaces\u2019 components of tested applications are collected while applications are running and this information is used to build a GUI structural model. This model is used as an input for the process of generating test cases automatically and on considering different aspects of GUI coverage such as GUI paths, edges, nodes, or components. Microsoft NModel and SpecExplorer are used as tools for the GUI model verification. This is usually accomplished indirectly through generating test cases from the model and evaluating the model through the effectiveness and coverage of the generated test cases.", "num_citations": "4\n", "authors": ["1980"]}
{"title": "Automatic model based methods to improve test effectiveness\n", "abstract": " Software testing covers a large percent of the software development expenses. However, formal methods are applied, usually, to improve or ensure the correctness of the requirements, design, code, or testing. In order to utilize formal methods particularized to different cases, the subject matter needs to be written in a formal language or syntax. In this research, several model based methods are investigated and experimented in order to reduce testing expenses, improve test coverage, and the effectiveness of the testing process.Formal models are generated from the application during runtime. For this purpose a tool is developed to automatically derive the formal syntax from the application at runtime. Later on, the formal model is used in improving test effectiveness. In addition, the model is used to find some possible dynamic problems in the application that might be hard to be discovered by traditional testing methods. Finally, a test monkey tool is proposed in order to test the application for deadlock or progress problems and test the application ability to reject invalid test cases as well.", "num_citations": "4\n", "authors": ["1980"]}
{"title": "Source code-based defect prediction using deep learning and transfer learning\n", "abstract": " Ensuring the quality of software products is important for them to be successful. Discovering errors and fixing defective software modules early in the project lifecycle (eg in the testing phase) can save resources and enhance software quality. Developers should prioritize testing procedures and continuously maintain their software projects; however, when there are few instances of a new project, it is hard to build an accurate defect prediction model. Different information about software projects is available and can be utilized through open repositories. Developers can leverage the labeled defect information to build a defect prediction model. The abundance of historical software information in similar domains can assist in transferring the knowledge gained from training this information to other domains for cross-project defect prediction models. Deep learning is a promising machine learner. Deep Belief network (DBN\u00a0\u2026", "num_citations": "3\n", "authors": ["1980"]}
{"title": "A New Data Mining-Based Framework to Test Case Prioritization Using Software Defect Prediction\n", "abstract": " Test cases do not have the same importance when used to detect faults in software; therefore, it is more efficient to test the system with the test cases that have the ability to detect the faults. This research proposes a new framework that combines data mining techniques to prioritize the test cases. It enhances fault prediction and detection using two different techniques: 1) the data mining regression classifier that depends on software metrics to predict defective modules, and 2) the k-means clustering technique that is used to select and prioritize test cases to identify the fault early. Our approach of test case prioritization yields good results in comparison with other studies. The authors used the Average Percentage of Faults Detection (APFD) metric to evaluate the proposed framework, which results in 19.9% for all system modules and 25.7% for defective ones. Our results give us an indication that it is effective to start\u00a0\u2026", "num_citations": "3\n", "authors": ["1980"]}
{"title": "USING FORMAL METHODS FOR TEST CASE GENERATION ACCORDING TO TRANSITION-BASED COVERAGE CRITERIA\n", "abstract": " Formal methods play an important role in increasing the quality, reliability, robustness and effectiveness of the software. Also, the uses of formal methods, especially in safety-critical systems, help in the early detection of software errors and failures which will reduce the cost and effort involved in software testing.The aim of this paper is to prove the role and effectiveness of formal specification for the cruise control system (CCS) as a case study. A CCS formal model is built using Perfect formal specification language, and its correctness is validated using the Perfect Developer toolset. We develop a software testing tool in order to generate test cases using three different algorithms. These test cases are evaluated to improve their coverage and effectiveness. The results show that random test case generation with full restriction algorithm is the best in its coverage results; the average of the path coverage is 77.78% and the average of the state coverage is 100%. Finally, our experimental results show that Perfect formal specification language is appropriate to specify CCS which is one of the most safety-critical software systems, so the process of detecting all future possible cases becomes easier.", "num_citations": "3\n", "authors": ["1980"]}
{"title": "Implementing and evaluating a runtime conformance checker for mobile agent systems\n", "abstract": " A Mobile Agent System (MAS) is a special kind of distributed system in which the agent software can move from one physical host to another. This paper describes a new approach, together with its implementation and evaluation, for checking the conformance of a MAS with respect to an executable model. In order to check the effectiveness of our conformance check, we have built a mutation-based evaluation framework. Part of the framework is a set of 29 new mutation operators for mobile agent systems. Our conformance checking approach is used to compare the mutated agents with the executable model and determine nonconformance. Our experimental results suggest that our approach holds promise for the generation and detection of non-equivalent mutants.", "num_citations": "3\n", "authors": ["1980"]}
{"title": "Comparison between ad-hoc retrieval and filtering retrieval using arabic documents\n", "abstract": " The objective of this research is to study the process of examining documents by computing comparisons between the representation of the information need (the queries) and the representations of the documents. Also, we will automate the process of representing information needs as user profiles by computing the comparison between the user profile and the representations of the documents.         We consider an automated process to be successful when it produces results similar to those produced by human comparison of the documents themselves with actual information need. Thus, we will compare ad-hoc retrieval and filtering retrieval tasks and examine the differences between them in terms of the information retrieval process.         We have selected 242 Arabic abstracts that were used by Hmeidi [7]. All these abstracts involve computer science and information systems. We have also designed and built a\u00a0\u2026", "num_citations": "3\n", "authors": ["1980"]}
{"title": "Mutation Testing for Evaluating PHP Web Applications\n", "abstract": " Web applications provide services to hundreds of billions of people over the world, so they should be tested, to insure their validity. In this article, we are investigating the ability of testing web application based on traditional mutation testing. To perform this test, we have defined 54 mutation operators, classified into six categories: SQL data retrieving, data manipulation; domain name and IP address look up; internet protocol and service information; HTTP; connection to server and to database. The test was applied to websites that are built using PHP programming for two reasons. The majority of websites nowadays are built using ASP. net or PHP and most of the testing efforts that have been applied on web applications were using the Java programming language. We have implemented a prototype tool called \u03bcWebPHP for automatically generating mutants for PHP web applications based on the identified mutation\u00a0\u2026", "num_citations": "2\n", "authors": ["1980"]}
{"title": "AN EFFICIENT APPROACH FOR TEST SUITE REDUCTION USING K-MEANS CLUSTERING.\n", "abstract": " Software testing is the primary approach that is used to test and evaluate software under development. The main goal of testing is to find defects before customers find them out. It is very costly. Therefore, reducing the cost of the test is a big challenge. This paper aims at reducing the cost of the test by eliminating the redundant test cases. Our methodology begins with generating the test cases randomly. The Procedural Language/Structured Query Language (PL/SQL) tool is used to generate test cases from the payroll system database functions. The SPSS software package is used to apply the K-means Clustering algorithm to reduce the test cases. The results reveal that the proposed approach significantly reduces the number of test cases from 776 to 240 while keeping the same coverage.", "num_citations": "2\n", "authors": ["1980"]}
{"title": "Cloud testing: Steps, tools, challenges\n", "abstract": " Cloud computing has protruded as a modernistic computing model that influences many research areas, such as software testing that has been extended with boundless resources such as scalability and availability of expanded testing environments. Cloud computing minimizes the time needed for testing large software and lead to a decrease in testing cost. Cloud computing also gives the chance for developing many efficacious and wide reach software testing methods. This research will conduct a comparison between traditional software testing and cloud based testing. We also introduced some testing mechanisms in cloud and focus on the challenges of these testing types. Moreover, we discussed the special objectives, lineaments, requirements that are necessary for cloud testing. At the end of the paper we identified several testing tools and determine the key tools for cloud testing.", "num_citations": "2\n", "authors": ["1980"]}
{"title": "Runtime Conformance Checking of Mobile Agent Systems Using Executable Models\n", "abstract": " Mobility occurs naturally in many distributed system applications such as telecommunications and electronic commerce. Mobility may reduce bandwidth consumption and coupling and increase flexibility. However, it seems that relatively little work has been done to support quality assurance techniques such as testing and verification of mobile systems.This thesis describes an approach for checking the conformance of a mobile, distributed application with respect to an executable model at runtime. The approach is based on kiltera\u2014a novel, high-level language supporting the description and execution of models of concurrent, mobile, distributed, and timed computation. The approach allows distributed, rather than centralized, monitoring. However, it makes very few assumptions about the platform that the mobile agent system is implemented in.", "num_citations": "2\n", "authors": ["1980"]}
{"title": "Mutation Testing to Evaluate Android Applications\n", "abstract": " Android is an operating system source which offers flexibility and support for most mobile applications, and easy access to social networks. It is important to understand the complexity of design, development, implementation, and testing of Android apps. A number of challenges may be faced in testing android applications, including the lack of testing processes and methods, testing experts being unavailable, poor in-house testing environment, and time restrictions. Mutation testing is a fault-based testing technique, applied by generating mutants and running the application with these mutants to analyze the killed and equivalent mutants. We defined a set of mutation operators according to the features of android applications: apps with content sharing, apps with multimedia, apps with graphics, and apps with user location and maps. We identified 42 mutation operators. In addition, we implemented a new tool,\u201c\u00b5\u00a0\u2026", "num_citations": "1\n", "authors": ["1980"]}
{"title": "A hybrid pre-post constraint-based framework for discovering multi-dimensional association rules using ontologies\n", "abstract": " Association rule mining is a very useful knowledge discovery technique to identify co-occurrence patterns in transactional data sets. In this article, the authors proposed an ontology-based framework to discover multi-dimensional association rules at different levels of a given ontology on user defined pre-processing constraints which may be identified using, 1) a hierarchy discovered in datasets; 2) the dimensions of those datasets; or 3) the features of each dimension. The proposed framework has post-processing constraints to drill down or roll up based on the rule level, making it possible to check the validity of the discovered rules in terms of support and confidence rule validity measures without re-applying association rule mining algorithms. The authors conducted several preliminary experiments to test the framework using the Titanic dataset by identifying the association rules after pre-and post-constraints are\u00a0\u2026", "num_citations": "1\n", "authors": ["1980"]}
{"title": "Traceability between Code and Design Documentation in Database Management System: A Case Study\n", "abstract": " Traceability builds many strong connections or links between requirements and design, so the main purpose of traceability is to maintain consistency between a high level conceptual view and a low level implementation view. The purpose of this paper is to have full consistency between all components over all phases in the Oracle designer tool by allowing traceability to be carried out not only between the requirements and design but also between the code and design. In this paper, we propose a new methodology to support traceability and completeness checking between code and design of Oracle database applications. The new algorithm consists of a set of interrelated steps to initialize the comparison environment. An example of a Student Information System is used to illustrate the work.", "num_citations": "1\n", "authors": ["1980"]}