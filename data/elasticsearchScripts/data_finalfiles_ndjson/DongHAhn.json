{"title": "Beyond DVFS: A first look at performance under a hardware-enforced power bound\n", "abstract": " Dynamic Voltage Frequency Scaling (DVFS) has been the tool of choice for balancing power and performance in high-performance computing (HPC). With the introduction of Intel's Sandy Bridge family of processors, researchers now have a far more attractive option: user-specified, dynamic, hardware-enforced processor power bounds. In this paper we provide a first look at this technology in the HPC environment and detail both the opportunities and potential pitfalls of using this technique to control processor power. As part of this evaluation we measure power and performance for single-processor instances of several of the NAS Parallel Benchmarks. Additionally, we focus on the behavior of a single benchmark, MG, under several different power bounds. We quantify the well-known manufacturing variation in processor power efficiency and show that, in the absence of a power bound, this variation has no\u00a0\u2026", "num_citations": "218\n", "authors": ["1830"]}
{"title": "Scalable analysis techniques for microprocessor performance counter metrics\n", "abstract": " Contemporary microprocessors provide a rich set of integrated performance counters that allow application developers and system architects alike the opportunity to gather important information about workload behaviors. Current techniques for analyzing data produced from these counters use raw counts, ratios, and visualization techniques help users make decisions about their application performance. While these techniques are appropriate for analyzing data from one process, they do not scale easily to new levels demanded by contemporary computing systems. Very simply, this paper addresses these concerns by evaluating several multivariate statistical techniques on these datasets. We find that several techniques, such as statistical clustering, can automatically extract important features from the data. These derived results can, in turn, be fed directly back to an application developer, or used as input to a\u00a0\u2026", "num_citations": "94\n", "authors": ["1830"]}
{"title": "Scalable I/O-aware job scheduling for burst buffer enabled HPC clusters\n", "abstract": " The economics of flash vs. disk storage is driving HPC centers to incorporate faster solid-state burst buffers into the storage hierarchy in exchange for smaller parallel file system (PFS) bandwidth. In systems with an underprovisioned PFS, avoiding I/O contention at the PFS level will become crucial to achieving high computational efficiency. In this paper, we propose novel batch job scheduling techniques that reduce such contention by integrating I/O awareness into scheduling policies such as EASY backfilling. We model the available bandwidth of links between each level of the storage hierarchy (ie, burst buffers, I/O network, and PFS), and our I/O-aware schedulers use this model to avoid contention at any level in the hierarchy. We integrate our approach into Flux, a next-generation resource and job management framework, and evaluate the effectiveness and computational costs of our I/O-aware scheduling. Our\u00a0\u2026", "num_citations": "64\n", "authors": ["1830"]}
{"title": "Flux: A next-generation resource management framework for large HPC centers\n", "abstract": " Resource and job management software is crucial to High Performance Computing (HPC) for efficient application execution. However, current systems and approaches can no longer keep up with the challenges large HPC centers are facing due to ever-increasing system scales, resource and workload diversity, interplays between various resources (e.g., between compute clusters and a global file system), and complexity of resource constraints such as strict power budgeting. To address this gap, we propose Flux, an extensible job and resource management framework specifically designed to deal with the requirements of next-generation HPC centers. Flux targets an entire computing facility as one common pool of diverse sets of resources, enabling the facility to accommodate site-wide constraints (e.g., for power limits). Yet, its scalable and distributed design still offers scalable and effective scheduling strategies\u00a0\u2026", "num_citations": "53\n", "authors": ["1830"]}
{"title": "Massively parallel loading\n", "abstract": " Dynamic linking has many advantages for managing large code bases, but dynamically linked applications have not typically scaled well on high performance computing systems. Splitting a monolithic executable into many dynamic shared object (DSO) files decreases compile time for large codes, reduces runtime memory requirements by allowing modules to be loaded and unloaded as needed, and allows common DSOs to be shared among many executables. However, launching an executable that depends on many DSOs causes a flood of file system operations at program start-up, when each process in the parallel application loads its dependencies. At large scales, this operation has an effect similar to a site-wide denial-of-service attack, as even large parallel file systems struggle to service so many simultaneous requests. In this paper, we present SPINDLE, a novel approach to parallel loading that\u00a0\u2026", "num_citations": "27\n", "authors": ["1830"]}
{"title": "Prionn: Predicting runtime and io using neural networks\n", "abstract": " For job allocation decision, current batch schedulers have access to and use only information on the number of nodes and runtime because it is readily available at submission time from user job scripts. User-provided runtimes are typically inaccurate because users overestimate or lack understanding of job resource requirements. Beyond the number of nodes and runtime, other system resources, including IO and network, are not available but play a key role in system performance. There is the need for automatic, general, and scalable tools that provide accurate resource usage information to schedulers so that, by becoming resource-aware, they can better manage system resources.", "num_citations": "26\n", "authors": ["1830"]}
{"title": "Exascale algorithms for generalized mpi_comm_split\n", "abstract": " In the quest to build exascale supercomputers, designers are increasing the number of hierarchical levels that exist among system components. Software developed for these systems must account for the various hierarchies to achieve maximum efficiency. The first step in this work is to identify groups of processes that share common resources. We develop, analyze, and test several algorithms that can split millions of processes into groups based on arbitrary, user-defined data. We find that bitonic sort and our new hash-based algorithm best suit the task.", "num_citations": "26\n", "authors": ["1830"]}
{"title": "Flux: Overcoming scheduling challenges for exascale workflows\n", "abstract": " Many emerging scientific workflows that target high-end HPC systems require complex interplay with the resource and job management software\u00a0(RJMS). However, portable, efficient and easy-to-use scheduling and execution of these workflows is still an unsolved problem. We present Flux, a novel, hierarchical RJMS infrastructure that addresses the key scheduling challenges of modern workflows in a scalable, easy-to-use, and portable manner. At the heart of Flux lies its ability to be seamlessly nested within batch allocations created by other schedulers as well as itself. Once a hierarchy of Flux instances is created within each allocation, its consistent and rich set of well-defined APIs portably and efficiently support those workflows that can often feature non-traditional execution patterns such as requirements for complex co-scheduling, massive ensembles of small jobs and coordination among jobs in an ensemble\u00a0\u2026", "num_citations": "24\n", "authors": ["1830"]}
{"title": "FLiT: Cross-platform floating-point result-consistency tester and workload\n", "abstract": " Understanding the extent to which computational results can change across platforms, compilers, and compiler flags can go a long way toward supporting reproducible experiments. In this work, we offer the first automated testing aid called FLiT (Floating-point Litmus Tester) that can show how much these results can vary for any user-given collection of computational kernels. Our approach is to take a collection of these kernels, disperse them across a collection of compute nodes (each with a different architecture), have them compiled and run, and bring the results to a central SQL database for deeper analysis. Properly conducting these activities requires a careful selection (or design) of these kernels, input generation methods for them, and the ability to interpret the results in meaningful ways. The results in this paper are meant to inform two different communities: (a) those interested in seeking higher performance\u00a0\u2026", "num_citations": "20\n", "authors": ["1830"]}
{"title": "Pynamic: the python dynamic benchmark\n", "abstract": " Python is widely used in scientific computing to facilitate application development and to support features such as computational steering. Making full use of some of Python's popular features, which improve programmer productivity, leads to applications that access extremely high numbers of dynamically linked libraries (DLLs). As a result, some important Python-based applications severely stress a system's dynamic linking and loading capabilities and also cause significant difficulties for most development environment tools, such as debuggers. Furthermore, using the Python paradigm for large scale MPI-based applications can create significant file IO and further stress tools and operating systems. In this paper, we present Pynamic, the first benchmark program to support configurable emulation of a wide-range of the DLL usage of Python-based applications for large scale systems. Pynamic has already\u00a0\u2026", "num_citations": "17\n", "authors": ["1830"]}
{"title": "OpenMP tools interface: synchronization information for data race detection\n", "abstract": " When it comes to data race detection, complete information about synchronization, concurrency and memory accesses is needed. This information might be gathered at various levels of abstraction. For best results regarding accuracy this information should be collected at the abstraction level of the parallel programming paradigm. With the latest preview of the OpenMP specification, a tools interface (OMPT) was added to OpenMP. In this paper we discuss whether the synchronization information provided by OMPT is sufficient to apply accurate data race analysis for OpenMP applications. We further present some implementation details and results for our data race detection tool called Archer which derives the synchronization information from OMPT.", "num_citations": "11\n", "authors": ["1830"]}
{"title": "Multi-level analysis of compiler-induced variability and performance tradeoffs\n", "abstract": " Successful HPC software applications are long-lived. When ported across machines and their compilers, these applications often produce different numerical results, many of which are unacceptable. Such variability is also a concern while optimizing the code more aggressively to gain performance. Efficient tools that help locate the program units (files and functions) within which most of the variability occurs are badly needed, both to plan for code ports and to root-cause errors due to variability when they happen in the field. In this work, we offer an enhanced version of the open-source testing framework FLiT to serve these roles. Key new features of FLiT include a suite of bisection algorithms that help locate the root causes of variability. Another added feature allows an analysis of the tradeoffs between performance and the degree of variability. Our new contributions also include a collection of case studies. Results\u00a0\u2026", "num_citations": "8\n", "authors": ["1830"]}
{"title": "Statistical fault detection for parallel applications with AutomaDeD\n", "abstract": " Today's largest systems have over 100,000 cores, with million-core systems expected over the next few years. The large component count means that these systems fail frequently and often in very complex ways, making them difficult to use and maintain. While prior work on fault detection and diagnosis has focused on faults that significantly reduce system functionality, the wide variety of failure modes in modern systems makes them likely to fail in complex ways that impair system performance but are difficult to detect and diagnose. This paper presents AutomaDeD, a statistical tool that models the timing behavior of each application task and tracks its behavior to identify any abnormalities. If any are observed, AutomaDeD can immediately detect them and report to the system administrator the task where the problem began. This identification of the fault's initial manifestation can provide administrators with valuable insight into the fault's root causes, making it significantly easier and cheaper for them to understand and repair it. Our experimental evaluation shows that AutomaDeD detects a wide range of faults immediately after they occur 80% of the time, with a low false-positive rate. Further, it identifies weaknesses of the current approach that motivate future research.", "num_citations": "8\n", "authors": ["1830"]}
{"title": "Diagnosis of Performance Faults in LargeScale MPI Applications via Probabilistic Progress-Dependence Inference\n", "abstract": " Debugging large-scale parallel applications is challenging. Most existing techniques provide little information about failure root causes. Further, most debuggers significantly slow down program execution, and run sluggishly with massively parallel applications. This paper presents a novel technique that scalably infers the tasks in a parallel program on which a failure occurred, as well as the code in which it originated. Our technique combines scalable runtime analysis with static analysis to determine the least-progressed task(s) and to identify the code lines at which the failure arose. We present a novel algorithm that infers probabilistically progress dependence among MPI tasks using a globally constructed Markov model that represents tasks' control-flow behavior. In comparison to previous work, our algorithm infers more precisely the least-progressed task. We combine this technique with static backward slicing\u00a0\u2026", "num_citations": "7\n", "authors": ["1830"]}
{"title": "Dynamic binary instrumentation and data aggregation on large scale systems\n", "abstract": " Dynamic binary instrumentation for performance analysis on large scale architectures such as the IBM Blue Gene/L system (BG/L) poses unique challenges. Their unprecedented scale and often limited OS support require new mechanisms to organize binary instrumentation, to interact with the target application, and to collect the resulting data.               We describe the design and current status of a new implementation of the Dynamic Probe Class Library (DPCL) API for large scale systems. DPCL provides an easy to use layer for dynamic instrumentation on parallel MPI applications based on the DynInst dynamic instrumentation library for sequential platforms. Our work includes modifying DynInst to control instrumentation from remote I/O nodes and porting DPCL\u2019s communication for performance data collection to use MRNet, a tree-based overlay network that (TBON) supports scalable multicast and data reduction\u00a0\u2026", "num_citations": "7\n", "authors": ["1830"]}
{"title": "Enabling rapid COVID-19 small molecule drug design through scalable deep learning of generative models\n", "abstract": " We improved the quality and reduced the time to produce machine learned models for use in small molecule antiviral design. Our globally asynchronous multi-level parallel training approach strong scales to all of Sierra with up to 97.7% efficiency. We trained a novel, character-based Wasserstein autoencoder that produces a higher quality model trained on 1.613 billion compounds in 23 minutes while the previous state of the art takes a day on 1 million compounds. Reducing training time from a day to minutes shifts the model creation bottleneck from computer job turnaround time to human innovation time. Our implementation achieves 318 PFLOPs for 17.1% of half-precision peak. We will incorporate this model into our molecular design loop enabling the generation of more diverse compounds; searching for novel, candidate antiviral drugs improves and reduces the time to synthesize compounds to be tested in\u00a0\u2026", "num_citations": "6\n", "authors": ["1830"]}
{"title": "Statuner: Efficient tuning of cuda kernels parameters\n", "abstract": " CUDA programmers often need to decide the block size to use for a kernel launch that yields the lowest execution time. However, existing models to predict the best block size are not always accurate and involve a lot of manual effort from programmers. We identify a list of static metrics that can be used to characterize a kernel and build a Support Vector Machine (SVM) classifier model to predict block size that can be used in a kernel launch to minimize execution time. We use a set of kernels to train our model based on these identified static metrics and compare its predictions with the well-known NVIDIA tool called Occupancy Calculator on test kernels. Our model is able to predict block size that gives average error of 4.4% in comparison to Occupancy Calculator that gives error of 6.6%. Our model\u2014called STATuner\u2014requires no trial runs of the kernel and lesser effort compared to Occupancy Calculator.", "num_citations": "6\n", "authors": ["1830"]}
{"title": "Thread-local concurrency: a technique to handle data race detection at programming model abstraction\n", "abstract": " With greater adoption of various high-level parallel programming models to harness on-node parallelism, accurate data race detection has become more crucial than ever. However, existing tools have great difficulty spotting data races through these high-level models, as they primarily target low-level concurrent execution models (eg, concurrency expressed at the level of POSIX threads). In this paper, we propose a novel technique to accurately detect those data races that can occur at higher levels of concurrent execution. The core idea of our technique is to introduce the general concept of Thread-Local Concurrency (TLC) as a new way to translate the concurrency expressed by a high-level programming paradigm into the low execution level understood by the existing tools. Specifically, we extend the definition of vector clocks to allow the existing state-of-the-art race detectors to recognize those races that occur\u00a0\u2026", "num_citations": "5\n", "authors": ["1830"]}
{"title": "Lessons learned from implementing OMPD: a debugging interface for openMP\n", "abstract": " With complex codes moving to systems of increasing on-node parallelism using OpenMP, debugging these codes is becoming increasingly challenging. While debuggers can significantly aid programmers, existing ones support OpenMP at a low system-thread level, reducing their effectiveness. The previously published draft for a standard OpenMP debugging interface (OMPD) is supposed to enable the debuggers to raise their debugging abstraction to the conceptual levels of OpenMP by mediating the tools and OpenMP runtime library. In this paper, we present our experiences and the issues that we have found on implementing an OMPD library prototyp for a commonly used OpenMP runtime and a parallel debugger.", "num_citations": "4\n", "authors": ["1830"]}
{"title": "Scalable parallel debugging via loop-aware progress dependence analysis\n", "abstract": " Debugging large-scale parallel applications is challenging, as this often requires extensive manual intervention to isolate the origin of errors. For many bugs in scientific applications, where parallel tasks progress forward in a coordinated fashion, finding tasks that progressed the least can significantly reduce the time to isolate error root causes. We present a novel run-time technique, the loop-aware progressdependence analysis, that improves the accuracy of identifying the least-progressed (LP) task (s). Our technique extends an existing analysis technique (AutomaDeD) to detect LP task (s) even when the error arises within complex loop structures. Our preliminary evaluation shows that it accurately finds LP task (s) on several hangs, where the previous technique failed.", "num_citations": "4\n", "authors": ["1830"]}
{"title": "Keeping science on keel when software moves\n", "abstract": " An approach to reproducibility problems related to porting software across machines and compilers.", "num_citations": "3\n", "authors": ["1830"]}
{"title": "A Scalable Prescriptive Parallel Debugging Model\n", "abstract": " Debugging is a critical step in the development of any parallel program. However, the traditional interactive debugging model, where users manually step through code and inspect their application, does not scale well even for current supercomputers due its centralized nature. While lightweight debugging models, which have been proposed as an alternative, scale well, they can currently only debug a subset of bug classes. We therefore propose a new model, which we call prescriptive debugging, to fill this gap between these two approaches. This user-guided model allows programmers to express and test their debugging intuition in a way that helps to reduce the error space. Based on this debugging model we introduce a prototype implementation embodying this model, the DySectAPI, allowing programmers to construct probe trees for automatic, event-driven debugging at scale. In this paper we introduce the\u00a0\u2026", "num_citations": "3\n", "authors": ["1830"]}
{"title": "Measuring flops using hardware performance counter technologies on lc systems\n", "abstract": " FLOPS (FLoating-point Operations Per Second) is a commonly used performance metric for scientific programs that rely heavily on floating-point (FP) calculations. The metric is based on the number of FP operations rather than instructions, thereby facilitating a fair comparison between different machines. A well-known use of this metric is the LINPACK benchmark that is used to generate the Top500 list. It measures how fast a computer solves a dense N by N system of linear equations Ax= b, which requires a known number of FP operations, and reports the result in millions of FP operations per second (MFLOPS). While running a benchmark with known FP workloads can provide insightful information about the efficiency of a machine's FP pipelines in relation to other machines, measuring FLOPS of an arbitrary scientific application in a platform-independent manner is nontrivial. The goal of this paper is twofold. First, we explore the FP microarchitectures of key processors that are underpinning the LC machines. Second, we present the hardware performance monitoring counter-based measurement techniques that a user can use to get the native FLOPS of his or her program, which are practical solutions readily available on LC platforms. By nature, however, these native FLOPS metrics are not more\u00bb", "num_citations": "3\n", "authors": ["1830"]}
{"title": "ExaWorks: Workflows for Exascale\n", "abstract": " Exascale computers will offer transformative capabilities to combine data-driven and learning-based approaches with traditional simulation applications to accelerate scientific discovery and insight. These software combinations and integrations, however, are difficult to achieve due to challenges of coordination and deployment of heterogeneous software components on diverse and massive platforms. We present the ExaWorks project, which can address many of these challenges: ExaWorks is leading a co-design process to create a workflow software development Toolkit (SDK) consisting of a wide range of workflow management tools that can be composed and interoperate through common interfaces. We describe the initial set of tools and interfaces supported by the SDK, efforts to make them easier to apply to complex science challenges, and examples of their application to exemplar cases. Furthermore, we discuss how our project is working with the workflows community, large computing facilities as well as HPC platform vendors to sustainably address the requirements of workflows at the exascale.", "num_citations": "2\n", "authors": ["1830"]}
{"title": "ArcherGear: data race equivalencing for expeditious HPC debugging\n", "abstract": " There is growing uptake of shared memory parallelism in high performance computing, and this has increased the need for data race checking during the creation of new parallel codes or parallelizing existing sequential codes. While race checking concepts and implementations have been around for many concurrency models, including tasking models such as Cilk and PThreads (eg, the Thread Sanitizer tool), practically usable race checkers for other APIs such as OpenMP have been lagging. For example, the OpenMP parallelization of an important library (namely Hypre) was initially unsuccessful due to inexplicable nondeterminism introduced when the code was optimized, and later root-caused to a race by the then recently developed OpenMP race checker Archer [2]. The open-source Archer now enjoys significant traction within several organizations.", "num_citations": "2\n", "authors": ["1830"]}
{"title": "Dysectapi: Scalable prescriptive debugging\n", "abstract": " We present the DySectAPI, a tool that allow users to construct probe trees for automatic, event-driven debugging at scale. The traditional, interactive debugging model, whereby users manually step through and inspect their application, does not scale well even for current supercomputers. While lightweight debugging models scale well, they can currently only debug a subset of bug classes. DySectAPI fills the gap between these two approaches with a novel user-guided approach. Using both experimental results and analytical modeling we show how DySectAPI scales and can run with a low overhead on current systems.", "num_citations": "2\n", "authors": ["1830"]}
{"title": "Generalizable coordination of large multiscale workflows: challenges and learnings at scale\n", "abstract": " The advancement of machine learning techniques and the heterogeneous architectures of most current supercomputers are propelling the demand for large multiscale simulations that can automatically and autonomously couple diverse components and map them to relevant resources to solve complex problems at multiple scales. Nevertheless, despite the recent progress in workflow technologies, current capabilities are limited to coupling two scales. In the first-ever demonstration of using three scales of resolution, we present a scalable and generalizable framework that couples pairs of models using machine learning and in situ feedback. We expand upon the massively parallel Multiscale Machine-Learned Modeling Infrastructure (MuMMI), a recent, award-winning workflow, and generalize the framework beyond its original design. We discuss the challenges and learnings in executing a massive multiscale\u00a0\u2026", "num_citations": "1\n", "authors": ["1830"]}
{"title": "Mcem: Multi-level cooperative exception model for hpc workflows\n", "abstract": " As fault recovery mechanisms become increasingly important in HPC systems, the need for a new recovery model for workflows on these systems grows as well. While the traditional approach in which each system component attempts its own independent recovery after a fault works well at each individual application level, this model does not scale to the new level demanded by workflow-level exception handling. As today's workflows must often run many components simultaneously (eg, workflow manager components, many simulation instances, data analytics etc), any uncoordinated model can quickly result in redundant or contradictory recovery actions. In this paper, we propose a multi-level cooperative exception model (MCEM), a novel exception handling approach that solves this coordination challenge for HPC workflows. We present our model, describe how it can be applied to common system faults and\u00a0\u2026", "num_citations": "1\n", "authors": ["1830"]}
{"title": "2 Performance Analysis and Debugging Tools at Scale\n", "abstract": " While high-performance computing (HPC) systems are often assessed in terms of their raw computing power, it is their ability to efficiently and correctly perform large-scale scientific simulations that makes them invaluable tools for modern scientific research. Performance tools and debuggers are critical components that enable computational scientists to fully exploit the computing power of these systems. Performance tools help developers of applications, frameworks, and libraries use HPC platforms efficiently by collecting information to analyze and model the performance of their code on a given architecture and they help to identify costly program regions and quantify their impact on code performance and scalability.Debugging tools help developers to identify and correct a variety of correctness problems in applications including logic errors, data corruption, and parallel race conditions. Debugging tools enable\u00a0\u2026", "num_citations": "1\n", "authors": ["1830"]}
{"title": "Testing Infrastructure for OpenMP Debugging Interface Implementations\n", "abstract": " With complex codes moving to systems of greater on-node parallelism using OpenMP, debugging these codes is becoming increasingly challenging. While debuggers can significantly aid programmers, OpenMP support within existing debuggers is either largely ineffective or unsustainable. The OpenMP tools working group is working to specify a debugging interface for the OpenMP standard to be implemented by every OpenMP runtime implementation. To increase the acceptance of this interface by runtime implementers and to ensure the quality of these interface implementations, availability of a common testing infrastructure compatible with any runtime implementation is critical. In this paper, we present a promising software architecture for such a testing infrastructure.", "num_citations": "1\n", "authors": ["1830"]}
{"title": "A High Performance Computing Scheduling and Resource Management Primer\n", "abstract": " This primer introduces the concepts of High Performance Computing, scheduling, and resource management. It focuses first on the state of the art and then introduces the issues under consideration in the design of a next generation resource manager known as Flux. The Flux design addresses not just the challenges of resource management at a much larger scale of systems, but it also enables and promotes a much broader and richer model that invites collaboration with areas whose requirements cannot be met in current systems.The term High performance computing (HPC) as it is commonly used refers to a category of computing systems that combine computing power from multiple units to deliver vastly higher performance than desktop computers can provide. HPC is distinguished not just by computing power. HPC systems are designed to run a variety of large applications for extended periods of time in order to solve complex problems. HPC is distinguished from web servers in that they dedicate requested resources to run applications provided by users. Current HPC systems are distinguished from cloud services like AWS in that they generally provide a single operating system with a runtime environment managed by the facility.", "num_citations": "1\n", "authors": ["1830"]}
{"title": "An optimal algorithm for extreme scale job launching\n", "abstract": " All distributed software systems execute a bootstrapping phase upon instantiation. During this phase, the composite processes of the system are deployed onto a set of computational nodes and initialization information is disseminated amongst these processes. However, with the growing trend toward high-end systems with very large numbers of compute cores, the bootstrapping phase increasingly is becoming a bottleneck. This presents significant challenges to several key elements of extreme-scale machines: the usefulness of interactive run-time tools and the efficiency of newly emerging computational models such as many-task computing and uncertainty quantification runs are increasingly subject to the inefficient bootstrapping problem. In this paper, we propose a novel algorithm that determines an optimal bootstrapping strategy. Our algorithm is based on a process launch performance model and finds the\u00a0\u2026", "num_citations": "1\n", "authors": ["1830"]}
{"title": "Vision and Plan for a Next Generation Resource Manager\n", "abstract": " Resource Management (RM) software is critical for High Performance Computing (HPC). It is the centerpiece that allows efficient execution of HPC applications while providing an HPC center with the main means to maximize the utilization of its computing resources. However, several growing trends make even the best-in-breed RM software largely ineffective. As numbers and types of compute cores of HPC systems continue to grow, key RM challenges associated only with today\u2019s capability-class machines are becoming increasingly pervasive for all computing resources including commodity Linux clusters. The challenges include having to provide extreme scalability, low noise, fault tolerance, and heterogeneity management while under a strict power budget. ned-review: Citation needed. The shortcomings identified in this section form the core justification for why NGRM is needed, so you should be prepared to back them up with references.In addition, greater difficulties in code development on larger systems have begun to impose far more complex requirements on the RM. For example, without adequate RM support, debugging, tuning, testing and verification of the applications have become too difficult and time-consuming for end-users. The next-generation code development environments require the RM to provide effective mechanisms to support the reproducible results of program execution, to provide accurate correlations between user-level errors and system-level events, and to integrate and accelerate a rich set of scalable tools.", "num_citations": "1\n", "authors": ["1830"]}