{"title": "Evaluating models for model-based debugging\n", "abstract": " Developing model-based automatic debugging strategies has been an active research area for several years, with the aim of locating defects in a program by utilising fully automated generation of a model of the program from its source code. We provide an overview of current techniques in model-based debugging and assess strengths and weaknesses of the individual approaches. An empirical comparison is presented that investigates the relative accuracy of different models on a set of test programs and fault assumptions, showing that our abstract interpretation based model provides high accuracy at significantly less computational effort than slightly more accurate techniques. We compare a range of model-based debugging techniques with other state-of-the-art automated debugging approaches and outline possible future developments in automatic debugging using model-based reasoning as the central\u00a0\u2026", "num_citations": "103\n", "authors": ["1834"]}
{"title": "Model-based debugging\u2013state of the art and future challenges\n", "abstract": " A considerable body of work on model-based software debugging (MBSD) has been published in the past decade. We summarise the underlying ideas and present the different approaches as abstractions of the concrete semantics of the programming language. We compare the model-based framework with other well-known Automated Debugging approaches and present open issues, challenges and potential future directions of MBSD.", "num_citations": "71\n", "authors": ["1834"]}
{"title": "Formalising Natural Language Specifications Using a Cognitive Linguistics/Configuration Based Approach\n", "abstract": " This paper addresses the problem of transforming business specifications written in natural language into formal models suitable for use in information systems development. It proposes a method for transforming controlled natural language specifications based on the Semantics of Business Vocabulary and Business Rules standard. This approach is unique in combining techniques from Model-Driven Engineering (MDE), Cognitive Linguistics, and Knowledge-based Configuration, which allows the reliable semantic processing of specifications and integration with existing MDE tools to improve productivity, quality, and time-to-market in software development. The method first learns the vocabulary of the specification from glossary-like definitions then parses the rules of the specification and outputs the resulting formal SBVR model. Both aspects of the method are tested separately, with the system correctly learning\u00a0\u2026", "num_citations": "61\n", "authors": ["1834"]}
{"title": "Refining spectrum-based fault localization rankings\n", "abstract": " Spectrum-based fault localization is a statistical technique that aims at helping software developers to find faults quickly by analyzing abstractions of program traces to create a ranking of most probable faulty components (eg, program statements). Although spectrum-based fault localization has been shown to be effective, its diagnostic accuracy is inherently limited, since the semantics of components are not considered. In particular, components that exhibit identical execution patterns cannot be distinguished. To enhance its diagnostic quality, in this paper, we combine spectrum-based fault localization with a model-based debugging approach based on abstract interpretation within a framework coined Deputo. The model-based approach is used to refine the ranking obtained from the spectrum-based method by filtering out those components that do not explain the observed failures when the program's semantics is\u00a0\u2026", "num_citations": "48\n", "authors": ["1834"]}
{"title": "Prioritising model-based debugging diagnostic reports\n", "abstract": " Model-based debugging has proved successful as a tool to guide automated debugging efforts, but the technique may suffer from large result sets in practice, since no means to rank or discriminate between the returned candidate explanations are available. We present a unique combination of model-and spectrum-based fault localisation approach to rank explanations and show that the combined framework outperforms the individual approaches as well as other state of the art automated debugging mechanisms.", "num_citations": "33\n", "authors": ["1834"]}
{"title": "Model-based debugging using multiple abstract models\n", "abstract": " This paper introduces an automatic debugging framework that relies on model-based reasoning techniques to locate faults in programs. In particular, model-based diagnosis, together with an abstract interpretation based conflict detection mechanism is used to derive diagnoses, which correspond to possible faults in programs. Design information and partial specifications are applied to guide a model revision process, which allows for automatic detection and correction of structural faults.", "num_citations": "33\n", "authors": ["1834"]}
{"title": "Service composition as generative constraint satisfaction\n", "abstract": " The ability to build new (complex) services by composing existing services is one of the key benefits of the Service Oriented Architecture paradigm. Existing approaches to automate composition requires pre-planning or prediction of the number of required services, making them unsuitable in dynamic composition scenarios. To address this gap, we present a consistency-based service composition approach, where composition problems are modeled in a generative constraint-based formalism. We illustrate how the configuration of service processes differs from established constraint-based configuration techniques and develop an algorithm to synthesis valid service process compositions. We also show that our technique scales well to non-trivial problems.", "num_citations": "32\n", "authors": ["1834"]}
{"title": "Rule-based peer-to-peer framework for decentralised real-time service oriented architectures\n", "abstract": " Modularity has been a key issue in the design and development of modern embedded Real-Time Software Systems (RTS) where modularity enables flexibility with respect to changes in platform, environment, and requirements, as well as reuse. In distributed RTS, similar ideas have led to the adoption of Commercial Off-The-Shelf (COTS) components integrated via Service-Oriented Architecture (SOA) principles and technologies that are already well-established in business-oriented information systems. However, current SOA technologies for orchestration, such as Enterprise Service Busses, do not meet strict time-dependent constraints on scalability and latency required by RTS.We present a novel approach to RTS development where the orchestration of real-time processes is decentralised among the services within a fully distributed rule-driven process framework. Our framework wraps around COTS\u00a0\u2026", "num_citations": "29\n", "authors": ["1834"]}
{"title": "Extending diagnosis to debug programs with exceptions\n", "abstract": " Even with modern software development methodologies, the actual debugging of source code, i.e., location and identification of errors in the program when errant behavior is encountered during testing, remains a crucial part of software development. To apply model-based diagnosis techniques which have long been state of the art in hardware diagnosis, for automatic debugging a model of a given program must be automatically created from the source code. This work describes a model that reflects the sequential execution semantics of the Java language, including exceptions and unstructured control flow, thereby providing unprecedented scope in the application of model-based diagnosis to programs. Notably, this approach omits the strict view of a component representing one statement of earlier work and provides a more flexible mapping from code to model.", "num_citations": "27\n", "authors": ["1834"]}
{"title": "Abstract interpretation of programs for model-based debugging\n", "abstract": " Developing model-based automatic debugging strategies has been an active research area for several years. We present a model-based debugging approach that is based on Abstract Interpretation, a technique borrowed from program analysis. The Abstract Interpretation mechanism is integrated with a classical model-based reasoning engine. We test the approach on sample programs and provide the first experimental comparison with earlier models used for debugging. The results show that the Abstract Interpretation based model provides more precise explanations than previous models or standard non-model based approaches.", "num_citations": "23\n", "authors": ["1834"]}
{"title": "Diagnosis of service failures by trace analysis with partial knowledge\n", "abstract": " The identification of the source of a fault (\u201cdiagnosis\u201d) of orchestrated Web service process executions is a task of growing importance, in particular in automated service composition scenarios. If executions fail because activities of the process do not behave as intended, repair mechanisms are envisioned that will try re-executing some activities to recover from the failure. We present a diagnosis method for identifying incorrect activities in service process executions. Our method is novel both in that it does not require exact behavioral models for the activities and that its accuracy improves upon dependency-based methods. Observations obtained from partial executions and re-executions of a process are exploited. We formally characterize the diagnosis problem and develop a symbolic encoding that can be solved using constraint solvers. Our evaluation demonstrates that the framework yields superior\u00a0\u2026", "num_citations": "22\n", "authors": ["1834"]}
{"title": "A conceptual framework for large-scale ecosystem interoperability and industrial product lifecycles\n", "abstract": " One of the most significant challenges in information system design is the constant and increasing need to establish interoperability between heterogeneous software systems at increasing scale. The automated translation of data between the data models and languages used by information ecosystems built around official or de facto standards is best addressed using model-driven engineering techniques, but requires handling both data and multiple levels of metadata within a single model. Standard modelling approaches are generally not built for this, compromising modelling outcomes. We establish the SLICER conceptual framework built on multilevel modelling principles and the differentiation of basic semantic relations (such as specialisation, instantiation, specification and categorisation) that dynamically structure the model. Moreover, it provides a natural propagation of constraints over multiple levels of\u00a0\u2026", "num_citations": "20\n", "authors": ["1834"]}
{"title": "Approximate modeling for debugging of program loops\n", "abstract": " Developing model-based automatic debugging strategies has been an active research area for several years. We analyze shortcomings of previous modeling approaches when dealing with objectoriented languages and present a revised modeling approach. We employ Abstract Interpretation, a technique borrowed from program analysis, to improve the debugging of programs including loops, recursive procedures, and heap data structures. Together with heuristic model refinement, our approach delivers superior results than the previous models. The principle of our approach is demonstrated on a set of examples.", "num_citations": "19\n", "authors": ["1834"]}
{"title": "Representing network knowledge using provenance-aware formalisms for cyber-situational awareness\n", "abstract": " Due to the volume, variety, and veracity of network data available, information fusion and reasoning techniques are needed to support network analysts\u2019 cyber-situational awareness. These techniques rely on formal knowledge representation to define the network semantics with data provenance at various levels of granularity. To this end, this paper proposes the Communication Network Topology and Forwarding Ontology, a state-of-the-art ontology that enables the formal, unified representation of complex network concepts regardless of the type of the data source. The implementation of this ontology allows network analysts to represent expert knowledge and query network data fused from disparate data sources.", "num_citations": "15\n", "authors": ["1834"]}
{"title": "Diagnosability analysis without fault models\n", "abstract": " This paper addresses the problem of diagnosability analysis, which allows a system designer to anticipate the performance of a model based diagnosis (MBD) algorithm for a given system. Such analysis requires a formal description of the system behavior, called model, which can be very difficult to establish, especially when faults occur in the system. Despite this, all known diagnosability frameworks rely on some specification of the system behavior under the absence and presence of faults.This paper presents a diagnosability analysis algorithm related to a diagnosis approach in which the model of faulty system components is unspecified. Diagnosis is based on the description of the normal behavior as well as a decomposition of the system into components, and assesses which components cannot be behaving normally. Diagnosability is defined in a way that copes with the capabilities of such diagnosis approaches. An algorithm for checking diagnosability incrementally or hierarchically is described and illustrated.", "num_citations": "15\n", "authors": ["1834"]}
{"title": "Weakening faithfulness: some heuristic causal discovery algorithms\n", "abstract": " We examine the performance of some standard causal discovery algorithms, both constraint-based and score-based, from the perspective of how robust they are against (almost) failures of the Causal Faithfulness Assumption. For this purpose, we make only the so-called Triangle-Faithfulness assumption, which is a fairly weak consequence of the Faithfulness assumption, and otherwise allows unfaithful distributions. In particular, we allow violations of Adjacency-Faithfulness and Orientation-Faithfulness. We show that the (conservative) PC algorithm, a representative constraint-based method, can be made more robust against unfaithfulness by incorporating elements of the GES algorithm, a representative score-based method; similarly, the GES algorithm can be made less error-prone by incorporating elements of the conservative PC algorithm. As our simulations demonstrate, the increased robustness\u00a0\u2026", "num_citations": "13\n", "authors": ["1834"]}
{"title": "A declarative framework for work process configuration\n", "abstract": " This article describes the technical principles and representation of a constraint-based configuration method for work processes. Methods developed for the configuration of modular systems comprising components have traditionally adopted a representation where the properties and compatibility requirements are expressed as constraints associated with individual components. However, this representation does not accurately capture constraints on paths and subprocesses and is therefore unsuitable for process configuration. This article extends established constraint-based configuration methods with a constraint language for specifying properties of execution paths in work processes. A framework for semiautomated process customization is presented. It integrates the extended constraint language with a metamodel of the work processes in an organization and allows to adapt generic work processes to fit the\u00a0\u2026", "num_citations": "13\n", "authors": ["1834"]}
{"title": "Construction delay risk taxonomy, associations and regional contexts: A systematic review and meta-analysis\n", "abstract": " PurposeThe purpose of this paper is to systematically develop a delay risk terminology and taxonomy. This research also explores two external and internal dimensions of the taxonomy to determine how much the taxonomy as a whole or combinations of its elements are generalisable.Design/methodology/approachUsing mixed methods research, this systematic literature review incorporated data from 46 articles to establish delay risk terminology and taxonomy. Qualitative data of the top 10 delay risks identified in each article were coded based on the grounded theory and constant comparative analysis using a three-stage coding approach. Word frequency analysis and cross-tabulation were used to develop the terminology and taxonomy. Association rules within the taxonomy were also explored to define risk paths and to unmask associations among the risks.FindingsIn total, 26 delay risks were identified and\u00a0\u2026", "num_citations": "12\n", "authors": ["1834"]}
{"title": "Automated Reasoning over Provenance-Aware Communication Network Knowledge in Support of Cyber-Situational Awareness\n", "abstract": " Cyber-situational awareness is crucial to applications such as network monitoring and management, vulnerability assessment, and defense. To gain improved cyber-situational awareness, analysts can benefit from automated reasoning-based frameworks. However, such frameworks would require the processing of enormous amounts of network data, which are characterized by syntactic variability. The formal representation of networking concepts, their properties, and interrelations using RDF can narrow the interoperability gaps between routing information and network semantics. Formal knowledge representation also enables automated reasoning, which facilitates network knowledge discovery by making implicit statements explicit. However, capturing and reasoning over the provenance of RDF statements, which is essential to build analysts\u2019 trust in automated support tools, is not trivial. This paper\u00a0\u2026", "num_citations": "12\n", "authors": ["1834"]}
{"title": "On the application of software modelling principles on ISO 15926\n", "abstract": " The ISO 15926 standard was developed to facilitate the integration of data in support of the life-cycle activities and processes of process plants. It is currently used for the handover of design documents between those companies that design and engineer engineering equipment to those organisations responsible for their operation and maintenance within the oil & gas industry. Part 2 of the standard contains a generic data model which represents the core of the standard. In this paper we applied well established software modelling principles on this part to overcome some previously identified problems such as inconsistent terminology and complexity. The two main outcomes are:(1) a multi-level view on Part 2 that formalises some aspects of the current data model and (2) the simplification of the data model, lowering the barriers to the adoption of ISO 15926 in industry.", "num_citations": "12\n", "authors": ["1834"]}
{"title": "On-the-fly change propagation for the co-evolution of business processes\n", "abstract": " In large organisations multiple stakeholders may modify the same business process. This paper addresses the problem when stakeholders perform changes on process views which become inconsistent with the business process and other views. Related work addressing this problem is based on execution trace analysis which is performed in a post-analysis phase and can be complex when dealing with large business process models. In this paper we propose a design-based approach that can efficiently check consistency criteria and propagate changes on-the-fly from a process view to its reference process and related process views. The technique is based on consistent specialisation of business processes and supports the data- and control flow perspective. This technique reduces the steps performed in the evolution of business processes by embedding the consistency checks and change\u00a0\u2026", "num_citations": "11\n", "authors": ["1834"]}
{"title": "Ontology-based process modelling for design optimisation support\n", "abstract": " The integration and reuse of simulation and process information is not wellintegrated into current development practices. We introduce a framework to integrate Multidisciplinary Design Optimisation (MDO) processes using ontological engineering. Based on a multi-disciplinary design scenario drawn from the automotive industry, we illustrate how semantic integration of process, artifact and simulation models can contribute to more effective optimisation-driven development. Ontology standards are evaluated to assess where existing work may be applicable and which aspects of MDO processes require further extensions.", "num_citations": "11\n", "authors": ["1834"]}
{"title": "Modeling programs with unstructured control flow for debugging\n", "abstract": " Even with modern software development methodologies, the actual debugging of source code, i.e., location and identification of errors in the program when errant behavior is encountered during testing, remains a crucial part of software development. To apply model-based diagnosis techniques, which have long been state of the art in hardware diagnosis, for automatic debugging, a model of a given program must be automatically created from the source code. This work describes a model that reflects the execution semantics of the Java language, including exceptions and unstructured control flow, thereby providing unprecedented scope in the application of model-based diagnosis to programs. Besides the structural model building process, a behavioral description of some of the model components is given. Finally, impacts of the modeling decisions on the diagnostic process are considered.", "num_citations": "11\n", "authors": ["1834"]}
{"title": "A conceptual framework for large-scale ecosystem interoperability\n", "abstract": " One of the most significant challenges in information system design is the constant and increasing need to establish interoperability between heterogeneous software systems at increasing scale. The automated translation of data between the data models and languages used by information ecosystems built around official or de facto standards is best addressed using model-driven engineering techniques, but requires handling both data and multiple levels of metadata within a single model. Standard modelling approaches are generally not built for this, compromising modelling outcomes. We establish the SLICER conceptual framework built on multilevel modelling principles and the differentiation of basic semantic relations that dynamically structure the model and can capture existing multilevel notions. Moreover, it provides a natural propagation of constraints over multiple levels of instantiation.", "num_citations": "10\n", "authors": ["1834"]}
{"title": "Configuring services and processes\n", "abstract": " Configuring Services and Processes \u2014 Aalto University's research portal Skip to main navigation Skip to search Skip to main content Aalto University's research portal Logo Accessibility statement English Suomi Home Profiles Research output Datasets Projects Prizes Activities Press / Media Infrastructure Research Units Impacts Search by expertise, name or affiliation Configuring Services and Processes Juha Tiihonen, Wolfgang Mayer, Markus Stumptner, Mikko Heiskala Research output: Chapter in Book/Report/Conference proceeding \u203a Chapter \u203a Scientific \u203a peer-review 2 Citations (Scopus) Overview Original language English Title of host publication Knowledge-based Configuration -- From Research to Business Cases Editors Alexander Felfernig, Lothar Hotz, Claire Bagley, Juha Tiihonen Place of Publication Waltham, MA, USA Pages 251-260 Publication status Published - 2014 MoE publication type A3 Part of \u2026", "num_citations": "10\n", "authors": ["1834"]}
{"title": "On solving complex rack configuration problems using CSP methods\n", "abstract": " Constraint Satisfaction Techniques have been widely applied to the configuration of complex systems in various domain. While much progress has been achieved on algorithms and theoretical characterisations of special cases that can be solved efficiently, many of these results make strong assumptions. In this paper we investigate different modelling and search techniques to assess their suitability for on-line configuration of a complex prototypical configuration problem. We found that CSP solving techniques perform poorly on our problem, while local search and repair techniques may achieve good performance while permitting simpler constraint formalisms.", "num_citations": "10\n", "authors": ["1834"]}
{"title": "Semantic Service Discovery by Consistency-Based Matchmaking\n", "abstract": " Automated discovery of web services with desired functionality is an active research area because of its role in realising the envisioned advantages of the semantic web, such as functional reuse and automated composition. Existing approaches generally determine matches by inferring subsumption relationships between a request and a service specification, but may return poor results if service profiles are overspecified or provide only partial information. We present a two-staged consistency-based matchmaking approach where services that potentially match the request are identified in the first stage, these services are queried for concrete information, and finally this information is used to determine the matches. We evaluate our matchmaking scheme in the context of the Discovery II and Simple Composition scenario proposed by the SWS Challenge group. Preliminary evaluation shows that our\u00a0\u2026", "num_citations": "10\n", "authors": ["1834"]}
{"title": "Semantic web service composition by consistency-based model refinement\n", "abstract": " Modelling complex processes in the service oriented architecture paradigm typically requires the composition of a number of simpler services to achieve a desired goal. We present a semantic service composition approach that is based on the use of UML activity diagrams as abstract specification language and propose a service selection process that combines both conceptual and instance-level analysis to locate suitable services. Model-driven principles allow to adhere to design requirements that would otherwise be difficult to incorporate informal composition frameworks. Explicit modelling of behaviour in the presence of failures permit to create workflows that are robust in case of service execution errors. This approach may allow to synthesise concrete service orchestrations that are superior to similar approaches purely based on type-based or conceptual matchmaking.", "num_citations": "10\n", "authors": ["1834"]}
{"title": "An architecture for establishing legal semantic workflows in the context of Integrated Law Enforcement\n", "abstract": " Traditionally the integration of data from multiple sources is done on an ad-hoc basis for each analysis scenario and application. This is a solution that is inflexible, incurs high costs, leads to \u201csilos\u201d that prevent sharing data across different agencies or tasks, and is unable to cope with the modern environment, where workflows, tasks, and priorities frequently change. Operating within the Data to Decision Cooperative Research Centre (D2D CRC), the authors are currently involved in the Integrated Law Enforcement Project, which has the goal of developing a federated data platform that will enable the execution of integrated analytics on data accessed from different external and internal sources, thereby providing effective support to an investigator or analyst working to evaluate evidence and manage lines of inquiries in the investigation. Technical solutions should also operate ethically, in compliance with\u00a0\u2026", "num_citations": "9\n", "authors": ["1834"]}
{"title": "A Knowledge-based Approach to the Configuration of Business Process Model Abstractions.\n", "abstract": " Methods for abstraction have been proposed to ease comprehension, monitoring, and validation of large processes and their running instances. To date, abstraction mechanisms have focused predominantly on structural aggregation, projection, and ad-hoc transformations.We propose an approach for configuration of process abstractions tailored to a specific abstraction goal expressed as constraints on the abstraction relation and process transformation operators. Our framework goes beyond simple structural aggregation and leverages domain-specific properties, taxonomies, meronymy, and flow criteria. In this paper we outline the constraint-based framework and its underlying inference procedure. We show that our approach can handle most of the common process analysis use cases.", "num_citations": "9\n", "authors": ["1834"]}
{"title": "On computing correct processes and repairs using partial behavioral models.\n", "abstract": " Diagnosis and repair of failed process executions is an important task for almost any process oriented application. Because in practice complete specifications of process activities are not available, diagnosis and repair methods for partial behavior models are of great importance. We show that if the assumption of complete behavioral models is lifted, basic diagnosis and repair problems reside on the second level of the Polynomial Hierarchy.", "num_citations": "9\n", "authors": ["1834"]}
{"title": "Models and tradeoffs in model-based debugging\n", "abstract": " Developing model-based automatic debugging strategies has been an active research area for several years that aims at locating defects in a program, utilising the fully automated generation of an declarative model of the program semantics from source code. We provide an overall comparison of past developments in model-based debugging and predicate abstraction based frameworks and assess strengths and weaknesses of the individual approaches. An empirical comparison is presented that investigates the relative accuracy of different models on a set of test programs and fault assumptions, showing that our abstract interpretation based model may provide a trade-off between accuracy and computational complexity. Further, compare selected model-based debugging techniques with other state-of-the-art automated debugging approaches and outline possible future developments in automatic debugging using model-based reasoning frameworks.", "num_citations": "9\n", "authors": ["1834"]}
{"title": "Debugging program loops using approximate modeling\n", "abstract": " Developing model-based automatic debugging strategies has been an active research area for several years. We analyze shortcomings of previous modeling approaches when dealing with objectoriented languages and present a revised modeling approach. We employ Abstract Interpretation, a technique borrowed from program analysis, to improve the debugging of programs including loops, recursive procedures, and heap data structures. Together with heuristic model refinement, our approach delivers superior results than the previous models. The principle of our approach is demonstrated on a running example.", "num_citations": "9\n", "authors": ["1834"]}
{"title": "Modellbasierte Diagnose von Java-Programmen Entwurf und Implementierung eines wertbasierten Modells\n", "abstract": " Diese Diplomarbeit hat zum Ziel, ein Modell f\u00fcr Java-Programme zu entwickeln, welches zum Debugging mittels modellbasierter Diagnose eingesetzt werden kann. Dabei werden insbesondere die objektorientierten Sprachelemente von Java ber\u00fccksichtigt. Die in der Arbeit entwickelten Modelle sind auf eine Untermenge der Java-Sprache beschr\u00e4nkt, da die Abarbeitungsreihenfolge der Anweisungen bereits w\u00e4hrend der Bildung der Modelle bekannt sein mu\u00df. Die vorliegende Arbeit betrachtet zun\u00e4chst ein Modell, welches alle Programmelemente als Komponenten und die ben\u00f6tigten bzw. ver\u00e4nderten Variablen als Verbindungen zwischen den Komponenten darstellt. Es erscheint daher nur f\u00fcr sehr einfache Programme anwendbar. Weniger einfache Programme k\u00f6nnten mit Erweiterungen dieses Modells behandelt werden, dabei erreichen die Modelle jedoch eine \u00fcberaus hohe Komplexit\u00e4t. Aufbauend auf dem ersten Modell wird ein weiteres Modell entwickelt, das im Aufbau dem ersten Modell gleicht, die Instanzvariablen von Objekten im Gegensatz zum ersten Modell jedoch gesondert betrachtet. Dadurch wird es m\u00f6glich, auch rekursive Methodenaufrufe, Arrays und Strings zu modellieren. Weiters wird die hohe Komplexit\u00e4t des ersten Modells vermieden. Ein weiterer Abschnitt dieser Arbeit widmet sich der Implementierung des zweiten Modells in Form eines Constraint-Netzwerkes.", "num_citations": "8\n", "authors": ["1834"]}
{"title": "SAT-based causal discovery under weaker assumptions\n", "abstract": " Using the flexibility of recently developed methods for causal discovery based on Boolean satisfiability (SAT) solvers, we encode a variety of assumptions that weaken the Faithfulness assumption. The encoding results in a number of SAT-based algorithms whose asymptotic correctness relies on weaker conditions than are standardly assumed. This implementation of a whole set of assumptions in the same platform enables us to systematically explore the effect of weakening the Faithfulness assumption on causal discovery. An important effect, suggested by simulation results, is that adopting weaker assumptions greatly alleviates the problem of conflicting constraints and substantially shortens solving time. As a result, SAT-based causal discovery is potentially more scalable under weaker assumptions.", "num_citations": "7\n", "authors": ["1834"]}
{"title": "Change Propagation and Conflict Resolution for the Co-Evolution of Business Processes\n", "abstract": " In large organizations, multiple stakeholders may modify the same business process. This paper addresses the problem when stakeholders perform changes on process views which become inconsistent with the business process and other views. Related work addressing this problem is based on execution trace analysis which is performed in a post-analysis phase and can be complex when dealing with large business process models. In this paper, we propose a design-based approach that can efficiently check consistency criteria and propagate changes on-the-fly from a process view to its reference process and related process views. The technique is based on consistent specialization of business processes and supports the control flow aspect of processes. Consistency checks can be performed during the design time by checking simple rules which support an efficient change propagation between views and\u00a0\u2026", "num_citations": "7\n", "authors": ["1834"]}
{"title": "Consistent abstraction of business processes based on constraints\n", "abstract": " Exploring and understanding large business process models are important tasks in the context of business process management. In recent years, several techniques have been proposed for the abstraction of business processes. Automated abstraction techniques have been devised for verifying correctness and consistency of process models and for providing customised process views for business process analysts. Yet a goal-focused and semantic-based approach to generate purposeful abstraction of business processes is an open issue. We propose an approach for configuration of process abstractions with respect to a specific abstraction goal expressed as constraints on the correspondence relation between concrete and abstract process and process transformation operators. Our framework goes beyond simple structural aggregation and leverages domain-specific properties, taxonomies\u00a0\u2026", "num_citations": "7\n", "authors": ["1834"]}
{"title": "Model-based debugging with high-level observations\n", "abstract": " Recent years have seen considerable developments in modeling techniques for automatic fault location in programs. However, much of this research considered the models from a standalone perspective. Instead, this paper focuses on the highly unusual properties of the testing and measurement process, where capabilities differ strongly from the classical hardware diagnosis paradigm. In particular, in an interactive debugging process user interaction may result in highly complex input to improve the process. This work extends the standard entropy-based measurement selection algorithm proposed in (de Kleer and Williams, 1987) to deal with high-level observations about the intended behavior of Java programs, specific to a set of test cases. We show how to incorporate the approach into previously developed model-based debugging frameworks and to how reasoning about high-level properties of\u00a0\u2026", "num_citations": "7\n", "authors": ["1834"]}
{"title": "Mixed Reality Interaction and Presentation Techniques for Medical Visualisations\n", "abstract": " Mixed, Augmented and Virtual reality technologies are burgeoning with new applications and use cases appearing rapidly. This chapter provides a brief overview of the fundamental display presentation methods; head-worn, hand-held and projector-based displays. We present a summary of visualisation methods that employ these technologies in the medical domain with a diverse range of examples presented including diagnostic and exploration, intervention and clinical, interaction and gestures, and education.", "num_citations": "6\n", "authors": ["1834"]}
{"title": "Knowledge Representation of Network Semantics for Reasoning-Powered Cyber-Situational Awareness\n", "abstract": " For network analysts, understanding how network devices are interconnected and how information flows around the network is crucial to the cyber-situational awareness required for applications such as proactive network security monitoring. Many heterogeneous data sources are useful for these applications, including router configuration files, routing messages, and open datasets. However, these datasets have interoperability issues, which can be overcome by using formal knowledge representation techniques for network semantics. Formal knowledge representation also enables automated reasoning over statements about network concepts, properties, entities, and relationships, thereby enabling knowledge discovery. This chapter describes formal knowledge representation formalisms to capture the semantics of communication network concepts, their properties, and the relationships between them, in\u00a0\u2026", "num_citations": "6\n", "authors": ["1834"]}
{"title": "Semantic interpretation of requirements through cognitive grammar and configuration\n", "abstract": " Many attempts have been made to apply Natural Language Processing to requirements specifications. However, typical approaches rely on shallow parsing to identify object-oriented elements of the specifications (e.g. classes, attributes, and methods). As a result, the models produced are often incomplete, imprecise, and require manual revision and validation. In contrast, we propose a deep Natural Language Understanding approach to create complete and precise formal models of requirements specifications. We combine three main elements to achieve this: (1) acquisition of lexicon from a user-supplied glossary requiring little specialised prior knowledge; (2) flexible syntactic analysis based purely on word-order; and (3) Knowledge-based Configuration unifies several semantic analysis tasks and allows the handling of ambiguities and errors. Moreover, we provide feedback to the user, allowing the\u00a0\u2026", "num_citations": "6\n", "authors": ["1834"]}
{"title": "Conflict resolution for on-the-fly change propagation in business processes\n", "abstract": " Process models are widely used in organisations and can easily become large and complex. In the context of business process management, views are a useful technique to reduce complexity by providing only those process fragments that are relevant for a particular stakeholder. A key challenge in view management is the handling of changes that are performed concurrently by different stakeholders. Since the views may refer to the same process, the performed changes may affect the same region of a business process and cause a conflict.Many approaches have been proposed for resolving conflicts in a post-analysis phase after all changes have been applied. They can be become costly when dealing with multiple changes that lead to multiple conflicts which cannot be resolved automatically and require an additional negotiation phase between stakeholders. In this paper we propose a framework for the on-the-fly conflict resolution of changes that have been performed on views their underlying reference process. Different to existing approaches this framework applies behaviour consistency rules for business processes which consider the execution semantics and can be checked efficiently on the structure of processes without generating all possible execution traces or keeping track of change operations.", "num_citations": "6\n", "authors": ["1834"]}
{"title": "Evaluation of Value-based Models for Java Debugging \u0403\n", "abstract": " This document compares the value-based model for Java programs and its loop-free variant. The observations obtained from an evaluation of the models using a set of example programs are presented. In particular, weaknesses, room for improvements and possible enhancements of the models are pointed out.", "num_citations": "6\n", "authors": ["1834"]}
{"title": "Towards a linked information architecture for integrated law enforcement\n", "abstract": " Law enforcement agencies are facing an ever-increasing flood of data to be acquired, stored, assessed and used. Automation and advanced data analy-sis capabilities are required to supersede traditional manual work processes and legacy information silos by automatically acquiring information from a range of sources, analyzing it in the context of on-going investigations, and linking it to other pieces of knowledge pertaining to the investigation. This paper outlines a modular architecture for management of linked data in the law enforcement domain and discusses legal and policy issues related to workflows and infor-mation sharing in this context.", "num_citations": "5\n", "authors": ["1834"]}
{"title": "Building a web observatory for south Australian government: supporting an age friendly population\n", "abstract": " The South Australian Government has played a leading role in the digital government trend, and Adelaide has led the move towards becoming a\u201d smart city\u201d ever since the\u201d multi-function polis\u201d in the early 1990s. As a part of the move towards Open Government many jurisdictions have created platforms upon which to publish open public data, and South Australia\u2019s data. sa. gov. au is where its public government data are made available as csv files for use by business and the community. However, there is much more to opening up data than just publishing it, because when it comes to data that contains sensitive and private information, the current data publishing and sharing solutions hit barriers due to the lack of data provenance and security mechanisms, as well as limited use/usability for applications. Further, building analytic applications that make use of those datasets in line with their sharing policies is another challenge as many users have little digital literacy. These issues can be addressed by a new data sharing platform called the Web Observatory which allows both data and analytics to be published, shared and consumed in a secure and transparent manner. This paper describes the joint effort of building a Web Observatory for the South Australian Government through the collaboration of the University of South Australia and the University of Southampton.", "num_citations": "5\n", "authors": ["1834"]}
{"title": "Diagnosing process trajectories under partially known behavior\n", "abstract": " Diagnosis of process executions is an important task in many application domains, especially in the area of workflow management systems and orchestrated Web Services. If executions fail because activities of the process do not behave as intended, recovery procedures re-execute some activities to recover from the failure. We present a diagnosis method for identifying incorrect activities in process executions. Our method is novel both in that it does not require exact behavioral models for the activities and that its accuracy improves upon dependency-based methods. Observations obtained from partial executions and re-executions of a process are exploited. We formally characterize the diagnosis problem and develop a symbolic encoding that can be solved using CLP (FD) solvers. Our evaluation demonstrates that the framework yields superior accuracy to dependency-based methods on realistically-sized examples.", "num_citations": "5\n", "authors": ["1834"]}
{"title": "Debugging Failures in Web Services Coordination.\n", "abstract": " The rise of Web Services over the past years offers a new development paradigm for distributed applications: high level communication using exchange of structured XML data, using communication protocols orchestrated by workflow languages with complex control constructs. We study the use of model-based techniques that have been used for fault analysis in imperative (Java) and concurrent (VHDL) languages in a Web Service environment, with the goal of diagnosing Web service interactions specified in BPEL4WS, using an Abstract Interpretation approach.", "num_citations": "5\n", "authors": ["1834"]}
{"title": "Debugging failures in web services coordination\n", "abstract": " The rise of Web Services over the past years offers a new development paradigm for distributed applications: high level communication using exchange of structured XML data, using communication protocols orchestrated by workflow languages with complex control constructs. We study the use of model-based techniques that have been used for fault analysis in imperative (Java) and concurrent (VHDL) languages in a Web Service environment, with the goal of diagnosing Web service interactions specified in BPEL4WS, using an Abstract Interpretation approach.", "num_citations": "5\n", "authors": ["1834"]}
{"title": "ASP-based Discovery of Semi-Markovian Causal Models under Weaker Assumptions\n", "abstract": " In recent years the possibility of relaxing the so-called Faithfulness assumption in automated causal discovery has been investigated. The investigation showed (1) that the Faithfulness assumption can be weakened in various ways that in an important sense preserve its power, and (2) that weakening of Faithfulness may help to speed up methods based on Answer Set Programming. However, this line of work has so far only considered the discovery of causal models without latent variables. In this paper, we study weakenings of Faithfulness for constraint-based discovery of semi-Markovian causal models, which accommodate the possibility of latent variables, and show that both (1) and (2) remain the case in this more realistic setting.", "num_citations": "4\n", "authors": ["1834"]}
{"title": "Provenance-Aware LOD Datasets for Detecting Network Inconsistencies\n", "abstract": " Contextualized knowledge graphs (CKGs) have been gaining importance in recent years by providing context-aware datasets in various knowledge domains. In communication network analysis, for example, CKGs can be used to improve cyber-situational awareness or to reason about network topologies. Despite the potential of these graphs, there is a lack of published CKG-based datasets for communication networks. The complexity, scale, and rapid changes of real-world communication networks make it crucial to capture not only network knowledge in network datasets, but also additional metadata. Therefore, this paper presents communication network datasets, enriched with provenance, timestamps, and location data, which can be used for benchmarking, in silico experiments, and aimed at serving as the basis for further applications and research.", "num_citations": "4\n", "authors": ["1834"]}
{"title": "Variety Management for Big Data\n", "abstract": " Of the core challenges originally associated with Big Data, namely Volume, Velocity, and Variety, the Variety aspect is the one that is least addressed by the standard analytics architectures. In this chapter, we analyze types and sources of variety and describe data- and metadata management principles for organizing data lakes. We discuss how semantic metadata can help describe and manage variety in structure, provenance, visibility and permitted use. Moreover, ontologies and metadata catalogs can aid discovery, navigation, exploration, and interpretation of heterogeneous data lakes, and can simplify interpretation, lift data quality, and simplify integration of multiple data sets. We present an application of these principles in a data architecture for the Law Enforcement domain in Australia.", "num_citations": "4\n", "authors": ["1834"]}
{"title": "Semantic Interoperability in the Oil and Gas Industry: A Challenging Testbed for Semantic Technologies\n", "abstract": " This paper outlines some of the inherent difficulties present in large-scale standards-based semantic interoperability in the Oil and Gas industry. This domain in particularly interesting for semantic interoperability, as the complexity is manifold: data sets are large, span many different domains, are modeled and represented differently in various standards, which have evolved considerably over time. We outline the main challenges with respect to sustained interoperability and advocate that the interoperability scenarios could serve as an interesting test bed for evaluating semantic interoperability techniques.", "num_citations": "4\n", "authors": ["1834"]}
{"title": "A rule-based platform for distributed real-time SOA with application in defence systems\n", "abstract": " Modularity has been a key issue in the design and development of modern embedded Real-Time Software Systems (RTS), where modularity enables flexibility with respect to changes in platform, environment, and requirements, as well as reuse. In distributed RTS, similar ideas have led to the adoption of COTS components integrated via Service-Oriented Architecture (SOA) principles and technologies that are already well-established in business-oriented information systems. However, current SOA implementations, with respect to service activation and orchestration, do not meet strict time-dependent constraints on scalability and latency required by RTS. We present a novel approach to RTS development where the orchestration of real-time processes is distributed among the services within a fully distributed data-driven process framework. Our framework wraps around COTS components implementing individual\u00a0\u2026", "num_citations": "4\n", "authors": ["1834"]}
{"title": "A generative framework for service process composition\n", "abstract": " In our prior work we showed the benefits of formulating service composition as a Generative Constraint Satisfaction Problem (GCSP), where available services and composition problems are modeled in a generic manner and are instantiated on-the-fly during the solving process, in dynamic composition scenarios. In this paper, we (1) outline the salient features of our framework, (2) present extensions to our framework in the form of process-level invariants, and (3) evaluate the effectiveness of our framework in difficult scenarios, where a number of similar and potentially unsuitable services have to be explored during composition.", "num_citations": "4\n", "authors": ["1834"]}
{"title": "An Ontological Core for Conformance Checking in the Engineering Life-cycle\n", "abstract": " Effective exchange of information about processes and industrial plants, their design, construction, operation, and maintenance requires sophisticated information modelling and exchange mechanisms that enable the transfer of semantically meaningful information between a vast pool of heterogeneous information systems. In order to represent entities relevant to the engineering life-cycle, social concepts, descriptions, roles, artefacts, functions, and information objects must be integrated in a coherent whole. Forming the basis of this integration in our framework is the DOLCE foundational ontology. In this paper we propose an ontologically well-founded approach to modelling artefacts, their requirement specifications and functional roles, such that consistency of their relationships in the data model can be verified. Specifically, we discuss the modelling of engineering artefacts, roles and role-filling capacity in the context of data modelling for industrial information exchange.", "num_citations": "3\n", "authors": ["1834"]}
{"title": "Towards a reference architecture for the co-evolution of business processes\n", "abstract": " In large organisations different stakeholders are usually responsible for the management of business processes. This paper addresses the situation where multiple stakeholders with different interests are handling parts of the same business processes. One technique to support this are process views where each stakeholder holds a personal view which can be executed or changed. In this paper we investigated the requirements on view management for the co-evolution of business processes. A literature review revealed that there is no approach that can support all requirements. We therefore propose an architecture which can overcome existing gaps.", "num_citations": "3\n", "authors": ["1834"]}
{"title": "Multilevel modelling for interoperability.\n", "abstract": " Model-driven approaches to establishing interoperability between information systems have recently embraced meta-modelling frameworks spanning multiple levels. However, no consensus has yet been established as to which techniques adequately support situations where heterogeneous domain-specific models must be linked within a common modelling approach. We introduce modelling primitives that support the multilevel modelling paradigm for information integration in heterogeneous information systems. We extend standard specialisation and instantiation mechanisms to enable the propagation of semantic and schema information across model levels and compare our approach using a suite of criteria to show that our approach improves modularity, redundancy, query complexity, and level stratification.", "num_citations": "3\n", "authors": ["1834"]}
{"title": "Re-engineering the ISO 15926 Data Model: A Multi-level Metamodel Perspective\n", "abstract": " The ISO\u00a015926 standard was developed to facilitate the integration of life-cycle data of process plants. The core of the standard is a highly generic and extensible data model trying to capture a holistic view of the world. We investigated the standard from a software modelling point of view and identified some challenges in terminology, circular definitions and inconsistencies in relationships during the mapping from concepts specified in the standard to an object-oriented model. This makes the standard difficult to understand and more challenging to implement. In this paper we look at mapping the ISO\u00a015926 data model to a multilevel metamodel, and aim to formalise critical aspects of the data model which will simplify the model and ease the adoption process.", "num_citations": "3\n", "authors": ["1834"]}
{"title": "Configuring Domain Knowledge for Natural Language Understanding.\n", "abstract": " Knowledge-based configuration has been used for numerous applications including natural language processing (NLP). By formalising property grammars as a configuration problem, it has been shown that configuration can provide a flexible, nondeterministic, method of parsing natural language. However, it focuses only on syntactic parsing. In contrast, configuration is usually performed using knowledge about a domain and is semantic in nature. Therefore, we argue that configuration has the potential to be used, not only for syntactic processing, but for the semantic processing of natural language, effectively supporting Natural Language Understanding (NLU).In this paper, we propose an approach to NLP that applies configuration to the (partial) domain model evoked by the processing of a sentence. This has the benefit of ensuring the meaning of the sentence is consistent with the existing domain knowledge. Moreover, it allows the dynamic incorporation of domain knowledge in the configuration model as the text is processed. We demonstrate the approach on a business specification based on the Semantics of Business Vocabulary and Rules.", "num_citations": "3\n", "authors": ["1834"]}
{"title": "Performance analysis of a rule-based SOA component for real-time applications\n", "abstract": " In this paper we consider an event based architectural component that may overcome the present reluctance to the use of Service Orientation (SOA) in military distributed real-time environments. In particular, we propose a hybrid architecture component that is decentralised in all respects, making it more reactive to real-time events, as well as being easier to analyse and adapt to changing needs. As centralised scheduling and orchestration of SOA services does not scale to distributed systems, our architecture removes this key inhibitor by distributing the data and control flow to a rule-driven Distributed Real-Time SOA (DRT-SOA) component that resides with each service. Embedded deadline driven task scheduling means each service can now dynamically adjust to changes in process priorities. We also propose a method of analysing the performance of the new architecture using the dynamics of Petri Nets and the\u00a0\u2026", "num_citations": "3\n", "authors": ["1834"]}
{"title": "A Schema-Driven Synthetic Knowledge Graph Generation Approach with Extended Graph Differential Dependencies (GDDxs)\n", "abstract": " Knowledge Graphs (KGs), as one of the key trends which are driving the next wave of technologies, have now become a new form of knowledge representation, and a cornerstone for several applications from generic to specific industrial use cases. However, in some specific domains such as law enforcement, a real and large domain-oriented KG is often unavailable due to data privacy concerns. In such domains it is necessary to generate a synthetic KG which mimics the properties of a real KG in the domain. Although during the last two decades, a variety of graph data generators has been proposed to achieve the generation of different kinds of networks, the state-of-the-art synthetic graph data generators are not feasible to generate a realistic and synthetic KGs because KGs always contain data characteristics with specified semantics. In this work, we propose a schema-driven synthetic KG generation approach with\u00a0\u2026", "num_citations": "2\n", "authors": ["1834"]}
{"title": "A cartography of delay risks in the Australian construction industry: impact, correlations and timing\n", "abstract": " PurposeThe purpose of this research is to identify the most impactful delay risks in Australian construction projects, including the associations amongst those risks as well as the project phases in which they are most likely present. The correlation between project and organisational characteristics with the impact of delay risks was also studied.Design/methodology/approachA questionnaire survey was used to collect data from 118 delayed construction projects in Australia. Data were analysed to rank the most impactful delay risks, their correlation to project and organisational characteristics and project phases where those risks are likely to emerge. Association rule learning was used to capture associations between the delay risks.FindingsThe top five most impactful delay risks in Australia were changes by the owner, slow decisions by the owner, preparation and approval of design drawings, underestimation of\u00a0\u2026", "num_citations": "2\n", "authors": ["1834"]}
{"title": "Network Path Estimation in Uncertain Data via Entity Resolution\n", "abstract": " Network Path Estimation is the problem of finding best paths among multiple potential routes between two devices, which is important to cyber situational awareness. In this context, information obtained from multiple sources and at different points in time must be integrated. However, duplicate representations of the same entities in different data sources must be identified and merged to accurately infer and rank network paths. We extend previous work on deterministic rule-based Entity Resolution with similarity flooding principles to obtain a probabilistic entity matching technique. Our approach outperforms the rule-based approach, allows for domain-specific ontologies to be incorporated, and accounts for provenance across data sources. Using the probabilistic resolutions, we rank network paths according to certainty of the resolutions, which improves network path estimation and contributes to cyber situational\u00a0\u2026", "num_citations": "2\n", "authors": ["1834"]}
{"title": "A Service oriented architecture for data integration in asset management\n", "abstract": " The integration of data plays a crucial role in condition monitoring and active data warehousing. It is classified in horizontal and vertical integration which capture different integration scenarios. Horizontal scenarios deal with application providing complementary functionality, whereas vertical scenarios deal with the integration of applications on different abstraction levels. Processing data for effective decision support in condition monitoring is usually performed by different software applications that are integrated in a common business process. In order to execute the business process, data must be exported from one application and imported into another. However, due to heterogeneous underlying data models, the data export and import is not straight forward and a translation of data from one representation into another is required. We propose a Service Oriented Architecture (SOA) based on Web services\u00a0\u2026", "num_citations": "2\n", "authors": ["1834"]}
{"title": "Modeling context-dependent faults for diagnosis\n", "abstract": " Most Model-based diagnosis frameworks rely on incremental probing and the assumption that faults occur independently to infer the most likely explanation for a symptom. For systems where additional sensors are unavailable or a repair action must be issued at once, these assumptions are often inadequate and dependent faults must be considered explicitly. We introduce explicit models of context-dependent component fault behavior and show that our compositional models are well-suited for the one-shot fault diagnosis of pseudo-static systems. We develop extensions to the well-known Conflict-Directed A* algorithm to infer the most-likely system state given a fixed set of observations and show that our approach complements earlier dependency models.", "num_citations": "2\n", "authors": ["1834"]}
{"title": "Ontology-based Process Modeling and Execution Using STEP/EXPRESS.\n", "abstract": " A common data format as provided by the STEP/EXPRESS initiative is an important step toward interoperability in heterogeneous design and manufacturing environments. Ontologies further support integration by providing an explicit formalism of process and design knowledge, thereby enabling semantic integration and re-use of process-information. By formalizing the process-model in EXPRESS, we gain access to the domain knowledge in the STEP application protocols. We present an approach to process modeling using different models for abstract process knowledge and implementation details. The abstract process model supports re-use and is independent of the implementation. As a result, we translate the process model in combination with the implementation model to an executable workflow.", "num_citations": "2\n", "authors": ["1834"]}
{"title": "The importance of accounting-integrated information systems for realising productivity and sustainability in the agricultural sector\n", "abstract": " Agricultural information systems are an integral part of modern farming and are helping to make a significant contribution to improved farm productivity and profitability. To date, however, there has been a failure to integrate accounting information systems with on-farm data, despite today\u2019s farmers facing unprecedented and interconnected economic and resource pressures. This study explores this problem in more detail, defines the objectives of the solution and develops a model of integrated accounting and agricultural information systems, drawing on a \u2018fads and fashions\u2019 framework and advancing our understanding of bundled innovations. Using data from a participatory case study in Australian potato farming, the study integrates accounting data with soil moisture and climate data to track, alert and inform irrigation decisions. Development of preliminary digital software based on the model demonstrates how\u00a0\u2026", "num_citations": "1\n", "authors": ["1834"]}
{"title": "Microservice API design to support C2 semantic integration\n", "abstract": " Operational Headquarters (HQ) need to maintain situation awareness and command and control (C2) in an increasing variety of operations. As the operational focus of HQ changes over time, so too do the information requirements of HQ staff. C2 and decision support systems need to be highly flexible to support this level of information variety. Achieving 5th Generation HQ capabilities will place higher still demands for organizational agility, which must be supported by agile information systems. We use the term agile semantic integration for the process of timely deployment of new information sources into operational HQ, such that it is amenable to human and machine reasoning, and integrated with existing HQ information sources. We propose a microservices architecture, and review technology implementation choices for information access and integration, focusing in particular on Application Programming Interface design, and the agility and maintainability of the system.", "num_citations": "1\n", "authors": ["1834"]}
{"title": "FEDSA: A Data Federation Platform for Law Enforcement Management\n", "abstract": " In the era of big data, new challenges occur in the field of data federation research. New types of data sources with new formats of data have emerged, and end users need to conduct complex search and data analytical tasks, which impose requirements such flexible data federation, customized security mechanism and high-performance processing (for example, near real time query). To address these challenges, this paper proposes a data federation platform named FEDSA and reports on an initial implementation. Distinctive features of the platform include process-driven data federation, Data Federation as a Service, a simple query language over a high-level common data model, data security protection over all federation services, query re-writing and full distribution. We demonstrate how these features address the challenges, discuss the performance of the current implementation, and outline future extensions.", "num_citations": "1\n", "authors": ["1834"]}
{"title": "Relationship Matching of Data Sources: A Graph-Based Approach\n", "abstract": " Relationship matching is a key procedure during the process of transforming structural data sources, like relational data bases, spreadsheets into the common data model. The matching task refers to the automatic identification of correspondences between relationships of source columns and the relationships of the common data model. Numerous techniques have been developed for this purpose. However, the work is missing to recognize relationship types between entities in information obtained from data sources in instance level and resolve ambiguities. In this paper, we develop a method for resolving ambiguous relationship types between entity instances in structured data. The proposed method can be used as standalone matching techniques or to complement existing relationship matching techniques of data sources. The result of an evaluation on a large real-world data set demonstrated the high\u00a0\u2026", "num_citations": "1\n", "authors": ["1834"]}
{"title": "Automated Techniques for Generating Behavioural Models for Constructive Combat Simulations\n", "abstract": " Constructive combat simulation is widely used across Defence Science and Technology Group, typically using behavioural models written by software developers in a scripting or programming language for a specific simulation. This approach is time-consuming, can lead to inconsistencies between the same behaviour in different simulations, and is difficult to engage military subject matter experts in the elicitation and verification of behaviours. Therefore, a representation is required that is both comprehensible to non-programmers and is translatable to different simulation execution formats. This paper presents such a representation, the Hierarchical Behaviour Model and Notation (HBMN), which incorporates aspects of existing business process and behaviour representations to provide a hierarchical schema allowing an incremental approach to developing and refining behaviour models from abstract\u00a0\u2026", "num_citations": "1\n", "authors": ["1834"]}
{"title": "Rule-Based Control of Decentralised Asynchronous SOA for Real-Time Applications\n", "abstract": " In this paper we consider the core issues that are inhibiting the application of general Service Oriented Architecture (SOA) principles to distributed real-time military environments. In particular, we propose a Distributed Real-Time Service Oriented Architecture (DRT-SOA) framework intended to overcome some of the problems that distributed SOA systems face in needing to exhibit verifiable deterministic behaviour. Although the time behaviour of asynchronous systems is inherently non-deterministic, such systems offer advantages in terms of extensibility, cost and utilisation. We show that DRT-SOA presents a framework where the sources of non-determinism can be supressed, making it suitable for certain classes of real-time system. We also show that decentralised execution of complex workflows is achievable using dynamic rule-based logic. We also present preliminary results demonstrating the architecture's\u00a0\u2026", "num_citations": "1\n", "authors": ["1834"]}
{"title": "Inconsistencies in the Process Specification Language (PSL)\n", "abstract": " The Process Specification Language (PSL)[6] is a first-order logical theory designed to describe manufacturing or business processes and formalize reasoning about them. It has been developed by several authors over a period of years, yet it is inconsistent with the simplest axioms that preclude non-trivial models. We demonstrate several inconsistencies using an automated theorem prover and attempt to repair the inconsistencies. We conclude that even with our amendments, PSL with its infinite models remains inadequate to represent complex industrial processes. We propose an alternative axiomatization that admits finite models, which may be better suited to automated theorem provers and model finders than the current version of PSL.", "num_citations": "1\n", "authors": ["1834"]}
{"title": "Diagnosing component interaction errors from abstract event traces\n", "abstract": " While discrete event systems have been widely applied for diagnosing distributed communicating systems, existing models may not completely satisfy the requirements for the application of fault identification and repair in software systems. This paper presents a model-based diagnosis approach that identifies possible faults based on generic fault models in abstract traces where events may be associated to multiple system components. We overcome the common limitation that precise fault models are available for each component and leverage generic fault models of classes of faults instead. We show that diagnoses representing entire classes of equivalent solutions can be computed based on local information and investigate the performance of our algorithm.", "num_citations": "1\n", "authors": ["1834"]}
{"title": "Ontological Support for Consistency Checking of Engineering Design Workflows\n", "abstract": " In this paper we describe a novel approach for engineering process representation based on the application of formal ontologies. We illustrate components of a framework that comprises domain abstractions, design interfaces and meta-data in the engineering design domain in the form of process-, artefact-and task ontologies. A concrete application and use case of the framework components is detailed in order to demonstrate the capabilities for representation of design processes on a high level of abstraction. The realm of Planning is employed to showcase workflow decomposition. We show, how the framework can be applied to perform workflow consistency checking and point out some scenarios where the developed framework can provide support in the task of engineering design analysis and improvement.", "num_citations": "1\n", "authors": ["1834"]}
{"title": "Distributed repair of nondiagnosability\n", "abstract": " Automated fault diagnosis has significant practical impact by improving reliability and facilitating maintenance of systems [1]. Given a monitor continuously receiving observations from a dynamic eventdriven system, diagnosis algorithms infer possible fault events that explain the observations. For many applications, it is not sufficient to identify what faults could have occurred; rather, one wishes to know what faults have definitely occurred. Computing the latter requires diagnosability of the system, that is, the guarantee that the occurrence of a fault can be detected with certainty after a finite number of subsequent observations [2].This paper defines a distributed framework that assists in assessing and improving the diagnosability of discrete-event systems. In this context, a system is diagnosable iff the presence or absence of each unobservable fault event can always be deduced once sufficiently many subsequent\u00a0\u2026", "num_citations": "1\n", "authors": ["1834"]}
{"title": "Modelling and management of design artefacts in design optimisation\n", "abstract": " Complex design processes often embrace various degrees of virtual development, where complex models and simulations replace traditional construction and testing of physical models. However, as the number of models and their inter-relationships grows, managing processes and models becomes increasingly difficult. We describe how to support product development by applying ontologies to manage and guide the design of simulations and to make domain knowledge readily available though context-specific reuse mechanisms. Based on established engineering standards like ISO 10303 we defined domain-specific abstractions and operators to facilitate information reuse.", "num_citations": "1\n", "authors": ["1834"]}
{"title": "Better Debugging through More Abstract Observations\n", "abstract": " Better Debugging through More Abstract Observations | Proceedings of the 2006 conference on ECAI 2006: 17th European Conference on Artificial Intelligence August 29 -- September 1, 2006, Riva del Garda, Italy ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsProceedings of the 2006 conference on ECAI 2006: 17th European Conference on Artificial Intelligence August 29 -- September 1, 2006, Riva del Garda, ItalyBetter Debugging through More Abstract Observations Article Better Debugging through More Abstract Observations Share on Authors: Wolfgang Mayer Advanced Computing Research Centre, University of \u2026", "num_citations": "1\n", "authors": ["1834"]}
{"title": "High-level observations in Java debugging\n", "abstract": " Recent years have seen considerable developments in modeling techniques for automatic fault location in programs. However, much of this research considered the models from a standalone perspective. Instead, this paper focuses on the properties of the testing and measurement process, where capabilities differ strongly from the classical hardware diagnosis paradigm. In particular, in an interactive debugging process user interaction may result in highly complex input to improve the process. This work proposes an heuristic entropy-based measurement selection algorithm, which incorporates high-level properties of the intended behavior of Java programs, specific to a set of test cases. We show how to integrate the approach into previously developed model-based debugging frameworks and to how reasoning about high-level properties of programs can improve fault localization.INTRODUCTION This paper extends prior research on modelbased diagnosis for locating bugs in programs written in mainstream programming languages (eg Java). The idea behind the model-based debugging approach is (1) to automatically compile a program to its logical model or to a constraint satisfaction problem,(2) to use the model together with test cases and a model-based diagnostic engine for computing the diagnosis candidates, and (3) to map the candidates back to their corresponding locations within the original program. Formally, given a set of test cases\u00a1\u00a3\u00a2 on which the program is run, a (minimal) diagnosis is defined as a (minimal) set of incorrectness assumptions\u00a4\u00a6\u00a5 \u00a7 \u00a9\u00a2 on a subset\u00a2 of components\u00a2\u00a3 in the program (usually statements) such that", "num_citations": "1\n", "authors": ["1834"]}
{"title": "Assertions in model-based debugging\u00a3\n", "abstract": " This document describes the design and implementation of an assertion language that is used within the Jade debugging environment. The Jade assertion language helps to specify assertions and expected values of variables and expressions of Java programs. In particular, the language provides means to embed assertions either directly in the Java source code or to store them in separate files. The debugging environment is modified such that the assertions replace the traditional observation specification mechanisms of the debugger. Consequently, it is now possible to run test cases unattended. Also, multiple test cases can be applied concurrently to improve diagnosis quality.", "num_citations": "1\n", "authors": ["1834"]}