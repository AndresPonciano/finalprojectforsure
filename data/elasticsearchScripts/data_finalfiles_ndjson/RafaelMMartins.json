{"title": "Visual analysis of dimensionality reduction quality for parameterized projections\n", "abstract": " In recent years, many dimensionality reduction (DR) algorithms have been proposed for visual analysis of multidimensional data. Given a set of n-dimensional observations, such algorithms create a 2D or 3D projection thereof that preserves relative distances or neighborhoods. The quality of resulting projections is strongly influenced by many choices, such as the DR techniques used and their various parameter settings. Users find it challenging to judge the effectiveness of a projection in maintaining features from the original space and to understand the effect of parameter settings on these results, as well as performing related tasks such as comparing two projections. We present a set of interactive visualizations that aim to help users with these tasks by revealing the quality of a projection and thus allowing inspection of parameter choices for DR algorithms, by observing the effects of these choices on the resulting\u00a0\u2026", "num_citations": "114\n", "authors": ["936"]}
{"title": "Toward a quantitative survey of dimension reduction techniques\n", "abstract": " Dimensionality reduction methods, also known as projections, are frequently used in multidimensional data exploration in machine learning, data science, and information visualization. Tens of such techniques have been proposed, aiming to address a wide set of requirements, such as ability to show the high-dimensional data structure, distance or neighborhood preservation, computational scalability, stability to data noise and/or outliers, and practical ease of use. However, it is far from clear for practitioners how to choose the best technique for a given use context. We present a survey of a wide body of projection techniques that helps answering this question. For this, we characterize the input data space, projection techniques, and the quality of projections, by several quantitative metrics. We sample these three spaces according to these metrics, aiming at good coverage with bounded effort. We describe our\u00a0\u2026", "num_citations": "87\n", "authors": ["936"]}
{"title": "Graph Layouts by t\u2010SNE\n", "abstract": " We propose a new graph layout method based on a modification of the t\u2010distributed Stochastic Neighbor Embedding (t\u2010SNE) dimensionality reduction technique. Although t\u2010SNE is one of the best techniques for visualizing high\u2010dimensional data as 2D scatterplots, t\u2010SNE has not been used in the context of classical graph layout. We propose a new graph layout method, tsNET, based on representing a graph with a distance matrix, which together with a modified t\u2010SNE cost function results in desirable layouts. We evaluate our method by a formal comparison with state\u2010of\u2010the\u2010art methods, both visually and via established quality metrics on a comprehensive benchmark, containing real\u2010world and synthetic graphs. As evidenced by the quality metrics and visual inspection, tsNET produces excellent layouts.", "num_citations": "49\n", "authors": ["936"]}
{"title": "Explaining Neighborhood Preservation for Multidimensional Projections\n", "abstract": " Dimensionality reduction techniques are the tools of choice for exploring high-dimensional datasets by means of low-dimensional projections. However, even state-of-the-art projection methods fail, up to various degrees, in perfectly preserving the structure of the data, expressed in terms of inter-point distances and point neighborhoods. To support better interpretation of a projection, we propose several metrics for quantifying errors related to neighborhood preservation. Next, we propose a number of visualizations that allow users to explore and explain the quality of neighborhood preservation at different scales, captured by the aforementioned error metrics. We demonstrate our exploratory views on three real-world datasets and two state-of-the-art multidimensional projection techniques.", "num_citations": "39\n", "authors": ["936"]}
{"title": "Attribute-based Visual Explanation of Multidimensional Projections\n", "abstract": " Multidimensional projections (MPs) are key tools for the analysis of multidimensional data. MPs reduce data dimensionality while keeping the original distance structure in the low-dimensional output space, typically shown by a 2D scatterplot. While MP techniques grow more precise and scalable, they still do not show how the original dimensions (attributes) influence the projection\u2019s layout. In other words, MPs show which points are similar, but not why. We propose a visual approach to describe which dimensions contribute mostly to similarity relationships over the projection, thus explain the projection\u2019s layout. For this, we rank dimensions by increasing variance over each point-neighborhood, and propose a visual encoding to show the least-varying dimensions over each neighborhood. We demonstrate our technique with both synthetic and real-world datasets.", "num_citations": "39\n", "authors": ["936"]}
{"title": "Explaining three-dimensional dimensionality reduction plots\n", "abstract": " Understanding three-dimensional projections created by dimensionality reduction from high-variate datasets is very challenging. In particular, classical three-dimensional scatterplots used to display such projections do not explicitly show the relations between the projected points, the viewpoint used to visualize the projection, and the original data variables. To explore and explain such relations, we propose a set of interactive visualization techniques. First, we adapt and enhance biplots to show the data variables in the projected three-dimensional space. Next, we use a set of interactive bar chart legends to show variables that are visible from a given viewpoint and also assist users to select an optimal viewpoint to examine a desired set of variables. Finally, we propose an interactive viewpoint legend that provides an overview of the information visible in a given three-dimensional projection from all possible\u00a0\u2026", "num_citations": "36\n", "authors": ["936"]}
{"title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections\n", "abstract": " t-Distributed Stochastic Neighbor Embedding (t-SNE) for the visualization of multidimensional data has proven to be a popular approach, with successful applications in a wide range of domains. Despite their usefulness, t-SNE projections can be hard to interpret or even misleading, which hurts the trustworthiness of the results. Understanding the details of t-SNE itself and the reasons behind specific patterns in its output may be a daunting task, especially for non-experts in dimensionality reduction. In this article, we present t-viSNE, an interactive tool for the visual exploration of t-SNE projections that enables analysts to inspect different aspects of their accuracy and meaning, such as the effects of hyper-parameters, distance and neighborhood preservation, densities and costs of specific neighborhoods, and the correlations between dimensions and visual patterns. We propose a coherent, accessible, and well\u00a0\u2026", "num_citations": "35\n", "authors": ["936"]}
{"title": "Multidimensional projections for visual analysis of social networks\n", "abstract": " Visual analysis of social networks is usually based on graph drawing algorithms and tools. However, social networks are a special kind of graph in the sense that interpretation of displayed relationships is heavily dependent on context. Context, in its turn, is given by attributes associated with graph elements, such as individual nodes, edges, and groups of edges, as well as by the nature of the connections between individuals. In most systems, attributes of individuals and communities are not taken into consideration during graph layout, except to derive weights for force-based placement strategies. This paper proposes a set of novel tools for displaying and exploring social networks based on attribute and connectivity mappings. These properties are employed to layout nodes on the plane via multidimensional projection techniques. For the attribute mapping, we show that node proximity in the layout\u00a0\u2026", "num_citations": "27\n", "authors": ["936"]}
{"title": "The Nordic Tweet Stream: A dynamic real-time monitor corpus of big and rich language data\n", "abstract": " This article presents the Nordic Tweet Stream (NTS), a cross-disciplinary corpus project of computer scientists and a group of sociolinguists interested in language variability and in the global spread of English. Our research integrates two types of empirical data: We not only rely on traditional structured corpus data but also use unstructured data sources that are often big and rich in metadata, such as Twitter streams. The NTS downloads tweets and associated metadata from Denmark, Finland, Iceland, Norway and Sweden. We first introduce some technical aspects in creating a dynamic real-time monitor corpus, and the following case study illustrates how the corpus could be used as empirical evidence in sociolinguistic studies focusing on the global spread of English to multilingual settings. The results show that English is the most frequently used language, accounting for almost a third. These results can be used to assess how widespread English use is in the Nordic region and offer a big data perspective that complement previous small-scale studies. The future objectives include annotating the material, making it available for the scholarly community, and expanding the geographic scope of the data stream outside Nordic region.", "num_citations": "17\n", "authors": ["936"]}
{"title": "StanceVis Prime: visual analysis of sentiment and stance in social media texts\n", "abstract": " Abstract Text visualization and visual text analytics methods have been successfully applied for various tasks related to the analysis of individual text documents and large document collections such as summarization of main topics or identification of events in discourse. Visualization of sentiments and emotions detected in textual data has also become an important topic of interest, especially with regard to the data originating from social media. Despite the growing interest in this topic, the research problem related to detecting and visualizing various stances, such as rudeness or uncertainty, has not been adequately addressed by the existing approaches. The challenges associated with this problem include the development of the underlying computational methods and visualization of the corresponding multi-label stance classification results. In this paper, we describe our work on a visual analytics platform, called\u00a0\u2026", "num_citations": "12\n", "authors": ["936"]}
{"title": "Stackgenvis: Alignment of data, algorithms, and models for stacking ensemble learning using performance metrics\n", "abstract": " In machine learning (ML), ensemble methods-such as bagging, boosting, and stacking-are widely-established approaches that regularly achieve top-notch predictive performance. Stacking (also called \u201cstacked generalization\u201d) is an ensemble method that combines heterogeneous base models, arranged in at least one layer, and then employs another metamodel to summarize the predictions of those models. Although it may be a highly-effective approach for increasing the predictive performance of ML, generating a stack of models from scratch can be a cumbersome trial-and-error process. This challenge stems from the enormous space of available solutions, with different sets of data instances and features that could be used for training, several algorithms to choose from, and instantiations of these algorithms using diverse parameters (i.e., models) that perform differently according to various metrics. In this work\u00a0\u2026", "num_citations": "11\n", "authors": ["936"]}
{"title": "Quality models inside out: Interactive visualization of software metrics by means of joint probabilities\n", "abstract": " Assessing software quality, in general, is hard; each metric has a different interpretation, scale, range of values, or measurement method. Combining these metrics automatically is especially difficult, because they measure different aspects of software quality, and creating a single global final quality score limits the evaluation of the specific quality aspects and trade-offs that exist when looking at different metrics. We present a way to visualize multiple aspects of software quality. In general, software quality can be decomposed hierarchically into characteristics, which can be assessed by various direct and indirect metrics. These characteristics are then combined and aggregated to assess the quality of the software system as a whole. We introduce an approach for quality assessment based on joint distributions of metrics values. Visualizations of these distributions allow users to explore and compare the quality metrics\u00a0\u2026", "num_citations": "10\n", "authors": ["936"]}
{"title": "StanceXplore: Visualization for the interactive exploration of stance in social media\n", "abstract": " The use of interactive visualization techniques in Digital Humanities research can be a useful addition when traditional automated machine learning techniques face difficulties, as is often the case with the exploration of large volumes of dynamic\u2014and in many cases, noisy and conflicting\u2014textual data from social media. Recently, the field of stance analysis has been moving from a predominantly binary approach\u2014either pro or con\u2014to a multifaceted one, where each unit of text may be classified as one (or more) of multiple possible stance categories. This change adds more layers of complexity to an already hard problem, but also opens up new opportunities for obtaining richer and more relevant results from the analysis of stance-taking in social media. In this paper we propose StanceXplore, a new visualization for the interactive exploration of stance in social media. Our goal is to offer DH researchers the chance to explore stance-classified text corpora from different perspectives at the same time, using coordinated multiple views including user-defined topics, content similarity and dissimilarity, and geographical and temporal distribution. As a case study, we explore the activity of Twitter users in Sweden, analyzing their behavior in terms of topics discussed and the stances taken. Each textual unit (tweet) is labeled with one of eleven stance categories from a cognitive-functional stance framework based on recent work. We illustrate how StanceXplore can be used effectively to investigate multidimensional patterns and trends in stance-taking related to cultural events, their geographical distribution, and the confidence of the stance classifier.", "num_citations": "10\n", "authors": ["936"]}
{"title": "Impact of the vendor lock-in problem on testing as a service (TaaS)\n", "abstract": " Testing as a Service (TaaS) is a new business and service model that provides efficient and effective software quality assurance and enables the use of a cloud for the meeting of quality standards, requirements and consumer's needs. However, problems that limit the effective use of TaaS involve lack of standardization in writing, execution, configuration and management of tests and lack of portability and interoperability among TaaS platforms - the so-called lock-in problem. The lock-in problem is a serious threat to software testing in the cloud and may become critical when a provider decides to suddenly increase prices, or shows serious technical availability problems. This paper proposes a novel approach for solving the lock-in problem in TaaS with the use of design patterns. The aim to assist software engineers and quality control managers in building testing solutions that are both portable and interoperable\u00a0\u2026", "num_citations": "8\n", "authors": ["936"]}
{"title": "Efficient dynamic time warping for big data streams\n", "abstract": " Many common data analysis and machine learning algorithms for time series, such as classification, clustering, or dimensionality reduction, require a distance measurement between pairs of time series in order to determine their similarity. A variety of measures can be found in the literature, each with their own strengths and weaknesses, but the Dynamic Time Warping (DTW) distance measure has occupied an important place since its early applications for the analysis and recognition of spoken word. The main disadvantage of the DTW algorithm is, however, its quadratic time and space complexity, which limits its practical use to relatively small time series. This issue is even more problematic when dealing with streaming time series that are continuously updated, since the analysis must be re-executed regularly and with strict running time constraints. In this paper, we describe enhancements to the DTW algorithm\u00a0\u2026", "num_citations": "6\n", "authors": ["936"]}
{"title": "Analysis of VINCI 2009-2017 proceedings\n", "abstract": " Both the metadata and the textual contents of scientific publications can provide us with insights about the development and the current state of the corresponding scientific community. In this short paper, we take a look at the proceedings of VINCI from the previous years and conduct several types of analyses. We summarize the yearly statistics about different types of publications, identify the overall authorship statistics and the most prominent contributors, and analyze the current community structure with a co-authorship network. We also apply topic modeling to identify the most prominent topics discussed in the publications. We hope that the results of our work will provide insights for the visualization community and will also be used as an overview for researchers previously unfamiliar with VINCI.", "num_citations": "6\n", "authors": ["936"]}
{"title": "MVN-Reduce: Dimensionality Reduction for the Visual Analysis of Multivariate Networks.\n", "abstract": " Abstract The analysis of Multivariate Networks (MVNs) can be approached from two different perspectives: a multidimensional one, consisting of the nodes and their multiple attributes, or a relational one, consisting of the network\u2019s topology of edges. In order to be comprehensive, a visual representation of an MVN must be able to accommodate both. In this paper, we propose a novel approach for the visualization of MVNs that works by combining these two perspectives into a single unified model, which is used as input to a dimensionality reduction method. The resulting 2D embedding takes into consideration both attribute-and edge-based similarities, with a user-controlled trade-off. We demonstrate our approach by exploring two real-world data sets: a co-authorship network and an open-source software development project. The results point out that our method is able to bring forward features of MVNs that could not be easily perceived from the investigation of the individual perspectives only.", "num_citations": "6\n", "authors": ["936"]}
{"title": "Visual text mining: ensuring the presence of relevant studies in systematic literature reviews\n", "abstract": " One of the activities associated with the Systematic Literature Review (SLR) process is the selection review of primary studies. When the researcher faces large volumes of primary studies to be analyzed, the process used to select studies can be arduous. In a previous experiment, we conducted a pilot test to compare the performance and accuracy of PhD students in conducting the selection review activity manually and using Visual Text Mining (VTM) techniques. The goal of this paper is to describe a replication study involving PhD and Master students. The replication study uses the same experimental design and materials of the original experiment. This study also aims to investigate whether the researcher's level of experience with conducting SLRs and research in general impacts the outcome of the primary study selection step of the SLR process. The replication results have confirmed the outcomes of the\u00a0\u2026", "num_citations": "6\n", "authors": ["936"]}
{"title": "VisEvol: Visual analytics to support hyperparameter search through evolutionary optimization\n", "abstract": " During the training phase of machine learning (ML) models, it is usually necessary to configure several hyperparameters. This process is computationally intensive and requires an extensive search to infer the best hyperparameter set for the given problem. The challenge is exacerbated by the fact that most ML models are complex internally, and training involves trial\u2010and\u2010error processes that could remarkably affect the predictive result. Moreover, each hyperparameter of an ML algorithm is potentially intertwined with the others, and changing it might result in unforeseeable impacts on the remaining hyperparameters. Evolutionary optimization is a promising method to try and address those issues. According to this method, performant models are stored, while the remainder are improved through crossover and mutation processes inspired by genetic algorithms. We present VisEvol, a visual analytics tool that\u00a0\u2026", "num_citations": "5\n", "authors": ["936"]}
{"title": "Xtreaming: an incremental multidimensional projection technique and its application to streaming data\n", "abstract": " Streaming data applications are becoming more common due to the ability of different information sources to continuously capture or produce data, such as sensors and social media. Despite recent advances, most visualization approaches, in particular, multidimensional projection or dimensionality reduction techniques, cannot be directly applied in such scenarios due to the transient nature of streaming data. Currently, only a few methods address this limitation using online or incremental strategies, continuously processing data, and updating the visualization. Despite their relative success, most of them impose the need for storing and accessing the data multiple times, not being appropriate for streaming where data continuously grow. Others do not impose such requirements but are not capable of updating the position of the data already projected, potentially resulting in visual artifacts. In this paper, we present Xtreaming, a novel incremental projection technique that continuously updates the visual representation to reflect new emerging structures or patterns without visiting the multidimensional data more than once. Our tests show that Xtreaming is competitive in terms of global distance preservation if compared to other streaming and incremental techniques, but it is orders of magnitude faster. To the best of our knowledge, it is the first methodology that is capable of evolving a projection to faithfully represent new emerging structures without the need to store all data, providing reliable results for efficiently and effectively projecting streaming data.", "num_citations": "4\n", "authors": ["936"]}
{"title": "Content based visual mining of document collections using ontologies\n", "abstract": " Document collections are important data sets in many applications. It has been shown that content based visual mappings of documents can be done effectively through projection and point placement strategies. An important step in this process is the creation of a vector space model, in which terms selected from the text and weighted are used as attributes for the vector space. That step in many cases impairs the quality of the projection due to the existence, in the data set, of many terms that are frequent but do not represent important concepts in the user\u2019s particular context. This paper proposes and evaluates the use of ontologies for content based visual analysis of textual data sets as a means to improve the displays for the analysis of the collection. The results show that when the ontology effectively represents the data domain it increases quality of maps.", "num_citations": "4\n", "authors": ["936"]}
{"title": "Explanatory visualization of multidimensional projections\n", "abstract": " Visual analytics tools play an important role in the scenario of big data solutions, combining data analysis and interactive visualization techniques in effective ways to support the incremental exploration of large data collections from a wide range of domains. One particular challenge for visual analytics is the analysis of multidimensional datasets, which consist of many observations, each being described by a large number of dimensions, or attributes. Finding and understanding data-related patterns present in such spaces, such as trends, correlations, groups of related observations, and outliers, is hard. Dimensionality reduction methods, or projections, can be used to construct low (two or three) dimensional representations of high-dimensional datasets. The resulting representation can then be used as a proxy for the visual interpretation of the high-dimensional space to efficiently and effectively support the above-mentioned data analysis tasks. Projections have important advantages over other visualization techniques for multidimensional data, such as visual scalability, high degree of robustness to noise and low computational complexity. However, a major obstacle to the effective practical usage of projections relates to their difficult interpretation. Two main types of interpretation challenges for projections are studied in this thesis. First, while projection techniques aim to preserve the so-called structure of the original dataset in the final produced layout, and effectively achieve the proxy effect mentioned earlier, they may introduce a certain amount of errors that influence the interpretation of their results. However, it is hard to convey to users where\u00a0\u2026", "num_citations": "3\n", "authors": ["936"]}
{"title": "Scaling the Growing Neural Gas for Visual Cluster Analysis\n", "abstract": " The growing neural gas (GNG) is an unsupervised topology learning algorithm that models a data space through interconnected units that stand on the most populated areas of that space. Its output is a graph that can be visually represented on a two-dimensional plane, disclosing cluster patterns in datasets. It is common, however, for GNG to result in highly connected graphs when trained on high-dimensional data, which in turn leads to highly cluttered 2D representations that may fail to disclose meaningful patterns. Moreover, its sequential learning limits its potential for faster executions on local datasets, and, more importantly, its potential for training on distributed datasets while leveraging from the computational resources of the infrastructures in which they reside.This paper presents two methods that improve GNG for the visualization of cluster patterns in large-scale and high-dimensional datasets. The first one\u00a0\u2026", "num_citations": "2\n", "authors": ["936"]}
{"title": "Progressive multidimensional projections: A process model based on vector quantization\n", "abstract": " As large datasets become more common, so becomes the necessity for exploratory approaches that allow iterative, trial-anderror analysis. Without such solutions, hypothesis testing and exploratory data analysis may become cumbersome due to long waiting times for feedback from computationally-intensive algorithms. This work presents a process model for progressive multidimensional projections (P-MDPs) that enables early feedback and user involvement in the process, complementing previous work by providing a lower level of abstraction and describing the specific elements that can be used to provide early system feedback, and those which can be enabled for user interaction. Additionally, we outline a set of design constraints that must be taken into account to ensure the usability of a solution regarding feedback time, visual cluttering, and the interactivity of the view. To address these constraints, we propose the use of incremental vector quantization (iVQ) as a core step within the process. To illustrate the feasibility of the model, and the usefulness of the proposed iVQ-based solution, we present a prototype that demonstrates how the different usability constraints can be accounted for, regardless of the size of a dataset.", "num_citations": "2\n", "authors": ["936"]}
{"title": "Visual Learning Analytics of Multidimensional Student Behavior in Self-regulated Learning\n", "abstract": " In Self-Regulated Learning (SLR), the lack of a predefined, formal learning trajectory makes it more challenging to assess students\u2019 progress (e.g. by comparing it to specific baselines) and to offer relevant feedback and scaffolding when appropriate. In this paper we describe a Visual Learning Analytics (VLA) solution for exploring students\u2019 datasets collected in a Web-Based Learning Environment (WBLE). We employ mining techniques for the analysis of multidimensional data, such as t-SNE and clustering, in an exploratory study for identifying patterns of students with similar study behavior and interests. An example use case is presented as evidence of the effectiveness of our proposed method, with a dataset of learning behaviors of 6423 students who used an online study tool during 18 months.", "num_citations": "2\n", "authors": ["936"]}
{"title": "t-viSNE: A visual inspector for the exploration of t-SNE\n", "abstract": " The use of t-Distributed Stochastic Neighborhood Embedding (t-SNE) for the visualization of multidimensional data has proven to be a popular approach, with applications published in a wide range of domains. Despite their usefulness, t-SNE plots can sometimes be hard to interpret or even misleading, which hurts the trustworthiness of the results. By opening the black box of the algorithm and showing insights into its behavior through visualization, we may learn how to use it in a more effective way. In this work, we present t-viSNE, a visual inspection tool that enables users to explore anomalies and assess the quality of t-SNE results by bringing forward aspects of the algorithm that would normally be lost after the dimensionality reduction process is finished.", "num_citations": "2\n", "authors": ["936"]}
{"title": "MIST: Multiscale Information and Summaries of Texts\n", "abstract": " Combining distinct visual metaphors has been the mechanism adopted by several systems to enable the simultaneous visualization of multiple levels of information in a single layout. However, providing a meaningful layout while avoiding visual clutter is still a challenge. In this work we combine word clouds and a rigid-body simulation engine into an intuitive visualization tool that allows a user to visualize and interact with the content of document collections using a single overlap-free layout. The proposed force scheme ensures that neighboring documents are kept close to each other during and after layout change. Each group of neighboring documents formed on the layout generates a word cloud. A multi-seeded procedure guarantees a harmonious arrangement of distinct word clouds in visual space. The visual metaphor employs disks to represent document instances where the size of each disk defines the\u00a0\u2026", "num_citations": "2\n", "authors": ["936"]}
{"title": "Analyzing the quality of local and global multidimensional projections using performance evaluation planning\n", "abstract": " Among the challenges of the big data era, the analysis of high-dimensional data is still an open research area. As a result, several multidimensional projection techniques have been developed to reduce data dimensionality, becoming important visualization and visual analytics tools. In order to ensure the quality of projections, it is necessary to assess the low-dimensional embeddings by using different dataset configurations as input and analyzing evaluation metrics. However, it is not clear to the user how factors such as the number of dimensions, instances, or clusters, can affect the projection mapping and its quality regarding different projection techniques and assessment metrics. The research reported in this paper aims to clarify how much these factors affect each response variable via performance evaluation planning. We present an evaluation approach, supported by factorial design, that carries out a\u00a0\u2026", "num_citations": "1\n", "authors": ["936"]}
{"title": "FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise Selection and Semi-Automatic Extraction Approaches\n", "abstract": " The machine learning (ML) life cycle involves a series of iterative steps, from the effective gathering and preparation of the data, including complex feature engineering processes, to the presentation and improvement of results, with various algorithms to choose from in every step. Feature engineering in particular can be very beneficial for ML, leading to numerous improvements such as boosting the predictive results, decreasing computational times, reducing excessive noise, and increasing the transparency behind the decisions taken during the training. Despite that, while several visual analytics tools exist to monitor and control the different stages of the ML life cycle (especially those related to data and algorithms), feature engineering support remains inadequate. In this paper, we present FeatureEnVi, a visual analytics system specifically designed to assist with the feature engineering process. Our proposed system helps users to choose the most important feature, to transform the original features into powerful alternatives, and to experiment with different feature generation combinations. Additionally, data space slicing allows users to explore the impact of features on both local and global scales. FeatureEnVi utilizes multiple automatic feature selection techniques; furthermore, it visually guides users with statistical evidence about the influence of each feature (or subsets of features). The final outcome is the extraction of heavily engineered features, evaluated by multiple validation metrics. The usefulness and applicability of FeatureEnVi are demonstrated with two use cases, using a popular red wine quality data set and publicly available data\u00a0\u2026", "num_citations": "1\n", "authors": ["936"]}
{"title": "SAVis: a Learning Analytics Dashboard with Interactive Visualization and Machine Learning\n", "abstract": " A dashboard that provides a central location to monitor and analyze data is an efficient way to track multiple data sources. In the educational community, for example, using dashboards can be a straightforward introduction into the concepts of visual learning analytics. In this paper, the design and implementation of Student Activity Visualization (SAVis), a new Learning Analytics Dashboard (LAD) using interactive visualization and Machine Learning (ML) is presented and discussed. The design of the dashboard was directed towards answering a set of 22 pedagogical questions that teachers might want to investigate in an educational dataset. We evaluate SAVis with an educational dataset containing more than two million samples, including the learning behaviors of 6,423 students who used a web-based learning platform for one year. We show how SAVis can deliver relevant information to teachers and support them to interact with and analyze the students\u2019 data to gain a better overview of students\u2019 activities in terms of, for example, their performance in number of correct/incorrect answers per each topic.", "num_citations": "1\n", "authors": ["936"]}
{"title": "Improving Classification in Imbalanced Educational Datasets using Over-sampling\n", "abstract": " Learning Analytics (LA) involves a growing range of methods for understanding and optimizing learning and the environments in which it occurs. Different Machine Learning (ML) algorithms or learning classifiers can be used to implement LA, with the goal of predicting learning outcomes and classifying the data into predetermined categories. Many educational datasets are imbalanced, where the number of samples in one category is significantly larger than in other categories. Ordinarily, it is ML\u2019s performance on the minority categories that is the most important. Since most ML classification algorithms ignore the minority categories, and in turn have poor performance, so learning from imbalanced datasets is really challenging. In order to address this challenge and also to improve the performance of different classifiers, Synthetic Minority Over-sampling Technique (SMOTE) is used to oversample the minority categories. In this paper, the accuracy of seven well-4nown classifiers considering 5 and 10-fold cross-validation and the F1-score are compared. The imbalanced dataset collected based on self-regulated learning activities contains the learning behaviour of 6,423 medical students who used a web-based study platform\u2014Hypocampus\u2014with different educational topics for one year. Also, two diagnostic tools including Area Under the Receiver Operating Characteristics (AUC-ROC) curves and Precision-Recall (PR) curves are applied to predict probabilities of an observation belonging to each category in a classification problem. Using these diagnostic tools may help LA researchers on how to deal with imbalanced educational datasets. The\u00a0\u2026", "num_citations": "1\n", "authors": ["936"]}
{"title": "Overlap Removal of Dimensionality Reduction Scatterplot Layouts\n", "abstract": " Dimensionality Reduction (DR) scatterplot layouts have become a ubiquitous visualization tool for analyzing multidimensional data items with presence in different areas. Despite its popularity, scatterplots suffer from occlusion, especially when markers convey information, making it troublesome for users to estimate items' groups' sizes and, more importantly, potentially obfuscating critical items for the analysis under execution. Different strategies have been devised to address this issue, either producing overlap-free layouts, lacking the powerful capabilities of contemporary DR techniques in uncover interesting data patterns, or eliminating overlaps as a post-processing strategy. Despite the good results of post-processing techniques, the best methods typically expand or distort the scatterplot area, thus reducing markers' size (sometimes) to unreadable dimensions, defeating the purpose of removing overlaps. This paper presents a novel post-processing strategy to remove DR layouts' overlaps that faithfully preserves the original layout's characteristics and markers' sizes. We show that the proposed strategy surpasses the state-of-the-art in overlap removal through an extensive comparative evaluation considering multiple different metrics while it is 2 or 3 orders of magnitude faster for large datasets.", "num_citations": "1\n", "authors": ["936"]}
{"title": "Artifact: Quality Models Inside Out: Interactive Visualization of Software Metrics by Means of Joint Probabilities\n", "abstract": " Assessing software quality, in general, is hard; each metric has a different interpretation, scale, range of values, or measurement method. Combining these metrics automatically is especially difficult, because they measure different aspects of software quality, and creating a single global final quality score limits the evaluation of the specific quality aspects and trade-offs that exist when looking at different metrics. We present a way to visualize multiple aspects of software quality. In general, software quality can be decomposed hierarchically into characteristics, which can be assessed by various direct and indirect metrics. These characteristics are then combined and aggregated to assess the quality of the software system as a whole. We introduce an approach for quality assessment based on joint distributions of metrics values. Visualizations of these distributions allow users to explore and compare the quality metrics of software systems and their artifacts, and to detect patterns, correlations, and anomalies. Furthermore, it is possible to identify common properties and flaws, as our visualization approach provides rich interactions for visual queries to the quality models\u2019 multivariate data. We evaluate our approach in two use cases based on: 30 real-world technical documentation projects with 20,000 XML documents, and an open source project written in Java with 1000 classes. Our results show that the proposed approach allows an analyst to detect possible causes of bad or good quality.", "num_citations": "1\n", "authors": ["936"]}
{"title": "S\u00edntese de algumas iniciativas para Defini\u00e7\u00e3o de um Processo de Desenvolvimento de Software para Sistemas Embarcados Cr\u00edticos, no \u00e2mbito do INCT-SEC\n", "abstract": " Contexto: Em geral, os desenvolvedores de Sistemas Embarcados Cr\u00edticos iniciam o desenvolvimento a partir de modelos de baixo n\u00edvel, como modelos Simulink. Isso dificulta o entendimento da aplica\u00e7\u00e3o e n\u00e3o prov\u00ea uma documenta \u00e7\u00e3o apropriada. Objetivo: O objetivo deste artigo \u00e9 apresentar os trabalhos que est\u00e3o sendo conduzidos com o intuito de definir um processo para desenvolvimento de sistemas embarcados cr\u00edticos, contemple atividades de engenharia de requisitos, de modelagem, e de V&V. M\u00e9todo: Para definir o processo tem-se partido do modelo Simulink e definido os artefatos com base em atividades de engenharia reversa e reengenharia. Resultados e conclus\u00f5es: No estado atual dos trabalhos, os resultados obtidos d\u00e3o ind\u00edcios da contribui\u00e7\u00e3o de algumas t\u00e9cnicas para compor o processo, como por exemplo, UML Statecharts, SysML. Al\u00e9m disso, m\u00e9tricas tem sido aplicadas para avaliar a qualidade dos modelos e do c\u00f3digo.", "num_citations": "1\n", "authors": ["936"]}