{"title": "Value-based software engineering\n", "abstract": " The IT community has always struggled with questions concerning the value of an organization\u2019s investment in software and hardware. It is the goal of value-based software engineering (VBSE) to develop models and measures of value which are of use for managers, developers and users as they make tradeoff decisions between, for example, quality and cost or functionality and schedule\u2013such decisions must be economically feasible and comprehensible to the stakeholders with differing value perspectives. VBSE has its roots in work on software engineering economics, pioneered by Barry Boehm in the early 1980s. However, the emergence of a wider scope that defines VBSE is more recent. VBSE extends the merely technical ISO software engineering definition with elements not only from economics, but also from cognitive science, finance, management science, behavioral sciences, and decision sciences, giving rise to a truly multi-disciplinary framework. Biffl and his co-editors invited leading researchers and structured their contributions into three parts, following an introduction into the area by Boehm himself. They first detail the foundations of VBSE, followed by a presentation of state-of-the-art methods and techniques. The third part demonstrates the benefits of VBSE through concrete examples and case studies. This book deviates from the more anecdotal style of many management-oriented software engineering books and so appeals particularly to all readers who are interested in solid foundations for high-level aspects of software engineering decision making, ie, to product or project managers driven by economics and to software\u00a0\u2026", "num_citations": "510\n", "authors": ["1145"]}
{"title": "On the effectiveness of the test-first approach to programming\n", "abstract": " Test-driven development (TDD) is based on formalizing a piece of functionality as a test, implementing the functionality such that the test passes, and iterating the process. This paper describes a controlled experiment for evaluating an important aspect of TDD: in TDD, programmers write functional tests before the corresponding implementation code. The experiment was conducted with undergraduate students. While the experiment group applied a test-first strategy, the control group applied a more conventional development technique, writing tests after the implementation. Both groups followed an incremental process, adding new features one at a time and regression testing them. We found that test-first students on average wrote more tests and, in turn, students who wrote more tests tended to be more productive. We also observed that the minimum quality increased linearly with the number of programmer tests\u00a0\u2026", "num_citations": "448\n", "authors": ["1145"]}
{"title": "Cloud computing: Does nirvana hide behind the nebula?\n", "abstract": " At the core of cloud computing is a simple concept: software as a service, or SaaS. Whether the underlying software is an application, application component, platform, framework, environment, or some other soft infrastructure for composing applications to be delivered as a service on the Web, it's all software in the end. But the simplicity ends there. Just a step away from that core, a complex concoction of paradigms, concepts, and technologies envelop cloud computing.", "num_citations": "349\n", "authors": ["1145"]}
{"title": "Scaling agile methods\n", "abstract": " Using agile methods to develop large systems presents a thorny set of issues. If large teams are to produce lots of software functionality quickly, the agile methods involved must scale to meet the task. After all, a small team could create the software if the functionality to be delivered was small and, conversely, could be delivered given we had the time. Scaling agile teams thus becomes an issue if the only option for meeting a system delivery deadline is to have many developers working concurrently.", "num_citations": "132\n", "authors": ["1145"]}
{"title": "Return on investment\n", "abstract": " Wolfgang Strigel, Software Productivity Center, Canada prices might lead to targeting the investment in quality to specific parts of the software product or service. In this way, the value of increasing software quality is framed in terms that senior management can understand and use to make informed decisions. But even technical personnel can benefit from familiarity with this type of analysis. From the new agile development methodologies that explicitly incorporate notions of customer value to the latest asset-based repositories such as product lines, it\u2019s clear that the software industry currently views few initiatives from a purely technical perspective.", "num_citations": "87\n", "authors": ["1145"]}
{"title": "Keep your options open: Extreme programming and the economics of flexibility\n", "abstract": " Financial evaluation and strategic analysis have long been considered two distinct approaches to evaluating new capital initiatives. An emerging valuation approach, known as real options, attempts to align finance and strategy through a new perspective: The value of an asset lies not only in the amount of direct revenues that it is expected to generate, but also in the options that it creates for flexible decision making in the future. In general, the more uncertain the future is, the higher the value of flexibility embedded in an asset, whether financial or real. This perspective has significant implications for the economics of flexible processes. Applied to software development, it could imply that a lightweight process that is well positioned to respond to change and future opportunities creates more value than a heavy-duty process that tends to freeze development decisions early. Thus, the feasibility of Extreme Programming (XP) can be supported by the option value of flexibility inherent in it. What is the theory that underlies this statement? How does it relate to the fundamental assumptions of XP? How does it impact the value of an XP project? What are the implications of such value propositions for project decisions? If you are curious, read on\u2026", "num_citations": "64\n", "authors": ["1145"]}
{"title": "Quantitative approaches for assessing the value of COTS-centric development\n", "abstract": " Software development based on commercial off-the-shelf or COTS, components is currently drawing considerable attention. This paper presents the results of two complementary quantitative valuation methods applied to the assessment of the COTS-centric software development projects. We use a standard corporate finance tool, Net Present Value, as a basis for both methods. The first method, comparative valuation, investigates the economic incentive to choose the COTS centric strategy in a project vis a vis the alternative, the custom development strategy, through an incentive metric based on NPV. The analysis concentrates on the impact of product risk and development time on the defined metric. The second method, real options valuation, primarily deals with uncertainty. It is employed to investigate the value of strategic flexibility inherent in COTS-centric development. We provide examples of several such\u00a0\u2026", "num_citations": "63\n", "authors": ["1145"]}
{"title": "Comparative evaluation of software development strategies based on Net Present Value\n", "abstract": " Assessment of alternative development strategies in a software project can be difficult due to the interactions among the multiple factors involved. A rigorous, value-based approach allows a more objective comparison of the available alternatives. For example, such an approach can be used to analyze the economic incentive to choose a strategy that promises rapid development over a strategy that promises high earnings. The corporate finance concept of Net Present Value can be used in this context to define a hierarchy of comparison metrics based on six high-level variables. The toplevel metric measures relative economic incentive. The analysis focuses on the impact of product risk and rapid development on the decision to select the most valuable strategy.", "num_citations": "54\n", "authors": ["1145"]}
{"title": "Valuation of learning options in software development under private and market risk\n", "abstract": " Commercial software development is an inherently uncertain activity. Private risk is high, schedule and cost overruns are common, and market success is elusive. Such circumstances call for a disciplined project evaluation approach. This paper addresses the use of market and earned value management data in assessing the economic value of commercial software development projects that are simultaneously subject to schedule, development cost, and market risk. The assessment is based on real options analysis, a financial valuation technique that can tackle dynamic investment decisions under uncertainty. The paper demonstrates the application of real options analysis to a development scenario that consists of two consecutive stages: a mandatory prototyping stage and an optional full-development stage. The full-development stage is undertaken only if the prototype is successful and the market\u00a0\u2026", "num_citations": "53\n", "authors": ["1145"]}
{"title": "Operational definition and automated inference of test-driven development with Zorro\n", "abstract": " Test-driven development (TDD) is a style of development named for its most visible characteristic: the design and implementation of test cases prior to the implementation of the code required to make them pass. Many claims have been made for TDD: that it can improve implementation as well as design quality, that it can improve productivity, that it results in 100% coverage, and so forth. However, research to validate these claims has yielded mixed and sometimes contradictory results. We believe that at least part of the reason for these results stems from differing interpretations of the TDD development style, along with an inability to determine whether programmers actually follow whatever definition of TDD is in use.               Zorro is a system designed to automatically determine whether a developer is complying with an operational definition of Test-Driven Development (TDD) practices. Automated\u00a0\u2026", "num_citations": "49\n", "authors": ["1145"]}
{"title": "The economic impact of learning and flexibility on process decisions\n", "abstract": " The perception that software projects have unusually high failure rates has fueled the debate on software process. In the 1990s, the Standish Group estimated that the total economic toll of cancelled or overrun software projects could reach several tens of billions of dollars in terms of wasted effort and opportunity costs. Worse yet, even when delivered on time and on budget, software rarely has any significant salvage value when it fails to meet user needs. This article argues for an economic basis for rationalizing process selection decisions. It demonstrates how, under conditions of uncertainty, learning and flexibility affect such decisions. Uncertainty is a critical driver in process selection because it's ubiquitous in software development and it determines the degree of flexibility and learning needed to maximize economic value.", "num_citations": "36\n", "authors": ["1145"]}
{"title": "Architecture meets agility\n", "abstract": " In an Agile Timesarticle published several years ago (\u201cLet\u2019s Scale Agile Up,\u201d Agile Times, Apr. 2003), I argued against attempts to scale up agile software development approaches beyond their intended home ground of self-contained projects undertaken by small coherent teams. I still don\u2019t agree with advocating an \u201cagile\u201d approach for anything and everything under the sun. If you\u2019re developing an interpreter for a domain-specific language whose specification is known, what\u2019s the point of insisting on using short time-boxed iterations? However, I\u2019ve changed my opinions about the need to port the practices and processes included nowadays under the agile umbrella. I\u2019ve come to appreciate the benefits of moving those practices and processes, selectively and with necessary adaptations and embellishments, to different application domains and project contexts. And the concept of architecture, although still downplayed by many agile advocates, has a major role to play in expanding the traditional scope of agile software development. The architecture-agility reconciliation was unmistakable at a May 2009 workshop organized by IEEE Software, the University of Southern California, and the University of California, Irvine (see http://computingnow. computer. org/sac21). The stamp of endorsement on that partnership was posted simultaneously, but independently, by no fewer than two gurus whose professional lives have straddled the ordinarily separate worlds of the two realms. They are Software\u2019s very own Philippe Kruchten and Grady Booch. Such convergence on the part of two pioneers may not be so surprising given that their paths crossed in\u00a0\u2026", "num_citations": "33\n", "authors": ["1145"]}
{"title": "Value of commercial software development under technology risk\n", "abstract": " Jean-Fran\u00e7ois LAPOINTE and Pierre BOULANGER National Research Council of Canada Institute for Information Technology 1200 Montreal Road, Building M-50, Ottawa, Ontario, Canada, K1A 0R6 [Jean-Francois. Lapointe][Pierre. Boulanger]@ nrc. ca", "num_citations": "31\n", "authors": ["1145"]}
{"title": "Valuation of software initiatives under uncertainty: concepts, issues, and techniques\n", "abstract": " State of the practice in software engineering economics often focuses exclusively on cost issues and technical considerations for decision making. Value-based software engineering (VBSE) expands the cost focus by also considering benefits, opportunities, and risks. Of central importance in this context is valuation, the process for determining the economic value of a product, service, or a process. Uncertainty is a major challenge in the valuation of software assets and projects. This chapter first introduces uncertainty along with other significant issues and concepts in valuation, and surveys the relevant literature. Then it discusses decision tree and options-based techniques to demonstrate how valuation can help with dynamic decision making under uncertainty in software development projects.", "num_citations": "30\n", "authors": ["1145"]}
{"title": "The role of process measurement in test-driven development\n", "abstract": " Test-Driven Development (TDD) is a coding technique in which programmers write unit tests before writing or revising production code. We present a process measurement approach for TDD that relies on the analysis of fine-grained data collected during coding activities. This data is mined to produce abstractions regarding programmers\u2019 work patterns. Programmers, instructors, and coaches receive concrete feedback by visualizing these abstractions. Process measurement has the potential to accelerate the learning of TDD, enhance its effectiveness, aid in its empirical evaluation, and support project tracking.", "num_citations": "28\n", "authors": ["1145"]}
{"title": "Advances in Software Engineering: Comprehension, Evaluation and Evolution\n", "abstract": " Software engineering is a rapidly growing and changing field. Over the last dec ade, it has gained significant popularity, and it is now heralded as a discipline of its own. This edited collection presents recent advances in software engineering in the areas of evolution, comprehension, and evaluation. The theme of the book addresses the increasing need to understand and assess software systems in order to measure their quality, maintain them, adapt them to changing requirements and technology, and migrate them to new platforms. This need can be satisfied by studying how software systems are built and maintained, by finding new paradigms, and by building new tools to support the activities involved in devel oping contemporary software systems. The contributions to the book are from major results and findings of leading researchers, under the mandate of the Consortium for Software Engineering Re search (CSER). CSER has been in existence since 1996. The five founding in dustrial and academic partners wanted to create a research environment that would appeal to the applied nature of the industrial partners, as well as to ad vance the state of the art and develop fresh expertise. The research projects of the Consortium are partially funded by the industrial partners, and partially by the Natural Sciences and Engineering Research Council of Canada. Technical and administrative management of the Consortium is provided by the National Research Council of Canada-specifically by members of the Software Engi neering Group ofthe Institute for Information Technology.", "num_citations": "24\n", "authors": ["1145"]}
{"title": "Valuation of complex options in software development\n", "abstract": " Embedded strategic flexibility may have a significant impact on the net worth of a software development project. Disregarding the additional value of flexibility may make a project look less attractive than it actually is. In this position paper we show how strategic flexibility in software projects can be valued in a practical and methodical manner using the concept of real options. The valuation method relies on the identification of two basic kinds of constructs for composing options: staggering and nesting. These two constructs allow the formulation and valuation of a complex strategic scenario as a portfolio of real options. We motivate the work with several examples of options found in software development involving COTS components.", "num_citations": "23\n", "authors": ["1145"]}
{"title": "Software engineering economics: background, current practices, and future directions\n", "abstract": " The field of software economics seeks to develop technical theories, guidelines, and practices of software development based on sound, established, and emerging models of value and value-creation-adapted to the domain of software development as necessary. The premise of the field is that software development is an ongoing investment activity-in which developers and managers continually make investment decisions requiring the expenditure of valuable resources, such as time, talent, and money. The overriding aim of this activity is to maximize the value added subject to an equitable distribution among the participating stakeholders. The goal of the paper is to expose the audience to this line of thinking and introduce the tools pertinent to its pursuit. The paper is designed to be self-contained and will cover concepts from introductory to advanced. Both practitioners and researchers with an interest in the\u00a0\u2026", "num_citations": "22\n", "authors": ["1145"]}
{"title": "Essentials of software process\n", "abstract": " Process trends can be placed inside a triangular map according to their emphasis on three aspects, represented by the vertices: people, technology, and rigor. Plan-oriented, engineering, and research-based approaches tend to view software as a rigid artifact, so they stress technology and rigor over people. Evolutionary approaches tend to view software development as an organic, skills-driven technical activity, so they stress people and technology over rigor. But this scheme of positioning process approaches is rather rough. A more complete scheme requires dissection in terms of seven essential, mutually reinforcing characteristics: human-centricity, technical orientation, discipline, pragmatism, empiricism, experimentation, and value orientation.", "num_citations": "17\n", "authors": ["1145"]}
{"title": "Management of license cost uncertainty in software development: a real options approach\n", "abstract": " Software development based on Commercial Off-the-Shelf products is subject to multiple sources of uncertainty. One potential source of uncertainty is the license costs of the COTS product used in the system. The management of such uncertainty requires strategies that effectively mitigate the underlying risk with minimal impact on the economic value generated. This paper presents such a strategy, and shows how it can be evaluated using a state-of-the-art financial valuation technique, namely, real options analysis.", "num_citations": "17\n", "authors": ["1145"]}
{"title": "Flipping a graduate-level software engineering foundations course\n", "abstract": " Creating a graduate-level software engineering breadth course is challenging. The scope is wide. Students prefer hands-on work over theory. Industry increasingly values soft skills. Changing software technology requires the syllabus to be technology-agnostic, yet abstracting away technology compromises realism. Instructors must balance scope with depth of learning. At Carnegie Mellon University, we designed a flipped-classroom course that tackles these tradeoffs. The course has been offered since Fall 2014 in the Silicon Valley campus. In this paper, we describe the course's key features and summarize our experiences and lessons learned while designing, teaching, and maintaining it. We found that the pure flipped-classroom format was not optimal in ensuring sufficient transfer of knowledge, especially in remote settings. We initially underestimated teaching assistantship resources. We gradually\u00a0\u2026", "num_citations": "16\n", "authors": ["1145"]}
{"title": "Tracking progress through earned value\n", "abstract": " Considered a gross misnomer by some, earned value for progress tracking in software development projects is still little known in the software industry. But the concept shouldn't be so unfamiliar: tracking progress by earned value is similar to tracking progress through burndown charts-the ubiquitous, simple and powerful visuals that are so popular in agile software development. The two techniques, although developed independently in very different contexts, are similar in terms of their information content. This isn't a new discovery: Alistair Cockburn talked about the relationship between earned value and burndown charts as early as 2004 in Chapter 3 of his book Crystal Clear (Addison-Wesley, 2004). Section 7.3 of Gngori Melnik's and Gerard Meszaros' volume 1 of Acceptance Test Engineering Guide also discusses the same relationship.", "num_citations": "16\n", "authors": ["1145"]}
{"title": "Test-Driven Development.\n", "abstract": " Test-driven development (TDD) is a software development approach using a growing scaffold of tests that guide and support the production of code. This entry describes TDD, explains the underlying dynamics, provides a small worked example, and offers a theory of how and why it works. It relates TDD to other approaches that employ a similar style of development and presents a summary of the empirical results about TDD\u2019s effectiveness. The entry also raises some of the known concerns and challenges about this development practice and provides pointers to solutions. TDD is a practice that has widespread impact on the software development lifecycle. Adopting it takes a great amount of discipline. However, we cannot say whether TDD is right for everyone and for all types of software: this entry should help the reader decide whether to explore further.", "num_citations": "15\n", "authors": ["1145"]}
{"title": "An empirical characterization of scientific software development projects according to the Boehm and Turner model: A progress report\n", "abstract": " A number of recent studies reported on the success of applying agile methods in scientific software development projects. These studies found that agile methods are well suited to the exploratory, iterative, and collaborative nature of scientific inquiry. However, these findings might not be applicable in all situations pertaining to scientific software development projects. In addition, they only constitute a subset of the important factors while deciding which development methods and practices should be adopted. Therefore, it becomes important to conduct further research before making recommendations regarding the adoption of certain development methods and practices in this domain. In this progress report, we discuss our on-going research that will empirically study the characteristics of various scientific software development projects according to a model suggested by Boehm and Turner. We plan to conduct\u00a0\u2026", "num_citations": "15\n", "authors": ["1145"]}
{"title": "An environment for collaborative iteration planning\n", "abstract": " Existing project planning software for agile development processes offers limited support for face-to-face, synchronous collaboration. In this paper, we describe an environment, AgilePlanner, that supports team collaboration during planning meetings. Our approach utilizes advanced technologies of pen computing and digital tabletops to create a collaborative work environment to emulate project planning using index cards. It combines the benefits of paper index cards with those of traditional desktop planning solutions. AgilePlanner is intended as an integral resource in the planning process.", "num_citations": "12\n", "authors": ["1145"]}
{"title": "Codeaware: sensor-based fine-grained monitoring and management of software artifacts\n", "abstract": " Current continuous integration (CI) tools, although extensible, can be limiting in terms of flexibility. In particular, artifact analysis capabilities available through plug in mechanisms are both coarse-grained and centralized. To address this limitation, this paper introduces a new paradigm, Code Aware, for distributed and fine-grained artifact analysis. Code Aware is an ecosystem inspired by sensor networks, consisting of monitors and actuators, aimed at improving code quality and team productivity. Code ware's vision entails (a) the ability to probe software artifacts of any granularity and localization, from variables to classes or files to entire systems, (b) the ability to perform both static and dynamic analyses on these artifacts, and (c) the ability to describe targeted remediation actions, for example to notify interested developers, through automated actuators. We provide motivational examples for the use of Code Aware\u00a0\u2026", "num_citations": "11\n", "authors": ["1145"]}
{"title": "How Important Is Evidence, Really?\n", "abstract": " The utility of evidence in the adoption of software engineering ideas depends on several factors. The type of evidence, the adoption context, the attitudes of decision makers, and the size of the idea and its bundle all play a role in the adoption decision. Feasibility check might suffice for small, viral ideas, whereas systematic evidence might be warranted for medium-scale ideas considered for limited-scale but rapid adoption.", "num_citations": "11\n", "authors": ["1145"]}
{"title": "Representing architectural evolution\n", "abstract": " Software engineers informally use block diagrams with boxes and lines to express system architectures. Diagrammatic representations of this type are also found in many specification techniques. However, rarely are architectural documents containing such representations systematically maintained; as a system evolves, architectural documents become obsolete, and the design history of the system is ultimately lost. Additionally, box-and-line representations used in these documents do not possess a precise semantics invariant across the different techniques that rely on them. This paper addresses expression of system evolution at the architectural level based on a formal model of box-and-line diagrams. The formal model (a) provides semantic uniformity and precision; and (b) allows evolutionary steps to be represented as structural transformations. Interesting classes of such transformations are characterized in terms of the underlying operators. With these tools, the architectural evolution of a system is captured as a directed acyclic graph of baselines, where each baseline consists of a system of box-and-line diagrams, and is mapped to a successor baseline by a set of structural transformations. It is also shown how familiar design concepts\u2014such as extension, abstraction, and structural refinement\u2014can be formalized in simple terms within the framework developed.", "num_citations": "11\n", "authors": ["1145"]}
{"title": "Verifying semantic relations in SPIN\n", "abstract": " Spin is a general verification tool for proving correctness properties of concurrent/distributed systems specified in the CSP-like modeling language Promela. We extended Promela\u2019s syntax to differentiate between external and internal transitions in a given model and the Spin tool with the ability to verify a particular class of semantic relations between two Promela models. This document describes this extension and gives an overview of the relevant theoretical foundations.", "num_citations": "11\n", "authors": ["1145"]}
{"title": "A process that is not\n", "abstract": " TikiWiki is a large open source project that embraces Eric Raymond's Bazaar model. TikiWiki is highly successful, yet it adopts a process that lacks many of the characteristics that are thought to be necessary for projects of comparable size and complexity. The article discusses TikiWiki's development in the context of these characteristics.", "num_citations": "10\n", "authors": ["1145"]}
{"title": "Building a business case for cots-centric development: an investment analysis perspective\n", "abstract": " Software development centered on Commercial Off-the-Shelf, or COTS, components is becoming exceedingly important. However, building a business case for COTS-centric development may be hard, owing to the risks and uncertainties involved. Investment analysis can be a valuable tool to support the business case for a new software project. This position paper discusses an approach based on economic value and two complementary investment analysis techniques. Comparative evaluation is proposed to analyze the economic incentive to choose a COTS-centric strategy in a project vis \u00e0 vis the alternative, the custom development strategy, through an appropriately defined metric. Real options analysis is proposed to value strategic flexibility inherent in COTS-centric development in the face of uncertainty.", "num_citations": "10\n", "authors": ["1145"]}
{"title": "A cost effectiveness indicator for software development\n", "abstract": " Product quality, development productivity, and staffing needs are main cost drivers in software development. The paper proposes a cost-effectiveness indicator that combines these drivers using an economic criterion.", "num_citations": "9\n", "authors": ["1145"]}
{"title": "On-demand enterprise services: Where's the catch?\n", "abstract": " Enterprise software is plainly problematic, no matter whose perspective you take - that of the vendor selling it, the organization acquiring and operating it, or the people using it. Although software as a service promises to address some of the ills afflicting product-based enterprise software, caveats remain.", "num_citations": "8\n", "authors": ["1145"]}
{"title": "What's good software, anyway?\n", "abstract": " If you plan to license a piece of code to a company, what objectively testable guarantees could you offer about the source code so that the people who will be responsible for its future development feel comfortable taking on the challenge? What criteria collectively constitute a definitive, objective trademark of good software? Hakan and a colleague analyze this question in detail.", "num_citations": "8\n", "authors": ["1145"]}
{"title": "Extreme Programming and Agile Methods-XP/Agile Universe 2004: 4th Conference on Extreme Programming and Agile Methods, Calgary, Canada, August 15-18, 2004; Proceedings\n", "abstract": " It was 1999 when Extreme Programming Explained was? rst published, making this year\u2019s event arguably the? fth anniversary of the birth of the XP/Agile movement in software development. Our fourth conference re? ected the evolution and the learning that have occurred in these exciting? ve years as agile practices have become part of the mainstream in software development. These pages are the proceedingsof XP Agile Universe 2004, held in beautiful Calgary, gateway to the Canadian Rockies, in Alberta, Canada. Evidentintheconferenceis thefactthatourlearningis still inits earlystages. While at times overlooked, adaptation has beena core principleof agile software development since the earliest literature on the subject. The conference and these proceedings re-force that principle. Although some organizations are able to practice agile methods in the near-pure form, most are not, re? ecting just how radically innovativethese methods areto thisday. Anyinnovationmustcoexistwithan existingenvironmentandagileso-ware development is no different. There are numerous challenges confronting IT and software development organizations today, with many solutions pitched by a cadre of advocates. Be it CMM, offshoring, outsourcing, security, or one of many other current topics in the industry, teams using or transitioning to Extreme Programming and other agile practices must integrate with the rest of the organization in order to succeed. The papers here offer some of the latest experiences that teams are having in those efforts. XP Agile Universe 2004consisted of workshops, tutorials, papers, panels, the Open Space session, the Educators\u00a0\u2026", "num_citations": "8\n", "authors": ["1145"]}
{"title": "Formal Verification Based on Relation Checking in SPIN: A Case Study\n", "abstract": " A case study in formal verification of concurrent/distributed software is presented. The study concerns the modular specification and verification of a remote task protocol. The verification methodology used is based on semantic equivalence checking and is applicable to systems with hierarchical architectures. To support the methodology, we extended the verification tool Spin with the ability to check a particular class of semantic relations, and the language Promela upon which Spin is based with a simple mechanism to specify external operations. The foundations of semantic equivalence checking are also discussed briefly.", "num_citations": "8\n", "authors": ["1145"]}
{"title": "A formal framework for software architectures\n", "abstract": " A formal framework for reasoning about architectural properties of software systems is presented. The systems of interest are represented as hierarchies of interconnected components. The main concept introduced is that of a module whose single most important attribute is its architecture, defined by means of an interface and a description of the module's internal configuration in terms of components and connections between them. The components are themselves instances of other modules. In this context, a central notion of is that of two systems being architecturally equivalent. It is also discussed how architectural refinement and extension can be formalized. A link with behavioral theories is established by introducing a notion of functional equivalence derived from architectural properties.## Contents 1 Introduction 3 2 Concepts 4 2.1 Static Modules.................................. 4 2.2 Interface of a Module............................... 5 2.3 Designation of a Module........", "num_citations": "8\n", "authors": ["1145"]}
{"title": "D\u00e9j\u00e0 Vu: the life of software engineering ideas\n", "abstract": " The story of software engineering since the label came into use is thus a story of compromise among generality and specificity, heuristics and formalism, procedures and data, sequence and cycle. The practical response was combination and accommodation-covering all bases or splitting the difference, synthesizing complementary approaches or accommodating inescapable trade-offs. Pragmatists argued for mixed strategies of testing and proving, the use of tailored reliability models and development environments, the use of a full set of metrics, and the synthesis of life-cycle models. But while seizing the middle ground appeared to be a practical way to cope with difficulties, it seemed unlikely to produce a revolution. If software technologists are nowadays devoting more effort to engaging in a pragmatic fashion with the complexity of their problems, it is to their credit. That is symptomatic of maturity and of real\u00a0\u2026", "num_citations": "7\n", "authors": ["1145"]}
{"title": "Agile's coming of age... or not\n", "abstract": " The agile software development movement is displaying signs of maturity, with sobering, reconciliatory, and reflective undertones. An unscientific, informal survey of attendees at the Agile 2007 conference yielded several interesting results. Most respondents were not newcomers to the agile field. A small majority of respondents believed that agile had come of age and become mainstream. And an overwhelming majority said that \"agile\" had become a buzzword and that the field had had impact.", "num_citations": "7\n", "authors": ["1145"]}
{"title": "Diversity and Software Development\n", "abstract": " University Press, 2007), diversity is good beyond altruistic and moral reasons. And many insights drawn from his thinking apply to software development practice, research, and education. Page suggests that diversity enhances our ability to tackle hard problems and make accurate predictions. Our ability to tackle hard problems affects our productivity, and our ability to make good predictions impacts how we make decisions and deal with risk and uncertainty. Clearly, software development can benefit from improvement in both dimensions. Page\u2019s conclusions aren\u2019t based on an empirical model. His theoretical framework builds on a few simple but beautifully intuitive concepts, definitions, and axioms. His insights are based on provable mathematical assertions with sufficient and necessary conditions rather than on rules of thumb based on observations of human behavior. Thus, they are refutable to the extent that the underlying concepts, definitions, and axioms fail to capture real-world behavior or to the extent that the stipulated conditions fail to hold true. And surely, such failure is possible in many situations. The insights are nevertheless invaluable.", "num_citations": "6\n", "authors": ["1145"]}
{"title": "Tips for software authors\n", "abstract": " If you're thinking of writing for IEEE Software, you'll increase the odds of a favorable outcome by following a few simple tips. The current issue marks the fifth anniversary of the publication of former editor in chief Steve McConnell's essay \"How to Write a Good Technical Article\" (September/October 2002), and Steve's advice is as valid today as it was then. So, this is an opportune time to review his advice and point out what makes or breaks a candidate article.", "num_citations": "6\n", "authors": ["1145"]}
{"title": "Cots workshop: continuing collaborations for successful cots development\n", "abstract": " In early June of 2000 a COTS Workshop entitled \"Continuing Collaborations for Successful COTS Development\" was held in Limerick, Ireland in conjunction with ICSE 2000. The purpose of the workshop was to collect experience reports regarding the use of commercial off-the-shelf (COTS) software to build systems, identify best-practices for the use of COTS software, and to establish a research agenda for those researchers interested in COTS-based software systems. This one and a half day workshop was an extension of the work begun during the workshop entitled \"Ensuring Successful COTS Development\" held in conjunction with ICSE '99. Results from that workshop demonstrated that there were a number of common research areas, including acquisition, planning and management, architecture and implementation, and evaluation and testing, for which researchers saw the possibility of collaboration. These\u00a0\u2026", "num_citations": "6\n", "authors": ["1145"]}
{"title": "OpenAlerts: A software system to evaluate smart emergency alerts and notifications\n", "abstract": " Alerts and notifications are increasingly being used in emergency situations. However, notifying users when an alert is not contextually useful results in those users opting out of this mode of alerting. There is a need to extensively evaluate such notifications and understand their utility before adopting them in real-world alert systems.", "num_citations": "5\n", "authors": ["1145"]}
{"title": "A real options perspective of software reuse\n", "abstract": " Economics-Driven Software Engineering Research (EDSER) seeks to develop technical theories, guidelines, and practices of software development based on sound, established, and new models of value-creation, adapted to the domain of software development as necessary [1, 2]. Most advances in software engineering have not been based upon generally accepted ways of thinking about economic and financial value in any significant way. The EDSER community is addressing this gap by adopting and tailoring work from other disciplines, such as economics, finance, decision theory, and game theory, to software engineering problems. Applications range from improving technical and design decisions to the evaluation of best practices and process choices, from project and risk management to the treatment of contract and industry structures.Real options analysis is one such tool that is well suited to address many software engineering problems from a valueoriented perspective. These problems are often strategic in nature and involve a variety technical, managerial and process decisions. Strategic software reuse falls under this category, and thus can also benefit from the insight that real options analysis provide.", "num_citations": "5\n", "authors": ["1145"]}
{"title": "Architecture-driven verification of concurrent systems\n", "abstract": " This paper proposes a method to construct a set of proof obligations from the architectural specification of a concurrent system. The architectural specifications used express correctness requirements of a concurrent system at a high level without any reference to component functionality. Then the proof obligations derived from such specifications are discharged as model checking tasks in a suitable behavioral model where components are assigned their respective functionalities. An experimental extension to the SPIN tool is used as the model checker. The block diagram notation used to specify architectures allows interchangeable components with equivalent intended functionalities to be encapsulated within a representative module. A proof obligation of such a system is discharged as an equivalence checking task in the behavioral model chosen. It is shown how infeasible proof obligations can be decomposed by decomposing the architectural specification. Obligation decomposition relies on assume-guarantee conditions.", "num_citations": "5\n", "authors": ["1145"]}
{"title": "Cost-benefit analysis of software development techniques and practices\n", "abstract": " Investigations of software development practices, processes, and techniques frequently report separately on the costs and benefits of a phenomenon under study, but rarely adequately address the combined bottomline implications. In particular, tensions between the quality and productivity effects are hard to reconcile, making objective, high-level insights elusive. For example, is a practice that is believed to improve product quality significantly, but incurs a mild developer productivity penalty economically feasible? In other words, do the benefits outweigh the costs? And if they do, under which conditions? Such questions can be tackled through synthesizing effects and analyzing the resulting behaviors. In this light, the tutorial presented an approach that leverages well-known and simple economic concepts and models. It is used to wrap empirical findings, and is also applicable to the assessment of software projects\u00a0\u2026", "num_citations": "4\n", "authors": ["1145"]}
{"title": "On the operational semantics of nondeterminism and divergence\n", "abstract": " An operational model of nondeterministic processes coupled with a novel theory of divergence is presented. The operational model represents internal nondeterminism without using explicit internal transitions. Here the notion of internal state effectively replaces the familiar notion of internal transition, giving rise to an alternative operational view of processes: the weak process. Roughly, a weak process is a collection of stable internal states together with a set of transitions each of which is defined from an internal state to another weak process. Internal nondeterminism arises from such refinement of processes into multiple internal states.A simple extension to the basic weak process model gives rise to an elaborate operational theory of divergence. According to this theory, the ability of a process to undertake an infinite internal computation which is pathological, or persistent, is distinguished from its ability to\u00a0\u2026", "num_citations": "4\n", "authors": ["1145"]}
{"title": "Introducing low-stakes just-in-time assessments to a flipped software engineering course\n", "abstract": " Objective: We present a Teaching-as-Research project that implements a new intervention in a flipped software engineering course over two semesters. The short-term objective of the intervention was to improve students\u2019 preparedness for live sessions. The long-term objective was to improve their knowledge retention evaluated in time-separated high-stakes assessments. Intervention: The intervention involved adding weekly low-stakes just-in-time assessments to course modules to motivate students to review assigned instructional materials in a timely manner. The assessments consisted of, per course module, two preparatory quizzes embedded within off-class instructional materials and a non-embedded in-class quiz. Method: Embedded assessments were deployed to two subgroups of students in an alternating manner. In-class assessments were deployed to all students. The impact of embedded assessments on in-class assessments and on final exam performance was measured. Results: Embedded assessments improved students\u2019 preparedness for live sessions. The effect was statistically significant, but variable. Embedded assessments did not impact long-term knowledge retention assessed on final exam. We have decided to keep the intervention and deploy it to all students in the future.", "num_citations": "3\n", "authors": ["1145"]}
{"title": "Cost effectiveness analysis in software engineering\n", "abstract": " The tutorial presented an approach that leverages well-known economic and financial concepts for evaluating the cost effectiveness of software development processes and techniques. Software engineering studies often report separately on the costs and benefits of a phenomenon of interest, and rarely adequately address the combined bottom line implications. In particular, tensions between quality and productivity are hard to reconcile, making objective, high-level insights elusive. To address this need, the tutorial focused on quantitative methods for synthesizing co-dependent cost-benefit effects and analyzing the resulting behaviors.", "num_citations": "3\n", "authors": ["1145"]}
{"title": "The infamous ratio measure\n", "abstract": " IEEE Software article \u201cMisleading Metrics and Unsound Analyses\u201d(by Barbara Kitchenham, David Ross Jeffery, and Colin Connaughton) drew attention to the pitfalls of using ratios; however, ratios and other derived metrics are inevitable in software measurement. An appreciation of what they\u2019re good for, and not good for, can help to avoid these pitfalls. I focus on ratios here.", "num_citations": "3\n", "authors": ["1145"]}
{"title": "From configurations to styles: An algebraic theory\n", "abstract": " /Resume.................................... v 1 Introduction 1 2 Configurations 3 2.1 Syntax....................................... 3 2.2 Constructs..................................... 4 2.2. 1 Typed Nodes............................... 5 2.2. 2 Naming a Single Node.......................... 5 2.2. 3 Grouping................................. 6 2.2. 4 Singular Connection........................... 6 2.2. 5 Hiding................................... 7 2.2. 6 Universal Renaming........................... 7 2.2. 7 Naming a Group of Nodes........................ 8 2.2. 8 Simultaneous Connections........................ 8 2.3 Denotational Semantics............................. 9 2.4 Axiomatic Semantics............................... 11 2.5 Soundness, Normal Forms, and Completeness................. 14 3 Styles 17 3.1 Syntax....................................... 18 3.2 New Constructs.................................. 22 3.2. 1 The Empty Style............................. 22 3.2. 2...", "num_citations": "3\n", "authors": ["1145"]}
{"title": "Seven essentials of software process\n", "abstract": " A hybrid view of software development has risen from the collision and convergence of grassroots and craft-based process trends with research and engineering-based ones. In this talk, I will first explain the seven essential characteristics of this contemporary view: human-centricity, pragmatism, discipline, technical orientation, empiricism, experimentation, and value orientation. These characteristics are not simply complementary: they are mutually reinforcing and mutually balancing. I will then focus on the seventh characteristic, value orientation, in terms of how it interacts with the remaining characteristics. The resulting thinking sheds light to weaknesses and strengths of current packaged approaches, and how successful software development is expected to unfold and effective strategies will be uncovered in the era to come.", "num_citations": "2\n", "authors": ["1145"]}
{"title": "So many languages, so little time\n", "abstract": " What's up and coming in the programming language arena? A rudimentary analysis of the 200+ sessions' titles and abstracts at OOPSLA 07 (22nd Int'l Conf. Object-Oriented Programming, Systems, Languages, and Applications) provides a rough idea of what's happening with object-oriented, functional, dynamic, and domain-specific languages.", "num_citations": "2\n", "authors": ["1145"]}
{"title": "Aligning Software Development Investment Decisions with the Markets\n", "abstract": " Risk, or exposure to uncertainty, is an inherent part of software development. Therefore, risk is an important factor in making investment decisions about software development. This position paper illustrates a disciplined approach to treating risk, an approach that allows software development decisions to be aligned with the capital markets. We start from the premise that uncertainty creates value when managed properly. Then we show using a real options approach that if the markets contain traded assets whose values depend on the source of uncertainty to which the investment is subject, we can estimate the resulting risk, and then use this estimate to determine the value of the investment. The approach is applied to a familiar software development scenario involving technology risk, namely that of Java.", "num_citations": "2\n", "authors": ["1145"]}
{"title": "COTS Process Issues in Military Applications\n", "abstract": " The growing emphasis on the use of commercial off-theshelf, or COTS, products in the software arena has caused a fundamental change to the way the next generation of military combat systems will be procured and developed. With that change comes the need to clarify and understand this new development paradigm. This paper is an attempt to bring out the main features of the COTS-based system development process and identify the critical issues. The opinions expressed in this document resulted from a twoday meeting held in January 2000 at the Software Engineering Institute.", "num_citations": "2\n", "authors": ["1145"]}
{"title": "Aligning Software Investment Decisions with the Markets\n", "abstract": " Risk, or exposure to uncertainty, is an inherent part of software development. Risk, therefore, is an important factor in software investment decisions. This paper illustrates a disciplined approach to treating risk\u2014an approach that allows software development decisions to be aligned with the capital markets. We start from the premise that uncertainty creates value when managed properly. Then we show using a real options approach that if the markets contain traded assets whose values depend partly on the source of uncertainty to which the investment is subject, we can estimate the underlying risk, and then use this estimate to determine the value of the investment. The approach is applied to a familiar software development scenario involving technology risk, namely that of Java.", "num_citations": "2\n", "authors": ["1145"]}
{"title": "A framework to support structural reuse in simulation environments\n", "abstract": " In the design of computer-based systems, simulation tools employed during various stages of the design cycle can provide significant insight into the behavior of the proposed design. Unfortunately the knowledge gained through the course of a simulation exercise is typically lost and inaccessible to other designs. One promising solution to this is the utilization of development environments that can support libraries of models at high levels of abstraction\u2014more suitable for reuse. One such environment is the Design Analysis and Synthesis Environment (DASE) based on the Design Specification Language, or DSL. DASE allows designers to model, experiment with, and reuse model components in other designs. Whereas component reuse has been studied extensively and widely supported in environments such as DASE, reuse of a different form\u2014structural reuse\u2014is still relatively poorly understood. The Extended Style Notation (ESN) establishes a basis for describing the abstract structure of systems at a high level\u2014thus facilitating structural reuse. This paper presents a framework\u2014a marriage of ESN and DSL\u2014to create a rapid prototyping and simulation environment supporting both structural and component reuse.", "num_citations": "2\n", "authors": ["1145"]}
{"title": "On using distributed representations of source code for the detection of C security vulnerabilities\n", "abstract": " This paper presents an evaluation of the code representation model Code2vec when trained on the task of detecting security vulnerabilities in C source code. We leverage the open-source library astminer to extract path-contexts from the abstract syntax trees of a corpus of labeled C functions. Code2vec is trained on the resulting path-contexts with the task of classifying a function as vulnerable or non-vulnerable. Using the CodeXGLUE benchmark, we show that the accuracy of Code2vec for this task is comparable to simple transformer-based methods such as pre-trained RoBERTa, and outperforms more naive NLP-based methods. We achieved an accuracy of 61.43% while maintaining low computational requirements relative to larger models.", "num_citations": "1\n", "authors": ["1145"]}
{"title": "Lightweight source code monitoring with Triggr\n", "abstract": " Existing tools for monitoring the quality of codebases modified by multiple developers tend to be centralized and inflexible. These tools increase the visibility of quality by producing effective reports and visualizations when a change is made to the codebase and triggering alerts when undesirable situations occur. However, their configuration is invariably both (a) centrally managed in that individual maintainers cannot define local rules to receive customized feedback when a change occurs in a specific part of the code in which they are particularly interested, and (b) coarse-grained in that analyses cannot be turned on and off below the file level. Triggr, the tool proposed in this paper, addresses these limitations by allowing distributed, customized, and fine-grained monitoring. It is a lightweight re-implementation of our previous tool, CodeAware, which adopts the same paradigm. The tool listens on a codebase\u2019s\u00a0\u2026", "num_citations": "1\n", "authors": ["1145"]}
{"title": "SECM 2017 Workshop Summary\n", "abstract": " Summary form only given. The 1st International Workshop on Software Engineering Curricula for Millennials (SECM 2017) aims at bringing students, educators, and prospective employees together, to discuss the demands and challenges of software engineering education for Millennials, in both higher education and professional settings. Millennials are defined as the demographic cohort following Generation X, born between early 1980s and early 2000s. Millennials have been dominating the higher education programs for some time. This cohort has unique needs, learning styles, and skills. They are diverse, collaborative, creative, tech-savvy, and keenly interested in emerging technologies. They drive the growth of the software industry, which itself is in a constant state of flux, with new technologies, techniques, paradigms, and application domains popping up with increasing frequency. Companies quickly adjust\u00a0\u2026", "num_citations": "1\n", "authors": ["1145"]}
{"title": "Must software research stand divided?\n", "abstract": " A not-so-subtle divide separates empirical and constructionist software research. Constructionists maintain that software research should be about creating technologies, devising new methods. Empiricists are interested in studying and understanding existing approaches. The antagonism between the two camps does not serve our industry well\u2014it needs both modes of research.", "num_citations": "1\n", "authors": ["1145"]}
{"title": "Scaling Agile methods\n", "abstract": " 6. How do you handle dispersed development within an agile project?(6 votes) 7. Who does integration testing as systems get bigger from a customer viewpoint?(5 votes) 8. What is agile (when polluted)?(5 votes) 9. What project management practices do we use for large agile projects?(4 votes) 10. How do we respond to RFP\u2019s when embracing agile methods?(4 votes)", "num_citations": "1\n", "authors": ["1145"]}
{"title": "An Options View of Extreme Programming\n", "abstract": " The last decade has been a productive one in the evolution of software development methodologies. It has been claimed that the new lightweight or agile methodologies are destined to have the same impact as structured analysis and design did a generation ago [Boehm2002]. Even more than the methodologies of a generation ago, the new methodologies are promoted not only for their technical characteristics, but also for their ability to contribute to economic and strategic goals. Over the past year we have been examining the principles, values, and strategies of Extreme Programming (XP), the most visible of the agile methodologies. Not surprisingly, the perspective of real options has turned out to be useful, given the emphasis of XP on flexibility and \u201cembracing change.\u201d", "num_citations": "1\n", "authors": ["1145"]}
{"title": "Costing of COTS-Based Systems: An Initial Framework\n", "abstract": " Costing is an important part of COTS-based system development. Many development process activities and costing activities are intricately related, and thus best be addressed in tandem. Like the development process itself, costing should be treated as a continuous activity in COTS-based development. The necessary infrastructures must be in place for its support and monitoring. In this paper, we address the main features of a rudimentary costing model for COTS-based systems with these features, and identify some additional areas for improvement.", "num_citations": "1\n", "authors": ["1145"]}
{"title": "CASCON'98 Workshop Report: Work injuries of computer users\n", "abstract": " Work-related injuries are increasingly common among computer users. At particular risk are employees who chronically spend long hours at a computer workstation. In such persons, sometimes the vulnerable soft tissues of the body cannot withstand the stresses placed on them by the repetitive tasks and static, awkward postures associated with computer work. The result is a complex ensemble of musculoskeletal disorders with varying degrees of severity and multitude. Both external factors, such as work habits and poorly designed workstations, and internal factors, such as systemic and pre-existing muskuloskeletal conditions, have been cited as potential contributors to these painful, and sometimes debilitating, disorders.The situation has a potentially significant social and financial impact on employees, employers, and governments alike. However, the scope of the problem is unknown, and its nature is still not well understood, despite the many existing theories. This workshop, geared toward computer workers and their managers, addressed a variety of topics: risk factors, prevention, treatment options, ergonomics, workplace accommodation, coping strategies, employers' responsibilities, behavioral contributors, and some of the latest research findings.", "num_citations": "1\n", "authors": ["1145"]}
{"title": "Component based simulation of ATM switch fabrics\n", "abstract": " As designs have become more complex, the need for tools to support reuse in the early stages of the design process has become increasingly important. Such tools have the potential to facilitate the simulation and evaluation of designs far before actual implementation. To support these applications, simulation environments must be able to support the reuse of models in many ways. This paper complements work regarding structural reuse and presents some insight in designing models that are amiable for component based simulation.", "num_citations": "1\n", "authors": ["1145"]}
{"title": "Structural reuse in the design of ATM switch fabrics\n", "abstract": " As designs have become more complex, the need for tools to support reuse in the early stages of the design process has become increasingly important. Such tools have the potential to facilitate the simulation and evaluation of designs far before actual implementation. To support these applications, simulation environments must be able to support the reuse of models in many ways. Although there is a large body of research on component reuse, not enough work has been done to address structural reuse. With potentially valuable applications, this second form of reuse is both complementary and orthogonal to component reuse. One such application, the design of Asynchronous Transfer Mode switch fabrics, is presented in this paper from the perspective of structural reuse.", "num_citations": "1\n", "authors": ["1145"]}