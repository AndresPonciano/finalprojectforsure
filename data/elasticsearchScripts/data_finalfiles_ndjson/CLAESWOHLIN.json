{"title": "Experimentation in software engineering\n", "abstract": " Like other sciences and engineering disciplines, software engineering requires a cycle of model building, experimentation, and learning. Experiments are valuable tools for all software engineers who are involved in evaluating and choosing between different methods, techniques, languages and tools. The purpose of Experimentation in Software Engineering is to introduce students, teachers, researchers, and practitioners to empirical studies in software engineering, using controlled experiments. The introduction to experimentation is provided through a process perspective, and the focus is on the steps that we have to go through to perform an experiment. The book is divided into three parts. The first part provides a background of theories and methods used in experimentation. Part II then devotes one chapter to each of the five experiment steps: scoping, planning, execution, analysis, and result presentation. Part III completes the presentation with two examples. Assignments and statistical material are provided in appendixes. Overall the book provides indispensable information regarding empirical studies in particular for experiments, but also for case studies, systematic literature reviews, and surveys. It is a revision of the authors\u0393\u00c7\u00d6 book, which was published in 2000. In addition, substantial new material, eg concerning systematic literature reviews and case study research, is introduced. The book is self-contained and it is suitable as a course book in undergraduate or graduate studies where the need for empirical studies in software engineering is stressed. Exercises and assignments are included to combine the more theoretical material with\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8190\n", "authors": ["48"]}
{"title": "Guidelines for Snowballing in Systematic Literature Studies and a Replication in Software Engineering\n", "abstract": " Background: Systematic literature studies have become common in software engineering, and hence it is important to understand how to conduct them efficiently and reliably.Objective: This paper presents guidelines for conducting literature reviews using a snowballing approach, and they are illustrated and evaluated by replicating a published systematic literature review.Method: The guidelines are based on the experience from conducting several systematic literature reviews and experimenting with different approaches.Results: The guidelines for using snowballing as a way to search for relevant literature was successfully applied to a systematic literature review.Conclusions: It is concluded that using snowballing, as a first search strategy, may very well be a good alternative to the use of database searches.", "num_citations": "2096\n", "authors": ["48"]}
{"title": "Using students as subjects\u0393\u00c7\u00f6a comparative study of students and professionals in lead-time impact assessment\n", "abstract": " In many studiesin software engineering students are used instead of professionalsoftware developers, although the objective is to draw conclusionsvalid for professional software developers. This paper presentsa study where the difference between the two groups is evaluated.People from the two groups have individually carried out a non-trivialsoftware engineering judgement task involving the assessmentof how ten different factors affect the lead-time of softwaredevelopment projects. It is found that the differences are onlyminor, and it is concluded that software engineering studentsmay be used instead of professional software developers undercertain conditions. These conditions are identified and describedbased on generally accepted criteria for validity evaluationof empirical studies.", "num_citations": "763\n", "authors": ["48"]}
{"title": "Systematic literature studies: database searches vs. backward snowballing\n", "abstract": " Systematic studies of the literature can be done in different ways. In particular, different guidelines propose different first steps in their recommendations, e.g. start with search strings in different databases or start with the reference lists of a starting set of papers. In software engineering, the main recommended first step is using search strings in a number of databases, while in information systems, snowballing has been recommended as the first step. This paper compares the two different search approaches for conducting literature review studies. The comparison is conducted by searching for articles addressing \u0393\u00c7\u00a3Agile practices in global software engineering\u0393\u00c7\u00a5. The focus of the paper is on evaluating the two different search approaches. Despite the differences in the included papers, the conclusions and the patterns found in both studies are quite similar. The strengths and weaknesses of each first step are discussed\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "600\n", "authors": ["48"]}
{"title": "Engineering and managing software requirements\n", "abstract": " Requirements engineering is the process by which the requirements for software systems are gathered, analyzed, documented, and managed throughout their complete lifecycle. Traditionally it has been concerned with technical goals for, functions of, and constraints on software systems. Aurum and Wohlin, however, argue that it is no longer appropriate for software systems professionals to focus only on functional and non-functional aspects of the intended system and to somehow assume that organizational context and needs are outside their remit. Instead, they call for a broader perspective in order to gain a better understanding of the interdependencies between enterprise stakeholders, processes, and software systems, which would in turn give rise to more appropriate techniques and higher-quality systems.Following an introductory chapter that provides an exploration of key issues in requirements\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "491\n", "authors": ["48"]}
{"title": "Empirical evidence in global software engineering: a systematic review\n", "abstract": " Recognized as one of the trends of the 21st century, globalization of the world economies brought significant changes to nearly all industries, and in particular it includes software development. Many companies started global software engineering (GSE) to benefit from cheaper, faster and better development of software systems, products and services. However, empirical studies indicate that achieving these benefits is not an easy task. Here, we report our findings from investigating empirical evidence in GSE-related research literature. By conducting a systematic review we observe that the GSE field is still immature. The amount of empirical studies is relatively small. The majority of the studies represent problem-oriented reports focusing on different aspects of GSE management rather than in-depth analysis of solutions for example in terms of useful practices or techniques. Companies are still driven by cost\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "422\n", "authors": ["48"]}
{"title": "Empirical research methods in software engineering\n", "abstract": " Software engineering is not only about technical solutions. It is to a large extent also concerned with organizational issues, project management and human behaviour. For a discipline like software engineering, empirical methods are crucial, since they allow for incorporating human behaviour into the research approach taken. Empirical methods are common practice in many other disciplines. This chapter provides a motivation for the use of empirical methods in software engineering research. The main motivation is that it is needed from an engineering perspective to allow for informed and well-grounded decision. The chapter continues with a brief introduction to four research methods: controlled experiments, case studies, surveys and post-mortem analyses. These methods are then put into an improvement context. The four methods are presented with the objective to introduce the reader to the methods\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "362\n", "authors": ["48"]}
{"title": "State\u0393\u00c7\u00c9of\u0393\u00c7\u00c9the\u0393\u00c7\u00c9art: software inspections after 25 years\n", "abstract": " Software inspections, which were originally developed by Michael Fagan in 1976, are an important means to verify and achieve sufficient quality in many software projects today. Since Fagan's initial work, the importance of software inspections has been long recognized by software developers and many organizations. Various proposals have been made by researchers in the hope of improving Fagan's inspection method. The proposals include structural changes to the process and several types of support for the inspection process. Most of the proposals have been empirically investigated in different studies. This is a review paper focusing on the software inspection process in the light of Fagan's inspection method and it summarizes and reviews other types of software inspection processes that have emerged in the last 25 years. This paper also addresses important issues related to the inspection process and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "304\n", "authors": ["48"]}
{"title": "Requirements abstraction model\n", "abstract": " Software requirements arrive in different shapes and forms to development organizations. This is particularly the case in market-driven requirements engineering, where the requirements are on products rather than directed towards projects. This results in challenges related to making different requirements comparable. In particular, this situation was identified in a collaborative effort between academia and industry. A model, with four abstraction levels, was developed as a response to the industrial need. The model allows for placement of requirements on different levels and supports abstraction or break down of requirements to make them comparable to each other. The model was successfully validated in several steps at a company. The results from the industrial validation point to the usefulness of the model. The model will allow companies to ensure comparability between requirements, and hence it\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "289\n", "authors": ["48"]}
{"title": "Agile Practices in Global Software Engineering-A Systematic Map.\n", "abstract": " This paper presents the results of systematically reviewing the current research literature on the use of agile practices and lean software development in global software engineering (GSE). The primary purpose is to highlight under which circumstances they have been applied efficiently. Some common terms related to agile practices (e.g. scrum, extreme programming) were considered in formulating the search strings, along with a number of alternatives for GSE such as offshoring, outsourcing, and virtual teams. The results were limited to peer-reviewed conference papers/journal articles, published between 1999 and 2009. The synthesis was made through classifying the papers into different categories (e.g. research type, distribution). The analysis revealed that in most cases agile practices were modified with respect to the context and situational requirements. This indicates the need for future research on how to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "214\n", "authors": ["48"]}
{"title": "An experimental comparison of usage-based and checklist-based reading\n", "abstract": " Software quality can be defined as the customers' perception of how a system works. Inspection is a method to monitor and control the quality throughout the development cycle. Reading techniques applied to inspections help reviewers to stay focused on the important parts of an artifact when inspecting. However, many reading techniques focus on finding as many faults as possible, regardless of their importance. Usage-based reading helps reviewers to focus on the most important parts of a software artifact from a user's point of view. We present an experiment, which compares usage-based and checklist-based reading. The results show that reviewers applying usage-based reading are more efficient and effective in detecting the most critical faults from a user's point of view than reviewers using checklist-based reading. Usage-based reading may be preferable for software organizations that utilize or start utilizing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "199\n", "authors": ["48"]}
{"title": "Towards a Decision-making Structure for Selecting a Research Design in Empirical Software Engineering\n", "abstract": " Several factors make empirical research in software engineering particularly challenging as it requires studying not only technology but its stakeholders\u0393\u00c7\u00d6 activities while drawing concepts and theories from social science. Researchers, in general, agree that selecting a research design in empirical software engineering research is challenging, because the implications of using individual research methods are not well recorded. The main objective of this article is to make researchers aware and support them in their research design, by providing a foundation of knowledge about empirical software engineering research decisions, in order to ensure that researchers make well-founded and informed decisions about their research designs. This article provides a decision-making structure containing a number of decision points, each one of them representing a specific aspect on empirical software engineering\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "192\n", "authors": ["48"]}
{"title": "Global software engineering and agile practices: a systematic review\n", "abstract": " Agile practices have received attention from industry as an alternative to plan\u0393\u00c7\u00c9driven software development approaches. Agile encourages, for example, small self\u0393\u00c7\u00c9organized collocated teams, whereas global software engineering (GSE) implies distribution across cultural, temporal, and geographical boundaries. Hence, combining them is a challenge. A systematic review was conducted to capture the status of combining agility with GSE. The results were limited to peer\u0393\u00c7\u00c9reviewed conference papers or journal articles, published between 1999 and 2009. The synthesis was made through classifying the papers into different categories (e.g. publication year, contribution type, research method). At the end, 81 papers were judged as primary for further analysis. The distribution of papers over the years indicated that GSE and Agile in combination has received more attention in the last 5\u0393\u00c7\u00ebyears. However, the majority of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "190\n", "authors": ["48"]}
{"title": "An evaluation of k-nearest neighbour imputation using likert data\n", "abstract": " Studies in many different fields of research suffer from the problem of missing data. With missing data, statistical tests will lose power, results may be biased, or analysis may not be feasible at all. There are several ways to handle the problem, for example through imputation. With imputation, missing values are replaced with estimated values according to an imputation method or model. In the k-nearest neighbour (k-NN) method, a case is imputed using values from the k most similar cases. In this paper, we present an evaluation of the k-NN method using Likert data in a software engineering context. We simulate the method with different values of k and for different percentages of missing data. Our findings indicate that it is feasible to use the k-NN method with Likert data. We suggest that a suitable value of k is approximately the square root of the number of complete cases. We also show that by relaxing the method\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "188\n", "authors": ["48"]}
{"title": "On the Reliability of Mapping Studies in Software Engineering\n", "abstract": " BackgroundSystematic literature reviews and systematic mapping studies are becoming increasingly common in software engineering, and hence it becomes even more important to better understand the reliability of such studies.ObjectiveThis paper presents a study of two systematic mapping studies to evaluate the reliability of mapping studies and point out some challenges related to this type of study in software engineering.MethodThe research is based on an in-depth case study of two published mapping studies on software product line testing.ResultsWe found that despite the fact that the two studies are addressing the same topic, there are quite a number of differences when it comes to papers included and in terms of classification of the papers included in the two mapping studies.ConclusionsFrom this we conclude that although mapping studies are important, their reliability cannot simply be taken for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "169\n", "authors": ["48"]}
{"title": "A product management challenge: Creating software product value through requirements selection\n", "abstract": " It is important for a software company to maximize value creation for a given investment. The purpose of requirements engineering activities is to add business value that is accounted for in terms of return on investment of a software product. This paper provides insight into the release planning processes used in the software industry to create software product value, by presenting three case studies. It examines how IT professionals perceive value creation through requirements engineering and how the release planning process is conducted to create software product value. It also presents to what degree the major stakeholders\u0393\u00c7\u00d6 perspectives are represented in the decision-making process. Our findings show that the client and market base of the software product represents the most influential group in the decision to implement specific requirements. This is reflected both in terms of deciding the processes followed\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "167\n", "authors": ["48"]}
{"title": "Measuring the flow in lean software development\n", "abstract": " Responsiveness to customer needs is an important goal in agile and lean software development. One major aspect is to have a continuous and smooth flow that quickly delivers value to the customer. In this paper we apply cumulative flow diagrams to visualize the flow of lean software development. The main contribution is the definition of novel measures connected to the diagrams to achieve the following goals: (1) increase throughput and reduce lead\u0393\u00c7\u00c9time to achieve high responsiveness to customers' needs and (2) to provide a tracking system that shows the progress/status of software product development. An evaluation of the measures in an industrial case study showed that practitioners found them useful and identify improvements based on the measurements, which were in line with lean and agile principles. Furthermore, the practitioners found the measures useful in seeing the progress of development for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "159\n", "authors": ["48"]}
{"title": "An empirically based terminology and taxonomy for global software engineering\n", "abstract": " Many organizations nowadays strive for utilization of benefits offered by global software engineering (GSE) and sourcing strategies are thus discussed more often. Since there are so many variations of the attributes associated with global software projects a large amount of new terms has been introduced. The diversity in sourcing jargon however has caused difficulties in determining which term to use in which situation, and thus causing further obstacles to searching and finding relevant research during e.g. systematic literature reviews. The inability of judging the applicability of the research in an industrial context is another important implication on the transferability of research into practice. Thus the need for accurate terminology and definitions for different global sourcing situations emerges as a way for the community to build upon each other\u0393\u00c7\u00d6s work and hence making progress more quickly. In this paper\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "157\n", "authors": ["48"]}
{"title": "Certification of software components\n", "abstract": " Reuse is becoming one of the key areas in dealing with the cost and quality of software systems. An important issue is the reliability of the components, hence making certification of software components a critical area. The objective of this article is to try to describe methods that can be used to certify and measure the ability of software components to fulfil the reliability requirements placed on them. A usage modelling technique is presented, which can be used to formulate usage models for components. This technique will make it possible not only to certify the components, but also to certify the system containing the components. The usage model describes the usage from a structural point of view, which is complemented with a profile describing the expected usage in figures. The failure statistics from the usage test form the input of a hypothesis certification model, which makes it possible to certify a specific\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "133\n", "authors": ["48"]}
{"title": "Software process improvement through the Lean Measurement (SPI-LEAM) method\n", "abstract": " Software process improvement methods help to continuously refine and adjust the software process to improve its performance (e.g., in terms of lead-time, quality of the software product, reduction of change requests, and so forth). Lean software development propagates two important principles that help process improvement, namely identification of waste in the process and considering interactions between the individual parts of the software process from an end-to-end perspective. A large shift of thinking about the own way of working is often required to adopt lean. One of the potential main sources of failure is to try to make a too large shift about the ways of working at once. Therefore, the change to lean has to be done in a continuous and incremental way. In response to this we propose a novel approach to bring together the quality improvement paradigm and lean software development practices, the approach\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "132\n", "authors": ["48"]}
{"title": "A quality-driven decision-support method for identifying software architecture candidates\n", "abstract": " To sustain the qualities of a software system during evolution, and to adapt  the quality attributes as the requirements evolve, it is necessary to have a  clear software architecture that is understood by all developers and to  which all changes to the system adheres. This software architecture can be  created beforehand, but must also be updated as the domain of the software,  and hence the requirements on the software system evolve. Creating a  software architecture for a system or part of a system so that the  architecture fulfils the desired quality requirements is often hard. In this  paper we propose a decision-support method to aid in the understanding of  different architecture candidates for a software system. We propose a method  that is adaptable with respect to both the set of potential architecture  candidates and quality attributes relevant for the system's domain to help  in this task. The method creates a support\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "126\n", "authors": ["48"]}
{"title": "Empirical software engineering experts on the use of students and professionals in experiments\n", "abstract": " [Context] Controlled experiments are an important empirical method to generate and validate theories. Many software engineering experiments are conducted with students. It is often claimed that the use of students as participants in experiments comes at the cost of low external validity while using professionals does not. [Objective] We believe a deeper understanding is needed on the external validity of software engineering experiments conducted with students or with professionals. We aim to gain insight about the pros and cons of using students and professionals in experiments. [Method] We performed an unconventional, focus group approach and a follow-up survey. First, during a session at ISERN 2014, 65 empirical researchers, including the seven authors, argued and discussed the use of students in experiments with an open mind. Afterwards, we revisited the topic and elicited experts\u0393\u00c7\u00d6 opinions to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "125\n", "authors": ["48"]}
{"title": "Experiences from using snowballing and database searches in systematic literature studies\n", "abstract": " Background: Systematic literature studies are commonly used in software engineering. There are two main ways of conducting the searches for these type of studies; they are snowballing and database searches. In snowballing, the reference list (backward snowballing-BSB) and citations (forward snowballing-FSB) of relevant papers are reviewed to identify new papers whereas in a database search, different databases are searched using predefined search strings to identify new papers. Objective: Snowballing has not been in use as extensively as database search. Hence it is important to evaluate its efficiency and reliability when being used as a search strategy in literature studies. Moreover, it is important to compare it to database searches. Method: In this paper, we applied snowballing in a literature study, and reflected on the outcome. We also compared database search with backward and forward\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "125\n", "authors": ["48"]}
{"title": "Pair-wise comparisons versus planning game partitioning\u0393\u00c7\u00f6experiments on requirements prioritisation techniques\n", "abstract": " The process of selecting the right set of requirements for a product release is dependent on how well the organisation succeeds in prioritising the requirements candidates. This paper describes two consecutive controlled experiments comparing different requirements prioritisation techniques with the objective of understanding differences in time-consumption, ease of use and accuracy. The first experiment evaluates Pair-wise comparisons and a variation of the Planning game. As the Planning game turned out as superior, the second experiment was designed to compare the Planning game to Tool-supported pair-wise comparisons. The results indicate that the manual pair-wise comparisons is the most time-consuming of the techniques, and also the least easy to use. Tool-supported pair-wise comparisons is the fastest technique and it is as easy to use as the Planning game. The techniques do not differ\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "121\n", "authors": ["48"]}
{"title": "Empirical evidence on the link between object-oriented measures and external quality attributes: a systematic literature review\n", "abstract": " There is a plethora of studies investigating object-oriented measures and their link with external quality attributes, but usefulness of the measures may differ across empirical studies. This study aims to aggregate and identify useful object-oriented measures, specifically those obtainable from the source code of object-oriented systems that have gone through such empirical evaluation. By conducting a systematic literature review, 99 primary studies were identified and traced to four external quality attributes: reliability, maintainability, effectiveness and functionality. A vote-counting approach was used to investigate the link between object-oriented measures and the attributes, and to also assess the consistency of the relation reported across empirical studies. Most of the studies investigate links between object-oriented measures and proxies for reliability attributes, followed by proxies for maintainability. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "102\n", "authors": ["48"]}
{"title": "Capture\u0393\u00c7\u00f4recapture in software inspections after 10 years research\u0393\u00c7\u00f4\u0393\u00c7\u00f4theory, evaluation and application\n", "abstract": " Software inspection is a method to detect faults in the early phases of the software life cycle. In order to estimate the number of faults not found, capture\u0393\u00c7\u00f4recapture was introduced for software inspections in 1992 to estimate remaining faults after an inspection. Since then, several papers have been written in the area, concerning the basic theory, evaluation of models and application of the method. This paper summarizes the work made in capture\u0393\u00c7\u00f4recapture for software inspections during these years. Furthermore, and more importantly, the contribution of the papers are classified as theory, evaluation or application, in order to analyse the performed research as well as to highlight the areas of research that need further work. It is concluded that (1) most of the basic theory is investigated within biostatistics, (2) most software engineering research is performed on evaluation, a majority ending up in recommendation of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "102\n", "authors": ["48"]}
{"title": "Experimental context classification: incentives and experience of subjects\n", "abstract": " There is a need to identify factors that affect the result of empirical studies in software engineering research. It is still the case that seemingly identical replications of controlled experiments result in different conclusions due to the fact that all factors describing the experiment context are not clearly defined and hence controlled. In this article, a scheme for describing the participants of controlled experiments is proposed and evaluated. It consists of two main factors, the incentives for participants in the experiment and the experience of the participants. The scheme has been evaluated by classifying a set of previously conducted experiments from literature. It can be concluded that the scheme was easy to use and understand. It is also found that experiments that are classified in the same way to a large extent point at the same results, which indicates that the scheme addresses relevant factors.", "num_citations": "95\n", "authors": ["48"]}
{"title": "A value-based approach in requirements engineering: explaining some of the fundamental concepts\n", "abstract": " Today\u0393\u00c7\u00d6s rapid changes and global competition forces software companies to become increasingly competitive and responsive to consumers and market developments. The purpose of requirements engineering activities is to add business value that is accounted for in terms of return-on-investment of a software product. This article introduces some of the fundamental aspects of value by borrowing theories from economic theory, discusses a number of the challenges that face requirements engineers and finally provides a model that illustrates value from business, product and project perspectives.", "num_citations": "90\n", "authors": ["48"]}
{"title": "Reporting empirical research in global software engineering: A classification scheme\n", "abstract": " Increased popularity of global software engineering (GSE) has resulted in quite a number of research and industrial studies. As the area matures, an increased focus on empirically supported results leads to a greater potential impact on future research and industrial practice. However, since GSE scenarios are diverse, what works in one context might not directly apply in another. Thus it is necessary to understand, how GSE-related empirical findings should be reported to be useful for practitioners and researchers. Furthermore, itdasias important to summarize progress and get the big picture of published research to identify gaps and commonalities. In this paper we analyze differentiating factors of GSE scenarios and offer a classification scheme for describing the context of a GSE study. In addition, we report initial results of a systematic review on GSE-related empirical literature using papers from ICGSE 2006 and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "89\n", "authors": ["48"]}
{"title": "An empirical study on views of importance of change impact analysis issues\n", "abstract": " Change impact analysis is a change management activity that previously has been studied much from a technical perspective. For example, much work focuses on methods for determining the impact of a change. In this paper, we present results from a study on the role of impact analysis in the change management process. In the study, impact analysis issues were prioritised with respect to criticality by software professionals from an organisational perspective and a self-perspective. The software professionals belonged to three organisational levels: operative, tactical and strategic. Qualitative and statistical analyses with respect to differences between perspectives as well as levels are presented. The results show that important issues for a particular level are tightly related to how the level is defined. Similarly, issues important from an organisational perspective are more holistic than those important from a self\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "89\n", "authors": ["48"]}
{"title": "Systematic Literature Reviews in Software Engineering\n", "abstract": " Editorial: Systematic literature reviews in software engineering: Information and Software Technology: Vol 55, No 6 ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Information and Software Technology Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsInformation and Software TechnologyVol. , No. Editorial: Systematic literature reviews in software engineering article Editorial: Systematic literature reviews in software engineering Share on Authors: Claes Wohlin profile image Claes Wohlin Information and Software Technology, Blekinge Institute of Technology, Sweden Information and Software Technology, Blekinge Institute of Technology, , , ()(\u0393\u00c7\u00aa", "num_citations": "87\n", "authors": ["48"]}
{"title": "What is important when deciding to include a software requirement in a project or release?\n", "abstract": " The requirements on software systems are so many that not all requirements may be included in the next development project or the next release. This means that it is necessary to select a set of requirements to implement in the forthcoming project, and hence to postpone the implementation of other requirements to a later point in time. In this selection process different criteria are used. In many cases, the criteria are not officially stated, but rather implicitly used by the decision-makers. However, to be able to support this decision-making process, it is important to know and understand the underlying reasons for the decisions. This paper presents an empirical study of the decision criteria. In particular, the paper focuses on how different perspectives have different influence on the decision-making process. It is concluded that business-oriented and management-oriented criteria are more important than technical\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "87\n", "authors": ["48"]}
{"title": "An empirical study on using stereotypes to improve understanding of UML models\n", "abstract": " Stereotypes were introduced into the Unified Modeling Language (UML) to provide means of customizing this visual, general purpose, object-oriented modeling language, for its usage in specific application domains. The primary purpose of stereotypes is to brand an existing model element with a specific semantics. In addition, stereotypes can also be used as notational shorthand. The paper elaborates on this role of stereotypes from the perspective of UML, clarifies the role and describes a controlled experiment aimed at evaluation of the role - in the context of model understanding. The results of the experiment support the claim that stereotypes with graphical icons for their representation play a significant role in comprehension of models and show the size of the improvement.", "num_citations": "83\n", "authors": ["48"]}
{"title": "Assessing project success using subjective evaluation factors\n", "abstract": " Project evaluation is essential to understand and assess the key aspects of a project that make it either a success or failure. The latter is influenced by a large number of factors, and many times it is hard to measure them objectively. This paper addresses this by introducing a new method for identifying and assessing key project characteristics, which are crucial for a project's success. The method consists of a number of well-defined steps, which are described in detail. The method is applied to two case studies from different application domains and continents. It is concluded that patterns are possible to detect from the data sets. Further, the analysis of the two data sets shows that the proposed method using subjective factors is useful, since it provides an increased understanding, insight and assessment of which project factors might affect project success.", "num_citations": "78\n", "authors": ["48"]}
{"title": "An experimental study of individual subjective effort estimations and combinations of the estimates\n", "abstract": " The required effort of a task can be estimated subjectively in interviews with experts in an organization in different ways. Interview techniques dealing with which type of questions to ask are evaluated and techniques for combining estimates from individuals into one estimate are compared in an experiment. The result shows that the interview technique is not as important as the combination technique. The estimate which is best with respect to mean value and standard deviation of the effort is based on an equal weighting of all individual estimates. The experiment is performed within the Personal Software Process (PSP).", "num_citations": "78\n", "authors": ["48"]}
{"title": "Software teams and their knowledge networks in large-scale software development\n", "abstract": " ContextLarge software development projects involve multiple interconnected teams, often spread around the world, developing complex products for a growing number of customers and users. Succeeding with large-scale software development requires access to an enormous amount of knowledge and skills. Since neither individuals nor teams can possibly possess all the needed expertise, the resource availability in a team's knowledge network, also known as social capital, and effective knowledge coordination become paramount.ObjectiveIn this paper, we explore the role of social capital in terms of knowledge networks and networking behavior in large-scale software development projects.MethodWe conducted a multi-case study in two organizations, Ericsson and ABB, with software development teams as embedded units of analysis. We organized focus groups with ten software teams and surveyed 61\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "76\n", "authors": ["48"]}
{"title": "Criteria for selecting software requirements to create product value: An industrial empirical study\n", "abstract": " Product value is based on which requirements are included in a specific release of a software product. This chapter provides an overview of the value concept and presents an empirical study conducted as an industrial survey. The objective of the survey was to quantify the importance of different decision making criteria when deciding whether to include a requirement in a project or release. The results reported from the survey are based on responses from two companies. It was discovered that there were similarities in responses at a company level, although major differences existed between individual respondents to the survey. The most important criteria were found to be those related to specific customers or markets and criteria, such as development cost-benefit, delivery date, and resources. The least important criteria were those related to development and maintenance. The results also indicate that a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "76\n", "authors": ["48"]}
{"title": "A Systematic Literature Review on the Industrial Use of Software Process Simulation\n", "abstract": " ContextSoftware process simulation modelling (SPSM) captures the dynamic behaviour and uncertainty in the software process. Existing literature has conflicting claims about its practical usefulness: SPSM is useful and has an industrial impact; SPSM is useful and has no industrial impact yet; SPSM is not useful and has little potential for industry.ObjectiveTo assess the conflicting standpoints on the usefulness of SPSM.MethodA systematic literature review was performed to identify, assess and aggregate empirical evidence on the usefulness of SPSM.ResultsIn the primary studies, to date, the persistent trend is that of proof-of-concept applications of software process simulation for various purposes (e.g. estimation, training, process improvement, etc.). They score poorly on the stated quality criteria. Also only a few studies report some initial evaluation of the simulation models for the intended purposes\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "75\n", "authors": ["48"]}
{"title": "Empirical assessment of using stereotypes to improve comprehension of UML models: A set of experiments\n", "abstract": " Stereotypes were introduced into the Unified Modeling Language to provide means of customizing this general purpose modeling language for its usage in specific application domains. The primary role of stereotypes is to brand an existing model element with specific semantics, but stereotypes can also be used to provide means of a secondary classification of modeling elements. This paper elaborates on the influence of stereotypes on the comprehension of models. The paper describes a set of controlled experiments performed in academia and industry which were aimed at evaluating the role of stereotypes in improving comprehension of UML models. The results of the experiments show that stereotypes play a significant role in the comprehension of models and the improvement achieved both by students and industry professionals.", "num_citations": "74\n", "authors": ["48"]}
{"title": "Faults\u0393\u00c7\u00c9slip\u0393\u00c7\u00c9through - a concept for measuring the efficiency of the test process\n", "abstract": " In market\u0393\u00c7\u00c9driven development where time\u0393\u00c7\u00c9to\u0393\u00c7\u00c9market is of crucial importance, software development companies seek improvements that can decrease the lead\u0393\u00c7\u00c9time and improve the delivery precision. One way to achieve this is by analyzing the test process since rework commonly accounts for more than half of the development time. A large reason for high rework costs is fault slippages from earlier phases where they are cheaper to find and remove. As an input to improvements, this article introduces a measure that can quantify this relationship. That is, a measure called faults\u0393\u00c7\u00c9slip\u0393\u00c7\u00c9through, which determines the faults that would have been more cost\u0393\u00c7\u00c9effective to find in an earlier phase. The method presented in this article also determines the excessive costs of the faults that slipped through phases, i.e. the improvement potential. The method was validated through practical application in two software development\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "73\n", "authors": ["48"]}
{"title": "An experimental evaluation of capture\u0393\u00c7\u00c9recapture in software inspections\n", "abstract": " The use of capture\u0393\u00c7\u00c9recapture to estimate the residual faults in a software artifact has evolved as a promising method. However, the assumptions needed to make the estimates are not completely fulfilled in software development, leading to an underestimation of the residual fault content. Therefore, a method employing a filtering technique with an experience factor to improve the estimate of the residual faults is proposed in this paper. An experimental study of the capture\u0393\u00c7\u00c9recapture method with this correction method has been conducted. It is concluded that the correction method improves the capture\u0393\u00c7\u00c9recapture estimate of the number of residual defects in the inspected document.", "num_citations": "71\n", "authors": ["48"]}
{"title": "A General Theory of Software Engineering: Balancing Human, Social and Organizational Capitals\n", "abstract": " There exists no generally accepted theory in software engineering, and at the same time a scientific discipline needs theories. Some laws, hypotheses and conjectures exist, but yet no generally accepted theory. Several researchers and initiatives emphasize the need for theory in the discipline. The objective of this paper is to formulate a theory of software engineering. The theory is generated from empirical observations of industry practice, including several case studies and many years of experience in working closely between academia and industry. The theory captures the balancing of three different intellectual capitals: human, social and organizational capitals, respectively. The theory is formulated using a method for building theories in software engineering. It results in a theory where the relationships between the three different intellectual capitals are explored and explained. The theory is illustrated based\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "68\n", "authors": ["48"]}
{"title": "Resources contributing to gaining competitive advantage for open source software projects: An application of resource-based theory\n", "abstract": " Open Source Software (OSS) is an important asset in today's software-intensive society. The success of OSS projects is highly dependent on a number of factors. These factors must be understood and managed as an OSS project progresses. Thus, project management of an OSS project has a decisive role in ensuring the success of its software. The objective of the research is to increase the understanding of the resources affecting the competitiveness of OSS projects. Herewith, the responsiveness of OSS projects to users' needs is assessed via an investigation of the defect-fixing process. A Resource-Based View of the firm (RBV) is used to build theoretical justifications for a set of hypotheses proposed in this study. Data gathered from 427 OSS projects confirmed that developers' interest in and users' contribution to the project as well as frequently updating and releasing the software affect the project's ability to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "67\n", "authors": ["48"]}
{"title": "A method for understanding quality attributes in software architecture structures\n", "abstract": " To sustain the qualities of a software system during evolution, and to adapt the quality attributes as the requirements evolve, it is necessary to have a clear software architecture that is understood by all developers and to which all changes to the system adheres. This software architecture can be created beforehand, but must also be updated as the domain of the software, and hence the requirements on the software system evolves. Creating an architectural structure for a system or part of a system so that the architecture fulfils the desired quality requirements is often hard. In this paper we propose a decision support method to aid in the understanding of different architecture structure candidates for a software system. We propose a method that is adaptable with respect to both the set of potential architecture structures, and quality attributes relevant for the system's domain to help in this task. The method creates a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "63\n", "authors": ["48"]}
{"title": "A comparison between software design and code metrics for the prediction of software fault content\n", "abstract": " Software metrics play an important role in measuring the quality of software. It is desirable to predict the quality of software as early as possible, and hence metrics have to be collected early as well. This raises a number of questions that has not been fully answered. In this paper we discuss, prediction of fault content and try to answer what type of metrics should be collected, to what extent design metrics can be used for prediction, and to what degree prediction accuracy can be improved if code metrics are included. Based on a data set collected from a real project, we found that both design and code metrics are correlated with the number of faults. When the metrics are used to build prediction models of the number of faults, the design metrics are as good as the code metrics, little improvement can be achieved if both design metrics and code metrics are used to model the relationship between the number of faults\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "63\n", "authors": ["48"]}
{"title": "Software Component Decision-making: In-house, OSS, COTS or Outsourcing - A Systematic Literature Review\n", "abstract": " Context: Component-based software systems require decisions on component origins for acquiring components. A component origin is an alternative of where to get a component from.Objective: To identify factors that could influence the decision to choose among different component origins and solutions for decision-making (For example, optimization) in the literature.Method: A systematic review study of peer-reviewed literature has been conducted.Results: In total we included 24 primary studies. The component origins compared were mainly focused on in-house vs. COTS and COTS vs. OSS. We identified 11 factors affecting or influencing the decision to select a component origin. When component origins were compared, there was little evidence on the relative (either positive or negative) effect of a component origin on the factor. Most of the solutions were proposed for in-house vs. COTS selection and time\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "60\n", "authors": ["48"]}
{"title": "Industry evaluation of the requirements abstraction model\n", "abstract": " Software requirements are often formulated on different levels and hence they are difficult to compare to each other. To address this issue, a model that allows for placing requirements on different levels has been developed. The model supports both abstraction and refinement of requirements, and hence requirements can both be compared with each other and to product strategies. Comparison between requirements will allow for prioritization of requirements, which in many cases is impossible if the requirements are described on different abstraction levels. Comparison to product strategies will enable early and systematic acceptance or dismissal of requirements, minimizing the risk for overloading. This paper presents an industrial evaluation of the model. It has been evaluated in two different companies, and the experiences and findings are presented. It is concluded that the requirements abstraction\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "60\n", "authors": ["48"]}
{"title": "Empirical Software Engineering Research with Industry: Top 10 Challenges\n", "abstract": " Software engineering research can be done in many ways, in particular it can be done in different ways when it comes to working with industry. This paper presents a list of top 10 challenges to work with industry based on our experience from working with industry in a very close collaboration with continuous exchange of knowledge and information. The top 10 list is based on a large number of research projects and empirical studies conducted with industrial research partners since 1983. It is concluded that close collaboration is a long-term undertaking and a large investment. The importance of addressing the top 10 challenges is stressed, since they form the basis for a long-term sustainable and successful collaboration between industry and academia.", "num_citations": "59\n", "authors": ["48"]}
{"title": "Assuring fault classification agreement - an empirical evaluation\n", "abstract": " Inter-rater agreement is a well-known challenge and is a key issue when discussing fault classification. Fault classification is, by nature, a subjective task since it highly depends on the people performing the classification. Measures are required to hinder the subjective nature of fault classification to propagate through the fault classification process and onto subsequent activities using the classified faults, for example process improvement. One approach to prevent the subjective nature of fault classification is to use multiple raters and measure inter-rater agreement. We evaluate the possibility to have an independent group of people classifying faults. The objective is to evaluate whether such a group could be used in a process improvement initiative. An empirical study is conducted with eight persons classifying 30 faults independently. The study concludes that the provided material were unsatisfactory to obtain\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "59\n", "authors": ["48"]}
{"title": "Quality improvement by identification of fault-prone modules using software design metrics\n", "abstract": " Quality improvement in terms of lower costs, shorter development times and increased reliability is desired \u2229\u00bc\u00fc'0m both the developer\u0393\u00c7\u00d6s and their customer's point of view. To enable early identi\u2229\u00bc\u00fccation of problems, and subsequently to support planning and scheduling, methods for identi\u2229\u00bc\u00fccation of fault-prone modules are desirable. This paper presents a case study of how design metrics were used at Ericsson Telecom AB to identifyfault-prone modules at the design phase. Derivation ofa model for identi\u2229\u00bc\u00fccation of fault-prone modules is discussed, with emphasis on the use of appropriate statistical methods. The model derivation and statistical methods are exemplified using data from two releases of a large software system. The model is derived from the \u2229\u00bc\u00fcrst release and its ability to pinpoint the most fault-prone modules is evaluated for the second release. It is concluded from the empirical study that it is possible to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "59\n", "authors": ["48"]}
{"title": "Requirements Mean Decisions!\u0393\u00c7\u00f4Research issues for understanding and supporting decision-making in Requirements Engineering\n", "abstract": " Requirements result from stakeholders\u0393\u00c7\u00d6 decisions. These decisions are governed by hard issues such as the balance between cost and functionality, and soft issues such as social processes and organisational politics. The quality of the decision-making process is crucial as good-enough requirements is the foundation for a successful focusing of the available development resources. In this paper it is argued that research should focus more on Requirements Engineering (RE) as a decision-making process with focus on describing and understanding it, and on providing and evaluating methods to improve and support RE decisionmaking. There are many opportunities of fruitful interdisciplinary research when combining RE with areas such as decision theory, decision support systems, operations research and management science. A number of research issues are identified and several aspects of RE decisionmaking are described, with the aim of promoting research on methods which can better support requirements engineers in their decision-making.", "num_citations": "57\n", "authors": ["48"]}
{"title": "Software reliability prediction incorporating information from a similar project\n", "abstract": " Although there are many models for the prediction of software reliability using the failure data collected during testing, the estimation is usually inaccurate, especially at the early stages of the testing phase, and hence many practitioners are hesitant to use software reliability models. On the other hand, the traditional software reliability growth models do not make use of information from earlier or similar projects. For example, software systems today are usually an improvement or modification of an earlier version or at least within the same application domain, which implies that some information should be available from similar projects. In this paper we study some approaches for the estimation of software reliability by incorporating information from a similar project. In particular, we use the Goel\u0393\u00c7\u00f4Okumoto model and assume the same value of the fault detection rate. The other parameter is then estimated based on the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "57\n", "authors": ["48"]}
{"title": "Prioritization of issues and requirements by cumulative voting: A compositional data analysis framework\n", "abstract": " Cumulative Voting (CV), also known as Hundred-Point Method, is a simple and straightforward technique, used in various prioritization studies in software engineering. Multiple stakeholders (users, developers, consultants, marketing representatives or customers) are asked to prioritize issues concerning requirements, process improvements or change management in a ratio scale. The data obtained from such studies contain useful information regarding correlations of issues and trends of the respondents towards them. However, the multivariate and constrained nature of data requires particular statistical analysis. In this paper we propose a statistical framework; the multivariate Compositional Data Analysis (CoDA) for analyzing data obtained from CV prioritization studies. Certain methodologies for studying the correlation structure of variables are applied to a dataset concerning impact analysis issues prioritized\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "55\n", "authors": ["48"]}
{"title": "A subjective effort estimation experiment\n", "abstract": " Effort estimation is difficult in general, and in software development it becomes even more complicated if the software process is changed. In this paper a number of alternative interview-based effort estimation methods is presented. The main focus of the paper is to present an experiment in which software engineers were asked to use different methods to estimate the actual effort it would take to perform a number of tasks. The result from the subjective data is compared with the actual outcome from performing the tasks.", "num_citations": "54\n", "authors": ["48"]}
{"title": "Code decay analysis of legacy software through successive releases\n", "abstract": " Prediction of problematic software components is an important activity today for many organizations as they manage their legacy systems and the maintenance problems they cause. This means that there is a need for methods and models to identify troublesome components. We apply a model for classification of software components as green, yellow and red according to the number of times they required corrective maintenance over successive releases. Further, we apply a principal component and box plot analysis to investigate the causes for the code decay and try to characterize the releases. The case study includes eight releases and 130 software components. The outcome indicates a large number of healthy components as well as a small set of troublesome components requiring extensive repair repeatedly. The analysis characterizes the releases and indicates that it is the relationship between\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "53\n", "authors": ["48"]}
{"title": "Identification of green, yellow and red legacy components\n", "abstract": " Software systems are often getting older than expected, and it is a challenge to try to make sure that they grow old gracefully. This implies that methods are needed to ensure that system components are possible to maintain. In this paper, the need to investigate, classify and study software components is emphasized. A classification method is proposed. It is based on classifying the software components into green, yellow and red components. The classification scheme is complemented with a discussion of suitable models to identify problematic components. The scheme and the models are illustrated in a minor case study to highlight the opportunities. The long term objective of the work is to define methods, models and metrics which are suitable to use in order to identify software components which have to be taken care of through either tailored processes (e.g. additional focus on verification and validation) or\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "53\n", "authors": ["48"]}
{"title": "Packaging software process improvement issues: a method and a case study\n", "abstract": " Software process improvement is a challenge in general and in particular for small\u0393\u00c7\u00c9 and medium\u0393\u00c7\u00c9sized companies. Assessment is one important step in improvement. However, given that a list of improvement issues has been derived, it is often very important to be able to prioritize the improvement proposals and also look at the potential dependencies between them. This paper comes from an industrial need to enable prioritization of improvement proposals and to identify their dependencies. The need was identified in a small\u0393\u00c7\u00c9 and medium\u0393\u00c7\u00c9sized software development company. Based on the need, a method for prioritization and identification of dependencies of improvement proposals was developed. The prioritization part of the method is based on a multi\u0393\u00c7\u00c9decision criteria method and the dependencies are identified using a dependency graph. The developed method has been successfully applied in the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "51\n", "authors": ["48"]}
{"title": "Towards integration of use case modelling and usage-based testing\n", "abstract": " This paper focuses on usage modelling as a basis for both requirements engineering (RE) and testing, and investigates the possibility of integrating the two disciplines of use case modelling (UCM) and statistical usage testing (SUT). The paper investigates the conceptual framework for each discipline, and discusses how they can be integrated to form a seamless transition from requirements models to test models for reliability certification. Two approaches for such an integration are identified: integration by model transformation and integration by model extension. The integration approaches are illustrated through an example, and advantages as well as disadvantages of each approach are discussed. Based on the fact that the two disciplines have models with common information and similar structure, it is argued that an integration may result in coordination benefits and reduced costs. Several areas of further\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "51\n", "authors": ["48"]}
{"title": "An analysis of the most cited articles in software engineering journals-2000\n", "abstract": " Citations and related work are crucial in any research to position the work and to build on the work of others. A high citation count is an indication of the influence of specific articles. The importance of citations means that it is interesting to analyze which articles are cited the most. Such an analysis has been conducted using the ISI Web of Science to identify the most cited software engineering journal articles published in 2000. The objective of the analysis is to identify and list the articles that have influenced others the most as measured by citation count. An understanding of which research is viewed by the research community as most valuable to build upon may provide valuable insights into what research to focus on now and in the future. Based on the analysis, a list of the 20 most cited articles is presented here. The intention of the analysis is twofold. First, to identify the most cited articles, and second, to invite the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "48\n", "authors": ["48"]}
{"title": "An investigation of a method for identifying a software architecture candidate with respect to quality attributes\n", "abstract": " To sustain the qualities of a software system during evolution, and to adapt the quality attributes as the requirements evolve, it is necessary to have a clear software architecture that is understood by all developers and to which all changes to the system adheres. This software architecture can be created beforehand, but must also be updated to reflect changes in the domain, and hence the requirements of the software. The choice of which software architecture to use is typically based on informal decisions. There exist, to the best of our knowledge, little factual knowledge of which quality attributes are supported or obstructed by different architecture approaches. In this paper we present an empirical study of a method that enables quantification of the perceived support different software architectures give for different quality attributes. This in turn enables an informed decision of which architecture candidate\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "48\n", "authors": ["48"]}
{"title": "Prioritizing and assessing software project success factors and project characteristics using subjective data\n", "abstract": " This paper presents a method for analyzing the impact software project factors have on project success as defined by project success factors that have been prioritized. It is relatively easy to collect measures of project attributes subjectively (i.e., based on expert judgment). Often Likert scales are used for that purpose. It is much harder to identify whether and how a large number of such ranked project factors influence project success, and to prioritize their influence on project success. At the same time, it is desirable to use the knowledge of project personnel effectively. Given a prioritization of project goals, it is shown how some key project characteristics can be related to project success. The method is applied in a case study consisting of 46 projects. For each project, six success factors and 27 project attributes were measured. Successful projects show common characteristics. Using this knowledge can\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "48\n", "authors": ["48"]}
{"title": "Evaluation of usage-based reading - conclusions after three experiments\n", "abstract": " Software inspections have been introduced in software engineering in order to detect faults before testing is performed. Reading techniques provide reviewers in software inspections with guidelines on how they should check the documents under inspection. Several reading techniques with different purposes have been introduced and empirically evaluated. In this paper, we describe a reading technique with the special aim to detect faults that are severe from a user\u0393\u00c7\u00d6s point of view. The reading technique is named usage-based reading (UBR) and it can be used to inspect all software artefacts. In the series of experiments, a high-level design document is used. The main focus of the paper is on the third experiment, which investigates the information needed for UBR in the individual preparation and the meeting of software inspections. Hence, the paper discusses (1) the series of three experiments of UBR\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "47\n", "authors": ["48"]}
{"title": "Second-generation Systematic Literature Studies using Snowballing\n", "abstract": " Systematic literature studies have become standard practice in software engineering to synthesize evidence in different areas of the discipline. As more such studies are published, there is also a need to extend previously published systematic literature studies to cover new research papers. These first extensions become second-generation systematic literature studies. It has been asserted that snowballing would be a suitable search strategy for these types of second-generation studies, since newer studies ought to refer to previous research on a topic, and in particular to systematic literature studies published in an area. This paper compares using a snowballing search strategy with a published second-generation study using a database search strategy in the area of cross-company vs. within-company effort estimation. It is concluded that the approaches are comparable when it comes to which papers they find\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "46\n", "authors": ["48"]}
{"title": "Empirical research methods in web and software engineering\n", "abstract": " Web and software engineering are not only about technical solutions. They are to a large extent also concerned with organisational issues, project management and human behaviour. For disciplines like Web and software engineering, empirical methods are crucial, since they allow for incorporating human behaviour into the research approach taken. Empirical methods are common practice in many other disciplines. This chapter provides a motivation for the use of empirical methods in Web and software engineering research. The main motivation is that it is needed from an engineering perspective to allow for informed and well-grounded decisions. The chapter continues with a brief introduction to four research methods: controlled experiments, case studies, surveys and post-mortem analyses. These methods are then put into an improvement context. The four methods are presented with the objective to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "46\n", "authors": ["48"]}
{"title": "Soft factors and their impact on time to market\n", "abstract": " Time to market is considered to be one of the most important quality attributes for an organization to retain its competitive edge. Soft factors are at least as important as technical issues in improving and controlling the time to market. Soft factors are used as a collective term for factors that are difficult to quantify exactly, i.e. non-technical aspects. Based on the identified importance of the soft factors, a method, incorporating the soft factors, has been developed to increase the predictability of time to market. The method consists of three main parts: model development, model usage and model maintenance. The proposed method is general, while the actual model is primarily useful for the organization for which data have been collected. The objective is two-fold: first to prevent a general method of incorporating soft factors into the prediction of the time to market, and secondly to identify a set of critical soft factors\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "46\n", "authors": ["48"]}
{"title": "Writing for synthesis of evidence in empirical software engineering\n", "abstract": " Context: Systematic literature reviews have become common in software engineering in the last decade, but challenges remain.Goal: Given the challenges, the objective is to describe improvement areas in writing primary studies, and hence provide a good basis for researchers aiming at synthesizing research evidence in a specific area.Method: The results presented are based on a literature review with respect to synthesis of research results in software engineering with a particular focus on empirical software engineering. The literature review is complemented and exemplified with experiences from conducting systematic literature reviews and working with research methodologies in empirical software engineering.Results: The paper presents three areas where improvements are needed to become more successful in synthesizing empirical evidence. These three areas are: terminology, paper content and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "44\n", "authors": ["48"]}
{"title": "Distribution patterns of effort estimations\n", "abstract": " Effort estimations within software development projects and the ability to work within these estimations are perhaps the single most important, and at the same time inadequately mastered, discipline for overall project success. This study examines some characteristics of accuracies in software development efforts and identifies patterns that can be used to increase the understanding of the effort estimation discipline as well as to improve the accuracy of effort estimations. The study complements current research by taking a more simplistic approach than usually found within mainstream research concerning effort estimations. It shows that there are useful patterns to be found as well as interesting causalities, usable to increase the understanding and effort estimation capability.", "num_citations": "43\n", "authors": ["48"]}
{"title": "Understanding the relations between software quality attributes-a survey approach\n", "abstract": " Software quality is a complex concept containing a large number of quality attributes. These attributes often have different meaning for different people and different attributes are not of equal importance. Moreover, the actual relations between the attributes are mostly poorly understood. Companies have to cope with these relations in their daily software development. On the one hand, companies take management decisions based on experience. On the other hand, researchers address software quality too. However, the two views are not necessarily the same.To increase the understanding of software quality attributes and their relations, two surveys have been conducted. The first survey focuses on the research literature and the second is an interview survey with people from industry. From these surveys, it is concluded that there is an agreement, in qualitative terms, that quality attributes are dependent. However, different opinions exist about the actual relations. No quantitative relations have been found. The main conclusion is that there is a gap between research literature that poses mostly generic relations between quality attributes and the tacit knowledge in industry. The tacit knowledge within industry is largely focused on system specific relations between quality attributes. The result from these surveys provides a compilation of relations between quality attributes that illustrates the gap between the views in industry and academia respectively. The understanding of the gap is the first step towards bringing the two views closer to each other.", "num_citations": "43\n", "authors": ["48"]}
{"title": "Deriving a fault architecture from defect history\n", "abstract": " As software systems mature, there is the danger that not only code decays, but software architectures as well. We adapt a reverse architecting technique to defect reports of a series of releases. Relationships among system components are identified based on whether they are involved in the same defect report, and for how many defect reports this occurs. There are degrees of fault-coupling between components depending on how often two components are involved in a defect fix. After these fault-coupling relationships between components are extracted, they are abstracted to the subsystem level. The resulting fault architecture figures show for each release what its most fault-prone relationships are. Comparing across releases shows whether some relationships between components are repeatedly fault prone, indicating an underlying systemic architecture problem. We illustrate our technique on a large\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "42\n", "authors": ["48"]}
{"title": "A project effort estimation study\n", "abstract": " This paper outlines a four step effort estimation study and focuses on the first and second step. The four steps are formulated to successively introduce a more formal effort experience base. The objective of the study is to evaluate the needed formalism to improve effort estimation and to study different approaches to record and reuse experiences from effort planning in software projects. In the first step (including seven projects), the objective is to compare estimation of effort based on a rough figure (indicating approximate size of the projects) with an informal experience base. The objective of the second step is on reuse of experiences from an effort experience base, where the outcomes of seven previous projects were stored. Seven new projects are planned based on the previous experiences.The plans are, after project completion, compared with the initial plans and with the data from six out of the seven new\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "42\n", "authors": ["48"]}
{"title": "Investigating the applicability of Agility assessment surveys: A case study\n", "abstract": " ContextAgile software development has become popular in the past decade without being sufficiently defined. The Agile principles can be instantiated differently which creates different perceptions of Agility. This has resulted in several frameworks being presented in the research literature to evaluate the level of Agility. However, the evidence of their actual use in practice is limited.ObjectiveThe objective is to identify online surveys that assess/profile Agility in practice, and to evaluate the surveys in an industrial setting.MethodThe Agility assessment surveys were identified through searching the web. Then, they were explored and two surveys were identified as most promising for our objective. The selected surveys were evaluated in a case study with three Agile teams in a software consultancy company.ResultsEach team and its customer separately judged the team's Agility. This outcome was compared with the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "38\n", "authors": ["48"]}
{"title": "Benchmarking k-nearest neighbour imputation with homogeneous Likert data\n", "abstract": " Missing data are common in surveys regardless of research field, undermining statistical analyses and biasing results. One solution is to use an imputation method, which recovers missing data by estimating replacement values. Previously, we have evaluated the hot-deck k-Nearest Neighbour (k-NN) method with Likert data in a software engineering context. In this paper, we extend the evaluation by benchmarking the method against four other imputation methods: Random Draw Substitution, Random Imputation, Median Imputation and Mode Imputation. By simulating both non-response and imputation, we obtain comparable performance measures for all methods. We discuss the performance of k-NN in the light of the other methods, but also for different values of k, different proportions of missing data, different neighbour selection strategies and different numbers of data attributes. Our results show that the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "37\n", "authors": ["48"]}
{"title": "Strategies for industrial relevance in software engineering education\n", "abstract": " This paper presents a collection of experiences related to success factors in graduate and postgraduate education. The experiences are mainly concerned with how to make the education relevant from an industrial viewpoint. This is emphasized as a key issue in software engineering education and research, as the main objective is to give the students a good basis for large-scale software development in an industrial environment. The presentation is divided into experiences at the graduate and postgraduate levels, respectively. For each level a number of strategies to achieve industrial relevance are presented. On the graduate level a course in large-scale software development is described to exemplify how industrial relevance can be achieved on the graduate level. The strategies on the postgraduate level have been successful, but it is concluded that more can be done regarding industrial collaboration in the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "37\n", "authors": ["48"]}
{"title": "Software product quality: Ensuring a common goal\n", "abstract": " Software qualities are in many cases tacit and hard to measure. Thus, there is a potential risk that they get lower priority than deadlines, cost and functionality. Yet software qualities impact customers, profits and even developer efficiency. This paper presents a method to evaluate the priority of software qualities in an industrial context. The method is applied in a case study, where the ISO 9126 model for software quality is combined with Theory-W to create a process for evaluating the alignment between success-critical stakeholder groups in the area of software product quality. The results of an exploratory case study using this tool is then presented and discussed. It is shown that the method provides valuable information about software qualities.", "num_citations": "36\n", "authors": ["48"]}
{"title": "Aligning software project decisions: a case study\n", "abstract": " Recent research in software engineering has highlighted the need to ensure alignment between business objectives, customer requirements and product development. If the business is to meet its strategic objectives, the Requirements Engineering (RE) activities must be executed in a manner such that they support these higher level objectives. A clear alignment between RE activities and the strategic objectives of the organization should underscore the merit of IT investment and the opportunities for competitive advantage that can be pursued as a result by the organization. This research begins with a detailed investigation of the complexity of decision-making during RE activities on business, product and project levels. Secondly, it investigates \"in-project\" level RE decisions and provides empirical findings from an industrial case study. The findings show that RE project related decisions are influenced by business\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "36\n", "authors": ["48"]}
{"title": "Offshore insourcing in software development: Structuring the decision-making process\n", "abstract": " A variety of new forms of business are enabled through globalization and practiced by software organizations today. While companies go global to reduce their development costs, access a larger pool of resources and explore new markets, it is often assumed that the level of delivered services shall remain the same after implementing the sourcing decisions. In contrast, critical studies identified that global software development is associated with unique challenges, and a lot of global projects fail to mitigate the implications of a particular global setting. In this paper we explore offshore insourcing decisions on the basis of empirical research literature and an empirical field study conducted at Ericsson. By analyzing decisions in two different cases we found that each offshore insourcing decision consisted of deciding what, where, when, how and why to insource. Related empirical research and field observations\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "32\n", "authors": ["48"]}
{"title": "A practical method for the estimation of software reliability growth in the early stage of testing\n", "abstract": " The traditional approach of reliability prediction using software reliability growth models requires a large number of failures which might not be available at the beginning of the testing. The commonly used maximum likelihood estimates may not even exist or converge to a reasonable value. In this paper, an approach of making use of information from similar projects in order to obtain an early estimation of one model parameter for a current project is studied. As most of the two-parameter reliability growth models contains one parameter related to the number of faults in the software and a reliability growth rate parameter related to the testing efficiency, information from a similar project can used to estimate the reliability growth rate parameter and the limited failure data from initial testing is used to estimate the other parameter. Our case study shows that this approach is very easy to use as the estimation does not\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "32\n", "authors": ["48"]}
{"title": "On the Performance of Hybrid Search Strategies for Systematic Literature Reviews in Software Engineering\n", "abstract": " ContextWhen conducting a Systematic Literature Review (SLR), researchers usually face the challenge of designing a search strategy that appropriately balances result quality and review effort. Using digital library (or database) searches or snowballing alone may not be enough to achieve high-quality results. On the other hand, using both digital library searches and snowballing together may increase the overall review effort.ObjectiveThe goal of this research is to propose and evaluate hybrid search strategies that selectively combine database searches with snowballing.MethodWe propose four hybrid search strategies combining database searches in digital libraries with iterative, parallel, or sequential backward and forward snowballing. We simulated the strategies over three existing SLRs in SE that adopted both database searches and snowballing. We compared the outcome of digital library searches\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["48"]}
{"title": "Lessons learned from transferring software products to India\n", "abstract": " Globalization has influenced the way software is developed today, and many software organizations have started to actively utilize resources from around the world. However, these endeavors are recognized as very challenging, and they have attracted a lot of attention in software research in the past decade. Unlike many other research initiatives, which explore the complexities of distributed software development activities, the focus of this paper is on software transfers. Software transfers refer to activities that are moved from one location to another. The authors draw attention to the lessons learned from an empirical investigation of two transfer projects conducted at Ericsson. Both transfers were performed between a site in Sweden and a site in India. The observations outline a set of generic practices that have been found useful for transferring software development within a company. It also highlights a number of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["48"]}
{"title": "Quest for a silver bullet: Creating software product value through requirements selection\n", "abstract": " This paper provides results of an empirical study on how software product value is both understood and created through release planning for software products in Australia. We examine how IT professionals perceive value creation through requirements engineering and how the release planning process is conducted to create software product value. We then look at the degree to which the major stakeholders' perspectives are represented in the decision making process. Our findings show that the client and market base of the software product represents the most influential group in the decision to implement specific requirements. This is reflected both in terms of deciding the processes followed and the decision making criteria applied when selecting requirements for the product. It is concluded that the creation of software product value is dependant on the context in which the software product exists, including\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["48"]}
{"title": "An industrial case study on the choice between language customization mechanisms\n", "abstract": " Effective usage of a general purpose modeling language in software engineering poses a need for languagecustomization \u0393\u00c7\u00f4 adaptation of the language for a specific purpose. In the context of the Unified Modeling Language (UML) the customization could be done using two mechanisms: developing profiles and extending the metamodel of UML. This paper presents an industrial case study on the choice between metamodel extensions and profiles as well as the influence of the choice on the quality of products based on the extensions. The results consist of a set of nine prioritized industrial criteria which complement six theoretical criteria previously identified in the literature. The theoretical criteria are focused on the differences between the extension mechanisms of UML while the industrial criteria are focused on development of products based on these extensions. The case study reveals that there are\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["48"]}
{"title": "Understanding software defect detection in the personal software process\n", "abstract": " There is a general need to understand software defects and the ability to detect defects during different activities. This is particularly important in relation to software process improvement, where one objective may be to decrease the number of defects. The Personal Software Process (PSP) has gained attention during the last couple of years as a way to individual improvement in software development. Thus, the PSP is an interesting starting point to understand software defects and in particular the detection process. The paper presents a study of software defect detection for 59 students taking a PSP course. In summary, the study provides valuable insight into software defect detection in the PSP. Some of the results are interesting not only for the PSP, but from a general perspective in the understanding of software defect detection. The understanding of software defects forms the basis for improving the reliability of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["48"]}
{"title": "Aggregating viewpoints for strategic software process improvement\u0393\u00c7\u00f6a method and a case study\n", "abstract": " Decisions regarding strategic software process improvement (SPI) are generally based on the management's viewpoint of the situation, and in some cases also the viewpoints of some kind of an SPI group. This may result in strategies which are not accepted throughout the organisation, as the views of how the process is functioning are different throughout the company. A method for identifying the major factors affecting a process-improvement goal and how the perception of the importance of the factors varies throughout the organisation are described The method lets individuals from the whole development organisation rate the expected effect of these factors from their own viewpoint. In this way the strategic SPI decision can be taken using input from the entire organisation, and any discrepancies in the ratings can also give important SPI-decision information. The method is applied to a case study performed at\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["48"]}
{"title": "Estimating the number of components with defects post\u0393\u00c7\u00c9release that showed no defects in testing\n", "abstract": " Components that have defects after release, but not during testing, are very undesirable as they point to \u0393\u00c7\u00ffholes\u0393\u00c7\u00d6 in the testing process. Either new components were not tested enough, or old ones were broken during enhancements and defects slipped through testing undetected. The latter is particularly pernicious, since customers are less forgiving when existing functionality is no longer working than when a new feature is not working quite properly. Rather than using capture\u0393\u00c7\u00f4recapture models and curve\u0393\u00c7\u00c9fitting methods to estimate the number of remaining defects after inspection, these methods are adapted to estimate the number of components with post\u0393\u00c7\u00c9release defects that have no defects in testing. A simple experience\u0393\u00c7\u00c9based method is used as a basis for comparison. The estimates can then be used to make decisions on whether or not to stop testing and release software. While most investigations so far\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["48"]}
{"title": "Investigating the Use of a Hybrid Search Strategy for Systematic Reviews\n", "abstract": " [Background] Systematic Literature Reviews (SLRs) are one of the important pillars when employing an evidence-based paradigm in Software Engineering. To date most SLRs have been conducted using a search strategy involving several digital libraries. However, significant issues have been reported for digital libraries and applying such search strategy requires substantial effort. On the other hand, snowballing has recently arisen as a potentially more efficient alternative or complementary solution. Nevertheless, it requires a relevant seed set of papers. [Aims] This paper proposes and evaluates a hybrid search strategy combining searching in a specific digital library (Scopus) with backward and forward snowballing. [Method] The proposed hybrid strategy was applied to two previously published SLRs that adopted database searches. We investigate whether it is able to retrieve the same included papers with\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "28\n", "authors": ["48"]}
{"title": "A new index for the citation curve of researchers\n", "abstract": " Internet has made it possible to move towards researcher and article impact instead of solely focusing on journal impact.  To support citation measurement, several indexes have been proposed, including the h-index. The h-index provides a point estimate.  To address this, a new index is proposed that takes the citation curve of a researcher into account. This article introduces  the index, illustrates its use and compares it to rankings based on the h-index as well as rankings based on publications.  It is concluded that the new index provides an added value, since it balances citations and publications through the citation  curve.", "num_citations": "28\n", "authors": ["48"]}
{"title": "An analysis of the most cited articles in software engineering journals\u0393\u00c7\u00f42001\n", "abstract": " Citations and related work are crucial in any research to position the work and to build on the work of others. A high citation count is an indication of the influence of specific articles. The importance of citations means that it is interesting to analyze which articles are cited the most. Such an analysis has been conducted using the ISI Web of Science to identify the most cited software engineering journal articles published in 2001. The objective of the analysis is to identify and list the articles that have influenced others the most as measured by citation count. An understanding of which research is viewed by the research community as most valuable to build upon may provide valuable insights into what research to focus on now and in the future. Based on the analysis, a list of the 20 most cited articles is presented here. The intention of the analysis is twofold. First, to identify the most cited articles, and second, to invite the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "28\n", "authors": ["48"]}
{"title": "A study of the exponential smoothing technique in software reliability growth prediction\n", "abstract": " Software reliability models can provide quantitative measures of the reliability of software systems which are of growing importance today. Most of the models are parametric ones which rely on the modelling of the software failure process as a Markov or non\u0393\u00c7\u00c9homogeneous Poisson process. It has been noticed that many of them do not give a very accurate prediction of future software failures as the focus is on the fitting of past data. In this paper we study the use of the double exponential smoothing technique to predict software failures. The proposed approach is a non\u0393\u00c7\u00c9parametric one and has the ability of providing more accurate prediction compared with traditional parametric models because it gives a higher weight to the most recent failure data for a better prediction of future behaviour. The method is very easy to use and requires a very limited amount of data storage and computational effort. It can be updated\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["48"]}
{"title": "When to Update Systematic Literature Reviews in Software Engineering\n", "abstract": " [Context] Systematic Literature Reviews (SLRs) have been adopted by the Software Engineering (SE) community for approximately 15 years to provide meaningful summaries of evidence on several topics. Many of these SLRs are now potentially outdated, and there are no systematic proposals on when to update SLRs in SE. [Objective] The goal of this paper is to provide recommendations on when to update SLRs in SE. [Method] We evaluated, using a three-step approach, a third-party decision framework (3PDF) employed in other fields, to decide whether SLRs need updating. First, we conducted a literature review of SLR updates in SE and contacted the authors to obtain their feedback relating to the usefulness of the 3PDF within the context of SLR updates in SE. Second, we used these authors\u0393\u00c7\u00d6 feedback to see whether the framework needed any adaptation; none was suggested. Third, we applied the 3PDF to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["48"]}
{"title": "An analysis of the most cited articles in software engineering journals\u0393\u00c7\u00f61999\n", "abstract": " Citations and related work are crucial in any research to position the work and to build on the work of others. A high citation count is an indication of the influence of specific articles. The importance of citations means that it is interesting to analyze which articles are cited the most. Such an analysis has been conducted using the ISI Web of Science to identify the most cited software engineering journal articles published in 1999. The objective of the analysis is to identify and list the articles that have influenced others the most as measured by citation count. An understanding of which research is viewed as most valuable to build upon may provide valuable insights into what research to focus on now and in the future. Based on the analysis, a list of the 20 most cited articles is presented here. The intention of the analysis is twofold. First, to actually show the most cited articles, and second, to invite the authors of the most\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["48"]}
{"title": "How much information is needed for usage-based reading? A series of experiments\n", "abstract": " Software inspections are regarded as an important technique to detect faults throughout the software development process. The individual preparation phase of software inspections has enlarged its focus from only comprehension to also include fault searching. Hence, reading techniques to support the reviewers on fault detection are needed. Usage-based reading (UBR) is a reading technique, which focuses on the important parts of a software document by using prioritized use cases. This paper presents a series of three UBR experiments on design specifications, with focus on the third. The first experiment evaluates the prioritization of UBR and the second compares UBR against checklist-based reading. The third experiment investigates the amount of information needed in the use cases and whether a more active approach helps the reviewers to detect more faults. The third study was conducted at two different\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["48"]}
{"title": "Offshore insourcing: A case study on software quality alignment\n", "abstract": " Background: Software quality issues are commonly reported when off shoring software development. Value-based software engineering addresses this by ensuring key stakeholders have a common understanding of quality. Aim: This work seeks to understand the levels of alignment between key stakeholders on aspects of software quality for two products developed as part of an offshore in sourcing arrangement. The study further aims to explain the levels of alignment identified. Method: Representatives of key stakeholder groups for both products ranked aspects of software quality. The results were discussed with the groups to gain a deeper understanding. Results: Low levels of alignment were found between the groups studied. This is associated with insufficiently defined quality requirements, a culture that does not question management and conflicting temporal reflections on the product's quality\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "25\n", "authors": ["48"]}
{"title": "Differences in views between development roles in software process improvement\u0393\u00c7\u00f4a quantitative comparison\n", "abstract": " This paper presents a quantitative study that evaluates how different roles in a software development organization view different issues in software process improvement. The study is conducted in a large Swedish telecommunication organization with the traditional roles of software development. The respondents of the study got five different questions related to process improvement. The result was that the different roles disagreed in three of the questions while they agreed in two of the questions. The disagreement was related to issues about importance of improvement, urgency of problems, and threat against successful process management, while the questions where the roles agreed focused on communication of the processes (documentation and teaching). It is concluded that it is important to be aware and take into account the different needs of different roles and that looking into other areas (e.g. marketing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "25\n", "authors": ["48"]}
{"title": "Applying sampling to improve software inspections\n", "abstract": " The main objective of software inspections is to find faults in software documents. The benefits of inspections are reported from researchers as well as software organizations. However, inspections are time consuming and the resources may not be sufficient to inspect all documents. Sampling of documents in inspections provides a systematic solution to select what to be inspected in the case resources are not sufficient to inspect everything. The method presented in this paper uses sampling, inspection and resource scheduling to increase the efficiency of an inspection session. A pre-inspection phase is used in order to determine which documents need most inspection time, i.e. which documents contain most faults. Then, the main inspection is focused on these documents. We describe the sampling method and provide empirical evidence, which indicates that the method is appropriate to use. A Monte Carlo\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["48"]}
{"title": "Identification of improvement issues using a lightweight triangulation approach\n", "abstract": " One of the challenges in requirements engineering is the ability to improve the process and establish one that is \u0393\u00c7\u00a3good-enough\u0393\u00c7\u00a5. The objective of this paper is to present a lightweight approach to identify process improvement issues. The approach is developed to capture both the views of different stakeholders and different sources of information. An industrial investigation from a small company is presented. In the investigation both projects and the line organization have been interviewed and documentation from them has been studied to capture key issues for improvement. The issues identified from one source are checked against other sources. The dependencies between the issues have been studied. In total nine issues for improvement of the requirements engineering work at the company were identified. It is concluded that the approach is effective in capturing issues, and that the approach helps different stakeholders to get their view represented in the process improvement work.", "num_citations": "24\n", "authors": ["48"]}
{"title": "An empirical study of effort estimation during project execution\n", "abstract": " Presents an empirical study of effort estimation in software engineering projects. In particular, this study is focused on improvements in effort estimations as more information becomes available. For example, after the requirements phase, the requirements specification is available, and the question is whether the knowledge regarding the number of requirements helps in improving the effort estimation of the project. The objective is twofold. First, it is important to find suitable measures that can be used in the re-planning of the projects. Second, the objective is to study how the effort estimations evolve as a software project is performed. The analysis is based on data from 26 projects. The analysis consists of two main steps: model building based on data from part of the projects, and evaluation of the models for the other projects. No single measure was found to be a particular good measure for an effort prediction\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["48"]}
{"title": "Software Quality Across Borders: Three Case Studies on Company Internal Alignment\n", "abstract": " ContextSoftware quality issues are commonly reported when offshoring software development. Value-based software engineering addresses this by ensuring key stakeholders have a common understanding of quality.ObjectiveThis work seeks to understand the levels of alignment between key stakeholder groups within a company on the priority given to aspects of software quality developed as part of an offshoring relationship. Furthermore, the study aims to identify factors impacting the levels of alignment identified.MethodThree case studies were conducted, with representatives of key stakeholder groups ranking aspects of software quality in a hierarchical cumulative exercise. The results are analysed using Spearman rank correlation coefficients and inertia. The results were discussed with the groups to gain a deeper understanding of the issues impacting alignment.ResultsVarious levels of alignment were\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["48"]}
{"title": "Balancing software product investments\n", "abstract": " The long-term sustainability of a software product depends on more than developing features. Priorities are placed on aspects that support the development of software, like software product quality (eg. ISO 9126), project constraints - time and cost, and even the development of intellectual capital (IC). A greater focus on any one aspect takes priority from another, but as each aspects delivers a different type of value managers have trouble comparing and balancing these aspects. This paper presents a method to help determine the balance between key priorities in the software development process. The method is applied to a new case study, that also combines with results from previous studies. The results show it is possible to compare features, quality, time, cost and IC in a comprehensive way, with the case study showing that participants perceive a change from a shorter-term product perspective to a longer-term\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["48"]}
{"title": "Students as study subjects in software engineering experimentation\n", "abstract": " Experimentation using different types of subjects is an important issue in empirical software engineering. In particular, the use of students as subjects is many times questioned. This paper addresses this issue by dividing subjects into four types, where one type can be viewed as a worst case. It is discussed how students could be viewed as suitable subjects in the worst case. In particular, if it is possible to reject the null hypothesis then it is highly likely that the outcome is also valid for other types of subjects. An example experiment is presented that uses students as subjects and fulfill the criteria for being the worst case. It is concluded that students are good subjects under certain circumstances and that these types of experiments provide an opportunity for using students as subjects.", "num_citations": "23\n", "authors": ["48"]}
{"title": "A classification scheme for studies on fault-prone components\n", "abstract": " Various approaches are presented in the literature to identify faultprone components. The approaches represent a wide range of characteristics and capabilities, but they are not comparable, since different aspects are compared and different data sets are used. In order to enable a consistent and fair comparison, we propose a classification scheme, with two parts, 1) a characterisation scheme which captures information on input, output and model characteristics, and 2) an evaluation scheme which is designed for comparing different models\u0393\u00c7\u00d6 capabilities. The schemes and the rationale for the elements of the schemes are presented in the paper. Important capabilities to evaluate when comparing different models are rate of misclassification, classification efficiency and total classification cost. Further, the schemes are applied in an example study to illustrate the use of the schemes. It is expected that applying\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["48"]}
{"title": "A Proposal for Comparison of Models for Identification of Fault-Proneness\n", "abstract": " In striving for high-quality software, the management of faults plays an important role. The faults reside in software products are not evenly distributed over the software modules; some modules are more fault-prone than others. In order to spend the improvement effort in a cost-effective manner, we want to identify those modules that are most fault-prone. Various models are presented in the literature which identify fault-prone modules, which can be used in quality management processes. The models represent a wide range of characteristics and capabilities. However it is not easy to compare and evaluate different models. Therefore we propose two schemes, 1) a characterization scheme which captures information on input, output and model characteristics, and 2) an evaluation scheme which is designed for comparing different models\u0393\u00c7\u00d6 prediction capabilities. The schemes and the rationale for the elements of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["48"]}
{"title": "Combining data from reading experiments in software inspections: a feasibility study\n", "abstract": " Software inspections have been around for 25 years, and most software engineering researchers and professionals know that they are mostly a cost-effective means for removing software defects. However, this does not mean that there is consensus about how they should be conducted in terms of reading techniques, number of reviewers or the effectiveness of reviewers. Still, software inspections are probably the most extensively empirically studied technique in software engineering. Thus, a large body of knowledge is available in literature. This paper uses 30 data sets from software inspections found in the literature to study different aspects of software inspections. As a feasibility study, the data are amalgamated to increase our understanding and illustrate what could be achieved if we manage to conduct studies where a combination of data can be collected. It is shown how the combinated data may help to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["48"]}
{"title": "Modelling fault\u0393\u00c7\u00c9proneness statistically over a sequence of releases: a case study\n", "abstract": " Many of today's software systems evolve through a series of releases that add new functionality and features, in addition to the results of corrective maintenance. As the systems evolve over time it is necessary to keep track of and manage their problematic components. Our focus is to track system evolution and to react before the systems become difficult to maintain. To do the tracking, we use a method based on a selection of statistical techniques. In the case study we report here that had historical data available primarily on corrective maintenance, we apply the method to four releases of a system consisting of 130 components. In each release, components are classified as fault\u0393\u00c7\u00c9prone if the number of defect reports written against them are above a certain threshold. The outcome from the case study shows stabilizing principal components over the releases, and classification trees with lower thresholds in their\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["48"]}
{"title": "Meeting the challenge of large-scale software development in an educational environment\n", "abstract": " This paper describes a final-year Master's course in large-scale software development. A number of issues such as closeness to an application domain and executing the software on a real target system are stressed in order to imitate an industrial environment. The experience gained from the course is discussed. In particular, effort data (in terms of man-hours) from the course are presented, both from the plan devised by the students, and the actual outcome. Furthermore, it is discussed how the data can be used to create an experience base for the future. The objective for next year's course is to let the students plan their projects based on the experience base. The experience base will also form the basis to complement the course with a complete experience factory.", "num_citations": "22\n", "authors": ["48"]}
{"title": "Empirical software engineering: Teaching methods and conducting studies\n", "abstract": " Empirical software engineering has grown in importance in the software engineering research community over the last 20 years. This means that it has become very important to also include empirical studies systematically into the curricula in computer science and software engineering. This chapter presents several aspects and challenges to have in mind when doing this. The chapter presents three different educational levels to have in mind when introducing empirical software engineering into the curricula. An introduction into the curricula also means increased possibilities to run empirical studies in student settings. Some challenges in relation to this is presented and the need to balance educational and research objectives is stressed.", "num_citations": "21\n", "authors": ["48"]}
{"title": "Are individual differences in software development performance possible to capture using a quantitative survey?\n", "abstract": " Software engineering is human intensive. Thus, it is important to understand and evaluate the value of different types of experiences, and their relation to the quality of the developed software. Many job advertisements focus on requiring knowledge of, for example, specific programming languages. This may seem sensible at first sight, but is it really possible to capture software development performance using this kind of simple measure? On the other hand, maybe it is sufficient to have general knowledge in programming and then it is enough to learn a specific language within the new job. Two key questions are (1) whether prior knowledge of a specific language actually does improve software quality and (2) whether it is possible to capture performance using simple quantitative measures? This paper presents an empirical study where the experience, for example with respect to a specific programming\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["48"]}
{"title": "Is it possible to decorate graphical software design and architecture models with qualitative Information? - An experiment\n", "abstract": " Software systems evolve over time and it is often difficult to maintain them. One reason for this is that often it is hard to understand the previous release. Further, even if architecture and design models are available and up to date, they primarily represent the functional behavior of the system. To evaluate whether it is possible to also represent some nonfunctional aspects, an experiment has been conducted. The objective of the experiment is to evaluate the cognitive suitability of some visual representations that can be used to represent a control relation, software component size and component external and internal complexity. Ten different representations are evaluated in a controlled environment using 35 subjects. The results from the experiment show that representations with low cognitive accessibility weight can be found. In an example, these representations are used to illustrate some qualities in an SDL block\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["48"]}
{"title": "Software faults: spreading, detection and costs\n", "abstract": " The authors consider, through modelling, how software faults are spread throughout the entire life-cycle of a large software product and how fault detection and correction processes will affect the spreading mechanism. The study is further enlarged to incorporate models for cost estimation. The models can be described as being of a qualitative rather than a quantitative nature, in that they highlight the effects of different approaches relative to each other before giving 'exact' values for each approach. The study reveals that the behaviour and consequences of different ways of spreading and detection, as well as different cost mixtures, can be studied and thus understood.< >", "num_citations": "21\n", "authors": ["48"]}
{"title": "Guidelines for the Search Strategy to Update Systematic Literature Reviews in Software Engineering\n", "abstract": " ContextSystematic Literature Reviews (SLRs) have been adopted within Software Engineering (SE) for more than a decade to provide meaningful summaries of evidence on several topics. Many of these SLRs are now potentially not fully up-to-date, and there are no standard proposals on how to update SLRs in SE.ObjectiveThe objective of this paper is to propose guidelines on how to best search for evidence when updating SLRs in SE, and to evaluate these guidelines using an SLR that was not employed during the formulation of the guidelines.MethodTo propose our guidelines, we compare and discuss outcomes from applying different search strategies to identify primary studies in a published SLR, an SLR update, and two replications in the area of effort estimation. These guidelines are then evaluated using an SLR in the area of software ecosystems, its update and a replication.ResultsThe use of a single\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["48"]}
{"title": "Cost savings in global software engineering - Where\u0393\u00c7\u00d6s the Evidence\n", "abstract": " The popularity of offshoring (developing software in low cost countries) keeps growing fueled by the rumors of significant costsavings. Despite claims that offshoring is motivated for less offensive reasons than simply reducing costs, the main driving force has always been related to costs [2]. The realization of economic benefits, however, seems to be mystified, resulting in a lack of clarity. On one hand, industrial experiences demonstrate that the assumed benefits are not always achieved and thus shall not be taken for granted [5]. On the other hand, senior executives are easily driven away by promised salary differences. An industry report about a successful offshore outsourcing project with timely delivered good quality software mentions attractive hourly rates in India versus Germany and consequent \u0393\u00c7\u00a3significant cost saving\u0393\u00c7\u00a5 expressed as \u0393\u00c7\u00a3several million\u0393\u00e9\u00bc/year\u0393\u00c7\u00a5[1]. But how trustworthy is such claims? Is there any actual evidence and transparency in the way costs have been calculated? Other industrial studies indicate that cost-savings are not as drastic as pure salary comparisons suggest, due to additional managerial overhead [5],[6]. In fact, the benefit of low-cost labor must be weighed against the risk of missed deadlines, dissatisfied users, and failure to reduce development costs. So what shall we trust? To shed some light on this matter, we trace the claims of cost-savings and offer our judgment of the evidence.Evidence is the basis for decision-making. The most commonly known use of evidence in society is probably in law. Taking the juridical system as a metaphor, we here extend, summarize and use a concept denoted the evidence profile\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["48"]}
{"title": "A method proposal for early software reliability estimation.\n", "abstract": " This paper presents a method proposal for estimation of software reliability before the implementation phase. The method is based upon that a formal description technique is used and that it is possible to develop a tool performing dynamic analysis, ie locating semantic faults in the design. The analysis is performed with both applying a usage profile as input as well as doing a full analysis, ie locate all faults that the tool can find. The tool must provide failure data in terms of time since the last failure was detected. The mapping of the dynamic failures to the failures encountered during statistical usage testing and operation is discussed. The method can be applied either on the software specification or as a step in the development process by applying it on the design descriptions. The proposed method will allow for software reliability estimations that can be used both as a quality indicator, but also for planning and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["48"]}
{"title": "An Evolutionary Perspective on Socio-Technical Congruence: The Rubber Band Effect\n", "abstract": " Conway's law assumes a strong association between the system's architecture and the organization's communication structure that designs it. In the light of contemporary software development, when many companies rely on geographically distributed teams, which often turn out to be temporarily composed and thus having an often-changing communication structure, the importance of Conway's law and its inspired work grows. In this paper, we examine empirical research related to Conway's law and its application for cross-site coordination. Based on the results obtained we conjecture that changes in the communication structure alone sooner or later trigger changes in the design structure of the software products to return the socio-technical system into the state of congruence. This is further used to formulate a concept of a rubber band effect and propose a replication study that goes beyond the original idea of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["48"]}
{"title": "Evaluation of three methods to predict project success: a case study\n", "abstract": " To increase the likelihood for software project success, it is important to be able to identify the drivers of success. This paper compares three methods to identify similar projects with the objective to predict project success. The hypothesis is that projects with similar characteristics are likely to have the same outcome in terms of success. Two of the methods are based on identifying similar projects using all available information. The first method of these aims at identifying the most similar project. The second method identifies a group of projects as most similar. Finally, the third method pinpoints some key characteristics to identify project similarity. Our measure of success for these identifications is whether project success for these projects identified as similar is the same. The comparison between methods is done in a case study with 46 projects with varying characteristics. The paper evaluates the performance\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["48"]}
{"title": "An empirical study of experience-based software defect content estimation methods\n", "abstract": " Capture-recapture models and curve-fitting models have been proposed to estimate the remaining number of defects after a review. This estimation gives valuable information to monitor and control software reliability. However, the different models provide different estimates making it difficult to know which estimate is the most accurate. One possible solution is to, as in this paper, focus on different opportunities to estimate intervals. The study is based on thirty capture-recapture data sets from software reviews. Twenty of the data sets are used to create different models to perform estimation. The models are then evaluated on the remaining ten data sets. The study shows that the use of historical data in model building is one way to overcome some of the problems experienced with both capture-recapture and curve-fitting models, to estimate the defect content after a review.", "num_citations": "19\n", "authors": ["48"]}
{"title": "A Decision-making Process-line for Selection of Software Asset Origins and Components\n", "abstract": " Selecting sourcing options for software assets and components is an important process that helps companies to gain and keep their competitive advantage. The sourcing options include: in-house, COTS, open source and outsourcing. The objective of this paper is to further refine, extend and validate a solution presented in our previous work. The refinement includes a set of decision-making activities, which are described in the form of a process-line that can be used by decision-makers to build their specific decision-making process. We conducted five case studies in three companies to validate the coverage of the set of decision-making activities. The solution in our previous work was validated in two cases in the first two companies. In the validation, it was observed that no activity in the proposed set was perceived to be missing, although not all activities were conducted and the activities that were conducted were\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "18\n", "authors": ["48"]}
{"title": "Fostering and sustaining innovation in a fast growing agile company\n", "abstract": " Sustaining innovation in a fast growing software development company is difficult. As organisations grow, peoples\u0393\u00c7\u00d6 focus often changes from the big picture of the product being developed to the specific role they fill. This paper presents two complementary approaches that were successfully used to support continued developer-driven innovation in a rapidly growing Australian agile software development company. The method \u0393\u00c7\u00a3FedExTM Day\u0393\u00c7\u00a5 gives developers one day to showcase a proof of concept they believe should be part of the product, while the method \u0393\u00c7\u00a320% Time\u0393\u00c7\u00a5 allows more ambitious projects to be undertaken. Given the right setting and management support, the two approaches can support and improve bottom-up innovation in organizations.", "num_citations": "18\n", "authors": ["48"]}
{"title": "Supporting Strategic Decision-making for Selection of Software Assets\n", "abstract": " Companies developing software are constantly striving to gain or keep their competitive advantage on the market. To do so, they should balance what to develop themselves and what to get from elsewhere, which may be software components or software services. These strategic decisions need to be aligned with business objectives and the capabilities and constraints of possible options. These sourcing options include: in-house, COTS, open source and outsourcing. The objective of this paper is to present an approach to support decision-makers in selecting appropriate types of origins in a specific case that maximizes the benefits of the selected business strategy. The approach consists of three descriptive models, as well as a decision process and a knowledge repository. The three models are a decision model that comprises three cornerstones (stakeholders, origins and criteria) and is based on a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["48"]}
{"title": "Consensus building when comparing software architectures\n", "abstract": " When designing a software system it is beneficial to study and use architectural styles from literature, to ensure certain quality attributes. However, as the interpretation of literature may differ depending on the background and area of expertise of the person reading the literature, we suggest that structured discussions about different architecture candidates provides more valuable insight not only in the architectures themselves, but in peoples\u0393\u00c7\u00d6 opinions of the architectures\u0393\u00c7\u00d6 benefits and liabilities. In this paper, we propose a method to elicit the views of individuals concerning architecture candidates for a software system and pinpoint where discussions are needed to come to a consensus view of the architectures.", "num_citations": "17\n", "authors": ["48"]}
{"title": "Evaluation of using Capture-Recapture Methods in Software Review Data\n", "abstract": " Software defect content estimation is important, to control and ensure software quality. One possible method to achieve this is by applying capturerecapture methods. This type of methods can be applied on data collected from reviews, and it can be used to estimate the remaining number of defects after a review. This paper focuses on replicating a previous study made by other researchers. The replication is concerned with looking at different estimation methods and the ability to select a good enough estimation method. The evaluation in this paper is based on three data sets from three different types of documents: a text document, code and a requirements document. It is concluded that no estimation method is generally superior, and the best method is dependent on several factors, including the actual data sets and the number of reviewers. Moreover, it is concluded that the selection method proposed in the previous study is not superior to one specific model. Thus, the selection procedure is unable to pinpoint the best estimation method in our data sets.", "num_citations": "17\n", "authors": ["48"]}
{"title": "An Evidence Profile for Software Engineering Research and Practice\n", "abstract": " Evidence-based software engineering has emerged as an important part of software engineering. The need for empirical evaluation and hence evidence when developing new models, methods, techniques and tools in research has grown in the last couple of decades. Furthermore, industrial decision-making ought to become more evidence-based. The objective here is to develop and present an evidence-based profile, which could be used to divide pieces of evidence into different types and hence create an overall picture of evidence in a specific case. The evidence profile is developed in such a way that it allows evidence to be judged in context. The evidence profile consists of five types of evidence, and it is illustrated for perspective-based reading. It is shown how pieces of evidence can be classified into the different types. It is concluded that this type of approach may be useful for capturing the evidence\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["48"]}
{"title": "Most cited journal articles in software engineering\n", "abstract": " Editorial: Most cited journal articles in software engineering: Information and Software Technology: Vol 49, No 1 ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Information and Software Technology Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsInformation and Software TechnologyVol. , No. Editorial: Most cited journal articles in software engineering article Editorial: Most cited journal articles in software engineering Share on Authors: Claes Wohlin profile image Claes Wohlin Blekinge Institute of Technology, Department of Systems and Software Engineering, School of Engineering, Box 520, SE-372 25 Ronneby, Sweden Blekinge Institute -\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["48"]}
{"title": "Adding Value to Software Requirements\u2229\u255d\u00dc An Empirical Study in the Chinese Software Industry\n", "abstract": " The rapid growth of the Chinese software industry has attracted attention from all over the world. Meanwhile, software requirements selection has a crucial impact on the final value of a software product and the satisfaction of stakeholders. This paper presents an empirical study, which focuses on the decision-making criteria for requirements selection in market-driven software development projects in international companies in China. The outcome shows that some criteria, such as business strategy, customer satisfaction, and software features, are more important than others when making decisions for requirements selection.", "num_citations": "16\n", "authors": ["48"]}
{"title": "Understanding some software quality aspects from architecture and design models\n", "abstract": " Software systems evolve over time and it is often difficult to maintain them. One reason for this is often that it is hard to understand the previous release. Further even if architecture and design models are available and up to date, they primarily represent the functional behaviour of the system. To evaluate whether it is possible to also represent some non-functional aspects, an experiment has been conducted. The objective of the experiment is to evaluate the cognitive suitability of some visual representations that can be used to represent a control relation, software component size and component external and internal complexity. Ten different representations are evaluated in a controlled environment using 35 subjects. The results from the experiment show that it is possible to also represent some non-functional aspects. It is concluded that the incorporation of these representations in architecture and design\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["48"]}
{"title": "Revisiting measurement of software complexity\n", "abstract": " Software complexity measures are often proposed as suitable indicators of different software quality attributes. The paper presents a study of some complexity measures and their correlation with the number of failure reports, and a number of problems are identified. The measures are such poor predictors that one may as well use a very simple measure. The proposed measure is supposed to be ironic to stress the need for a more scientific approach to software measurement. The objective is primarily to encourage discussions concerning methods to estimate different quality attributes. It is concluded that either completely new methods are needed to predict software quality attributes or a new view on predictions from complexity measures is needed. This is particularly crucial if the software industry is to use software metrics successfully on a broad basis.", "num_citations": "15\n", "authors": ["48"]}
{"title": "An extended global software engineering taxonomy\n", "abstract": " In Global Software Engineering (GSE), the need for a common terminology and knowledge classification has been identified to facilitate the sharing and combination of knowledge by GSE researchers and practitioners. A GSE taxonomy was recently proposed to address such a need, focusing on a core set of dimensions; however its dimensions do not represent an exhaustive list of relevant GSE factors. Therefore, this study extends the existing taxonomy, incorporating new GSE dimensions that were identified by means of two empirical studies conducted recently. To address the research questions of this study, we used evidence found through a systematic literature review and a survey. Based on literature, new dimensions were added to the existing taxonomy. We identified seven dimensions to extend and incorporate into the recently proposed GSE taxonomy. The resulting extended taxonomy was later on\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["48"]}
{"title": "The impact of time controlled reading on software inspection effectiveness and efficiency: a controlled experiment\n", "abstract": " Reading techniques help to guide reviewers during individual software inspections. In this experiment, we completely transfer the principle of statistical usage testing to inspection reading techniques for the first time. Statistical usage testing relies on a usage profile to determine how intensively certain parts of the system shall be tested from the users' perspective. Usage-based reading applies statistical usage testing principles by utilizing prioritized use cases as a driver for inspecting software artifacts (eg, design). In order to reflect how intensively certain use cases should be inspected, time budgets are introduced to usage-based reading where a maximum inspection time is assigned to each use case. High priority use cases receive more time than low priority use cases. A controlled experiment is conducted with 23 Software Engineering M. Sc. students inspecting a design document. In this experiment, usage\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["48"]}
{"title": "Analysing primary and lower order project success drivers\n", "abstract": " Project success is influenced by many factors. Some are primary drivers of project success, others secondary. However, these secondary factors may be no less important to understand and learn from than the primary factors. In addition, project success drivers are often qualitative and subjective, eluding analysis through traditional statistical methods. This paper presents a method that analyses the influence and nature of primary and lower order project success drivers. A case study illustrates the usefulness of this analysis and the additional understanding gained from including lower order success drivers in the analysis. The method extends existing work that has been restricted to primary drivers.", "num_citations": "14\n", "authors": ["48"]}
{"title": "The Personal Software Process as a context for empirical studies\n", "abstract": " This paper discusses the use of the Personal Software Process (PSP) as a context for doing empirical studies. It is argued that the PSP provides an interesting environment for doing empirical studies. In particular, if we already teach or use the PSP then it could be wise to also conduct empirical studies as part of that effort. The objective of this paper is to present the idea and discuss the opportunities in combining the PSP with empirical studies. Two empirical studies, one experiment and one case study, are presented to illustrate the idea. It is concluded that we obtain some new and interesting opportunities, in particular we obtain a well-defined context and hence ease replication of the empirical studies considerably.", "num_citations": "14\n", "authors": ["48"]}
{"title": "Software metrics: fault content estimation and software process control\n", "abstract": " The paper shows how software metrics can be used to plan and control software projects. Software metrics will be essential if the software industry is to continue growing and developing complex systems. The only way to increase knowledge of the software development and maintenance processes and the final product is to measure them and use the measurements in models for estimating their future behaviour. The emphasis of this paper is on complexity metrics and reliability models, and especially on their use for fault content estimation and control of the development and maintenance processes. Empirical results and guidelines of how to use complexity metrics and reliability models are presented.", "num_citations": "14\n", "authors": ["48"]}
{"title": "Assessing and understanding efficiency and success of software production\n", "abstract": " One of the goals of collectingproject data during software development and evolution is toassess how well the project did and what should be done to improvein the future. With the wide range of data often collected andthe many complicated relationships between them, this is notalways easy. This paper suggests to use production models (DataEnvelope Analysis) to analyze objective variables and their impacton efficiency. To understand the effect of subjective variables,it is suggested to apply principal component analysis (PCA).Further, we propose to combine the results from the productionmodels and the analysis of the subjective variables. We showcapabilities of production models and illustrate how productionmodels can be combined with other approaches to allow for assessingand hence understanding software project data. The approach isillustrated on a data set consisting of 46 software\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["48"]}
{"title": "Contextualizing research evidence through knowledge translation in software engineering\n", "abstract": " Usage of software engineering research in industrial practice is a well-known challenge. Synthesis of knowledge from multiple research studies is needed to provide evidence-based decision-support for industry. The objective of this paper is to present a vision of how a knowledge translation framework may look like in software engineering research, in particular how to translate research evidence into practice by combining contextualized expert opinions with research evidence. We adopted the framework of knowledge translation from health care research, adapted and combined it with a Bayesian synthesis method. The framework provided in this paper includes a description of each step of knowledge translation in software engineering. Knowledge translation using Bayesian synthesis intends to provide a systematic approach towards contextualized, collaborative and consensus-driven application of research\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["48"]}
{"title": "Alignment of business, architecture, process, and organisation in a software development context\n", "abstract": " In this paper we investigate the current state of work regarding alignment of Business, Architecture, Process, and Organisation (BAPO) perspectives in a software product development context. We planned to do that by conducting a systematic literature study to capture the state of the art in alignment of BAPO in software development. But, as it turned out we found that almost no substantial information is available about the alignment of BAPO in software development. Thus, based on the available literature and a small qualitative study, we defined a conceptual model of the alignment of BAPO including five levels of alignment that can be used as a basis for future empirical studies.", "num_citations": "11\n", "authors": ["48"]}
{"title": "Classification of Software Transfers\n", "abstract": " Many companies have development sites around the globe. This inevitably means that development work may be transferred between the sites. This paper defines a classification of software transfer types, it divides transfers into three main types: full, partial and gradual transfers to describe the context of a transfer. The differences between transfer types, and hence the need for a classification, are illustrated with staffing curves for two different transfer types. The staffing curves are obtained through a combination of interviews with both high-level management and a group of experts, and an industrial case study. From the empirical work, it is concluded that the distribution of personnel differs for different types of transfer, which means that it is crucial to be clear about different classes of software transfers. If not, it is easy to underestimate the effort needed to transfer software work as well as additional costs related to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["48"]}
{"title": "Understanding Impact Analysis: An Empirical Study to Capture Knowledge on Different Organisational Level.\n", "abstract": " Change impact analysis is a crucial change management activity that previously has been studied much from a technical perspective. In this paper, we present a systematic interview-based study of a non-technical aspect of impact analysis. In the study, we have investigated how potential issues and uses of impact analysis are viewed by industrial experts at three organisational levels, based on Anthony's decision-making model: operative, tactical and strategic. The results from our analyses show that on the whole, agreement on both issues and uses was large. There were, however, some differences among the levels in terms of issues. Thus, we conclude that it is both relevant and important to study impact analysis on different organisational levels.", "num_citations": "11\n", "authors": ["48"]}
{"title": "Defect content estimation for two reviewers\n", "abstract": " Estimation of the defect content is important to enable quality control throughout the software development process. Capture-recapture methods and curve fitting methods have been suggested as tools to estimate the defect content after a review. The methods are highly reliant on the quality of the data. If the number of reviewers is fairly small, it becomes difficult or even impossible to get reliable estimates. This paper presents a comprehensive study of estimates based on two reviewers, using real data from reviews. Three experience-based defect content estimation methods are evaluated vs. methods that use data only from the current review. Some models are possible to distinguish from each other in terms of statistical significance. In order to gain an even better understanding, the best models are compared subjectively. It is concluded that the experience-based methods provide some good opportunities to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["48"]}
{"title": "Understanding the sources of software defects: A filtering approach\n", "abstract": " The paper presents a method proposal of how to use product measures and defect data to enable understanding and identification of design and programming constructs that contribute more than expected to the defect statistics. The paper describes a method that can be used to identify the most defect-prone design and programming constructs and the method proposal is illustrated on data collected from a large software project in the telecommunication domain. The example indicates that it is feasible, based on defect data and product measures, to identify the main sources of defects in terms of design and programming constructs. Potential actions to be taken include less usage of particular design and programming constructs, additional resources for verification of the constructs and further education into how to use the constructs.", "num_citations": "11\n", "authors": ["48"]}
{"title": "Evaluating defect content estimation rules in software inspections\n", "abstract": " This paper is concerned with evaluating two different improvements of an existing defect content estimation model. The model improved is a curve-fitting model. Two new estimation rules are evaluated and compared with the original model. Further, the new estimation rules and the original model are evaluated against one of the most successful defect content estimation models, which is a capture-recapture model. It is concluded that one of the new estimation rules for the curve-fitting model could be a good complement to the capture-recapture model. Moreover, it is concluded that the results support previously published results and hence show strong evidence for that the studied model are mature enough to be transferred to industrial use to support continuous quality assessment and control.", "num_citations": "11\n", "authors": ["48"]}
{"title": "Software reliability and performance modelling for telecommunication systems\n", "abstract": " Degree: Takn. drDegreeYear: 1991Institute: Lunds Universitet (Sweden)Publisher: Department Of Communication Systems, Lund Institute Of Technology, Box 118, S-221 00 Lund, Sweden.", "num_citations": "11\n", "authors": ["48"]}
{"title": "Software testing and reliability for telecommunication systems\n", "abstract": " This paper deals with software testing, and how to estimate reliability through models during testing. An existing classification of software reliability models are described. The existing models do not work during function testing of telecommunication systems and due to this an investigation of a couple of projects was made. A Markovian model for prediction of software error occurrences is developed and the use of the model is described in three parts; early use, main use and further development. Results obtained from the model are compared with software error data from another telecommunication project, than those investigated. The necessity of a mathematical and scientific foundation of Software Engineering is stressed.", "num_citations": "11\n", "authors": ["48"]}
{"title": "A model for software rework reduction through a combination of anomaly metrics\n", "abstract": " Analysis of anomalies reported during testing of a project can tell a lot about how well the processes and products work. Still, organizations rarely use anomaly reports for more than progress tracking although projects commonly spend a significant part of the development time on finding and correcting faults. This paper presents an anomaly metrics model that organizations can use for identifying improvements in the development process, i.e. to reduce the cost and lead-time spent on rework-related activities and to improve the quality of the delivered product. The model is the result of a four year research project performed at Ericsson.", "num_citations": "10\n", "authors": ["48"]}
{"title": "Determining the improvement potential of a software development organization through fault analysis: a method and a case study\n", "abstract": " Successful software process improvement depends on the ability to analyze past projects and determine which parts of the process that could become more efficient. One typical data source is the faults that are reported during product development. From an industrial need, this paper provides a solution based on a measure called faults-slip-through, i.e. the measure tells which faults that should have been found in earlier phases. From the measure, the improvement potential of different parts of the development process is estimated by calculating the cost of the faults that slipped through the phase where they should have been found. The usefulness of the method was demonstrated by applying it on two completed development projects at Ericsson AB. The results show that the implementation phase had the largest improvement potential since it caused the largest faults-slip-through cost to later phases, i.e\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["48"]}
{"title": "Modelling and generation of software usage\n", "abstract": " Usage testing or operational profile testing seems to be the general trend in testing software systems, but it is not enough. This paper presents briefly a view which extends the usage testing concept into other software evaluation techniques than traditional testing. It is also shown how software usage can be generated semi-automatically for use throughout the software life cycle. The generation of software usage is made by modelling usage with a state hierarchy and then by transforming it into a usage model in a well-defined description language.", "num_citations": "10\n", "authors": ["48"]}
{"title": "Bayesian Synthesis for Knowledge Translation in Software Engineering: Method and Illustration\n", "abstract": " Systematic literature reviews in software engineering are necessary to synthesize evidence from multiple studies to provide knowledge and decision support. However, synthesis methods are underutilized in software engineering research. Moreover, translation of synthesized data (outcomes of a systematic review) to provide recommendations for practitioners is seldom practiced. The objective of this paper is to introduce the use of Bayesian synthesis in software engineering research, in particular to translate research evidence into practice by providing the possibility to combine contextualized expert opinions with research evidence. We adopted the Bayesian synthesis method from health research and customized it to be used in software engineering research. The proposed method is described and illustrated using an example from the literature. Bayesian synthesis provides a systematic approach to incorporate\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["48"]}
{"title": "A study on prioritisation of impact analysis issues: A comparison between perspectives\n", "abstract": " Impact analysis, which concerns the analysis of the impact of proposed changes to a system, is an important change management activity that previously has been studied mostly with respect to technical aspects. In this paper, we present results from a study where issues with impact analysis were prioritised with respect to criticality by professional software developers from an organisational perspective and a self-perspective. We visualise the prioritisation in a way that allows us to identify priority classes of issues and to discuss differences between the perspectives. Furthermore, we look at issue characteristics that relate to said differences, and identify a number of improvements that could help mitigate the issues. We conclude that looking at multiple perspectives is rewarding and entails certain benefits when dealing with software process improvement, but also that the prioritisation and visualisation approach seems to be good for optimising software process improvement efforts in general.", "num_citations": "9\n", "authors": ["48"]}
{"title": "Is prior knowledge of a programming language important for software quality?\n", "abstract": " Software engineering is human intensive. Thus, it is important to understand and evaluate the value of different types of experiences, and their relation to the quality of the developed software. Many job advertisements focus on requiring knowledge of specific programming languages. This may seem sensible at first sight, but maybe it is sufficient to have general knowledge in programming and then it is enough to learn a specific language within the new job. A key question is whether prior knowledge actually does improve software quality. This paper presents an empirical study where the programming experience of students is assessed using a survey at the beginning of a course on the Personal Software Process (PSP), and the outcome of the course is evaluated, for example, using the number of defects and development time. Statistical tests are used to analyse the relationship between programming experience\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["48"]}
{"title": "Sample Driven Inspections\n", "abstract": " The main objective of software inspections is to find faults in software artefacts. The benefits of inspections are reported from researchers as well as software organizations. In some studies, the fault detection in inspections has shown to be more efficient than other validation and verification activities. A problem, however, is that inspections sometimes are not as efficient and effective as expected. The reason may be that the software artefact inspected contains few faults. In addition, when a software project runs late, inspections are often not properly conducted. This leads to that many faults are not detected, valuable time is lost and people\u0393\u00c7\u00d6s trust in inspections is affected negatively. Sample-Driven Inspections (SDI) provides a solution to these problems. The concept of SDI uses sampling, inspection and resource scheduling to increase the efficiency of an inspection session. SDI uses a pre-inspection phase in order to determine which artefacts need more inspection time, ie which artefacts contain most faults. The second phase of SDI is a main inspection with a special attention on the artefacts with most faults. In this paper, the SDI method is described and empirical evidence is provided, which indicates that the method is appropriate to use. A Monte Carlo simulation is used to evaluate the proposed method. Furthermore, the paper discusses the results and important future research in the area of SDI.", "num_citations": "9\n", "authors": ["48"]}
{"title": "Search Strategy to Update Systematic Literature Reviews in Software Engineering\n", "abstract": " [Context] Systematic Literature Reviews (SLRs) have been adopted within the Software Engineering (SE) domain for more than a decade to provide meaningful summaries of evidence on several topics. Many of these SLRs are now outdated, and there are no standard proposals on how to update SLRs in SE. [Objective] The goal of this paper is to provide recommendations on how to best to search for evidence when updating SLRs in SE. [Method] To achieve our goal, we compare and discuss outcomes from applying different search strategies to identifying primary studies in a previously published SLR update on effort estimation. [Results] The use of a single iteration forward snowballing with Google Scholar, and employing the original SLR and its primary studies as a seed set seems to be the most cost-effective way to search for new evidence when updating SLRs. [Conclusions] The recommendations can be\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["48"]}
{"title": "Is there a future for empirical software engineering?\n", "abstract": " Empirical studies of different kinds are nowadays regularly published in software engineering journals and conferences. Many empirical studies have been published, but are this sufficient? Individual studies are important, but the actual potential in relation to evidence-based software engineering [1] is not fully exploited. As a discipline we have to be able to go further to make our individual studies more useful. Other research should be able to leverage on the studies and industry should be able to make informed decisions based on the empirical research.", "num_citations": "8\n", "authors": ["48"]}
{"title": "Capture-recapture in software unit testing: a case study\n", "abstract": " Quantitative failure estimates for software systems are traditionally made at end of testing using software reliability growth modeling. A persistent problem with most kinds of failure estimation methods and models is the dependency on historical data. This paper presents a method for estimating the total amount of failures possible to provoke from a unit, without historical data dependency. The method combines the results from having several developers testing the same unit with capture-recapture models to create an estimate of'remaining'number of failures. The evaluation of the approach consists of two steps: first a pre-study where the tools and methods are tested in a large open source project, followed by an add-on to a project at a medium sized software company. The evaluation was a success. An estimate was created, and it can be used both as a quality gatekeeper for units and input to functional and system\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["48"]}
{"title": "A comparative study of quantitative and qualitative views of software architectures\n", "abstract": " In order to obtain a software architecture with the right quality attributes, it is vital to fully understand the benefits and liabilities of all involved architecture candidates. To this end, all possible sources should be used. In this paper we compare a quantitative description of software architectures and the support given for different quality attributes with a qualitative description. In some areas they strengthen each other, but in others they do not. In conclusion, this study shows a need to increase the understanding of the quality strengths and weaknesses of different software architectures.", "num_citations": "8\n", "authors": ["48"]}
{"title": "Evaluation of software quality attributes during software design\n", "abstract": " 2 ObjectivesThe main objective of this work is to formulate a general (independent of software description technique) method for functional, performance and reliability evaluation at an early stage of software development. The long term objective is to formulate a method that can be applied throughout the software life cycle to evaluate and assess the quality attributes of software systems. The objective is that the principles presented can be used throughout the software life cycle even if the actual level of detail in the models used may vary depending on available information. The aim is to provide a method for evaluation of functional real time behaviour, performance (in terms of capacity) and reliability of software design descriptions. The method is based on that the software design descriptions are specified with a well-defined language, for example SDL, which can be transformed automatically into a simulation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["48"]}
{"title": "Performance analysis in the early design of software\n", "abstract": " Introduces a methodology to transform descriptions of systems into models suitable for performance analysis. The need for metrics and measurements of different system qualities throughout the life-cycle of a system, following the ideas in Rapp and Sjodin (1983) is also discussed. Special emphasis is put on the early design phases in the sense that most of the design-decisions are tried out before implementing the system, to ensure anticipated behaviour. The main idea of the modelling methodology is to divide the system into two model types. One model concerns the use processes of the system, which are reflected in user behaviour and application software. The other model concerns the queue structures and architecture of the system. These two models could be combined in different ways resulting in either a so called Queue-Flow Model or a Performance Prototyping Simulator. The maturity of the models will\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["48"]}
{"title": "Alignment of software product quality goals in two outsourcing relationships\n", "abstract": " Background: Issues with software product quality are commonly reported when organisations engage in outsourcing relationships. To address this issue, value-based software engineering literature emphasises the need for all success-critical stakeholder groups to work towards a mutually agreed goal. Aim: This paper presents a case study that aims to compare and contrast the priority two groups place on software product quality \u0393\u00c7\u00f6 stakeholders internal to the development organisation, and stakeholders from outsourcing relationships. Method: A model of software product quality was developed and used for this study based on ISO 9126 standard. Questionnaires were collected from 38 representatives of the two key stakeholder groups, in which each person rates the relative importance of aspects of software product quality using the hierarchical cumulative voting (HCV) technique. The results of these two groups were then analysed and compared. Results: The results show the stakeholders priorities to be a merging of the priorities from both the software development organsiation, and the firm providing the outsourced services. Further, stakeholders from outsourced relationships had greater difficulty define an ideal future balance of software product qualities. Conclusions: One of the keys to success when outsourcing is to ensure both the internal and external groups understand the needs of each other \u0393\u00c7\u00f6 and ensure they can work towards a sufficiently compatible goal. It may be necessary to change the way work is outsourced to align the goals of both firms to be compatible.", "num_citations": "7\n", "authors": ["48"]}
{"title": "Trade-off analysis of software quality attributes\n", "abstract": " Software quality is elusive. It is easy to agree that software quality is important. However, it is far from easy to determine what it actually means. To address this problem, different software quality models have been introduced with the objective to structure and systematize the concept of software quality into software quality attributes or software qualities. These models are mainly focused on software product quality and its different attributes. The product attributes could also have different stakeholders. One stakeholder is the end user who is interested in external attributes such as reliability, usability and so forth. Another group of stakeholders is the developers who may be interested in internal attributes such as maintainability and the ability to further develop the software into new versions and releases. In addition to these views on product quality, there is also a management perspective that includes quality aspects\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["48"]}
{"title": "we must keep going i guess\n", "abstract": " Increasing the understanding of effectiveness in software inspections using published data sets Since its inception into software engineering, software inspection has been viewed as a cost-effective way of increasing software quality. Despite this, many questions remain unanswered regarding, for example, ideal team size or cost effectiveness. This paper addresses some of these questions by performing an analysis using 30 published data sets from empirical experiments of software inspections. The main question is concerned with determining a suitable team size for software inspections. The effectiveness of different team sizes is also studied. Furthermore, the differences in mean effectiveness between different team sizes are investigated based on the inspection environmental context, document types and reading technique. It is concluded that it is possible to choose a suitable team size based on the effectiveness of inspections. This can be used as a tool to assist in the planning of inspections. A\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["48"]}
{"title": "A quality constraint model to be used during the test phase of the software life cycle\n", "abstract": " This paper deals with the problem of when to stop the test phase in the software lifecycle, in order to get an efficient, reliable, manageable and maintainable software product. A quality constraint model is introduced and formulas for the means and variances of the number of remaining errors at a specific time and the time when a specific number of errors are remaining are derived. A natural approach to the use of the model and results for some different failure time distributions are presented.", "num_citations": "7\n", "authors": ["48"]}
{"title": "An empirical study of an ER-model inspection meeting\n", "abstract": " A great benefit of software inspections is that they can be applied at almost any stage of the software development life cycle. We document a large-scale experiment conducted during an entity relationship (ER) model inspection meeting. The experiment was aimed at finding empirically validated answers to the question \"which reading technique has a more efficient detection rate when searching for defects in an ER model\". Secondly, the effect of the usage of roles in a team meeting was also explored. Finally, we investigate the reviewers' ability to find defects belonging to certain defect categories. The findings showed that the participants using a checklist had a significantly higher detection rate than the ad hoc groups. Overall, the groups using roles had a lower performance than those without roles. Furthermore, the findings showed that when comparing the groups using roles to those without roles, the proportion\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["48"]}
{"title": "Agile Processes in Software Engineering and Extreme Programming: 13th International Conference, XP 2012, Malm\u251c\u2562, Sweden, May 21-25, 2012, Proceedings\n", "abstract": " This book contains the refereed proceedings of the 13th International Conference on Agile Software Development, XP 2012, held in Malm\u251c\u2562, Sweden, in May 2012. In the last decade, we have seen agile and lean software development strongly influence the way software is developed. Agile and lean software development has moved from being a way of working for a number of pioneers to becoming, more or less, the expected way of developing software in industry. The topics covered by the selected full papers include general aspects of agility, agile teams, studies related to the release and maintenance of software, and research on specific practices in agile and lean software development. They are complemented by four short papers capturing additional aspects of agile and lean projects.", "num_citations": "5\n", "authors": ["48"]}
{"title": "Systematic reviews in evidence-based software technology and software engineering\n", "abstract": " It is a pleasure to inform the authors and readers of Information and Software Technology that the journal now is welcoming a new type of contribution\u0393\u00c7\u00f6in addition to regular research papers\u0393\u00c7\u00f6in the form of systematic reviews. These are an integral part of an evidence-based approach to software technology and software engineering. The objective of systematic reviews is to collate available evidence regarding a specific technique or method in software development. This makes systematic reviews different from traditional literature reviews, where the objective mostly is to summarise the literature without any real synthesis of the findings. Systematic reviews on the other hand are intended to collect results/evidence from different sources and systematically analyse the findings in an objective and repeatable fashion so that the current best knowledge is presented.Systematic reviews are intended to be used as a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["48"]}
{"title": "The role of empirical studies in process improvement\n", "abstract": " Empirical studies can play a multitude of roles in what is loosely called process improvement. In this paper we examine what is meant in the software industry by process improvement and how we can and should be able to use empirical studies to improve software processes. This paper evolved from discussions at the Empirical Studies in Software Development and Evolution Workshop at ICSE99.", "num_citations": "5\n", "authors": ["48"]}
{"title": "Reusable simulation models for performance analysis of intelligent networks\n", "abstract": " New services will be introduced at a much faster rate than today when Intelligent Networks are employed. It is important to control the process of introducing services and be able to forecast the consequences of a new service. This paper describes a method for performance simulation based on reusable simulation models which can be used for this purpose. The method builds on dividing the simulation model into three parts describing architecture, software of services and usage of the services respectively. Services and nodes can be described on different abstraction levels. If a new service is introduced all the old parts of the simulation program can be reused.", "num_citations": "5\n", "authors": ["48"]}
{"title": "Re-Certification of Software Reliability without Re-Testing\n", "abstract": " Usage testing or operational profile testing is depicted as an important test technique to remove the most critical faults from an operational perspective. The technique also allows for determination of the software reliability through application of software reliability models. This is, of course, beneficial, but a problem arises as the usage changes and this can be expected as new services are added to the existing system or the behaviour of the users change due to some reason. Therefore, a method to re-certify software reliability without re-testing the software for all potential changes that may occur is needed. This paper outlines such a procedure based on fault content estimations and by recording a number of measures during usage testing. The procedure gives a basis for determining whether more testing or other means to remove faults is needed prior to the usage changes or if the reliability requirements\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["48"]}
{"title": "A Specialized Global Software Engineering Taxonomy for Effort Estimation\n", "abstract": " To facilitate the sharing and combination of knowledge by Global Software Engineering (GSE) researchers and practitioners, the need for a common terminology and knowledge classification scheme has been identified, and as a consequence, a taxonomy and an extension were proposed. In addition, one systematic literature review and a survey on respectively the state of the art and practice of effort estimation in GSE were conducted, showing that despite its importance in practice, the GSE effort estimation literature is rare and reported in an ad-hoc way. Therefore, this paper proposes a specialized GSE taxonomy for effort estimation, which was built on the recently proposed general GSE taxonomy (including the extension) and was also based on the findings from two empirical studies and expert knowledge. The specialized taxonomy was validated using data from eight finished GSE projects. Our effort\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["48"]}
{"title": "Software product quality in global software development: Finding groups with aligned goals\n", "abstract": " The development of a software product in an organization involves various groups of stakeholders who may prioritize the qualities of the product differently. This paper presents an empirical study of 65 individuals in different roles and in different locations, including on shoring, outsourcing and off shoring, prioritizing 24 software quality aspects. Hierarchical cluster analysis is applied to the prioritization data, separately for the situation today and the ideal situation, and the composition of the clusters, regarding the distribution of the inherent groupings within each of them, is analyzed. The analysis results in observing that the roles are not that important in the clustering. However, compositions of clusters regarding the onshore-offshore relationships are significantly different, showing that the offshore participants have stronger tendency to cluster together. In conclusion, stakeholders seem to form clusters of aligned\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["48"]}
{"title": "Understanding the Importance of Roles in Architecture-Related Process Improvement\u0393\u00c7\u00f4A Case Study\n", "abstract": " In response to the increasingly challenging task of developing software, many companies turn to Software Process Improvement (SPI). One of many factors that SPI depends on is user (staff) involvement, which is complicated by the fact that process users may differ in viewpoints and priorities. In this paper, we present a case study in which we performed a pre-SPI examination of process users\u0393\u00c7\u00d6 viewpoints and priorities with respect to their roles. The study was conducted by the means of a questionnaire sent out to the process users. The analysis reveals differences among roles regarding priorities, in particular for product managers and designers, but not regarding viewpoints. This indicates that further research should investigate in which situations roles are likely to differ and in which they are likely to be similar. Moreover, since we initially expected both viewpoints and priorities to differ, it indicates that it is\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["48"]}
{"title": "Monitoring fault classification agreement in an industrial context\n", "abstract": " Based on prior investigations and the request from a collaborative research partner, UIQ Technology, an investigation to develop an improved and more informative fault classification scheme was launched. The study investigates the level of agreement, a prerequisite for using a fault classification, between classifiers in an industrial setting.The method used is an experimental approach performed in an industrial setting for determining the agreement among classifiers facilitating for example Kappa statistics for determining the agreement. From the study it is concluded that the agreement within the industrial setting is higher than obtained in a previous study within an academic setting, but it is still in need of improvement.", "num_citations": "4\n", "authors": ["48"]}
{"title": "Simple Is Better? - An Experiment on Requirements Prioritisation\n", "abstract": " The process of selecting the right set of requirements for a product release is highly dependent on how well we succeed in prioritising the requirements candidates. There are different techniques available for requirements prioritisation, some more elaborate than others. In order to compare different techniques, a controlled experiment was conducted with the objective of understanding differences regarding time consumption, ease of use, and accuracy. The requirements prioritisation techniques compared in the experiment are the Analytical Hierarchy Process (AHP) and a variation of the Planning Game (PG), isolated from Extreme Programming. The subjects were 15 Ph. D. students and one professor, who prioritised mobile phone features using both methods. It was found that the straightforward and intuitive PG was less time consuming, and considered by the subjects as easier to use, and more accurate than AHP.", "num_citations": "4\n", "authors": ["48"]}
{"title": "Experimentation with usage-based reading\n", "abstract": " Software inspections are regarded as an important technique to detect faults throughout the software development process. The individual preparation phase of software inspections has enlarged its focus from only comprehension to also include fault searching. Hence, reading techniques to support the reviewers on fault detection are needed. Usage-based reading (UBR) is a reading technique, which focuses on the important parts from a user\u0393\u00c7\u00d6s point of view in a software document by using prioritized use cases. UBR has been evaluated in two previously conducted experiments, which investigate the prioritization of UBR and compare UBR against checklist-based reading (CBR). This chapter presents two controlled experiments with UBR on requirements and design specifications. The experiments include individual preparation and inspection meeting, i.e. the first steps of the traditional inspection process\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["48"]}
{"title": "A Statistical approach to feature interaction\n", "abstract": " This paper presents the concept of a new, statistical approach to the feature interaction problem. The approach is based on the ideas originating from Statistical Usage Testing, and aims at rationalizing the process of feature interaction detection during service creation by utilizing the statistical properties of system usage. The most probable combinations of service features should receive the most attention, whereas the combinations whose occurrence probabilities are extremely low or zero may not be considered. In this sense, the proposed statistical approach is an added value to other feature interaction detection methods. By taking advantage of the statistical distributions of the user\u0393\u00c7\u00d6s behaviour, it is also possible to provide for efficient verification of service software and certification of service quality from the feature interaction point of view.The paper discusses general aspects of the feature interaction problem. The principles of statistical testing and statistical usage modelling of services are outlined. The process of creating the usage specifications of telecom services is presented, and possible applications of the usage specification in feature interaction detection, service software verification, and service quality certification are discussed.", "num_citations": "4\n", "authors": ["48"]}
{"title": "Performance analysis of SDL systems from SDL descriptions\n", "abstract": " This paper gives a brief introduction to a methodology for early performance analysis based on SDL descriptions and models of the hardware architecture and the behaviour of the users. It is described how the SDL descriptions can be transformed automatically into descriptions capturing the original behaviour as well as the performance. In particular, the benefits and the opportunities with the methodology are presented by an example.", "num_citations": "4\n", "authors": ["48"]}
{"title": "Case Study Research in Software Engineering\u0393\u00c7\u00f6It is a Case, and it is a Study, but is it a Case Study?\n", "abstract": " Background:Case studies are regularly published in the software engineering literature, and guidelines for conducting case studies are available. Based on a perception that the label \u0393\u00c7\u00a3case study\u0393\u00c7\u00a5 is assigned to studies that are not case studies, an investigation has been conducted.Objective:The aim was to investigate whether or not the label \u0393\u00c7\u00a3case study\u0393\u00c7\u00a5 is correctly used in software engineering research.Method:To address the objective, 100 recent articles found through Scopus when searching for case studies in software engineering have been investigated and classified.Results:Unfortunately, the perception of misuse of the label \u0393\u00c7\u00a3case study\u0393\u00c7\u00a5 is correct. Close to 50% of the articles investigated were judged as not being case studies according to the definition of a case study.Conclusions:We either need to ensure correct use of the label \u0393\u00c7\u00a3case study\u0393\u00c7\u00a5, or we need another label for its definition. Given that \u0393\u00c7\u00a3case study\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["48"]}
{"title": "Experiences of fault data in a large software system\n", "abstract": " Early identification of fault-prone modules is desirable both from developer and customer perspectives because it supports planning and scheduling activities that facilitate cost avoidance and improved time to market. Large-scale software systems are rarely built from scratch, and usually involve modification and enhancement of existing systems. This suggests that development planning and software quality could greatly be enhanced, because knowledge about product complexity and quality of previous releases can be taken into account when making improvements in subsequent projects. In this article we present results from empirical studies at Ericsson Telecom AB that examine the use of metrics to predict fault-prone modules in successive product releases. The results show that such prediction appears to be possible and has potential to enhance project maintenance.", "num_citations": "3\n", "authors": ["48"]}
{"title": "Interval Estimation in Software Reliability Analysis\n", "abstract": " Software reliability is traditionally estimated by analyzing failure data collected during software testing. There are many software reliability models available, but the estimation of model parameters usually requires a large number of failure data which might not be available. Hence the estimated parameters are not accurate and frequent revisions are needed as more failure data become available. In this paper, we study the use of interval estimation in software reliability prediction. For a commonly used software reliability model, we present the interval estimates of the parameters and their uses for a better planning during reliability testing. Con\u2229\u00bc\u00fcdence limits for the reliability and failure intensity function are presented. The results are useful, for example, in the detennination of software release time which is a difficult problem in practice.", "num_citations": "3\n", "authors": ["48"]}
{"title": "Software reliability estimations through usage analysis of specifications and designs\n", "abstract": " This paper presents a method proposal for estimation of software reliability before the implementation phase. The method is based upon that a formal specification technique is used and that it is possible to develop a tool performing dynamic analysis, i.e., locating semantic faults in the design. The analysis is performed with both applying a usage profile as input as well as doing a full analysis, i.e., locate all faults that the tool can find. The tool must provide failure data in terms of time since the last failure was detected. The mapping of the dynamic failures to the failures encountered during statistical usage testing and operation is discussed. The method can be applied either on the software specification or as a step in the development process by applying it on the software design. The proposed method allows for software reliability estimations that can be used both as a quality indicator, and for planning and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["48"]}
{"title": "Challenges and recommendations to publishing and using credible evidence in software engineering\n", "abstract": " Context:An evidence-based scientific discipline should produce, consume and disseminate credible evidence. Unfortunately, mistakes are sometimes made, resulting in the production, consumption and dissemination of invalid or otherwise questionable evidence. In the worst cases, such questionable evidence achieves the status of accepted knowledge. There is, therefore, the need to ensure that producers and consumers seek to identify and rectify such situations.Objectives:To raise awareness of the negative impact of misinterpreting evidence and of propagating that misinterpreted evidence, and to provide guidance on how to improve on the type of issues identified.Methods:We use a case-based approach to present and analyse the production, consumption and dissemination of evidence. The cases are based on the literature and our professional experience. These cases illustrate a range of challenges\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["48"]}
{"title": "A Method for Investigating the Quality of Evolving Object\u0393\u00c7\u00c9oriented Software using Defects in Global Software Development Projects\n", "abstract": " Context: Global software development (GSD) projects can have distributed teams that work independently in different locations or team members that are dispersed. The various development settings in GSD can influence quality during product evolution. When evaluating quality using defects as a proxy, the development settings have to be taken into consideration.   Objective: The aim is to provide a systematic method for supporting investigations of the implication of GSD contexts on defect data as a proxy for quality.   Method: A method engineering approach was used to incrementally develop the proposed method. This was done through applying the method in multiple industrial contexts and then using lessons learned to refine and improve the method after application.   Results: A measurement instrument and visualization was proposed incorporating an understanding of the release history and understanding\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["48"]}
{"title": "An experience-based framework for evaluating alignment of software quality goals\n", "abstract": " Efficient quality management of software projects requires knowledge of how various groups of stakeholders involved in software development prioritize the product and project goals. Agreements or disagreements among members of a team may originate from inherent groupings, depending on various professional or other characteristics. These agreements are not easily detected by conventional practices (discussions, meetings, etc.) since the natural language expressions are often obscuring, subjective, and prone to misunderstandings. It is therefore essential to have objective tools that can measure the alignment among the members of a team; especially critical for the software development is the degree of alignment with respect to the prioritization goals of the software product. The paper proposes an experience-based framework of statistical and graphical techniques for the systematic study of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["48"]}
{"title": "Introduction to section most cited journal articles in software engineering\n", "abstract": " Editorial: Introduction to section most cited journal articles in software engineering: Information and Software Technology: Vol 50, No 1-2 ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Information and Software Technology Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsInformation and Software TechnologyVol. , No. -2Editorial: Introduction to section most cited journal articles in software engineering article Editorial: Introduction to section most cited journal articles in software engineering Share on Author: Claes Wohlin profile image Claes Wohlin View Profile Authors Info & Affiliations Publication: Information and Software Technology2008 ://\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["48"]}
{"title": "Using Game Theory to Study Bidding for Software Projects\n", "abstract": " Cost estimation is only one part in obtaining software projects. Another important aspect is pricing and the related activity of bidding for projects. In order to obtain a software project, companies may have several bidding strategies. We used game theory in a bidding study to increase the understanding of bidding behaviour. It is the first time that game theory is used to study bidding for software projects. We show that game theory is applicable to evaluate bidding for software projects. The main results from the study are that risks do not pay off and that it is hard to recover from loses.", "num_citations": "2\n", "authors": ["48"]}
{"title": "Can the personal software process be used for empirical studies\n", "abstract": " This paper discusses the use of the Personal Software Process (PSP) as a context for conducting empirical studies. It is argued that the PSP provides an interesting environment for doing empirical studies. Student experiments are frequently used to perform empirical studies to improve the understanding of software development as an engineering discipline. The PSP may be one context in which student experiments can be performed, in particular if the PSP is taught independently of the objective of doing empirical studies. In other words, the empirical study becomes a natural by-product in teaching the PSP. This has some benefits in comparison with other types of studies. The PSP provides a well-defined context and eases replication of the studies. The objective of this paper is to present the idea and discuss the opportunities in combining the PSP with empirical studies.", "num_citations": "2\n", "authors": ["48"]}
{"title": "Performance and functional prototyping from software descriptions\n", "abstract": " The use of formal description techniques, in particular standardised, are a prerequisite for transformation of the descriptions into other representations. These can be either a step in the development process or a way to analyse or dimensioning the qualities of the system at an early stage. The paper formulates a general (independent of description technique) methodology for performance analysis at an early stage. It is pointed out that as a side effect it is possible to do functional analysis in a real time model as well. The author presents some results in transforming SDL system specifications into SDL descriptions describing the original SDL system from a performance viewpoint. SDL (specification and description language) has been standardised by CCITT as a suitable description technique for telecommunication systems. The methodology is independent of description technique, and SDL has only been chosen\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["48"]}
{"title": "Some new aspects on software reliability model comparisons\n", "abstract": " Tools and techniques for achieving and demonstrating high reliability are needed. One such is software reliability models. In order to make these usable in the industry, the area has to be structured. This calls for comparisons between models and a software reliability handbook based on the comparisons. The comparisons have to be done based on a number of criteria. The author adopts five criteria (predictive validity, capability, quality of assumptions, applicability and simplicity) presented by I.A. Musq et al. (1984) and complements them on some points. He also suggests two new criteria that have to be evaluated when comparing models: predictive quantities and coverage. The criteria have been structured into three separate parts of comparison to go through with a model; other models, environment and failure data.< >", "num_citations": "2\n", "authors": ["48"]}
{"title": "An Evaluation of Knowledge Translation in Software Engineering\n", "abstract": " Knowledge translation is defined, in health sciences, as \u0393\u00c7\u00a3the exchange, synthesis and ethically sound application of research results in practice\u0393\u00c7\u00a5. The objective of this paper is to implement and conduct a feasibility evaluation of a knowledge translation framework in software engineering. We evaluated the outcome of the knowledge translation framework in an industrial setting, along with the effectiveness of the interventions undertaken as part of knowledge translation in a multi-case study. The results of the evaluation suggest that the practitioners perceive the knowledge translation framework to be valuable and useful. In conclusion, this paper contributes towards the reporting of a systematic implementation of knowledge translation and evaluating its use in software engineering.", "num_citations": "1\n", "authors": ["48"]}
{"title": "Using Checklists to Support the Change Control Process\u0393\u00c7\u00f6A Case Study\n", "abstract": " Change control is crucial in a software development organisation, as changes arrive during all stages of development. Changes that are introduced in the software at the peril of individual developers may affect aspects of the software such as stability, usability, and overall quality. Typically, change proposals are handled in a change control process, where the implications and impact of the change are analysed. In order to support this type of analysis, we have created a checklist-based process support instrument, the purpose of which is to allow for objective and systematic analyses. We present an approach for adapting generic checklists along three axes: process, domain, and roles, as well as characteristics of the resulting support instrument. We also describe two evaluations of the change control process, of which the latter is designed with the stepwise character of the process in mind. We discuss some preliminary evaluation results, and in the light of these conclude that the use of checklistbased process support seems promising so far.", "num_citations": "1\n", "authors": ["48"]}
{"title": "Successful Software Project and Products: An Empirical Investigation Comparing Australia and Sweden\n", "abstract": " The success and failure of software projects have been discussed in literature for many years. Research findings do not agree with one another given different types of projects, products and cultures that have been investigated. This research examines different perceptions about what effect various factors have on software project success among different cultures, namely Australia and Sweden. Our findings indicate that there are differences of certain factors effect on software project success across various cultures. Furthermore, software developers from both countries agreed that \u0393\u00c7\u00a3satisfied customer\u0393\u00c7\u00a5 is the most important factor for software product success.", "num_citations": "1\n", "authors": ["48"]}
{"title": "Risk-based trade-off between verification and validation\u0393\u00c7\u00f4an industry-motivated study\n", "abstract": " Within industry the demand for short lead-time and reduced effort consumption is in focus. For an associated industry partner the lead-time and effort focus has meant turning the interest towards the Verification and Validation (V&V) process. The industry cooperation motivating this study aims at providing a tailored and applicable V&V process, where the order of verification and validation may be changed as well as the amount of V&V activities conducted. Through the industry cooperation as well as industrial and academic experience, a method has been formulated that address how to select a suitable V&V process depending on the functionality being developed. The method describes how a suitable process is created and selected, where the appropriate process is identified based on functionality and coupling between the system entities being developed. It is concluded that the method provides support\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["48"]}
{"title": "A Case Study Approach to Evaluation of Three Methods to Predict Project Success\n", "abstract": " To increase the likelihood for software project success, it is important to be able to identify the drivers of success. This paper compares three methods to identify similar projects with the objective to predict project success. The hypothesis is that projects with similar characteristics are likely to have the same outcome in terms of success. Two of the methods are based on identifying similar projects using all available information. The first method of these aims at identifying the most similar project. The second method identifies a group of projects as most similar. Finally, the third method pinpoints some key characteristics to identify project similarity. Our measure of success for these identifications is whether project success for these projects identified as similar is the same. The comparison between methods is done in a case study with 46 projects with varying characteristics. The paper evaluates the performance of each\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["48"]}
{"title": "An Empirical Study: Object-Based Cleanroom vs. Object-Oriented Software Engineering\n", "abstract": " Experimental Software Engineering is needed to enable transfer of research results from the universities to industrial use. It is mostly not possible to take research results and apply them in a large experiment directly. Therefore, the technology transfer must be carried out in a number of steps: minor case study within a university environment, an enlarged experiment in industry and finally into real projects as the normal method or technique to use. This paper focuses on reporting some results from the first step in the dissemination process. A comparative study between two development methods is reported. The two methods are: object-based Cleanroom and Object-Oriented Software Engineering. The study shows that the object-based Cleanroom produces higher quality software with approximately the same effort as the more widely spread method. The results are, due to the small study, not statistical significant, but they are promising for the future and they ought to encourage industry to perform a larger experiment in an industrial setting. Both quantitative and qualitative results are presented.", "num_citations": "1\n", "authors": ["48"]}
{"title": "Early estimation of software reliability through dynamic analysis\n", "abstract": " Early estimations and predictions of software quality attributes are essential to be in control of software development and to allow for delivery of software products which fulfil the requirements put on them. This paper focuses on a method enabling estimation and prediction of software reliability from the specification and design documents. The method is based on dynamic analysis of a well-defined high level description technique, and by applying usage-oriented analysis, it is illustrated, through a case study, how the reliability can be controlled. Furthermore, it is described how the output from the analysis can be used as an acceptance criterion of the design, as support in the planning process for the test phases to come and finally as a method to enable estimation and prediction of the reliability in the testing phase and operational phase. The method is still being evaluated and improved, but it can be\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["48"]}
{"title": "Identification of Failure\u0393\u00c7\u00f4Prone Modules in Two Software System Releases\n", "abstract": " This paper presents a case study of fault and failure data from two consecutive releases of a large telecommunication system. In this context it is important to have clear interpretations of errors, faults and failures. Thus, we would like to make the following distinction between them. Errors are made by humans, which may result in faults in the software. The faults may manifest themselves as failures during operation. Thus, faults can be interpreted as defects in the software and failures are the actual malfunction in an operational environment. In this paper we have used fault\u0393\u00c7\u00f4prone modules to denote the modules that account for the highest number of faults disclosed during testing, while failure\u0393\u00c7\u00f4prone modules is used to denote the modules accounting for the highest number of faults disclosed during the first office application and in operation. The general objective of the study is to investigate methods of identifying failure\u0393\u00c7\u00f4prone software modules. Furthermore, the goal is to use the knowledge acquired to improve the software development process in order to improve software quality in the future.Some early results using parametric statistics have been reported in (Ohlsson and Alberg, 1996). The models have since been refined and analysed with non\u0393\u00c7\u00f4parametric statistics (Ohlsson et al., 1996). Identification of fault\u0393\u00c7\u00f4prone modules has also been addressed by other researchers (Khoshgoftaar and Kalaichelvan, 1995) and (Munson and Khoshgoftaar, 1992). Few, if any, studies have exploited the opportunities to identify not only fault\u0393\u00c7\u00f4prone modules, but also failure\u0393\u00c7\u00f4prone modules which are the main concern of the user. There is also a general lack of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["48"]}
{"title": "Improving through an Incremental Approach\n", "abstract": " A major potential problem in software development is the contradiction between on-time delivery and fulfilment of reliability requirements of the software. Therefore, improvements are always needed to support on-time delivery without sacrificing the reliability requirement. Incremental development and certification allows for a better combination of on-time delivery and reliability fulfilment than other development approaches. Therefore, our research is very much focused on the incremental approach, which includes both improving the specification technique for increments and certification of increments. The main reason for focusing on Cleanroom from an incremental viewpoint is the conviction that an incremental approach is superior when it comes to achieving the right reliability in the shortest possible time. To illustrate this conviction a method to determine a suitable order of development and certification of increments is presented before giving an introduction to research areas pursued by the software engineering research group at the department of Communication Systems.", "num_citations": "1\n", "authors": ["48"]}
{"title": "Engineering reliable software\n", "abstract": " Software reliability engineering is not only the use of software reliability models and similar techniques, it is the use of sensible engineering principles with cost/benefit analysis throughout the software life cycle to obtain reliable software. The need to have a comprehensive view on software development to engineer reliable software is emphasized. Cleanroom Software Engineering is proposed as being the basis for developing reliable software. The paper in particular discusses some extensions to Cleanroom, both in terms of adaptations and additions. Particular emphasis is on high-level design techniques and methods for reliability certification of the software. The comprehensive view of software is supported by several success stores, both with references to results presented in literature as well as experiences from projects conducted by Q-Labs. The results obtained are encouraging. The methods proposed are\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["48"]}
{"title": "Lead times and soft factors\n", "abstract": " This paper presents a study of the influence of and the relationship between lead times and soft factors in large software development projects. Soft factors are used as a collective term for factors that are hard to quantify exactly, ie non-technical aspects. Data has been collected for 12 projects. The lead times have been recorded together with a grade in the range 1-5 for 10 soft factors. The grading is based on a subjectively made judgement from project reports and by talking to the project managers etc. The ten soft factors are then divided into three groups based on their identified influence on the relationship lead time/log (man-hours). The division of the factors into three groups is the basis for proposing a scheme for calculating a soft factor goodness figure for each project. The proposal is based on 4 of the projects. The goodness figure is then evaluated on the other 8 projects. The projects are divided into 3\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["48"]}
{"title": "Technical Report: papers found in the two updated SLRs\n", "abstract": " 83. Schultis, K.-B., Elsner, C., Lohmann, D., 2014. Architecture challenges for internal software ecosystems: A large-scale industry case study. In: Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering. FSE 2014. ACM, New York, NY, USA, pp. 542\u0393\u00c7\u00f4552.", "num_citations": "1\n", "authors": ["48"]}