{"title": "Optimization: A comparative study of genetic and tabu search algorithms\n", "abstract": " Examination timetabling problem like all scheduling problems are NP-hard problems in which the complexity and time needed to solve the problem increase with the problem size. This paper aims to compare Genetic Algorithm and Tabu Search approaches to solve this kind of problem. Both algorithms were tested with regard to the quality of generated timetables and the speed with which the timetables are generated using collected test data. The test shows that though both algorithms are capable of handling the examination timetabling problem, the Tabu Search approach can produce better timetables than Genetic Algorithm, even at a greater speed.", "num_citations": "35\n", "authors": ["2005"]}
{"title": "Software defect prediction using ensemble learning: an ANP based evaluation method\n", "abstract": " Software defect prediction (SDP) is the process of predicting defects in software modules, it identifies the modules that are defective and require extensive testing. Classification algorithms that help to predict software defects play a major role in software engineering process. Some studies have depicted that the use of ensembles is often more accurate than using single classifiers. However, variations exist from studies, which posited that the efficiency of learning algorithms might vary using different performance measures. This is because most studies on SDP consider the accuracy of the model or classifier above other performance metrics. This paper evaluated the performance of single classifiers (SMO, MLP, kNN and Decision Tree) and ensembles (Bagging, Boosting, Stacking and Voting) in SDP considering major performance metrics using Analytic Network Process (ANP) multi-criteria decision method. The experiment was based on 11 performance metrics over 11 software defect datasets. Boosted SMO, Voting and Stacking Ensemble methods ranked highest with a priority level of 0.0493, 0.0493 and 0.0445 respectively. Decision tree ranked highest in single classifiers with 0.0410. These clearly show that ensemble methods can give better classification results in SDP and Boosting method gave the best result. In essence, it is valid to say that before deciding which model or classifier is better for software defect prediction, all performance metrics should be considered.", "num_citations": "27\n", "authors": ["2005"]}
{"title": "A PROMETHEE based evaluation of software defect predictors\n", "abstract": " A software defect is an error, flaw, mistake, or fault in a computer program or system that produces incorrect or unexpected results and the process of locating defective modules in software is software defect prediction. Defect prediction in software improves quality and testing efficiency by constructing predictive stand-alone classifier models or by the use of ensembles methods to identify fault-prone modules. Selection of the appropriate set of single classifier models or ensemble methods for the software defect prediction over the years has shown inconsistent results. In previous analysis, inconsistencies exist and the performance of learning algorithms varies using different performance measures. Therefore, there is need for more research in this field to evaluate the performance of single classifiers and ensemble algorithms in software defect prediction. This study assesses the quality of the ensemble methods alongside single classifier models in the software defect prediction using Preference Ranking Organization Method for Enrichment Evaluation (PROMETHEE), a multi criteria decision making (MCDM) approach. Using PROMETHEE, the performance of some popular ensemble methods based on 11 performance metrics over 10 public-domain software defect datasets from the NASA Metric Data Program (MDP) repository was evaluated. Noise is removed from the dataset by performing attribute selection. The classifiers and ensemble methods are applied on each dataset; Adaboostgave the best results. Boosted PART comes first followed by Na\u00efve Bayes and then Bagged PART as the best models for mining of datasets.", "num_citations": "17\n", "authors": ["2005"]}
{"title": "Software Defect Prediction: Effect of feature selection and ensemble methods\n", "abstract": " Software defect prediction is the process of locating defective modules in software. It facilitates testing efficiency and consequently software quality. It enables a timely identification of fault-prone modules. The use of single classifiers and ensembles for predicting defects in software has been met with inconsistent results. Previous analysis say ensemble are often more accurate and are less affected by noise in datasets, also achieving lower average error rates than any of the constituent classifiers. However, inconsistencies exist in these various experiments and the performance of learning algorithms may vary using different performance measures and under different circumstances. Therefore, more research is needed to evaluate the performance of ensemble algorithms in software defect prediction. Adding feature selection reduces data sets with fewer features and improves the classifiers and ensemble performance over the datasets. The goal of this paper is to assess the efficiency of ensemble methods in software defect prediction using feature selection. This study compares the performance of four ensemble algorithms using 11 different performance metrics over 11 software defect datasets from the NASA MDP repository. The results indicate that feature selection and use of ensemble methods can improve the classification results of software defect prediction. Bagged ensemble models have the best results. In addition, Voting and Stacking also performed better than individual base classifiers. In terms of single classifier, SMO performs best as it outperformed Decision Tree (J48), MLP, and KNN with and without feature selection. Thus, it can\u00a0\u2026", "num_citations": "14\n", "authors": ["2005"]}
{"title": "SMOTE-Based Homogeneous Ensemble Methods for Software Defect Prediction\n", "abstract": " Class imbalance is a prevalent problem in machine learning which affects the prediction performance of classification algorithms. Software Defect Prediction (SDP) is no exception to this latent problem. Solutions such as data sampling and ensemble methods have been proposed to address the class imbalance problem in SDP. This study proposes a combination of Synthetic Minority Oversampling Technique (SMOTE) and homogeneous ensemble (Bagging and Boosting) methods for predicting software defects. The proposed approach was implemented using Decision Tree (DT) and Bayesian Network (BN) as base classifiers on defects datasets acquired from NASA software corpus. The experimental results showed that the proposed approach outperformed other experimental methods. High accuracy of 86.8% and area under operating receiver characteristics curve value of 0.93% achieved by the\u00a0\u2026", "num_citations": "10\n", "authors": ["2005"]}
{"title": "Memetic approach for multi-objective overtime planning in software engineering projects\n", "abstract": " Software projects often suffer from unplanned overtime due to uncertainty and risk incurred due to changing requirement and attempt to meet up with time-to-market of the software product. This causes stress to developers and can result in poor quality. This paper presents a memetic algorithmic approach for solving the overtime-planning problem in software development projects. The problem is formulated as a three-objective optimization problem aimed at minimizing overtime hours, project makespan and cost. The formulation captures the dynamics of error generation and propagation due to overtime using simulation. Multi-Objective Shuffled Frog-Leaping Algorithm (MOSFLA) specifically designed for overtime planning is applied to solve the formulated problem. Empirical evaluation experiments on six real-life software project datasets were carried out using three widely used multi-objective quality indicators. Results showed that MOSFLA significantly outperformed the existing traditional overtime management strategies in software engineering projects in all quality indicators with 0.0118, 0.3893 and 0.0102 values for Contribution (IC), Hypervolume (IHV) and Generational Distance (IGD) respectively. The proposed approach also produced significantly better IHV and IGD results than the state of the art approach (NSGA-IIV ) in 100% of the project instances. However, the approach could only outperform NSGA-IIV in approximately 67% of projects instances with respect to IC.", "num_citations": "10\n", "authors": ["2005"]}
{"title": "Impact of feature selection on classification via clustering techniques in software defect prediction\n", "abstract": " Software testing using software defect prediction aims to detect as many defects as possible in software before the software release. This plays an important role in ensuring quality and reliability. Software defect prediction can be modeled as a classification problem that classifies software modules into two classes: defective and non-defective; and classification algorithms are used for this process. This study investigated the impact of feature selection methods on classification via clustering techniques for software defect prediction. Three clustering techniques were selected; Farthest First Clusterer, K-Means and Make-Density Clusterer, and three feature selection methods: Chi-Square, Clustering Variation, and Information Gain were used on software defect datasets from NASA repository. The best software defect prediction model was farthest-first using information gain feature selection method with an accuracy of 78.69%, precision value of 0.804 and recall value of 0.788. The experimental results showed that the use of clustering techniques as a classifier gave a good predictive performance and feature selection methods further enhanced their performance. This indicates that classification via clustering techniques can give competitive results against standard classification methods with the advantage of not having to train any model using labeled dataset; as it can be used on the unlabeled datasets.", "num_citations": "9\n", "authors": ["2005"]}
{"title": "Software defect prediction: A multi-criteria decision-making approach\n", "abstract": " Failure of software systems as a result of software testing is very much rampant as modern software systems are large and complex. Software testing which is an integral part of the software development life cycle (SDLC), consumes both human and capital resources. As such, software defect prediction (SDP) mechanisms are deployed to strengthen the software testing phase in SDLC by predicting defect prone modules or components in software systems. Machine learning models are used for developing the SDP models with great successes achieved. Moreover, some studies have highlighted that a combination of machine learning models as a form of an ensemble is better than single SDP models in terms of prediction accuracy. However, the efficiency of machine learning models can change with diverse predictive evaluation metrics. Thus, more studies are needed to establish the effectiveness of ensemble SDP models over single SDP models. This study proposes the deployment of Multi-Criteria Decision Method (MCDM) techniques to rank machine learning models. Analytic Network Process (ANP) and Preference Ranking Organization Method for Enrichment Evaluation (PROMETHEE) which are types of MCDM techniques are deployed on 9 machine learning models with 11 performance evaluation metrics and 11 software defects datasets. The experimental results showed that ensemble SDP models are best appropriate SDP models as Boosted SMO and Boosted PART ranked highest for each of the MCDM techniques. Besides, the experimental results also validated the stand of not considering accuracy as the only performance\u00a0\u2026", "num_citations": "8\n", "authors": ["2005"]}
{"title": "An assessment of ICT literacy among secondary school students in a rural area of Kwara State, Nigeria: A community advocacy approach\n", "abstract": " In recent times, public schools in Nigeria have enjoyed some benefits in terms of deployment of Information and Communication Technologies (ICTs), but no constant attention and continuous interest is paid to fill the digital gap between schools in the rural and urban areas. The contribution of private sectors in the education system has elevated the use of ICT in both private and public schools especially in the urban areas of Nigeria. However, schools in rural areas have not benefitted much in this area. This research used community advocacy program referred to as COBES (Community Based Experience Scheme) to assess ICT literacy of secondary school students in a rural area of Kwara State, Nigeria. The study employed mixed research approach that combined both quantitative and qualitative data collection strategies. The initial findings of the study revealed low level of ICT skills among secondary school students in the rural area. Although, majority of the students who served as the respondents claimed they have computer teacher and can operate computer systems, yet, the study showed that there is dearth of ICT facilities for hands-on training. Nevertheless, through the one week long COBES program, the findings from three focus group discussion conducted at the end of the COBES program showed that students\u201f interest to use ICT increased and majority of them expressed their willingness to continue interacting with computer and internet facilities. Findings further revealed that the main reason for low ICT skills is the lack of ICT facilities for teaching and learning. The study recommended that ICT project implementation should be\u00a0\u2026", "num_citations": "7\n", "authors": ["2005"]}
{"title": "Data sampling-based feature selection framework for software defect prediction\n", "abstract": " High dimensionality and class imbalance are latent data quality problems that have a negative effect on the predictive capabilities of prediction models in software defect prediction (SDP). As a viable solution, data sampling and feature selection (FS) has been used to address the class imbalance and high dimensionality problem respectively. Most of the existing studies in SDP addressed these data quality problem individually which often affects the generalizability of such studies. Hence, this study proposed a novel framework based on correlation-based feature selection (CFS) and synthetic minority oversampling (SMOTE) methods for software defect prediction. CFS based on best-first search (BFS) method is used to handle feature selection while SMOTE sampling technique is used for the class imbalance. The proposed framework was developed with Bayesian Networks (BN) and Decision Tree (DT\u00a0\u2026", "num_citations": "4\n", "authors": ["2005"]}
{"title": "Performance Analysis of Particle Swarm Optimization for Feature Selection.\n", "abstract": " One of the key task in data mining is the selection of relevant features from datasets with high dimensionality. This is expected to reduce the time and space complexity, and consequently improve the performance of data mining algorithms for tasks such as classification. This study presents an empirical study of the effect of particle swarm optimization as a feature selection technique on the performance of classification algorithms. Two dataset from different domains were used: SMS spam detection and sentiment analysis datasets. Particle swarm optimization is applied on the datasets for feature selection. Both the reduced and raw dataset are separately classified using C4.5 decision tree, k-nearest neighbour and support vector machine. The result of the analysis showed that the improvement of classifier performance is case-dependent; some significant improvements are noticed in the sentiment analysis datasets and not in the SMS spam dataset. Although some marginal effect are observed on performance, it implies that with particle swarm optimization features selection the space complexity is reduced while maintaining the accuracy of the classifiers.", "num_citations": "4\n", "authors": ["2005"]}
{"title": "Solving the Next Release Problem using a Hybrid Metaheuristic\n", "abstract": " The Next Release Problem is characterized by the need to determine the features that are to be included in a particular software system to make up the next release. These features are to be selected, such that users\u2019 demands and needs are satisfied as much as possible, given a limited resources, by ensuring that the available resources are used to develop the most important features first. This work applies a hybrid of Variable Neighbourhood Search (VNS) and Tabu Search (TS) for solving bi-objective NRP, using a cost-value model for requirements. Experiments showed the hybrid metaheuristics to produce a Pareto optimal set with a controllable dynamic number of options whose score and cost value range can be controlled via parameters that can be modified without a significant effect on execution time.", "num_citations": "2\n", "authors": ["2005"]}
{"title": "A Novel Metric For Measuring The Readability of Software Source Code\n", "abstract": " Software metrics gives developers and their client and users of Software information about the quality of their Software products. They are employed in taking decisions at various levels of Software Life Cycle (SLC). This paper presents a novel metric for measuring the readability of Software Source Code (SC). Maintenance changes (addition, removal or modification), though, are initially carried out on the Software design artifacts, they are eventually performed on the Software SC as the finished product. Thus this metric is relevant in estimating the readability quality of SCs. The metric is ratio scaled and size independent. Results showed that the metric objectively measures the readability of SC.", "num_citations": "2\n", "authors": ["2005"]}
{"title": "Heterogeneous Ensemble with Combined Dimensionality Reduction for Social Spam Detection.\n", "abstract": " Spamming is one of the challenging problems within social networks which involves spreading malicious or scam content on a network; this often leads to a huge loss in the value of real-time social network services, compromise the user and system reputation and jeopardize users trust in the system. Existing methods in spam detection still suffer from misclassification caused by redundant and irrelevant features in the dataset as a result of high dimensionality. This study presents a novel framework based on a heterogeneous ensemble method and a hybrid dimensionality reduction technique for spam detection in micro-blogging social networks. A hybrid of Information Gain (IG) and Principal Component Analysis (PCA)(dimensionality reduction) was implemented for the selection of important features and a heterogeneous ensemble consisting of Na\u00efve Bayes (NB), K Nearest Neighbor (KNN), Logistic Regression (LR) and Repeated Incremental Pruning to Produce Error Reduction (RIPPER) classifiers based on Average of Probabilities (AOP) was used for spam detection. To empirically investigate its performance, the proposed framework was applied on MPI_SWS and SAC\u201913 Tip spam datasets and the developed models were evaluated based on accuracy, precision, recall, f-measure, and area under the curve (AUC). From the experimental results, the proposed framework (Ensemble+ IG+ PCA) outperformed other experimented methods on studied spam datasets. Specifically, the proposed framework had an average accuracy value of 87.5%, an average precision score of 0.877, an average recall value of 0.845, an average F-measure value of\u00a0\u2026", "num_citations": "1\n", "authors": ["2005"]}
{"title": "Improving the Accuracy of Static Source Code Based Software Change Impact Analysis Through Hybrid Techniques: A Review\n", "abstract": " Change is an inevitable phenomenon of life. This inevitability of change in the real world has made a software change an indispensable characteristic of software systems and a fundamental task of software maintenance and evolution. The continuous evolution process of software systems can greatly affect the systems\u2019 quality and reliability if proper mechanisms to manage them are not adequately provided. Therefore, there is a need for automated techniques to effectively make an assessment of proposed software changes that may arise due to bug fixes, technological advancements, changing user requirements etc., before their implementation. Software Change Impact Analysis (CIA) is an essential activity for comprehending and identifying potential change impacts of software changes that can help prevent the system from entering into an erroneous state. Despite the emergence of different CIA techniques, they are yet to reach an optimal level of accuracy desired by software engineers. Consequently, researchers in recent years have come up with hybrid CIA techniques which are a blend of multiple CIA approaches, as a way of improving the accuracy of change impacts analysis techniques. This study presents these hybrid CIA techniques and how they improve accuracy. They are also compared and areas for further research are identified.", "num_citations": "1\n", "authors": ["2005"]}
{"title": "Performance Evaluation of Optimised Backpropagation Algorithms for Yor\u00f9b\u00e1 Character Feature Extraction and Recognition\n", "abstract": " Character recognition has been an important area of research in the last few decades. It is basically divided into two major types namely online and offline (handwritten) character recognitions. Characters with tonal marks (diacritics) such as Yor\u00f9b\u00e1 characters (orthography) had posed more challenges than their counterparts with no tonal marks and as a result require some optimization methods to improve the recognition rate and reduce the error rate. This study evaluated the performance of four optimized backpropagation algorithms, Levenberg-Marquardt, Quasi-Newton BFGS, Resilient Propagation and Scaled Conjugate Gradient, on Yor\u00f9b\u00e1 character recognition. The method used in this study involves the five basic stages of image processing namely; image acquisition, image preprocessing, segmentation, feature extraction and classification. The performances of the algorithms were experimentally measured using mean squared error (MSE), epochs, accuracy and response time. From the experiments, it was observed that the Levenberg-Marquardt training algorithm has the best accuracy of 98.8%; Resilient Propagation and Scaled Conjugate Gradient are the fastest to converge with an average response time of 2 seconds. The results obtained can serve as a fundamental guideline in adopting the most relevant training algorithm for character image recognition.", "num_citations": "1\n", "authors": ["2005"]}
{"title": "EVALUATION OF AN OPTICAL CHARACTER RECOGNITION MODEL FOR YORUBA TEXT.\n", "abstract": " The optical character recognition (OCR) for different languages has been developed and in use with diverse applications over the years. The development of OCR enables the digitization of paper document that would have been neglected over a period of time as well as serving as a form of backup for those documents. The system proposed is for isolated characters of Yoruba language. Yoruba language is a tonal language that carries accent on the vowel alphabets. The process used involves image gray scal, binarization, de-skew, and segmentation. Thus, the OCR enable the system read the images and convert them to text data. The proposed model was evaluated using the information retrieval metrics: Precision and Recall. Results showed a significant performance with a recall of 100% in the sample document used, and precision results that varies between 76%, 97%, and 100% in the sample document.", "num_citations": "1\n", "authors": ["2005"]}
{"title": "Investigating Self-Regulation Property of Evolving Open Source Systems: An Empirical Study\n", "abstract": " The research work investigates the validity of Lehman\u2019s Law of Self-Regulation on Open Source Systems (OSS). Three metrics namely incremental difference in function statement, function count and non-comment non-blank lines of code were used to capture self-regulation property in evolving systems. The goal is to verify if the Law of Self-regulation is valid in the context of OSS using the aforementioned metrics. Empirical analysis was performed using historical data collected on 354 versions of four evolving OSS. Results show that the considered Lehman\u2019s law is valid for the collected data and the metrics. In particular, the trend of incremental difference of the metrics oscillates around their average incremental difference as they evolve. The oscillations are a reflection of positive and negative feedbacks. The oscillations also depict the balance desired changes and the resultant constraints. Evolution of OSS is a self-regulating process.", "num_citations": "1\n", "authors": ["2005"]}
{"title": "Performance Evaluation Of Select Data Mining Software Tools For Data Clustering\n", "abstract": " Data mining is used to discover knowledge from information system. Clustering is one of the techniques used for data mining. It can be defined as a technique of grouping un-labelled data objects such that objects belonging to one cluster are not similar to the objects belonging to another cluster. Data mining tools refer to the software that are used for the process of efficiently analysing, summarizing and extracting useful information from different perspectives of data. This paper presents a comparative analysis of four open-source data mining software tools (WEKA, KNIME, Tanagra and Orange) in the context of data clustering, specifically K-Means and Hierarchical clustering methods. The results of the performance analysis based on the execution time and quality of clusters showed that WEKA tool outperforms the other tools with the lowest SSE of 199.7308 with an average execution time of 1.535 seconds. Knime has SSE of 222.217 but with an average execution time of 7.13 seconds, and then Tanagra with SSE of 269.3902 and average execution time of 2.01 seconds, Orange has the poorest performance with SSE of 388.78.", "num_citations": "1\n", "authors": ["2005"]}
{"title": "Empirical Study of Continuous Change of Open Source System.\n", "abstract": " 4 Author\u2019s Address: Michael Abayomi Olatunji4, Rufus Olalere Oladele and Amos Orenyi Bajeh Department of Computer Science, University of Ilorin, Ilorin, Nigeria, PMB 1515, Ilorin, Nigeria. mikeolaus@ gmail. com, roladele@ yahoo. com, bajeh_amos@ yahoo. com\" Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than IJCIR must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.\"\u00a9 International Journal of Computing and ICT Research 2008. International Journal of Computing and ICT Research, ISSN 1818-1139 (Print), ISSN 1996-1065 (Online), Vol. 11, Issue 2, pp. 31-52, June 2017.", "num_citations": "1\n", "authors": ["2005"]}
{"title": "Internet of Robotic Things: Its Domain, Methodologies, and Applications\n", "abstract": " Abstract have not fully explored the use of IoRT. This chapter discusses the (potential) applications of IoT-aided robotics in different domains, explaining how robots can extend the capabilities of existing IoT architectures to make them more knowledgeable and smarter; discuss some of the challenges in the full realization and application of IoRT; and lastly proposes an IoRT architecture for smart library management, an area that has not received much attention in the research community.Robotics involves design, construction, operation, and use of intelligent machines that possess the ability to sense, compute, manipulate, and navigate environments to monitor events and execute an appropriate course of action. Internet of Things (IoT) on the other hand is a fast-developing novel technology consisting of group of uniquely addressable heterogeneous smart objects or tiny devices (things) interconnected via the Internet to share and process data from different sources. IoT is designed with the goal to \u201cconnect everything and everyone everywhere to everything and everyone else.\u201d The two technologies, IoT and robotics, have evolved into Internet of Robotic Things (IoRT) by the creation of a synergy between the two. IoRT aims at enhancing the current IoT with active sensing and actuation from robotics. This idea opened a novel opportunity for collaboration between IoT and robotics applications and research communities. However, most application domains of IoT and robotics", "num_citations": "1\n", "authors": ["2005"]}