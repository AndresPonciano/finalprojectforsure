{"title": "Validation methods for calibrating software effort models\n", "abstract": " COCONUT calibrates effort estimation models using an exhaustive search over the space of calibration parameters in a Cocomo I model. This technique is much simpler than other effort estimation method yet yields PRED levels comparable to those other methods. Also, it does so with less project data and fewer attributes (no scale factors). However, a comparison between COCONUT and other methods is complicated by differences in the experimental methods used for effort estimation. A review of those experimental methods concludes that software effort estimation models should be calibrated to local data using incremental holdout (not jack knife) studies, combined with randomization and hypothesis testing, repeated a statistically significant number of times.", "num_citations": "126\n", "authors": ["232"]}
{"title": "Comparative studies of the model evaluation criterions mmre and pred in software cost estimation research\n", "abstract": " Software cost model research results depend on model accuracy criteria such as MMRE and PRED. Despite criticism, MMRE has emerged as the de facto standard criterion. Many alternatives have been proposed and studied, surprisingly however PRED, the second most popular criterion, has not been extensively studied. This work attempts to fill this gap in the literature and expand the understanding and use of evaluation criterion in general. The majority of this work is empirically based, applying MMRE and PRED to a number of COCOMO model variations with respect to a simulated data set and four publicly available cost estimation data sets. We replicate a number of results based on MMRE and extend them to PRED. We study qualities of MMRE and PRED as sample estimator statistics for parameters of a cost model error distribution. Standard error is used to ensure greater confidence in replicated and new\u00a0\u2026", "num_citations": "114\n", "authors": ["232"]}
{"title": "Differential equations: theory and applications\n", "abstract": " Page 1 RED HEFFER DIFFERENTIAL EQUATIONS THEORY AND APPLICATIONS rde k2 k dr Page 2 Page 3 Page 4 Table of Laplace Transforms 1 1 a 1 eat A S S-a n! b th n! (s \u2013 a)\"+1 treat B 9n+1 1 \u0441 sin bt b 1 82 + 62 eat sin bt b \u0421 (s \u2013 a)2 + 62 S - a d cos bt S 82 + 62 eat cos bt D (s \u2013 a)? + 62 1 1 1 e sinh bt 6 eat sinh b\u1ecb b E 32 - 62 (s \u2013 a)\u2013 62 S - a f cosh bt S 82 \u2013 62 eat cosh bt F (s \u2013 a)? \u2013 62 sin bt g sin bt b 2s (82 + 62)? 262 (s2 + 62) tcos bt G b h h(t - c)f (t - c) e-scF(s) F (s + c) e-ct f (t) H i f (ct) F(9) F(cs) (1) I I j f'(t) SF (s) \u2013 f (0) F' (s) -tf (t) J F(8) k f(t) t K S [ () dt, L\u00ba F(6) des (LJ) (L9) = L (5 + 9) where (8 + 9) (e) = \"s(+1) 9 (t \u2013 t\u0131) d\u0131 Page 5 DIFFERENTIAL EQUATIONS THEORY AND APPLICATIONS 1 This one X8QG-8F4-9X1J Page 6 Page 7 DIFFERENTIAL EQUATIONS THEORY AND APPLICATIONS by Ray Redheffer University of California at Los Angeles with Dan Port JONES AND BARTLETT PUBLISHERS Page '\u2026", "num_citations": "74\n", "authors": ["232"]}
{"title": "Not all CBS are created equally: COTS-intensive project types\n", "abstract": " COTS products affect development strategies and tactics, but not all CBS development efforts are equal. Based on our experiences with 20 large government and industry CBS projects assessed during our development of the COCOTS estimation model, and our hands-on experience with 52 small e-services CBS projects within USC\u2019s graduate level software engineering course, we have identi.ed four distinct CBS activity areas: assessment intensive, tailoring intensive, glue-code intensive, and non-COTS intensive. The CBS activity type fundamentally affects the COTS related activity effort and project risks. In this work we define the three COTS activity intensive CBS types and discuss their strategic comparisons based on an empirical study of the spectrum of large and small CBS projects.", "num_citations": "71\n", "authors": ["232"]}
{"title": "Confidence in software cost estimation results based on MMRE and PRED\n", "abstract": " Bootstrapping is used to approximate the standard error and 95% confidence intervals of MMRE and PRED for a number of COCOMO I model variations applied to four PROMISE data sets. This is used to illustrate a lack of confidence in numerous published cost estimation research results based on MMRE and PRED comparisons such as model selection. We show that many such results are of questionable significance due to large possible variations resulting from population sampling error and suggest that a number of inconsistent and contradictory results may be explained by this. By using more standard statistical approaches that account for standard error, we may reduce the incidence of this and obtain greater confidence cost estimation in research results.", "num_citations": "66\n", "authors": ["232"]}
{"title": "Using simulation to investigate requirements prioritization strategies\n", "abstract": " Agile and traditional plan-based approaches to software system development both agree that prioritizing requirements is an essential activity. They differ in basic strategy - when to prioritize, to what degree, and how to guide implementation. As with many software engineering methods, verifying the benefit of following a particular approach is a challenge. Industry and student/classroom based experimental studies are generally impractical to use for large numbers of controlled experiments and benefits are difficult to measure directly. We use simulation to validate the fundamental, yet typically intangible benefits of requirements prioritization strategies. Our simulation is directly based on detailed empirical studies of agile and plan-based requirements management studies. Our simulation shows, as many have claimed, that an agile strategy excels when requirements are highly volatile, whereas a plan-based strategy\u00a0\u2026", "num_citations": "49\n", "authors": ["232"]}
{"title": "Simple software cost analysis: safe or unsafe?\n", "abstract": " Delta estimation uses changes to old projects to estimate new projects. Delta estimation assumes that new costs can be extrapolated from old projects. In this study, we show that in certain real-world data sets. there exists attributes where this assumption does not hold. We define here an automatic method to find which attributes can be safely used for delta estimation.", "num_citations": "46\n", "authors": ["232"]}
{"title": "Composable process elements for developing COTS-based applications\n", "abstract": " Data collected from five years of developing e-service applications at USC-CSE reveals that an increasing fraction have been commercial-off-the-shelf (COTS)-based applications (CBA) projects: from 28% in 1997 to 60% in 2001. Data from both small and large CBA projects show that CBA effort is primarily distributed among the three activities of COTS assessments, COTS tailoring, and glue code development and integration, with wide variations in their distribution across projects. We have developed a set of data-motivated composable process elements, in terms of these three activities, for developing CBA's as well an overall decision framework for applying the process elements. We present data regarding the movement towards CBA's and effort distribution among them; we then proceed to describe the decision framework and to present a real-world example showing how it operates within the WinWin Spiral\u00a0\u2026", "num_citations": "37\n", "authors": ["232"]}
{"title": "Assessing COTS assessment: How much is enough?\n", "abstract": " COTS products are now ubiquitous and clearly have become a key factor in modern software systems development. If COTS are chosen poorly, a project will likely fail. As a result, the careful assessment of COTS products has become an essential element of the development process. There are numerous approaches to COTS assessment; however none of them address the crucial question of how much assessment effort to perform. If too little assessment is done, inappropriate COTS may be used; if too much assessment is done, the effort expended may place the project at risk. It is important to achieve a satisfactory balance between COTS uncertainty risks and risks resulting from project delay. To address this, we develop a method for the strategic planning of COTS assessment by determining \u201chow much is enough\u201d effort (in time, cost, or quality) with respect to critical project risk factors such as project\u00a0\u2026", "num_citations": "33\n", "authors": ["232"]}
{"title": "The MBASE life cycle architecture milestone package\n", "abstract": " This paper summarizes the primary criteria for evaluating software/system architectures in terms of key system stakeholders\u2019 concerns. It describes the Model Based Architecting and Software Engineering (MBASE) approach for concurrent definition of a system\u2019s architecture, requirements, operational concept, prototypes, and life cycle plans. It summarizes our experiences in using and refining the MBASE approach on 31 digital library projects. It concludes that a Feasibility Rationale demonstrating consistency and feasibility of the various specifications and plans is an essential part of the architecture\u2019s definition, and presents the current MBASE annotated outline and guidelines for developing such a Feasibility Rationale.", "num_citations": "20\n", "authors": ["232"]}
{"title": "Conceptual modeling challenges for model-based architecting and software engineering (MBASE)\n", "abstract": " The difference between failure and success in developing a software-intensive system can often be traced to the presence or absence of clashes among the models used to define the system\u2019s product, process, property, and success characteristics. (Here, we use a simplified version of one of Webster\u2019s definitions of \u201cmodel\u201d a description or analogy used to help visualize something. We include analysis as a form of visualization).             Section 2 of this paper introduces the concept of model clashes, and provides examples of common clashes for each combination of product, process, property, and success models. Section 3 introduces the Model-Based Architecting and Software Engineering (MBASE) approach for endowing a software project with a mutually supportive base of models. Section 4 presents examples of applying the MBASE approach to a family of digital library projects.             Section 5\u00a0\u2026", "num_citations": "20\n", "authors": ["232"]}
{"title": "Empirical analysis of COTS activity effort sequences\n", "abstract": " Empirical data has revealed that COTS based application (CBA) development lifecycles are unique and differ from traditional software development processes. Each project will vary considerably in the particular amount of effort expended on COTS assessment, COTS tailoring, and COTS glue-code development. As such, there are wide variations in cost/schedule/quality factors, risk items, and project decision process profiles. Previous work has described these variations and provided a composable COTS decision framework that models how such variations emerge within the COTS application development process. We expand on this work by elaborating the sequence in which COTS assessment (A), tailoring (T), glue-code (G), and custom development (C) activities are performed. These sequences provide a \u201cgenetic code\u201d for a CBA development project and are useful in characterizing the uniqueness of\u00a0\u2026", "num_citations": "16\n", "authors": ["232"]}
{"title": "Balancing plan-driven and agile methods in software engineering project courses\n", "abstract": " For the past 6 years, we have been teaching a two-semester software engineering project course. The students organize into 5-person teams and develop largely web-based electronic services projects for real USC campus clients. We have been using and evolving a method called Model- Based (System) Architecting and Software Engineering (MBASE) for use in both the course and in industrial applications. The MBASE Guidelines include a lot of documents. We teach risk-driven documentation: if it is risky to document something, and not risky to leave it out (e.g., GUI screen placements), leave it out. Even so, students tend to associate more documentation with higher grades, although our grading eventually discourages this. We are always on the lookout for ways to have students learn best practices without having to produce excessive documentation. Thus, we were very interested in analyzing the various\u00a0\u2026", "num_citations": "16\n", "authors": ["232"]}
{"title": "Polynomial maps with applications to combinatorics and probability theory\n", "abstract": " In this work we consider a broad class of polynomial transformations which generalize the exponential Bell polynomials. These transformations correspond to a variety of convolutions (such as Hadamard and Cauchy) and have been extensively studied in combinatorics (for example see Rota [31], or Comtet [10]), but relatively little in connection with probability theory; and then usually for particular cases such as the Stirling numbers of the second kind. See [6],[19],[26],[25],[34],[25],[32], and [7] as examples.This apparent gap is curious as convolutions play a fundamental role in probability theory, and hence so do their associated polynomials; examples include sums of independent identically distributed random variables, the renewal equation, cumulants, and Hermite polynomials. Although this work concentrates more on using probability to develop combinatorial concepts, we hope it will lead to further consideration of the rich connection between combinatorics and probability beyond simple enumeration. In this regard we find that the Bell polynomials have a natural place in the study of the moments of a random variable. Indeed, in Chapter I of the thesis it is shown that the exponential Bell polynomials Yn (x1, x2,.), n> 1, map moment sequences to moment sequences. This is demonstrated by giving two constructions involving sums of iid random variables, first as the moments of a compound Poisson process and secondly as a limit of a sum of iid random variables. The distribution with moments Y,(xl,...) might not be unique as the moments do not always determine a unique distribution (see [33, p. viii]).", "num_citations": "15\n", "authors": ["232"]}
{"title": "Applications of simulation and ai search: Assessing the relative merits of agile vs traditional software development\n", "abstract": " This paper augments Boehm-Turner's model of agile and plan-based software development augmented with an AI search algorithm. The AI search finds the key factors that predict for the success of agile or traditional plan-based software developments. According to our simulations and AI search algorithm: (1) in no case did agile methods perform worse than plan-based approaches; (2) in some cases, agile performed best. Hence, we recommend that the default development practice for organizations be an agile method. The simplicity of this style of analysis begs the question: why is so much time wasted on evidence-less debates on software process when a simple combination of simulation plus automatic search can mature the dialog much faster?", "num_citations": "13\n", "authors": ["232"]}
{"title": "Specialization and extrapolation of software cost models\n", "abstract": " Despite the widespread availability of software effort estimation models (eg COCOMO [2], Price-S [12], SEER-SEM [13], SLIM [14]), most managers still estimate new projects by extrapolating from old projects [3, 5, 7]. In this delta method, the cost of the next project is the cost of the last project multiplied by some factors modeling the difference between old and new projects [2]. Delta estimation is simple, fast, and best of all, can take full advantage of local costing information. However delta estimation fails when the experience base (the old projects) can not be extrapolated to the new projects. Previously [10], we have shown that for a set of NASA projects, delta estimation would usually fail since most of the features and coefficients of the learned model vary wildly across sub-samples of the training data. In that prior work, no solution was offered for this problem. Here, we offer a solution and report the results of\u00a0\u2026", "num_citations": "11\n", "authors": ["232"]}
{"title": "A study on the perceived value of software quality assurance at JPL\n", "abstract": " As software quality assurance (SQA) moves from being a compliance-driven activity to one driven by value, it is important that all stakeholders involved in a software development project have a clear understanding of how SQA contributes to their efforts. However, a recent study at JPL indicates that different groups of stakeholders have significantly different perceptions about the value of SQA activities and their expectations of what constitutes SQA success. This lack of a common understanding of value has fragmented SQA efforts. Activities are driven by the desires of whichever group of stakeholders happens to hold the greatest influence at the moment leading the project as a whole not realizing the full or needed value of SQA. We examine this and other results of the recent study and how these impact both the real and the perceived value of SQA.", "num_citations": "9\n", "authors": ["232"]}
{"title": "Empirical and face validity of software maintenance defect models used at the jet propulsion laboratory\n", "abstract": " Context: At the Mission Design and Navigation Software Group at the Jet Propulsion Laboratory we make use of finite exponential based defect models to aid in maintenance planning and management for our widely used critical systems. However a number of pragmatic issues arise when applying defect models for a post-release system in continuous use. These include: how to utilize information from problem reports rather than testing to drive defect discovery and removal effort, practical model calibration, and alignment of model assumptions with our environment.Goal: To show how we can develop confidence in the practical applicability of our models for obtaining stable maintenance funding.Method: We describe the strong empirical and face validity we have investigated for our maintenance defect discovery and introduction models. We discuss the practical details of calibration and application within a\u00a0\u2026", "num_citations": "8\n", "authors": ["232"]}
{"title": "A characterization of exponential and ordinary generating functions\n", "abstract": " It is common to represent a sequence a=(a0,\u00a0a1,\u00a0\u2026) of complex numbers with a generating function. G.C. Rota once remarked that among all the possible generating functions that might be used to represent a, the ordinary and exponential generating functions are the most ubiquitous. It is unclear what, if anything, makes these two particular representations special. We show here that the ordinary and exponential representations uniquely possess the property that the determinants of the Hankel matrices of certain convolutional polynomials in a are independent of a1. Hankel matrices are closely associated with the problem of moments and the problem of moment preserving maps and hence the independence of a1 has some curious implications. For example determining if a is a sequence of cumulants for some distribution is necessarily independent of the value for the mean a1. We explore this and other\u00a0\u2026", "num_citations": "8\n", "authors": ["232"]}
{"title": "Introduction to differential equations\n", "abstract": " We are indebted to Charles and Ray Eames for the X ray photo of a nautilus shell that is correlated with a logarithmic spiral in Chapter 3 and forms part of our logo. The satellite orbit of Chapter 1, Figure 1 was computed by MC Davidson. With permission from the McGraw-Hill Book Company, we have used a few problems and excerpts from the Sokolnikoff-Redheffer text", "num_citations": "8\n", "authors": ["232"]}
{"title": "The value proposition for assurance of JPL systems\n", "abstract": " This paper presents a value proposition for systems assurance. The need for a value proposition is motivated by common misconceptions about the definition of assurance and the value of performing systems assurance activities. The focus of the value proposition is that assurance reduces uncertainty so that projects can make more confident decisions about their systems. Applying the value proposition has led to insights into the nature of assurance and has improved the practice of software assurance, where it has been applied at the Jet Propulsion Laboratory (JPL). Ongoing work on using the value proposition for \u201cvalue-based tailoring\u201d of requirements and integrating value considerations into assurance cost models are also discussed.", "num_citations": "7\n", "authors": ["232"]}
{"title": "Composable Process Elements for Developing COTS-Based Applications\n", "abstract": " In his ICSE 2002 keynote address [3], Robert Balzer issued a challenge to the software engineering community to provide better methods for dealing with COTS-based software systems, and to present them at subsequent ICSE\u2019s. This paper provides a partial response to this challenge. It presents some data that we have found useful in understanding COTS-based application (CBA) trends and effort distributions. The COTS effort distributions and sequences also suggest a framework for the primary contributions of the paper: a set of composable process elements and a decision framework for using them in the development of CBA\u2019s. Traditional sequential requirements-design-code-test (waterfall) processes do not work for CBA\u2019s [11], simply because the decision to use a COTS product constitutes acceptance of many, if not most, of the requirements that led to the product, and to its design and implementation. In fact, it is most often the case that a COTS product\u2019s capabilities will drive the \u201crequired\u201d feature set for the new product rather than the other way around, though the choice of COTS products to be used should be driven by the new project\u2019s initial set of \u201cmost significant requirements.\u201d Additionally, the volatility of COTS products [9] introduces a great deal of recursion and concurrency into CBA processes. Some recent CBA process models have partially addressed these issues by adding CBA extensions to a sequential process framework [8]. These work in some situations, but not in others where the requirements, architecture, and COTS choices evolve concurrently; the example in Section 4 illustrates this point. Other process frameworks\u00a0\u2026", "num_citations": "7\n", "authors": ["232"]}
{"title": "Risk-based strategic software design: how much COTS evaluation is enough?\n", "abstract": " Risk consideration is a valuable assessment aid when making strategic software design decisions. Expressing development considerations in terms of risk exposures over an independent variable (eg time, cumulative effort, etc.) enables the quantitative assessment of typically qualitative attributes. Assuming total risk exposure is additive over individual risk exposure functions, optimal levels for the individual considerations can be identified as function of loss-magnitude and loss-probability estimates for risk sources. Such levels provide strategic trade off considerations (with respect to risk) and have proven valuable in several previous applications such as \u201chow much testing is enough\u201d with respect to defect removal and market window strategic risk considerations. Here we consider a similar application for making strategic design decisions in determining how much effort (or time) should be spent evaluating COTS products with respect to project cost, market window, and a multitude of COTS assessment attributes such as availability, ease of use, maturity, and vendor support.", "num_citations": "7\n", "authors": ["232"]}
{"title": "Identification and classification of common risks in space science missions\n", "abstract": " Due to the highly constrained schedules and budgets that NASA missions must contend with, the identification and management of cost, schedule and risks in the earliest stages of the lifecycle is critical. At the Jet Propulsion Laboratory (JPL) it is the concurrent engineering teams that first address these items in a systematic manner. Foremost of these concurrent engineering teams is Team X. Started in 1995, Team X has carried out over 1000 studies, dramatically reducing the time and cost involved, and has been the model for other concurrent engineering teams both within NASA and throughout the larger aerospace community. The ability to do integrated risk identification and assessment was first introduced into Team X in 2001. Since that time the mission risks identified in each study have been kept in a database. In this paper we will describe how the Team X risk process is evolving highlighting the strengths\u00a0\u2026", "num_citations": "5\n", "authors": ["232"]}
{"title": "Decisions and disasters: modeling decisions that contribute to mishaps\n", "abstract": " Ever since the decision to launch the Challenger - and the deadly explosion that followed - it has been widely known that \"decision failure\" can lead to disaster. But despite this awareness and the availability of a wide variety of decision models, we found no single model that adequately describes all the ways that decisions can fail and how flawed decisions contribute to mishaps. In this paper, we present our model of decision failure. Then we show how we used this model to gain insight into that decisions that have contributed to NASA mishaps (including the Challenger). This work presents both the model and the insights from its application. The theoretical contribution is a new way to encode and analyze the decision data found in mishap reports, providing insight into the causes of decision failure. The practical contribution is the potential for using this to improve decision-making at NASA and other high-reliability\u00a0\u2026", "num_citations": "4\n", "authors": ["232"]}
{"title": "Software Dependability Risks and the Insurance Process\n", "abstract": " The concept of software dependability is intuitively understood but difficult to quantify into a constructive model. In this paper, we present a dependability risk model to convey that dependability is a relative economic measure of the dependability attribute risk factors. A system that is \u201cundependable\u201d is \u201crisky\u201d relative to the value (or benefit) of items at risk that is expected from that system. We also propose the questions that our dependability risk model can answer and provide a simple example.", "num_citations": "4\n", "authors": ["232"]}
{"title": "Tailoring a successful project-based course\n", "abstract": " The ultimate preparation for a career in software development is a project-based course in which students learn to work in teams on the development of useful software products for real clients. Published educational materials provide little support for the educator who wishes to teach such an ambitious course. We have embarked on a project whose goal is the development of materials which will enable educators to adapt Barry Boehm and Dan Port\u2019s USC CS577, one of the most successful such courses, to their special needs and resources, and to disseminate these materials in the form of a textbook, student and instructor manuals, a web-based course delivery framework, videotapes of USC course lectures, a CD archive of completed student projects, and public training in both adaptation and teaching of the course.Our starting point is the two-semester project-based Introduction to Software Engineering which\u00a0\u2026", "num_citations": "4\n", "authors": ["232"]}
{"title": "An empirical study of process policies and metrics to manage productivity and quality for maintenance of critical software systems at the jet propulsion laboratory\n", "abstract": " Context/Background: The Mission Design and Navigation Software (MDN) Group at the Jet Propulsion Laboratory (JPL) develops and continuously maintains software systems critical for NASA deep space missions. Given limited budgets, staffing resources, and a time critical need for repair or enhancement, there is an ever-present temptation to sacrifice quality for higher productivity or slip release target to ensure better quality. We have learned that poor management of this increases risk of mission failure. As a result, our process must be both highly productive and maintain high quality (eg reliability, maintainability, usability). Inspired by the\" quality is free\" paradigm, we have instituted a set of\" Rapid Release\" maintenance process policies and measures aimed to continually manage productivity and quality. Six Rapid Release polices were established from well-known engineering principles and best practices to\u00a0\u2026", "num_citations": "3\n", "authors": ["232"]}
{"title": "Actionable analytics for strategic maintenance of critical software: An industry experience report\n", "abstract": " NASA has been successfully sustaining the continuous operation of its critical navigation software systems for over 12 years. To accomplish this, NASA scientists must continuously monitor their process, report on current system quality, forecast maintenance effort, and sustain required staffing levels. This report presents some examples of the use of a robust software metrics and analytics program that enables actionable strategic maintenance management of a critical system (Monte) in a timely, economical, and risk-controlled fashion. This article is part of a special issue on Actionable Analytics for Software Engineering.", "num_citations": "3\n", "authors": ["232"]}
{"title": "Experiences using domain specific techniques within multimedia software engineering\n", "abstract": " Domain specific techniques take advantage of the commonalities among applications developed within a certain domain. They are known to improve quality and productivity by incorporating domain knowledge and previous project experiences and promote reuse. This paper describes six domain specific software engineering techniques for developing multimedia applications within the digital library domain. We provide examples of each technique from several projects in which they were used, how the techniques are used within general software engineering practice (in particular, MBASE), how the techniques address some of the particular challenges multimedia software engineering, and the positive impacts we have measured resulting from their use within a graduate level software engineering course.", "num_citations": "3\n", "authors": ["232"]}
{"title": "Classifying Risk Uncertainty for Decision Making\n", "abstract": " Studies of NASA mishaps often reveal a flawed decision-making process \u2013 one that underestimates risk. In this paper we turn our attention from the risk itself to uncertainty about the risk. In particular, we look at how decision-making accounts for uncertainties about a risk\u2019s likelihood of occurring and the consequence if it does occur. We propose a simple way of classifying risks according to these uncertainties. Then we use this classification scheme to gain insight into the flawed decision-making that contributed to the Challenger disaster and other NASA mishaps as well. We show how our risk classification scheme can improve decision-making and help avoid mishaps in the future.", "num_citations": "2\n", "authors": ["232"]}
{"title": "Staffing Strategies for Maintenance of Critical Software Systems at the Jet Propulsion Laboratory\n", "abstract": " Context: The Mission Design and Navigation Software Group at the Jet Propulsion Laboratory (JPL) maintains mission critical software systems. We have good empirical data and models for maintenance demand---when defects will occur, how many and how severe they will be, and how much effort is needed to address them. However, determining the level of staffing needed to address maintenance issues is an ongoing challenge and is often done ad-hoc. There are two common strategies are (1) reactive-add/remove staff as needed to respond to maintenance issues, and (2) capacitive-retain a given staff size to address issues as they occur, proactively address issues and prevent defects.Goal: To use our empirical models for maintenance demand to address the issue of staffing from a risk perspective.Method: We develop a staffing model that allows us to simulate large numbers of maintenance histories. From\u00a0\u2026", "num_citations": "2\n", "authors": ["232"]}
{"title": "Challenges of COTS IV & v\n", "abstract": " COTS can significantly complicate the independent verification and validation (IV&V) process. The necessarily pessimistic culture of IV&V has a perspective on COTS that greatly differs from a developer\u2019s generally optimistic, success-oriented perspective. For example, there is no basis for assuming that the COTS assessments made by developers will ultimately be consistent or even compatible with those made by an IV&V group. This frequently results in higher project risk and uncertainty. This workshop seeks to illuminate these and other COTS and IV&V related challenges.", "num_citations": "2\n", "authors": ["232"]}
{"title": "Tools for outcomes assessment of education and training in the software development process\n", "abstract": " Conference proceedings front matter may contain various advertisements, welcome messages, committee or program information, and other miscellaneous conference information. This may in some cases also include the cover art, table of contents, copyright statements, title-page or half title-pages, blank pages, venue maps or other general information relating to the conference that was part of the original conference proceedings.", "num_citations": "2\n", "authors": ["232"]}
{"title": "A Decision-Theoretic Approach to Measuring Security\n", "abstract": " The question \u201cis this system secure?\u201d is notoriously difficult to answer. The question implies that there is a system-wide property called \u201csecurity,\u201d which we can measure with some meaningful threshold of sufficiency. In this concept paper, we discuss the difficulty of measuring security sufficiency, either directly or through proxy such as the number of known vulnerabilities. We propose that the question can be better addressed by measuring confidence and risk in the decisions that depend on security. A novelty of this approach is that it integrates use of both subjective information (e.g. expert judgment) and empirical data. We investigate how this approach uses well-known methods from the discipline of decision-making under uncertainty to provide a more rigorous and useable measure of security sufficiency.", "num_citations": "1\n", "authors": ["232"]}
{"title": "COTS-Based Software Systems: 4th International Conference, ICCBSS 2005, Bilbao, Spain, February 7-11, 2005, Proceedings\n", "abstract": " The theme \u201cBuild and Conquer\u201d chosen for this year\u2019s conference fully represents what we (the organizers) want to put across to the software community: software development is an engineering discipline, and not an artistic expression. Once we are ready to \u201cbuild\u201d our software systems using pieces previously builtin (similar to any other technology manufacturer), we will be able to \u201cconquer\u201d the software engineering process. If we take a look at other engineering disciplines such as car manufacturing, house appliances or aeronautics, we see that the final products are built through the integration of multiprovider commercial components. These components are successfully integrated and constitute an important part of the final product. Most software-related organizations still build software from scratch, omitting thousands of ready-built commercially available software components that could be used very effectively during the development phase. This year ICCBSS moves to Europe for the first time since the first conference took place in Orlando, FL, USA in 2002. The conference scope has enlarged over the years to include the Open Source community and Web Services technologies. The reason for this is that I believe both are considered components-off-the-shelf, so many of the characteristics of COTS are also applied to Open Source and Web Services. Due to this, we will enjoy the presence of keynote speakers and researchers presenting on these two topics for the first time.", "num_citations": "1\n", "authors": ["232"]}