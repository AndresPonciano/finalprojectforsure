{"title": "Decision procedures\n", "abstract": " A decision procedure is an algorithm that, given a decision problem, terminates with a correct yes/no answer. In this book, we focus on decision procedures for decidable first-order theories that are useful in the context of automated software and hardware verification, theorem proving, compiler optimization, and, since we are covering propositional logic, any problem that is in the complexity class NP and is not polynomial. The range of modeling languages that we cover in this book\u2014propositional logic, linear arithmetic, bitvectors, quantified formulas etc.\u2014and the modeling examples that we include for each of those, will assist the reader to translate their particular problem and solve it with one of the publically available tools. The common term for describing this field is Satisfiability Modulo Theories, or SMT for short, and software that solves SMT formulas is called an SMT solver. Since coping with the above\u00a0\u2026", "num_citations": "685\n", "authors": ["1802"]}
{"title": "Pruning techniques for the SAT-based bounded model checking problem\n", "abstract": " Bounded Model Checking (BMC) is the problem of checking if a model satisfies a temporal property in paths with bounded length k. Propositional SAT-based BMC is conducted in a gradual manner, by solving a series of SAT instances corresponding to formulations of the problem with increasing k. We show how the gradual nature can be exploited for shortening the overall verification time. The concept is to reuse constraints on the search space which are deduced while checking a k instance, for speeding up the SAT checking of the consecutive k+1 instance. This technique can be seen as a generalization of\u2018pervasive clauses\u2019, a technique introduced by Silva and Sakallah in the context of Automatic Test Pattern Generation (ATPG). We define the general conditions for reusability of constraints, and define a simple procedure for evaluating them. This technique can theoretically be used in any solution that\u00a0\u2026", "num_citations": "192\n", "authors": ["1802"]}
{"title": "Regression verification\n", "abstract": " Proving the equivalence of successive, closely related versions of a program has the potential of being easier in practice than functional verification, although both problems are undecidable. There are two main reasons for this claim: it circumvents the problem of specifying what the program should do, and in many cases it is computationally easier. We study theoretical and practical aspects of this problem, which we call regression verification.", "num_citations": "126\n", "authors": ["1802"]}
{"title": "Deciding equality formulas by small domains instantiations\n", "abstract": " We introduce an efficient decision procedure for the theory of equality based on finite instantiations. When using the finite instantiations method, it is a common practice to take a range of [1..n] (where n is the number of input non-Boolean variables) as the range for all non-Boolean variables, resulting in a state-space of n n. Although various attempts to minimize this range were made, typically they either required various restrictions on the investigated formulas or were not very effective. In many cases, the n n state-space cannot be handled by BDD-based tools within a reasonable amount of time. In this paper we show that significantly smaller domains can be algorithmically found, by analyzing the structure of the formula. We also show an upper bound for the state-space based on this analysis. This method enabled us to verify formulas containing hundreds of integer and floating point variables.", "num_citations": "117\n", "authors": ["1802"]}
{"title": "The Code Validation Tool(CVT)\n", "abstract": " We describe CVT-a fully automatic tool for Code-Validation, ie verifying that the target code produced by a code-generator (equivalently, a compiler or a translator) is a correct implementation of the source speci cation. This approach is a viable alternative to a full formal veri cation of the code-generator program, and has the advantage of not'freezing'the code generator design after veri cation.CVT was developed in the context of the ESPRIT project SACRES, and validates the translation from State-Mate/Sildex mixed speci cation into C. The use of novel techniques based on uninterpreted functions and their analysis over a BDD-represented small model enables us to validate source speci cations of several thousands lines, which represents a typical industrial size safetycritical application.", "num_citations": "109\n", "authors": ["1802"]}
{"title": "Accelerating bounded model checking of safety properties\n", "abstract": " Bounded Model Checking based on SAT methods has recently been introduced as a complementary technique to BDD-based Symbolic Model Checking. The basic idea is to search for a counterexample in executions whose length is bounded by some integer k. The BMC problem can be efficiently reduced to a propositional satisfiability problem, and can therefore be solved by SAT methods rather than BDDs. SAT procedures are based on general-purpose heuristics that are designed for any propositional formula. We show how the unique characteristics of BMC invariant formulas (G p) can be exploited for a variety of optimizations in the SAT checking procedure. Experiments with these optimizations on real designs prove their efficiency in many of the hard test cases, in comparison to both the standard SAT procedure and a BDD-based model checker.", "num_citations": "94\n", "authors": ["1802"]}
{"title": "Regression verification: proving the equivalence of similar programs\n", "abstract": " Proving the equivalence of successive, closely related versions of a program has the potential of being easier in practice than functional verification, although both problems are undecidable. There are three main reasons for this claim: (i) it circumvents the problem of specifying what the program should do; (ii) the problem can be naturally decomposed and hence is computationally easier; and (iii) there is an automatic invariant that enables to prove equivalence of loops and recursive functions in most practical cases. Theoretical and practical aspects of this problem are considered. Copyright \u00a9 2012 John Wiley & Sons, Ltd.", "num_citations": "88\n", "authors": ["1802"]}
{"title": "The small model property: How small can it be?\n", "abstract": " Efficient decision procedures for equality logic (quantifier-free predicate calculus+the equality sign) are of major importance when proving logical equivalence between systems. We introduce an efficient decision procedure for the theory of equality based on finite instantiations. The main idea is to analyze the structure of the formula and compute accordingly a small domain to each variable such that the formula is satisfiable iff it can be satisfied over these domains. We show how the problem of finding these small domains can be reduced to an interesting graph theoretic problem. This method enabled us to verify formulas containing hundreds of integer and floating point variables that could not be efficiently handled with previously known techniques.", "num_citations": "86\n", "authors": ["1802"]}
{"title": "Explaining abstract counterexamples\n", "abstract": " When a program violates its specification a model checker produces a counterexample that shows an example of undesirable behavior. It is up to the user to understand the error, locate it, and fix the problem. Previous work introduced a technique for explaining and localizing errors based on finding the closest execution to a counterexample, with respect to a distance metric. That approach was applied only to concrete executions of programs. This paper extends and generalizes the approach by combining it with predicate abstraction. Using an abstract state-space increases scalability and makes explanations more informative. Differences between executions are presented in terms of predicates derived from the specification and program, rather than specific changes to variable values. Reasoning to the cause of an error from the factthat in the failing run x< y, but in the successful execution x= y is easier than\u00a0\u2026", "num_citations": "78\n", "authors": ["1802"]}
{"title": "On solving Presburger and linear arithmetic with SAT\n", "abstract": " We show a reduction to propositional logic from quantifier-free Presburger arithmetic, and disjunctive linear arithmetic, based on Fourier-Motzkin elimination. While the complexity of this procedure is not better than competing techniques, it has practical advantages in solving verification problems. It also promotes the option of deciding a combination of theories by reducing them to this logic.", "num_citations": "74\n", "authors": ["1802"]}
{"title": "Inference rules for proving the equivalence of recursive procedures\n", "abstract": " Inspired by Hoare\u2019s rule for recursive procedures, we present three proof rules for the equivalence between recursive programs. The first rule can be used for proving partial equivalence of programs; the second can be used for proving their mutual termination; the third rule can be used for proving the equivalence of reactive programs. There are various applications to such rules, such as proving equivalence of programs after refactoring and proving backward compatibility.", "num_citations": "64\n", "authors": ["1802"]}
{"title": "Faster extraction of high-level minimal unsatisfiable cores\n", "abstract": " Various verification techniques are based on SAT\u2019s capability to identify a small, or even minimal, unsatisfiable core in case the formula is unsatisfiable, i.e., a small subset of the clauses that are unsatisfiable regardless of the rest of the formula. In most cases it is not the core itself that is being used, rather it is processed further in order to check which clauses from a preknown set of Interesting Constraints (where each constraint is modeled with a conjunction of clauses) participate in the proof. The problem of minimizing the participation of interesting constraints was recently coined high-level minimal unsatisfiable core by Nadel\u00a0[15]. Two prominent examples of verification techniques that need such small cores are 1) abstraction-refinement model-checking techniques, which use the core in order to identify the state variables that will be used for refinement (smaller number of such variables in the core implies\u00a0\u2026", "num_citations": "61\n", "authors": ["1802"]}
{"title": "Optimized L*-based assume-guarantee reasoning\n", "abstract": " In this paper, we suggest three optimizations to the L*-based automated Assume-Guarantee reasoning algorithm for the compositional verification of concurrent systems. First, we use each counterexample from the model checker to supply multiple strings to L*, saving candidate queries. Second, we observe that in existing instances of this paradigm, the learning algorithm is coupled weakly with the teacher. Thus, the learner ignores completely the details about the internal structure of the system and specification being verified, which are available already to the teacher. We suggest an optimization that uses this information in order to avoid many unnecessary \u2013 and expensive, since they involve model checking \u2013 membership and candidate queries. Finally, and most importantly, we develop a method for minimizing the alphabet used by the assumption, which reduces the size of the assumption and the\u00a0\u2026", "num_citations": "59\n", "authors": ["1802"]}
{"title": "Deriving small unsatisfiable cores with dominators\n", "abstract": " The problem of finding a small unsatisfiable core of an unsatisfiable CNF formula is addressed. The proposed algorithm, Trimmer, iterates over each internal node d in the resolution graph that \u2018consumes\u2019 a large number of clauses M (i.e. a large number of original clauses are present in the unsat core only for proving d) and attempts to prove them without the M clauses. If this is possible, it transforms the resolution graph into a new graph that does not have the M clauses at its core. Trimmer can be integrated into a fixpoint framework similarly to Malik and Zhang\u2019s fix-point algorithm (run_till_fix). We call this option trim_till_fix. Experimental evaluation on a large number of industrial CNF unsatisfiable formulas shows that trim_till_fix doubles, on average, the number of reduced clauses in comparison to run_till_fix. It is also better when used as a component in a bigger system that enforces short timeouts.", "num_citations": "54\n", "authors": ["1802"]}
{"title": "Translation Validation: From SIGNAL to C\n", "abstract": " Translation validation is an alternative to the verification of translators (compilers, code generators). Rather than proving in advance that the compiler always produces a target code which correctly implements the source code (compiler verification), each individual translation (i.e. a run of the compiler) is followed by a validation phase which verifies that the target code produced on this run correctly implements the submitted source program. In order to be a practical alternative to compiler verification, a key feature of this validation is its full automation.               Since the validation process attempts to \u201cxunravel\u201d the transformation effected by the translators, its task becomes increasingly more difficult (and necessary) with the increase of sophistication and variety of the optimizations methods employed by the translator. In this paper we address the practicability of translation validation for highly optimizing\u00a0\u2026", "num_citations": "54\n", "authors": ["1802"]}
{"title": "Abstraction refinement for bounded model checking\n", "abstract": " Counterexample-Guided Abstraction Refinement (cegar) techniques have been very successful in model checking large systems. While most previous work has focused on model checking, this paper presents a Counterexample-Guided abstraction refinement technique for Bounded Model Checking (bmc). Our technique makes bmc much faster, as indicated by our experiments. bmc is also used for generating refinements in the Proof-Based Refinement (pbr) framework. We show that our technique unifies pbr and cegar into an abstraction-refinement framework that can balance the model checking and refinement efforts.", "num_citations": "47\n", "authors": ["1802"]}
{"title": "Regression verification-a practical way to verify programs\n", "abstract": " When considering the program verification challenge [8] one should not forget a lesson learned in the testing community: when it comes to industrial size programs, it is not realistic to expect programmers to formally specify their program beyond simple assertions. It is well known that large parts of real code cannot be described naturally with high level invariants or temporal properties, and further that it is often the case that the process of describing what a code segment should do is as difficult and at least as complicated as the coding itself. Indeed, high-level temporal property-based testing, although by now supported by commercial tools such as Temporal-Rover[4], is in very limited use. The industry typically attempts to circumvent this problem with Regression Testing, which is probably the most popular testing method for general computer programs. It is based on the idea of reasoning by induction: check\u00a0\u2026", "num_citations": "45\n", "authors": ["1802"]}
{"title": "Efficient MUS extraction with resolution\n", "abstract": " We report advances in state-of-the-art algorithms for the problem of Minimal Unsatisfiable Subformula (MUS) extraction. First, we demonstrate how to apply techniques used in the past to speed up resolution-based Group MUS extraction to plain MUS extraction. Second, we show that model rotation, presented in the context of assumption-based MUS extraction, can also be used with resolution-based MUS extraction. Third, we introduce an improvement to rotation, called eager rotation. Finally, we propose a new technique for speeding-up resolution-based MUS extraction, called path strengthening. We integrated the above techniques into the publicly available resolution-based MUS extractor HaifaMUC, which, as a result, now outperforms leading MUS extractors.", "num_citations": "43\n", "authors": ["1802"]}
{"title": "Linear-time reductions of resolution proofs\n", "abstract": " DPLL-based SAT solvers progress by implicitly applying binary resolution. The resolution proofs that they generate are used, after the SAT solver\u2019s run has terminated, for various purposes. Most notable uses in formal verification are: extracting an unsatisfiable core, extracting an interpolant, and detecting clauses that can be reused in an incremental satisfiability setting (the latter uses the proof only implicitly, during the run of the SAT solver). Making the resolution proof smaller can benefit all of these goals. We suggest two methods that are linear in the size of the proof for doing so. Our first technique, called Recycle-Units, uses each learned constant (unit clause) (x) for simplifying resolution steps in which x was the pivot, prior to when it was learned. Our second technique, called Recycle-Pivots, simplifies proofs in which there are several nodes in the resolution graph, one of which dominates the others, that\u00a0\u2026", "num_citations": "43\n", "authors": ["1802"]}
{"title": "Local restarts\n", "abstract": " Most or even all competitive DPLL-based SAT solvers have a \u201crestart\u201d policy, by which the solver is forced to backtrack to decision level 0 according to some criterion. Although not a sophisticated technique, there is mounting evidence that this technique has crucial impact on performance. The common explanation is that restarts help the solver avoid spending too much time in branches in which there is neither an easy-to-find satisfying assignment nor opportunities for fast learning of strong clauses. All existing techniques rely on a global criterion such as the number of conflicts learned as of the previous restart, and differ in the method of calculating the threshold after which the solver is forced to restart. This approach disregards, in some sense, the original motivation of focusing on \u2018bad\u2019 branches. It is possible that a restart is activated right after going into a good branch, or that it spends all of its time in a\u00a0\u2026", "num_citations": "42\n", "authors": ["1802"]}
{"title": "Translation validation: From simulink to c\n", "abstract": " Translation validation is a technique for formally establishing the semantic equivalence of the source and the target of a code generator. In this work we present a translation validation tool for the Real-Time Workshop code generator that receives as input Simulink models and generates optimized C code.", "num_citations": "40\n", "authors": ["1802"]}
{"title": "Ultimately incremental SAT\n", "abstract": " Incremental SAT solving under assumptions, introduced in Minisat, is in wide use. However, Minisat\u2019s algorithm for incremental SAT solving under assumptions has two main drawbacks which hinder performance considerably. First, it is not compliant with the highly effective and commonly used preprocessor SatELite. Second, all the assumptions are left in the formula, rather than being represented as unit clauses, propagated, and eliminated. Two previous attempts to overcome these problems solve either the first or the second of them, but not both. This paper remedies this situation by proposing a comprehensive solution for incremental SAT solving under assumptions, where SatELite is applied and all the assumptions are propagated. Our algorithm outperforms existing approaches over publicly available instances generated by a prominent industrial application in hardware validation.", "num_citations": "36\n", "authors": ["1802"]}
{"title": "An approach for extracting a small unsatisfiable core\n", "abstract": " The article addresses the problem of finding a small unsatisfiable core of an unsatisfiable CNF formula. The proposed algorithm, CoreTrimmer, iterates over each internal node d in the resolution graph that \u2018consumes\u2019 a large number of clauses M (i.e., a large number of original clauses are present in the unsat core with the sole purpose of proving\u00a0d) and attempts to prove them without the M clauses. If this is possible, it transforms the resolution graph into a new graph that does not have the M clauses at its core. CoreTrimmer can be integrated into a fixpoint framework similarly to Malik and Zhang\u2019s fix-point algorithm run_till_ fix. We call this option trim_till_fix. Experimental evaluation on a large number of industrial CNF unsatisfiable formulas shows that trim_till_fix doubles, on average, the number of reduced clauses in comparison to run_till_fix. It is also better when used as a component in a bigger system\u00a0\u2026", "num_citations": "30\n", "authors": ["1802"]}
{"title": "Accelerated deletion-based extraction of minimal unsatisfiable cores\n", "abstract": " Various technologies are based on the capability to find small unsatisfiable cores given an unsatisfiable CNF formula, ie, a subset of the clauses that are unsatisfiable regardless of the rest of the formula. If that subset is irreducible, it is called a Minimal Unsatisfiable Core (MUC). In many cases, the MUC is required not in terms of clauses, rather in terms of a preknown user-given set of high-level constraints, where each such constraint is a conjunction of clauses. We call the problem of minimizing the participation of such constraints high-level minimal unsatisfiable core (HLMUC) extraction. All the current state-of-the-art tools for MUC-and HLMUC-extraction are deletion-based, which means that they iteratively try to delete clauses from the core. We propose nine optimizations to this general strategy, although not all apply to both MUC and HLMUC. For both cases we achieved over a 2X improvement in run time\u00a0\u2026", "num_citations": "29\n", "authors": ["1802"]}
{"title": "Sharing information between instances of a propositional satisfiability (SAT) problem\n", "abstract": " A technique is disclosed for sharing information between closely-related SAT instances (instances with a non-empty intersection between their sets of clauses), which enables a speed-up in the overall solution time. This technique is particularly effective in SAT-based bounded model checking (BMC), and in problems of planning and logistics.", "num_citations": "27\n", "authors": ["1802"]}
{"title": "Easier and more informative vacuity checks\n", "abstract": " In formal verification, we verify that a system is correct with respect to a specification. Cases like antecedent failure can make a successful pass of the verification procedure meaningless. Vacuity detection can signal such \"meaningless\" passes of the specification, and indeed vacuity checks are now a standard component in many commercial model checkers. We address two dimensions of vacuity: the computational effort and the information that is given to the user. As for the first dimension, we present several preliminary vacuity checks that can be done without the design itself, which implies that some information can be found with a significantly smaller effort. As for the second dimension, we present algorithms for deriving three types of information that are not provided by standard vacuity checks, assuming M \\= phi for a model M and property phi: a) behaviors that are possibly missing from M (or wrongly restricted\u00a0\u2026", "num_citations": "26\n", "authors": ["1802"]}
{"title": "HaifaSat: A New Robust SAT Solver\n", "abstract": " The popular abstraction/refinement model frequently used in verification, can also explain the success of a SAT decision heuristic like Berkmin. According to this model, conflict clauses are abstractions of the clauses from which they were derived. We suggest a clause-based decision heuristic called Clause-Move-To-Front (CMTF), which attempts to follow an abstraction/refinement strategy (based on the resolve-graph) rather than satisfying the clauses in the chronological order in which they were created, as done in Berkmin. We also show a resolution-based score function for choosing the variable from the selected clause and a similar function for choosing the sign. We implemented the suggested heuristics in our SAT solver HaifaSat. Experiments on hundreds of industrial benchmarks demonstrate the superiority of this method comparing to the Berkmin heuristic. There is still room for research on how to\u00a0\u2026", "num_citations": "26\n", "authors": ["1802"]}
{"title": "Range allocation for separation logic\n", "abstract": " Separation Logic consists of a Boolean combination of predicates of the form v                                       i                \u2265 v                                       j                + c where c is a constant and v                                       i               ,v                                       j                are variables of some ordered infinite type like real or integer. Any equality or inequality can be expressed in this logic. We propose a decision procedure for Separation Logic based on allocating small domains (ranges) to the formula\u2019s variables that are sufficient for preserving satisfiability. Given a Separation Logic formula \u03c6, our procedure constructs the inequalities graph of \u03c6, based on \u03c6\u2019s predicates. This graph represents an abstraction of the formula, as there are many formulas with the same set of predicates. Our procedure then analyzes this graph and allocates a range to each variable that is adequate for all of these formulas. This approach of finding\u00a0\u2026", "num_citations": "26\n", "authors": ["1802"]}
{"title": "Cost-effective hyper-resolution for preprocessing CNF formulas\n", "abstract": " We present an improvement to the Hyper preprocessing algorithm that was suggested by Bacchus and Winter in SAT 2003 [1]. Given the power of modern SAT solvers, Hyper is currently one of the only cost-effective preprocessors, at least when combined with some modern SAT solvers and on certain classes of problems. Our algorithm, although it produces less information than Hyper, is much more efficient. Experiments on a large set of industrial Benchmark sets from previous SAT competitions show that HyperBinFast is always faster than Hyper (sometimes an order of magnitude faster on some of the bigger CNF formulas), and achieves faster total run times, including the SAT solver\u2019s time. The experiments also show that HyperBinFast is cost-effective when combined with three state-of-the-art SAT solvers.", "num_citations": "25\n", "authors": ["1802"]}
{"title": "Minimal unsatisfiable core extraction for SMT\n", "abstract": " Finding a minimal (i.e., irreducible) unsatisfiable core (MUC), and high-level minimal unsatisfiable core (also known as group MUC, or GMUC), are well-studied problems in the domain of propositional satisfiability. In contrast, in the domain of SMT, no solver in the public domain produces a minimal or group-minimal core. Several SMT solvers, like Z3, produce a core but do not attempt to minimize it. The SMT solver MATHSAT has an option to try to make the core smaller, but does not guarantee minimality. In this article we present a method and tool, HSMTMUC, for finding MUC and GMUC for SMT solvers. The method is based on the well-known deletion-based MUC extraction that is used in most propositional MUC extractors, together with several new optimizations such as theory-rotation, and an adaptive activation strategy based on measurements, during execution, of the time consumed by various components\u00a0\u2026", "num_citations": "22\n", "authors": ["1802"]}
{"title": "Learning general constraints in CSP\n", "abstract": " We present a new learning scheme for solvers of the Constraint Satisfaction Problem (CSP), which is based on learning (general) constraints rather than the generalized no-goods or signed-clauses that were used in the past. The new scheme is integrated in a conflict-analysis algorithm reminiscent of a modern systematic propositional satisfiability (SAT) solver: it traverses the conflict graph backwards and gradually builds an asserting conflict constraint. This construction is based on new inference rules that are tailored for various pairs of constraints types, eg, x\u2264 y 1+ k 1 and x\u2265 y 2+ k 2, or y 1\u2264 x and [x, y 2]\u2288[a, b]. The learned constraint is stronger than what can be learned via signed resolution. Our experiments show that our solver HCSP backtracks orders of magnitude less than other state-of-the-art solvers, and is overall on par with the winner of this year's MiniZinc challenge.", "num_citations": "22\n", "authors": ["1802"]}
{"title": "Finite instantiations in equivalence logic with uninterpreted functions\n", "abstract": " We introduce a decision procedure for satisfiability of equivalence logic formulas with uninterpreted functions and predicates. In a previous work ([PRSS99]) we presented a decision procedure for this problem which started by reducing the formula into a formula in equality logic. As a second step, the formula structure was analyzed in order to derive a small range of values for each variable that is sufficient for preserving the formula's satisfiability. Then, a standard BDD based tool was used in order to check the formula under the new small domain. In this paper we change the reduction method and perform a more careful analysis of the formula, which results in significantly smaller domains. Both theoretical and experimental results show that the new method is superior to the previous one and to the method suggested in [BGV99].", "num_citations": "22\n", "authors": ["1802"]}
{"title": "Yet another decision procedure for equality logic\n", "abstract": " We introduce a new decision procedure for Equality Logic. The procedure improves on Bryant and Velev\u2019s sparse method [4] from CAV\u201900, in which each equality predicate is encoded with a Boolean variable, and then a set of transitivity constraints are added to compensate for the loss of transitivity of equality. We suggest the Reduced Transitivity Constraints (RTC) algorithm, that unlike the sparse method, considers the polarity of each equality predicate, i.e. whether it is an equality or disequality when the given equality formula \u03d5                                            E                  is in Negation Normal Form (NNF). Given this information, we build the Equality Graph corresponding to \u03d5                                            E                  with two types of edges, one for each polarity. We then define the notion of Contradictory Cycles to be cycles in that graph that the variables corresponding to their edges cannot be simultaneously satisfied\u00a0\u2026", "num_citations": "21\n", "authors": ["1802"]}
{"title": "Regression verification for unbalanced recursive functions\n", "abstract": " We address the problem of proving the equivalence of two recursive functions that have different base-cases and/or are not in lock-step. None of the existing software equivalence checkers (like r\u00eave, rvt, Symdiff), or general unbounded software model-checkers (like Seahorn, HSFC, Automizer) can prove such equivalences. We show a proof rule for the case of different base cases, based on separating the proof into two parts\u2014inputs which result in the base case in at least one of the two compared functions, and all the rest. We also show how unbalanced unrolling of the functions can solve the case in which the functions are not in lock-step. In itself this type of unrolling may again introduce the problem of the different base cases, and we show a new proof rule for solving it. We implemented these rules in our regression-verification tool rvt. We conclude by comparing our approach to that of Felsig et al.\u2019s\u00a0\u2026", "num_citations": "17\n", "authors": ["1802"]}
{"title": "Before and after vacuity\n", "abstract": " In formal verification, we verify that a system is correct with respect to a specification. Cases like antecedent failure can make a successful pass of the verification procedure meaningless. Vacuity detection can signal such \u201cmeaningless\u201d passes of the specification, and indeed vacuity checks are now a standard component in many commercial model checkers.               We address two dimensions of vacuity: the computational effort and the information that is given to the user. As for the first dimension, we present several preliminary vacuity checks that can be done without the design itself, which implies that some information can be found with a significantly smaller effort. As for the second dimension, we present algorithms for deriving two types of information that are not provided by standard vacuity checks, assuming    for a model M and formula \u03c6: (a) behaviors that are possibly missing from M (or\u00a0\u2026", "num_citations": "17\n", "authors": ["1802"]}
{"title": "Three optimizations for Assume\u2013Guarantee reasoning with L\n", "abstract": " The learning-based automated Assume\u2013Guarantee reasoning paradigm has been applied in the last few years for the compositional verification of concurrent systems. Specifically, L* has been used for learning the assumption, based on strings derived from counterexamples, which are given to it by a model-checker that attempts to verify the Assume\u2013Guarantee rules. We suggest three optimizations to this paradigm. First, we derive from each counterexample multiple strings to L*, rather than a single one as in previous approaches. This small improvement saves candidate queries and hence model-checking runs. Second, we observe that in existing instances of this paradigm, the learning algorithm is coupled weakly with the teacher. Thus, the learner completely ignores the details of the internal structure of the system and specification being verified, which are available already to the teacher. We suggest\u00a0\u2026", "num_citations": "17\n", "authors": ["1802"]}
{"title": "Preprocessing in incremental SAT\n", "abstract": " Preprocessing of CNF formulas is an invaluable technique when attempting to solve large formulas, such as those that model industrial verification problems. Unfortunately, the best combination of preprocessing techniques, which involve variable elimination combined with subsumption, is incompatible with incremental satisfiability. The reason is that soundness is lost if a variable is eliminated and later reintroduced. Look-ahead is a known technique to solve this problem, which simply blocks elimination of variables that are expected to be part of future instances. The problem with this technique is that it relies on knowing the future instances, which is impossible in several prominent domains. We show a technique for this realm, which is empirically far better than the known alternatives: running without preprocessing at all or applying preprocessing separately at each step.", "num_citations": "16\n", "authors": ["1802"]}
{"title": "A proof-producing CSP solver\n", "abstract": " PCS is a CSP solver that can produce a machine-checkable deductive proof in case it decides that the input problem is unsatisfiable. The roots of the proof may be nonclausal constraints, whereas the rest of the proof is based on resolution of signed clauses, ending with the empty clause. PCS uses parameterized, constraint-specific inference rules in order to bridge between the nonclausal and the clausal parts of the proof. The consequent of each such rule is a signed clause that is 1) logically implied by the nonclausal premise, and 2) strong enough to be the premise of the consecutive proof steps. The resolution process itself is integrated in the learning mechanism, and can be seen as a generalization to CSP of a similar solution that is adopted by competitive SAT solvers.", "num_citations": "15\n", "authors": ["1802"]}
{"title": "Reducing the size of resolution proofs in linear time\n", "abstract": " DPLL-based SAT solvers progress by implicitly applying binary resolution. The resolution proofs that they generate are used, after the SAT solver\u2019s run has terminated, for various purposes. Most notable uses in formal verification are: extracting an unsatisfiable core, extracting an interpolant, and detecting clauses that can be reused in an incremental satisfiability setting (the latter uses the proof only implicitly, during the run of the SAT solver). Making the resolution proof smaller can benefit all of these goals: it can lead to smaller cores, smaller interpolants, and smaller clauses that are propagated to the next SAT instance in an incremental setting. We suggest two methods that are linear in the size of the proof for doing so. Our first technique, called Recycle-Units uses each learned constant (unit clause) (x) for simplifying resolution steps in which x was the pivot, prior to when it was learned. Our second\u00a0\u2026", "num_citations": "14\n", "authors": ["1802"]}
{"title": "Regression verification: Proving the equivalence of similar programs\n", "abstract": " The ability to prove equivalence of successive, closely-related versions of a program can be useful for maintaining backward compatibility. This problem has the potential of being easier in practice than functional verification for at least two reasons: First, it circumvents the problem of specifying what the program should do; Second, in many cases it is computationally easier, because it offers various opportunities for abstraction and decomposition that are only relevant in this context.", "num_citations": "14\n", "authors": ["1802"]}
{"title": "Decision-making with cross-entropy for self-adaptation\n", "abstract": " Approaches to decision-making in self-adaptive systems are increasingly becoming more effective at managing the target system by taking into account more elements of the decision problem that were previously ignored. These approaches have to solve complex optimization problems at run time, and even though they have been shown to be suitable for different kinds of systems, their time complexity can make them excessively slow for systems that have a large adaptation-relevant state space, or that require a tight control loop driven by fast decisions. In this paper we present an approach to speed up complex proactive latency-aware self-adaptation decisions, using the cross-entropy (CE) method for combinatorial optimization. The CE method is an any-time algorithm based on random sampling from the solution space, and is not guaranteed to find an optimal solution. Nevertheless, our experiments using two\u00a0\u2026", "num_citations": "12\n", "authors": ["1802"]}
{"title": "Inference rules for proving the equivalence of recursive procedures\n", "abstract": " We present two proof rules for the equivalence of recursive procedures, in the style of Hoare\u2019s rule for recursive invocation of procedures. The first rule can be used for proving partial equivalence of programs; the second can be used for proving their mutual termination. There are various applications to these rules, such as proving backward compatibility.", "num_citations": "11\n", "authors": ["1802"]}
{"title": "Regression verification: Theoretical and implementation aspects\n", "abstract": " Proving the equivalence of successive (closely related) versions of a program has the potential of being easier in practice than functional verification, although both problems are undecidable. There are two main reasons for this claim: it circumvents the problem of specifying what the program should do, and in many cases it is computationally easier. In this thesis we study theoretical and practical aspects of this problem, which we call regression verification.The thesis is divided into two parts. In the first part we propose several notions of equivalence between programs, and corresponding proof rules in the style of Hoare's rule for recursive procedures. These rules enable us to prove the equivalence of recursive and mutually recursive programs, and also have an advantage from the perspective of the computational effort, since it allows us to decompose and abstract the two programs. This method is sound but incomplete.", "num_citations": "11\n", "authors": ["1802"]}
{"title": "A new class of lineage expressions over probabilistic databases computable in p-time\n", "abstract": " We study the problem of query evaluation over tuple-independent probabilistic databases. We define a new characterization of lineage expressions called disjoint branch acyclic, and show this class to be computed in P-time. Specifically, this work extends the class of lineage expressions for which evaluation can be performed in PTIME. We achieve this extension with a novel usage of junction trees to compute the probability of these lineage expressions.", "num_citations": "10\n", "authors": ["1802"]}
{"title": "A theory-based decision heuristic for DPLL (T)\n", "abstract": " We study the decision problem of disjunctive linear arithmetic over the reals from the perspective of computational geometry. We show that traversing the linear arrangement induced by the formula's predicates, rather than the DPLL(T) method of traversing the Boolean space, may have an advantage when the number of variables is smaller than the number of predicates (as it is indeed the case in the standard SMT-Lib benchmarks). We then continue by showing a branching heuristic that is based on approximating T-implications, based on a geometric analysis. We achieve modest improvement in run time comparing to the commonly used heuristic used by competitive solvers.", "num_citations": "10\n", "authors": ["1802"]}
{"title": "Building small equality graphs for deciding equality logic with uninterpreted functions\n", "abstract": " The logic of Equalities with Uninterpreted Functions is used in the formal verification community mainly for proofs of equivalence: proving that two versions of a hardware design are the same, or that input and output of a compiler are semantically equivalent are two prominent examples of such proofs. We introduce a new decision procedure for this logic that generalizes two leading decision procedures that were published in the last few years: the Positive Equality approach suggested by Bryant et al. [Exploiting positive equality in a logic of equality with uninterpreted functions, in: Proc. 11th Intl. Conference on Computer Aided Verification (CAV\u201999), 1999], and the Range-Allocation algorithm suggested by Pnueli et al. [The small model property: how small can it be? Information and Computation 178 (1) (2002) 279\u2013293]. Both of these methods reduce this logic to pure Equality Logic (without Uninterpreted Functions\u00a0\u2026", "num_citations": "10\n", "authors": ["1802"]}
{"title": "Optimizations in decision procedures for propositional linear inequalities\n", "abstract": " Several decision procedures that were published in the last few years for sub-theories of propositional linear inequalities, ie a Boolean combination of predicates that belong to the theory, are based on a graph-based analysis of the formulas predicates. The analysis is always based on the predicates while ignoring the Boolean connectives between them. In this note we show how taking this information into account can significantly reduce the practical complexity of the decision procedure.Descriptors:", "num_citations": "10\n", "authors": ["1802"]}
{"title": "The'Logic Assurance (LA)'system-a tool for testing and controlling real-time systems\n", "abstract": " The LA system combines the testing, debugging, monitoring and control of real-time systems. It is capable of improving a system's reliability and development productivity. This is achieved by enabling the developer to describe parts of the specification with logic and temporal logic assertions. The assertions are automatically compared to the actual behavior of the system under development (SUD), and its environment information about the SUD's behavior is represented by informative events and states. This information is transferred to LA from different resources by using special directives inserted inside the code (an approach we call 'informative box'). When an assertion is violated, an informative message is given so that bugs can be detected and focused on. In addition to the message, a user function can be called. This can be used for real-time control and monitoring, either while testing the system or after it has\u00a0\u2026", "num_citations": "9\n", "authors": ["1802"]}
{"title": "Local restarts in SAT\n", "abstract": " Most or even all competitive DPLL-based SAT solvers have a \u201crestart\u201d policy, by which the solver is forced to backtrack to decision level 0 according to some criterion. Although not a sophisticated technique, there is mounting evidence that this technique has crucial impact on performance. The common explanation is that restarts help the solver avoid spending too much time in branches in which there is neither an easy-to-find satisfying assignment nor opportunities for fast learning of strong clauses. All existing techniques rely on a global criterion such as the number of conflicts learned as of the previous restart, and differ in the method of calculating the threshold after which the solver is forced to restart. This approach disregards, in some sense, the original motivation of avoiding \u2018bad\u2019branches. It is possible that a restart is activated right after going into a good branch, or that it spends all of its time in a single bad branch. We suggest instead to localize restarts, ie, apply restarts according to measures local to each branch. This adds a dimension to the restart policy, namely the decision level in which the solver is currently in. Our experiments with both Minisat 2007 and Eureka show that with certain parameters this improves the run time by 15%-30% on average (when applied to the 100 test benchmarks of SAT-race\u201906), and reduces the number of time-outs. We begin the paper by considering various possible explanations for the effectiveness of restarts.", "num_citations": "8\n", "authors": ["1802"]}
{"title": "Proving mutual termination\n", "abstract": " Two programs are said to be mutually terminating if they terminate on exactly the same inputs. We suggest inference rules and a proof system for proving mutual termination of a given pair of procedures                          ,                           and the respective subprograms that they call under a free context. Given a (possibly partial) mapping between the procedures of the two programs, the premise of the rule requires proving that given the same arbitrary input in, f(in) and  call procedures mapped in the mapping with the same arguments. A variant of this proof rule with a weaker premise allows to prove termination of one of the programs if the other is known to terminate. In addition, we suggest various techniques for battling the inherent incompleteness of our solution, including a case in which the interface of the two procedures is not identical, and a case in which partial equivalence (the equivalence of\u00a0\u2026", "num_citations": "7\n", "authors": ["1802"]}
{"title": "A probabilistic analysis of coverage methods\n", "abstract": " Coverage is an important measure for the quality and completeness of the functional verification of hardware logic designs. Verification teams spend a significant amount of time looking for bugs in the design and in providing high-quality coverage. This process is performed through the use of various sampling strategies for selecting test inputs. The selection of sampling strategies to achieve the verification goals is typically carried out in an intuitive manner. We studied several commonly used sampling strategies and provide a probabilistic framework for assessing and comparing their relative values. For this analysis, we derived results for two measures of interest: first, the probability of finding a bug within a given number of samplings; and second, the expected number of samplings until a bug is detected. These results are given for both recurring sampling schemes, in which the same inputs might be selected\u00a0\u2026", "num_citations": "7\n", "authors": ["1802"]}
{"title": "Reduced functional consistency of uninterpreted functions\n", "abstract": " A reduction of Equality Logic with Uninterpreted Functions (EUF) to Equality Logic with Ackermann's method suffers from a quadratic growth in the number of functional consistency constraints (constraints of the form x= y\u2192 F (x)= F (y)). We propose a framework in which syntactic characteristics of function instances (their signature) is used for guessing which constraints will possibly be needed for the proof. This framework can be either combined in an abstraction-refinement loop, or, in some cases, be used without refinement iterations. The framework is suitable for equivalence verification problems, which is one of the typical uses of Uninterpreted Functions. It enabled us to verify dozens of verification conditions resulting from Translation Validation that we could not prove otherwise.", "num_citations": "7\n", "authors": ["1802"]}
{"title": "Range allocation for equivalence logic\n", "abstract": " The range allocation problem was recently introduced as part of an efficient decision procedure for deciding satisfiability of equivalence logic formulas with or without uninterpreted functions. These type of formulas are mainly used when proving equivalence or refinement between systems (hardware designs, compiler\u2019s translation, etc). The problem is to find in polynomial time a small finite domain for each of the variables in an equality formula \u03c6, such that \u03c6 is valid if and only if it is valid over this small domain. The heuristic that was presented for finding small domains was static, i.e. it finds a small set of integer constants for each variable. In this paper we show new, more flexible range allocation methods. We also show the limitations of these and other related approaches by proving a lower bound on the size of the state space generated by such procedures. To prove this lower bound we reduce the\u00a0\u2026", "num_citations": "7\n", "authors": ["1802"]}
{"title": "Using simulation to increase efficiency in an army recruitment office\n", "abstract": " The Israeli army used queuing analysis, simulation, and decision-support modeling to redesign its recruitment procedures. The analysis and simulation helped it to achieve significant savings and improvements in quality of service. Simulation was the key technique used to evaluate the various possible strategies, and it helped management to visualize their potential benefits. The application of these methods in the service sector, which deals with people rather than assembled parts, was the main challenge in this project.", "num_citations": "7\n", "authors": ["1802"]}
{"title": "program equivalence\n", "abstract": " Program equivalence, while generally being undecidable, is arguably one of the most important problems in formal verification. While program functional correctness, which is also undecidable, has always been the \u2018holy grail\u2019of formal verification, program equivalence is easier to solve in many realistic cases, which makes it a lower-hanging fruit and hence more attractive as a topic for research and tool development. Despite the fact that the latter can be reduced to the former, a simple reduction misses the opportunities to exploit the similarity between the two compared programs if it exists. One of the articles in this issue (Automating", "num_citations": "6\n", "authors": ["1802"]}
{"title": "Generating minimum transitivity constraints in P-time for deciding equality logic\n", "abstract": " In a CAV'05 paper [Meir, O. and O. Strichman, Yet another decision procedure for equality logic, in: K. Etessami and S. Rajamani, editors, Proc. 17th Intl. Conference on Computer Aided Verification (CAV'05), Lect. Notes in Comp. Sci. 3576 (2005), pp. 307\u2013320] we introduced a new decision procedure for Equality Logic: each equality predicate is encoded with a Boolean variable, and then a set of transitivity constraints are added to compensate for the loss of transitivity of equality. The constraints are derived by analyzing Contradictory Cycles: cycles in the equality graph with exactly one disequality. Such a cycle is called constrained under a formula \u03d5 if \u03d5 is not satisfied with an assignment of true to all equality edges and false to the disequality edge. While we proved in [Meir, O. and O. Strichman, Yet another decision procedure for equality logic, in: K. Etessami and S. Rajamani, editors, Proc. 17th Intl. Conference\u00a0\u2026", "num_citations": "6\n", "authors": ["1802"]}
{"title": "The Code Validation Tool (CVT)\u2013Automatic verification of code generated from synchronous languages\n", "abstract": " We describe CVT-a fully automatic tool for Code-Validation, ie verifying that the target code produced by a code-generator (equivalently, a compiler or a translator) is a correct implementation of the source specification. This approach is a viable alternative to a full formal verification of the code-generator program, and has the advantage of not\u2019freezing\u2019the code generator design after verification. The CVT tool has been developed in the context of the ESPRIT project SACRES, and validates the translation from StateMate/Sildex mixed specification into C. The use of novel techniques based on uninterpreted functions and their analysis over a BDD-represented small model enables us to validate source specifications of several thousands lines, which represents a typical industrial size safety-critical application.", "num_citations": "6\n", "authors": ["1802"]}
{"title": "HaifaSat: a SAT solver based on an Abstraction/Refinement model\n", "abstract": " The popular abstraction/refinement model frequently used in verification, can also explain the success of a SAT decision heuristic like Berkmin. According to this model, conflict clauses are abstractions of the clauses from which they were derived. We suggest a clause-based decision heuristic called Clause-Move-To-Front (CMTF), which attempts to follow an abstraction/refinement strategy (based on the resolve-graph) rather than satisfying the clauses in the chronological order in which they were created, as done in Berkmin. We also show a resolution-based score function for choosing the variable from the selected clause and a similar function for choosing the sign. We implemented the suggested heuristics in our SAT solver HaifaSat. Experiments on hundreds of industrial benchmarks demonstrate the superiority of this method comparing to the Berkmin heuristic. HaifaSat won the 3rd place in the industrial\u00a0\u2026", "num_citations": "5\n", "authors": ["1802"]}
{"title": "Program equivalence (Dagstuhl Seminar 18151)\n", "abstract": " Program equivalence is the problem of proving that two programs are equal under some definition of equivalence, eg, input-output equivalence. The field draws researchers from formal verification, semantics and logics. This report documents the program and the outcomes of Dagstuhl Seminar 18151\" Program Equivalence\". The seminar was organized by the four official organizers mentioned above, and Dr. Nikos Tzevelekos from Queen-Mary University in London.", "num_citations": "4\n", "authors": ["1802"]}
{"title": "Proving mutual termination of programs\n", "abstract": " Two programs are said to be mutually terminating if they terminate on exactly the same inputs. We suggest a proof rule that uses a mapping between the functions of the two programs for proving mutual termination of functions f, f\u2032. The rule\u2019s premise requires proving that given the same arbitrary input in, f(in) and f\u2019(in) call mapped functions with the same arguments. A variant of this rule with a weaker premise allows to prove termination of one of the programs if the other is known to terminate for all inputs. We present an algorithm for decomposing the verification problem of whole programs to that of proving mutual termination of individual functions, based on our suggested rules.", "num_citations": "4\n", "authors": ["1802"]}
{"title": "Clause and Proof Tightening\n", "abstract": " A computer-implemented method for verification of a target system includes defining a formula describing the target system, the formula including clauses, which include variables and which express constraints on states of the target system. The formula is processed so as to derive, using the clauses, a proof relating to a property of the target system. After deriving the proof, a variable that has a constant value is identified in the proof. The number of the variables in the proof is reduced using the identified variable, thereby producing a tightened expression, which is applied in making a determination of whether the target system satisfies the formula.", "num_citations": "4\n", "authors": ["1802"]}
{"title": "Underapproximation for model-checking based on random cryptographic constructions\n", "abstract": " For two naturals m,n such that m\u2009<\u2009n, we show how to construct a circuit C with m inputs and n outputs, that has the following property: for some 0\u2009\u2264\u2009k\u2009\u2264\u2009m, the circuit defines a k-universal function. This means, informally, that for every subset K of k outputs, every possible valuation of the variables in K is reachable (we prove that k is very close to m with an arbitrarily high probability). Now consider a circuit M with n inputs that we wish to model-check. Connecting the inputs of M to the outputs of C gives us a new circuit M\u2032 with m inputs, that its original inputs have freedom defined by k. This is a very attractive feature for underapproximation in model-checking: on one hand the combined circuit has a smaller number of inputs, and on the other hand it is expected to find an error state fast if there is one.               We report initial experimental results with bounded model checking of industrial designs (the\u00a0\u2026", "num_citations": "4\n", "authors": ["1802"]}
{"title": "Translation validation: From DC+ to C\n", "abstract": " Translation validation is an alternative to the verification of translators (compilers, code generators). Rather than proving in advance that the compiler always produces a target code which correctly implements the source code (compiler verification), each individual translation (i.e. a run of the compiler) is followed by a validation phase which verifies that the target code produced on this run correctly implements the submitted source program. In order to be a practical alternative to compiler verification, a key feature of this validation is its full automation.               In this paper we demonstrate the feasibility of translation validation for industrial code generators from DC+ -a widely used intermediate format for synchronous languages- to C. We explain the compilation pattern from DC+ to C and advocate new abstraction techniques for a fragment of first order logic as part of the automation of our approach.", "num_citations": "4\n", "authors": ["1802"]}
{"title": "Near-optimal course scheduling at the Technion\n", "abstract": " The focus of this article is the automation of course, classroom, and exam scheduling for the faculty of Industrial Engineering (IE) at the Technion in Haifa, Israel. The system, called the Technion Industrial Engineering Scheduler (TieSched), has been operational since 2012. It is based on a distributed collection of constraints and multiple engines running in parallel, including SAT, pseudo-Boolean, CSP, and weighted-Max-SAT solvers. A sophisticated decision support subsystem accommodates manual edits to the schedule. This article describes the manual process used previously and the TieSched system architecture, and it provides details about the model formulation and solving engines. It also presents the new process that TieSched enables and the path to stakeholder acceptance. The benefits of TieSched include improved efficiency of the scheduling process (i.e., a reduction from 9\u201310 to 3\u20134 weeks), better\u00a0\u2026", "num_citations": "3\n", "authors": ["1802"]}
{"title": "Mining backbone literals in incremental SAT\n", "abstract": " In incremental SAT solving, information gained from previous similar instances has so far been limited to learned clauses that are still relevant, and heuristic information such as activity weights and scores. In most settings in which incremental satisfiability is applied, many of the instances along the sequence of formulas being solved are unsatisfiable. We show that in such cases, with a P-time analysis of the proof, we can compute a set of literals that are logically implied by the next instance. By adding those literals as assumptions, we accelerate the search.", "num_citations": "3\n", "authors": ["1802"]}
{"title": "A proof-producing CSP solver (a proof supplement)\n", "abstract": " Abstract In [1] we described PCS, a CSP solver that can produce a machinecheckable deductive proof for an unsatisfiable input problem. This report supplements [1] in several ways: it provides soundness proof for the inference rules that were presented in Table 3; it provides proofs of correctness for several algorithms and claims; and, finally, it adds several missing algorithms and optimizations.", "num_citations": "3\n", "authors": ["1802"]}
{"title": "Linear-time reductions of resolution proofs (full version\n", "abstract": " DPLL-based SAT solvers progress by implicitly applying bi-nary resolution. The resolution proofs that they generate are used, after the SAT solver\u2019s run has terminated, for various purposes. Most notable uses in formal verification are: extracting an unsatisfiable core, extracting an interpolant, and detecting clauses that can be reused in an incremen-tal satisfiability setting (the latter uses the proof only implicitly, during the run of the SAT solver). Making the resolution proof smaller can ben-efit all of these goals: it can lead to smaller cores, smaller interpolants, and smaller clauses that are propagated to the next SAT instance in an incremental setting. We suggest two methods that are linear in the size of the proof for doing so. Our first technique, called Recycle-Units, uses each learned constant (unit clause)(x) for simplifying resolution steps in which x was the pivot, prior to when it was learned. Our second technique, called Recycle-Pivots, simplifies proofs in which there are several nodes in the resolution graph, one of which dominates the others, that correspond to the same pivot. Our experiments with industrial in-stances show that these simplifications reduce the core by\u2248 5% and the proof by\u2248 13%. It reduces the core less than competing methods such as run-till-fix, but whereas our algorithms are linear in the size of the proof, the latter and other competing techniques are all exponential as they are based on SAT runs. If we consider the size of the proof graph as being polynomial in the number of variables (it is not necessarily the case in general), this gives our method an exponential time reduction com-paring to existing tools for small core\u00a0\u2026", "num_citations": "2\n", "authors": ["1802"]}
{"title": "An approach to extracting a small unsatisfiable core\n", "abstract": " The article addresses the problem of finding a small unsatisfiable core of an unsatisfiable CNF formula. The proposed algorithm, Core-Trimmer, iterates over each internal node d in the resolution graph that \u2018consumes\u2019a large number of clauses M (ie, a large number of original clauses are present in the unsat core with the sole purpose of proving d) and attempts to prove them without the M clauses. If this is possible, it transforms the resolution graph into a new graph that does not have the M clauses at its core. CoreTrimmer can be integrated into a fixpoint framework similarly to Malik and Zhang\u2019s fix-point algorithm run till fix. We call this option trim till fix. Experimental evaluation on a large number of industrial CNF unsatisfiable formulas shows that trim till fix doubles, on average, the number of reduced clauses in comparison to run till fix. It is also better when used as a component in a bigger system that enforces short timeouts.", "num_citations": "2\n", "authors": ["1802"]}
{"title": "Finite instantiations in equivalence logic with uninterpreted functions\n", "abstract": " We introduce a decision procedure for satis ability of equivalence logic formulas with uninterpreted functions and predicates. In a previous work (PRSS98]) we presented a decision procedure for this problem which starts by reducing the formula to a formula in equality logic. As a second step, the formula structure was analyzed in order to derive a small range of values for each variable that is su cient for preserving the formula's satis ability. Then, a standard BDD based tool was used in order to check the formula under the new small domain. In this paper we change the reduction method and perform a more careful analysis of the formula, which results in signi cantly smaller domains. Both theoretical and experimental results show that the new method is superior to the previous one and to other methods that were suggested in the last few years.", "num_citations": "2\n", "authors": ["1802"]}
{"title": "Hardware and Software: Verification and Testing: 13th International Haifa Verification Conference, HVC 2017, Haifa, Israel, November 13-15, 2017, Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the 13th International Haifa Verification Conference, HVC 2017, held in Haifa, Israel in November 2017. The 13 revised full papers presented together with 4 poster and 5 tool demo papers were carefully reviewed and selected from 45 submissions. They are dedicated to advance the state of the art and state of the practice in verification and testing and are discussing future directions of testing and verification for hardware, software, and complex hybrid systems.", "num_citations": "1\n", "authors": ["1802"]}
{"title": "Model Counting of Monotone Conjunctive Normal Form Formulas with Spectra\n", "abstract": " Model counting is the #P problem of counting the number of satisfying solutions of a given propositional formula. Here we focus on a restricted variant of this problem, where the input formula is monotone (i.e., there are no negations). A monotone conjunctive normal form (CNF) formula is sufficient for modeling various graph problems, e.g., the vertex covers of a graph. Even for this restricted case, there is no known efficient approximation scheme. We show that the classical Spectra technique that is widely used in network reliability can be adapted for counting monotone CNF formulas. We prove that the proposed algorithm is logarithmically efficient for random monotone 2-CNF instances. Although we do not prove the efficiency of Spectra for k-CNF where k > 2, our experiments show that it is effective in practice for such formulas.", "num_citations": "1\n", "authors": ["1802"]}
{"title": "Answering queries with acyclic lineage expressions over probabilistic databases\n", "abstract": " This work extends the class of lineage expressions of queries over tuple independent probabilistic databases for which evaluation can be performed in PTIME. We define a new characterization of lineage expressions, called \u03b3-acyclic, and present a method to compute the probability of such expressions in PTIME. The method is based on the junction tree message passing algorithm and applies both to conjunctive queries without self joins and, under certain constraints, also to union of such queries.", "num_citations": "1\n", "authors": ["1802"]}
{"title": "Underapproximation for model-checking based on universal circuits\n", "abstract": " For two naturals m, n such that m< n, we show how to construct a circuit C with m inputs and n outputs, that has the following property: for some 0\u2a7d k\u2a7d m, the circuit defines a k-universal function. This means, informally, that for every subset K of k outputs, every possible valuation of the variables in K is reachable. Now consider a circuit M with n inputs that we wish to model-check. Connecting the inputs of M to the outputs of C gives us a new circuit M\u2032 with m inputs, that its original inputs have freedom defined by k. This is a very attractive feature for underapproximation in model-checking: on one hand the combined circuit has a smaller number of inputs, and on the other hand it is expected to find an error state fast if there is one. We show a random construction of a k-universal circuit that guarantees that k is very close to m, with an arbitrarily high probability. We also present a deterministic construction of such a circuit\u00a0\u2026", "num_citations": "1\n", "authors": ["1802"]}
{"title": "Device, System and Method of Underapproximated Model-Checking\n", "abstract": " Some demonstrative embodiments include devices, systems and/or methods of model checking. A method of checking a model having a first number of inputs may include, for example, automatically underapproximating the model by an underapproximated model having a second number of inputs, wherein the second number is smaller than the first number, and wherein automatically underapproximating comprises mapping the second number of inputs to the first number of inputs such that any possible combination of two or more values of any subset of two or more respective inputs of the first number of inputs is obtainable by assigning to each of one or more inputs of the second number of inputs a value of a predefined set of input values. Other embodiments are described and claimed.", "num_citations": "1\n", "authors": ["1802"]}
{"title": "Underapproximation for model-checking based on universal circuits (full version)\n", "abstract": " For two naturals m, n such that m< n, we show how to construct a circuit C with m inputs and n outputs, that has the following property: for some 0\u2264 k\u2264 m, the circuit defines a k-universal function. This means, informally, that for every subset K of k outputs, every possible valuation of the variables in K is reachable. Now consider a circuit M with n inputs that we wish to model-check. Connecting the inputs of M to the outputs of C gives us a new circuit M with m inputs, that its original inputs have freedom defined by k. This is a very attractive feature for underapproximation in model-checking: on one hand the combined circuit has a smaller number of inputs, and on the other hand it is expected to find an error state fast if there is one. We show a random construction of a k-universal circuit that guarantees that k is very close to m, with an arbitrarily high probability. We also present a deterministic construction of such a circuit, but here the value of k is smaller with respect to a fixed value of m. We report initial experimental results with bounded model checking of industrial designs (the method is equally applicable to unbounded model checking and to simulation), which shows mixed results. An interesting observation, however, is that in 13 out of 17 designs, setting m to be n/5 is sufficient to detect the bug. This is in contrast to other underapproximation that are based on reducing the number of inputs, which in most cases cannot detect the bug even with m= n/2.", "num_citations": "1\n", "authors": ["1802"]}
{"title": "Model Checking In-The-Loop\n", "abstract": " Model checkers for program verification have enjoyed considerable success in recent years. In the control systems domain, however, they suffer from an inability to account for the physical environment. For control systems, simulation is the most widely used approach for validating system designs. We present a new technique that uses a software model checker to perform a systematic simulation of the software implementation of a controller coupled with a continuous plant. Instead of performing a large set of independent simulations, our approach uses the model checking notion of state-space exploration by piecing together numerical simulations of the plant and transitions of the controller. Our implementation of this technique uses an explicit-state source-code model checker to analyze the software and the MATLAB/Simulink environment to model and simulate the plant. We present an illustrative example involving a supervisory controller for an unmanned aerial vehicle (UAV). We show that our technique is able to detect an error in the controller design.", "num_citations": "1\n", "authors": ["1802"]}
{"title": "Deciding Disjunctive Linear Arithmetic with SAT\n", "abstract": " Disjunctive Linear Arithmetic (DLA) is a major decidable theory that is supported by almost all existing theorem provers. The theory consists of Boolean combinations of predicates of the form , where the coefficients , the bound  and the variables  are of type Real (). We show a reduction to propositional logic from disjunctive linear arithmetic based on Fourier-Motzkin elimination. While the complexity of this procedure is not better than competing techniques, it has practical advantages in solving verification problems. It also promotes the option of deciding a combination of theories by reducing them to this logic. Results from experiments show that this method has a strong advantage over existing techniques when there are many disjunctions in the formula.", "num_citations": "1\n", "authors": ["1802"]}
{"title": "Advances in counterexample-guided abstraction refinement\n", "abstract": " This report is a collection of six articles on model checking in the abstraction/refinement framework. This framework is used by various techniques for tackling the state-space explosion problem that is frequently encountered in model checking.", "num_citations": "1\n", "authors": ["1802"]}
{"title": "Efficient decision procedures for validation\n", "abstract": " The dissertation focuses on two subjects in formal verification. The first subject is the development of methods and tools for the formal verification of compilers. We offer the Translation Validation approach, according to which the translation of the compiler is validated after each run. The evidence from applying translation validation to two compilers proves that this approach has some strong advantages over the more traditional formal verification of the compiler itself. The research included the development of several new techniques for handling industrial-size programs, among them a new decision procedure for equality logic.", "num_citations": "1\n", "authors": ["1802"]}
{"title": "Hardware and Software: Verification and Testing\n", "abstract": " These are the conference proceedings of the 13th Haifa Verification Conference (HVC), held on the IBM Research campus in Haifa (HRL), Israel, during November 13\u201315, 2017. HVC is an annual conference dedicated to advancing the state of the art in verification and testing. The conference provides a forum for researchers and practitioners from academia and industry to share their work, exchange ideas, and discuss the future directions of testing and verification for hardware, software, and complex hybrid systems. It is also an opportunity to view tool demos that are related to the scope of the conference.The first day of HVC 2017 was dedicated to tutorials. The conference itself was shortened to two days this year, which improved the acceptance ratio and raised the overall quality. This year, 34 full papers were submitted, out of which 13 were accepted. Further, six tool papers were submitted, out of which five were\u00a0\u2026", "num_citations": "1\n", "authors": ["1802"]}