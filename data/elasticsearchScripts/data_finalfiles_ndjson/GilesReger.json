{"title": "Quantified event automata: Towards expressive and efficient runtime monitors\n", "abstract": " Runtime verification is the process of checking a property on a trace of events produced by the execution of a computational system. Runtime verification techniques have recently focused on parametric specifications where events take data values as parameters. These techniques exist on a spectrum inhabited by both efficient and expressive techniques. These characteristics are usually shown to be conflicting - in state-of-the-art solutions, efficiency is obtained at the cost of loss of expressiveness and vice-versa. To seek a solution to this conflict we explore a new point on the spectrum by defining an alternative runtime verification approach. We introduce a new formalism for concisely capturing expressive specifications with parameters. Our technique is more expressive than the currently most efficient techniques while at the same time allowing for optimizations.", "num_citations": "165\n", "authors": ["1773"]}
{"title": "A tutorial on runtime verification\n", "abstract": " This tutorial presents an overview of the field referred as to runtime verification. Runtime Verification is the study of algorithms, data structures, and tools focused on analyzing executions of system. The performed analysis aims at improving the confidence in systems behavior, either by improving program understanding, or by checking conformance to specifications or algorithms. This chapter focuses specifically on checking execution traces against requirements formalized in terms of monitors. It is first shown on examples how such monitors can be written using aspect-oriented programming, exemplified by ASPECTJ. Subsequently four monitoring systems are illustrated on the same examples. The systems cover such formalisms as regular expressions, temporal logics, state machines, and rule-based programming, as well as the distinction between external and internal DSLs.", "num_citations": "161\n", "authors": ["1773"]}
{"title": "MarQ: monitoring at runtime with QEA\n", "abstract": " Runtime monitoring is the process of checking whether an execution trace of a running system satisfies a given specification. For this to be effective, monitors which run trace-checking algorithms must be efficient so that they introduce minimal computational overhead. We present the MarQ tool for monitoring properties expressed as Quantified Event Automata. This formalism generalises previous automata-based specification methods. MarQ extends the established parametric trace slicing technique and incorporates existing techniques for indexing and garbage collection as well as a new technique for optimising runtime monitoring: structural specialisations where monitors are generated based on structural characteristics of the monitored property. MarQ recently came top in two tracks in the 1st international Runtime Verification competition, showing that MarQ is one of the most efficient existing\u00a0\u2026", "num_citations": "106\n", "authors": ["1773"]}
{"title": "Playing with AVATAR\n", "abstract": " Modern first-order resolution and superposition theorem provers use saturation algorithms to search for a refutation in clauses derivable from the input clauses. On hard problems, this search space often grows rapidly and performance degrades especially fast when long and heavy clauses are generated. One approach that has proved successful in taming the search space is splitting where clauses are split into components with disjoint variables and the components are asserted in turn. This reduces the length and weight of clauses in the search space at the cost of keeping track of splitting decisions.                 This paper considers the new AVATAR (Advanced Vampire Architecture for Theories And Resolution) approach to splitting which places a SAT (or SMT) solver at the centre of the theorem prover and uses it to direct the exploration of the search space. Using such an approach also allows the\u00a0\u2026", "num_citations": "45\n", "authors": ["1773"]}
{"title": "Selecting the selection\n", "abstract": " Modern saturation-based Automated Theorem Provers typically implement the superposition calculus for reasoning about first-order logic with or without equality. Practical implementations of this calculus use a variety of literal selections and term orderings to tame the growth of the search space and help steer proof search. This paper introduces the notion of lookahead selection that estimates (looks ahead) the effect of selecting a particular literal on the number of immediate children of the given clause and selects to minimize this value. There is also a case made for the use of incomplete selection strategies that attempt to restrict the search space instead of satisfying some completeness criteria. Experimental evaluation in the Vampire theorem prover shows that both lookahead selection and incomplete selection significantly contribute to solving hard problems unsolvable by other methods.", "num_citations": "41\n", "authors": ["1773"]}
{"title": "New techniques in clausal form generation\n", "abstract": " In automated reasoning it is common that first-order formulas need to be translated into clausal normal form for proof search. The structure of this normal form can have a large impact on the performance of first-order theorem provers, influencing whether a proof can be found and how quickly.", "num_citations": "38\n", "authors": ["1773"]}
{"title": "Third international competition on runtime verification\n", "abstract": " We report on the Third International Competition on Runtime Verification (CRV-2016). The competition was held as a satellite event of the 16th International Conference on Runtime Verification (RV\u201916). The competition consisted of two tracks: offline monitoring of traces and online monitoring of Java programs. The intention was to also include a track on online monitoring of C programs but there were too few participants to proceed with this track. This report describes the format of the competition, the participating teams, the submitted benchmarks and the results. We also describe our experiences with transforming trace formats from other tools into the standard format required by the competition and report on feedback gathered from current and past participants and use this to make suggestions for the future of the competition.", "num_citations": "37\n", "authors": ["1773"]}
{"title": "Monitoring events that carry data\n", "abstract": " Very early runtime verification systems focused on monitoring what we can refer to as propositional events: just names of events. For this, finite state machines, standard regular expressions, or propositional temporal logics were sufficient formalisms for expressing properties. However, in practice there is a need for monitoring events that in addition carry data arguments. This adds complexity to both the property specification languages, and monitoring algorithms, which is reflected in the many alternative such approaches suggested in the literature. This chapter presents five different formalisms and monitoring approaches that support specifications with data, in order to illustrate the challenges and various solutions.", "num_citations": "36\n", "authors": ["1773"]}
{"title": "The Vampire and the FOOL\n", "abstract": " This paper presents new features recently implemented in the theorem prover Vampire, namely support for first-order logic with a first class boolean sort (FOOL) and polymorphic arrays. In addition to having a first class boolean sort, FOOL also contains if-then-else and let-in expressions. We argue that presented extensions facilitate reasoning-based program analysis, both by increasing the expressivity of first-order reasoners and by gains in efficiency.", "num_citations": "34\n", "authors": ["1773"]}
{"title": "A pattern-based approach to parametric specification mining\n", "abstract": " This paper presents a technique for using execution traces to mine parametric temporal specifications in the form of quantified event automata (QEA) - previously introduced as an expressive and efficient formalism for runtime verification. We consider a pattern-based mining approach that uses a pattern library to generate and check potential properties over given traces, and then combines successful patterns. By using predefined models to measure the tool's precision and recall we demonstrate that our approach can effectively and efficiently extract specifications in realistic scenarios.", "num_citations": "34\n", "authors": ["1773"]}
{"title": "AVATAR modulo theories\n", "abstract": " This paper introduces a new technique for reasoning with quantifiers and theories. Traditionally, first-order theorem provers (ATPs) are well suited to reasoning with first-order problems containing many quantifiers and", "num_citations": "31\n", "authors": ["1773"]}
{"title": "A combinator-based superposition calculus for higher-order logic\n", "abstract": " We present a refutationally complete superposition calculus for a version of higher-order logic based on the combinatory calculus. We also introduce a novel method of dealing with extensionality. The calculus was implemented in the Vampire theorem prover and we test its performance against other leading higher-order provers. The results suggest that the method is competitive.", "num_citations": "26\n", "authors": ["1773"]}
{"title": "Finding finite models in multi-sorted first-order logic\n", "abstract": " This work extends the existing MACE-style finite model finding approach to multi-sorted first-order logic. This existing approach iteratively assumes increasing domain sizes and encodes the related ground problem as a SAT problem. When moving to the multi-sorted setting each sort may have a different domain size, leading to an explosion in the search space. This paper focusses on methods to tame that search space. The key approach adds additional information to the SAT encoding to suggest which domains should be grown. Evaluation of an implementation of techniques in the Vampire theorem prover shows that they dramatically reduce the search space and that this is an effective approach to find finite models in multi-sorted first-order logic.", "num_citations": "25\n", "authors": ["1773"]}
{"title": "Induction in saturation-based proof search\n", "abstract": " Many applications of theorem proving, for example program verification and analysis, require first-order reasoning with both quantifiers and theories such as arithmetic and datatypes. There is no complete procedure for reasoning in such theories but the state-of-the-art in automated theorem proving is still able to reason effectively with real-world problems from this rich domain. In this paper we contribute to a missing part of the puzzle: automated induction inside a saturation-based theorem prover. Our goal is to incorporate lightweight automated induction in a way that complements the saturation-based approach, allowing us to solve problems requiring a combination of first-order reasoning, theory reasoning, and inductive reasoning. We implement a number of techniques and heuristics and evaluate them within the Vampire theorem prover. Our results show that these new techniques enjoy practical\u00a0\u2026", "num_citations": "24\n", "authors": ["1773"]}
{"title": "Restricted combinatory unification\n", "abstract": " First-order theorem provers are commonly utilised as backends to proof assistants. In order to improve efficiency, it is desirable that such provers can carry out some higher-order reasoning. In his 1991 paper, Dougherty proposed a combinatory unification algorithm for higher-order logic. The algorithm removes the need to deal with -binders and -renaming, making it attractive to implement in first-order provers. However, since publication it has garnered little interest due to a number of characteristics that make it unsuitable for a practical implementation. It fails to terminate on many trivial instances and requires polymorphism. We present a restricted version of Dougherty\u2019s algorithm that is incomplete, terminating and does not require polymorphism. Further, we describe its implementation in the Vampire theorem prover, including a novel use of a substitution tree as a filtering index for higher-order\u00a0\u2026", "num_citations": "23\n", "authors": ["1773"]}
{"title": "From first-order temporal logic to parametric trace slicing\n", "abstract": " Parametric runtime verification is the process of verifying properties of execution traces of (data carrying) events produced by a running system. This paper considers the relationship between two widely-used specification approaches to parametric runtime verification: trace slicing and first-order temporal logic. This work is a first step in understanding this relationship. We introduce a technique of identifying syntactic fragments of temporal logics that admit notions of sliceability. We show how to translate formulas in such fragments into automata with a slicing-based semantics. In exploring this relationship, the paper aims to allow monitoring techniques to be shared between the two approaches and initiate a wider effort to unify specification languages for runtime verification.", "num_citations": "22\n", "authors": ["1773"]}
{"title": "What is a trace? A runtime verification perspective\n", "abstract": " Runtime Monitoring or Verification deals with traces. In its most simple form a monitoring system takes a trace produced by a system and a specification of correct behaviour and checks if the trace conforms to the specification. More complex applications may introduce notions of feedback and reaction. The notion that unifies the field is that we can abstract the runtime behaviour of a system by an execution trace and check this for conformance. However, there is little uniform understanding of what a trace is. This is most keenly seen when comparing theoretical and practical work. This paper surveys the different notions of trace and reflects on the related issues.", "num_citations": "21\n", "authors": ["1773"]}
{"title": "Unification with abstraction and theory instantiation in saturation-based reasoning\n", "abstract": " We make a new contribution to the field by providing a new method of using SMT solvers in saturation-based reasoning. We do this by introducing two new inference rules for reasoning with non-ground clauses. The first rule utilises theory constraint solving (an SMT solver) to perform reasoning within a clause to find an instance where we can remove one or more theory literals. This utilises the power of SMT solvers for theory reasoning with non-ground clauses, reasoning which is currently achieved by the addition of often prolific theory axioms. The second rule is unification with abstraction where the notion of unification is extended to introduce constraints where theory terms may not otherwise unify. This abstraction is performed lazily, as needed, to allow the superposition theorem prover to make as much progress as possible without the search space growing too quickly. Additionally, the first rule can be\u00a0\u2026", "num_citations": "20\n", "authors": ["1773"]}
{"title": "Set of Support for Higher-Order Reasoning.\n", "abstract": " Higher-order logic (HOL) is utilised in numerous domains from program verification to the formalisation of mathematics. However, automated reasoning in the higher-order domain lags behind first-order automation. Many higher-order automated provers translate portions of HOL problems to first-order logic (FOL) and pass them to FOL provers. However, FOL provers are not optimised for dealing with these translations. One of the reasons for this is that the axioms introduced during the translation (eg those defining combinators) may combine with each other during proof search, deriving consequences of the axioms irrelevant to the goal. In this work we evaluate the extent to which this issue affects proof search and introduce heuristics based on the set-of-support strategy for minimising the effects. Our experiments are carried out within the Vampire theorem prover and show that limiting how axioms introduced during translation can improve proof search with higher-order problems.", "num_citations": "20\n", "authors": ["1773"]}
{"title": "Automata based monitoring and mining of execution traces\n", "abstract": " This thesis contributes work to the fields of runtime monitoring and specification mining. It develops a formalism for specifying patterns of behaviour in execution traces and defines techniques for checking these patterns in, and extracting patterns from, traces. These techniques represent an extension in the expressiveness of properties that can be efficiently and effectively monitored and mined. The behaviour of a computer system is considered in terms of the actions it performs, captured in execution traces. Patterns of behaviour, formally defined in trace specifications, denote the traces that the system should (or should not) exhibit. The main task this work considers is that of checking that the system conforms to the specification ie is correct. Additionally, trace specifications can be used to document behaviour to aid maintenance and development. However, formal specifications are often missing or incomplete\u00a0\u2026", "num_citations": "20\n", "authors": ["1773"]}
{"title": "Runtime verification logics a language design perspective\n", "abstract": " Runtime Verification is a light-weight approach to systems verification, where actual executions of a system are processed and analyzed using rigorous techniques. In this paper we shall narrow the term\u2019s definition to represent the commonly studied variant consisting of verifying that a single system execution conforms to a specification written in a formal specification language. Runtime verification (in this sense) can be used for writing test oracles during testing when the system is too complex for full formal verification, or it can be used during deployment of the system as part of a fault protection strategy, where corrective actions may be taken in case the specification is violated. Specification languages for runtime verification appear to differ from for example temporal logics applied in model checking, in part due to the focus on monitoring of events that carry data, and specifically due to the desire to relate\u00a0\u2026", "num_citations": "18\n", "authors": ["1773"]}
{"title": "Specification of parametric monitors\n", "abstract": " Specification-based runtime verification is a technique for monitoring program executions against specifications formalized in formal logic. Such logics are usually temporal in nature, capturing the relation between events occurring at different time points. A particular challenge in runtime verification is the elegant specification and efficient monitoring of streams of events that carry data, also referred to as parametric monitoring. This paper presents two parametric runtime verification systems representing two quite different approaches to the problem. qea (Quantified Event Automata) is a state machine approach based on trace-slicing, while LogFire is a rule-based approach based on the Rete algorithm, known from AI as being the basis for many rule systems. The presentation focuses on how easy it is to specify properties in the two approaches by specifying a collection of properties gathered during the 1st\u00a0\u2026", "num_citations": "18\n", "authors": ["1773"]}
{"title": "Automata-based Pattern Mining from Imperfect Traces\n", "abstract": " This paper considers automata-based pattern mining techniques for extracting specifications from runtime traces and suggests a novel extension that allows these techniques to work with so-called imperfect traces i.e. traces that do not exactly satisfy the intended specification of the system that produced them. We show that by taking a so-called edit-distance between an input trace and the language of a pattern we can extract specifications from imperfect traces and identify the parts of an input trace that do not satisfy the mined specification, thus aiding the identification and location of errors in programs.", "num_citations": "17\n", "authors": ["1773"]}
{"title": "The smt competition 2015\u20132018\n", "abstract": " The International Satisfiability Modulo Theories Competition is an annual competition between Satisfiability Modulo Theories (SMT) solvers. The 2018 edition of the competition was part of the FLoC Olympic Games, which comprised 14 competitions in various areas of computational logic. We report on the design and selected results of the SMT Competition during the last FLoC Olympiad, from 2015 to 2018. These competitions set several new records regarding the number of participants, number of benchmarks used, and amount of computation performed.", "num_citations": "16\n", "authors": ["1773"]}
{"title": "Set of Support for Theory Reasoning.\n", "abstract": " This paper describes initial experiments using the set of support strategy to improve how a saturation-based theorem prover performs theory reasoning with explicit theory axioms. When dealing with theories such as arithmetic, modern automated theorem provers often resort to adding explicit theory axioms, for example x+ y= y+ x. Reasoning with such axioms can be explosive. However, little has been done to explore methods that mitigate the negative impact of theory axioms on saturation-based reasoning. The set of support strategy requires that all inferences involve a premise with an ancestor in a so-called set of support, initially taken to be a subset of the input clauses, usually those corresponding to the goal. This leads to completely goal orientated reasoning but is incomplete for practical reasoning (eg in the presence of ordering constraints). The idea of this paper is to apply the set of support strategy to theory axioms only, and then to explore the effect of allowing some limited reasoning within this set. The suggested approach is implemented and evaluated within the Vampire theorem prover.", "num_citations": "16\n", "authors": ["1773"]}
{"title": "The Challenges of Evaluating a New Feature in Vampire.\n", "abstract": " This paper was originally called Playing with AVATAR and was meant to present and explore experimental results examining the usage of a new feature in Vampire: the AVATAR (Advanced Vampire Architecture for Theories and Resolution)[9] approach to splitting. The results and analysis from this original study have been extended and published elsewhere [6] and instead we use this space to briefly discuss the issues that arise when attempting to evaluate a new feature in Vampire. We suggest four different approaches to evaluating new features.", "num_citations": "16\n", "authors": ["1773"]}
{"title": "A neurally-guided, parallel theorem prover\n", "abstract": " We present a prototype of a neurally-guided automatic theorem prover for first-order logic with equality. The prototype uses a neural network trained on previous proof search attempts to evaluate subgoals based directly on their structure, and hence bias proof search toward success. An existing first-order theorem prover is employed to dispatch easy subgoals and prune branches which cannot be solved. Exploration of the search space is asynchronous with respect to both the evaluation network and the existing prover, allowing for efficient batched neural network execution and for natural parallelism within the prover. Evaluation on the MPTP dataset shows that the prover can improve with learning.", "num_citations": "14\n", "authors": ["1773"]}
{"title": "Dynamic strategy priority: Empower the strong and abandon the weak\n", "abstract": " Many modern automated theorem provers (eg CVC4 1 [1], E [8], iProver [2], Vampire [3]) rely on portfolio modes [7] utilising from tens to hundreds of distinct strategies, of which only a few might solve a hard problem quickly. Typically, a portfolio of strategies has a pre-defined order that the prover will execute the strategies in, running each until a strategy succeeds. Portfolios are important as, in practice, there is no best strategy ie it is uncommon that two hard problems are efficiently solvable by the same strategy. However, portfolio execution is not without problems: selecting the optimal ordering and time allocation is hard in general [6], and produces overly-rigid, brittle engineering when applied to specific domains, such as those found in TPTP [9]. Moreover, for any particular problem, some lengthy strategies that are successful on other problems are doomed to failure from the outset, but are left to run unchecked by the prover, wasting time that could be spent on more productive strategies. In this work we first demonstrate correlation between trends in dynamic properties of proof search, and the success or failure of a strategy. We then utilise this to implement strategy scheduling, prioritising those strategies most likely to succeed. This approach differs from previous work [4, 5, 8] which attempts to predict successful strategies a priori from static features of the input problem; instead we tip running strategies for success based on run-time features and use this information to make scheduling decisions. Initial experiments on Vampire produce a performant neural-network that achieves classification accuracy of 81%(\uc18c 2%).Obtaining and filtering\u00a0\u2026", "num_citations": "12\n", "authors": ["1773"]}
{"title": "A Report of RV-CuBES 2017.\n", "abstract": " RV-CuBES was an international workshop that took place alongside the 17th International Conference on Runtime Verification in Seattle during 13-16th September, 2017. The focus of the competition was to consider tools for Runtime Verification (RV). The acronym CuBES stands for Competitions, usability, Benchmarks, Evaluation, and Standardisation. The workshop consisted of poster presentations of tool overview papers and a discussion session of position papers. This report gives an overview of submissions and discussions.", "num_citations": "11\n", "authors": ["1773"]}
{"title": "Cooperating proof attempts\n", "abstract": " This paper introduces a pseudo-concurrent architecture for first-order saturation-based theorem provers with the eventual aim of developing it into a truly concurrent architecture. The motivation behind this architecture is two-fold. Firstly, first-order theorem provers have many configuration parameters and commonly utilise multiple strategies to solve problems. It is also common that one of these strategies will solve the problem quickly but it may have to wait for many other strategies to be tried first. The architecture we propose interleaves the execution of these strategies, increasing the likeliness that these \u2018quick\u2019 proofs will be found. Secondly, previous work has established the existence of a synergistic effect when allowing proof attempts to communicate by sharing information about their inferences or clauses. The recently introduced AVATAR approach to splitting uses a SAT solver to explore the clause\u00a0\u2026", "num_citations": "9\n", "authors": ["1773"]}
{"title": "Explaining violations of properties in control-flow temporal logic\n", "abstract": " Runtime Verification is the process of deciding whether a run of a program satisfies a given property. This work considers the more challenging problem of explaining why a run does or does not satisfy the property. We look at this problem in the context of CFTL, a low-level temporal logic. Our main contribution is a method for reconstructing representative execution paths, separating them into good and bad paths, and producing partial parse trees explaining their differences. This requires us to extend CFTL and our second contribution is a partial semantics used to identify the first violating observation in a trace. This is extended with a notion of severity of violation, allowing us to handle real-time properties sensitive to small timing variations. These techniques are implemented as an extension to the publicly available VyPR2 tool. Our work is motivated by results obtained applying VyPR2 to a web service on\u00a0\u2026", "num_citations": "8\n", "authors": ["1773"]}
{"title": "Checkable Proofs for First-Order Theorem Proving.\n", "abstract": " Inspired by the success of the DRAT proof format for certification of boolean satisfiability (SAT), we argue that a similar goal of having unified automatically checkable proofs should be sought by the developers of automatic first-order theorem provers (ATPs). This would not only help to further increase assurance about the correctness of prover results, but would also be indispensable for tools which rely on ATPs, such as \u201chammers\u201d employed within interactive theorem provers. The current situation, represented by the TSTP format, is unsatisfactory, because this format does not have a standardised semantics and thus cannot be checked automatically. Providing such semantics, however, is a challenging endeavour. One would ideally like to have a proof format which covers only-satisfiability-preserving operations such as Skolemisation and is versatile enough to encompass various proving methods (ie not just superposition) or is perhaps even open-ended towards yet to be conceived methods or at least easily extendable in principle. Going beyond pure first-order logic to theory reasoning in the style of SMT, or beyond proofs to certification of satisfiability are further interesting challenges. Although several projects have already provided partial solutions in this direction, we would like to use the opportunity of ARCADE to further promote the idea and gather critical mass needed for its satisfactory realisation.", "num_citations": "8\n", "authors": ["1773"]}
{"title": "Better Proof Output for Vampire.\n", "abstract": " Vampire produces highly usable and informative proofs, but now they are even better and this paper explains how. It is important that the proofs produced by automated theorem provers are both understandable and machine checkable. Producing something that satisfies both of these goals is challenging, especially when dealing with complex steps performed by the solver. The main areas where proof output has been improved for understanding include (i) introduction of new symbols (such as Skolem functions) in preprocessing,(ii) representation of unifiers (for example, in resolution steps), and (iii) presentation of AVATAR proofs. These improvements will be illustrated via a number of examples. For checkable proofs Vampire provides a mode that outputs the proof as a number of individual (TPTP) problems that can be independently checked. This process is explained and illustrated with examples.", "num_citations": "8\n", "authors": ["1773"]}
{"title": "Suggesting edits to explain failing traces\n", "abstract": " Runtime verification involves checking whether an execution trace produced by a running system satisfies a specification. However, a simple \u2018yes\u2019 or \u2018no\u2019 answer may not be sufficient; often we need to understand why a violation occurs. This paper considers how computing the edit-distance between a trace and a specification can explain violations by suggesting correcting edits to the trace. By including information about the code location producing events in the trace, this method can highlight sources of bugs and suggest potential fixes.", "num_citations": "8\n", "authors": ["1773"]}
{"title": "A Knuth-Bendix-like ordering for orienting combinator equations\n", "abstract": " We extend the graceful higher-order basic Knuth-Bendix order (KBO) of Becker et al. to an ordering that orients combinator equations left-to-right. The resultant ordering is highly suited to parameterising the first-order superposition calculus when dealing with the theory of higher-order logic, as it prevents inferences between the combinator axioms. We prove a number of desirable properties about the ordering including it having the subterm property for ground terms, being transitive and being well-founded. The ordering fails to be a reduction ordering as it lacks compatibility with certain contexts. We provide an intuition of why this need not be an obstacle when using it to parameterise superposition.", "num_citations": "7\n", "authors": ["1773"]}
{"title": "VyPR2: a framework for runtime verification of Python web services\n", "abstract": " Runtime Verification (RV) is the process of checking whether a run of a system holds a given property. In order to perform such a check online, the algorithm used to monitor the property must induce minimal overhead. This paper focuses on two areas that have received little attention from the RV community: Python programs and web services. Our first contribution is the VyPR runtime verification tool for single-threaded Python programs. The tool handles specifications in our, previously introduced, Control-Flow Temporal Logic (CFTL), which supports the specification of state and time constraints over runs of functions. VyPR minimally (in terms of reachability) instruments the input program with respect to a CFTL specification and then uses instrumentation information to optimise the monitoring algorithm. Our second contribution is the lifting of VyPR to the web service setting, resulting in the VyPR2 tool. We first describe the necessary modifications to the architecture of VyPR, and then describe our experience applying VyPR2 to a service that is critical to the physics reconstruction pipeline on the CMS Experiment at CERN.", "num_citations": "7\n", "authors": ["1773"]}
{"title": "Measuring progress to predict success: Can a good proof strategy be evolved\n", "abstract": " One of the main parameters of the superposition calculus employed by Automated Theorem Provers (ATPs) is the simplification ordering, and the choice of an ordering can have a huge impact on the success of a theorem proving attempt. However, it is difficult to choose a good ordering in advance and ATPs typically provide only a few heuristical schemes for determining an ordering from a large space of possibilities. The aim of this work is to establish to what extent the space of possible orderings can be better utilised during the construction of new successful proving strategies.There is a well known principle in automated deduction which states that a strategy that leads to a slowly growing search space will likely be more successful (at finding a proof in reasonable time) than a strategy that leads to a rapidly growing one. We propose to employ this principle and search for a strategy which, for a given problem, minimises the number of derived clauses after a certain number of iterations of the saturation loop. Focusing on the search for a good ordering as a simplifying restriction on the set of available strategies, we experimentally investigate the practical potential of this idea.", "num_citations": "7\n", "authors": ["1773"]}
{"title": "Directed graph networks for logical reasoning\n", "abstract": " We introduce a neural model for approximate logical reasoning based upon learned bi-directional graph convolutions on directed syntax graphs. The model avoids inflexible inductive bias found in some previous work on this domain, while still producing competitive results on a benchmark propositional entailment dataset. We further demonstrate the generality of our work in a first-order context with a premise selection task. Such models have applications for learned functions of logical data, such as in guiding theorem provers.", "num_citations": "6\n", "authors": ["1773"]}
{"title": "Old or heavy? Decaying gracefully with age/weight shapes\n", "abstract": " Modern saturation theorem provers are based on the given-clause algorithm, which iteratively selects new clauses to process. This clause selection has a large impact on the performance of proof search and has been the subject of much folklore. The standard approach is to alternate between selecting the oldest clause and the lightest clause with a fixed, but configurable age/weight ratio (AWR). An optimal fixed value of this ratio is shown to produce proofs significantly more quickly on a given problem, and further that varying AWR during proof search can improve upon a fixed ratio. Several new modes for the Vampire prover which vary AWR according to a \u201cshape\u201d during proof search are developed based on these observations. The modes solve a number of new problems in the TPTP benchmark set.", "num_citations": "6\n", "authors": ["1773"]}
{"title": "Specification of temporal properties of functions for runtime verification\n", "abstract": " Runtime verification (RV) is the process of checking whether a run of a computer system satisfies a specification. RV techniques often utilise specification languages that are (i) reasonably expressive, and (ii) relatively abstract (ie they operate on a level of abstraction separating them from the monitored system). Inspired by the problem of monitoring systems involved in processing data generated by the high energy physics experiments at CERN, we propose a specification language, Control-Flow Temporal Logic (CFTL), whose distinguishing characteristic is its tight coupling with the control-flow of the programs for which it is used to write specifications. The coupling admits an efficient monitoring algorithm and optimised instrumentation techniques based on static analysis.", "num_citations": "6\n", "authors": ["1773"]}
{"title": "A shared challenge in behavioural specification (Dagstuhl seminar 17462)\n", "abstract": " This report documents the program and the outcomes of Dagstuhl Seminar 17462\" A Shared Challenge in Behavioural Specification\". The seminar considered the issue of behavioral specification with a focus on its usage in Runtime Verification. The seminar was motivated by the observations that, whilst the field of Runtime Verification is becoming more mature, there is a lack of common specification language, in the main part due to the rich setting allowing for highly expressive languages. The aim of the Seminar was to shed light on the similarities and differences between the different existing languages, and specifically, suggest directions for future collaboration and research. The seminar consisted of two talk sessions, two working group sessions, and a feedback and reflection session. Working group topics were suggested and agreed in response to points raised in talks. One significant outcome was the proposal of a shared challenge project in which different Runtime Verification approaches can be compared, as outlined in one of the working group reports.", "num_citations": "6\n", "authors": ["1773"]}
{"title": "An overview of MarQ\n", "abstract": " MarQ is a runtime monitoring tool for specifications written as quantified event automata, an expressive automata-based specification language based on the notion of parametric trace slicing. MarQ has performed well in the runtime verification competition and implements advanced indexing and redundancy elimination techniques. This overview describes the basic structure and functionality provided by MarQ and gives a brief description of how to use the tool.", "num_citations": "6\n", "authors": ["1773"]}
{"title": "The Uses of SAT Solvers in Vampire.\n", "abstract": " Reasoning in a saturation-based first-order theorem prover is generally expensive, involving complex term-indexing data structures and inferences such as subsumption resolution whose (worst case) running time is exponential in the length of the clause. In contrast, SAT solvers are very cheap, being able to solve large problems quickly and with relatively little memory overhead. Consequently, utilising this cheap power within Vampire to carry out certain tasks has proven highly successful. We give an overview of the different ways in which SAT solvers are utilised within Vampire and discuss ways in which this usage could be extended.", "num_citations": "6\n", "authors": ["1773"]}
{"title": "lazyCoP 0.1\n", "abstract": " We describe lazyCoP, a fully-automatic theorem prover for first-order logic with equality. The system implements a connection-tableau calculus with a specific variant of ordered paramodulation inference rules, rather than the usual preprocessing approaches. We explore practical aspects and refinements of this calculus. The system also implements fully-parallel proof search and support for the integration of learned search heuristics.", "num_citations": "5\n", "authors": ["1773"]}
{"title": "A Polymorphic Vampire\n", "abstract": " We have modified the Vampire theorem prover to support rank-1 polymorphism. In this paper we discuss the changes required to enable this and compare the performance of polymorphic Vampire against other polymorphic provers. We also compare its performance on monomorphic problems against standard Vampire. Finally, we discuss how polymorphism can be used to support theory reasoning and present results related to this.", "num_citations": "5\n", "authors": ["1773"]}
{"title": "What is the point of an SMT-LIB problem\n", "abstract": " Many areas of automated reasoning are goal-oriented ie the aim is to prove a goal from a set of axioms. Some methods, including the method of saturation-based reasoning explored in this paper, do not rely on having an explicit goal but can employ specific heuristics if one is present. SMTLIB problems do not record a specific goal, meaning that we cannot straightforwardly employ goaloriented proof search heuristics. In this work we examine methods for identifying the potential goal in an SMT-LIB problem and evaluate (using the Vampire theorem prover) whether this can help theorem provers (using saturation-based proof search). We also discuss (very broadly) where SMT solvers could make use of goal information.", "num_citations": "5\n", "authors": ["1773"]}
{"title": "Testing a saturation-based theorem prover: Experiences and challenges\n", "abstract": " This paper attempts to address the question of how best to assure the correctness of saturation-based automated theorem provers using our experience with developing the theorem prover Vampire. We describe the techniques we currently employ to ensure that Vampire is correct and use this to motivate future challenges that need to be addressed to make this process more straightforward and to achieve better correctness guarantees.", "num_citations": "5\n", "authors": ["1773"]}
{"title": "Instantiation and Pretending to be an SMT Solver with Vampire\n", "abstract": " This paper aims to do two things. Firstly, it discusses how the VAMPIRE automatic theorem prover both succeeds and fails at pretending to be an SMT solver. We discuss how Vampire reasons with problems containing both quantification and theories, the limitations this places on what it can do, and the advantages this provides over the standard SMT approach. Secondly, it focuses on the problem of instantiation of quantified formulas and asks whether VAMPIRE needs it (it does) and whether it can directly borrow techniques from SMT solving (maybe).", "num_citations": "5\n", "authors": ["1773"]}
{"title": "Rule-based runtime verification in a multicore system setting\n", "abstract": " This project examines whether rule-based runtime verification can benefit from being used within a multicore system setting, and if so how. This chapter presents the motivations behind this project, the overall aims and objectives of the project and the project\u2019s achievements, finishing with an overview of the rest of this document.", "num_citations": "5\n", "authors": ["1773"]}
{"title": "PerfCI: a toolchain for automated performance testing during continuous integration of Python projects\n", "abstract": " Software performance testing is an essential quality assurance mechanism that can identify optimization opportunities. Automating this process requires strong tool support, especially in the case of Continuous Integration (CI) where tests need to run completely automatically and it is desirable to provide developers with actionable feedback. A lack of existing tools means that performance testing is normally left out of the scope of CI. In this paper, we propose a toolchain - PerfCI - to pave the way for developers to easily set up and carry out automated performance testing under CI. Our toolchain is based on allowing users to (1) specify performance testing tasks, (2) analyze unit tests on a variety of python projects ranging from scripts to full-blown flask-based web services, by extending a performance analysis framework (VyPR) and (3) evaluate performance data to get feedback on the code. We demonstrate the\u00a0\u2026", "num_citations": "4\n", "authors": ["1773"]}
{"title": "Analysing the Performance of Python-Based Web Services with the VyPR Framework\n", "abstract": " In this tutorial paper, we present the current state of VyPR, a framework for the performance analysis of Python-based web services. We begin by summarising our theoretical contributions which take the form of an engineer-friendly specification language; instrumentation and monitoring algorithms; and an approach for explanation of property violations. We then summarise the VyPR ecosystem, which includes an intuitive library for writing specifications and powerful tools for analysing monitoring results. We conclude with a brief description of how VyPR was used to improve our understanding of the performance of a critical web service at the CMS Experiment at CERN.", "num_citations": "3\n", "authors": ["1773"]}
{"title": "Reinforced external guidance for theorem provers\n", "abstract": " We introduce a reinforcement learning environment for deriving useful \u201clemma\u201d facts to aid existing automated theorem provers: agents receive reward for making deductions which reduce system effort. This forms a challenging reinforcement task with applications for practical theorem proving. We present and train an exemplar deep neural agent for the environment and demonstrate deduction of helpful formulae for unseen, harder problems once trained on similar, easy problems. The environment is fully general and can accommodate any automated theorem prover, deduction system or reinforcement algorithm.", "num_citations": "3\n", "authors": ["1773"]}
{"title": "Boldly going where no prover has gone before\n", "abstract": " I argue that the most interesting goal facing researchers in automated reasoning is being able to solve problems that cannot currently be solved by existing tools and methods. This may appear obvious, and is clearly not an original thought, but focusing on this as a primary goal allows us to examine other goals in a new light. Many successful theorem provers employ a portfolio of different methods for solving problems. This changes the landscape on which we perform our research: solving problems that can already be solved may not improve the state of the art and a method that can solve a handful of problems unsolvable by current methods, but generally performs poorly on most problems, can be very useful. We acknowledge that forcing new methods to compete against portfolio solvers can stifle innovation. However, this is only the case when comparisons are made at the level of total problems solved. We propose a movement towards focussing on unique solutions in evaluation and competitions i.e. measuring the potential contribution to a portfolio solver. This state of affairs is particularly prominent in first-order logic, which is undecidable. When reasoning in a decidable logic there can be a focus on optimising a decision procedure and measuring average solving times. But in a setting where solutions are difficult to find, average solving times lose meaning, and whilst improving the efficiency of a technique can move potential solutions within acceptable time limits, in general, complementary strategies may be more successful.", "num_citations": "3\n", "authors": ["1773"]}
{"title": "Smt-comp 2019\n", "abstract": " \u2022 Incremental Track (previously: Application Track)\u25e6 multiple check-sat and push/pop commands\u25e6 solvers are executed on benchmarks via trace executor\u25e6 New selection of benchmarks\u25e6 New keep benchmarks with first check-sat status unknown\u25e6 New execute solver beyond first status unknown check-sat call\u25e6 time limit: 2400s (40 min)", "num_citations": "3\n", "authors": ["1773"]}
{"title": "Incremental solving with Vampire\n", "abstract": " Both SMT and SAT solvers can be used incrementally. This can be useful in lots of applications. Indeed, internally, Vampire uses both Minisat and Z3 incrementally. In this paper, we explore how VAMPIRE could be used incrementally. There are two forms of incremental solving. The first is where a solver is provided with formulas from a problem one by one with consistency being checked at certain points. The second more general form is where stack operations are used to create different solving contexts. We explore both ideas and show how they can be achieved within VAMPIRE. We argue that the second approach may be more suited to VAMPIRE as it allows for the incremental solving of unsatisfiable problems (whereas the first assumes a series of satisfiable problems) and the use of different solving contexts allows VAMPIRE to make use of incomplete proof search strategies. For the first approach, it will be necessary to restrict preprocessing steps to ensure completeness when additional formulas are added. For the second approach, we can make use of clauses labelled with assertions and take advantage of AVATAR to keep track of the stack information.", "num_citations": "3\n", "authors": ["1773"]}
{"title": "Higher-order reasoning vampire style\n", "abstract": " Higher-order logic (HOL) is utilised in numerous domains from program verification to the formalisation of mathematics. However, automated reasoning in the higher-order domain lags behind first-order automation. Many higher-order automated provers translate portions of HOL problems to first-order logic (FOL) and pass them to FOL provers. However, FOL provers are not optimised for dealing with these translations. We extend the Vampire automated theorem prover with special inference rules to facilitate efficient reasoning with translated HOL problems. We present these inferences and explore preliminary results on their experimental performance compared to translations using axioms and to an automated HOL prover.", "num_citations": "3\n", "authors": ["1773"]}
{"title": "Considering typestate verification for quantified event automata\n", "abstract": " This paper discusses how the existing static analyses developed for typestate properties may be extended to a more expressive class of properties expressible by a specification formalism originally developed for runtime verification. The notion of typestate was introduced as a refinement of the notion of type and captures the allowed operations in certain contexts (states) as a subset of those operations allowed on the type. Typestates therefore represent per-object safety properties. There exist effective static analysis techniques for checking typestate properties and this has been an area of research since typestates were first introduced in 1986. It has already been observed that common properties monitored in runtime verification activities take the form of typestate properties. Additionally, the notion of typestate has been extended to reflect the more expressive properties seen in this area and additional\u00a0\u2026", "num_citations": "3\n", "authors": ["1773"]}
{"title": "Global Subsumption Revisited (Briefly).\n", "abstract": " Global subsumption is an existing simplification technique for saturation-based first-order theorem provers. The general idea is that we can replace a clause C by its subclause D if D follows from the initial problem as D will subsume C. The effectiveness of the technique comes from a cheap, global approach for (incompletely) checking whether D is a consequence of the initial problem. The idea is to produce and maintain a set S of ground clauses that follow from the input (eg grounded versions of all derived clauses) and to check whether a grounding of D follows from this set. As this is now a propositional problem this check can be performed by a SAT solver, making it efficient. In this paper we review the global subsumption technique and pose a number of questions related to the practical implementation of global subsumption and possible variations of the approach. We consider, for example, which groundings to place in S, how to select the subclause (s) D to check, how to integrate this technique with the AVATAR approach and whether it makes sense to replace the SAT solver with an SMT solver. This discussion takes place within the context of the Vampire theorem prover.", "num_citations": "3\n", "authors": ["1773"]}
{"title": "Making Theory Reasoning Simpler\n", "abstract": " Reasoning with quantifiers and theories is at the core of many applications in program analysis and verification. Whilst the problem is undecidable in general and hard in practice, we have been making large pragmatic steps forward. Our previous work proposed an instantiation rule for theory reasoning that produced pragmatically useful instances. Whilst this led to an increase in performance, it had its limitations as the rule produces ground instances which (i) can be overly specific, thus not useful in proof search, and (ii) contribute to the already problematic search space explosion as many new instances are introduced. This paper begins by introducing that specifically addresses these two concerns as it produces general solutions and it is a simplification rule, ie it replaces an existing clause by a \u2018simpler\u2019one. Encouraged by initial success with this new rule, we performed an experiment to identify further common cases where the complex structure of theory terms blocked existing methods. This resulted in four further simplification rules for theory reasoning. The resulting extensions are implemented in the Vampire theorem prover and evaluated on SMT-LIB, showing that the new extensions result in a considerable increase in the number of problems solved, including 90 problems unsolved by state-of-the-art SMT solvers.", "num_citations": "2\n", "authors": ["1773"]}
{"title": "Automated Theorem Proving, Fast and Slow\n", "abstract": " State-of-the-art automated theorem provers explore large search spaces with carefully-engineered routines, but do not learn from past experience as human mathematicians can. Unfortunately, machine-learned heuristics for theorem proving are typically either fast or accurate, not both. Therefore, systems must make a tradeoff between the quality of heuristic guidance and the reduction in inference rate required to use it. We present a system that is completely insulated from heuristic overhead, allowing the use of even deep neural networks with no measurable reduction in inference rate. Given 10 seconds to find proofs in a corpus of mathematics, the system improves from 64% to 70% when trained on its own proofs.", "num_citations": "2\n", "authors": ["1773"]}
{"title": "Towards an efficient architecture for intelligent theorem provers\n", "abstract": " High-performance automated theorem provers for first-order logic (eg CVC4 [2], E [11], iProver [6], Vampire [7]) include hand-coded heuristics to guide proof search, often exposed as individual prover options. These heuristics perform well, but have a number of disadvantages including a lack of generality over problems (necessitating portfolio modes [9, 10]), inability to learn from experience, and maintenance overhead. There is therefore interest in employing machine-learning techniques to guide proof search in automatic theorem provers, with approaches such as FEMaLeCoP [5], ENIGMA [4], or Deep Network Guided Proof Search [8](DNGPS).", "num_citations": "2\n", "authors": ["1773"]}
{"title": "Runtime Verification: 17th International Conference, RV 2017, Seattle, WA, USA, September 13-16, 2017, Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the 17th International Conference on Runtime Verification, RV 2017, held in Seattle, WA, USA, in September 2017. The 18 revised full papers presented together with 3 invited presentations, 4 short papers, 5 tool papers, and 3 tutorials, were carefully reviewed and selected from 58 submissions. The RV conference is concerned with all aspects of monitoring and analysis of hardware, software and more general system executions. Runtime verification techniques are lightweight techniques to assess correctness, reliability, and robustness; these techniques are significantly more powerful and versatile than conventional testing, and more practical than exhaustive formal verification.", "num_citations": "2\n", "authors": ["1773"]}
{"title": "A story of parametric trace slicing, garbage and static analysis\n", "abstract": " This paper presents a proposal (story) of how statically detecting unreachable objects (in Java) could be used to improve a particular runtime verification approach (for Java), namely parametric trace slicing. Monitoring algorithms for parametric trace slicing depend on garbage collection to (i) cleanup data-structures storing monitored objects, ensuring they do not become unmanageably large, and (ii) anticipate the violation of (non-safety) properties that cannot be satisfied as a monitored object can no longer appear later in the trace. The proposal is that both usages can be improved by making the unreachability of monitored objects explicit in the parametric property and statically introducing additional instrumentation points generating related events. The ideas presented in this paper are still exploratory and the intention is to integrate the described techniques into the MarQ monitoring tool for quantified event automata.", "num_citations": "2\n", "authors": ["1773"]}
{"title": "A Tutorial on Runtime Verification\n", "abstract": " This tutorial presents an overview of the field referred as to runtime verification. Runtime Verification is the study of algorithms, data structures, and tools focused on analyzing executions of systems. The performed analysis aims at improving the confidence in systems behavior, either by improving program understanding, or by checking conformance to specifications or algorithms. This chapter focuses specifically on checking execution traces against requirements formalized in terms of monitors. It is first shown on examples how such monitors can be written using aspect-oriented programming, exemplified by ASPECTJ. Subsequently four monitoring systems are illustrated on the same examples. The systems cover such formalisms as regular expressions, temporal logics, state machines, and rule-based programming, as well as the distinction between external and internal DSLs.", "num_citations": "2\n", "authors": ["1773"]}
{"title": "lazyCoP: Lazy Paramodulation Meets Neurally Guided Search\n", "abstract": " State-of-the-art automated theorem provers explore large search spaces with carefully-engineered routines, but most do not learn from past experience as human mathematicians can. Unfortunately, machine-learned heuristics for theorem proving are typically either fast or accurate, not both. Therefore, systems must make a tradeoff between the quality of heuristic guidance and the reduction in inference rate required to use it. We present a system (lazyCoP) based on lazy paramodulation that is completely insulated from heuristic overhead, allowing the use of even deep neural networks with no measurable reduction in inference rate. Given 10\u00a0s to find proofs in a corpus of mathematics, the system improves from 64% to 70% when trained on its own proofs.", "num_citations": "1\n", "authors": ["1773"]}
{"title": "Directed graph networks for logical entailment\n", "abstract": " We introduce a neural model for approximate logical reasoning based upon learned bi-directional graph convolutions on directed syntax graphs. The model avoids inexible inductive bias found in some previous work on this domain, while still producing competitive results on a benchmark propositional entailment dataset. We further demonstrate the generality of our work in a rst-order context with a premise selection task. Such models have applications for learned functions of logical data, such as in guiding theorem provers.", "num_citations": "1\n", "authors": ["1773"]}
{"title": "Autoencoding TPTP\n", "abstract": " Extracting features from problem files is a prerequisite in learning systems for automatic theorem proving, notably for strategy creation and scheduling. Such manually-designed features are crucial in enabling machine learning algorithms to help solve otherwise-difficult problems. We propose a neural autoencoder approach for problem sets (allowing automatic feature extraction), and aim to show that the learned features are complementary to human-designed problem features. Learned features may also shed some light on the structure and behaviour of problem sets frequently-used in the community. The TPTP problem set is used as a well-known running example.", "num_citations": "1\n", "authors": ["1773"]}
{"title": "Symmetry avoidance in MACE-style finite model finding\n", "abstract": " This work considers the MACE-style approach to finite model finding for (multi-sorted) first-order logic. This existing approach iteratively assumes increasing domain sizes and encodes the corresponding model existence problem as a SAT problem. The original MACE tool and its successors have considered techniques for avoiding introducing symmetries in the resulting SAT problem, but this has never been the focus of the previous work and has not received concentrated attention. In this work we formalise the symmetry avoiding problem, characterise the notion of a sound symmetry breaking heuristic, propose a number of such heuristics and evaluate them experimentally with an implementation in the Vampire theorem prover. Our results demonstrate that these new heuristics improve performance on a number of benchmarks taken from SMT-LIB and TPTP. Finally, we show that direct symmetry breaking\u00a0\u2026", "num_citations": "1\n", "authors": ["1773"]}
{"title": "Specification of State and Time Constraints for Runtime Verification of Functions\n", "abstract": " Techniques for runtime verification often utilise specification languages that are (i) reasonably expressive, and (ii) relatively abstract (i.e. they operate on a level of abstraction that separates them from the system being monitored). Inspired by the problem of monitoring systems involved in processing data generated by the high energy physics experiments at CERN, this report proposes a specification language, Control Flow Temporal Logic (CFTL), whose distinguishing characteristic is its tight coupling with the control flow of the programs for which it is used to write specifications. This coupling leads to a departure from the typically high level of abstraction used by most temporal logics. The remaining contributions are a static-analysis based instrumentation process, which is specific to CFTL and its formulas' structure, and a monitoring algorithm. The report concludes with analyses of CFTL and its monitoring algorithm when applied to a number of example programs.", "num_citations": "1\n", "authors": ["1773"]}
{"title": "Reinforcement-Learned Input for Saturation Provers\n", "abstract": " Many of today\u2019s best-performing automatic theorem provers (ATPs) for first-order logic rely on saturation algorithms. However, to date the most successful work on applying machine-learned guidance to ATPs relies on tableaux methods, as these are friendlier toward machine learning algorithms. We describe a reinforcement-learning system which selectively infers new clauses from an input problem. The system (in progress) is rewarded if generated clauses are subsequently used by the first-order saturation prover VAMPIRE in a proof. In this way, the system learns to generate new clauses which are important in the proof search, but might otherwise not be selected for some time in Vampire\u2019s proof search algorithm. The system is implemented via Q-learning, with a graph neural net processing the structure of the clause set acting as an approximator.", "num_citations": "1\n", "authors": ["1773"]}