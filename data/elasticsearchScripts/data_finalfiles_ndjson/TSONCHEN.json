{"title": "Adaptive random testing\n", "abstract": " In this paper, we introduce an enhanced form of random testing called Adaptive Random Testing. Adaptive random testing seeks to distribute test cases more evenly within the input space. It is based on the intuition that for non-point types of failure patterns, an even spread of test cases is more likely to detect failures using fewer test cases than ordinary random testing. Experiments are performed using published programs. Results show that adaptive random testing does outperform ordinary random testing significantly (by up to as much as 50%) for the set of programs under study. These results are very encouraging, providing evidences that our intuition is likely to be useful in improving the effectiveness of random testing.", "num_citations": "522\n", "authors": ["310"]}
{"title": "A new heuristic for test suite reduction\n", "abstract": " A testing objective has to be defined in testing a program. A test suite is then constructed to satisfy the testing objective. The constructed test suite contains redundancy when some of its proper subsets can still satisfy the same testing objective. Since the costs of executing test cases and maintaining a test suite for regression testing may be expensive, the problem of test suite reduction arises. This paper proposes a heuristic towards the optimization of a test suite.", "num_citations": "259\n", "authors": ["310"]}
{"title": "In black and white: an integrated approach to class-level testing of object-oriented programs\n", "abstract": " Because of the growing importance of object-oriented programming, a number of testing strategies have been proposed. They are based either on pure black-box or white-box techniques. We propose in this article a methodology to integrate the black- and white-box techniques. The black-box technique is used to select test cases. The white-box technique is mainly applied to determine whether two objects resulting from the program execution of a test care are observationally equivalent. It is also used to select test cases in some situations. We define the concept of a fundamental pair as a pair of equivalent terms that are formed by replacing all the variables on both sides of an axiom by normal forms. We prove that an implementation is consistent with respect to all equivalent terms if   and only if it is consistent with respect to all fundamental pairs. In other words, the testing coverage of fundamental pairs is as good\u00a0\u2026", "num_citations": "221\n", "authors": ["310"]}
{"title": "TACCLE: a methodology for object-oriented software testing at the class and cluster levels\n", "abstract": " Object-oriented programming consists of several different levels of abstraction, namely, the algorithmic level, class level, cluster level, and system level. The testing of object-oriented software at the algorithmic and system levels is similar to conventional program testing. Testing at the class and cluster levels poses new challenges. Since methods and objects may interact with one another with unforeseen combinations and invocations, they are much more complex to simulate and test than the hierarchy of functional calls in conventional programs. In this paper, we propose a methodology for object-oriented software testing at the class and cluster levels. In class-level testing, it is essential to determine whether objects produced from the execution of implemented systems would preserve the properties defined by the specification, such as behavioral equivalence and nonequivalence. Our class-level testing\u00a0\u2026", "num_citations": "213\n", "authors": ["310"]}
{"title": "Dividing strategies for the optimization of a test suite\n", "abstract": " Use of the divide-and-conquer approach to optimize a test suite does not guarantee that the optimal solutions could be re-constructed from those of the sub-problems. This paper investigates how the re-construction could be achieved with two dividing strategies. Results of a simulation study are also reported.", "num_citations": "198\n", "authors": ["310"]}
{"title": "On the expected number of failures detected by subdomain testing and random testing\n", "abstract": " We investigate the efficacy of subdomain testing and random testing using the expected number of failures detected (the E-measure) as a measure of effectiveness. Simple as it is, the E-measure does provide a great deal of useful information about the fault detecting capability of testing strategies. With the E-measure, we obtain new characterizations of subdomain testing, including several new conditions that determine whether subdomain testing is more or less effective than random testing. Previously, the efficacy of subdomain testing strategies has been analyzed using the probability of detecting at least one failure (the P-measure) for the special case of disjoint subdomains only. On the contrary, our analysis makes use of the E-measure and considers also the general case in which subdomains may or may not overlap. Furthermore, we discover important relations between the two different measures. From these\u00a0\u2026", "num_citations": "184\n", "authors": ["310"]}
{"title": "Proportional sampling strategy: guidelines for software testing practitioners\n", "abstract": " Recently, several sufficient conditions have been developed that guarantee partition testing to have a higher probability of detecting at least one failure than random testing. One of these conditions is that the number of test cases selected from each partition is proportional to the size of the partition. We call such a method of allocating test cases the proportional sampling strategy. Although this condition is not the most general one, it is the most easily and practically applicable one. In this paper, we discuss how the proportional sampling strategy can be applied effectively in practice. Some practical issues that need to be attended are identified and guidelines to deal with these issues are suggested.", "num_citations": "176\n", "authors": ["310"]}
{"title": "A preliminary survey on software testing practices in Australia\n", "abstract": " We present the findings of, to the best of our knowledge, the first survey on software testing practices carried out in Australian ICT industry. A total of 65 organizations from various major capital cities in Australia participated in the survey, which was conducted between 2002 and 2003. The survey focused on five major aspects of software testing, namely testing methodologies and techniques, automated testing tools, software testing metrics, testing standards, and software testing training and education. Based on the survey results, current practices in software testing are reported, as well as some observations and recommendations for the future of software testing in Australia for industry and academia.", "num_citations": "174\n", "authors": ["310"]}
{"title": "On the relationship between partition and random testing\n", "abstract": " Weyuker and Jeng (ibid., vol. SE-17, pp. 703-711, July 1991) have investigated the conditions that affect the performance of partition testing and have compared analytically the fault-detecting ability of partition testing and random testing. This paper extends and generalizes some of their results. We give more general ways of characterizing the worst case for partition testing, along with a precise characterization of when this worst case is as good as random testing. We also find that partition testing is guaranteed to perform at least as well as random testing so long as the number of test cases selected is in proportion to the size of the subdomains.< >", "num_citations": "166\n", "authors": ["310"]}
{"title": "An innovative approach for testing bioinformatics programs using metamorphic testing\n", "abstract": " Recent advances in experimental and computational technologies have fueled the development of many sophisticated bioinformatics programs. The correctness of such programs is crucial as incorrectly computed results may lead to wrong biological conclusion or misguide downstream experimentation. Common software testing procedures involve executing the target program with a set of test inputs and then verifying the correctness of the test outputs. However, due to the complexity of many bioinformatics programs, it is often difficult to verify the correctness of the test outputs. Therefore our ability to perform systematic software testing is greatly hindered. We propose to use a novel software testing technique, metamorphic testing (MT), to test a range of bioinformatics programs. Instead of requiring a mechanism to verify whether an individual test output is correct, the MT technique verifies whether a pair of test outputs conform to a set of domain specific properties, called metamorphic relations (MRs), thus greatly increases the number and variety of test cases that can be applied. To demonstrate how MT is used in practice, we applied MT to test two open-source bioinformatics programs, namely GNLab and SeqMap. In particular we show that MT is simple to implement, and is effective in detecting faults in a real-life program and some artificially fault-seeded programs. Further, we discuss how MT can be applied to test programs from various domains of bioinformatics. This paper describes the application of a simple, effective and automated technique to systematically test a range of bioinformatics programs. We show how MT can be implemented in\u00a0\u2026", "num_citations": "161\n", "authors": ["310"]}
{"title": "Proportional sampling strategy: A compendium and some insights\n", "abstract": " There have been numerous studies on the effectiveness of partition and random testing. In particular, the proportional sampling (PS) strategy has been proved, under certain conditions, to be the only form of partition testing that outperforms random testing regardless of where the failure-causing inputs are. This paper provides an integrated synthesis and overview of our recent studies on the PS strategy and its related work. Through this synthesis, we offer a perspective that properly interprets the results obtained so far, and present some of the interesting issues involved and new insights obtained during the course of this research.", "num_citations": "161\n", "authors": ["310"]}
{"title": "Metamorphic testing of programs on partial differential equations: a case study\n", "abstract": " We study the effect of applying metamorphic testing to alleviate the oracle problem for numerical programs. We discuss a case study on the testing of a program that solves an elliptic partial differential equation with Dirichlet boundary conditions. We identify a metamorphic relation for the equation and demonstrate the effectiveness of metamorphic testing in identifying the error. The relation identified should also be applicable to other numerical methods that yield better approximations on the refinement of grid points or step sizes.", "num_citations": "150\n", "authors": ["310"]}
{"title": "Testing context-sensitive middleware-based software applications\n", "abstract": " Context-sensitive middleware-based software is an emerging kind of ubiquitous computing application. The components of such software communicate proactively among themselves according to the situational attributes of their environments, known as the \"contexts\". The actual process of accessing and updating the contexts lies with the middleware. The latter invokes the relevant local and remote operations whenever any context inscribed in the situation-aware interface is satisfied. Since the applications operate in a highly dynamic environment, the testing of context-sensitive software is challenging. Metamorphic testing is a property-based testing strategy. It recommends that, even if a test case does not reveal any failure, follow-up test cases should be further constructed from the original to check whether the software satisfies some necessary conditions of the problem to be implemented. This work proposes to\u00a0\u2026", "num_citations": "122\n", "authors": ["310"]}
{"title": "Restricted random testing\n", "abstract": " This paper presents a novel adaptation of traditional random testing, called Restricted Random Testing (RRT). RRT offers a significant improvement over random testing, as measured by the F-measure. This paper describes the ideology behind RRT and explains its algorithm. RRT\u2019s performance is examined using several experiments, the results of which are presented and discussed.", "num_citations": "119\n", "authors": ["310"]}
{"title": "Resource constraints analysis of workflow specifications\n", "abstract": " A workflow specification is a formal description of business processes in the real world. Its correctness is critical to the workflow execution and hence the realisation of business objectives. In addition to structural and temporal constraints, resource constraints are also implied in workflow specifications. Therefore, they should be analysed to ensure that the workflow specification is resource consistent at build-time. In this paper, we first identify the problem of resource constraints in a workflow specification. Then we propose an innovative approach with corresponding algorithms to the checking of resource consistency for a workflow specification. Furthermore, we extend our analysis work to timed workflow specifications, where time information is taken into consideration for the checking of the resource consistency of a workflow specification. The work reported in this paper provides a theoretical foundation for workflow\u00a0\u2026", "num_citations": "118\n", "authors": ["310"]}
{"title": "A simulation study on some heuristics for test suite reduction\n", "abstract": " In order to test a program, software testers must first define a testing objective. From this a test suite is then constructed. In fact, some subsets of the constructed test suite may still satisfy the same testing objective. Such subsets are referred to as representative sets of the test suite. Finding the optimal representative sets of the test suite reduction problem is known to be NP-complete. Different heuristics, including G, GE, grE and H, have been proposed by different researchers. However, it has been demonstrated that none of these four heuristics are guaranteed to deliver smaller sized representative sets. This paper presents the result of a simulation study of these four heuristics. The aim of the study is to provide application guidelines for choosing the most appropriate heuristic for test suite reduction.", "num_citations": "114\n", "authors": ["310"]}
{"title": "Metamorphic slice: An application in spectrum-based fault localization\n", "abstract": " ContextBecause of its simplicity and effectiveness, Spectrum-Based Fault Localization (SBFL) has been one of the popular approaches towards fault localization. It utilizes the execution result of failure or pass, and the corresponding coverage information (such as program slice) to estimate the risk of being faulty for each program entity (such as statement). However, all existing SBFL techniques assume the existence of a test oracle to determine the execution result of a test case. But, it is common that test oracles do not exist, and hence the applicability of SBFL has been severely restricted.ObjectiveWe aim at developing a framework that can extend the application of SBFL to the common situations where test oracles do not exist.MethodOur approach uses a new concept of metamorphic slice resulting from the integration of metamorphic testing and program slicing. In SBFL, instead of using the program slice and the\u00a0\u2026", "num_citations": "100\n", "authors": ["310"]}
{"title": "Restricted random testing: Adaptive random testing by exclusion\n", "abstract": " Restricted Random Testing (RRT) is a new method of testing software that improves upon traditional Random Testing (RT) techniques. Research has indicated that failure patterns (portions of an input domain which, when executed, cause the program to fail or reveal an error) can influence the effectiveness of testing strategies. For certain types of failure patterns, it has been found that a widespread and even distribution of test cases in the input domain can be significantly more effective at detecting failure compared with ordinary RT. Testing methods based on RT, but which aim to achieve even and widespread distributions, have been called Adaptive Random Testing (ART) strategies. One implementation of ART is RRT. RRT uses exclusion zones around executed, but non-failure-causing, test cases to restrict the regions of the input domain from which subsequent test cases may be drawn. In this paper, we\u00a0\u2026", "num_citations": "97\n", "authors": ["310"]}
{"title": "Test case selection strategies based on boolean specifications\n", "abstract": " This paper considers test case selection for programs whose specifications are expressed by Boolean algebra. The approach is to select test cases based on Boolean specifications. Three test case selection strategies are proposed that aim at the detection of the literal insertion fault and the literal reference fault. Although the MAX-B strategy proposed by Weyuker et al. guarantees detection of these types of faults, the proposed strategies are more effective in the sense that the derived test cases form a subset of those selected by the MAX-B strategy. Copyright\u00a9 2001 John Wiley & Sons, Ltd.", "num_citations": "92\n", "authors": ["310"]}
{"title": "A choice relation framework for supporting category-partition test case generation\n", "abstract": " We describe in this paper a choice relation framework for supporting category-partition test case generation. We capture the constraints among various values (or ranges of values) of the parameters and environment conditions identified from the specification, known formally as choices. We express these constraints in terms of relations among choices and combinations of choices, known formally as test frames. We propose a theoretical backbone and techniques for consistency checks and automatic deductions of relations. Based on the theory, algorithms have been developed for generating test frames from the relations. These test frames can then be used as the basis for generating test cases. Our algorithms take into consideration the resource constraints specified by software testers, thus maintaining the effectiveness of the test frames (and hence test cases) generated.", "num_citations": "91\n", "authors": ["310"]}
{"title": "Application of metamorphic testing in numerical analysis\n", "abstract": " Application of metamorphic testing in numerical analysis - HKUST SPD | The Institutional Repository Skip to content Search Publications Advanced Search Profiles Application of metamorphic tes... Please use this identifier to cite or link to this item: http://hdl.handle.net/1783.1/70576 Application of metamorphic testing in numerical analysis Authors Chan, FT Chen, TY Cheung, Shing Chi View this author's profile Lau, MF Yiu, SM Issue Date 1998 Source Proceedings of the IASTED International Conference on Software Engineering (SE\u201998), 1998, p. 191-197 Language English Genre Conference paper Usage Metrics 319 Page views Similar Items A metamorphic testing methodology for online SOA application testing Author(s): Chan, WK; Cheung, Shing Chi; Leung, Karl RPH 2010 A metamorphic testing approach for online testing of service-oriented software applications Author(s): Chan, WK; Cheung, Shing Chi; Leung\u2026", "num_citations": "70\n", "authors": ["310"]}
{"title": "Integration testing of context-sensitive middleware-based applications: a metamorphic approach\n", "abstract": " During the testing of context-sensitive middleware-based software, the middleware checks the current situation to invoke the appropriate functions of the applications. Since the middleware remains active and the situation may continue to evolve, however, the conclusion of some test cases may not easily be identified. Moreover, failures appearing in one situation may be superseded by subsequent correct outcomes and, therefore, be hidden.         We alleviate the above problems by making use of a special kind of situation, which we call checkpoints, such that the middleware will not activate the functions under test. We recommend testers to generate test cases that start at a checkpoint and end at another. Testers may identify relations that associate different execution sequences of a test case. They then check the results of each test case to detect any contravention of such relations. We illustrate our technique with\u00a0\u2026", "num_citations": "67\n", "authors": ["310"]}
{"title": "Experience with teaching black-box testing in a computer science/software engineering curriculum\n", "abstract": " Software testing is a popular and important technique for improving software quality. There is a strong need for universities to teach testing rigorously to students studying computer science or software engineering. This paper reports the experience of teaching the classification-tree method as a black-box testing technique at the University of Melbourne, Melbourne, Australia, and Swinburne University of Technology, Melbourne, Australia. It aims to foster discussion of appropriate teaching methods of software testing.", "num_citations": "59\n", "authors": ["310"]}
{"title": "Forgetting test cases\n", "abstract": " Adaptive random testing (ART) methods are software testing methods which are based on random testing, but which use additional mechanisms to ensure more even and widespread distributions of test cases over an input domain. Restricted random testing (RRT) is a version of ART which uses exclusion regions and restriction of test case generation to outside these regions. RRT has been found to perform very well, but incurs some additional computational cost in its restriction of the input domain. This paper presents a method of reducing overheads called forgetting, where the number of test cases used in the restriction algorithm can be limited, and thus the computational overheads reduced. The motivation for forgetting comes from its importance as a human strategy for learning. Several implementations are presented and examined using simulations. The results are very encouraging", "num_citations": "58\n", "authors": ["310"]}
{"title": "A new method for constructing metamorphic relations\n", "abstract": " A fundamental problem for software testing is the oracle problem, which means that in many practical situations, it is extremely expensive, if not impossible, to verify the test result given any possible program input. Metamorphic testing is an approach to alleviating the oracle problem. The key part of metamorphic testing is a set of necessary properties of the software under test, namely metamorphic relations. Metamorphic relations not only help generate test cases, but also provide a mechanism to partially verify the test results without the need of oracle. In most previous studies, metamorphic relations were identified manually by testers in an ad hoc way. There is no systematic methodology that helps us identify metamorphic relations. In this paper, we propose a simple method, namely, the composition of metamorphic relations, for systematically constructing new metamorphic relations based on the already identified\u00a0\u2026", "num_citations": "56\n", "authors": ["310"]}
{"title": "Towards research on software cybernetics\n", "abstract": " Software cybernetics is a newly proposed area in software engineering. It makes better use of the interplay between control theory/engineering and software engineering. In this paper, we look into the research potentials of this emerging area.", "num_citations": "55\n", "authors": ["310"]}
{"title": "An integrated classification-tree methodology for test case generation\n", "abstract": " This paper describes an integrated methodology for the construction of test cases from functional specifications using the classification-tree method. It is an integration of our extensions to the classification-hierarchy table, the classification tree construction algorithm, and the classification tree restructuring technique. Based on the methodology, a prototype system ADDICT, which stands for AutomateD test Data generation system using the Integrated Classification-Tree method, has been built.", "num_citations": "54\n", "authors": ["310"]}
{"title": "Adaptive random testing by localization\n", "abstract": " Based on the intuition that widely spread test cases should have greater chance of hitting the nonpoint failure-causing regions, several adaptive random testing (ART) methods have recently been proposed to improve traditional random testing (RT). However, most of the ART methods require additional distance computations to ensure an even spread of test cases. In this paper, we introduce the concept of localization that can be integrated with some ART methods to reduce the distance computation overheads. By localization, test cases would be selected from part of the input domain instead of the whole input domain, and distance computation would be done for some instead of all previous test cases. Our empirical results show that the fault detecting capability of our method is comparable to those of other ART methods.", "num_citations": "52\n", "authors": ["310"]}
{"title": "A metamorphic approach to integration testing of context-sensitive middleware-based applications\n", "abstract": " During the testing of context-sensitive middleware-based software, the middleware identifies the current situation and invokes the appropriate functions of the applications. Since the middleware remains active and the situation may continue to evolve, however, the conclusion of some test cases may not be easily identified. Moreover, failures appearing in one situation may be superseded by subsequent correct outcomes and may, therefore, be hidden. We alleviate the above problems by making use of a special kind of situation, which we call checkpoints, such that the middleware will not activate the functions under test. We propose to generate test cases that start at a checkpoint and end at another. We identify functional relations that associate different execution sequences of a test case. Based on a metamorphic approach, we check the results of the test case to detect any contravention of such relations. We\u00a0\u2026", "num_citations": "51\n", "authors": ["310"]}
{"title": "Normalized restricted random testing\n", "abstract": " Restricted Random Testing (RRT) is a new method of testing software that improves upon traditional random testing (RT) techniques. This paper presents new data in support of the efficiency of RRT, and presents a variation of the algorithm, Normalized Restricted Random Testing (NRRT). NRRT permits the tester to have better information about the target exclusion rate (R) of RRT, the main control parameter of the method. We examine the performance of the NRRT and Original RRT (ORRT) methods using simulations and experiments, and offer some guidance for their use in practice.", "num_citations": "51\n", "authors": ["310"]}
{"title": "On the identification of categories and choices for specification-based test case generation\n", "abstract": " The category-partition method and the classification-tree method help construct test cases from specifications. In both methods, an early step is to identify a set of categories (or classifications) and choices (or classes). This is often performed in an ad hoc manner due to the absence of systematic techniques. In this paper, we report and discuss three empirical studies to investigate the common mistakes made by software testers in such an ad hoc approach. The empirical studies serve three purposes: (a) to make the knowledge of common mistakes known to other testers so that they can avoid repeating the same mistakes, (b) to facilitate researchers and practitioners develop systematic identification techniques, and (c) to provide a means of measuring the effectiveness of newly developed identification techniques. Based on the results of our studies, we also formulate a checklist to help testers detect such mistakes.", "num_citations": "48\n", "authors": ["310"]}
{"title": "On the divide-and-conquer approach towards test suite reduction\n", "abstract": " When testing a program, testers need to generate a set of test cases that satisfies a testing objective. This set of test cases is referred to as a test suite. Usually, the generated test suite may contain redundancy, that is, some test cases may be removed from the test suite without rendering the satisfaction of the testing objective. A representative set is a subset of a test suite that satisfies the same testing objective, and test suite reduction is aimed at finding the representative sets. This paper studies the properties of representative sets related to the divide-and-conquer approach towards test suite reduction. The results provide us a better understanding of the structural properties of representative sets and how to apply the divide-and-conquer approach towards test suite reduction.", "num_citations": "48\n", "authors": ["310"]}
{"title": "METRIC: METamorphic Relation Identification based on the Category-choice framework\n", "abstract": " Metamorphic testing is a promising technique for testing software systems when the oracle problem exists, and has been successfully applied to various application domains and paradigms. An important and essential task in metamorphic testing is the identification of metamorphic relations, which, due to the absence of a systematic and specification-based methodology, has often been done in an ad hoc manner\u2014something which has hindered the applicability and effectiveness of metamorphic testing. To address this, a systematic methodology for identifying metamorphic relations based on the category-choice framework, called metric, is introduced in this paper. A tool implementing this methodology has been developed and examined in an experiment to determine the viability and effectiveness of metric, with the results of the experiment confirming that metric is both effective and efficient at identifying\u00a0\u2026", "num_citations": "47\n", "authors": ["310"]}
{"title": "AIDA\u2013a dynamic data flow anomaly detection system for Pascal programs\n", "abstract": " This paper presents a description of, and experience in using, AIDA, an automated instrumentation system to perform data flow analysis for Pascal programs. AIDA is capable of detecting not only data flow anomalies, but also certain kinds of errors. It is a useful software testing and development tool for Pascal programs. The associated implementation problems and their solutions are presented.", "num_citations": "46\n", "authors": ["310"]}
{"title": "Test case prioritization for object-oriented software: An adaptive random sequence approach based on clustering\n", "abstract": " Test case prioritization (TCP) attempts to improve fault detection effectiveness by scheduling the important test cases to be executed earlier, where the importance is determined by some criteria or strategies. Adaptive random sequences (ARSs) can be used to improve the effectiveness of TCP based on white-box information (such as code coverage information) or black-box information (such as test input information). To improve the testing effectiveness for object-oriented software in regression testing, in this paper, we present an ARS approach based on clustering techniques using black-box information. We use two clustering methods: (1) clustering test cases according to the number of objects and methods, using the K-means and K-medoids clustering algorithms; and (2) clustered based on an object and method invocation sequence similarity metric using the K-medoids clustering algorithm. Our approach can\u00a0\u2026", "num_citations": "45\n", "authors": ["310"]}
{"title": "Automatic generation of test cases from Boolean specifications using the MUMCUT strategy\n", "abstract": " A recent theoretical study has proved that the MUMCUT testing strategy (1) guarantees to detect seven types of fault in Boolean specifications in irredundant disjunctive normal form, and (2) requires only a subset of the test sets that satisfy the previously proposed MAX-A and MAX-B strategies, which can detect the same types of fault. This paper complements previous work by investigating various methods for the automatic generation of test cases to satisfy the MUMCUT strategy. We evaluate these methods by using several sets of Boolean expressions, including those derived from real airborne software systems. Our results indicate that the greedy CUN and UCN methods are clearly better than others in consistently producing significantly smaller test sets, whose sizes exhibit linear correlation with the length of the Boolean expressions in irredundant disjunctive normal form. This study provides empirical evidences\u00a0\u2026", "num_citations": "44\n", "authors": ["310"]}
{"title": "MUMCUT: A fault-based strategy for testing Boolean specifications\n", "abstract": " We study the MUMCUT strategy that integrates the MUTP, MNFP and CUTPNFP strategies previously proposed separately for testing Boolean specifications. The MUMCUT strategy guarantees to detect seven types of faults found in Boolean expressions. We describe an implementation of generating test sets that satisfy the MUMCUT strategy, and empirically evaluate its cost effectiveness. With respect to a previously published set of Boolean expressions derived from a real specification, we find that on average the MUMCUT strategy requires only about one quarter the size of an exhaustive test set. Moreover, the MUMCUT strategy proves to be a substantial improvement to the MAX-A and MAX-B strategies which detect the same types of faults.", "num_citations": "44\n", "authors": ["310"]}
{"title": "A metamorphic relation-based approach to testing web services without oracles\n", "abstract": " Service Oriented Architecture (SOA) has become a major application development paradigm. As a basic unit of SOA applications, Web services significantly affect the quality of the applications constructed from them. In the context of SOA, the specification and implementation of Web services are completely separated. The lack of source code and the restricted control of Web services limit the testability of Web services, and make the oracle problem prominent. In this context, can one alleviate the test oracle problem, or effectively and efficiently test such Web services even without oracles? It is an important issue which has not been yet adequately addressed. To address the challenge of testing Web services, the authors propose a metamorphic relation-based approach to testing Web services without oracles. The proposed approach leverages so-called metamorphic relations to generate test cases and evaluate test\u00a0\u2026", "num_citations": "43\n", "authors": ["310"]}
{"title": "A revisit of fault class hierarchies in general Boolean specifications\n", "abstract": " Recently, Kapoor and Bowen [2007] have extended the works by Kuhn [1999], Tsuchiya and Kikuno [2002], and Lau and Yu [2005]. However, their proofs overlook the possibility that a mutant of the Boolean specifications under test may be equivalent. Hence, each of their fault relationships is either incorrect or has an incorrect proof. In this article, we give counterexamples to the incorrect fault relationships and provide new proofs for the valid fault relationships. Furthermore, a co-stronger fault relation is introduced to establish a new fault class hierarchy for general Boolean specifications.", "num_citations": "43\n", "authors": ["310"]}
{"title": "An overview of integration testing techniques for object-oriented programs\n", "abstract": " Object-oriented programs involve many unique features that are not present in their conventional counterparts. Examples are message passing, synchronization, dynamic binding, object instantiation, persistence, encapsulation, inheritance, and polymorphism. Integration testing for such programs is, therefore, more difficult than that for conventional programs. In this paper, we present an overview of current work on integration testing for object-oriented and/or concurrent programs, with a view to identifying areas for future research. We cover state-based testing, eventbased testing, fault-based testing, deterministic and reachability techniques, and formal and semiformal techniques.", "num_citations": "42\n", "authors": ["310"]}
{"title": "Dynamic program dicing\n", "abstract": " Program dicing, introduced by J. R. Lyle and M. Weiser (Proc. 2nd Int. Conf. Comput. Appl., pp. 887-883, 1987), is a debugging technique built upon program slicing. Their dicing methodology is referred to as static program dicing in this paper. Since it is difficult to apply static program dicing effectively under some circumstances, an extension of program dicing, dynamic program dicing, is introduced here. Several strategies of constructing dynamic dices are also discussed. Although a dynamic slice is included in its corresponding static slice, a dynamic dice is not necessarily included in its corresponding static dice. Some conditions under which dynamic dice is more precise than static dice are investigated.< >", "num_citations": "42\n", "authors": ["310"]}
{"title": "Spectrum-based fault localization: Testing oracles are no longer mandatory\n", "abstract": " Spectrum-based Fault Localization (SBFL) is one of the most popular approaches for locating software faults, and has received much attention because of its simplicity and effectiveness. It utilizes the execution result of each test case (failure or pass) and the corresponding coverage information to evaluate the likelihood of each program entity (e.g., a statement or a predicate) being faulty. Different formulas for computing such likelihood have been proposed based on different intuitions. All existing SBFL techniques have assumed the existence of a testing oracle, that is, a mechanism which can determine whether the execution of a test case fails or passes. However, such an assumption does not always hold. Recently, metamorphic testing has been proposed to alleviate the oracle problem. Thus, it is a natural extension to investigate how it can help SBFL techniques to locate faults even without using a testing oracle\u00a0\u2026", "num_citations": "41\n", "authors": ["310"]}
{"title": "Heuristics towards the optimization of the size of a test suite\n", "abstract": " When testing a program, software testers have to define the testing objec-tive first. A test suite is then constructed to satisfy the testing objective. Usually, the constructed test suite may contain redundancy. A test case in a test suite is said to be redundant if the same testing objective can still be satisfied by other test cases of the test suite. Since the execution of test cases is expensive, it is of paramount impor-tance to remove redundant test cases within a test suite. However, removal of all redundant test cases is practically infeasible because the problem is NP-complete. Recently, Harrold et. al.[2] have proposed a technique to reduce the size of a test suite. In this paper, we are going to propose other heuristics to optimize the size of a test suite. 1 Introduction W", "num_citations": "40\n", "authors": ["310"]}
{"title": "Construction of classification trees via the classification-hierarchy table\n", "abstract": " The classification-tree method developed by Grochtmann and coworkers is a black box testing technique to assist test engineers to construct test cases systematically from the functional specifications, via the construction of classification trees. This paper supplements their studies by proposing a methodology to construct classification trees systematically from given sets of classifications and their associated classes, via the notion of the classification-hierarchy table. The intuition of the classification-hierarchy table is to capture the hierarchical relationship for every pair of distinct classifications from which classification trees can be constructed.", "num_citations": "39\n", "authors": ["310"]}
{"title": "Metamorphic testing for web services: Framework and a case study\n", "abstract": " Service Oriented Architecture (SOA) has become a major application development paradigm. As a basic unit of SOA applications, Web services significantly affect the quality of the applications constructed from them. Since the development and consumption of Web services are completely separated under SOA environment, the consumers are normally provided with limited knowledge of the services and thus have little information about test oracles. The lack of source code and the restricted control of Web services limit the testability of Web services. To address the prominent oracle problem when testing Web services, we propose a metamorphic testing framework for Web services taking into account the unique features of SOA. We conduct a case study where the new metamorphic testing framework is employed to test a Web service that implements the electronic payment. The results of case study show the\u00a0\u2026", "num_citations": "35\n", "authors": ["310"]}
{"title": "Automated test case generation for BDI agents\n", "abstract": " We propose a coverage oriented test case generation methodology for BDI multi-agent systems. The coverage criteria involve plans and nodes within plans of multi-agent systems. We organise the criteria into a subsumption hierarchy to show the coverage relationships between the criteria. Then we apply the criteria on multi-agent systems to analyse some empirical data. The data analysed is the effect on the number of test cases generated automatically for each criterion. We use a tool, BDITESTER, to obtain the empirical data and to show that our proposal is pragmatic. Finally, we suggest some guidelines to select a criterion to automatically generate test cases for BDI agents.", "num_citations": "35\n", "authors": ["310"]}
{"title": "Adaptive random test case generation for combinatorial testing\n", "abstract": " Random testing (RT), a fundamental software testing technique, has been widely used in practice. Adaptive random testing (ART), an enhancement of RT, performs better than original RT in terms of fault detection capability. However, not much work has been done on effectiveness analysis of ART in the combinatorial test spaces. In this paper, we propose a novel family of ART-based algorithms for generating combinatorial test suites, mainly based on fixed-size-candidate-set ART and restricted random testing (that is, ART by exclusion). We use an empirical approach to compare the effectiveness of test sets obtained by our proposed methods and random selection strategy. Experimental data demonstrate that the ART-based tests cover all possible combinations at a given strength more quickly than randomly chosen tests, and often detect more failures earlier and with fewer test cases in simulations.", "num_citations": "34\n", "authors": ["310"]}
{"title": "Isolating suspiciousness from spectrum-based fault localization techniques\n", "abstract": " Spectrum-based fault localization (SBFL) is one of the most promising fault localization approaches, which normally uses the failed and passed program spectrum to evaluate the risks for all program entities. However, it does not explicitly distinguish the different degree in definiteness between the information associated with the failed spectrum and the passed spectrum, which may result in an unreliable location of faults. Thus in this paper, we propose a refinement method to improve the accuracy of the predication by SBFL, through eliminating the indefinite information. Our method categorizes all statements into two groups according to their different suspiciousness, and then uses different evaluation schemes for these two groups. In this way, we can reduce the use of the unreliable information in the ranking list, and finally provide a more precise result. Experimental study shows that for some SBFL techniques, our\u00a0\u2026", "num_citations": "33\n", "authors": ["310"]}
{"title": "A more general sufficient condition for partition testing to be better than random testing\n", "abstract": " Partition testing is an approach to program testing in which the input domain of the program is divided into partitions and test cases are selected from each partition. An alternative approach, known as random testing, is to select test cases at random from the entire input domain. Weyuker and Jeng (1991) observed that if all partitions are equal in sizes and the number of test cases selected from each partition is the same, then partition testing has a better chance of detecting at least one failure than random testing. This condition has been generalized by Chen and Yu (1994). They proved that partition testing is better than random testing so long as test cases are selected in proportion to the size of partitions. In this paper, we prove a more general sufficient condition. Partition testing performs better if the sampling rates are higher for partitions with higher failure rates. Some special cases that follow from this result are\u00a0\u2026", "num_citations": "33\n", "authors": ["310"]}
{"title": "An innovative approach to tackling the boundary effect in adaptive random testing\n", "abstract": " Adaptive random testing (ART) is an effective improvement of random testing (RT) in the sense that fewer test cases are needed to detect the first failure. It is based on the observation that failure-causing inputs are normally clustered in one or more contiguous regions in the input domain. Hence, it has been proposed that test case generation should refer to the locations of successful test cases (those that do not reveal failures) to ensure that all test cases are far apart and evenly spread in the input domain. Distance-based ART and restricted random testing are the first two previous attempts. However, test cases generated by these attempts are far apart but not necessarily evenly spread, since more test cases are generated near the boundary of the input domain. This paper analyzes the cause of this phenomenon and proposes an enhanced implementation based on the concept of virtual images of the successful\u00a0\u2026", "num_citations": "32\n", "authors": ["310"]}
{"title": "Adaptive and random partition software testing\n", "abstract": " Random testing (RT) and subdomain testing are two major software testing strategies. Their simplicity makes them likely the most efficient testing strategies with respect to the time required for test case selection. However, the disadvantage of RT is its defect detection effectiveness. Adaptive testing (AT) is a feedback-based software testing strategy that has been shown to be more effective than RT and partition testing (PT). However, a major concern in the application of AT is its complexity and computational cost for test case selection. In this paper, we propose a hybrid approach that uses AT and random partition testing (RPT) in an alternating manner. The motivation for this approach is that both strategies are employed such that the underlying computational complexity of AT is reduced by introducing RPT into the testing process without affecting the defect detection effectiveness. A case study with seven real-life\u00a0\u2026", "num_citations": "31\n", "authors": ["310"]}
{"title": "An assessment of systems and software engineering scholars and institutions (1999\u20132003)\n", "abstract": " This paper presents the findings of a five-year study of the top scholars and institutions in the Systems and Software Engineering field, as measured by the quantity of papers published in the journals of the field. The top scholar is Khaled El Emam of the Canadian National Research Council, and the top institution is Carnegie Mellon University and its Software Engineering Institute.This paper is part of an ongoing study, conducted annually, that identifies the top 15 scholars and institutions in the most recent five-year period.", "num_citations": "31\n", "authors": ["310"]}
{"title": "Two test data selection strategies towards testing of Boolean specifications\n", "abstract": " Test data selection for software represented as Boolean formulae has not received much attention until the last decade. Most of the approaches are expression driven because generation of test cases is based on the Boolean formulae being tested. The paper discusses test data generation using a fault based approach in the sense that generation of test cases is based on particular types of faults occurring in the Boolean expression. Two special types of faults are considered. Moreover, two strategies of test case selection that guarantee the detection of these two types of faults are proposed.", "num_citations": "30\n", "authors": ["310"]}
{"title": "An assessment of Systems and Software Engineering scholars and institutions (1996\u20132000)\n", "abstract": " This paper presents the findings of a five-year study of the top scholars and institutions in the Systems and Software Engineering (SSE) field, as measured by the quantity of papers published in the journals of the field. The top scholar is Richard Lai of La Trobe University in Australia, and the top institution is Carnegie Mellon University (CMU) and its Software Engineering Institute. The paper lists the top 15 scholars and institutions.", "num_citations": "29\n", "authors": ["310"]}
{"title": "On program dicing\n", "abstract": " Since it is sometimes difficult to apply the technique of static program dicing in debugging programs, we introduce dynamic dicing as the dynamic counterpart of static dicing. As the effectiveness of program dicing techniques depends on the size of the program dices used, this paper uses a probabilistic approach to investigate the relationship between the size of static and dynamic program dices. In making a comparison between dynamic and static program dicing, we present and prove six propositions. This leads us to the conditions under which a dynamic program dice is smaller than a static program dice. Based on those findings, we offer five strategies for constructing dynamic program dices. \u00a9\u20091997 by John Wiley & Sons, Ltd. J. Software Maintenance 9: 33\u201346, 1997", "num_citations": "29\n", "authors": ["310"]}
{"title": "Adaptive random testing by exclusion through test profile\n", "abstract": " One major objective of software testing is to reveal software failures such that program bugs can be removed. Random testing is a basic and simple software testing technique, but its failure-detection effectiveness is often controversial. Based on the common observation that program inputs causing software failures tend to cluster into contiguous regions, some researchers have proposed that an even spread of test cases should enhance the failure-detection effectiveness of random testing. Adaptive random testing refers to a family of algorithms to evenly spread random test cases based on various notions. Restricted random testing, an algorithm to implement adaptive random testing by the notion of exclusion, defines an exclusion region around each previously executed test case, and selects test cases only from outside all exclusion regions. Although having a high failure-detection effectiveness, restricted random\u00a0\u2026", "num_citations": "28\n", "authors": ["310"]}
{"title": "Adaptive testing of software components\n", "abstract": " Software components are popular in nowadays software industries. However, how to test software components is a problem since the source code of the software component under test may not be available for the third-party user. In this paper we show that the software component should be tested in an adaptive manner in the sense that the software defect detection rates are estimated on-line by using testing data collected during testing to improve test case selections. In doing so, we use a recursive least squares estimation method to do on-line parameter estimations. This paper further justifies the advantages of the controlled Markov chain (CMC) approach to software testing in particular, and the practicality of the idea of software cybermetics in general.", "num_citations": "28\n", "authors": ["310"]}
{"title": "Maximal unifiable subsets and minimal nonunifiable subsets\n", "abstract": " We address the problem of collecting information about failures and successes while unifying a set of equations. This is relevant to the study of efficient backtracking, for which Cox used the concept of maximal unifiable subsets while Bruynooghe and Pereira used a notion which is closely related to that of minimal non-unifiable subsets. As we show, both these concepts play a fundamental role in the process of exploring the search space for breadth first resolution in logic programs. In a special case they lead to similar search strategies but in general have complementary and even incompatible aspects. We then show that an algorithm due to Yasuura is particularly well suited as a basis for a method to construct the maximal unifiable subsets and minimal non-unifiable subsetsin conjunction with the unification process. In addition to its simplicity this method provides an answer for two problems raised by Cox\u00a0\u2026", "num_citations": "28\n", "authors": ["310"]}
{"title": "The art of divide and conquer: An innovative approach to improving the efficiency of adaptive random testing\n", "abstract": " Test case selection is a prime process in the engineering of test harnesses. In particular, test case diversity is an important concept. In order to achieve an even spread of test cases across the input domain, Adaptive Random Testing (ART) was proposed such that the history of previously executed test cases are taken into consideration when selecting the next test case. This was achieved through various means such as best candidate selection, exclusion, partitioning, and diversity metrics. Empirical studies showed that ART algorithms make good use of the concept of even spreading and achieve 40 to 50% improvement in test effectiveness over random testing in revealing the first failure, which is close to the theoretical limit. However, the computational complexity of ART algorithms may be quadratic or higher, and hence efficiency is an issue when a large number of previously executed test cases are involved\u00a0\u2026", "num_citations": "26\n", "authors": ["310"]}
{"title": "An assessment of systems and software engineering scholars and institutions (2000\u20132004)\n", "abstract": " This paper presents the findings of a five-year study of the top scholars and institutions in the Systems and Software Engineering field, as measured by the quantity of papers published in the journals of the field in 2000\u20132004. The top scholar is Hai Zhuge of the Chinese Academy of Sciences, and the top institution is Korea Advanced Institute of Science and Technology. This paper is part of an ongoing study, conducted annually, that identifies the top 15 scholars and institutions in the most recent five-year period.", "num_citations": "26\n", "authors": ["310"]}
{"title": "On Test Case Distributions of Adaptive Random Testing\u2217\n", "abstract": " Adaptive Random Resting (ART) has recently been pro-posed as an approach to enhancing the fault-detection ef-fectiveness of Random Testing (RT). The basic principle of ART is to enforce randomly selected test cases as evenly spread over the input domain as possible. Many ART meth-ods have been proposed to evenly spread test cases in dif-ferent ways, but no comparison has been made among these methods in terms of their test case distributions. In this paper, we conduct a comprehensive investigation on test case distributions of various ART methods. Our work shows many interesting aspects related to ART\u2019s performance and its test case distribution. Furthermore, it points out a new research direction on enhancing ART. 1.", "num_citations": "25\n", "authors": ["310"]}
{"title": "Software testing education and training in Hong Kong\n", "abstract": " While the use of computer applications is widely spread in every business and, hence, the reliability of software is critical, it is believed that many organizations involved in software development do not take software testing sufficiently seriously as an important task. It is worthwhile to find out how far organizations are carrying out software testing in a systematic and structured manner or still taking on an ad-hoc approach. A survey was conducted to understand the software testing practices and the level of related education and training in Hong Kong. It was found that most testing team members did not have formal training in software testing. University curricula generally did not prepare graduates with enough coverage in software testing. It is proposed that a review of the current software engineering curricula in the universities to examine the coverage of software testing will be useful to the development of quality\u00a0\u2026", "num_citations": "25\n", "authors": ["310"]}
{"title": "Adaptive random testing through test profiles\n", "abstract": " Random testing (RT), which simply selects test cases at random from the whole input domain, has been widely applied to test software and assess the software reliability. However, it is controversial whether RT is an effective method to detect software failures. Adaptive random testing (ART) is an enhancement of RT in terms of failure\u2010detection effectiveness. Its basic intuition is to evenly spread random test cases all over the input domain. There are various notions to achieve the goal of even spread, and each notion can be implemented by different algorithms. For example, \u2018by exclusion\u2019 and \u2018by partitioning\u2019 are two different notions to evenly spread test cases. Restricted random testing (RRT) is a typical algorithm for the notion of \u2018by exclusion\u2019, whereas the notion of \u2018by partitioning\u2019 can be implemented by either the technique of bisection (ART\u2010B) or the technique of random partitioning (ART\u2010RP). In this paper, we\u00a0\u2026", "num_citations": "24\n", "authors": ["310"]}
{"title": "Towards a problem-driven approach to perspective-based reading\n", "abstract": " The quality of a requirements specification has a great impact on the quality of the software developed. Because of this, a requirements specification should be complete, correct, consistent, and unambiguous. Otherwise, defects may remain undetected, resulting in the delivery of a faulty software product to the users. Motivated by this, Basili et al. have developed the perspective-based reading (PBR) technique to help identify defects in requirements specifications. In this paper we propose a problem-driven approach for supporting the PBR technique. We also discuss the experience of applying our proposal to a real-life requirements specification.", "num_citations": "24\n", "authors": ["310"]}
{"title": "Out of sight, out of mind: a distance-aware forgetting strategy for adaptive random testing\n", "abstract": " Adaptive random testing (ART) achieves better failure-detection effectiveness than random testing by increasing the diversity of test cases. However, the intention of ensuring even spread of test cases inevitably causes an overhead problem. Although two basic forgetting strategies (i.e. random forgetting and consecutive retention) were proposed to reduce the computation cost of ART, they only considered the temporal distribution of test cases. In the paper, we presented a distance-aware forgetting strategy for the fixed size candidate set version of ART (DF-FSCS), in which the spatial distribution of test cases is taken into consideration. For a given candidate, the test cases out of its \u201csight\u201d are ignored to reduce the distance computation cost. At the same time, the dynamic adjustment for partitioning and the second-round forgetting are adopted to ensure the linear complexity of DF-FSCS algorithm. Both\u00a0\u2026", "num_citations": "23\n", "authors": ["310"]}
{"title": "Prioritization of combinatorial test cases by incremental interaction coverage\n", "abstract": " Combinatorial interaction testing is a well-recognized testing method, and has been widely applied in practice, often with the assumption that all test cases in a combinatorial test suite have the same fault detection capability. However, when testing resources are limited, an alternative assumption may be that some test cases are more likely to reveal failure, thus making the order of executing the test cases critical. To improve testing cost-effectiveness, prioritization of combinatorial test cases is employed. The most popular approach is based on interaction coverage, which prioritizes combinatorial test cases by repeatedly choosing an unexecuted test case that covers the largest number of uncovered parameter value combinations of a given strength (level of interaction among parameters). However, this approach suffers from some drawbacks. Based on previous observations that the majority of faults in practical\u00a0\u2026", "num_citations": "23\n", "authors": ["310"]}
{"title": "An automatic test data generation system based on the integrated classification-tree methodology\n", "abstract": " Grochtmann and Grimm have developed the classification-tree method (CTM) to facilitate software testers in generating test cases from functional specifications. While the method is very useful, it is hindered by the lack of a systematic tree construction algorithm. This problem has been alleviated by Chen\u00a0et\u00a0al. via their \u201dintegrated\u201d classification-tree methodology (ICTM).\u00a0\u00a0In this paper, we describe and discuss a prototype system addict that is built on ICTM.", "num_citations": "23\n", "authors": ["310"]}
{"title": "On the completeness of a test suite reduction strategy\n", "abstract": " The problem of test suite reduction is to find a subset of test cases  from  the test suite that can still satisfy the same testing objective. Such a  subset is referred to as a representative set of the test suite. We use a  problem reduction technique, namely the 1-to-1 redundancy dividing strategy,  to find the optimal representative sets, which are the representative sets of  the smallest size. The 1-to-1 redundancy dividing strategy guarantees that an  optimal representative set of the reduced test suite is still an optimal  representative set of the original one. However, the converse is not  necessarily true. This paper investigates how to find all optimal  representative sets of the original test suite from the reduced one.  Furthermore, for the minimal representative sets whose proper subsets are not  representative sets, the same problem is addressed.", "num_citations": "23\n", "authors": ["310"]}
{"title": "Dynamic data flow analysis for C++\n", "abstract": " Although data flow analysis has been successfully applied in testing programs written in procedural programming languages, its current form as not powerful enough to test object-oriented programs. The methodology of conventional data flow analysis should be extended to incorporate new techniques to test C++ programs. We propose how data flow analysis should be extended and demonstrate how it can be used to detect data flow anomalies.", "num_citations": "23\n", "authors": ["310"]}
{"title": "Test case prioritization using adaptive random sequence with category-partition-based distance\n", "abstract": " Test case prioritization schedules test cases in a certain order aiming to improve the effectiveness of regression testing. Random sequence is a basic and simple prioritization technique, while Adaptive Random Sequence (ARS) makes use of extra information to improve the diversity of random sequence. Some researchers have proposed prioritization techniques using ARS with white-box information, such as code coverage information, or with black-box information, such as string distances of the input data. In this paper, we propose new black-box test case prioritization techniques using ARS, and the diversity of test cases is assessed by category-partition-based distance. Our experimental studies show that these new techniques deliver higher fault-detection effectiveness than random prioritization, especially in the case of smaller ratio of failed test cases. In addition, in the comparison of different distance metrics\u00a0\u2026", "num_citations": "21\n", "authors": ["310"]}
{"title": "A new approach for network vulnerability analysis\n", "abstract": " Despite a significant increase in security of modern information systems, cyber attacks have become more sophisticated as attackers combine multiple vulnerabilities to penetrate networks resulting in devastating consequences. In the past, attack graphs had been important tools for analyzing and understanding how various vulnerabilities could be combined through many potential interactions and connections between network components to compromise security. Full attack graphs for a realistic network, however, can be very large and complex, making it difficult to analyze and to decide what changes should be made in the network to make it sufficiently secure. We propose in this paper a novel approach to analyze network vulnerability and to identify all the combinations of exploits that are critical to the overall security of a network. Unlike previous graph-based algorithms that generate attack trees (or graphs\u00a0\u2026", "num_citations": "21\n", "authors": ["310"]}
{"title": "Identification of categories and choices in activity diagrams\n", "abstract": " The choice relation framework (CHOC'LATE) provides a systematic skeleton for constructing test cases from specifications. An early stage of the framework is to identify a set of categories and choices from the specification, which is not a trivial task when this document is largely informal and complex. Despite the difficulty, the identification task is very important because the quality of the identified categories and choices will affect the comprehensiveness of the test cases and, hence, the chance of revealing software faults. This paper alleviates the problem by introducing a technique for identifying categories and choices from the activity diagrams in the specification. This technique also helps determine the relations between some pair of choices in the choice relation table - an essential step of CHOC'LATE for the subsequent generation of test cases.", "num_citations": "21\n", "authors": ["310"]}
{"title": "Good random testing\n", "abstract": " Software Testing is recognized as an essential part of the Software Development process. Random Testing (RT), the selection of test cases at random from the input domain, is a simple and efficient method of Software Testing. Previous research has indicated that, under certain circumstances, the performance of RT can be improved by enforcing a more even, well-spread distribution of test cases over the input domain. Test cases that contribute to this goal can be considered \u2018good,\u2019 and are more desirable when choosing potential test cases than those that do not contribute. Fuzzy Set Theory enables a calculation of the degree of membership of the set of \u2018good\u2019 test cases for any potential test case, in other words, a calculation of how \u2018good\u2019 the test case is. This paper presents research in the area of improving on the failure finding efficiency of RT using Fuzzy Set Theory. An approach is proposed and\u00a0\u2026", "num_citations": "21\n", "authors": ["310"]}
{"title": "Classification-hierarchy table: a methodology for constructing the classification tree\n", "abstract": " The Classification-Tree Method developed by Grochtmann and Grimm (1993) provides a systematic approach to produce test cases based on the functional specification. This paper supplements the Classification-Tree Method by proposing a methodology to construct a classification-tree from a given set of classifications and classes based on the notion of Classification-Hierarchy Table. The Classification-Hierarchy Table is used to capture the hierarchy of a classification tree. From this table, useful information such as the relative level of the classifications in the tree, their parent classifications and parent classes could be obtained to guide the construction of the classification tree.", "num_citations": "21\n", "authors": ["310"]}
{"title": "Metamorphic testing: A simple method for alleviating the test oracle problem\n", "abstract": " The test oracle problem is regarded as one of the most challenging problems in software testing. Metamorphic testing has been developed to alleviate this problem, which is done using the relations involving relevant inputs and their outputs. This keynote speech will provide a summary of the state-of-the-art of metamorphic testing.", "num_citations": "20\n", "authors": ["310"]}
{"title": "Analyzing and extending MUMCUT for fault-based testing of general Boolean expressions\n", "abstract": " Boolean expressions are widely used to model decisions or conditions of a specification or source program. The MUMCUT, which is designed to detect seven common faults where Boolean expressions under test are assumed to be in Irredundant Disjunctive Normal Form (IDNF), is an efficient fault-based test case selection strategy in terms of the fault-detection capacity and the size of selected test suite. Following up our previous work that reported the fault-detection capacity of the MUMCUT when it is applied to general form Boolean expressions, in this paper we present the characteristic of the types of single faults committed in general Boolean expressions that a MUMCUT test suite fails to detect, analyze the certainty why a MUMCUT test suite fails to detect these types of undetected faults, and provide some extensions to enhance the detection capacity of the MUMCUT for these types of undetected faults.", "num_citations": "20\n", "authors": ["310"]}
{"title": "An Application of Adaptive Random Sequence in Test Case Prioritization.\n", "abstract": " Test case prioritization aims to schedule test cases in a certain order such that the effectiveness of regression testing can be improved. Prioritization using random sequence is a basic and simple technique, and normally acts as a benchmark to evaluate other prioritization techniques. Adaptive Random Sequence (ARS) makes use of extra information to improve the diversity of random sequence. Some researchers have proposed prioritization techniques using ARS with white-box code coverage information that is normally related to the test execution history of previous versions. In this paper, we propose several ARS-based prioritization techniques using black-box information. The proposed techniques schedule test cases based on the string distances of the input data, without referring to the execution history. Our experimental studies show that these new techniques deliver higher fault-detection effectiveness than random prioritization. In addition, as compared with an existing blackbox prioritization technique, the new techniques have similar fault-detection effectiveness but much lower computation overhead, and thus are more cost-effective.", "num_citations": "19\n", "authors": ["310"]}
{"title": "Spectrum-based fault localization without test oracles\n", "abstract": " Spectrum-Based Fault Localization (SBFL) is one of the most promising approaches towards fault localization, and has received a lot of attention due to its simplicity and effectiveness. It utilizes various program spectra and the associated testing result of each individual test case, namely failed or passed, to evaluate the risk of containing a fault for each program entity with different statistical formulas. However, it suffers from a crucial problem which makes it infeasible in many application domains, that is, its assumption of the existence of test oracle. In practice, there are many programs in various domains without such oracles, where obviously, the existing SBFL techniques cannot be applied at all. To address this problem, we introduce a novel type of slices, metamorphic slice (mslice), which is property based, as different from the traditional test case based slices in SBFL. We propose to use mslice instead of slice, and its associated metamorphic testing (MT) result of a metamorphic test group, which is either violated or non-violated, rather than the testing result of failed or passed for individual test cases, to evaluate the risk for each program entity of being faulty. In this way, we can extend SBFL to application domains without test oracles, needless to say that our proposed method is still applicable if the application domains have test oracles. We use a popular UNIX utility program, grep, in our case study to evaluate the effectiveness of the proposed method. With three popular risk formulas (Orchiai, Jaccard and Tarantula), we compare the effectiveness of using mslice and traditional slice in SBFL. The empirical results suggest that for test suites of the\u00a0\u2026", "num_citations": "19\n", "authors": ["310"]}
{"title": "Randomized quasi-random testing\n", "abstract": " Random testing is a fundamental testing technique that can be used to generate test cases for both hardware and software systems. Quasi-random testing was proposed as an enhancement to the cost-effectiveness of random testing: In addition to having similar computation overheads to random testing, it makes use of quasi-random sequences to generate low-discrepancy and low-dispersion test cases that help deliver high failure-detection effectiveness. Currently, few algorithms exist to generate quasi-random sequences, and these are mostly deterministic, rather than random. A previous study of quasi-random testing has examined two methods for randomizing quasi-random sequences to improve their applicability in testing. However, these randomization methods still have shortcomings-one method does not introduce much randomness to the test cases, while the other does not support incremental test case\u00a0\u2026", "num_citations": "18\n", "authors": ["310"]}
{"title": "DESSERT: a DividE-and-conquer methodology for identifying categorieS, choiceS, and choicE Relations for Test case generation\n", "abstract": " This paper extends the choce relation framework, abbreviated as choc'late, which assists software testers in the application of category/choice methods to testing. choc'late assumes that the tester is able to construct a single choice relation table from the entire specification; this table then forms the basis for test case generation using the associated algorithms. This assumption, however, may not hold true when the specification is complex and contains many specification components. For such a specification, the tester may construct a preliminary choice relation table from each specification component, and then consolidate all the preliminary tables into a final table to be processed by choc'late for test case generation. However, it is often difficult to merge these preliminary tables because such merging may give rise to inconsistencies among choice relations or overlaps among choices. To alleviate this problem, we\u00a0\u2026", "num_citations": "18\n", "authors": ["310"]}
{"title": "Metamorphic testing: A simple approach to alleviate the oracle problem\n", "abstract": " The oracle problem is very common in the testing of service-oriented systems. Metamorphic testing has been proposed to alleviate the oracle problem in software testing. This talk aims at presenting the state of the art in metamorphic testing. It summaries what testing techniques have been successfully integrated with metamorphic testing and what application areas metamorphic testing have been applied to successfully alleviate the oracle problem. It will also introduce potential research projects using the metamorphic approach.", "num_citations": "18\n", "authors": ["310"]}
{"title": "CHOC'LATE: a framework for specification-based testing\n", "abstract": " In spite of its importance in software reliability, testing is labor intensive and expensive. It has been found that software testing without a good strategy may not be more effective than testing the system with random data. Obviously, the effectiveness of testing relies heavily on how well the test suite --- the set of test cases actually used --- is generated. This is because the comprehensiveness of the test suite will affect the scope of testing and, hence, the chance of revealing software faults. There are two main approaches to generating test suites: specification-based and code-based. The former generates a test suite from information derived from the specification, without requiring the knowledge of the internal structure of the program. The latter approach, on the other hand, generates a test suite based on the source code of the program. Neither of these approaches is sufficient; they are complementary to one another\u00a0\u2026", "num_citations": "18\n", "authors": ["310"]}
{"title": "Constraints for safe partition testing strategies\n", "abstract": " Although previous studies have shown that partition testing strategies are not always very effective, with appropriate restrictions on the test allocation they can be guaranteed to be safe, in the sense that they will never be less reliable in detecting at least one failure than random testing. Several sufficient conditions for this have already been established in the literature. In particular, the proportional sampling strategy, which allocates test cases in proportion to the size of the subdomains from which they are selected, has been proved to be safe for all programs. In practice, since the number of test cases must be positive integers, often the proportional sampling strategy can only be approximated. This paper examines the necessary conditions for safe partition testing strategies. We also prove that, when the input domain is large enough with respect to the numbers of failure-causing inputs and test cases, a safe\u00a0\u2026", "num_citations": "18\n", "authors": ["310"]}
{"title": "On detecting faults for Boolean expressions\n", "abstract": " Fault based testing aims at detecting hypothesized faults based on specifications or program source. There are some fault based techniques for testing Boolean expressions which are commonly used to model conditions in specifications as well as logical decisions in program source. The MUMCUT strategy has been proposed to generate test cases from Boolean expressions. Moreover, it detects eight common types of hypothesized faults provided that the original expression is in irredundant disjunctive normal form, IDNF. Software practitioners are more likely to write the conditions and logical decisions in general form rather than IDNF. Hence, it is interesting to investigate the fault detecting capability of the MUMCUT strategy with respect to general form Boolean expressions. In this article, we perform empirical studies to investigate the fault detection capability of the MUMCUT strategy with respect to\u00a0\u2026", "num_citations": "17\n", "authors": ["310"]}
{"title": "Test case selection with and without replacement\n", "abstract": " Previous theoretical studies on the effectiveness of partition testing and random testing have assumed that test cases are selected with replacement. Although this assumption has been well known to be less realistic, it has still been used in previous theoretical work because it renders the analyses more tractable. This paper presents a theoretical investigation aimed at comparing the effectiveness when test cases are selected with and without replacement, and exploring the relationships between these two scenarios. We propose a new effectiveness metric for software testing, namely the expected number of distinct failures detected, to re-examine existing partition testing strategies.", "num_citations": "17\n", "authors": ["310"]}
{"title": "How to test bioinformatics software?\n", "abstract": " Bioinformatics is the application of computational, mathematical and statistical techniques to solve problems in biology and medicine. Bioinformatics programs developed for computational simulation and large-scale data analysis are widely used in almost all areas of biophysics. The appropriate choice of algorithms and correct implementation of these algorithms are critical for obtaining reliable computational results. Nonetheless, it is often very difficult to systematically test these programs as it is often hard to verify the correctness of the output, and to effectively generate failure-revealing test cases. Software testing is an important process of verification and validation of scientific software, but very few studies have directly dealt with the issues of bioinformatics software testing. In this work, we review important concepts and state-of-the-art methods in the field of software testing. We also discuss recent reports\u00a0\u2026", "num_citations": "16\n", "authors": ["310"]}
{"title": "Automated testing of WS-BPEL service compositions: a scenario-oriented approach\n", "abstract": " Nowadays, service oriented architecture (SOA) has become one mainstream paradigm for developing distributed applications. As the basic unit in SOA, web services can be composed to construct complex applications. The quality of web services and their compositions is critical to the success of SOA applications. Testing, as a major quality assurance technique, is confronted with new challenges in the context of service compositions. In this paper, we propose a scenario-oriented testing approach that can automatically generate test cases for service compositions. Our approach is particularly focused on the service compositions specified by Business Process Execution Language for web services (WS-BPEL), a widely recognized executable service composition language. In the approach, a WS-BPEL service composition is first abstracted into a graph model; test scenarios are then derived from the model; finally\u00a0\u2026", "num_citations": "16\n", "authors": ["310"]}
{"title": "Study of relativity between rockburst and microseismic activity zone in deep tunnel\n", "abstract": " Based on a large amount of microseismic monitoring data and hundreds of rockbursts with different intensities occurring in the deep diversion and drainage tunnels of Jinping\u2161hydropower station, relativity between rockburst and microseismic activity zone is studied. The conclusions are drawn as follows.(1) Microseismic activity mainly ranges from 3 times the tunnel diameter behind working face to 1.5 times ahead; and rockburst high-risk zone lies in the region as large as 3 times the tunnel diameter behind working face. Therefore rockburst high-risk zone is greatly in accordance with the main microseismic activity zone.(2) Potential rockburst high-risk zones in tunnel project are highly localized in the region already excavated as large as 3 times the tunnel diameter behind working face and the region under construction as large as 1.5 times the tunnel diameter ahead of working face.(3) Seismic events and rockburst distribution are characterized by regional aggregation. Some of them happen in the aggregation region of seismic events. Others happen on the edge of aggregation region of seismic events. That is an inherent phenomenon in the failure progress of rock mass and closely related to local stress concentration on the edge of aggregation region of seismic events.", "num_citations": "16\n", "authors": ["310"]}
{"title": "Applying testing to requirements inspection for software quality assurance\n", "abstract": " Developing software systems involves a series of activities that contain many opportunities to make errors. Such errors may occur at an early stage of the development process, when user requirements are incorrectly or incompletely specified, or in subsequent stages, when design and programming faults are introduced. Thus, software development should always be accompanied by quality assurance (QA) activities. Two common QA activities are requirements inspection and software testing (otherwise simply known as testing), which are often used in different phases of the software development life cycle (SDLC). 1 Traditionally, requirements inspection is performed at an early stage of SDLC to reveal defects in a requirements specification (hereafter simply referred to as a specification). On the other hand, testing is commonly done at a later stage of SDLC to look for program faults after coding. Because their purposes are different, requirements inspection and testing are often treated as separate and unrelated tasks by software practitioners.In recent years, many researchers have proposed to apply testing techniques to requirements inspection at an initial phase of the SDLC. 2, 3 The idea is that generating test cases from a specification may uncover requirements defects well before programming starts. 4 Thus, the possibility of inadvertently developing software based on an incorrect specification can be reduced. The benefits of such proposals are particularly prominent for large-scale projects, 5 where the specifications are complicated and may easily contain many requirements defects; the costs of repairing these defects at the late stages of\u00a0\u2026", "num_citations": "16\n", "authors": ["310"]}
{"title": "Improving the quality of classification trees via restructuring\n", "abstract": " The classification-hierarchy table developed by Chen and Poon (1996) provides a systematic approach to construct classification trees from given sets of classifications and their associated classes. The paper enhances their study by defining a metric to measure the \"quality\" of a classification tree, and providing an algorithm to improve this quality.", "num_citations": "16\n", "authors": ["310"]}
{"title": "Adaptive random testing with CG constraint\n", "abstract": " We introduce a C.G. constraint on adaptive random testing (ART) for programs with numerical input. One rationale behind adaptive random testing is to have the test candidates to be as widespread over the input domain as possible. However, the computation may be quite expensive in some cases. The C.G. constraint is introduced to maintain the widespreadness while reducing the computation requirement in terms of number of distance measures. Three variations of C.G. constraints and their performance when compared with ART are discussed.", "num_citations": "15\n", "authors": ["310"]}
{"title": "Teaching black box testing\n", "abstract": " Historically, software testing received relatively less attention compared with other activities (e.g. systems analysis and design) of the software life cycle in an undergraduate computer science/information systems curriculum. Nevertheless, it is a common and important technique used to detect errors in software. This paper reports our recent experience of using a new approach to teaching software testing (particularly black box testing) at the University of Melbourne. Through this paper, we aim to increase the profile of software testing as well as to foster discussion of effective teaching methods of software testing.", "num_citations": "15\n", "authors": ["310"]}
{"title": "Optimal improvement of the lower bound performance of partition testing strategies\n", "abstract": " Although partition testing strategies are intuitively more appealing than random testing, previous empirical and analytical studies show that under unfavourable circumstances partition testing can be very ineffective. The problem of maximally improving the lower bound performance of partition testing by the choice of appropriate test distributions is investigated. An algorithm that generates optimal test distributions for this purpose is proposed and analysed. Moreover, the algorithm can also serve to systematically approximate the proportional sampling strategy, which has previously been proved to be at least as good as random testing.", "num_citations": "15\n", "authors": ["310"]}
{"title": "METTLE: a METamorphic testing approach to assessing and validating unsupervised machine LEarning systems\n", "abstract": " Unsupervised machine learning is the training of an artificial intelligence system using information that is neither classified nor labeled, with a view to modeling the underlying structure or distribution in a dataset. Since unsupervised machine learning systems are widely used in many real-world applications, assessing the appropriateness of these systems and validating their implementations with respect to individual users\u2019 requirements and specific application scenarios/contexts are indisputably two important tasks. Such assessments and validation tasks, however, are fairly challenging due to the absence of  a priori  knowledge of the data. In view of this challenge, in this article, we develop a  MET amorphic  T esting approach to assessing and validating unsupervised machine  LE arning systems, abbreviated as  mettle . Our approach provides a new way to unveil the (possibly latent) characteristics of various\u00a0\u2026", "num_citations": "14\n", "authors": ["310"]}
{"title": "A decision-theoretic approach to the test allocation problem in partition testing\n", "abstract": " A partition testing strategy consists of two components: a partitioning scheme which determines the way in which the program's input domain is partitioned into subdomains; and an allocation of test cases which determines the exact number of test cases selected from each subdomain. This paper investigates the problem of determining the test allocation when a particular partitioning scheme has been chosen. We show that this problem can be formulated as a classic problem of decision-making under uncertainty, and analyze several well known criteria to resolve this kind of problem. We present algorithms that solve the test allocation problem based on these criteria, and evaluate these criteria by means of a simulation experiment. We also discuss the applicability and implications of applying these criteria in the context of partition testing.", "num_citations": "14\n", "authors": ["310"]}
{"title": "Test suite reduction and fault detecting effectiveness: An empirical evaluation\n", "abstract": " Test suite reduction is aimed at finding representative sets that can satisfy the same testing objective as their original test suite. As a subset of the original test suite, the representative set may have less fault detection capability. However, researches show that a representative set and its original test suite have similar fault detection capabilities for the case of coverage based criteria. This paper investigates the relationship between the fault detection capabilities of a representative set and its original test suite when the generation of the test suite is based on some fault-based test case selection criteria.", "num_citations": "14\n", "authors": ["310"]}
{"title": "A new restructuring algorithm for the classification-tree method\n", "abstract": " The classification-tree method developed by Grochtmann and Grimm facilitates the identification of test cases from functional specifications via the construction of classification trees. Their method has been enhanced by Chen and Poon through the classification-tree construction and restructuring methodologies. We find, however that the restructuring algorithm by Chen and Poon is applicable only to certain types of classification trees. We introduce a new tree-restructuring algorithm to supplement their work.", "num_citations": "14\n", "authors": ["310"]}
{"title": "On the effectiveness of classification trees for test case construction\n", "abstract": " The notion of the classification-hierarchy table and the classification-tree construction algorithm provide a systematic approach to the construction of classification trees from given sets of classifications and their associated classes. Using classification trees, the set of all possible test cases can be constructed from functional specifications. This paper extends their study by introducing a metric to measure the effectiveness of a classification tree with respect to the construction of test cases, and providing ways to improve this effectiveness.", "num_citations": "14\n", "authors": ["310"]}
{"title": "On the effectiveness of test case allocation schemes in partition testing\n", "abstract": " In partition testing, the input domain is divided into two or more disjoint subdomains according to some criteria and then test data are selected from each subdomain. Hence we must design a partitioning scheme that governs how to divide the input domain, and a test case allocation scheme that controls how to allocate test cases to the subdomains. Given any partitioning scheme, the test case allocation scheme plays a crucial part in the effectiveness of testing. This paper develops guidelines for comparing different test case allocation schemes. This provides solutions to the class of problems where various test case allocation schemes under the same partitioning scheme are compared for the effectiveness of testing. We illustrate with examples several potential applications of our findings.", "num_citations": "14\n", "authors": ["310"]}
{"title": "A new approach to on-line rescheduling for a semiconductor foundry fab\n", "abstract": " Production scheduling is a complex task in most real manufacturing settings and semiconductor manufacturing is no exception. To control such complex systems, it is a challenge to determine appropriate dispatching strategies under various system conditions. Therefore, dispatching strategies are important to improve the system performance, especially for the real time control of the system. In this paper, an on-line rescheduling mechanism combined with theory of constraints (TOC) is proposed. Genetic algorithm (GA) is for searching dispatching rule sets which promote better performance. With system conditions corresponding to dispatching rules, the support vector machine (SVM) classifier is constructed as the scheduler. Also, the adaptive neuro-fuzzy inference system (ANFIS) prediction model is built for the sake of on-line deciding the scheduling intervals. The results indicate that applying the proposed\u00a0\u2026", "num_citations": "13\n", "authors": ["310"]}
{"title": "Impact of the compactness of failure regions on the performance of adaptive random testing\n", "abstract": " Adaptive random testing (ART) is an enhanced version of random testing (RT). It has been observed that the compactness of failure regions is one of the factors that affect the performance of ART. However, this relationship has only been verified with rectangular failure regions. This paper further investigates the relationship between the compactness of failure regions and the performance of ART by conducting simulation experiments, where various regular and irregular failure regions are studied. The experimental results have shown that ART's performance improves as the compactness of failure regions increases. This study has provided further insights into the conditions where ART outperforms RT.", "num_citations": "13\n", "authors": ["310"]}
{"title": "A systematic method for auditing user acceptance tests\n", "abstract": " In the commercial sector, most user acceptance tests (UATs) belong to the \u201cblack-box testing approach1\u201d(ie test cases are derived directly from specifications without the knowledge of program coding). However, most black-box testing techniques are basically \u201cad hoc\u201d in nature. Therefore, difficulties are often encountered by IS auditors when auditing most UATs. This article solves this problem by introducing a methodology for the construction of test cases from specifications. Also, a prototype system is developed for this methodology in order to maximise its effectiveness and usability by means of automation.", "num_citations": "13\n", "authors": ["310"]}
{"title": "The application of Prolog to structured design\n", "abstract": " In this paper, we investigate the feasibility of applying logic programming to structured design. We propose to use Prolog as a common machinery for the representation of various structured tools. We illustrate through examples how to produce structure charts from data flow diagrams, and evaluate them according to commonly recommended design guidelines. If the structure charts produced are not satisfactory, the inherent backtracking mechanism in Prolog will help to produce other versions for further evaluation.", "num_citations": "13\n", "authors": ["310"]}
{"title": "Adaptive random testing through iterative partitioning revisited\n", "abstract": " Recently, Adaptive Random Testing through Iterative Partitioning (IP-ART) has been proposed as a random testing method that is more effective than pure Random Testing. Besides this, it is supposed to be equally effective as very good random testing techniques, namely Distance-Based Adaptive Random Testing and Restricted Random Testing, while only having between linear and quadratic runtime. In the present paper, it is investigated what influence the ratio of width and height of a rectangular input domain has on the effectiveness of various Adaptive Random Testing methods. Based on our findings, an improved version of IP-ART is proposed. The effectiveness of the new method is also analyzed for various ratios of width and height of the input domain.", "num_citations": "12\n", "authors": ["310"]}
{"title": "Adaptive Random Testing with Filtering: An Overhead Reduction Technique.\n", "abstract": " Adaptive Random Testing (ART) is an approach to testing software based on Random Testing (RT), but incoJporating additional mechanisms to ensure a more lvidespread and even distribution of test cases over the input domain. It has been found that ART, under certain conditions, can significantly outpe! form RT, in terms of number of test cases required to detect a failure (a measure referred to as the F-measure). One implementation of ART. based on the use of exclusion zones and restriction of test case selection to outside of these zones, is Restricted Random Testing (RRT). In this paper, we present an oven, iew of the basic RRT method. using circular and spherical exclusion regions, and then introduce an alternative exclusion shape, motivated by the promise of lower computational costs. Investigation into this alternative shape (square) exclusion method lead to a hybrid implementation of RRT, called filtering. Filtering enables the combination of the computationally cheaper square e: rclusion shape and the faster (for failure finding) original, circular exclusion shape. Simulation and experimental evidence are also presented supporting the methods.", "num_citations": "12\n", "authors": ["310"]}
{"title": "On the testing methods used by beginning software testers\n", "abstract": " This paper describes our experiences of the methods used by novice software testers to test their own programs, as well as their perception of the classification-tree method, which is a black box testing method first introduced by Grochtmann and Grimm. We conducted two case studies involving novice software testers. The subjects in the first study possessed one-year working experience while those in the second study had a wider range of working experiences. Both studies found that white box testing methods were initially far more popular than black box methods, but the majority of the subjects were convinced of the benefits of the classification-tree method after they had learned and used it. About two-third of them indicated their preference of the classification-tree method over the methods they originally used.", "num_citations": "12\n", "authors": ["310"]}
{"title": "Runtime data analysis for Java programs\n", "abstract": " Program analysis plays a key role in many areas of software development, such as performance tuning, testing, debugging, and maintenance. Program analysis can be carried out statically or dynamically, and these two approaches are generally seen as being complementary to each other. In this paper, we will focus on runtime data analysis for object oriented systems in general and the Java 2 platform in particular. More specifically, we are investigating various runtime approaches to monitor the access and modification of variables in a Java program in order to keep track of the their usage history, being of particular interest for dynamic data flow analysis. Our results indicate that the Java 2 platform does not provide sufficient tools to enable comprehensive runtime data analysis in this context, especially when the complete source code of an application under investigation is unavailable.", "num_citations": "12\n", "authors": ["310"]}
{"title": "Minimizing the mean delay of quorum-based mutual exclusion schemes\n", "abstract": " Achieving mutual exclusion is one of the most fundamental problems in distributed computing. The use of coteries is a well-known approach to this problem. A coterie is a special set of pair-wise intersecting node groups called quorums. The communication delay incurred in a quorum-based mutual exclusion scheme depends critically on the coterie adopted, and thus it is important to find a coterie with small delay. Recently, two related measures called max-delay and mean-delay have been introduced. The former measure represents the largest delay among all nodes, while the latter is the arithmetic mean of the delays. In a previous paper, we have proposed a polynomial-time algorithm to find max-delay optimal coteries, but there has been no algorithm to find mean-delay optimal coteries. In this paper, the first algorithm that finds mean-delay optimal coteries in general topology networks is proposed. This algorithm\u00a0\u2026", "num_citations": "12\n", "authors": ["310"]}
{"title": "KDFC-ART: a KD-tree approach to enhancing Fixed-size-Candidate-set Adaptive Random Testing\n", "abstract": " Adaptive random testing (ART) was developed as an enhanced version of random testing to increase the effectiveness of detecting failures in programs by spreading the test cases evenly over the input space. However, heavy computation may be incurred. In this paper, three enhanced algorithms for fixed-size-candidate-set ART (FSCS-ART) are proposed based on the k-dimensional tree (KD-tree) structure. The first algorithm Naive-KDFC constructs a KD-tree by splitting the input space with respect to every dimension successively in a round-robin fashion. The second algorithm SemiBal-KDFC improves the balance of the KD-tree by prioritizing the splitting according to the spread in each dimension. In order to control the number of traversed nodes in backtracking, the third algorithm LimBal-KDFC introduces an upper bound for the nodes involved. Simulation and empirical studies have been conducted to\u00a0\u2026", "num_citations": "11\n", "authors": ["310"]}
{"title": "Fundamentals of test case selection: Diversity, diversity, diversity\n", "abstract": " Our recent investigations in software testing reveal that diversity constitutes the underlying foundation in many test case selection strategies. This talk attempts to provide an overview of the concept of diversity in test case selection through two families of test case selection strategies, namely, random testing and partition testing. We also present some areas of software testing where the application of data mining techniques shows great potential in identifying key aspects of diversity in various forms.", "num_citations": "11\n", "authors": ["310"]}
{"title": "On the online parameter estimation problem in adaptive software testing\n", "abstract": " Software cybernetics is an emerging area that explores the interplay between software and control. The controlled Markov chain (CMC) approach to software testing supports the idea of software cybernetics by treating software testing as a control problem, where the software under test serves as a controlled object modeled by a controlled Markov chain and the software testing strategy serves as the corresponding controller. The software under test and the corresponding software testing strategy form a closed-loop feedback control system. The theory of controlled Markov chains is used to design and optimize the testing strategy in accordance with the testing/reliability goal given explicitly and a priori. Adaptive software testing adjusts and improves software testing strategy online by using the testing data collected in the course of software testing. In doing so, the online parameter estimations play a key role. In this\u00a0\u2026", "num_citations": "11\n", "authors": ["310"]}
{"title": "A study on a path-based strategy for selecting black-box generated test cases\n", "abstract": " Various black-box methods for the generation of test cases have been proposed in the literature. Many of these methods, including the category-partition method and the classification-tree method, follow the approach of partition testing, in which the input domain is partitioned into subdomains according to important aspects of the specification, and test cases are then derived from the subdomains. Though comprehensive in terms of these important aspects, execution of all the test cases so generated may not be feasible under the constraint of tight testing resources. In such circumstances, there is a need to select a smaller subset of test cases from the original test suite for execution. In this paper, we propose the use of white-box information to guide the selection of test cases from the original test suite generated by a black-box testing method. Furthermore, we have developed some techniques and algorithms to\u00a0\u2026", "num_citations": "11\n", "authors": ["310"]}
{"title": "The use of prolog in the modelling and evaluation of structure charts\n", "abstract": " In this paper, we summarize our experience in the use of Prolog to model and evaluate structure charts according to standard guidelines in structured design. We discuss how to construct first-cut structure charts automatically from dataflow diagrams using transform and transaction analyses, evaluate them using recommended criteria such as coupling, cohesion, morphology and tramp, and improve on the resulting structure charts by means of automatic backtracking.", "num_citations": "11\n", "authors": ["310"]}
{"title": "Scenario-oriented testing for web service compositions using BPEL\n", "abstract": " Applications are increasingly constructed by orchestrating Web services. Ensuring the reliability of such loosely coupled service compositions is a challenging task. In this paper, we propose an automatic scenario-oriented testing approach for service compositions specified by Business Process Execution Language (BPEL). In our approach, an abstract test model (BGM) is first defined to represent the control flows of BPEL specifications, then test scenarios are derived from the BGM with respect to the given coverage criteria, finally test data are generated to drive the execution of resulting test scenarios. A case study is conducted to demonstrate how our approach can be applied to complex real-life service compositions and the experimental results validate the effectiveness of our approach.", "num_citations": "10\n", "authors": ["310"]}
{"title": "The universal safeness of test allocation strategies for partition testing\n", "abstract": " Given a specification, a partition testing strategy is said to be safe for a program if it is not less probable of detecting at least one failure than random testing, and universally safe, if it is safe for every program. The proportional sampling strategy (PSS) has previously been proved to be universally safe. This paper extends previous results by proving that PSS is actually the only universally safe test allocation strategy for partition testing, thereby settling an open problem in the search for universally safe strategies. Despite this, we show that a close approximation to PSS may still be safe for most programs. We also discuss the practical implications of this result.", "num_citations": "10\n", "authors": ["310"]}
{"title": "White on black: a white-box-oriented approach for selecting black box-generated test cases\n", "abstract": " Many useful test case construction methods that are based on important aspects of the specification have been proposed in the literature. A comprehensive test suite thus obtained is often very large and yet is non-redundant with respect to the aspects identified from the specification. This paper addresses the problem of selecting a subset of test cases from such a test suite. We propose the use of white box criteria to select test cases from the initial black-box-generated test suite. We illustrate our ideas with examples and demonstrate the viability and benefits of our approach by means of a case study.", "num_citations": "10\n", "authors": ["310"]}
{"title": "An empirical study on the effectiveness of the greedy MUTP strategy\n", "abstract": " We propose the use of a greedy heuristic in finding a subset of test cases from the set of all unique true points so that it satisfies the multiple unique true points (or MUTP) strategy. Moreover we report an empirical study on the effectiveness of the greedy heuristic in finding a set of test cases that satisfies the MUTP strategy.", "num_citations": "10\n", "authors": ["310"]}
{"title": "COD\u2014A dynamic data flow analysis system for Cobol\n", "abstract": " This paper presents a description of an automated data flow analysis system for Cobol programs \u2014 COD. It detects all data flow anomalies as well as certain kinds of errors and has been found to be a very helpful tool for testing and developing Cobol programs.", "num_citations": "10\n", "authors": ["310"]}
{"title": "Generating biased dataset for metamorphic testing of machine learning programs\n", "abstract": " Although both positive and negative testing are important for assuring quality of programs, generating a variety of test inputs for such testing purposes is difficult for machine learning software. This paper studies why it is difficult, and then proposes a new method of generating datasets that are test inputs to machine learning programs. The proposed idea is demonstrated with a case study of classifying hand-written numbers.", "num_citations": "9\n", "authors": ["310"]}
{"title": "Metric+: A metamorphic relation identification technique based on input plus output domains\n", "abstract": " Metamorphic testing is well known for its ability to alleviate the oracle problem in software testing. The main idea of metamorphic testing is to test a software system by checking whether each identified metamorphic relation (MR) holds among several executions. In this regard, identifying MRs is an essential task in metamorphic testing. In view of the importance of this identification task, METRIC (METamorphic Relation Identification based on Category-choice framework) was developed to help software testers identify MRs from a given set of complete test frames. However, during MR identification, METRIC primarily focuses on the input domain without sufficient attention given to the output domain, thereby hindering the effectiveness of METRIC. Inspired by this problem, we have extended METRIC into METRIC+ by incorporating the information derived from the output domain for MR identification. A tool implementing\u00a0\u2026", "num_citations": "9\n", "authors": ["310"]}
{"title": "A cloud-based framework for applying metamorphic testing to a bioinformatics pipeline\n", "abstract": " Testing of bioinformatics software often suffers from the oracle problem, especially when testing software that analyses human genome sequencing data. Metamorphic testing has been proposed to alleviate the oracle problem. Nonetheless, smaller research or clinical centres may be challenged by the complexity and resources required to implement a suitable metamorphic testing framework in practice. This paper presents a case study on how a cloud-based metamorphic testing framework can be applied to a widely used genomic sequencing pipeline, and discusses the future of implementing large-scale on-demand automated metamorphic testing using cloud-based resources.", "num_citations": "9\n", "authors": ["310"]}
{"title": "Probabilistic adaptive random testing\n", "abstract": " Adaptive random testing (ART) methods are software testing methods which are based on random testing, but which use additional mechanisms to ensure more even and widespread distributions of test cases over an input domain. Restricted random testing (RRT) is a version of ART which uses exclusion regions and restricts test case generation to outside of these regions. RRT has been found to perform very well, but its use of strict exclusion regions (from within which test cases cannot be generated) has prompted an investigation into the possibility of modifying the RRT method such that all portions of the input domain remain available for test case generation throughout the duration of the algorithm. In this paper, we present a probabilistic approach, probabilistic ART (PART), and explain two different implementations. Preliminary empirical data supporting the methods is also examined", "num_citations": "9\n", "authors": ["310"]}
{"title": "Mobile diagnosis based on RFID for food safety\n", "abstract": " Manufacturers have already deployed RFID technology in products that could be spoiled during transportation due to temperature extremes. For example, frozen chicken has a high risk of salmonella contamination if it becomes too warm for a long time. If the temperature exceeds a certain threshold, a permanent electrical change occurs in the RFID-based label. When the RFID reader interrogates the tag, it will respond with data that indicates the warning state as well as its ID. In this paper, a mobile diagnosis system based on RFID (radio identification) for food safety is developed. The developed system consists of a mobile server and a remote server. In the mobile server, a PDA equipped with RFID CF reader card is used to acquire the food temperatures from RFID sensors. Moreover, the multi-sensor fusion algorithm is applied to integrate the sensor information. X-bar chart or trend chart is used to observe the\u00a0\u2026", "num_citations": "9\n", "authors": ["310"]}
{"title": "A study on input domain partitioning\n", "abstract": " A study on input domain partitioning | PolyU Institutional Research Archive Skip navigation PolyU logo PIRA logo PolyU Library logo PolyU logo PolyU Library logo PIRA logo Home Collections Research Outputs Patents Theses Open Educational Resources Outstanding Work by Students Departments and Schools About Search Please use this identifier to cite or link to this item: http://hdl.handle.net/10397/57916 Title: A study on input domain partitioning Authors: Chen, TY Cheng, MY Poon, PL Tse, TH Yu, YT Issue Date: 2002 Source: In TY Chen, MY Cheng, PL Poon, TH Tse & YT Yu (Eds.), Proceedings of the 20th International Multi-Conference on Applied Informatics (AI 2002), Innsbruck, Austria, 18-21 February 2002, p. 176-181 Appears in Collections: Conference Paper Show full item record Page view(s) 124 Last Week", "num_citations": "9\n", "authors": ["310"]}
{"title": "On the maximin algorithms for test allocations in partition testing\n", "abstract": " The proportional sampling (PS) strategy is a partition testing strategy that has been proved to have a better chance than random testing to detect at least one failure. A near proportional sampling (NPS) strategy is one that approximates the PS strategy when the latter is not feasible. We have earlier proved that the (basic) maximin algorithm generates a maximin test allocation, that is, an allocation of test cases that will maximally improve the lower bound performance of the partition testing strategy, and shown that the algorithm may serve as a systematic means of approximating the PS strategy. In this paper, we derive the uniqueness and completeness conditions of generating maximin test allocations, propose the complete maximin algorithm that generates all possible maximin test allocations and demonstrate empirically that the new algorithm is consistently better than random testing as well as several other NPS\u00a0\u2026", "num_citations": "9\n", "authors": ["310"]}
{"title": "MT4WS: an automated metamorphic testing system for web services\n", "abstract": " The use of web services has been growing significantly, with increasingly large numbers of applications being implemented through the web. A difficulty associated with this development is the quality assurance of these services, specifically the challenges encountered when testing the applications - amongst other things, testers may not have access to the source code, and the correctness of the output may not be easily ascertained (known as the oracle problem). Metamorphic testing (MT) has been introduced as a technique to alleviate the oracle problem. MT makes use of properties of the software under test, known as metamorphic relations, and checks whether or not these relations are violated. Since MT does not require source code to generate the metamorphic relations, it is suitable for testing web services-based applications. We have designed an XML-based language representation to facilitate the\u00a0\u2026", "num_citations": "8\n", "authors": ["310"]}
{"title": "Teaching software testing skills: Metamorphic testing as vehicle for creativity and effectiveness in software testing\n", "abstract": " In spite of its importance to software quality, software testing is often considered the \"poor man\" of software engineering processes, left to the end of many projects, and frequently omitted altogether. Compounding this is the reported perception of testing as a menial, low-level job, lacking any need for creativity or ingenuity, and of much lower status and attractiveness than others, such as design and implementation. When teaching about software testing, instructors often face very unmotivated students, professing little or no interest in becoming testers. This tutorial will address some testing misconceptions, and, through Metamorphic Testing - a new approach to testing which alleviates some of the major challenges in the field (including the Oracle Problem) - will provide a fresh and exciting new perspective on software testing.", "num_citations": "8\n", "authors": ["310"]}
{"title": "Fault class prioritization in Boolean expressions\n", "abstract": " A recent study has classified faults in Boolean expressions into ten classes and has proved that there are five key fault classes, namely CCF, CDF, ORF, ENF and ASF, such that if a test suite can kill all faulty versions of these five core fault classes, if can kill all faulty versions of all fault classes. In order to generate more effective test suites, we should prioritize these five fault classes further, such that test cases with stronger fault detection capability could be generated as early as possible. Such a process is referred to as the fault class prioritization. Based on the observation in the fault class hierarchy, we divide the five fault classes into two groups {CCF, CDF} and {ORF, ENF, ASF}. Two strategies of fault class prioritization are proposed to generate test cases efficiently. We design experiments using TCAS Boolean expressions and some randomly generated Boolean expressions. The experimental results suggest\u00a0\u2026", "num_citations": "8\n", "authors": ["310"]}
{"title": "An efficient defect estimation method for software defect curves\n", "abstract": " Software defect curves describe the behavior of the estimate of the number of remaining software defects as software testing proceeds. They are of two possible patterns: single-trapezoidal-like curves or multiple-trapezoidal-like curves. In this paper we present some necessary and/or sufficient conditions for software defect curves of the Goel-Okumoto NHPP model. These conditions can be used to predict the effect of the detection and removal of a software defect on the variations of the estimates of the number of remaining defects. A field software reliability dataset is used to justify the trapezoidal shape of software defect curves and our theoretical analyses. The results presented in this paper may provide useful feedback information for assessing software testing progress and have potentials in the emerging area of software cybernetics that explores the interplay between software and control.", "num_citations": "8\n", "authors": ["310"]}
{"title": "A new perspective of the proportional sampling strategy\n", "abstract": " To compare the performance of different testing strategies, P-measure and E-measure are two effectiveness measures used in previous analytical studies. P-measure, which is defined as the probability of detecting at least one failure, is a measure of how likely it is that failure-causing inputs are selected at least once as test cases. E-measure, which is defined as the expected number of failures detected, is a measure of how frequently failure-causing inputs are selected as test cases. However, we have no a priori knowledge of how many failure-causing inputs there are, or where they may lie. In this paper, we study P-measure and E-measure in terms of how much attention an arbitrary input receives. In the context of P-measure, the attention received by an arbitrary input is the probability that the input is selected at least once as a test case, while in the context of E-measure, the attention is the expected number of\u00a0\u2026", "num_citations": "8\n", "authors": ["310"]}
{"title": "The effect of maternal cardiac disease and digoxin administration on labour, fetal weight and maturity at birth\n", "abstract": " One hundred and twenty\u2010two patients with cardiac disease were compared with 250 controls with respect to the duration of pregnancy and labour, birth weight percentile and Apgar score. The babies of the patients with cardiac disease were light\u2010for\u2010dates (18% below the 10th percentile); the mothers, if multiparous, delivered at an earlier gestational age. The patients with cardiac disease did not have shorter labours than the control group. Digoxin administration and the severity of heart disease had no significant effect on these variables.", "num_citations": "8\n", "authors": ["310"]}
{"title": "Practical considerations in using partition testing\n", "abstract": " It has been proved by Weyuker and Jeng [4] that partition testing is more effective than random testing when all partitions are of equal sizes and equal number of test cases are selected from each partition. This result has been generalized by Chen and Yu [1] to partitions of variable sizes. They found that as long as the number of test cases selected from each partition is proportional to the size of the partition, partition testing is still more effective. The latter result is more applicable, as common testing strategies, such as path testing, usually divide the program domain into partitions of unequal sizes. In this paper, we discuss how these theoretical results can be applied", "num_citations": "8\n", "authors": ["310"]}
{"title": "Toward a K-means clustering approach to adaptive random testing for object-oriented software\n", "abstract": " Dear editor, Testing and debugging are mainstream methods for software quality assurance. In particular, random testing (RT, also known as fuzz testing) and partition testing (PT) are most widely adopted. Classical studies [1, 2] show that PT is only marginally better than RT in many cases but has considerably more overheads. On the other hand, RT does not consider the fact that failurecausing inputs tend to be conglomerated into regions. To address this issue, Chen et al.[3, 4] proposed adaptive random testing (ART), which targets at spreading the test cases as evenly as possible across the entire input domain. The failuredetection effectiveness is improved.Object-oriented (OO) software has become the de facto standard in the industry. However, traditional testing methods are not immediately applicable. ART is no exception. Adaptations are needed. Ciupa et al.[5] presented the notion of object distance and proposed ART for objectoriented software (ARTOO) based on the original fixed-sized-candidate-set ART (FSCS-ART) algorithm [3]. Chen et al.[6] further proposed the object and method invocation sequence similarity (OMISS) metric for OO software, covering not only distances between objects but also between method invocation sequences. Their OMISS-ART algorithm is based on FSCS-ART as well as the max-min criterion and the forgetting strategy. In particular, the forgetting strategy [7] simply considers some of the already executed test cases when selecting the next test case, thus reducing the computational overhead.", "num_citations": "7\n", "authors": ["310"]}
{"title": "On the analysis of spectrum based fault localization using hitting sets\n", "abstract": " Combining spectrum-based fault localization (SBFL) with other techniques is generally regarded as a feasible approach as advantages from both techniques would be preserved. Sendys which combines SBFL with slicing-hitting-set-computation is one of the promising techniques. However, all current evaluations on Sendys were obtained via empirical studies, which have inevitable threats to validity. Besides, purely empirical studies cannot reveal the essential reason that why Sendys performs well or badly, and whether all the complicated computations are necessary. Therefore, in this paper, we provide an in-depth theoretical analysis on Sendys, which can give definite and convincing conclusions. We generalize our previous theoretical framework on SBFL, to make it applicable to combined techniques like Sendys. We first provide a variant of current Sendys by patching its loophole of ignoring \u201czero or negative\u00a0\u2026", "num_citations": "7\n", "authors": ["310"]}
{"title": "Metamorphic testing: A simple yet effective approach for testing scientific software\n", "abstract": " Testing scientific software is a difficult task due to their inherent complexity and the lack of test oracles. In addition, these software systems are usually developed by end-user developers who are not normally trained as professional software developers nor testers. These factors often lead to inadequate testing. Metamorphic testing (MT) is a simple yet effective testing technique for testing such applications. Even though MT is a wellknown technique in the software testing community, it is not very well utilized by the scientific software developers. The objective of this paper is to present MT as an effective technique for testing scientific software. To this end, we discuss why MT is an appropriate testing technique for scientists and engineers who are not primarily trained as software developers. Specifically, how it can be used to conduct systematic and effective testing on programs that do not have test oracles without\u00a0\u2026", "num_citations": "7\n", "authors": ["310"]}
{"title": "Distribution-aware mutation analysis\n", "abstract": " Mutation analysis is widely employed to evaluate the effectiveness of various software testing techniques. In most situations, mutation operators are uniformly applied to the original programs, while the faults tend to be clustered in practice. This may result in the inappropriate simulation of faults, and thus cannot deliver the reliable evaluation results. To overcome this, we propose a distribution-aware mutation analysis technique and conducted empirical studies to investigate the impact of the mutation distribution on the effectiveness evaluation of testing techniques. As an illustration, we select the commonly practiced random testing technique and two versions of dynamic random testing techniques and apply them to testing Web services. Results of empirical studies suggest that the mutation distribution significantly affects the evaluation results. This observation further indicates that the effectiveness of testing\u00a0\u2026", "num_citations": "7\n", "authors": ["310"]}
{"title": "Choices, Choices: Comparing between CHOC\u2019LATE and the Classification-Tree Methodology\n", "abstract": " Two popular specification-based test case generation methods are the choice relation framework and the classification-tree methodology. Both of them come with associated tools and have been used in different applications with success. Since both methods are based on the idea of partition testing, they are similar in many aspects. Because of their similarities, software testers often find it difficult to decide which method to be used in a given testing scenario. This paper aims to provide a solution by first contrasting the strengths and weaknesses of both methods, followed by suggesting practical selection guidelines to cater for different testing scenarios.", "num_citations": "7\n", "authors": ["310"]}
{"title": "Using the incremental approach to generate test sets: A case study\n", "abstract": " With the increasing complexity of software systems, the set of testing requirements can become very large. If the set of testing requirements can be naturally decomposed into smaller subsets, one may construct a test set separately to satisfy each subset of testing requirements, and then combine the test sets to form the complete test set. Such an approach is referred to as the union approach. On the other hand, the incremental approach attempts to incrementally expand a test set to satisfy the subsets of testing requirements, one at a time. This paper investigates empirically the effect of the incremental approach as compared to the union approach. Our case study indicates that the incremental approach can result in a significantly smaller test set, particularly when supplemented with the greedy heuristics.", "num_citations": "7\n", "authors": ["310"]}
{"title": "On the use of the classification-tree method by beginning software testers\n", "abstract": " The classification-tree method is a black box testing method first introduced by Grochtmann and Grimm and recently enhanced by Chen et al. to become an integrated methodology. This paper describes a case study through which we attempt to understand the methods used by novice software testers to test their own programs, as well as their perception of the classification-tree method. The subjects of this study were advanced undergraduate students at a university with one year full-time working experience in the software industry. Our study found that white box testing methods were initially far more popular than black box methods, but the majority of them were convinced of the benefits of the classification-tree method after they have learned and used it.", "num_citations": "7\n", "authors": ["310"]}
{"title": "On the criteria of allocating test cases under uncertainty\n", "abstract": " A partition testing strategy consists of two components: a partitioning scheme which determines the way in which the program's input domain is partitioned into subdomains, and an allocation of test cases which determines the exact number of test cases selected from each subdomain. Whereas previous research studies have suggested many partitioning schemes, there have been few guidelines as to how the test allocations should be chosen, and in practice allocations are often done in an ad hoc manner. This paper investigates the problem of determining the test allocation when a particular partitioning scheme has been chosen. We show that this problem can be formulated as a classic problem of decision-making under uncertainty, and analyze the several most common criteria used to resolve this kind of problem. We also discuss the applicability and implications of applying these criteria in the context of\u00a0\u2026", "num_citations": "7\n", "authors": ["310"]}
{"title": "Dynamic dataflow analysis\n", "abstract": " This paper covers a comparison of the various solutions to some problems of dynamic dataflow analysis and proposes new solutions to some of these problems. It summarizes experiences of implementing and using several dynamic dataflow analysis systems. Some proposals about future directions and the essential features of automated dynamic dataflow analysis systems are also discussed in this paper.", "num_citations": "7\n", "authors": ["310"]}
{"title": "Adaptive partition testing\n", "abstract": " Random testing and partition testing are two major families of software testing techniques. They have been compared both theoretically and empirically in numerous studies for decades, and it has been widely acknowledged that they have their own advantages and disadvantages and that their innate characteristics are fairly complementary to each other. Some work has been conducted to develop advanced testing techniques through the integration of random testing and partition testing, attempting to preserve the advantages of both while minimizing their disadvantages. In this paper, we propose a new testing approach, adaptive partition testing, where test cases are randomly selected from some partition whose probability of being selected is adaptively adjusted along the testing process. We particularly develop two algorithms, Markov-chain based adaptive partition testing and reward-punishment based\u00a0\u2026", "num_citations": "6\n", "authors": ["310"]}
{"title": "Harnessing multiple source test cases in metamorphic testing: A case study in bioinformatics\n", "abstract": " Metamorphic testing (MT) has been applied to software verification, validation and quality assessment. In mostprevious studies, research has focused on deriving metamorphic relations (MRs) such that the input of one or more follow-up test cases is generated from one source test case. We note that some programs under test (PUT) naturally take multiple inputs and process them simultaneously to generate multiple outputs. This type of programs are common in the field of big data analysis and bioinformatics. This means, in the source execution of the program, we can obtain multiple outputs from multiple source test cases. Here we consider a type of MR in which multiple follow-up test cases are generated from multiple source test cases simultaneously. We hypothesise that harnessing the outputs from multiple source test cases enables us to obtain additional information about the PUT, and therefore allows us to\u00a0\u2026", "num_citations": "6\n", "authors": ["310"]}
{"title": "Bottom-up integration testing with the technique of metamorphic testing\n", "abstract": " In previous studies on the application of Metamorphic Testing (MT), testing was usually conducted at the system level directly, without any well-planned integration strategy. This could lead to a difficulty in constructing Metamorphic Relations (MRs), as well as tracking and debugging the faults. On the other hand, traditional integration testing requires extra cost. In this paper, we combine the bottom-up integration approach with MT to address all the above problems. We use the Feature Selection (FS) system to illustrate our method and a 2-phase MT is conducted. With this 2-phase MT method, we can ease the MR identification and provide a clear hierarchy to easily track and isolate the faults for a complex system. Moreover, since such an integration approach is logically achieved with MRs, it does not need to have those specific set-ups as required in traditional integration testing.", "num_citations": "6\n", "authors": ["310"]}
{"title": "Towards dynamic random testing for web services\n", "abstract": " In recent years, Service Oriented Architecture (SOA) has been increasingly adopted to develop applications in the context of Internet. To develop reliable SOA-based applications, an important issue is how to ensure the quality of Web services. In this paper, we propose a dynamic random testing (DRT) technique for Web services which is an improvement of the widely practiced random testing. We examine key issues when adapting DRT to the context of SOA and develop a prototype for such an adaptation. Empirical studies are reported where DRT is used to test two real-life Web services and mutation analysis is employed to measure the effectiveness. The experimental results show that DRT can save up to 24% test cases in terms of detecting the first seeded fault, and up to 21% test cases in terms of detecting all seeded faults, both with the cases of uniformed mutation analysis and distribution-aware mutation\u00a0\u2026", "num_citations": "6\n", "authors": ["310"]}
{"title": "An innovative approach to randomising quasi-random sequences and its application into software testing\n", "abstract": " Quasi-random sequences, which can evenly spread points across a hypercube, have been widely used in various areas. Recently, quasi-random testing technique, which makes use of quasi-random sequences to generate test cases, was proposed, and it normally has a higher failure-detection capability than pure random testing. However, there exist only a few distinct quasi-random sequences in the literature, and all these sequences are deterministic rather than random. Therefore, the applicability of quasi-random sequences in testing is restricted. In this paper, we propose a new approach to randomising quasi-random sequences. Out approach can generate many distinct randomised quasi-random sequences that have even distributions of points. The experimental results also show that these sequences can significantly enhance the effectiveness of random testing.", "num_citations": "6\n", "authors": ["310"]}
{"title": "An empirical evaluation and analysis of the fault-detection capability of MUMCUT for general Boolean expressions\n", "abstract": " Boolean expressions are extensively used in software specifications. It is important to generate a small-sized test set for Boolean expressions without sacrificing the fault-detection capability. MUMCUT is an efficient test case generation strategy for Boolean expressions in Irredundant Disjunctive Normal Form (IDNF). In the real world, however, Boolean expressions written by a software designer or programmer are not usually in IDNF. In this paper, we apply MUMCUT to generate test cases for general Boolean expressions and develop a mutation-based empirical evaluation of the effectiveness of this application. The experimental data show that MUMCUT can still detect single seeded faults in up to 98. 20% of general Boolean expressions. We also analyze patterns where test cases generated by MUMCUT cannot detect the seeded faults.", "num_citations": "6\n", "authors": ["310"]}
{"title": "Teaching family planning with expert system\n", "abstract": " In this paper, we present our experience of teaching medical students with the use of computers. An expert system was developed as a computer aided learning system for medical students and doctors to learn about family planning. Our expert system is a rule-based one in Arity Prolog, and can be run under an IBM-PC. Its rule-based representation of knowledge and explanation capability have been found to be very useful and stimulating in the learning process. In addition to being a computer aided learning system, it can also be used as a supporting tool to the medical doctors. We also describe the methodology of building up our expert system.", "num_citations": "6\n", "authors": ["310"]}
{"title": "On data flow analysis across a subroutine boundary\n", "abstract": " Abstract correctness of the program, even if the program is correct with respect to all Data flow analysis has been used in recent years to ensure software quality. Data flow anomalies, which are indica-. tions that programming errors might have been committed, can be detected ii. the analysis. Huang has proposed a method for performing the analysi\u0105 by means of program instrumentation. In this paper, we extend Huang's technique and present a new method for handling data flow analysis across a subroutine boundary. The method is simple, easy to implement, and has a better anomalydetection and anomaly-location capability. In addition, it can be used in line with the technique of modular programming in software development to increase software's reliability. these test data. Thus, developing better criteria for selecting test data has been given much attention. 3", "num_citations": "6\n", "authors": ["310"]}
{"title": "Diversity driven adaptive test generation for concurrent data structures\n", "abstract": " ContextTesting concurrent data structures remains a notoriously challenging task, due to the nondeterminism of multi-threaded tests and the exponential explosion on the number of thread schedules.ObjectiveWe propose an automated approach to generate a series of concurrent test cases in an adaptive manner, i.e., the next test cases are generated with the guarantee to discover the thread schedules that have not yet been activated by the previous test cases.MethodTwo diversity metrics are presented to induce such adaptive test cases from a static and a dynamic perspective, respectively. The static metric enforces the diversity in the program structures of the test cases; while the dynamic one enforces the diversity in their capabilities of exposing untested thread schedules. We implement three adaptive test generation approaches for C/C++ concurrent data structures, based on the state-of-the-art active testing\u00a0\u2026", "num_citations": "5\n", "authors": ["310"]}
{"title": "A revisit of the integration of metamorphic testing and test suite based automated program repair\n", "abstract": " The technique of metamorphic testing (MT) has been integrated with test suite based automated program repair (APR) to alleviate the test oracle problem of APR. The proposed integration yields APR-MT techniques, which can be applied regardless of the existence of a test oracle. In a previous study, the feasibility and effectiveness of the APR-MT technique have been demonstrated via GenProg-MT, an integration of MT and the APR technique GenProg. This paper aims to complement our previous study to investigate the feasibility and effectiveness of APR-MT across different categories of APR techniques. We present the integration of MT with CETI, an APR technique belonging to a different category to GenProg, and conductexperimental analysis on the integrated technique CETI-MT, showing that CETI-MT is comparable to CETI in terms of the repair effectiveness. These results not only demonstrate the feasibility\u00a0\u2026", "num_citations": "5\n", "authors": ["310"]}
{"title": "Error trapping and metamorphic testing for spreadsheet failure detection\n", "abstract": " This study deepens the research on error trapping (ET) and metamorphic testing (MT) for detecting spreadsheet failures. It is observed that some spreadsheet developers and testers are confused between ET and MT, because the two techniques are similar to each other in some aspects. Inspired by the observation, this paper first outlines the main concepts of ET and MT using several examples for illustration. This is followed by discussing an experiment with a view to investigating and comparing the failure detection capabilities of the two techniques. The results of the experiment indicate that neither technique is sufficient in spreadsheet testing. Rather, ET and MT are complementary and they should be used together in spreadsheet testing whenever possible.", "num_citations": "5\n", "authors": ["310"]}
{"title": "Input-driven active testing of multi-threaded programs\n", "abstract": " It is still a challenge to select \"good\" test inputs for concurrent programs within limited testing resources. We present in this paper a test case diversity metric for multi-threaded programs, which evaluates a test input with its effect in exposing concurrent thread interactions. We then propose an input-driven active testing approach with two test input selection strategies based on our test case diversity metric. We implement our testing approach based on Maple, an interleaving coverage-driven active testing tool. The effectiveness and efficiency of our testing approach are compared closely with Maple, which on its own is supplied with random test inputs. Experimental results show that our testing approach can outperform the original active testing approach in the number of test inputs executed and the time usage for fulfilling the interleaving coverage criterion of Maple. The selected test inputs based on our test case\u00a0\u2026", "num_citations": "5\n", "authors": ["310"]}
{"title": "Controlling Restricted Random Testing: An Examination of the Exclusion Ratio Parameter.\n", "abstract": " In Restricted Random Testing (RRT), the main control parameter is the Target Exclusion Ratio (R), the proportion of the input domain to be excluded from test case generation at each iteration. Empirical investigations have consistently indicated that best failure-finding performance is achieved when the value for the Target Exclusion Ratio is maximised, ie close to 100%. This paper explains an algorithm to calculate the Actual Exclusion Ratio for RRT, and applies the algorithm to several simulations, confirming that previous empirically determined values for the Maximum Target Exclusion Ratio do give Actual Exclusion Ratios close to 100%. Previously observed trends of improvement in failure-finding efficiency of RRT corresponding to increases in Target Exclusion Ratios are also identified for Actual Exclusion Ratios.", "num_citations": "5\n", "authors": ["310"]}
{"title": "On the effectiveness of the optimally refined proportional sampling testing strategy\n", "abstract": " Recently, the effectiveness of subdomain testing and random testing has been studied analytically. T.Y. Chen and Y.T. Yu (1994) found that, for the case of disjoint subdomains, as long as the number of test cases selected from each subdomain is proportional to its size (the proportional sampling strategy), the probability of revealing at least one failure using subdomain testing is not less than that using random testing. This paper investigates the effectiveness of the optimally refined proportional sampling (ORPS) strategy, which is a special case of the proportional sampling strategy. The ORPS strategy is simple in concept, and the implementation cost is usually low. An empirical study has been conducted for a sample of published programs with seeded errors. The performance of this strategy was found to be better than random testing.", "num_citations": "5\n", "authors": ["310"]}
{"title": "Data flow analysis for COBOL\n", "abstract": " This paper proposes new definitions of actions, states and state-transition diagrams for usage in the dynamic data flow analysis of Cobol programs.", "num_citations": "5\n", "authors": ["310"]}
{"title": "Formalization of correctness of recursive definitions\n", "abstract": " We use the fixpoint approach to formalize the correctness of recursive definitions within the framework of first-order predicate calculus. Although the least fixpoint semantics is used, our results suggest some general methods of proving the correctness of recursive definitions without knowing their least fixpoints explicitly.", "num_citations": "5\n", "authors": ["310"]}
{"title": "Dynamic random testing of web services: a methodology and evaluation\n", "abstract": " In recent years, Service Oriented Architecture (SOA) has been increasingly adopted to develop distributed applications in the context of the Internet. To develop reliable SOA-based applications, an important issue is how to ensure the quality of web services. In this paper, we propose a dynamic random testing (DRT) technique for web services, which is an improvement over the widely-practiced random testing (RT) and partition testing (PT). We examine key issues when adapting DRT to the context of SOA, including a framework, guidelines for parameter settings, and a prototype for such an adaptation. Empirical studies are reported where DRT is used to test three real-life web services, and mutation analysis is employed to measure the effectiveness. Our experimental results show that, compared with the three baseline techniques, RT, Adaptive Testing (AT) and Random Partition Testing (RPT), DRT demonstrates\u00a0\u2026", "num_citations": "4\n", "authors": ["310"]}
{"title": "Semiautomated metamorphic testing approach for geographic information systems: An empirical study\n", "abstract": " A geographic information system (GIS) provides basic location-enabled services for many different applications related to navigation, education, and telecommunications. It is a foundation for analysis and visualization. Testing GIS is critical, but challenging due to the difficulty to assess the correctness of GIS outputs, which is called the test oracle problem of software testing. Metamorphic testing alleviates the problem by constructing metamorphic relations (MRs) among multiple inputs and outputs of the program under test. In this article, a semiautomated metamorphic testing (SAMT) method, based on the formal MR model and an improved adaptive random testing algorithm, was proposed to the GIS. To evaluate the performance of our approach, we conducted a case study on a superficial area calculation program, a typical component of GIS. Six kinds of MR construction methods were suggested for the GIS domain\u00a0\u2026", "num_citations": "4\n", "authors": ["310"]}
{"title": "Analysing the category-partition method and the classification-tree method for software testing\n", "abstract": " Analysing the category-partition method and the classification-tree method for software testing | PolyU Institutional Research Archive Skip navigation PolyU logo PIRA logo PolyU Library logo PolyU logo PolyU Library logo PIRA logo Home Collections Research Outputs Patents Theses Open Educational Resources Outstanding Work by Students Departments and Schools About Search Please use this identifier to cite or link to this item: http://hdl.handle.net/10397/56181 Title: Analysing the category-partition method and the classification-tree method for software testing Authors: Chen, TY Poon, PL Yu, YT Issue Date: 2000 Source: The 4th World Multiconference on Systemics, Cybernetics and Informatics and The 6th International Conference on Information Systems Analysis and Synthesis, Orlando, Florida, USA, 23-26 July 2000, v. 2, p. 446-451 Appears in Collections: Conference Paper Show full item record Page (s) \u2026", "num_citations": "4\n", "authors": ["310"]}
{"title": "On some characterisation problems of subdomain testing\n", "abstract": " Subdomain testing is a very general approach to the selection of test cases. It captures the characteristics of testing strategies that require the test suite to cover some predefined testing requirements. This paper attempts to characterise precisely the failure distributions for the best and worst case of any given subdomain testing strategy. Our analysis has revealed some crucial factors and principles that affect the effectiveness of subdomain testing strategies.", "num_citations": "4\n", "authors": ["310"]}
{"title": "Teaching Specification-Based Testing\n", "abstract": " Historically, relatively less emphasis has been placed on software testing in comparison with other activities, such as systems analysis and design, of the software life cycle in an undergraduate computer science or software engineering curriculum. Testing, however, is a common and important technique used to detect program faults. Thus, testing must be taught rigorously to the students. This paper reports our experience of teaching the classificationtree method as a black-box testing technique at The University of Melbourne and Swinburne University of Technology.", "num_citations": "4\n", "authors": ["310"]}
{"title": "A Polyp Detection Method Based on FBnet [J]\n", "abstract": " The incidence of colorectal cancer (colorectal cancer, CRC) in China has in-creased in recent years. The mortality rate of CRC has become one of the highest among all cancers; CRC also increasingly affects the health and quality of people\u2019s lives. However, due to the insufficiency of medical resources in China, the workload on medical doctors has further increased. In the past few decades, the adult CRC mortality and morbidity rate dropped sharply, mainly because of CRC screening and removal of adenomatous polyps. However, due to the differences in polyp itself and the skills of endoscopists, the detection rate of polyps varies greatly. In this paper, we adopt an anchor-free mechanism and introduce a better method to factorize the process of bounding box regression. Firstly, we regress the shape of object by the variant of Faster RCNN. Secondly, we re-define the target function of the location of object. The\u00a0\u2026", "num_citations": "3\n", "authors": ["310"]}
{"title": "Abstract test case prioritization using repeated small-strength level-combination coverage\n", "abstract": " Abstract test cases (ATCs) have been widely used in practice, including in combinatorial testing and in software product line testing. When constructing a set of ATCs, due to limited testing resources in practice (e.g., in regression testing), test case prioritization (TCP) has been proposed to improve the testing quality, aiming at ordering test cases to increase the speed with which faults are detected. One intuitive and extensively studied TCP technique for ATCs is \u03bb-wise Level-combination Coverage based Prioritization (\u03bbLCP), a static, black-box prioritization technique that only uses the ATC information to guide the prioritization process. A challenge facing \u03bbLCP, however, is the necessity for the selection of the fixed prioritization strength \u03bb before testing-testers need to choose an appropriate \u03bb value before testing begins. Choosing higher \u03bb values may improve the testing effectiveness of \u03bbLCP (e.g., by finding faults\u00a0\u2026", "num_citations": "3\n", "authors": ["310"]}
{"title": "An empirical comparison of fixed-strength and mixed-strength for interaction coverage based prioritization\n", "abstract": " Test case prioritization (TCP) plays an important role in identifying, characterizing, diagnosing, and correcting faults quickly. The TCP has been widely used to order test cases of different types, including model inputs (also called abstract test cases). Model inputs are constructed by modeling the program according to its input parameters, values, and constraints, and has been used in different testing methods, such as combinatorial interaction testing and software product line testing. The Interaction coverage-based TCP (ICTCP) uses interaction coverage information derived from the model input to order inputs. Previous studies have focused generally on the fixed-strength ICTCP, which adopts a fixed strength (i.e., the level of parameter interactions) to support the ICTCP process. It is generally accepted that using more strengths for ICTCP, i.e., mixed-strength ICTCP, may give better ordering than fixed-strength. To\u00a0\u2026", "num_citations": "3\n", "authors": ["310"]}
{"title": "Identifying failed test cases through metamorphic testing\n", "abstract": " When testing a program without a test oracle, it is impossible to know whether a test case will lead to a failure or not. The infeasibility to identify a failed test case severely restricts the applicability of many testing, debugging and fault localization techniques. However, with the help of Metamorphic Testing (MT) which was proposed to alleviate the test oracle problem, we are able to estimate how likely a test case is a failed test case.", "num_citations": "3\n", "authors": ["310"]}
{"title": "Metamorphic testing as a test case selection strategy\n", "abstract": " Software testing is a well-known and popular approach to ensuring quality software. It involves execution of the software under test (SUT), in an attempt to either offer some degree of confidence that the software is relatively bug free, or to actually uncover bugs or problems (called failures). Software testing has been studied for many years, and has generated a large body of approaches, methods, and techniques [1]. An assumption underlying many of these approaches, however, is that it is possible (and practical) to identify the correctness of the SUT, when tested. Unfortunately, this assumption does not always hold true, and software testers often face the challenge of testing software without being able to say whether or not the output is correct\u2014a situation known as the Oracle Problem.An innovative approach to dealing with the Oracle Problem is metamorphic testing (MT)[2], which, instead of attempting to verify the\u00a0\u2026", "num_citations": "3\n", "authors": ["310"]}
{"title": "Weight Analysis of Different Influence Factors on Reliability of Power Communication Networks [J]\n", "abstract": " With the construction of smart grid in recent years, as a service in the electric power industry, the power communication network has become important components in the system. Its reliability plays an important role in the normal production of electric power system. This paper presents a fuzzy analytic hierarchy process, which uses the fuzzy consistent matrix to quantitatively analyze the influence of each component on the reliability of power communication network. This approach greatly improves the scientific evaluation. By analyzing the calculation results, it is found that the failure time of power supply system, the rate of double channelization, the ring formation rate and the cable interruption time have great influences on the reliability of power communication networks.", "num_citations": "3\n", "authors": ["310"]}
{"title": "Poster: Enhancing Partition Testing through Output Variation\n", "abstract": " A major test case generation approach is to divide the input domain into disjoint partitions, from which test cases can be selected. However, we observe that in some traditional approaches to partition testing, the same partition may be associated with different output scenarios. Such an observation implies that the partitioning of the input domain may not be precise enough for effective software fault detection. To solve this problem, partition testing should be fine-tuned to additionally use the information of output scenarios in test case generation, such that these test cases are more fine-grained not only with respect to the input partitions but also from the perspective of output scenarios.", "num_citations": "3\n", "authors": ["310"]}
{"title": "Morphological algorithm for color objects classification\n", "abstract": " A mathematical morphology based approach for automated classification of color objects is explored in this paper. Using an adapted set of mathematical morphology operators, the initial color images are analyzed until final color classification is accomplished. Morphological feature extraction algorithm includes morphological color gradient, homotopic skeleton, Hit-or-Miss transform and other special morphological processing steps. In the end, illustrative application examples of the presented approach on real acquired images are also provided.", "num_citations": "3\n", "authors": ["310"]}
{"title": "Meso-and Neoproterozoic stratigraphic sequences in the southern Ural area and discovery of molar-tooth structures in carbonate and clastic rocks of the sequences and their\u00a0\u2026\n", "abstract": " \u63d0\u8981: \u4e2d\u2014\u65b0\u5143\u53e4\u4ee3\u5730\u5c42\u5728\u5357\u4e4c\u62c9\u5c14\u6d77\u69fd\u4e2d\u6781\u4e3a\u53d1\u80b2, \u5730\u5c42\u539a\u5ea6\u5de8\u5927, \u51e0\u4e2a\u9636\u6bb5\u7684\u6784\u9020\u6f14\u5316\u548c\u6c89\u79ef\u7279\u5f81\u6e05\u6670\u53ef\u89c1. \u65b0\u592a\u53e4\u4ee3\u548c\u4e0b\u91cc\u83f2\u662f\u4fc4\u7f57\u65af\u91cd\u8981\u7684\u5927\u578b\u5c42\u72b6\u94c1\u77ff\u548c\u83f1\u9541\u77ff\u7684\u5bbf\u4e3b\u5730\u5c42, \u4e2d\u91cc\u83f2\u7fa4 (\u5143\u53e4\u5b99\u5730\u5c42) \u5730\u5c42\u539a\u5ea6\u6781\u5927, \u4f34\u968f\u4e86\u51e0\u6b21\u6c89\u79ef\u65cb\u56de, \u53d1\u80b2\u4e86\u4ece\u6df1\u6d77\u76f8\u5230\u5927\u9646\u7f13\u5761\u7684\u78b3\u9178\u76d0\u5ca9\u6c89\u79ef; \u968f\u7740\u65b0\u5143\u53e4\u4ee3\u672b\u6b21\u51b0\u671f\u4e4b\u540e, \u6587\u5fb7\u7cfb\u53d1\u80b2\u4e86\u53ef\u5168\u7403\u5bf9\u6bd4\u7684\u767d\u6d77\u52a8\u7269\u7fa4 (\u4f0a\u8fea\u5361\u62c9\u52a8\u7269\u7fa4). \u7b14\u8005\u9996\u6b21\u786e\u8ba4\u4e86\u5357\u4e4c\u62c9\u5c14\u5730\u533a\u4e2d\u2014\u65b0\u5143\u53e4\u4ee3\u5730\u5c42 3 \u5957\u81fc\u9f7f\u6784\u9020, \u5176\u4e2d\u5df4\u5361\u5c14\u7ec4 (Bakal) \u78b3\u9178\u76d0\u5ca9\u81fc\u9f7f\u6784\u9020\u4e0e\u788e\u5c51\u5ca9\u5730\u9707\u6db2\u5316\u8109\u4e92\u5c42\u5171\u751f, \u7279\u522b\u662f\u5927\u91cf\u81fc\u9f7f\u6784\u9020\u4e5f\u53d1\u80b2\u5728\u5927\u578b\u53e0\u5c42\u77f3\u4e2d. \u4ece\u81fc\u9f7f\u6784\u9020\u4e0e\u788e\u5c51\u5ca9\u6db2\u5316\u8109\u4e92\u5c42\u7684\u5171\u751f\u7279\u5f81, \u8bf4\u660e\u53d1\u80b2\u5728\u78b3\u9178\u76d0\u5ca9\u4e2d\u81fc\u9f7f\u6784\u9020\u4e0e\u5730\u9707\u673a\u7406\u7684\u6db2\u5316\u4f5c\u7528\u6709\u5173. \u8be5 3 \u5957\u81fc\u9f7f\u6784\u9020\u4e0e\u4e2d\u56fd\u534e\u5317\u5730\u53f0\u4e2d\u2014\u65b0\u5143\u53e4\u4ee3\u5730\u5c42\u4e2d\u53d1\u73b0\u7684\u81fc\u9f7f\u6784\u9020 (\u6db2\u5316\u8109) \u65f6\u4ee3\u5927\u4f53\u63a5\u8fd1.", "num_citations": "3\n", "authors": ["310"]}
{"title": "Dynamic data flow analysis for object oriented programs\n", "abstract": " There are many tools and techniques to help developers debug and test their programs. Dynamic data flow analysis is such a technique. Existing approaches for performing dynamic data flow analysis for object oriented programs have tended to be data focused and procedural in nature. An approach to dynamic data flow analysis that used object oriented principals would provide a more natural solution to analysing object oriented programs. Dynamic data flow analysis approaches consist of two primary aspects; a model of the data flow information, and a method for collecting action information from a running program. The model for data flow analysis presented in this thesis uses a meta-level object oriented approach. To illustrate the application of this meta-level model, a model for the Java programming language is presented. This provides an instantiation of the meta-level model provided. Finally, several methods are presented for collecting action information from Java programs. The meta-level model contains elements to represent both data items and scoping components (ie methods, blocks, objects, and classes). At runtime the model is used to create a representation of the executing program that is used to perform dynamic data flow analysis. The structure of the model is created in such a way that locating the appropriate meta-level entity follows the scoping rules of the language. In this way actions that are reported to the meta-model are routed through the model to their corresponding meta-level elements. The Java model presented contains classes that can be used to create the runtime representation of the program under analysis\u00a0\u2026", "num_citations": "3\n", "authors": ["310"]}
{"title": "On the identification of categories and choices for category-partition test case generation\n", "abstract": " The category-partition method and the classification-tree method help construct test cases from specifications. In both methods, an early step is to identify a set of categories (or classifications) and choices (or classes). This is often performed in an ad hoc manner due to the absence of systematic techniques. In this paper, we report and discuss three empirical studies to investigate the common mistakes made by software testers in such an ad hoc approach. The empirical studies serve three purposes:(a) to make the knowledge of common mistakes known to other testers so that they can avoid repeating the same mistakes,(b) to facilitate researchers and practitioners develop systematic identification techniques, and (c) to provide a means of measuring the effectiveness of newly developed identification techniques. Based on the results of our studies, we also formulate a checklist to help testers detect such mistakes.", "num_citations": "3\n", "authors": ["310"]}
{"title": "Classification-tree restructuring methodologies: a new perspective\n", "abstract": " The classification-tree method developed by Grochtmann et al. provided a useful approach for constructing test cases from functional specifications. It was automated by Chen and Poon through their tree construction methodology. In a follow-up study, Chen and Poon found that the effectiveness of constructing legitimate test cases could be improved under certain circumstances via a classification-tree restructuring algorithm. We develop another tree restructuring algorithm to cater for other situations not covered previously. The algorithms complement each other. We also compare the relative effectiveness of these algorithms and provide guidelines on applying them in practice.", "num_citations": "3\n", "authors": ["310"]}
{"title": "An experimental analysis of the identification of categories and choices from specifications\n", "abstract": " The category-partition method and classificationtree method both help construct test cases from specifications. To achieve this end, a set of categories and the associated choices (also known as classifications and the associated classes) have to be identified. However, the identification process is often done in an ad hoc manner. We have conducted a case study to examine the common mistakes made by software testers during this process. The result of our study will facilitate researchers and practitioners in the development of systematic identification methods.\u2217 c 2002 SNPD. This material is presented to ensure timely dissemination of scholarly and technical work. Personal use of this material is permitted. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author\u2019s copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder. Permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from SNPD 2002.\u2020 This work is supported in part by grants of the Research", "num_citations": "3\n", "authors": ["310"]}
{"title": "On the completeness of test cases for atomic arithmetic expressions\n", "abstract": " Most research on weak mutation testing focuses on predicate statements. Relative little attention has been paid to arithmetic expressions. In this paper we analyse the latter type of expression and prove that, given an atomic arithmetic expression, if it contains no variable or if the operator is the unary \"++\" or \"--\", then a single test case is sufficient and necessary to kill any fundamental mutant; otherwise, two test cases are sufficient and necessary.", "num_citations": "3\n", "authors": ["310"]}
{"title": "A revisit of the proportional sampling strategy\n", "abstract": " When test cases are selected with replacement, we have P/sub pss//spl ges/P/sub r/, where P/sub pss/ and P/sub r/ are the probabilities of detecting at least one failure by the proportional sampling strategy and random testing respectively. However, recent results on the proportional sampling strategy have highlighted that P/sub pss//spl ges/P/sub r/ may be very sensitive to whether test cases are selected with or without replacement. The paper provides more insights into this important issue, as the proportional sampling strategy is the most practically applicable partition testing strategy and it always performs better than random testing.", "num_citations": "3\n", "authors": ["310"]}
{"title": "An automated tool (IDAF) to manipulate interaction diagrams and fragmentations for multi-agent systems\n", "abstract": " Interaction diagrams are used in multi-agent systems to graphically describe agent computation threads and communications while fragmentations are the algebraic representations of interaction diagrams.  The IDAF (Interaction Diagrams And Fragmentations) tool suite has been developed based on the formalism of interaction diagrams and fragmentations.  The tool suite consists of ValidatoR, FormatteR, TranslatoR, GrapheR and TesteR. This paper describes the usage of the tool suite and demonstrates it in two different multi-agent systems.", "num_citations": "3\n", "authors": ["310"]}
{"title": "In Black and White: An Integrated Approach to Object-Oriented Program Testing\n", "abstract": " Given a pair of equivalent terms as a test case, we should then determine whether the objects that result from executing the implemented program are observationally equivalent. We prove, however, the observational equivalence of objects cannot be determined using a finite set of observable contexts (namely, operation sequences ending with an observer function) derived from any black-box technique. Hence we supplement our approach with a \u201crelevant observable context\u201d technique, which is a white-box technique, to determine the observational equivalence. The relevant observable contexts are constructed from a Data Member Relevance Graph, which is an abstraction of the given implementation for a given specification.", "num_citations": "3\n", "authors": ["310"]}
{"title": "On some properties of the optimally refined proportional sampling strategy\n", "abstract": " The optimally refined proportional sampling strategy has been recommended as a better alternative to random testing, when dividing the input domain into equal-sized partitions is easy. This paper investigates some properties of the optimally refined proportional sampling strategy. This investigation provides some useful information on how well the optimally refined proportional sampling strategy outperforms random testing.", "num_citations": "3\n", "authors": ["310"]}
{"title": "Formalization of equivalence of recursively defined functions\n", "abstract": " In this paper we use the fixpoint approach in formalizing the equivalence of recursively defined functions within the framework of predicate calculus. The results suggest some general mathematical techniques useful in proving equivalence of recursive programs.", "num_citations": "3\n", "authors": ["310"]}
{"title": "An experimental analysis of fault detection capabilities of covering array constructors\n", "abstract": " Combinatorial Interaction Testing (CIT) aims at constructing an effective test suite, such as a Covering Array (CA), that can detect faults that are caused by the interaction of parameters. In this paper, we report on some empirical studies conducted to examine the fault detection capabilities of five popular CA constructors: ACTS, Jenny, PICT, CASA, and TCA. The experimental results indicate that Jenny has the best performance, because it achieves better fault detection than the other four constructors in many cases. Our results also indicate that CAs generated using ACTS, PICT, or CASA should be prioritized before testing.", "num_citations": "2\n", "authors": ["310"]}
{"title": "Out of sight, out of mind: a distance-aware forgetting strategy for adaptive random testing\n", "abstract": " Adaptive random testing (ART) achieves better failure-detection effectiveness than random testing by increasing the diversity of test cases. However, the intention of ensuring even spread of test cases inevitably causes an overhead problem. Although two basic forgetting strategies (ie random forgetting and consecutive retention) were proposed to reduce the computation cost of ART, they only considered the temporal distribution of test cases. In the paper, we presented a distance-aware forgetting strategy for the fixed size candidate set version of ART (DF-FSCS), in which the spatial distribution of test cases is taken into consideration. For a given candidate, the test cases out of its \u201csight\u201d are ignored to reduce the distance computation cost. At the same time, the dynamic adjustment for partitioning and the second-round forgetting are adopted to ensure the linear complexity of DF-FSCS algorithm. Both simulation analysis and empirical study are employed to investigate the efficiency and effectiveness of DF-FSCS. The experimental results show that DF-FSCS significantly outperforms the classical ART algorithm FSCS-ART in efficiency, and has comparable failure-detection effectiveness. Com-pared with two basic forgetting methods, DF-FSCS is better in both efficiency and effectiveness. In contrast with a typical linear-time ART algorithm RBCVT-Fast, our algorithm requires less computational overhead and exhibits the similar failure-detection capability. In addition, DF-FSCS has more reliable performance than RBCVT-Fast in detecting failures for the programs with high-dimensional input domain.", "num_citations": "2\n", "authors": ["310"]}
{"title": "Incremental identification of categories and choices for test case generation: a study of the software practitioners' preferences\n", "abstract": " Test case generation is a vital procedure in the engineering of test harnesses. In particular, the choice relation framework and the category-partition method play an important role, by requiring software testers to identify categories (intuitively equivalent to input parameters or environment conditions) and choices (intuitively equivalent to ranges of values) from a specification and to systematically work on the identified choices to generate test cases. Other specification-based test case generation methods (such as the classification-tree method, cause-effect graphing, and combinatorial testing) also have similar requirements, although different terminology such as classifications and classes is used in place of categories and choices. For a large and complex specification that contains many specification components, categories and choices may be identified separately from various kinds of components. We call this\u00a0\u2026", "num_citations": "2\n", "authors": ["310"]}
{"title": "Correlation between time-delayed rockburst and blasting disturbance in deep-buried tunnel\n", "abstract": " The deep-buried tunnels in Jinping II Hydropower Station are characterized by great depth, long length, large size in diameter and high geo-stress. During the excavation, by drilling and blasting method, of these tunnels, lots of the time-delayed rockbursts occurred; and blasting disturbance was one of triggering factors of their occurrence. This paper discusses the correlation between time-delayed rockbursts and blasting disturbance profoundly from two aspects of blasting disturbance strength and the delayed time of the time-delayed rockbursts. And the main conclusions were presented as follows:", "num_citations": "2\n", "authors": ["310"]}
{"title": "An enhanced flow analysis technique for detecting unreachability faults in concurrent systems\n", "abstract": " We present a flow analysis technique for detecting unreachable states and actions in concurrent systems. It is an enhancement of the approach by Cheung and Kramer. Each process of a concurrent system is modeled as a finite state machine, whose states represent process execution states and whose transitions are labeled by actions. We construct dependency sets incrementally and eliminate spurious paths by checking the execution sequences of actions. We prove mathematically that our algorithm can detect more unreachability faults than the well-known Reif/Smolka and Cheung/Kramer algorithms. The algorithm is easy to manage and its complexity is still polynomial to the system size. Case studies on two commonly used communication protocols show that the technique is effective.", "num_citations": "2\n", "authors": ["310"]}
{"title": "CHOC\u2019LATE: a CHOiCe reLATion framEwork for specification-based testing\n", "abstract": " In spite of its importance in software reliability, testing is labor intensive and expensive. It has been found that software testing without a good strategy may not be more effective than testing the system with random data. Obviously, the effectiveness of testing relies heavily on how well the test suite\u2014the set of test cases actually used\u2014is generated. This is because the comprehensiveness of the test suite will affect the scope of testing and, hence, the chance of revealing software faults.", "num_citations": "2\n", "authors": ["310"]}
{"title": "Dynamic Test Profiles in Adaptive Random Testing: A Case Study.\n", "abstract": " Random testing (RT) is a basic software testing method. When used to detect software failures, RT usually generates random test cases according to a uniform distribution. Adaptive random testing (ART) is an innovative approach to enhancing the failure-detection capability of RT. Most ART algorithms are composed of two independent processes, namely the candidate generation process and the test case identification process. In these ART algorithms, some program inputs are first randomly generated as the test case candidates; then test cases are identified from these candidates in order to ensure an even spread of test cases across the input domain. Most previous studies on ART focused on the enhancement of the test case identification process, while using the uniform distribution in the candidate generation process. A recent study has shown that using a dynamic test profile in the candidate generation process can also improve the failure-detection capability of ART. In this paper, we develop various test profiles and integrate them with the test case identification process of a particular ART algorithm, namely fixedsize-candidate-set ART. It is observed that all these test profiles can significantly improve the failure-detection capability of ART.", "num_citations": "2\n", "authors": ["310"]}
{"title": "On detecting double literal faults in Boolean expressions\n", "abstract": " Fault-based testing aims at selecting test cases to detect hypothesized faults in a program. When a program contains a fault so that it behaves differently from as is expected, test cases that satisfy the detection condition of the fault will be able to detect it. Detection conditions of single occurrence of hypothesized faults had been studied and used to propose testing strategies and investigate relationships between different types of faults. As software developers may make several mistakes during software development, multiple faults may occur in a program. In this paper, we report our study in detection conditions of double occurrences of faults related to literals in a Boolean expression. This leads to some interesting observations. Some previous studies in double fault discarded equivalent mutants caused by a single fault. However, we observe that such equivalent mutants may give rise to non-equivalent\u00a0\u2026", "num_citations": "2\n", "authors": ["310"]}
{"title": "Are successful test cases useless or not?\n", "abstract": " Test cases are said to be successful if they do not reveal failures. With the exception of fault-based testing, successful test cases are generally regarded as useless. If test cases are selected according to some testing objectives, intuitively speaking, successful test cases should still carry some information which may be useful. Hence, we are interested to see how to make use of this implicit information to help reveal failures. Recently, we have studied this problem from two different perspectives: one based on the properties of the problem to be implemented; and the other based on the notion of failure patterns which are formed by the failure-causing inputs, that is, inputs that reveal failures.", "num_citations": "2\n", "authors": ["310"]}
{"title": "Improving the Cost-Effectiveness of a Test Suite for User Acceptance Tests\n", "abstract": " In user acceptance testing (UAT), there are two essential requirements for a test suite (defined as the set of test cases used for testing), namely (a) the complete coverage of all possible transaction types of a program, and (b) a manageable size because of the testing resource constraints encountered by most software testers when conducting a UAT. Motivated by these two requirements, several test suite construction methods, such as the category-partition method and the classification-tree method, have been developed. In this paper, we propose a methodology to improve the cost-effectiveness of conducting UAT with test suites generated by these methods.", "num_citations": "2\n", "authors": ["310"]}
{"title": "Improving the effectiveness of the classification-tree methodology in software testing\n", "abstract": " This paper reports on the experience in the applications of the classification-tree methodology to a real-life system. As a result of the study, we found the need to supplement the original method by introducing a new relation operator and a new technique for constructing classification trees. Our supplement helps to enforce the completeness of test cases and improve on the effectiveness of classification trees.", "num_citations": "2\n", "authors": ["310"]}
{"title": "On the test allocations for the best lower bound performance of partition testing\n", "abstract": " A partition testing strategy divides the program's input domain into subdomains according to a partitioning scheme, and selects test cases from each subdomain according to a test allocation scheme. Previous studies have shown that partition testing strategies can be very effective or very ineffective in detecting faults, depending on both the partitioning scheme and the test allocation scheme. Given a partitioning scheme, the maximin criterion chooses a test allocation scheme that will maximally improve the lower bound performance of the partition testing strategy. In an earlier work, we have proved that the Basic Maximin Algorithm can generate such a test allocation scheme. In this paper, we derive further properties of the Basic Maximin Algorithm and present the Complete Maximin Algorithm that generates all possible test allocation schemes that satisfy the maximin criterion.", "num_citations": "2\n", "authors": ["310"]}
{"title": "Structural properties of post-dominator trees\n", "abstract": " The concepts of post dominators and post dominator trees are extensively used in code optimisation (J. Ferrante et al., 1987), program slicing (H. Agrawal and J.R. Horgan, 1990) and test suite reduction (R. Gupta and M.L. Soffa, 1093). The paper studies some characteristics of post dominator trees. These results can form the basis for the development of a more efficient construction algorithm of post dominator tree.", "num_citations": "2\n", "authors": ["310"]}
{"title": "More on the E-measure of subdomain testing strategies\n", "abstract": " The expected number of failures detected (the E-measure) has been found to be a useful measure of the effectiveness of testing strategies. This paper takes a fresh perspective on the formulation of the E-measure. From this, we deduce new sufficient conditions for subdomain testing to be better than random testing. These conditions are simpler and more easily applicable than many of those previously found. Moreover, we obtain new characterisations of subdomain testing strategies in terms of the E-measure.", "num_citations": "2\n", "authors": ["310"]}
{"title": "On the analysis of subdomain testing strategies\n", "abstract": " Weyuker and Jeng (1991) have investigated the conditions that affect the performance of partition testing and have compared analytically the fault-detecting ability of partition testing and random testing. Chen and Yu (1994) have generalized some of Weyuker and Jeng's results. We extend the analysis to subdomain testing in which subdomains may overlap. We derive several results for a special case and demonstrate a technique to extend some of our results to more general cases. We believe that this technique should be very useful in further investigating the behaviour of subdomain testing.", "num_citations": "2\n", "authors": ["310"]}
{"title": "An analysis of length equation using a dynamic approach\n", "abstract": " This paper is to compare the dynamic and static approaches towards the analysis of the length equation, which is the most fundamental and important equation in the theory of software science. In addition to intuition, our empirical results give further support to the claim that dynamic approach is more appropriate than the static approach towards the theory of software science.", "num_citations": "2\n", "authors": ["310"]}
{"title": "On the fixpoints of nondeterministic recursive definitions\n", "abstract": " (1) Computational semantics. The recursive definition is used recursively as an algorithm, subject to some computational rules for computing an equation. Once the computational rule is specified, the computed function is assumed to be exactly the function defined by the recursive definition.(2) Fixpoint semantics. The recursive definition is regarded as a functional equation. Its solution, which is known as fixpoint, is assumed to be the function defined by the recursive definition. Since a recursive definition may have no fixpoint or many fixpoints, it is of paramount importance to know what are the general conditions that ensure the existence of lixpoints, and to know how to select one as the basis for assigning a semantics to the definition when it has more than one fixpoint.", "num_citations": "2\n", "authors": ["310"]}
{"title": "On the relationship between computed functions and fixpoints of nondeterministic recursive definitions\n", "abstract": " (1) Computational approach: The recursive definition is used recursively as an algorithm, subject to some computational rules for computing an equation.", "num_citations": "2\n", "authors": ["310"]}
{"title": "Experience of teaching debugging and testing\n", "abstract": " Experience of teaching debugging and testing TY Chen & IK Mak Department of Computer Science, University of Melbourne, Parkville 3052, Australia ABSTRACT For most of the software engineering courses at the undergraduate levels, the emphasis is on the coding and pre-coding phases, that is, the phases of requirements analysis and specification, design as well as coding. The coverage of testing and maintenance is relatively superficial, despite of the fact that these two phases consume more resources than the others. One of the main reasons is that fewer methodologies in these two phases have been developed as compared with those in the other phases. Consequently, fewer literature on these areas appear in the undergraduate texts. Secondly, there is a lack of automated systems to support the teaching of these topics. However, the industry is now expecting to have a more thorough train-ing for the computer science graduates in the areas of debugging and testing, as sou", "num_citations": "2\n", "authors": ["310"]}
{"title": "On data flow testing strategy\n", "abstract": " For the studies on the data flow oriented selection of test data [4],[5],[6],[8], the selection strategies are all based on the intuition that an incorrect definition could never be uncovered if its use is not tested. In this paper, we are proposing another data flow oriented selection strategy based on the intuition that errors tend to be clustered together and are more likely to be associated with the improper or bad programming practice. It has been found that our strategy is complementary to [4],[5],[6] and [8], and can be incorporated at nominal overheads with Laski and Korel's strategy [5]. INTRODUCTION The technique of data flow analysis was originally used in compiler object code optimization [1]. Then, it has been used in program testing, to detect data flow anomalies [2],[7], and to select test data [4],[5].", "num_citations": "2\n", "authors": ["310"]}
{"title": "Validating class integration test order generation systems with Metamorphic Testing\n", "abstract": " Context:Previous studies proposed different kinds of approaches for class integration test order generation, and corresponding systems can be implemented based on these approaches. Such class integration test order generation systems can facilitate the process of software integration testing if they are implemented correctly.Objective:However, a test oracle problem exists in the class integration test order generation systems. Since these approaches for class integration test order generation normally deliver a local optimum rather than a global optimum, there are no practically feasible ways to validate their generated class integration test orders, that is, these implementation systems are untestable.Method:To address the test oracle problem, we apply Metamorphic Testing (MT) to validate class integration test order generation systems. Metamorphic Relations (MRs), which are the key components of MT, reason\u00a0\u2026", "num_citations": "1\n", "authors": ["310"]}
{"title": "Perception Matters: Detecting Perception Failures of VQA Models Using Metamorphic Testing\n", "abstract": " Visual question answering (VQA) takes an image and a natural-language question as input and returns a natural-language answer. To date, VQA models are primarily assessed by their accuracy on high-level reasoning questions. Nevertheless, Given that perception tasks (eg, recognizing objects) are the building blocks in the compositional process required by high-level reasoning, there is a demanding need to gain insights into how much of a problem low-level perception is. Inspired by the principles of software metamorphic testing, we introduce MetaVQA, a model-agnostic framework for benchmarking perception capability of VQA models. Given an image i, MetaVQA is able to synthesize a low level perception question q. It then jointly transforms (i, q) to one or a set of sub-questions and sub-images. MetaVQA checks whether the answer to (i, q) satisfies metamorphic relationships (MRs), denoting perception consistency, with the composed answers of transformed questions and images. Violating MRs denotes a failure of answering perception questions. MetaVQA successfully detects over 4.9 million perception failures made by popular VQA models with metamorphic testing. The state-of-the-art VQA models (eg, the champion of VQA 2020 Challenge) suffer from perception consistency problems. In contrast, the Oscar VQA models, by using anchor points to align questions and images, show generally better consistency in perception tasks. We hope MetaVQA will revitalize interest in enhancing the low-level perceptual abilities of VQA models, a cornerstone of high-level reasoning.", "num_citations": "1\n", "authors": ["310"]}
{"title": "ReMuSSE: A Redundant Mutant Identification Technique Based on Selective Symbolic Execution\n", "abstract": " Mutation testing is basically a fault-based software testing technique, which has been proposed to measure the fault detection effectiveness of a test suite using programs with simulated faults (namely mutants). However, mutation testing is time consuming and computationally expensive because of the normal use of a large amount of mutants. Thus, reducing the mutants is of great significance. To address this problem, various mutant reduction techniques have been proposed. Among them, the identification of redundant mutants aims at removing mutants whose test results can be inferred by other mutants. This article proposes a redundant mutant identification technique based on selective symbolic execution called ReMuSSE for weak mutation testing. Redundant mutants could be revealed by identifying those with similar program execution state changes within a program block involving mutated statements. An\u00a0\u2026", "num_citations": "1\n", "authors": ["310"]}
{"title": "Exploiting the Largest Available Zone: A Proactive Approach to Adaptive Random Testing by Exclusion\n", "abstract": " Adaptive random testing (ART) has been proposed to enhance the effectiveness of random testing (RT) through more even spreading of the test cases. In particular, restricted random testing (RRT) is an ART algorithm based on the intuition of skipping all the candidate test cases that are within the neighborhoods (or zones) of previously executed test cases. RRT has higher effectiveness than RT in terms of failure detection but incurs a higher time cost. In this paper, we aim to further reduce the time costs for RRT and improve the effectiveness for RT and ART methods. We propose a proactive technique known as \u201cRRT by largest available zone\u201d (RRT-LAZ). Like RRT, RRT-LAZ first defines an exclusion zone around every executed test case in order to determine the available zones. Unlike the original RRT, RRT-LAZ then compares all the available zones to proactively pick the largest one, from which the next test\u00a0\u2026", "num_citations": "1\n", "authors": ["310"]}
{"title": "An iterative metamorphic testing technique for web services and case studies\n", "abstract": " Metamorphic testing (MT) is an innovative approach to alleviating the oracle problem in software testing, which uses metamorphic relations of the program under test, instead of the test oracles, to verify its outputs. To alleviate the oracle problem of testing web services, we had previously proposed an MT framework for web services. In this paper, we further improve the efficiency and automation of this framework by leveraging metamorphic relations to iteratively generate test cases. We present a fixed-size iterative MT algorithm and implement it in the MT framework. We conduct three case studies to evaluate the fault detection effectiveness and efficiency of the proposed approach. Experimental results suggest that, compared with the conventional MT, iterative MT can achieve a comparable fault detection effectiveness, but with significantly fewer resources. Observations and limitations are summarised to provide new\u00a0\u2026", "num_citations": "1\n", "authors": ["310"]}
{"title": "Prioritising abstract test cases: an empirical study\n", "abstract": " Test-case prioritisation (TCP) attempts to schedule the order of test-case execution such that faults can be detected as quickly as possible. TCP has been widely applied in many testing scenarios such as regression testing and fault localisation. Abstract test cases (ATCs) are derived from models of the system under test and have been applied to many testing environments such as model-based testing and combinatorial interaction testing. Although various empirical and analytical comparisons for some ATC prioritisation (ATCP) techniques have been conducted, to the best of the authors' knowledge, no comparative study focusing on the most current techniques has yet been reported. In this study, they investigated 18 ATCP techniques, categorised into four classes. They conducted a comprehensive empirical study to compare 16 of the 18 ATCP techniques in terms of their testing effectiveness and efficiency. They\u00a0\u2026", "num_citations": "1\n", "authors": ["310"]}
{"title": "On the selection of strength for fixed-strength interaction coverage based prioritization\n", "abstract": " Abstract test cases are derived by modeling the system under test, and have been widely applied in practice, such as for software product line testing and combinatorial testing. Abstract test case prioritization (ATCP) is used to prioritize abstract test cases and aims at achieving higher rates of fault detection. Many ATCP algorithms have been proposed, using different prioritization criteria and information. One ATCP approach makes use of fixed-strength level-combinations information covered by abstract test cases, and is called fixed-strength interaction coverage based prioritization (FICBP). Before using FICBP, the prioritization strength \u03bb needs to be decided. Previous studies have generally focused on \u03bb values ranging between 1 and 6. However, no study has investigated the appropriateness of such a range, nor how to assign the prioritization strength for FICBP. To answer these questions, this paper reports on\u00a0\u2026", "num_citations": "1\n", "authors": ["310"]}
{"title": "Looking for an MR? Try METWiki today\n", "abstract": " Metamorphic Testing (MT) has been demonstrated to successfully alleviate oracle problems in many areas, including machine learning, compilers, bioinformatics, etc. However, given a new MT task, it is still very challenging to identify enough Metamorphic Relations (MRs) which are key components of MT. Aiming at this problem, we revisited previous MT applications and realized that they could form a very valuable database. Currently there is a lack of efficient link to get testers access these historical data. Therefore, in this paper, we propose to build METWiki, an MR repository, for collection and retrieval of these MRs. By providing exploration and search facilities, testers can find their desired MRs for reuse, reference, or inference. We also illustrate three sample usages of METWiki, which show important illuminations on how MRs can be obtained in practice.", "num_citations": "1\n", "authors": ["310"]}
{"title": "On the integration of metamorphic testing and model checking.\n", "abstract": " Metamorphic testing, an innovative software testing technique, generates test cases based on domain specific properties. Model checking is a technique that verifies software designs against system properties. Motivated by the fact that both techniques are based on some properties of software under development, we investigate how to integrate metamorphic testing and model checking. This paper will introduce and discuss some potential topics on this new research direction.", "num_citations": "1\n", "authors": ["310"]}
{"title": "On-line rescheduling for semiconductor manufacturing\n", "abstract": " Semiconductor wafer fabrication involves one of the most complex manufacturing processes ever used. To control such complex systems, it is a challenge to determine appropriate dispatching strategies under various system conditions. Dispatching strategies are crucial for the system performance, especially for the real time control of the system. In this paper, an interval variant rescheduling mechanism is proposed. In order to deploy different dispatching rules to different intrabays, k-means is used for clustering the intrabays of the fab. Then genetic algorithm (GA) is used for searching dispatching rule sets which promote better performance. In terms of the system conditions corresponding to dispatching rules, the support vector machine (SVM) classifier is constructed as the scheduler. In addition, the adaptive neuro-fuzzy inference system (ANFIS) prediction model is built for the sake of on-line deciding the\u00a0\u2026", "num_citations": "1\n", "authors": ["310"]}
{"title": "Temporal interaction diagrams\n", "abstract": " An inter-action diagram is a graphical view of computation processes and communication between different entities. It can be used for the design and testing of distributed systems. In particular, interaction diagrams offer significant advantages to the design of multi-agent systems, especially when they can be expressed in a linear form, known as fragmentation, facilitating automation of design and testing of such systems. Existing interaction diagram formalisms lack the capability of describing flexible temporal order constraints. They only support rigid temporal order, and hence have limited semantic expressiveness. In this paper, we propose an improved interaction diagram formalism in which more flexible temporal constraints can be expressed.", "num_citations": "1\n", "authors": ["310"]}
{"title": "Integrating approximation methods with the generalised proportional sampling strategy\n", "abstract": " Previous studies have shown that partition testing strategies can be very effective in detecting faults, but they can also be less effective than random testing under unfavourable circumstances. When test cases are allocated in proportion to the size of subdomains, partition testing strategies are provably better than random testing, in the sense of having a higher or equal probability of detecting at least one failure (the P-measure). Recently, the Generalised Proportional Sampling (GPS) strategy, which is always satisfiable, was proposed to relax the proportionality condition. The paper studies the use of approximation methods to generate test distributions satisfying the GPS strategy, and evaluates this proposal empirically. Our results are very encouraging, showing that on average about 98.72% to almost 100% of the test distributions obtained in this way are better than random testing in terms of the P-measure.", "num_citations": "1\n", "authors": ["310"]}
{"title": "CDFA: a testing system for C++\n", "abstract": " CDFA is a dynamic data flow analysis system for testing C++ programs. It instruments C++ programs with code that reports actions on data. The instrumented program is executed and monitored for anomalies. To effectively test C++ programs, CDFA incorporates extended actions, extended state transition diagrams, implicit state variables, functional instrumentation and object-based instrumentation.", "num_citations": "1\n", "authors": ["310"]}
{"title": "On the structural properties of the set of fixpoints for nondeterministic recursive definitions\n", "abstract": " In this paper, we investigate the structural properties of the set of fixpoints for the class of nondeterministic recursive definitions. Our study reveals close resemblance between the structural properties of the set of fixpoints and those of the set of prefixpoints, and it establishes some equality relationships between various fixpoints and their corresponding prefixpoints. Also observed are some nonequality relationships between various fixpoints and their corresponding postfixpoints.", "num_citations": "1\n", "authors": ["310"]}
{"title": "\u2018Evaluation of structure charts: a logic programming approach\n", "abstract": " We apply the techniques of logic programming to evaluate structure charts. We find that structure charts can be represented naturally in Prolog, and useful information can be derived in a straightforward manner. Standard techniques in the evaluation of structure charts can be formalized, and a few previous problems can be solved easily.", "num_citations": "1\n", "authors": ["310"]}
{"title": "On the consistency of multi-valued functions\n", "abstract": " Gallier1 generalised the notion of consistency in studying the sufficient conditions for the existence of optimal fixpoints. Chen2 introduced the notion of \u2a7d-relatedness in investigating some fixpoints for the class of non-deterministic recursive programs. In this paper, it is proved that the notions of \u2a7d-relatedness and consistency are equivalent for the class of multi-valued functions.", "num_citations": "1\n", "authors": ["310"]}