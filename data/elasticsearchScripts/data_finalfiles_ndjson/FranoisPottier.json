{"title": "Information flow inference for ML\n", "abstract": " This paper presents a type-based information flow analysis for a call-by-value \u03bb-calculus equipped with references, exceptions and let-polymorphism, which we refer to as ML. The type system is constraint-based and has decidable type inference. Its noninterference proof is reasonably light-weight, thanks to the use of a number of orthogonal techniques. First, a syntactic segregation between values and expressions allows a lighter formulation of the type system. Second, noninterference is reduced to subject reduction for a nonstandard language extension. Lastly, a semi-syntactic approach to type soundness allows dealing with constraint-based polymorphism separately.", "num_citations": "335\n", "authors": ["1751"]}
{"title": "Information flow inference for ML\n", "abstract": " This paper presents a type-based information flow analysis for a call-by-value \u03bb-calculus equipped with references, exceptions and let-polymorphism, which we refer to as Core ML. The type system is constraint-based and has decidable type inference. Its non-interference proof is reasonably lightweight, thanks to the use of a number of orthogonal techniques. First, a syntactic segregation between values and expressions allows a lighter formulation of the type system. Second, non-interference is reduced to subject reduction for a non-standard language extension. Lastly, a semi-syntactic approach to type soundness allows dealing with constraint-based polymorphism separately.", "num_citations": "290\n", "authors": ["1751"]}
{"title": "Information flow inference for free\n", "abstract": " This paper shows how to systematically extend an arbitrary type system with dependency information, and how soundness and non-interference proofs for the new system may rely upon, rather than duplicate, the soundness proof of the original system. This allows enriching virtually any of the type systems known today with information flow analysis, while requiring only a minimal proof effort. Our approach is based on an untyped operational semantics for a labelled calculus akin to core ML. Thus, it is simple, and should be applicable to other computing paradigms, such as object or process calculi. The paper also discusses access control, and shows it may be viewed as entirely independent of information flow control. Letting the two mechanisms coexist, without interacting, yields a simple and expressive type system, which allows, in particular,\" selective\" declassification.", "num_citations": "165\n", "authors": ["1751"]}
{"title": "Simplifying subtyping constraints\n", "abstract": " This paper studies type inference for a functional, ML-style language with subtyping, and focuses on the issue of simplifying inferred constraint sets. We propose a powerful notion of entailment between constraint sets, as well as an algorithm to check it, which we prove to be sound. The algorithm, although very powerful in practice, is not complete. We also introduce two new typing rules which allow simplifying constraint sets. These rules give very good practical results.", "num_citations": "125\n", "authors": ["1751"]}
{"title": "A simple view of type-secure information flow in the/spl pi/-calculus\n", "abstract": " One way of enforcing an information flow control policy is to use a static type system capable of guaranteeing a noninterference property. Noninterference requires that two processes with distinct \"high\"-level components, but common \"low\"-level structure, cannot be distinguished by \"low\"-level observers. We state this property in terms of a rather strict notion of process equivalence, namely weak barbed reduction congruence. Because noninterference is not a safety property, it is often regarded as more difficult to establish than a conventional type safety result. This paper aims to provide an elementary noninterference proof in the setting of the /spl pi/-calculus. This is done by reducing the problem to subject reduction - a safety property - for a nonstandard, but fairly natural, extension of the /spl pi/-calculus, baptized the -calculus.", "num_citations": "101\n", "authors": ["1751"]}
{"title": "Simplifying subtyping constraints: a theory\n", "abstract": " This paper offers a theoretical study of constraint simplification, a fundamental issue for the designer of a practical type inference system with subtyping. In the simpler case where constraints are equations, a simple isomorphism between constrained type schemes and finite state automata yields a complete constraint simplification method. Using it as a guide for the intuition, we move on to the case of subtyping, and describe several simplification algorithms. Although no longer complete, they are conceptually simple, efficient, and very effective in practice. Overall, this paper gives a concise theoretical account of the techniques found at the core of our type inference system. Our study is restricted to the case where constraints are interpreted in a non-structural lattice of regular terms. Nevertheless, we highlight a small number of general ideas, which explain our algorithms at a high level and may be applicable to a\u00a0\u2026", "num_citations": "95\n", "authors": ["1751"]}
{"title": "A framework for type inference with subtyping\n", "abstract": " In type systems based on subtyping, type equality is replaced with subtyping, which is a less restrictive relationship. The idea is, if 71 is a subtype of~ 2, then a value of type~ 1 can be transparently supplied wherever a value of type r~ is expected.Subtyping has been used as a key concept to create formal type systems for object-oriented languages. These systems often require the programs to be annotated with user-supplied type information. Being able to omit this information--or at least part of it-provides the programmer with a greater degree of freedom; hence, the desire arises to do type inference in the presence of subtyping. This issue has been extensively studied in the past few years. Many type systems have been proposed, with varying degrees of richness and complexity. Possible features are the existence of a least type I and a greatest type T, the presence of contravariant type constructors such as+, the\u00a0\u2026", "num_citations": "84\n", "authors": ["1751"]}
{"title": "A systematic approach to static access control\n", "abstract": " The Java JDK 1.2 Security Architecture includes a dynamic mechanism for enforcing access control checks, so-called stack inspection. This paper studies type systems which can statically guarantee the success of these checks. We develop these systems using a new, systematic methodology: we show that the security-passing style translation, proposed by Wallach and Felten as a dynamic implementation technique, also gives rise to static security-aware type systems, by composition with conventional type systems. To define the latter, we use the general HM(X) framework, and easily construct several constraint- and unification-based type systems. They offer significant improvements on a previous type system for JDK access control, both in terms of expressiveness and in terms of readability of inferred type specifications.", "num_citations": "80\n", "authors": ["1751"]}
{"title": "Stratified type inference for generalized algebraic data types\n", "abstract": " Stratified type inference for generalized algebraic data types.", "num_citations": "77\n", "authors": ["1751"]}
{"title": "An overview of C\u03b1ml\n", "abstract": " C\u03b1ml is a tool that turns a so-called \u201cbinding specification\u201d into an Objective Caml compilation unit. A binding specification resembles an algebraic data type declaration, but also includes information about names and binding. C\u03b1ml is meant to help writers of interpreters, compilers, or other programs-that-manipulate-programs deal with \u03b1-conversion in a safe and concise style. This paper presents an overview of C\u03b1ml's binding specification language and of the code that C\u03b1ml produces.", "num_citations": "76\n", "authors": ["1751"]}
{"title": "Functional translation of a calculus of capabilities\n", "abstract": " Reasoning about imperative programs requires the ability to track aliasing and ownership properties. We present a type system that provides this ability, by using regions, capabilities, and singleton types. It is designed for a high-level calculus with higher-order functions, algebraic data structures, and references (mutable memory cells). The type system has polymorphism, yet does not require a value restriction, because capabilities act as explicit store typings.", "num_citations": "70\n", "authors": ["1751"]}
{"title": "A versatile constraint-based type inference system\n", "abstract": " The combination of subtyping, conditional constraints and rows yields a powerful constraint-based type inference system. We illustrate this claim by proposing solutions to three delicate type inference problems: \"accurate\" pattern matchings, record concatenation, and first-class messages. Previously known solutions involved a different technique in each case: our theoretical contribution is in using only a single set of tools. On the practical side, this allows all three problems to benefit from a common set of constraint simplification techniques, a formal description of which is given in an appendix.", "num_citations": "67\n", "authors": ["1751"]}
{"title": "Type inference in the presence of subtyping: from theory to practice\n", "abstract": " From a purely theoretical point of view, type inference for a functional language with parametric polymorphism and subtyping poses little difficulty. Indeed, it suffices to generalize the inference algorithm used in the ML language, so as to deal with type inequalities, rather than equalities. However, the number of such inequalities is linear in the program size-whence, from a practical point of view, a serious efficiency and readability problem. To solve this problem, one must simplify the inferred constraints. So, after studying the logical properties of subtyping constraints, this work proposes several simplification algorithms. They combine seamlessly, yielding a homogeneous, fully formal framework, which directly leads to an efficient implementation. Although this theoretical study is performed in a simplified setting, numerous extensions are possible. Thus, this framework is realistic, and should allow a practical appearance of subtyping in languages with type inference. This document is the English version of the author's PhD thesis.", "num_citations": "67\n", "authors": ["1751"]}
{"title": "Static name control for FreshML\n", "abstract": " FreshML extends ML with constructs for declaring and manipulating abstract syntax trees that involve names and statically scoped binders. It is impure: name generation is an observable side effect. In practice, this means that FreshML allows writing programs that create fresh names and unintentionally fail to bind them. Following in the steps of early work by Pitts and Gabbay, this paper defines Pure FreshML, a subset of FreshML equipped with a static proof system that guarantees purity. Pure FreshML relies on a rich binding specification language, on user-provided assertions, expressed in a logic that allows reasoning about values and about the names that they contain, and on a conservative, automatic decision procedure for this logic. It is argued that pure FreshML can express non-trivial syntax-manipulating algorithms.", "num_citations": "64\n", "authors": ["1751"]}
{"title": "A constraint-based approach to guarded algebraic data types\n", "abstract": " We study HMG(X), an extension of the constraint-based type system HM(X) with deep pattern matching, polymorphic recursion, and guarded algebraic data types. Guarded algebraic data types subsume the concepts known in the literature as indexed types, guarded recursive datatype constructors, (first-class) phantom types, and equality qualified types, and are closely related to inductive types. Their characteristic property is to allow every branch of a case construct to be typechecked under different assumptions about the type variables in scope. We prove that HMG(X) is sound and that, provided recursive definitions carry a type annotation, type inference can be reduced to constraint solving. Constraint solving is decidable, at least for some instances of X, but prohibitively expensive. Effective type inference for guarded algebraic data types is left as an issue for future research.", "num_citations": "64\n", "authors": ["1751"]}
{"title": "A Hoare logic for call-by-value functional programs\n", "abstract": " We present a Hoare logic for a call-by-value programming language equipped with recursive, higher-order functions, algebraic data types, and a polymorphic type system in the style of Hindley and Milner. It is the theoretical basis for a tool that extracts proof obligations out of programs annotated with logical assertions. These proof obligations, expressed in a typed, higher-order logic, are discharged using off-the-shelf automated or interactive theorem provers. Although the technical apparatus that we exploit is by now standard, its application to call-by-value functional programming languages appears to be new, and (we claim) deserves attention. As a sample application, we check the partial correctness of a balanced binary search tree implementation.", "num_citations": "62\n", "authors": ["1751"]}
{"title": "Hiding local state in direct style: a higher-order anti-frame rule\n", "abstract": " Separation logic involves two dual forms of modularity: local reasoning makes part of the store invisible within a static scope, whereas hiding local state makes part of the store invisible outside a static scope. In the recent literature, both idioms are explained in terms of a higher-order frame rule. I point out that this approach to hiding local state imposes continuation-passing style, which is impractical. Instead, I introduce a higher-order anti-frame rule, which permits hiding local state in direct style. I formalize this rule in the setting of a type system, equipped with linear capabilities, for an ML-like programming language, and prove type soundness via a syntactic argument. Several applications illustrate the expressive power of the new rule.", "num_citations": "50\n", "authors": ["1751"]}
{"title": "Polymorphic typed defunctionalization\n", "abstract": " Defunctionalization is a program transformation that aims to turn a higher-order functional program into a first-order one, that is, to eliminate the use of functions as first-class values. Its purpose is thus identical to that of closure conversion. It differs from closure conversion, however, by storing a tag, instead of a code pointer, within every closure. Defunctionalization has been used both as a reasoning tool and as a compilation technique.Defunctionalization is commonly defined and studied in the setting of a simply-typed \u03bb-calculus, where it is shown that semantics and well-typedness are preserved. It has been observed that, in the setting of a polymorphic type system, such as ML or System F, defunctionalization is not type-preserving. In this paper, we show that extending System F with guarded algebraic data types allows recovering type preservation. This result allows adding defunctionalization to the toolbox of\u00a0\u2026", "num_citations": "50\n", "authors": ["1751"]}
{"title": "Verifying the correctness and amortized complexity of a union-find implementation in separation logic with time credits\n", "abstract": " Union-Find is a famous example of a simple data structure whose amortized asymptotic time complexity analysis is nontrivial. We present a Coq formalization of this analysis, following Alstrup et al.\u2019s recent proof. Moreover, we implement Union-Find as an OCaml library and formally endow it with a modular specification that offers a full functional correctness guarantee as well as an amortized complexity bound. In order to reason in Coq about imperative OCaml code, we use the CFML tool, which implements Separation Logic for a subset of OCaml, and which we extend with time credits. Although it was known in principle that amortized analysis can be explained in terms of time credits and that time credits can be viewed as resources in Separation Logic, we believe our work is the first practical demonstration of this approach. Finally, in order to explain the meta-theoretical foundations of our approach, we define\u00a0\u2026", "num_citations": "49\n", "authors": ["1751"]}
{"title": "A systematic approach to static access control\n", "abstract": " The Java Security Architecture includes a dynamic mechanism for enforcing access control checks, the so-called stack inspection process. While the architecture has several appealing features, access control checks are all implemented via dynamic method calls. This is a highly nondeclarative form of specification that is hard to read, and that leads to additional run-time overhead. This article develops type systems that can statically guarantee the success of these checks. Our systems allow security properties of programs to be clearly expressed within the types themselves, which thus serve as static declarations of the security policy. We develop these systems using a systematic methodology: we show that the security-passing style translation, proposed by Wallach et al. [2000] as a dynamic implementation technique, also gives rise to static security-aware type systems, by composition with conventional type\u00a0\u2026", "num_citations": "47\n", "authors": ["1751"]}
{"title": "Constraint-based type inference for guarded algebraic data types\n", "abstract": " Guarded algebraic data types subsume the concepts known in the literature as indexed types, guarded recursive datatype constructors, and first-class phantom types, and are closely related to inductive types. They have the distinguishing feature that, when typechecking a function defined by cases, every branch may be checked under different assumptions about the type variables in scope. This mechanism allows exploiting the presence of dynamic tests in the code to produce extra static type information. We propose an extension of the constraint-based type system HM(X) with deep pattern matching, guarded algebraic data types, and polymorphic recursion. We prove that the type system is sound and that, provided recursive function definitions carry a type annotation, type inference may be reduced to constraint solving. Then, because solving arbitrary constraints is expensive, we further restrict the form of type annotations and prove that this allows producing so-called tractable constraints. Last, in the specific setting of equality, we explain how to solve tractable constraints. To the best of our knowledge, this is the first generic and comprehensive account of type inference in the presence of guarded algebraic data types.", "num_citations": "44\n", "authors": ["1751"]}
{"title": "The essence of monotonic state\n", "abstract": " We extend a static type-and-capability system with new mechanisms for expressing the promise that a certain abstract value evolves monotonically with time; for enforcing this promise; and for taking advantage of this promise to establish non-trivial properties of programs. These mechanisms are independent of the treatment of mutable state, but combine with it to offer a flexible account of\" monotonic state\".", "num_citations": "43\n", "authors": ["1751"]}
{"title": "Machine-checked verification of the correctness and amortized complexity of an efficient union-find implementation\n", "abstract": " Union-Find is a famous example of a simple data structure whose amortized asymptotic time complexity analysis is non-trivial. We present a Coq formalization of this analysis. Moreover, we implement Union-Find as an OCaml library and formally endow it with a modular specification that offers a full functional correctness guarantee as well as an amortized complexity bound. Reasoning in Coq about imperative OCaml code relies on the CFML tool, which is based on characteristic formulae and Separation Logic, and which we extend with time credits. Although it was known in principle that amortized analysis can be explained in terms of time credits and that time credits can be viewed as resources in Separation Logic, we believe our work is the first practical demonstration of this approach.", "num_citations": "42\n", "authors": ["1751"]}
{"title": "A fistful of dollars: formalizing asymptotic complexity claims via deductive program verification\n", "abstract": " We present a framework for simultaneously verifying the functional correctness and the worst-case asymptotic time complexity of higher-order imperative programs. We build on top of Separation Logic with Time Credits, embedded in an interactive proof assistant. We formalize the O notation, which is key to enabling modular specifications and proofs. We cover the subtleties of the multivariate case, where the complexity of a program fragment depends on multiple parameters. We propose a way of integrating complexity bounds into specifications, present lemmas and tactics that support a natural reasoning style, and illustrate their use with a collection of examples.", "num_citations": "40\n", "authors": ["1751"]}
{"title": "Programming with permissions in Mezzo\n", "abstract": " We present Mezzo, a typed programming language of ML lineage. Mezzo is equipped with a novel static discipline of duplicable and affine permissions, which controls aliasing and ownership. This rules out certain mistakes, including representation exposure and data races, and enables new idioms, such as gradual initialization, memory re-use, and (type)state changes. Although the core static discipline disallows sharing a mutable data structure, Mezzo offers several ways of working around this restriction, including a novel dynamic ownership control mechanism which we dub \"adoption and abandon\".", "num_citations": "40\n", "authors": ["1751"]}
{"title": "A fresh look at programming with names and binders\n", "abstract": " A wide range of computer programs, including compilers and theorem provers, manipulate data structures that involve names and binding. However, the design of programming idioms which allow performing these manipulations in a safe and natural style has, to a large extent, remained elusive. In this paper, we present a novel approach to the problem. Our proposal can be viewed either as a programming language design or as a library: in fact, it is currently implemented within Agda. It provides a safe and expressive means of programming with names and binders. It is abstract enough to support multiple concrete implementations: we present one in nominal style and one in de Bruijn style. We use logical relations to prove that \"well-typed programs do not mix names with different scope\". We exhibit an adequate encoding of Pitts-style nominal terms into our system.", "num_citations": "35\n", "authors": ["1751"]}
{"title": "Polymorphic typed defunctionalization and concretization\n", "abstract": " Defunctionalization is a program transformation that eliminates functions as first-class values. We show that defunctionalization can be viewed as a type-preserving transformation of an extension of F with guarded algebraic data types into itself. We also suggest that defunctionalization is an instance of concretization, a more general technique that allows eliminating constructs other than functions. We illustrate this point by presenting two new type-preserving transformations that can be viewed as instances of concretization. One eliminates R\u00e9my-style polymorphic records; the other eliminates the dictionary records introduced by the standard compilation scheme for Haskell\u2019s type classes.", "num_citations": "31\n", "authors": ["1751"]}
{"title": "Subtyping recursive types modulo associative commutative products\n", "abstract": " This work sets the formal bases for building tools that help retrieve classes in object-oriented libraries. In such systems, the user provides a query, formulated as a set of class interfaces. The tool returns classes in the library that can be used to implement the user\u2019s request and automatically builds the required glue code. We propose subtyping of recursive types in the presence of associative and commutative products\u2014that is, subtyping modulo a restricted form of type isomorphisms\u2014as a model of the relation that exists between the user\u2019s query and the tool\u2019s answers. We show that this relation is a composition of the standard subtyping relation with equality up to associativity and commutativity of products and we present an efficient decision algorithm for it. We also provide an automatic way of constructing coercions between related types.", "num_citations": "31\n", "authors": ["1751"]}
{"title": "Time credits and time receipts in Iris\n", "abstract": " We present a machine-checked extension of the program logic Iris with time credits and time receipts, two dual means of reasoning about time. Whereas time credits are used to establish an upper bound on a program's execution time, time receipts can be used to establish a lower bound. More strikingly, time receipts can be used to prove that certain undesirable events-such as integer overflows-cannot occur until a very long time has elapsed. We present several machine-checked applications of time credits and time receipts, including an application where both concepts are exploited.", "num_citations": "29\n", "authors": ["1751"]}
{"title": "Syntactic soundness proof of a type-and-capability system with hidden state\n", "abstract": " This paper presents a formal definition and machine-checked soundness proof for a very expressive type-and-capability system, that is, a low-level type system that keeps precise track of ownership and side effects. The programming language has first-class functions and references. The type system's features include the following: universal, existential, and recursive types; subtyping; a distinction between affine and unrestricted data; support for strong updates; support for naming values and heap fragments via singleton and group regions; a distinction between ordinary values (which exist at runtime) and capabilities (which do not); support for dynamic reorganizations of the ownership hierarchy by disassembling and reassembling capabilities; and support for temporarily or permanently hiding a capability via frame and anti-frame rules. One contribution of the paper is the definition of the type-and-capability system\u00a0\u2026", "num_citations": "29\n", "authors": ["1751"]}
{"title": "A 3-part type inference engine\n", "abstract": " Extending a subtyping-constraint-based type inference framework with conditional constraints and rows yields a powerful type inference engine. We illustrate this claim by proposing solutions to three delicate type inference problems: \u201caccurate\u201d pattern matchings, record concatenation, and \u201cdynamic\u201d messages. Until now, known solutions required significantly different techniques; our theoretical contribution is in using only a single (and simple) set of tools. On the practical side, this allows all three problems to benefit from a common set of constraint simplification techniques, leading to efficient solutions.", "num_citations": "27\n", "authors": ["1751"]}
{"title": "Synth\u00e8se de types en pr\u00e9sence de sous-typage: de la th\u00e9orie \u00e0 la pratique\n", "abstract": " Ce travail traite de l'inference de types pour un langage dote de polymorphisme parametrique et de sous-typage, en s' attachant particulierement a la simplification des types inferes. L'algorithme d'inference est base sur la resolution de contraintes de sous-typage. Or, celles-ci sont en nombre proportionnel a la taille du programme, ce qui pose un probleme d'efficacite et de lisibilite. Il fallait donc concevoir un systeme de simplification des contraintes. De cette etude ressortent trois algorithmes de simplification, capables de reduire un jeu de contraintes sans en alterer la signification. De plus, les regles de typage ont ete reformulees de facon a faciliter le fonctionnement et l'integration de ces algorithmes. L'ensemble du systeme forme un tout homogene, clairement formalise, qui conduit directement a une implantation efficace. Le systeme obtenu est flexible, et supporte diverses extensions. Parmi les problemes non resolus, citons le typage des constructions imperatives dans le cadre de la compilation separee, ainsi que certaines inefficacites residuelles. En reponse a ces problemes, la mise au point d'un systeme de typage plus proche de celui de ml semble constituer une piste de recherche interessante.", "num_citations": "26\n", "authors": ["1751"]}
{"title": "The design and formalization of Mezzo, a permission-based programming language\n", "abstract": " The programming language Mezzo is equipped with a rich type system that controls aliasing and access to mutable memory. We give a comprehensive tutorial overview of the language. Then we present a modular formalization of Mezzo\u2019s core type system, in the form of a concurrent \u03bb-calculus, which we successively extend with references, locks, and adoption and abandon, a novel mechanism that marries Mezzo\u2019s static ownership discipline with dynamic ownership tests. We prove that well-typed programs do not go wrong and are data-race free. Our definitions and proofs are machine checked.", "num_citations": "24\n", "authors": ["1751"]}
{"title": "Numbering matters: First-order canonical forms for second-order recursive types\n", "abstract": " We study a type system equipped with universal types and equire-cursive types, which we refer to as F\u0358. We show that type equality may be decided in time O(nlog n), an improvement over the previous known bound of O(n2 ). In fact, we show that two more general problems, namely entailment of type equations and type unification, may be decided in time O(nlog n), a new result. To achieve this bound, we associate, with every F\u0358 type, a first-order canonical form, which may be computed in time O(nlogn). By exploiting this notion, we reduce all three problems to equality and unification of first-order recursive terms, for which efficient algorithms are known.", "num_citations": "24\n", "authors": ["1751"]}
{"title": "Towards efficient, typed LR parsers\n", "abstract": " The LR parser generators that are bundled with many functional programming language implementations produce code that is untyped, needlessly inefficient, or both. We show that, using generalized algebraic data types, it is possible to produce parsers that are well-typed (so they cannot unexpectedly crash or fail) and nevertheless efficient. This is a pleasing result as well as an illustration of the new expressiveness offered by generalized algebraic data types.", "num_citations": "23\n", "authors": ["1751"]}
{"title": "A constraint-based presentation and generalization of rows\n", "abstract": " We study the combination of possibly conditional nonstructural subtyping constraints with rows. We give a new presentation of rows, where row terms disappear; instead, we annotate constraints with filters. We argue that, in the presence of subtyping, this approach is simpler and more general. In the case where filters are finite or cofinite sets of row labels, we give a constraint solving algorithm whose complexity is O(n/sup 3/m log m), where n is the size of the constraint and m is the number of row labels that appear in it. We point out that this allows efficient type inference for record concatenation. Furthermore, by varying the nature of filters, we obtain several natural generalizations of rows.", "num_citations": "23\n", "authors": ["1751"]}
{"title": "Syntactic type soundness for HM (X)\n", "abstract": " The HM(X) framework is a constraint-based type framework with built-in let-polymorphism. This paper establishes purely syntactic type soundness for the framework, treating an extended version of the language containing state and recursive binding. These results demonstrate that any instance of HM(X), comprising a specialized constraint system and possibly additional functional constants and their types, enjoys syntactic type soundness.", "num_citations": "22\n", "authors": ["1751"]}
{"title": "Formal proof and analysis of an incremental cycle detection algorithm\n", "abstract": " We study a state-of-the-art incremental cycle detection algorithm due to Bender, Fineman, Gilbert, and Tarjan. We propose a simple change that allows the algorithm to be regarded as genuinely online. Then, we exploit Separation Logic with Time Credits to simultaneously verify the correctness and the worst-case amortized asymptotic complexity of the modified algorithm.", "num_citations": "21\n", "authors": ["1751"]}
{"title": "A semi-syntactic soundness proof for HM (X)\n", "abstract": " This document gives a soundness proof for the generic constraint-based type inference framework HM(X). Our proof is semi-syntactic. It consists in two steps. The first step is to define a ground type system, where polymorphism is extensional, and prove its correctness in a syntactic way. The second step is to interpret HM(X) judgements as (sets of) judgements in the underlying system, which gives a logical view of polymorphism and constraints. Overall, the approach may be seen as more modular than a purely syntactic approach: because polymorphism and constraints are dealt with separately, they do not clutter the subject reduction proof. However, it yields a slightly weaker result: it only establishes type soundness, rather than subject reduction, for HM(X).", "num_citations": "21\n", "authors": ["1751"]}
{"title": "Temporary read-only permissions for separation logic\n", "abstract": " We present an extension of Separation Logic with a general mechanism for temporarily converting any assertion (or \u201cpermission\u201d) to a read-only form. No accounting is required: our read-only permissions can be freely duplicated and discarded. We argue that, in circumstances where mutable data structures are temporarily accessed only for reading, our read-only permissions enable more concise specifications and proofs. The metatheory of our proposal is verified in Coq.", "num_citations": "20\n", "authors": ["1751"]}
{"title": "JOIN (X): Constraint-based type inference for the join-calculus\n", "abstract": " We present a generic constraint-based type system for the join-calculus. The key issue is type generalization, which, in the presence of concurrency, must be restricted.We first define a liberal generalization criterion, and prove it correct. Then, we find that it hinders type inference, and propose a cruder one, reminiscent of ML\u2019s value restriction. We establish type safety using a emsemi-syntactic technique, which we believe is of independent interest. It consists in interpreting typing judgements as (sets of) judgements in an underlying system, which itself is given a syntactic soundness proof.", "num_citations": "20\n", "authors": ["1751"]}
{"title": "Type soundness and race freedom for Mezzo\n", "abstract": " The programming language Mezzo is equipped with a rich type system that controls aliasing and access to mutable memory. We incorporate shared-memory concurrency into Mezzo and present a modular formalization of its core type system, in the form of a concurrent \u03bb-calculus, which we extend with references and locks. We prove that well-typed programs do not go wrong and are data-race free. Our definitions and proofs are machine-checked.", "num_citations": "18\n", "authors": ["1751"]}
{"title": "Depth-first search and strong connectivity in Coq\n", "abstract": " Using Coq, we mechanize Wegener's proof of Kosaraju's linear-time algorithm for computing the strongly connected components of a directed graph. Furthermore, also in Coq, we define an executable and terminating depth-first search algorithm.", "num_citations": "17\n", "authors": ["1751"]}
{"title": "Reachability and error diagnosis in LR (1) parsers\n", "abstract": " Given an LR (1) automaton, what are the states in which an error can be detected? For each such\" error state\", what is a minimal input sentence that causes an error in this state? We propose an algorithm that answers these questions. This allows building a collection of pairs of an erroneous input sentence and a (handwritten) diagnostic message, ensuring that this collection covers every error state, and maintaining this property as the grammar evolves. We report on an application of this technique to the CompCert ISO C99 parser, and discuss its strengths and limitations.", "num_citations": "16\n", "authors": ["1751"]}
{"title": "Lazy least fixed points in ML\n", "abstract": " In this paper, we present an algorithm for computing the least solution of a system of monotone equations. This algorithm can be viewed as an effective form of the following well-known fixed point theorem: Theorem Let V be a finite set of variables. Let (P,\u2264,\u22a5) be a partially ordered set", "num_citations": "15\n", "authors": ["1751"]}
{"title": "A typed store-passing translation for general references\n", "abstract": " We present a store-passing translation of System F with general references into an extension of System F \u03c9 with certain well-behaved recursive kinds. This seems to be the first type-preserving store-passing translation for general references. It can be viewed as a purely syntactic account of a possible worlds model.", "num_citations": "14\n", "authors": ["1751"]}
{"title": "A modern eye on ML type inference\n", "abstract": " J (\u0393\u22a3 x)= let\u2200 \u03b11... \u03b1n. \u03c4= \u0393 (x) do \u03b11,..., \u03b1n= fresh,..., fresh return [\u03b1i/\u03b1i] n i= 1 (\u03c4)\u2013take a fresh instance J (\u0393\u22a3 \u03bbx. e1)= do \u03b1= fresh do \u03c41= J (\u0393; x: \u03b1\u22a3 e1) return \u03b1\u2192 \u03c41\u2013form an arrow type", "num_citations": "14\n", "authors": ["1751"]}
{"title": "Producing all ideals of a forest, functionally\n", "abstract": " We present functional implementations of Koda and Ruskey's algorithm for generating all ideals of a forest poset as a Gray code. Using a continuation-based approach, we give an extremely concise formulation of the algorithm's core. Then, in a number of steps, we derive a first-order version whose efficiency is comparable to that of a C implementation given by Knuth.", "num_citations": "13\n", "authors": ["1751"]}
{"title": "Verifying a hash table and its iterators in higher-order separation logic\n", "abstract": " We describe the specification and proof of an (imperative, sequential) hash table implementation. The usual dictionary operations (insertion, lookup, and so on) are supported, as well as iteration via folds and iterators. The code is written in OCaml and verified using higher-order separation logic, embedded in Coq, via the CFML tool and library. This case study is part of a larger project that aims to build a verified OCaml library of basic data structures.", "num_citations": "12\n", "authors": ["1751"]}
{"title": "Generalizing the higher-order frame and anti-frame rules\n", "abstract": " This informal note presents generalized versions of the higherorder frame and anti-frame rules. The main insights reside in two successive generalizations of the \u201ctensor\u201d operator\u2297. In the first step, a form of \u201clocal invariant\u201d, which allows implicit reasoning about \u201cwell-bracketed state changes\u201d, is introduced. In the second step, a form of \u201clocal monotonicity\u201d is added.", "num_citations": "12\n", "authors": ["1751"]}
{"title": "Visitors unchained\n", "abstract": " Traversing and transforming abstract syntax trees that involve name binding is notoriously difficult to do in a correct, concise, modular, customizable manner. We address this problem in the setting of OCaml, a functional programming language equipped with powerful object-oriented features. We use visitor classes as partial, composable descriptions of the operations that we wish to perform on abstract syntax trees. We introduce \"visitors\", a simple type-directed facility for generating visitor classes that have no knowledge of binding. Separately, we present \"alphaLib\", a library of small hand-written visitor classes, each of which knows about a specific binding construct, a specific representation of names, and/or a specific operation on abstract syntax trees. By combining these components, a wide range of operations can be defined. Multiple representations of names can be supported, as well as conversions between\u00a0\u2026", "num_citations": "11\n", "authors": ["1751"]}
{"title": "Three comments on the anti-frame rule\n", "abstract": " It is well-known that a careless combination of parametric polymorphism and weak references is unsound.(A weak reference is one that can be read and written without restrictions, as in ML.) The standard way to work around this problem is to rely on the value restriction [7] that is, to restrict the\u2200-introduction rule to values (as opposed to arbitrary terms). Chargu\u00e9raud and Pottier [3] pointed out that, on the other hand, there is no adverse interaction between polymorphism and strong references.(A strong reference is one that can be read and written only by presenting a linear capability.) As a result, in a typeand-capability system equipped with strong references, the value restriction is not required.The anti-frame rule was presented in the setting of Chargu\u00e9raud and Pottier\u2019s type-and-capability system, which does not have the value restriction. I proved [4] that the combination of anti-frame and strong references allows encoding weak references. Therefore, the combination of Chargu\u00e9raud and Pottier\u2019s system with the antiframe rule is unsound. This important fact was unwittingly omitted in the anti-frame paper. In summary,", "num_citations": "10\n", "authors": ["1751"]}
{"title": "An implementation of Caml-Light with existential types\n", "abstract": " This work is based on a proposal by L\u00e4ufer and Odersky They show that it is possible to add existential types to an ML-like language without even modifying its syntax. ML\u2019s strong typing properties are of course retained. We implemented their proposal into Caml-Light 0.6 [4], thus making it possible to write realistic programs using existential types. The \ufb01rst part of this paper describes this simple form of existential types and shows how to use them in our enhanced version of Caml-Light. We then give several examples demonstrating their usefulness. Finally, a more technical section gives an overview of the changes made to the compiler and discusses some technical issues.", "num_citations": "10\n", "authors": ["1751"]}
{"title": "VOCAL\u2013A Verified OCAml Library\n", "abstract": " \u25bcAbstract Libraries are the basic building blocks of any realistic programming project. It is thus of utmost interest for a programmer to build her software on top of bug-free libraries. We present the ongoing VOCAL project, which aims at building a mechanically verified library of general-purpose data structures and algorithms, written in the OCaml language.", "num_citations": "8\n", "authors": ["1751"]}
{"title": "A simple, possibly correct LR parser for C11\n", "abstract": " The syntax of the C programming language is described in the C11 standard by an ambiguous context-free grammar, accompanied with English prose that describes the concept of \u201cscope\u201d and indicates how certain ambiguous code fragments should be interpreted. Based on these elements, the problem of implementing a compliant C11 parser is not entirely trivial. We review the main sources of difficulty and describe a relatively simple solution to the problem. Our solution employs the well-known technique of combining an LALR(1) parser with a \u201clexical feedback\u201d mechanism. It draws on folklore knowledge and adds several original aspects, including a twist on lexical feedback that allows a smooth interaction with lookahead; a simplified and powerful treatment of scopes; and a few amendments in the grammar. Although not formally verified, our parser avoids several pitfalls that other implementations have fallen\u00a0\u2026", "num_citations": "7\n", "authors": ["1751"]}
{"title": "Hindley-Milner elaboration in applicative style\n", "abstract": " Type inference\u2014the problem of determining whether a program is well-typed\u2014is well-understood. In contrast, elaboration\u2014the task of constructing an explicitly-typed representation of the program\u2014 seems to have received relatively little attention, even though, in a non-local type inference system, it is non-trivial. We show that the constraint-based presentation of Hindley-Milner type inference can be extended to deal with elaboration, while preserving its elegance. This involves introducing a new notion of \"constraint with a value\", which forms an applicative functor.", "num_citations": "6\n", "authors": ["1751"]}
{"title": "A unified treatment of syntax with binders\n", "abstract": " Atoms and de Bruijn indices are two well-known representation techniques for data structures that involve names and binders. However, using either technique, it is all too easy to make a programming error that causes one name to be used where another was intended. We propose an abstract interface to names and binders that rules out many of these errors. This interface is implemented as a library in Agda. It allows defining and manipulating term representations in nominal style and in de Bruijn style. The programmer is not forced to choose between these styles: on the contrary, the library allows using both styles in the same program, if desired. Whereas indexing the types of names and terms with a natural number is a well-known technique to better control the use of de Bruijn indices, we index types with worlds. Worlds are at the same time more precise and more abstract than natural numbers. Via logical\u00a0\u2026", "num_citations": "6\n", "authors": ["1751"]}
{"title": "Spy game: verifying a local generic solver in Iris\n", "abstract": " We verify the partial correctness of a \"local generic solver\", that is, an on-demand, incremental, memoizing least fixed point computation algorithm. The verification is carried out in Iris, a modern breed of concurrent separation logic. The specification is simple: the solver computes the optimal least fixed point of a system of monotone equations. Although the solver relies on mutable internal state for memoization and for \"spying\", a form of dynamic dependency discovery, it is apparently pure: no side effects are mentioned in its specification. As auxiliary contributions, we provide several illustrations of the use of prophecy variables, a novel feature of Iris; we establish a restricted form of the infinitary conjunction rule; and we provide a specification and proof of Longley's modulus function, an archetypical example of spying.", "num_citations": "5\n", "authors": ["1751"]}
{"title": "Types et contraintes\n", "abstract": " Theoreme (Completude) Soit \u0393 un environnement de typage. Soit (\u03c60, V0) un \u00e9tat satisfaisant les invariants de l\u2019algorithme. Supposons donn\u00e9s \u03b80 et \u03c40 tels que \u03b80\u03c60 (\u0393)\u22a3 e: \u03c40 soit un jugement. Alors, l\u2019ex\u00e9cution de J (\u0393\u22a3 e)a partir de l\u2019\u00e9tat initial (\u03c60, V0) r\u00e9ussit. Soient (\u03c61, V1) son \u00e9tat final et \u03c41 le type renvoy\u00e9. Alors il existe une substitution \u03b81 telle que \u03b80\u03c60 et \u03b81\u03c61 co\u0131ncident en dehors de V0 et telle que \u03c40 s\u2019 \u00e9crit \u03b81\u03c61 (\u03c41).", "num_citations": "5\n", "authors": ["1751"]}
{"title": "A separation logic for effect handlers\n", "abstract": " User-defined effects and effect handlers are advertised and advocated as a relatively easy-to-understand and modular approach to delimited control. They offer the ability of suspending and resuming a computation and allow information to be transmitted both ways between the computation, which requests a certain service, and the handler, which provides this service. Yet, a key question remains, to this day, largely unanswered: how does one modularly specify and verify programs in the presence of both user-defined effect handlers and primitive effects, such as heap-allocated mutable state? We answer this question by presenting a Separation Logic with built-in support for effect handlers, both shallow and deep. The specification of a program fragment includes a protocol that describes the effects that the program may perform as well as the replies that it can expect to receive. The logic allows local reasoning via a\u00a0\u2026", "num_citations": "4\n", "authors": ["1751"]}
{"title": "Cosmo: a concurrent separation logic for Multicore OCaml\n", "abstract": " Multicore OCaml extends OCaml with support for shared-memory concurrency. It is equipped with a weak memory model, for which an operational semantics has been published. This begs the question: what reasoning rules can one rely upon while writing or verifying Multicore OCaml code? To answer it, we instantiate Iris, a modern descendant of Concurrent Separation Logic, for Multicore OCaml. This yields a low-level program logic whose reasoning rules expose the details of the memory model. On top of it, we build a higher-level logic, Cosmo, which trades off some expressive power in return for a simple set of reasoning rules that allow accessing nonatomic locations in a data-race-free manner, exploiting the sequentially-consistent behavior of atomic locations, and exploiting the release/acquire behavior of atomic locations. Cosmo allows both low-level reasoning, where the details of the Multicore OCaml\u00a0\u2026", "num_citations": "4\n", "authors": ["1751"]}
{"title": "Hindley-milner elaboration in applicative style: functional pearl\n", "abstract": " Type inference-the problem of determining whether a program is well-typed-is well-understood. In contrast, elaboration-the task of constructing an explicitly-typed representation of the program-seems to have received relatively little attention, even though, in a non-local type inference system, it is non-trivial. We show that the constraint-based presentation of Hindley-Milner type inference can be extended to deal with elaboration, while preserving its elegance. This involves introducing a new notion of\" constraint with a value\", which forms an applicative functor.", "num_citations": "3\n", "authors": ["1751"]}
{"title": "The ins and outs of iteration in Mezzo\n", "abstract": " This is a talk proposal for HOPE 2013. Using iteration over a collection as a case study, we wish to illustrate the strengths and weaknesses of the prototype programming language Mezzo.", "num_citations": "3\n", "authors": ["1751"]}
{"title": "C\u03b1ml reference manual\n", "abstract": " C\u03b1ml (pronounced:\u201calphaCaml\u201d) is a tool that accepts a binding specification and turns it into Objective Caml type definitions and code. The generated code relies on a library known as alphaLib. Roughly speaking, a binding specification is a definition of one or several algebraic data types, enriched with information about names (henceforth referred to as atoms) and binding. This information gives rise to a notion of \u03b1-equivalence over the values that inhabit these types. The code produced by C\u03b1ml is intended to help deal with this notion in a safe and concise style.This document is a reference manual. It is not a discussion of the problems raised by \u03b1-equivalence and of the various ways in which they can be addressed. Neither is it a tutorial introduction to C\u03b1ml. These topics are covered in a separate paper [1], which should be read first. Having a look at the demos that are shipped with C\u03b1ml is also recommended.", "num_citations": "3\n", "authors": ["1751"]}
{"title": "Impl\u00e9mentation d'un syst\u00e8me de modules \u00e9volu\u00e9 en Caml-Light\n", "abstract": " Ce rapport d\u00e9crit la conception et l'impl\u00e9mentation d'un langage de modules \u00e9volu\u00e9 en Caml-Light. Nous d\u00e9crivons d'abord ce que nous entendons par langage de modules, et l'int\u00e9r\u00eat qu'un tel langage pr\u00e9sente pour la programmation \u00e0 moyenne ou grande \u00e9chelle. Nous expliquons ensuite en quoi le pr\u00e9curseur en la mati\u00e8re, SML, souffre de limitations graves vis-\u00e0-vis de la compilation s\u00e9par\u00e9e. Nous exposons un syst\u00e8me similaire mais mieux adapt\u00e9 \u00e0 la compilation s\u00e9par\u00e9e, et en m\u00eame temps simplifi\u00e9. Enfin, certains probl\u00e8mes d'impl\u00e9mentation inattendus sont \u00e9voqu\u00e9s.", "num_citations": "3\n", "authors": ["1751"]}
{"title": "Revisiting the CPS Transformation and its Implementation\n", "abstract": " We give a machine-checked definition and proof of semantic correctness for Danvy and Filinski\u2019s properly tail-recursive, one-pass, call-by-value CPS transformation. We do so in the setting of the pure \u03bb-calculus extended with a let construct. We propose a new first-order, one-pass, compositional formulation of the transformation. We point out that Danvy and Filinski\u2019s simulation diagram does not hold in the presence of let, and prove a slightly more complex diagram, which involves parallel reduction. We represent variables as de Bruijn indices and show that, given the current state of the art, this does not represent a significant impediment to formalization. Finally, we note that, given this representation of terms, it is not obvious how to efficiently implement the transformation. To address this issue, we propose a novel higher-order formulation of the transformation. We prove that it is correct and informally argue that it runs in time O (nlogn).", "num_citations": "2\n", "authors": ["1751"]}
{"title": "Reachability and error diagnosis in LR (1) automata\n", "abstract": " Given an LR(1) automaton, what are the states in which an error can be detected? For each such \" error state \" , what is a minimal input sentence that causes an error in this state? We propose an algorithm that answers these questions. Such an algorithm allows building a collection of pairs of an erroneous input sentence and a diagnostic message, ensuring that this collection covers every error state, and maintaining this property as the grammar evolves. We report on an application of this technique to the CompCert ISO C99 parser, and discuss its strengths and limitations.", "num_citations": "1\n", "authors": ["1751"]}