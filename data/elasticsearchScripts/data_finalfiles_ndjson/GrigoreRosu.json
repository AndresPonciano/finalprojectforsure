{"title": "Mop: an efficient and generic runtime verification framework\n", "abstract": " Monitoring-Oriented Programming (MOP1)[21, 18, 22, 19] is a formal framework for software development and analysis, in which the developer specifies desired properties using definable specification formalisms, along with code to execute when properties are violated or validated. The MOP framework automatically generates monitors from the specified properties and then integrates them together with the user-defined code into the original system.", "num_citations": "481\n", "authors": ["1590"]}
{"title": "An overview of the K semantic framework\n", "abstract": " K is an executable semantic framework in which programming languages, calculi, as well as type systems or formal analysis tools can be defined, making use of configurations, computations and rules. Configurations organize the system/program state in units called cells, which are labeled and can be nested. Computations carry \u201ccomputational meaning\u201d as special nested list structures sequentializing computational tasks, such as fragments of program; in particular, computations extend the original language or calculus syntax. K (rewrite) rules generalize conventional rewrite rules by making explicit which parts of the term they read, write, or do not care about. This distinction makes K a suitable framework for defining truly concurrent languages or calculi, even in the presence of sharing. Since computations can be handled like any other terms in a rewriting environment, that is, they can be matched, moved from one\u00a0\u2026", "num_citations": "461\n", "authors": ["1590"]}
{"title": "An executable formal semantics of C with applications\n", "abstract": " This paper describes an executable formal semantics of C. Being executable, the semantics has been thoroughly tested against the GCC torture test suite and successfully passes 99.2% of 776 test programs. It is the most complete and thoroughly tested formal definition of C to date. The semantics yields an interpreter, debugger, state space search tool, and model checker \"for free\". The semantics is shown capable of automatically finding program errors, both statically and at runtime. It is also used to enumerate nondeterministic behavior.", "num_citations": "291\n", "authors": ["1590"]}
{"title": "An overview of the MOP runtime verification framework\n", "abstract": " This article gives an overview of the, monitoring oriented programming framework (MOP). In MOP, runtime monitoring is supported and encouraged as a fundamental principle for building reliable systems. Monitors are automatically synthesized from specified properties and are used in conjunction with the original system to check its dynamic behaviors. When a specification is violated or validated at runtime, user-defined actions will be triggered, which can be any code, such as information logging or runtime recovery. Two instances of MOP are presented: JavaMOP (for Java programs) and BusMOP (for monitoring PCI bus traffic). The architecture of MOP is discussed, and an explanation of parametric trace monitoring and its implementation is given. A comprehensive evaluation of JavaMOP attests to its efficiency, especially in comparison with similar systems. The implementation of BusMOP is discussed in\u00a0\u2026", "num_citations": "270\n", "authors": ["1590"]}
{"title": "Institution morphisms\n", "abstract": " Institutions formalise the intuitive notion of logical system, including syntax, semantics, and the relation of satisfaction between them. Our exposition emphasises the natural way that institutions can support deduction on sentences, and inclusions of signatures, theories, etc.; it also introduces terminology to clearly distinguish several levels of generality of the institution concept. A surprising number of different notions of morphism have been suggested for forming categories with institutions as objects, and an amazing variety of names have been proposed for them. One goal of this paper is to suggest a terminology that is uniform and informative to replace the current chaotic nomenclature; another goal is to investigate the properties and interrelations of these notions in a systematic way. Following brief expositions of indexed categories, diagram categories, twisted relations and Kan extensions, we\u00a0\u2026", "num_citations": "256\n", "authors": ["1590"]}
{"title": "Kevm: A complete formal semantics of the ethereum virtual machine\n", "abstract": " A developing field of interest for the distributed systems and applied cryptography communities is that of smart contracts: self-executing financial instruments that synchronize their state, often through a blockchain. One such smart contract system that has seen widespread practical adoption is Ethereum, which has grown to a market capacity of 100 billion USD and clears an excess of 500,000 daily transactions. Unfortunately, the rise of these technologies has been marred by a series of costly bugs and exploits. Increasingly, the Ethereum community has turned to formal methods and rigorous program analysis tools. This trend holds great promise due to the relative simplicity of smart contracts and bounded-time deterministic execution inherent to the Ethereum Virtual Machine (EVM). Here we present KEVM, an executable formal specification of the EVM's bytecode stack-based language built with the K Framework\u00a0\u2026", "num_citations": "246\n", "authors": ["1590"]}
{"title": "Java-MOP: A monitoring oriented programming environment for Java\n", "abstract": " A Java-based tool-supported software development and analysis framework is presented, where monitoring is a foundational principle. Expressive requirements specification formalisms can be included into the framework via logic plug-ins, allowing one to refer not only to the current state, but also to both past and future states.", "num_citations": "243\n", "authors": ["1590"]}
{"title": "Towards monitoring-oriented programming: A paradigm combining specification and implementation\n", "abstract": " With the explosion of software size, checking conformance of implementation to specification becomes an increasingly important but also hard problem. Current practice based on ad-hoc testing does not provide correctness guarantees, while highly confident traditional formal methods like model checking and theorem proving are still too expensive to become common practice. In this paper we present a paradigm for combining formal specification with implementation, called monitoring-oriented programming (MoP), providing a light-weighted formal method to check conformance of implementation to specification at runtime. System requirements are expressed using formal specifications given as annotations inserted at various user selected places in programs. Efficient monitoring code using the same target language as the implementation is then automatically generated during a pre-compilation stage. The\u00a0\u2026", "num_citations": "207\n", "authors": ["1590"]}
{"title": "Maximal sound predictive race detection with control flow abstraction\n", "abstract": " Despite the numerous static and dynamic program analysis techniques in the literature, data races remain one of the most common bugs in modern concurrent software. Further, the techniques that do exist either have limited detection capability or are unsound, meaning that they report false positives. We present a sound race detection technique that achieves a provably higher detection capability than existing sound techniques. A key insight of our technique is the inclusion of abstracted control flow information into the execution model, which increases the space of the causal model permitted by classical happens-before or causally-precedes based detectors. By encoding the control flow and a minimal set of feasibility constraints as a group of first-order logic formulae, we formulate race detection as a constraint solving problem. Moreover, we formally prove that our formulation achieves the maximal possible\u00a0\u2026", "num_citations": "177\n", "authors": ["1590"]}
{"title": "Monitoring algorithms for metric temporal logic specifications\n", "abstract": " Program execution traces can be so large in practical testing and monitoring applications that it would be very expensive, if not impossible, to store them for detailed analysis. Monitoring execution traces without storing them, can be a nontrivial matter for many specification formalisms, because complex formulae may require a considerable amount of information about the past. Metric temporal logic (MTL) is an extension of propositional linear temporal logic with discrete-timebounded temporal operators. In MTL, one can specify time limits within which certain temporal properties must hold, thus making it very suitable to express real-time monitoring requirements. In this paper, we present monitoring algorithms for checking timestamped execution traces against formulae in MTL or certain important sublogics of it. We also present lower bounds for the monitoring problem, showing that the presented algorithms are\u00a0\u2026", "num_citations": "172\n", "authors": ["1590"]}
{"title": "The rewriting logic semantics project\n", "abstract": " Rewriting logic is a flexible and expressive logical framework that unifies algebraic denotational semantics and structural operational semantics (SOS) in a novel way, avoiding their respective limitations and allowing succinct semantic definitions. The fact that a rewrite logic theory\u2019s axioms include both equations and rewrite rules provides a useful \u201cabstraction dial\u201d to find the right balance between abstraction and computational observability in semantic definitions. Such semantic definitions are directly executable as interpreters in a rewriting logic language such as Maude, whose generic formal tools can be used to endow those interpreters with powerful program analysis capabilities.", "num_citations": "168\n", "authors": ["1590"]}
{"title": "K-Java: A complete semantics of Java\n", "abstract": " This paper presents K-Java, a complete executable formal semantics of Java 1.4. K-Java was extensively tested with a test suite developed alongside the project, following the Test Driven Development methodology. In order to maintain clarity while handling the great size of Java, the semantics was split into two separate definitions--a static semantics and a dynamic semantics. The output of the static semantics is a preprocessed Java program, which is passed as input to the dynamic semantics for execution. The preprocessed program is a valid Java program, which uses a subset of the features of Java. The semantics is applied to model-check multi-threaded programs. Both the test suite and the static semantics are generic and ready to be used in other Java-related projects.", "num_citations": "164\n", "authors": ["1590"]}
{"title": "Formal analysis of Java programs in JavaFAN\n", "abstract": " JavaFAN is a Java program analysis framework, that can symbolically execute multithreaded programs, detect safety violations searching through an unbounded state space, and verify finite state programs by explicit state model checking. Both Java language and JVM bytecode analyses are possible. JavaFAN\u2019s implementation consists of only 3,000 lines of Maude code, specifying formally the semantics of Java and JVM in rewriting logic and then using the capabilities of Maude for efficient execution, search and LTL model checking of rewriting theories.", "num_citations": "160\n", "authors": ["1590"]}
{"title": "Defining the undefinedness of C\n", "abstract": " We present a``negative''semantics of the C11 language---a semantics that does not just give meaning to correct programs, but also rejects undefined programs. We investigate undefined behavior in C and discuss the techniques and special considerations needed for formally specifying it. We have used these techniques to modify and extend a semantics of C into one that captures undefined behavior. The amount of semantic infrastructure and effort required to achieve this was unexpectedly high, in the end nearly doubling the size of the original semantics. From our semantics, we have automatically extracted an undefinedness checker, which we evaluate against other popular analysis tools, using our own test suite in addition to a third-party test suite. Our checker is capable of detecting examples of all 77 categories of core language undefinedness appearing in the C11 standard, more than any other tool we\u00a0\u2026", "num_citations": "147\n", "authors": ["1590"]}
{"title": "Efficient Monitoring of \u03c9-Languages\n", "abstract": " We present a technique for generating efficient monitors for \u03c9-regular-languages. We show how B\u00fcchi automata can be reduced in size and transformed into special, statistically optimal nondeterministic finite state machines, called binary transition tree finite state machines (BTT-FSMs), which recognize precisely the minimal bad prefixes of the original \u03c9-regular-language. The presented technique is implemented as part of a larger monitoring framework and is available for download.", "num_citations": "146\n", "authors": ["1590"]}
{"title": "Parametric trace slicing and monitoring\n", "abstract": " Analysis of execution traces plays a fundamental role in many program analysis approaches. Execution traces are frequently parametric, i.e., they contain events with parameter bindings. Each parametric trace usually consists of many trace slices merged together, each slice corresponding to a parameter binding. Several techniques have been proposed to analyze parametric traces, but they have limitations: some in the specification formalism, others in the type of traces they support; moreover, they share common notions, intuitions, even techniques and algorithms, suggesting that a fundamental understanding of parametric trace analysis is needed. This foundational paper gives the first solution to parametric trace analysis that is unrestricted by the type of parametric properties or traces that can be analyzed. First, a general purpose parametric trace slicing technique is discussed, which takes each event in\u00a0\u2026", "num_citations": "144\n", "authors": ["1590"]}
{"title": "KJS: A complete formal semantics of JavaScript\n", "abstract": " This paper presents KJS, the most complete and throughly tested formal semantics of JavaScript to date. Being executable, KJS has been tested against the ECMAScript 5.1 conformance test suite, and passes all 2,782 core language tests. Among the existing implementations of JavaScript, only Chrome V8's passes all the tests, and no other semantics passes more than 90%. In addition to a reference implementation for JavaScript, KJS also yields a simple coverage metric for a test suite: the set of semantic rules it exercises. Our semantics revealed that the ECMAScript 5.1 conformance test suite fails to cover several semantic rules. Guided by the semantics, we wrote tests to exercise those rules. The new tests revealed bugs both in production JavaScript engines (Chrome V8, Safari WebKit, Firefox SpiderMonkey) and in other semantics. KJS is symbolically executable, thus it can be used for formal analysis and\u00a0\u2026", "num_citations": "131\n", "authors": ["1590"]}
{"title": "A rewriting logic approach to operational semantics\n", "abstract": " This paper shows how rewriting logic semantics (RLS) can be used as a computational logic framework for operational semantic definitions of programming languages. Several operational semantics styles are addressed: big-step and small-step structural operational semantics (SOS), modular SOS, reduction semantics with evaluation contexts, continuation-based semantics, and the chemical abstract machine. Each of these language definitional styles can be faithfully captured as an RLS theory, in the sense that there is a one-to-one correspondence between computational steps in the original language definition and computational steps in the corresponding RLS theory. A major goal of this paper is to show that RLS does not force or pre-impose any given language definitional style, and that its flexibility and ease of use makes RLS an appealing framework for exploring new definitional styles.", "num_citations": "121\n", "authors": ["1590"]}
{"title": "Rewriting logic semantics: From language specifications to formal analysis tools\n", "abstract": " Formal semantic definitions of concurrent languages, when specified in a well-suited semantic framework and supported by generic and efficient formal tools, can be the basis of powerful software analysis tools. Such tools can be obtained for free from the semantic definitions; in our experience in just the few weeks required to define a language\u2019s semantics even for large languages like Java. By combining, yet distinguishing, both equations and rules, rewriting logic semantic definitions unify both the semantic equations of equational semantics (in their higher-order denotational version or their first-order algebraic counterpart) and the semantic rules of SOS. Several limitations of both SOS and equational semantics are thus overcome within this unified framework. By using a high-performance implementation of rewriting logic such as Maude, a language\u2019s formal specification can be automatically\u00a0\u2026", "num_citations": "119\n", "authors": ["1590"]}
{"title": "Java PathExplorer\u2013A runtime verification tool\n", "abstract": " We describe recent work on designing an environment called Java PathExplorer for monitoring the execution of Java programs. This environment facilitates the testing of execution traces against high level specifications, including temporal logic formulae. In addition, it contains algorithms for detecting classical error patterns in concurrent programs, such as deadlocks and data races. An initial prototype of the tool has been applied to the executive module of the planetary Rover K9, developed at NASA Ames. In this paper we describe the background and motivation for the development of this tool, including comments on how it relates to formal methods tools as well as to traditional testing, and we then present the tool itself.", "num_citations": "119\n", "authors": ["1590"]}
{"title": "Java pathexplorer: A runtime verification tool\n", "abstract": " We describe recent work on designing an environment called Java PathExplorer for monitoring the execution of Java programs. This environment facilitates the testing of execution traces against high level specifications, including temporal logic formulae. In addition, it contains algorithms for detecting classical error patterns in concurrent programs, such as deadlocks and data races. An initial prototype of the tool has been applied to the executive module of the planetary Rover K9, developed at NASA Ames. In this paper we describe the background and motivation for the development of this tool, including comments on how it relates to formal methods tools as well as to traditional testing, and we then present the tool itself.", "num_citations": "119\n", "authors": ["1590"]}
{"title": "Hardware runtime monitoring for dependable cots-based real-time embedded systems\n", "abstract": " COTS peripherals are heavily used in the embedded market, but their unpredictability is a threat for high-criticality real-time systems: it is hard or impossible to formally verify COTS components. Instead, we propose to monitor the runtime behavior of COTS peripherals against their assumed specifications. If violations are detected, then an appropriate recovery measure can be taken. Our monitoring solution is decentralized: a monitoring device is plugged in on a peripheral bus and monitors the peripheral behavior by examining read and write transactions on the bus. Provably correct (w.r.t. given specifications) hardware monitors are synthesized from high level specifications, and executed on FPGAs, resulting in zero runtime overhead on the system CPU. The proposed technique, called BusMOP, has been implemented as an instance of a generic runtime verification framework, called MOP, which until now has only\u00a0\u2026", "num_citations": "112\n", "authors": ["1590"]}
{"title": "A tree based router search engine architecture with single port memories\n", "abstract": " Pipelined forwarding engines are used in core router to meet speed demands. Tree-based searches are pipelined across a number of stages to achieve high throughput, but this results in unevenly distributed memory. To address this imbalance, conventional approaches use either complex dynamic memory allocation schemes or over-provision each of the pipeline stages. This paper describes the microarchitecture of a novel network search processor which provides both high execution throughput and balanced memory distributor by dividing the tree into subtrees and allocating each subtree separately, allowing searches to begin at any pipeline stage. The architecture is validated by implementing and simulating state of the art solutions for IPv4 lookup, VPN forwarding and packet classification. The new pipeline scheme and memory allocator can provide searches with a memory allocation, efficiency that is within\u00a0\u2026", "num_citations": "112\n", "authors": ["1590"]}
{"title": "Hidden logic\n", "abstract": " Cleverly designed software often fails to satisfy its requirements strictly, but instead satisfies them behaviorally, in the sense that they appear to be satisfied under every experiment that can be performed on the system. It is therefore becoming increasingly important to develop powerful techniques for behavioral specification and verification of systems, especially in the design stage, where most of the errors appear.", "num_citations": "112\n", "authors": ["1590"]}
{"title": "JavaMOP: Efficient parametric runtime monitoring framework\n", "abstract": " Runtime monitoring is a technique usable in all phases of the software development cycle, from initial testing, to debugging, to actually maintaining proper function in production code. Of particular importance are parametric monitoring systems, which allow the specification of properties that relate objects in a program, rather than only global properties. In the past decade, a number of parametric runtime monitoring systems have been developed. Here we give a demonstration of our system, JavaMOP. It is the only parametric monitoring system that allows multiple differing logical formalisms. It is also the most efficient in terms of runtime overhead, and very competitive with respect to memory usage.", "num_citations": "109\n", "authors": ["1590"]}
{"title": "Semantics-based program verifiers for all languages\n", "abstract": " We present a language-independent verification framework that can be instantiated with an operational semantics to automatically generate a program verifier. The framework treats both the operational semantics and the program correctness specifications as reachability rules between matching logic patterns, and uses the sound and relatively complete reachability logic proof system to prove the specifications using the semantics. We instantiate the framework with the semantics of one academic language, KernelC, as well as with three recent semantics of real-world languages, C, Java, and JavaScript, developed independently of our verification infrastructure. We evaluate our approach empirically and show that the generated program verifiers can check automatically the full functional correctness of challenging heap-manipulating programs implementing operations on list and tree data structures, like AVL trees\u00a0\u2026", "num_citations": "101\n", "authors": ["1590"]}
{"title": "Circular coinductive rewriting\n", "abstract": " Circular coinductive rewriting is a new method for proving behavioral properties, that combines behavioral rewriting with circular coinduction. This method is implemented in our new BOBJ (Behavioral OBJects) behavioral specification and computation system, which is used in examples throughout this paper. These examples demonstrate the surprising power of circular coinductive rewriting. The paper also sketches the underlying hidden algebraic theory and briefly describes BOBJ and some of its algorithms.", "num_citations": "100\n", "authors": ["1590"]}
{"title": "Mining parametric specifications\n", "abstract": " Specifications carrying formal parameters that are bound to concrete data at runtime can effectively and elegantly capture multi-object behaviors or protocols. Unfortunately, parametric specifications are not easy to formulate by nonexperts and, consequently, are rarely available. This paper presents a general approach for mining parametric specifications from program executions, based on a strict separation of concerns: (1) a trace slicer first extracts sets of independent interactions from parametric execution traces; and (2) the resulting non-parametric trace slices are then passed to any conventional non-parametric property learner. The presented technique has been implemented in jMiner, which has been used to automatically mine many meaningful and non-trivial parametric properties of OpenJDK 6.", "num_citations": "99\n", "authors": ["1590"]}
{"title": "ROSRV: Runtime verification for robots\n", "abstract": " We present ROSRV, a runtime verification framework for robotic applications on top of the Robot Operating System (ROS\u00a0[8]), a widely used open-source framework for robot software development. ROSRV\u00a0aims to address the safety and security issues of robots by providing a transparent monitoring infrastructure that intercepts and monitors the commands and messages passing through the system. Safety and security properties can be defined in a formal specification language, and are ensured by automatically generated monitors. ROSRV\u00a0integrates seamlessly with ROS\u2014no change in ROS nor the application code is needed. ROSRV\u00a0has been applied and evaluated on a commercial robot.", "num_citations": "98\n", "authors": ["1590"]}
{"title": "Matching logic: An alternative to Hoare/Floyd logic\n", "abstract": " This paper introduces matching logic, a novel framework for defining axiomatic semantics for programming languages, inspired from operational semantics. Matching logic specifications are particular first-order formulae with constrained algebraic structure, called patterns. Program configurations satisfy patterns iff they match their algebraic structure and satisfy their constraints. Using a simple imperative language (IMP), it is shown that a restricted use of the matching logic proof system is equivalent to IMP\u2019s Hoare logic proof system, in that any proof derived using either can be turned into a proof using the other. Extensions to IMP including a heap with dynamic memory allocation and pointer arithmetic are given, requiring no extension of the underlying first-order logic; moreover, heap patterns such as lists, trees, queues, graphs, etc., are given algebraically using fist-order constraints over patterns.", "num_citations": "98\n", "authors": ["1590"]}
{"title": "Hiding more of hidden algebra\n", "abstract": " Behavioral specification is a rapidly advancing area of algebraic semantics that supports practical applications by allowing models (implementations) that only behaviorally satisfy specifications, infinitary data structures (such as streams), behavioral refinements, and coinduction proof methods. This paper generalizes the hidden algebra approach to allow: (P1) operations with multiple hidden arguments, and (P2) defining behavioral equivalence with a subset of operations, in addition to the already present (P3) built-in data types, (P4) nondeterminism, (P5) concurrency, and (P6) non-congruent operations. All important results generalize, but more elegant formulations use the new institution in Section 5. Behavioral satisfaction appeared 1981 in [20], hidden algebra 1989 in [9], multiple hidden arguments 1992 in [1], congruent and behavioral operations in [1                 18], behavioral equivalence defined by a\u00a0\u2026", "num_citations": "91\n", "authors": ["1590"]}
{"title": "Kevm: A complete semantics of the ethereum virtual machine\n", "abstract": " A developing  field of interest for the distributed systems and applied cryptography community is that of smart contracts: self-executing financial instruments that synchronize their state, often through a blockchain.  One such smart contract system that has seen widespread practical adoption is Ethereum, which has grown to secure approximately 30 billion USD of currency value and in excess of 300,000 daily transactions.  Unfortunately, the rise of these technologies has been marred by a repeated series of security vulnerabilities and high pro file contract failures. To address these failures, the Ethereum community has turned to formal verification and program analysis which show great promise due to the computational simplicity and bounded-time execution inherent to smart contracts.  Despite this, no fully formal, rigorous, comprehensive, and executable semantics of the EVM (Ethereum Virtual Machine) currently exists, leaving a lack of rigor on which to base such tools.  In this work, we present KEVM, the first fully executable formal semantics of the EVM, the bytecode language in which smart contracts are executed.  We create this semantics in a framework for executable semantics, the K framework.  We show that our semantics not only passes the official 40,683-test stress test suite for EVM implementations, but also reveals ambiguities and potential sources of error in the existing on-paper formalization of EVM semantics on which our work is based.   These properties make KEVM an ideal formal reference implementation against which other implementations can be evaluated.  We proceed to argue for a semantics-first formal verification\u00a0\u2026", "num_citations": "85\n", "authors": ["1590"]}
{"title": "One-path reachability logic\n", "abstract": " This paper introduces (one-path) reachability logic, a language-independent proof system for program verification, which takes an operational semantics as axioms and derives reachability rules, which generalize Hoare triples. This system improves on previous work by allowing operational semantics given with conditional rewrite rules, which are known to support all major styles of operational semantics. In particular, Kahn's big-step and Plotkin's small-step semantic styles are now supported. The reachability logic proof system is shown sound (i.e., partially correct) and (relatively) complete. Reachability logic thus eliminates the need to independently define an axiomatic and an operational semantics for each language, and the nonnegligible effort to prove the former sound and complete w.r.t. the latter. The soundness result has also been formalized in Coq, allowing reachability logic derivations to serve as formal\u00a0\u2026", "num_citations": "85\n", "authors": ["1590"]}
{"title": "Checking reachability using matching logic\n", "abstract": " This paper presents a verification framework that is parametric in a (trusted) operational semantics of some programming language. The underlying proof system is language-independent and consists of eight proof rules. The proof system is proved partially correct and relatively complete (with respect to the programming language configuration model). To show its practicality, the generic framework is instantiated with a fragment of C and evaluated with encouraging results.", "num_citations": "85\n", "authors": ["1590"]}
{"title": "A formal monitoring-based framework for software development and analysis\n", "abstract": " A formal framework for software development and analysis is presented, which aims at reducing the gap between formal specification and implementation by integrating the two and allowing them together to form a system. It is called monitoring-oriented programming (MOP), since runtime monitoring is supported and encouraged as a fundamental principle. Monitors are automatically synthesized from formal specifications and integrated at appropriate places in the program, according to user-configurable attributes. Violations and/or validations of specifications can trigger user-defined code at any points in the program, in particular recovery code, outputting/sending messages, or raising exceptions. The major novelty of MOP is its generality w.r.t. logical formalisms: it allows users to insert their favorite or domain-specific specification formalisms via logic plug-in modules. A WWW repository has been created\u00a0\u2026", "num_citations": "84\n", "authors": ["1590"]}
{"title": "Matching logic\n", "abstract": " This paper presents matching logic, a first-order logic (FOL) variant for specifying and reasoning about structure by means of patterns and pattern matching. Its sentences, the patterns, are constructed using variables, symbols, connectives and quantifiers, but no difference is made between function and predicate symbols. In models, a pattern evaluates into a power-set domain (the set of values that match it), in contrast to FOL where functions and predicates map into a regular domain. Matching logic uniformly generalizes several logical frameworks important for program analysis, such as: propositional logic, algebraic specification, FOL with equality, modal logic, and separation logic. Patterns can specify separation requirements at any level in any program configuration, not only in the heaps or stores, without any special logical constructs for that: the very nature of pattern matching is that if two structures are matched as part of a pattern, then they can only be spatially separated. Like FOL, matching logic can also be translated into pure predicate logic with equality, at the same time admitting its own sound and complete proof system. A practical aspect of matching logic is that FOL reasoning with equality remains sound, so off-the-shelf provers and SMT solvers can be used for matching logic reasoning. Matching logic is particularly well-suited for reasoning about programs in programming languages that have an operational semantics, but it is not limited to this.", "num_citations": "80\n", "authors": ["1590"]}
{"title": "Efficient monitoring of parametric context-free patterns\n", "abstract": " Recent developments in runtime verification and monitoring show that parametric regular and temporal logic specifications can be efficiently monitored against large programs. However, these logics reduce to ordinary finite automata, limiting their expressivity. For example, neither can specify structured properties that refer to the call stack of the program. While context-free grammars (CFGs) are expressive and well-understood, existing techniques for monitoring CFGs generate large runtime overhead in real-life applications. This paper demonstrates that monitoring parametric CFGs is practical (with overhead on the order of 12% or lower in most cases). We present a monitor synthesis algorithm for CFGs based on an LR(1) parsing algorithm, modified to account for good prefix matching. In addition, a logic-independent mechanism is introduced to support matching against the suffixes of execution traces.", "num_citations": "80\n", "authors": ["1590"]}
{"title": "Circular coinduction: A proof theoretical foundation\n", "abstract": " Several algorithmic variants of circular coinduction have been proposed and implemented during the last decade, but a proof theoretical foundation of circular coinduction in its full generality is still missing. This paper gives a three-rule proof system that can be used to formally derive circular coinductive proofs. This three-rule system is proved behaviorally sound and is exemplified by proving several properties of infinite streams. Algorithmic variants of circular coinduction now become heuristics to search for proof derivations using the three rules.", "num_citations": "79\n", "authors": ["1590"]}
{"title": "Testing linear temporal logic formulae on finite execution traces\n", "abstract": " We present an algorithm for efficiently testing Linear Temporal Logic (LTL) formulae on finite execution traces. The standard models of LTL are infinite traces, reflecting the behavior of reactive and concurrent systems which conceptually may be continuously alive. In most past applications of LTL. theorem provers and model checkers have been used to formally prove that down-scaled models satisfy such LTL specifications. Our goal is instead to use LTL for up-scaled testing of real software applications. Such tests correspond to analyzing the conformance of finite traces against LTL formulae. We first describe what it means for a finite trace to satisfy an LTL property. We then suggest an optimized algorithm based on transforming LTL formulae. The work is done using the Maude rewriting system. which turns out to provide a perfect notation and an efficient rewriting engine for performing these experiments.", "num_citations": "79\n", "authors": ["1590"]}
{"title": "A formal verification tool for Ethereum VM bytecode\n", "abstract": " In this paper, we present a formal verification tool for the Ethereum Virtual Machine (EVM) bytecode. To precisely reason about all possible behaviors of the EVM bytecode, we adopted KEVM, a complete formal semantics of the EVM, and instantiated the K-framework's reachability logic theorem prover to generate a correct-by-construction deductive verifier for the EVM. We further optimized the verifier by introducing EVM-specific abstractions and lemmas to improve its scalability. Our EVM verifier has been used to verify various high-profile smart contracts including the ERC20 token, Ethereum Casper, and DappHub MakerDAO contracts.", "num_citations": "74\n", "authors": ["1590"]}
{"title": "RV-Monitor: Efficient parametric runtime verification with simultaneous properties\n", "abstract": " Runtime verification can effectively increase the reliability of software systems. In recent years, parametric runtime verification has gained a lot of traction, with several systems proposed. However, lack of real specifications and prohibitive runtime overhead when checking numerous properties simultaneously prevent developers or users from using runtime verification. This paper reports on more than 150 formal specifications manually derived from the Java API documentation of commonly used packages, as well as a series of novel techniques which resulted in a new runtime verification system, RV-Monitor. Experiments show that these specifications are useful for finding bugs and bad software practice, and RV-Monitor is capable of monitoring all our specifications simultaneously, and runs substantially faster than other state-of-the-art runtime verification systems.", "num_citations": "74\n", "authors": ["1590"]}
{"title": "Parametric and sliced causality\n", "abstract": " Happen-before causal partial orders have been widely used in concurrent program verification and testing. This paper presents a parametric approach to happen-before causal partial orders. Existing variants of happen-before relations can be obtained as instances of the parametric framework. A novel causal partial order, called sliced causality, is then defined also as an instance of the parametric framework, which loosens the obvious but strict happen-before relation by considering static and dynamic dependence information about the program. Sliced causality has been implemented in a runtime predictive analysis tool for Java, named jPredictor, and the evaluation results show that sliced causality can significantly improve the capability of concurrent verification and testing.", "num_citations": "73\n", "authors": ["1590"]}
{"title": "All-path reachability logic\n", "abstract": " This paper presents a language-independent proof system for reachability properties of programs written in non-deterministic (e.g. concurrent) languages, referred to as all-path reachability logic. It derives partial-correctness properties with all-path semantics (a state satisfying a given precondition reaches states satisfying a given postcondition on all terminating execution paths). The proof system takes as axioms any unconditional operational semantics, and is sound (partially correct) and (relatively) complete, independent of the object language; the soundness has also been mechanized (Coq). This approach is implemented in a tool for semantics-based verification as part of the  framework.", "num_citations": "72\n", "authors": ["1590"]}
{"title": "Hidden congruent deduction\n", "abstract": " This paper presents some techniques of this kind in the area called hidden algebra, clustered around the central notion of coinduction. We believe hidden algebra is the natural next step in the evolution of algebraic semantics and its first order proof technology. Hidden algebra originated in [7], and was developed further in [8, 10, 3, 12, 5] among other places; the most comprehensive survey currently available is [12]", "num_citations": "65\n", "authors": ["1590"]}
{"title": "K-Maude: A rewriting based tool for semantics of programming languages\n", "abstract": " K is a rewriting-based framework for defining programming languages. K-Maude is a tool implementing K on top of Maude. K-Maude provides an interface accepting K modules along with regular Maude modules and a collection of tools for transforming K language definitions into Maude rewrite theories for execution or analysis, or into LaTeX for documentation purposes. The current K-Maude prototype was successfully used in defining several languages and language analysis tools, both for research and for teaching purposes. This paper describes the K-Maude tool, both from a user and from an implementer perspective.", "num_citations": "63\n", "authors": ["1590"]}
{"title": "A formal executable semantics of Verilog\n", "abstract": " This paper describes a formal executable semantics for the Verilog hardware description language. The goal of our formalization is to provide a concise and mathematically rigorous reference augmenting the prose of the official language standard, and ultimately to aid developers of Verilog-based tools; e.g., simulators, test generators, and verification tools. Our semantics applies equally well to both synthesizeable and behavioral designs and is given in a familiar, operational-style within a logic providing important additional benefits above and beyond static formalization. In particular, it is executable and searchable so that one can ask questions about how a, possibly nondeterministic, Verilog program can legally behave under the formalization. The formalization should not be seen as the final word on Verilog, but rather as a starting point and basis for community discussions on the Verilog semantics.", "num_citations": "62\n", "authors": ["1590"]}
{"title": "Maximal causal models for sequentially consistent systems\n", "abstract": " This paper shows that it is possible to build a maximal and sound causal model for concurrent computations from a given execution trace. It is sound, in the sense that any program which can generate a trace can also generate all traces in its causal model. It is maximal (among sound models), in the sense that by extending the causal model of an observed trace with a new trace, the model becomes unsound: there exists a program generating the original trace which cannot generate the newly introduced trace. Thus, the maximal sound model has the property that it comprises all traces which all programs that can generate the original trace can also generate. The existence of such a model is of great theoretical value as it can be used to prove the soundness of non-maximal, and thus smaller, causal models.", "num_citations": "58\n", "authors": ["1590"]}
{"title": "Synthesizing dynamic programming algorithms from linear temporal logic formulae\n", "abstract": " The problem of testing a linear temporal logic (LTL) formula on a finite execution trace of events, generated by an executing program, occurs naturally in runtime analysis of software. We present an algorithm which takes an LTL formula and generates an efficient dynamic programming algorithm. The generated algorithm tests whether the LTL formula is satisfied by a finite trace of events given as input. The generated algorithm runs in linear time, its constant depending on the size of the LTL formula. The memory needed is constant, also depending on the size of the formula.", "num_citations": "57\n", "authors": ["1590"]}
{"title": "K overview and simple case study\n", "abstract": " This paper gives an overview of the tool-supported K framework for semantics-based programming language design and formal analysis. K provides a convenient notation for modularly defining the syntax and the semantics of a programming language, together with a series of tools based on these, including a parser and an interpreter. A case study is also discussed, namely the K definition of the dynamic and static semantics of SIMPLE, a non-trivial imperative programming language. The material discussed in this paper was presented in an invited talk at the K'11 workshop.", "num_citations": "54\n", "authors": ["1590"]}
{"title": "CIRC: A behavioral verification tool based on circular coinduction\n", "abstract": " CIRC is a tool for automated inductive and coinductive theorem proving. It includes an engine based on circular coinduction, which makes CIRC particularly well-suited for proving behavioral properties of infinite data-structures. This paper presents the current status of the coinductive features of the CIRC prover, focusing on new features added over the last two years. The presentation is by examples, showing how CIRC can automatically prove behavioral properties.", "num_citations": "54\n", "authors": ["1590"]}
{"title": "Runtime verification of C memory safety\n", "abstract": " C is the most widely used imperative system\u2019s implementation language. While C provides types and high-level abstractions, its design goal has been to provide highest performance which often requires low-level access to memory. As a consequence C supports arbitrary pointer arithmetic, casting, and explicit allocation and deallocation. These operations are difficult to use, resulting in programs that often have software bugs like buffer overflows and dangling pointers that cause security vulnerabilities. We say a C program is memory safe, if at runtime it never goes wrong with such a memory access error. Based on standards for writing \u201cgood\u201d C code, this paper proposes strong memory safety as the least restrictive formal definition of memory safety amenable for runtime verification. We show that although verification of memory safety is in general undecidable, even when restricted to closed, terminating\u00a0\u2026", "num_citations": "54\n", "authors": ["1590"]}
{"title": "From Hoare logic to matching logic reachability\n", "abstract": " Matching logic reachability has been recently proposed as an alternative program verification approach. Unlike Hoare logic, where one defines a language-specific proof system that needs to be proved sound for each language separately, matching logic reachability provides a language-independent and sound proof system that directly uses the trusted operational semantics of the language as axioms. Matching logic reachability thus has a clear practical advantage: it eliminates the need for an additional semantics of the same language in order to reason about programs, and implicitly eliminates the need for tedious soundness proofs. What is not clear, however, is whether matching logic reachability is as powerful as Hoare logic. This paper introduces a technique to mechanically translate Hoare logic proof derivations into equivalent matching logic reachability proof derivations. The presented technique\u00a0\u2026", "num_citations": "51\n", "authors": ["1590"]}
{"title": "Formal JVM code analysis in JavaFAN\n", "abstract": " JavaFAN uses a Maude rewriting logic specification of the JVM semantics as the basis of a software analysis tool with competitive performance. It supports formal analysis of concurrent JVM programs by means of symbolic simulation, breadth-first search, and LTL model checking. We discuss JavaFAN\u2019s executable formal specification of the JVM, illustrate its formal analysis capabilities using several case studies, and compare its performance with similar Java analysis tools.", "num_citations": "51\n", "authors": ["1590"]}
{"title": "Conditional circular coinductive rewriting with case analysis\n", "abstract": " We argue for an algorithmic approach to behavioral proofs, review the hidden algebra approach, develop circular coinductive rewriting for conditional goals, extend it with case analysis, and give some examples.", "num_citations": "49\n", "authors": ["1590"]}
{"title": "The rewriting logic semantics project\n", "abstract": " Rewriting logic is a flexible and expressive logical framework that unifies denotational semantics and SOS in a novel way, avoiding their respective limitations and allowing very succinct semantic definitions. The fact that a rewrite theory's axioms include both equations and rewrite rules provides a very useful \u201cabstraction knob\u201d to find the right balance between abstraction and observability in semantic definitions. Such semantic definitions are directly executable as interpreters in a rewriting logic language such as Maude, whose generic formal tools can be used to endow those interpreters with powerful program analysis capabilities.", "num_citations": "48\n", "authors": ["1590"]}
{"title": "Allen linear (interval) temporal logic\u2013translation to LTL and monitor synthesis\n", "abstract": " The relationship between two well established formalisms for temporal reasoning is first investigated, namely between Allen\u2019s interval algebra (or Allen\u2019s temporal logic, abbreviated ATL) and linear temporal logic (LTL). A discrete variant of ATL is defined, called Allen linear temporal logic (ALTL), whose models are \u03c9-sequences of timepoints. It is shown that any ALTL formula can be linearly translated into an equivalent LTL formula, thus enabling the use of LTL techniques on ALTL requirements. This translation also implies the NP-completeness of ATL satisfiability. Then the problem of monitoring ALTL requirements is investigated, showing that it reduces to checking satisfiability; the similar problem for unrestricted LTL is known to require exponential space. An effective monitoring algorithm for ALTL is given, which has been implemented and experimented with in the context of planning applications.", "num_citations": "47\n", "authors": ["1590"]}
{"title": "A complete formal semantics of x86-64 user-level instruction set architecture\n", "abstract": " We present the most complete and thoroughly tested formal semantics of x86-64 to date. Our semantics faithfully formalizes all the non-deprecated, sequential user-level instructions of the x86-64 Haswell instruction set architecture. This totals 3155 instruction variants, corresponding to 774 mnemonics. The semantics is fully executable and has been tested against more than 7,000 instruction-level test cases and the GCC torture test suite. This extensive testing paid off, revealing bugs in both the x86-64 reference manual and other existing semantics. We also illustrate potential applications of our semantics in different formal analyses, and discuss how it can be useful for processor verification.", "num_citations": "46\n", "authors": ["1590"]}
{"title": "A formal semantics of C with applications\n", "abstract": " This dissertation shows that complex, real programming languages can be completely formalized in the [special characters omitted] Framework, yielding interpreters and analysis tools for testing and bug detection. This is demonstrated by providing, in [special characters omitted], the first complete formal semantics of the C programming language. With varying degrees of effort, tools such as interpreters, debuggers, and model-checkers, together with tools that check for memory safety, races, deadlocks, and undefined behavior are then generated from the semantics.", "num_citations": "46\n", "authors": ["1590"]}
{"title": "Weak inclusion systems\n", "abstract": " We define weak inclusion systems as a natural extension of inclusion systems. We prove that several properties of factorisation systems and inclusion systems remain valid under this extension and we obtain new properties as algebraic tools in abstract model theory.", "num_citations": "46\n", "authors": ["1590"]}
{"title": "Garbage collection for monitoring parametric properties\n", "abstract": " Parametric properties are behavioral properties over program events that depend on one or more parameters. Parameters are bound to concrete data or objects at runtime, which makes parametric properties particularly suitable for stating multi-object relationships or protocols. Monitoring parametric properties independently of the employed formalism involves slicing traces with respect to parameter instances and sending these slices to appropriate non-parametric monitor instances. The number of such instances is theoretically unbounded and tends to be enormous in practice, to an extent that how to efficiently manage monitor instances has become one of the most challenging problems in runtime verification. The previous formalism-independent approach was only able to do the obvious, namely to garbage collect monitor instances when all bound parameter objects were garbage collected. This led to\u00a0\u2026", "num_citations": "45\n", "authors": ["1590"]}
{"title": "Efficient formalism-independent monitoring of parametric properties\n", "abstract": " Parametric properties provide an effective and natural means to describe object-oriented system behaviors, where the parameters are typed by classes and bound to object instances at runtime. Efficient monitoring of parametric properties, in spite of increasingly growing interest due to applications such as testing and security, imposes a highly non-trivial challenge on monitoring approaches due to the potentially huge number of parameter instances. Existing solutions usually compromise their expressiveness for performance or vice versa. In this paper, we propose a generic, in terms of specification formalism, yet efficient, solution to monitoring parametric specifications. Our approach is based on a general algorithm for slicing parametric traces and makes use of static knowledge about the desired property to optimize monitoring. The needed knowledge is not specific to the underlying formalism and can be easily\u00a0\u2026", "num_citations": "45\n", "authors": ["1590"]}
{"title": "Composing hidden information modules over inclusive institutions\n", "abstract": " This paper studies the composition of modules that can hide information, over a very general class of logical systems called inclusive institutions. Two semantics are given for the composition of such modules using five familiar operations, and a property called conservativity is shown necessary and sufficient for these semantics to agree. The first semantics extracts the visible properties of the result of composing both the visible and hidden parts of modules, while the second uses only the visible properties of the components; the two semantics agree when the visible consequences of hidden information are enough to determine the result of the composition. A number of \u201claws of software composition\u201d are proved relating the five composition operations. Inclusive institutions simplify many of the proofs. The approach has application to module composition technology, for both programs and specifications.", "num_citations": "45\n", "authors": ["1590"]}
{"title": "Rule-based analysis of dimensional safety\n", "abstract": " Dimensional safety policy checking is an old topic in software analysis concerned with ensuring that programs do not violate basic principles of units of measurement. Scientific and/or navigation software is routinely dimensional and violations of measurement unit safety policies can hide significant domain-specific errors which are hard or impossible to find otherwise. Dimensional analysis of programs written in conventional programming languages is addressed in this paper. We draw general design principles for dimensional analysis tools and then discuss our prototypes, implemented by rewriting, which include both dynamic and static checkers. Our approach is based on assume/assert annotations of code which are properly interpreted by our tools and ignored by standard compilers/interpreters. The output of our prototypes consists of warnings that list those expressions violating the unit safety policy\u00a0\u2026", "num_citations": "45\n", "authors": ["1590"]}
{"title": "Incompleteness of behavioral logics\n", "abstract": " Incompleteness results for behavioral logics are investigated. We show that there is a basic finite behavioral specification for which the behavioral satisfaction problem is not recursively enumerable, which means that there are no automatic methods for proving all true statements; in particular, behavioral logics do not admit complete deduction systems. This holds for all of the behavioral logics of which we are aware. We also prove that the behavioral satisfaction problem is not co-recursively enumerable, which means that there is no automatic way to refute false statements in behavioral logics. In fact we show stronger results, that all behavioral logics are \u03a002-hard, and that, for some data algebras, the complexity of behavioral satisfaction is not even arithmetic; matching upper bounds are established for some behavioral logics. In addition, we show for the fixed-data case that if operations may have more than one\u00a0\u2026", "num_citations": "45\n", "authors": ["1590"]}
{"title": "Efficient Formalism-Independent Monitoring of Parametric Properties\n", "abstract": " Parametric properties provide an effective and natural means to describe object-oriented system behaviors, where the parameters are typed by classes and bound to object instances at runtime. Efficient monitoring of parametric properties, in spite of increasingly growing interest due to applications such as testing and security, imposes a highly non-trivial challenge on monitoring approaches due to the potentially huge number of parameter instances. Existing solutions usually compromise their expressiveness for performance or vice versa. In this paper, we propose a generic, in terms of specification formalism, yet efficient, solution to monitoring parametric specifications. Our approach is based on a general algorithm for slicing parametric traces and makes use of static knowledge about the desired property to optimize monitoring. The needed knowledge is not specific to the underlying formalism and can be easily computed when generating monitoring code from the property. Our approach works with any specification formalism, providing better and extensible expressiveness. Also, a thorough evaluation shows that our technique outperforms other state-of-art techniques optimized for particular logics or properties1.", "num_citations": "45\n", "authors": ["1590"]}
{"title": "A language-independent proof system for full program equivalence\n", "abstract": " Two programs are fully equivalent if, for the same input, either they both diverge or they both terminate with the same result. Full equivalence is an adequate notion of equivalence for programs written in deterministic languages. It is useful in many contexts, such as capturing the correctness of program transformations within the same language, or capturing the correctness of compilers between two different languages. In this paper we introduce a language-independent proof system for full equivalence, which is parametric in the operational semantics of two languages and in a state-similarity relation. The proof system is sound: a proof tree establishes the full equivalence of the programs given to it as input. We illustrate it on two programs in two different languages (an imperative one and a functional one), that both compute the Collatz sequence. The Collatz sequence is an interesting case study since it is\u00a0\u2026", "num_citations": "42\n", "authors": ["1590"]}
{"title": "Semantics and algorithms for parametric monitoring\n", "abstract": " Analysis of execution traces plays a fundamental role in many program analysis approaches, such as runtime verification, testing, monitoring, and specification mining. Execution traces are frequently parametric, i.e., they contain events with parameter bindings. Each parametric trace usually consists of many meaningful trace slices merged together, each slice corresponding to one parameter binding. This gives a semantics-based solution to parametric trace analysis. A general-purpose parametric trace slicing technique is introduced, which takes each event in the parametric trace and dispatches it to its corresponding trace slices. This parametric trace slicing technique can be used in combination with any conventional, non-parametric trace analysis technique, by applying the later on each trace slice. As an instance, a parametric property monitoring technique is then presented. The presented parametric trace slicing and monitoring techniques have been implemented and extensively evaluated. Measurements of runtime overhead confirm that the generality of the discussed techniques does not come at a performance expense when compared with existing parametric trace monitoring systems.", "num_citations": "42\n", "authors": ["1590"]}
{"title": "CIRC: A Circular Coinductive Prover\n", "abstract": " CIRC is an automated circular coinductive prover implemented as an extension of Maude. The circular coinductive technique that forms the core of CIRC is discussed, together with a high-level implementation using metalevel capabilities of rewriting logic. To reflect the strength of CIRC in automatically proving behavioral properties, an example defining and proving properties about infinite streams of infinite binary trees is shown. CIRC also provides limited support for automated inductive proving, which can be used in combination with coinduction.", "num_citations": "42\n", "authors": ["1590"]}
{"title": "Towards a unified theory of operational and axiomatic semantics\n", "abstract": " This paper presents a nine-rule language-independent proof system that takes an operational semantics as axioms and derives program reachability properties, including ones corresponding to Hoare triples. This eliminates the need for language-specific Hoare-style proof rules to verify programs, and, implicitly, the tedious step of proving such proof rules sound for each language separately. The key proof rule is Circularity, which is coinductive in nature and allows for reasoning about constructs with repetitive behaviors (e.g., loops). The generic proof system is shown sound and has been implemented in the MatchC verifier.", "num_citations": "40\n", "authors": ["1590"]}
{"title": "The rewriting logic semantics project: A progress report\n", "abstract": " Rewriting logic is an executable logical framework well suited for the semantic definition of languages. Any such framework has to be judged by its effectiveness to bridge the existing gap between language definitions on the one hand, and language implementations and language analysis tools on the other. We give a progress report on how researchers in the rewriting logic semantics project are narrowing the gap between theory and practice in areas such as: modular semantic definitions of languages; scalability to real languages; support for real time; semantics of software and hardware modeling languages; and semantics-based analysis tools such as static analyzers, model checkers, and program provers.", "num_citations": "38\n", "authors": ["1590"]}
{"title": "Computationally equivalent elimination of conditions\n", "abstract": " An automatic and easy to implement transformation of conditional term rewrite systems into computationally equivalent unconditional term rewrite systems is presented. No special support is needed from the underlying unconditional rewrite engine. Since unconditional rewriting is more amenable to parallelization, our transformation is expected to lead to efficient concurrent implementations of rewriting.", "num_citations": "38\n", "authors": ["1590"]}
{"title": "Equality of streams is a \u03a00 over 2-complete problem\n", "abstract": " This paper gives a precise characterization for the complexity of the problem of proving equal two streams defined with a finite number of equations: \u03a00 over 2. Since the \u03a0 0 over 2 class includes properly both the reursively enumerable and the corecursively enumerable classes, this result implies that neither the set of pairs of equal streams nor the set of pairs of unequal streams is recursively enumerable. Consequently, one can find no algorithm for determining equality of streams, as well as no algorithm for determining inequality of streams. In particular, there is no complete proof system for equality of streams and no complete system for inequality of streams.", "num_citations": "36\n", "authors": ["1590"]}
{"title": "K: A semantic framework for programming languages and formal analysis tools\n", "abstract": " We give an overview of the K framework, following the lecture notes presented by the author at the Marktoberdorf Summer School in year 2016.", "num_citations": "35\n", "authors": ["1590"]}
{"title": "Runtime verification with the RV system\n", "abstract": " The RV system is the first system to merge the benefits of Runtime Monitoring with Predictive Analysis. The Runtime Monitoring portion of RV is based on the successful Monitoring Oriented Programming system developed at the University of Illinois [6,7,9,21,5], while the Predictive Analysis capability is a vastly expanded version of the jPredictor System also developed at the University of Illinois [11,14].               With the RV system, runtime monitoring is supported and encouraged as a fundamental principle for building reliable software: monitors are automatically synthesized from specified properties and integrated into the original system to check its dynamic behaviors. When certain conditions of interest occur, such as a violation of a specification, user-defined actions will be triggered, which can be any code from information logging to runtime recovery. The RV system supports the monitoring of parametric\u00a0\u2026", "num_citations": "35\n", "authors": ["1590"]}
{"title": "A rewriting logic approach to type inference\n", "abstract": " Meseguer and Ro\u015fu proposed rewriting logic semantics (RLS) as a programing language definitional framework that unifies operational and algebraic denotational semantics. RLS has already been used to define a series of didactic and real languages, but its benefits in connection with defining and reasoning about type systems have not been fully investigated. This paper shows how the same RLS style employed for giving formal definitions of languages can be used to define type systems. The same term-rewriting mechanism used to execute RLS language definitions can now be used to execute type systems, giving type checkers or type inferencers. The proposed approach is exemplified by defining the Hindley-Milner polymorphic type inferencer  as a rewrite logic theory and using this definition to obtain a type inferencer by executing it in a rewriting logic engine. The inferencer obtained this way\u00a0\u2026", "num_citations": "33\n", "authors": ["1590"]}
{"title": "An executable rewriting logic semantics of K-Scheme\n", "abstract": " This paper presents an executable rewriting logic semantics of K-Scheme, a dialect of Scheme based (partially) on the informal definition given in the R5RS report (Kelsey et al. 1998). The presented semantics follows the K language definitional style (Rosu 2005 and 2006) and is a pure rewriting logic specification (Meseguer 1992) containing 772 equations and 1 rewrite rule, so it can also be regarded as an algebraic denotational specification with an initial model semantics. Rewriting logic specifications can be executed on common (context-insensitive) rewrite engines, provided that equations are oriented into rewrite rules, typically from left-to-right. While in theory rewriting logic specifications can let certain behaviors underspecified, thus allowing more models, in practice they need to completely specify all the desired behaviors if one wants to use their associated rewrite systems as \u201cinterpreters\u201d, or \u201cimplementations\u201d. To become executable, K-Scheme overspecifies certain features left undefined on purpose in R5RS. In spite of overspecifying for executability reasons, the rewriting logic semantics in this paper is the most complete formal definition of a language in the Scheme family that we are aware of, in the sense that it provides definitions for more Scheme language features than any other similar attempts. The presented executable definition of K-Scheme can serve as a platform for experimentation with variants and extensions of Scheme, for example concurrency. The Maude system is used in this paper, but other rewrite engines could have been used as well. Even though, on paper, K rewrite-based definitions tend to be as compact and\u00a0\u2026", "num_citations": "33\n", "authors": ["1590"]}
{"title": "GPredict: Generic predictive concurrency analysis\n", "abstract": " Predictive trace analysis (PTA) is an effective approach for detecting subtle bugs in concurrent programs. Existing PTA techniques, however, are typically based on adhoc algorithms tailored to low-level errors such as data races or atomicity violations, and are not applicable to high-level properties such as \"a resource must be authenticated before use\" and \"a collection cannot be modified when being iterated over\". In addition, most techniques assume as input a globally ordered trace of events, which is expensive to collect in practice as it requires synchronizing all threads. In this paper, we present GPredict: a new technique that realizes PTA for generic concurrency properties. Moreover, GPredict does not require a global trace but only the local traces of each thread, which incurs much less runtime overhead than existing techniques. Our key idea is to uniformly model violations of concurrency properties and the\u00a0\u2026", "num_citations": "32\n", "authors": ["1590"]}
{"title": "EnforceMOP: a runtime property enforcement system for multithreaded programs\n", "abstract": " Multithreaded programs are hard to develop and test. In order for programs to avoid unexpected concurrent behaviors at runtime, for example data-races, synchronization mechanisms are typically used to enforce a safe subset of thread interleavings. Also, to test multithreaded programs, devel-opers need to enforce the precise thread schedules that they want to test. These tasks are nontrivial and error prone.", "num_citations": "32\n", "authors": ["1590"]}
{"title": "Equational axiomatizability for coalgebra\n", "abstract": " A characterization result for equationally definable classes of certain coalgebras (including basic hidden algebra) shows that a class of coalgebras is definable by equations if and only if it is closed under coproducts, quotients, sources of morphisms and representative inclusions. The notions of equation and satisfaction are axiomatized in order to include the different approaches in the literature.", "num_citations": "31\n", "authors": ["1590"]}
{"title": "An overview of the Tatami project\n", "abstract": " Publisher SummaryThis chapter describes the Tatami project at University of California, which is aimed at developing a system to support distributed cooperative software development over the web, and validation of concurrent distributed software. The main components of this project are a proof assistant, a generator for documentation websites, a database, an equational proof engine, and a communication protocol to support distributed cooperative work. The Tatami system design was motivated by three main goals such as, verify distributed concurrent software, provide a distributed environment for cooperative work and produce proofs that are easier to read. This system also differs from related systems on many respects such as: (1)it is rigorously based on an advanced version of hidden algebra allowing first order sentences with equational atoms interpreted behaviorally, (2)design is separated from validation\u00a0\u2026", "num_citations": "31\n", "authors": ["1590"]}
{"title": "Program verification by coinduction\n", "abstract": " We present a novel program verification approach based on coinduction, which takes as input an operational semantics. No intermediates like program logics or verification condition generators are needed. Specifications can be written using any state predicates. We implement our approach in Coq, giving a certifying language-independent verification framework. Our proof system is implemented as a single module imported unchanged into language-specific proofs. Automation is reached by instantiating a generic heuristic with language-specific tactics. Manual assistance is also smoothly allowed at points the automation cannot handle. We demonstrate the power and versatility of our approach by verifying algorithms as complicated as Schorr-Waite graph marking and instantiating our framework for object languages in several styles of semantics. Finally, we show that our coinductive approach\u00a0\u2026", "num_citations": "30\n", "authors": ["1590"]}
{"title": "Certifying domain-specific policies\n", "abstract": " Proof-checking code for compliance to safety policies potentially enables a product-oriented approach to certain aspects of software certification. To date, previous research has focused on generic, low-level programming-language properties such as memory type safety. In this paper we consider proof-checking higher-level domain-specific properties for compliance to safety policies. The paper first describes a framework related to abstract interpretation in which compliance to a class of certification policies can be efficiently calculated. Membership equational logic is shown to provide a rich logic for carrying out such calculations, including partiality, for certification. The architecture for a domain-specific certifier is described, followed by an implemented case study. The case study considers consistency of abstract variable attributes in code that performs geometric calculations in Aerospace systems.", "num_citations": "30\n", "authors": ["1590"]}
{"title": "Circular coinduction\n", "abstract": " Circular coinduction is a technique for behavioral reasoning that extends cobasis coinduction to specifications with circularities. Because behavioral satisfaction is not recursively enumerable, no algorithm can work for every behavioral statement. However, algorithms using circular coinduction can prove every practical behavioral result that we know. This paper proves the correctness of circular coinduction and some consequences.", "num_citations": "30\n", "authors": ["1590"]}
{"title": "K: A Rewriting-Based Framework for Computations--Preliminary version--\n", "abstract": " K is a definitional framework based on term rewriting, in which programming languages, calculi, as well as type systems or formal analysis tools can be defined making use of special list and/or set structures, called cells, which can be potentially nested. In addition to cells, K definitions contain equations capturing structural equivalences that do not count as computational steps, and rewrite rules capturing computational steps or irreversible transitions. Rewrite rules in K are unconditional, i.e., they need no computational premises (they are rule schemata and may have ordinary side conditions, though), and they are context-insensitive, so in K rewrite rules apply concurrently as soon as they match, without any contextual delay or restrictions.  The distinctive feature of K compared to other term rewriting approaches in general and to rewriting logic in particular, is that K allows rewrite rules to apply concurrently even in cases when they overlap, provided that they do not change the overlapped portion of the term. This allows for truly concurrent semantics to programming languages and calculi. For example, two threads that read the same location of memory can do that concurrently, even though the corresponding rules overlap on the store location being read. The distinctive feature of K compared to other frameworks for true concurrency, like chemical abstract machines (Chams) or membrane systems (P-systems), is that equations and rewrite rules can match across multiple cells and thus perform changes many places at the same time, in one step.  K provides special support for list cells that carry ``computational meaning'', called computations\u00a0\u2026", "num_citations": "29\n", "authors": ["1590"]}
{"title": "A rewrite framework for language definitions and for generation of efficient interpreters\n", "abstract": " A rewrite logic semantic definitional framework for programming languages is introduced, called K, together with partially automated translations of K language definitions into rewriting logic and into C. The framework is exemplified by defining SILF, a simple imperative language with functions. The translation of K definitions into rewriting logic enables the use of the various analysis tools developed for rewrite logic specifications, while the translation into C allows for very efficient interpreters. A suite of tests show the performance of interpreters compiled from K definitions.", "num_citations": "28\n", "authors": ["1590"]}
{"title": "KOOL: An application of rewriting logic to language prototyping and analysis\n", "abstract": " This paper presents KOOL, a concurrent, dynamic, object-oriented language defined in rewriting logic. KOOL has been designed as an experimental language, with a focus on making the language easy to extend. This is done by taking advantage of the flexibility provided by rewriting logic, which allows for the rapid prototyping of new language features. An example of this process is illustrated by sketching the addition of synchronized methods. KOOL also provides support for program analysis through language extensions and the underlying capabilities of rewriting logic. This support is illustrated with several examples.", "num_citations": "28\n", "authors": ["1590"]}
{"title": "Symbolic reachability analysis for rewrite theories\n", "abstract": " This dissertation presents a significant step forward in automatic and semi-automatic reasoning for reachability properties of rewriting logic specifications, a major research goal in the current state of the art. In particular, this work develops deductive techniques for reasoning symbolically about specifications with initial model semantics, including:(i) new constructor-based notions for reachability analysis,(ii) a proof system for the task of proving safety properties, and (iii) a novel method for symbolic reachability analysis of rewrite theories with constrained built-ins. These three new techniques are not just theoretical developments: each of them has been implemented in freely available tools for the automated reasoning presented in this thesis and are validated through case studies. These case studies include:(i) a reliable communication protocol,(ii) a secure-by-design browser system, and (iii) a NASA language for\u00a0\u2026", "num_citations": "27\n", "authors": ["1590"]}
{"title": "Certifying measurement unit safety policy\n", "abstract": " Measurement unit safety policy checking is a topic in software analysis concerned with ensuring that programs do not violate basic principles of units of measurement. Such violations can hide significant domain-specific errors which are hard or impossible to find otherwise. Measurement unit analysis by means of automatic deduction is addressed in this paper. We draw general design principles for measurement unit certification tools and discuss our prototype for the C language, which includes both dynamic and static checkers. Our approach is based on assume/assert annotations of code, which are properly interpreted by our deduction-based tools and ignored by standard compilers. We do not modify the language in order to support units. The approach can be extended to incorporate other safety policies without great efforts.", "num_citations": "27\n", "authors": ["1590"]}
{"title": "Distributed cooperative formal methods tools\n", "abstract": " This paper describes some tools to support formal methods, and conversely some formal methods for developing such tools. We focus on distributed cooperative proving over the web. Our tools include a proof editor/assistant, servers for remote proof execution, a distributed truth protocol, an editor generator; and a new method for interface design called algebraic semiotics, which combines semiotics with algebraic specification. Some examples are given.", "num_citations": "27\n", "authors": ["1590"]}
{"title": "Matching \u03bc-logic\n", "abstract": " Matching logic is a logic for specifying and reasoning about structure by means of patterns and pattern matching. This paper makes two contributions. First, it proposes a sound and complete proof system for matching logic in its full generality. Previously, sound and complete deduction for matching logic was known only for particular theories providing equality and membership. Second, it proposes matching \u03bc -Iogic, an extension of matching logic with a least fixpoint \u03bc -binder, It is shown that matching \u03bc -Iogic captures as special instances many important logics in mathematics and computer science, including first-order logic with least fixpoints, modal \u03bc -Iogic as well as dynamic logic and various temporal logics such as infinite/finite-trace linear temporal logic and computation tree logic, and notably reachability logic, the underlying logic of the \\mathbbk framework for programming language semantics and formal\u00a0\u2026", "num_citations": "26\n", "authors": ["1590"]}
{"title": "Parametric trace slicing\n", "abstract": " A program trace is obtained and events of the program trace are traversed. For each event identified in traversing the program trace, a trace slice of which the identified event is a part is identified based on the parameter instance of the identified event. For each trace slice of which the identified event is a part, the identified event is added to an end of a record of the trace slice. These parametric trace slices can be used in a variety of different manners, such as for monitoring, mining, and predicting.", "num_citations": "26\n", "authors": ["1590"]}
{"title": "A rewriting logic approach to static checking of units of measurement in C\n", "abstract": " Many C programs assume the use of implicit domain-specific information. A common example is units of measurement, where values can have both a standard C type and an associated unit. However, since there is no way in the C language to represent this additional information, violations of domain-specific policies, such as unit safety violations, can be difficult to detect. In this paper we present a static analysis, based on the use of an abstract C semantics defined using rewriting logic, for the detection of unit violations in C programs. In contrast to typed approaches, the analysis makes use of annotations present in C comments on function headers and in function bodies, leaving the C language unchanged. Initial evaluation results show that performance scales well, and that errors can be detected without imposing a heavy annotation burden.", "num_citations": "26\n", "authors": ["1590"]}
{"title": "Defining and Executing P Systems with Structured Data in K\n", "abstract": " K is a rewrite-based framework proposed for giving formal executable semantics to programming languages and/or calculi. K departs from other rewrite-based frameworks in two respects: (1) it assumes multisets and lists as builtin, the former modeling parallel features, while the latter sequential ones; and (2) the parallel application of rewriting rules is extended from non-overlapping rules to rules which may overlap, but on parts which are not changed by these rules (may overlap on \u201cread only\u201d parts). This paper shows how P systems and variants can be defined as K (rewrite) systems. This is the first representation of P systems into a rewrite-based framework that captures the behavior (reaction steps) of the original P system step-for-step. In addition to providing a formal executable semantic framework for P systems, the embedding of P systems as K systems also serves as a basis for experimenting with and\u00a0\u2026", "num_citations": "26\n", "authors": ["1590"]}
{"title": "Certifying optimality of state estimation programs\n", "abstract": " The theme of this paper is certifying software for state estimation of dynamic systems, which is an important problem found in spacecraft, aircraft, geophysical, and in many other applications. The common way to solve state estimation problems is to use Kalman filters, i.e., stochastic, recursive algorithms providing statistically optimal state estimates based on noisy sensor measurements. We present an optimality certifier for Kalman filter programs, which is a system taking a program claiming to implement a given formally specified Kalman filter, as well as a formal certificate in the form of assertions and proof scripts merged within the program via annotations, and tells whether the code correctly implements the specified state estimation problem. Kalman filter specifications and certificates can be either produced manually by expert users or can be generated automatically: we also present our first steps in\u00a0\u2026", "num_citations": "25\n", "authors": ["1590"]}
{"title": "Axiomatizability in inclusive equational logics\n", "abstract": " A categorical framework for equational logics is presented, together with axiomatizability results in the style of Birkhoff. The distinctive categorical structures used are inclusion systems, which are an alternative to factorization systems in which factorization is required to be unique rather than unique \u2018up to an isomorphism\u2019. In this framework, models are any objects, and equations are special epimorphisms in C, while satisfaction is injectivity. A first result says that equations-as-epimorphisms define exactly the quasi-varieties, suggesting that epimorphisms actually represent conditional equations. In fact, it is shown that the projectivity/freeness of the domain of epimorphisms is what makes the difference between unconditional and conditional equations, the first defining the varieties, as expected. An abstract version of the axiom of choice seems to be sufficient for free objects to be projective, in which case the\u00a0\u2026", "num_citations": "25\n", "authors": ["1590"]}
{"title": "A Birkhoff-like axiomatizability result for hidden algebra and coalgebra\n", "abstract": " A characterization result for behaviorally definable classes of hidden algebras shows that a class of hidden algebras is behaviorally definable by equations if and only if it is closed under coproducts, quotients, morphisms and representative inclusions. The second part of the paper categorically generalizes this result to a framework of any category with coproducts, a final object and an inclusion system; this is general enough to include all coalgebra categories of interest. As a technical issue, the notions of equation and satisfaction are axiomatized in order to include the different approaches in the literature.", "num_citations": "25\n", "authors": ["1590"]}
{"title": " Framework Distilled\n", "abstract": " is a rewrite-based executable semantic framework in which programming languages, type systems, and formal analysis tools can be defined using configurations, computations and rules. Configurations organize the state in units called cells, which are labeled and can be nested. Computations are special nested list structures sequentializing computational tasks, such as fragments of program.  (rewrite) rules make it explicit which parts of the term they read-only, write-only, read-write, or do not care about. This makes  suitable for defining truly concurrent languages even in the presence of sharing. Computations are like any other terms in a rewriting environment: they can be matched, moved from one place to another, modified, or deleted. This makes  suitable for defining control-intensive features such as abrupt termination, exceptions or call/cc. This paper presents an overview of  Framework and\u00a0\u2026", "num_citations": "24\n", "authors": ["1590"]}
{"title": "Security-policy monitoring and enforcement with JavaMOP\n", "abstract": " Software security attacks represent an ever growing problem. One way to make software more secure is to use Inlined Reference Monitors (IRMs), which allow security specifications to be inlined inside a target program to ensure its compliance with the desired security specifications. The IRM approach has been developed primarily by the security community. Runtime Verification (RV), on the other hand, is a software engineering approach, which is intended to formally encode system specifications within a target program such that those specifications can be later enforced during the execution of the program. Until now, the IRM and RV approaches have lived separate lives; in particular RV techniques have not been applied to the security domain, being used instead to aid program correctness and testing. This paper discusses the usage of a formalism-generic RV system, JavaMOP, as a means to specify IRMs\u00a0\u2026", "num_citations": "23\n", "authors": ["1590"]}
{"title": "An effective algorithm for the membership problem for extended regular expressions\n", "abstract": " By adding the complement operator (\u00ac), extended regular expressions (ERE) can encode regular languages non-elementarily more succinctly than regular expressions. The ERE membership problem asks whether a word w of size n belongs to the language of an ERE                 R of size m. Unfortunately, the best known membership algorithms are either non-elementary in m or otherwise require space \u03a9(n                 2) and time \u03a9(n                 3); since in many practical applications n can be very large, these space and time requirements could be prohibitive. In this paper we present an ERE membership algorithm that runs in space O(n \u00b7(logn\u2009+\u2009m) \u00b72                   m                 ) and time O(n                 2 \u00b7(logn\u2009+\u2009m) \u00b72                   m                 ). The presented algorithm outperforms the best known algorithms when n is exponentially larger than m.", "num_citations": "23\n", "authors": ["1590"]}
{"title": "An Equational Specification for the Scheme Language.\n", "abstract": " This work describes the formal semantics of Scheme 3 as an equational theory in the Maude rewriting system. The semantics is based on continuations and is highly modular. We briefly investigate the relationship between our methodology for defining programming languages and other semantic formalisms. We conclude by showing some performance results of the interpreter obtained for free from the executable specification.", "num_citations": "23\n", "authors": ["1590"]}
{"title": "From conditional to unconditional rewriting\n", "abstract": " An automated technique to translate conditional rewrite rules into unconditional ones is presented, which is suitable to implement, or compile, conditional rewriting on top of much simpler and easier to optimize unconditional rewrite systems. An experiment performed on world\u2019s fastest conditional rewriting engines shows that speedups for conditional rewriting of an order of magnitude can already be obtained by applying the presented technique as a front-end transformation.", "num_citations": "23\n", "authors": ["1590"]}
{"title": "On equational Craig interpolation\n", "abstract": " \" Generalizations of Craig interpolation are investigated for equational logic. Our approach is to do as much as possible at a categorical level, before drawing out the concrete implications.", "num_citations": "23\n", "authors": ["1590"]}
{"title": "A generic framework for symbolic execution: A coinductive approach\n", "abstract": " We propose a language-independent symbolic execution framework. The approach is parameterised by a language definition, which consists of a signature for the syntax and execution infrastructure of the language, a model interpreting the signature, and rewrite rules for the language's operational semantics. Then, symbolic execution amounts to computing symbolic paths using a derivative operation. We prove that the symbolic execution thus defined has the properties naturally expected from it, meaning that the feasible symbolic executions of a program and the concrete executions of the same program mutually simulate each other. We also show how a coinduction-based extension of symbolic execution can be used for the deductive verification of programs. We show how the proposed symbolic-execution approach, and the coinductive verification technique based on it, can be seamlessly implemented in\u00a0\u2026", "num_citations": "22\n", "authors": ["1590"]}
{"title": "Rv-match: Practical semantics-based program analysis\n", "abstract": " We present RV-Match, a tool for checking C programs for undefined behavior and other common programmer mistakes. Our tool is extracted from the most complete formal semantics of the C11 language. Previous versions of this tool were used primarily for testing the correctness of the semantics, but we have improved it into a tool for doing practical analysis of real C programs. It beats many similar tools in its ability to catch a broad range of undesirable behaviors. We demonstrate this with comparisons based on a third-party benchmark.", "num_citations": "22\n", "authors": ["1590"]}
{"title": "Towards categorizing and formalizing the JDK API\n", "abstract": " Formal specification of correct library usage is extremely useful, both for software developers and for the formal analysis tools they use, such as model checkers or runtime monitoring systems. Unfortunately, the process of creating formal specifications is time consuming, and, for the most part, even the libraries in greatest use, such as the Java Development Kit (JDK) standard library, are left wholly without formal specification. This paper presents a tool-supported approach to help writing formal specifications for Java libraries and creating documentation augmented with highlighting and formal specifications. The presented approach has been applied to systematically and completely formalize the runtime properties of three core and commonly used packages of the JDK API, namely java.io, java.lang and java.util, yielding 137 formal specifications. Indirectly, this paper also brings empirical evidence that parametric specifications may be sufficiently powerful to express virtually all desirable runtime properties of the JDK API, and that its informal documentation can be formalized.", "num_citations": "22\n", "authors": ["1590"]}
{"title": "Language definitions as rewrite theories\n", "abstract": " K is a formal framework for defining operational semantics of programming languages. The K-Maude compiler translates K language definitions to Maude rewrite theories. The compiler enables program execution by using the Maude rewrite engine with the compiled definitions, and program analysis by using various Maude analysis tools. K supports symbolic execution in Maude by means of an automatic transformation of language definitions. The transformed definition is called the symbolic extension of the original definition. In this paper we investigate the theoretical relationship between K language definitions and their Maude translations, between symbolic extensions of K definitions and their Maude translations, and how the relationship between K definitions and their symbolic extensions is reflected on their respective representations in Maude. In particular, the results show how analysis performed with\u00a0\u2026", "num_citations": "20\n", "authors": ["1590"]}
{"title": "On safety properties and their monitoring\n", "abstract": " This paper addresses the problem of runtime verification from a foundational perspective, answering questions like \u201cIs there a consensus among the various definitions of a safety property?\u201d(Answer: Yes),\u201cHow many safety properties exist?\u201d(Answer: As many as real numbers),\u201cHow difficult is the problem of monitoring a safety property?\u201d(Answer: Arbitrarily complex),\u201cIs there any formalism that can express all safety properties?\u201d(Answer: No), etc. Various definitions of safety properties as sets of execution traces have been proposed in the literature, some over finite traces, others over infinite traces, yet others over both finite and infinite traces. By employing cardinality arguments and a novel notion of persistence, this paper first establishes the existence of bijective correspondences between the various notions of safety property. It then shows that safety properties can be characterized as \u201calways past\u201d properties. Finally, it proposes a general notion of monitor, which allows to show that safety properties correspond precisely to the monitorable properties, and then to establish that monitoring a safety property is arbitrarily hard.", "num_citations": "20\n", "authors": ["1590"]}
{"title": "A rewriting logic semantics approach to modular program analysis\n", "abstract": " The K framework, based on rewriting logic semantics, provides a powerful logic for defining the semantics of programming languages. While most work in this area has focused on defining an evaluation semantics for a language, it is also possible to define an abstract semantics that can be used for program analysis. Using the SILF language (Hills, Serbanuta and Rosu, 2007), this paper describes one technique for defining such a semantics: policy frameworks. In policy frameworks, an analysis-generic, modular framework is first defined for a language. Individual analyses, called policies, are then defined as extensions of this framework, with each policy defining analysis-specific semantic rules and an annotation language which, in combination with support in the language front-end, allows users to annotate program types and functions with information used during program analysis. Standard term rewriting techniques are used to analyze programs by evaluating them in the policy semantics.", "num_citations": "20\n", "authors": ["1590"]}
{"title": "Specification and error pattern based program monitoring\n", "abstract": " We briefly present Java PathExplorer (JPAX), a tool developed at NASA Ames for monitoring the execution of Java programs. JPAX can be used not only during program testing to reveal subtle errors, but also can be applied during operation to survey safety critical systems. The tool facilitates automated instrumentation of a program in order to properly observe its execution. The instrumentation can be either at the bytecode level or at the source level when the source code is available. JPaX is an instance of a more general project, called PathExplorer (PAX), which is a basis for experiments rather than a fixed system, capable of monitoring various programming languages and experimenting with other logics and analysis techniques", "num_citations": "20\n", "authors": ["1590"]}
{"title": "Rv-android: Efficient parametric android runtime verification, a brief tutorial\n", "abstract": " RV-Android is a new freely available open source runtime library for monitoring formal safety properties on Android. RV-Android uses the commercial RV-Monitor technology as its core monitoring library generation technology, allowing for the verification of safety properties during execution and operating entirely in userspace with no kernel or operating system modifications required. RV-Android improves on previous Android monitoring work by replacing the JavaMOP framework with RV-Monitor, a more advanced monitoring library generation tool with core algorithmic improvements that greatly improve resource consumption, efficiency, and battery life considerations. We demonstrate the developer usage of RV-Android with the standard Android build process, using instrumentation mechanisms effective on both Android binaries and source code. Our method allows for both property development and\u00a0\u2026", "num_citations": "19\n", "authors": ["1590"]}
{"title": "Maximal Causal Models for Sequentially Consistent Multithreaded Systems\n", "abstract": " This paper shows that it is possible to build a theoretically maximal and sound causal model for concurrent computations from a given execution trace. For an observed execution, the proposed model comprises all consistent executions which can be derived from it using only knowledge about the execution machine. The existence of such a model is of great theoretical value. First, by comprising all feasible executions, it can be used to prove soundness of other causal models: indeed, several models underlying existing techniques are shown to be embedded into the maximal model, so all these models are sound. Second, since it is maximal, the proposed model allows for natural and causal-model-independent definitions of trace-based properties; this paper proposes maximal definitions for causal dataraces and causal atomicity. Finally, although defined axiomatically, the set of traces comprised by the proposed model are shown to be effectively constructed from an initial observed trace. Thus, maximal causal models are not only theoretically relevant, but they are also amenable for developing practical analysis tools.", "num_citations": "19\n", "authors": ["1590"]}
{"title": "Monitoring oriented programming-a project overview\n", "abstract": " This paper gives a brief overview of Monitoring Oriented Programming (MOP). In MOP, runtime monitoring is supported and encouraged as a fundamental principle for building reliable software: monitors are automatically synthesized from specified properties and integrated into the original system to check its dynamic behaviors. When a specification is violated or validated at runtime, user-defined actions will be triggered, which can be any code from information logging to runtime recovery. Two instances of MOP are introduced: JavaMOP (for Java programs) and BusMOP (for monitoring PCI bus traffic). The architecture of MOP is discussed, and a brief explanation of parametric trace monitoring and its implementation is given. Finally, a comprehensive evaluation of JavaMOP attests to its efficiency, especially with respect to similar systems; BusMOP, in general, imposes 0 runtime overhead on the system it is monitoring.", "num_citations": "19\n", "authors": ["1590"]}
{"title": "A K definition of Scheme\n", "abstract": " This paper presents an executable rewriting logic semantics of R5RS Scheme using the K definitional technique [19]. We refer to this definition as K-Scheme. The presented semantics follows the K language definitional style but is almost entirely equational. It can also be regarded as a denotational specification with an initial model semantics of Scheme. Equational specifications can be executed on common rewrite engines, provided that equations are oriented into rewrite rules, typically from left-to-right. The rewriting logic semantics in this paper is the most complete formal definition of Scheme that we are aware of, in the sense that it provides definitions for more Scheme language features than any other similar attempts. The presented executable definition, K-Scheme, can serve as a platform for experimentation with variants and extensions of Scheme, for example concurrency. K-Scheme also serves to show the viability of K as a definitional framework for programming languages.", "num_citations": "19\n", "authors": ["1590"]}
{"title": "An effect system and language for deterministic-by-default parallel programming\n", "abstract": " This thesis presents a new, Java-based object-oriented parallel language called Deterministic Parallel Java (DPJ). DPJ uses a novel effect system to guarantee determinism by default. That means that parallel programs are guaranteed to execute deterministically unless nondeterminism is explicitly requested. This is in contrast to the shared-memory models in widespread use today, such as threads and locks (including threads in ordinary Java). Those models are inherently nondeterministic, do not provide any way to check or enforce that a computation is deterministic, and can even have unintended data races, which can lead to strange and unexpected behaviors. Because deterministic programs are much easier to reason about than arbitrary parallel code, determinism by default simplifies parallel programming.", "num_citations": "18\n", "authors": ["1590"]}
{"title": "CS322 Fall 2003: Programming Language Design-Lecture Notes\n", "abstract": " This report contains the complete lecture notes for CS322, Programming Language Design, taught by Grigore Rosu in the Fall 2003 semester at the University of Illinois at Urbana Champaign. This large PDF document has been generated automatically from the CS322's website at: http://fsl.cs.uiuc.edu/~grosu/classes/2003/fall/cs322/.  Of particular importance is the novel technique for defining concurrent languages that starts at page 673, based on a first-order representation of computations (called \"continuations\" for simplicity, though only their suffix is an actual \"continuation structure\").", "num_citations": "18\n", "authors": ["1590"]}
{"title": "A Protocol for Distributed Cooperative Work\n", "abstract": " After a brief review of hidden algebra, we give behavioral speci cations for set theory and closure operators, and then use these to give a behavioral speci cation of an abstract protocol to support distributed cooperative work structured by dependencies in such a way as to form what we call a weak closure operator. We give some correctness proofs for this protocol, and then describe a concrete instance of it, called the tatami protocol, that supports distributed cooperative proving. Finally, we draw some methodological conclusions.", "num_citations": "18\n", "authors": ["1590"]}
{"title": "Weak inclusion systems: Part two\n", "abstract": " \" New properties and implications of inclusion systems are investigated in the present paper. Many properties of lattices, factorization systems and special practical cases can be abstracted and adapted to our framework, making the various versions of inclusion systems useful tools for computer scientists and mathematicians.", "num_citations": "18\n", "authors": ["1590"]}
{"title": "A protocol for distributed cooperative work.\n", "abstract": " After a brief review of hidden algebra, we give behavioral speci cations for set theory and closure operators, and then use these to give a behavioral speci cation of an abstract protocol to support distributed cooperative work structured by dependencies in such a way as to form what we call a weak closure operator. We give some correctness proofs for this protocol, and then describe a concrete instance of it, called the tatami protocol, that supports distributed cooperative proving. Finally, we draw some methodological conclusions.", "num_citations": "18\n", "authors": ["1590"]}
{"title": "Tools for distributed cooperative design and validation\n", "abstract": " We describe some tools to support distributed cooperative design and validation of software systems. Workers at di erent sites can collaborate on tasks including speci cation, re nement, validation, veri cation, and documentation. A distributed database supports alternative and incomplete activities, and can be read using any web browser; remote proof execution, animation, and informal explanation are supported, and results are broadcast by a protocol that prevents inconsistencies. The Kumo tool assists with validations and generates documentation websites. A range of formality is supported, from full mechanical proofs to informal\\back of envelope \" arguments, using a fuzzy logic for con dence levels. Some conclusions drawn from experiments are reported. 1", "num_citations": "18\n", "authors": ["1590"]}
{"title": "P4K: A formal semantics of P4 and applications\n", "abstract": " Programmable packet processors and P4 as a programming language for such devices have gained significant interest, because their flexibility enables rapid development of a diverse set of applications that work at line rate. However, this flexibility, combined with the complexity of devices and networks, increases the chance of introducing subtle bugs that are hard to discover manually. Worse, this is a domain where bugs can have catastrophic consequences, yet formal analysis tools for P4 programs / networks are missing. We argue that formal analysis tools must be based on a formal semantics of the target language, rather than on its informal specification. To this end, we provide an executable formal semantics of the P4 language in the K framework. Based on this semantics, K provides an interpreter and various analysis tools including a symbolic model checker and a deductive program verifier for P4. This paper overviews our formal K semantics of P4, as well as several P4 language design issues that we found during our formalization process. We also discuss some applications resulting from the tools provided by K for P4 programmers and network administrators as well as language designers and compiler developers, such as detection of unportable code, state space exploration of P4 programs and of networks, bug finding using symbolic execution, data plane verification, program verification, and translation validation.", "num_citations": "17\n", "authors": ["1590"]}
{"title": "From rewriting logic, to programming language semantics, to program verification\n", "abstract": " Rewriting logic has proven to be an excellent formalism to define executable semantics of programming languages, concurrent or not, and then to derive formal analysis tools for the defined languages with very little effort, such as model checkers. In this paper we give an overview of recent results obtained in the context of the rewriting logic semantics framework , such as complete semantics of large programming languages like C, Java, JavaScript, Python, and deductive program verification techniques that allow us to verify programs in these languages using a common verification infrastructure.", "num_citations": "17\n", "authors": ["1590"]}
{"title": "Iele: An intermediate-level blockchain language designed and implemented using formal semantics\n", "abstract": " Most languages are given an informal semantics until they are implemented, so the formal semantics comes later. Consequently, there are usually inconsistencies among the informal semantics, the implementation, and the formal semantics. IELE is an LLVM-like language for the blockchain that was specified formally and its implementation, a virtual machine, generated from the formal specification. Moreover, its design was based on problems observed formalizing the semantics of the Ethereum Virtual Machine (EVM) and from formally specifying and verifying EVM programs (also called \u201csmart contracts\u201d), so even the design decisions made for IELE are based on formal specifications. A compiler from Solidity, the predominant high-level language for smart contracts, to IELE has also been implemented, so Ethereum contracts can now also be executed on IELE. The virtual machine automatically generated from the semantics of IELE is shown to be competitive in terms of performance with the state of the art and hence can stand as the de facto implementation of the language in a production setting. Indeed, IOHK, a major blockchain company, is currently experimenting with the IELE VM in order to deploy it as its computational layer in a few months. This makes IELE the first practical language that is designed and implemented as a formal specification. It took only 10 man-months to develop IELE, which demonstrates that the programming language semantics field has reached a level of maturity that makes it appealing over the traditional, adhoc approach even for pragmatic reasons.", "num_citations": "16\n", "authors": ["1590"]}
{"title": "Circular coinduction with special contexts\n", "abstract": " Coinductive proofs of behavioral equivalence often require human ingenuity, in that one is expected to provide a \u201cgood\u201d relation extending one\u2019s goal with additional lemmas, making automation of coinduction a challenging problem. Since behavioral satisfaction is a -hard problem, one can only expect techniques and methods that approximate the behavioral equivalence. Circular coinduction is an automated technique to prove behavioral equivalence by systematically exploring the behaviors of the property to prove: if all behaviors are circular then the property holds. Empirical evidence shows that one of the major reasons for which circular coinduction does not terminate in practice is that the circular behaviors may be guarded by a context. However, not all contexts are safe. This paper proposes a large class of contexts which are safe guards for circular behaviors, called special contexts, and extends\u00a0\u2026", "num_citations": "16\n", "authors": ["1590"]}
{"title": "Runtime verification\n", "abstract": " \u201cWe had planned to approach the planet at an altitude of about 150 kilometers (93 miles). We thought we were doing that, but upon review of the last six to eight hours of data leading up to arrival, we saw indications that the actual approach altitude had been much lower. It appears that the actual altitude was about 60 kilometers (37 miles)\u201d.", "num_citations": "16\n", "authors": ["1590"]}
{"title": "Behavioral coinductive rewriting\n", "abstract": " this report, we propose a new technique which combines behavioral rewriting and coinduction. Equational logic is not sound for behavioral satisfaction since there might be operations which are not behaviorally congruent, that is, which do not preserve the behavioral equivalence. We modified the congruence rule of equational logic accordingly in section 3 (see also [12, 9]), obtaining a sound five rule system for hidden algebra. Behavioral rewriting is for these rules what standard rewriting is for equational logic and (many sorted) algebra. In particular, in section 4 we show that if the behavioral rewriting relation is confluent, then an equation (8X) t= t", "num_citations": "16\n", "authors": ["1590"]}
{"title": "Finite-trace linear temporal logic: Coinductive completeness\n", "abstract": " Linear temporal logic (LTL) is suitable not only for infinite-trace systems, but also for finite-trace systems. In particular, LTL with finite-trace semantics is frequently used as a specification formalism in runtime verification, in artificial intelligence, and in business process modeling. The satisfiability of LTL with finite-trace semantics, a known PSPACE-complete problem, has been recently studied and both indirect and direct decision procedures have been proposed. However, the proof theory of LTL with finite traces is not that well understood. Specifically, complete proof systems of LTL with only infinite or with both infinite and finite traces have been proposed in the literature, but complete proof systems directly for LTL with only finite traces are missing. The only known results are indirect, by translation to other logics, e.g., infinite-trace LTL. This paper proposes a direct sound and complete proof system for finite\u00a0\u2026", "num_citations": "15\n", "authors": ["1590"]}
{"title": "Efficient parametric runtime verification with deterministic string rewriting\n", "abstract": " Early efforts in runtime verification show that parametric regular and temporal logic specifications can be monitored efficiently. These approaches, however, have limited expressiveness: their specifications always reduce to monitors with finite state. More recent developments showed that parametric context-free properties can be efficiently monitored with overheads generally lower than 12-15%. While context-free grammars are more expressive than finite-state languages, they still do not allow every computable safety property. This paper presents a monitor synthesis algorithm for string rewriting systems (SRS). SRSs are well known to be Turing complete, allowing for the formal specification of any computable safety property. Earlier attempts at Turing complete monitoring have been relatively inefficient. This paper demonstrates that monitoring parametric SRSs is practical. The presented algorithm uses a modified\u00a0\u2026", "num_citations": "15\n", "authors": ["1590"]}
{"title": "The k primer (version 3.3)\n", "abstract": " This paper serves as a brief introduction to the K tool, a system for formally defining programming languages. It is shown how sequential or concurrent languages can be defined in K simply and modularly. These formal definitions automatically yield an interpreter for the language, as well as program analysis tools such as a state-space explorer.", "num_citations": "14\n", "authors": ["1590"]}
{"title": "Towards a module system for k\n", "abstract": " Research on the semantics of programming languages has yielded a wide array of notations and methodologies for defining languages and language features. An important feature many of these notations and methodologies lack is modularity: the ability to define a language feature once, insulating it from unrelated changes in other parts of the language, and allowing it to be reused in other language definitions. This paper introduces ongoing work on modularity features in K, an algebraic, rewriting logic based formalism for defining language semantics.", "num_citations": "14\n", "authors": ["1590"]}
{"title": "A total approach to partial algebraic specification\n", "abstract": " Partiality is a fact of life, but at present explicitly partial algebraic specifications lack tools and have limited proof methods. We propose a sound and complete way to support execution and formal reasoning of explicitly partial algebraic specifications within the total framework of membership equational logic (MEL) which has a high-performance interpreter (Maude) and proving tools. This is accomplished by a sound and complete mapping PMEL \u2192 MEL of partial membership equational (PMEL) theories into total ones. Furthermore, we characterize and give proof methods for a practical class of theories for which this mapping has \u201calmost-zero representational distance,\u201d in that the partial theory and its total translation are identical up to minor syntactic sugar conventions. This then supports very direct execution of, and formal reasoning about, partial theories at the total level. In conjunction with tools like Maude\u00a0\u2026", "num_citations": "14\n", "authors": ["1590"]}
{"title": "Maximal causal models for multithreaded systems\n", "abstract": " Extracting causal models from observed executions has proved to be an effective approach to analyze concurrent programs. Most existing causal models are based on happens-before partial orders and/or Mazurkiewicz traces. Unfortunately, these models are inherently limited in the context of multithreaded systems, since multithreaded executions are mainly determined by consistency among shared memory accesses rather than by partial orders or event independence. This paper defines a novel theoretical foundation for multithreaded executions and a novel causal model, based on memory consistency con- straints. The proposed model is sound and maximal: (1) all traces consistent with the causal model are feasible executions of the multithreaded program under analysis; and (2) assuming only the observed execution and no knowledge about the source code of the program, the proposed model captures more feasible executions than any other sound causal model. An algorithm to systematically generate all the feasible executions comprised by maximal causal models is also proposed, which can be used for testing or model checking of multithreaded system executions. Finally, a specialized submodel of the maximal one is presented, which gives an efficient and effective solution to on-the-fly datarace detection. This datarace-focused model, still captures more feasible executions than the existing happens-before-based approaches.", "num_citations": "14\n", "authors": ["1590"]}
{"title": "IELE: A rigorously designed language and tool ecosystem for the blockchain\n", "abstract": " This paper proposes IELE, an LLVM-style language, together with a tool ecosystem for implementing and formally reasoning about smart contracts on the blockchain. IELE was designed by specifying its semantics formally in the K framework. Its implementation, a IELE virtual machine (VM), as well as a formal verification tool for IELE smart contracts, were automatically generated from the formal specification. The automatically generated formal verification tool allows us to formally verify smart contracts without any gap between the verifier and the actual VM. A compiler from Solidity, the predominant high-level language for smart contracts, to IELE has also been (manually) implemented, so Ethereum contracts can now also be executed on IELE.", "num_citations": "13\n", "authors": ["1590"]}
{"title": "A language-independent approach to smart contract verification\n", "abstract": " This invited paper reports the current progress on smart contract verification with the  framework in a language-independent style.", "num_citations": "13\n", "authors": ["1590"]}
{"title": "Defining the undefinedness of C\n", "abstract": " This paper investigates undefined behavior in C and offers a few simple techniques for operationally specifying such behavior formally. A semantics-based undefinedness checker for C is developed using these techniques, as well as a test suite of undefined programs. The tool is evaluated against other popular analysis tools, using the new test suite in addition to a third-party test suite. The semantics-based tool performs at least as well or better than the other tools tested.", "num_citations": "13\n", "authors": ["1590"]}
{"title": "Parametric and termination-sensitive control dependence\n", "abstract": " A parametric approach to control dependence is presented, where the parameter is any prefix-invariant property on paths in the control-flow graph (CFG). Existing control dependencies, both direct and indirect, can be obtained as instances of the parametric framework for particular properties on paths. A novel control dependence relation, called termination-sensitive control dependence, is obtained also as an instance of the parametric framework. This control dependence is sensitive to the termination information of loops, which can be given via annotations. If all loops are annotated as terminating then it becomes the classic control dependence, while if all loops are annotated as non-terminating then it becomes the weak control dependence; since in practice some loops are terminating and others are not, termination-sensitive control dependence is expected to improve the precision of analysis tools using\u00a0\u2026", "num_citations": "13\n", "authors": ["1590"]}
{"title": "Behavioral abstraction is hiding information\n", "abstract": " We show that for any behavioral \u03a3-specification B there is an ordinary algebraic specification B\u02dc over a larger signature, such that a model behaviorally satisfies B iff it satisfies, in the ordinary sense, the \u03a3-theorems of B\u02dc. The idea is to add machinery for contexts and experiments (sorts, operations and equations), use it, and then hide it. We develop a procedure, called unhiding, which takes a finite B and produces a finite B\u02dc. The practical aspect of this procedure is that one can use any standard equational inductive theorem prover to derive behavioral theorems, even if neither equational reasoning nor induction is sound for behavioral satisfaction.", "num_citations": "13\n", "authors": ["1590"]}
{"title": "Complete categorical equational deduction\n", "abstract": " A categorical four-rule deduction system for equational logics is presented. We show that under reasonable finiteness requirements this system is complete with respect to equational satisfaction abstracted as injectivity. The generality of the presented framework allows one to derive conditional equations as well at no extra cost. In fact, our deduction system is also complete for conditional equations, a new result at the author\u2019s knowledge.", "num_citations": "13\n", "authors": ["1590"]}
{"title": "On formal analysis of OO languages using rewriting logic: Designing for performance\n", "abstract": " Rewriting logic provides a powerful, flexible mechanism for language definition and analysis. This flexibility in design can lead to problems during analysis, as different designs for the same language feature can cause drastic differences in analysis performance. This paper describes some of these design decisions in the context of KOOL, a concurrent, dynamic, object-oriented language. Also described is a general mechanism used in KOOL to support model checking while still allowing for ongoing, sometimes major, changes to the language definition.", "num_citations": "12\n", "authors": ["1590"]}
{"title": "Towards certifying domain-specific properties of synthesized code\n", "abstract": " We present a technique for certifying domain-specific properties of code generated using program synthesis technology. Program synthesis is a maturing technology that generates code from high-level specifications in particular domains. For acceptance in safety-critical applications, the generated code must be thoroughly tested which is a costly process. We show how the program synthesis system AUTOFILTER can be extended to generate not only code but also proofs that properties hold in the code. This technique has the potential to reduce the costs of testing generated code.", "num_citations": "12\n", "authors": ["1590"]}
{"title": "Behavioral and Coinductive Rewriting (invited talk)\n", "abstract": " Behavioral rewriting differs from standard rewriting in taking account of the weaker inference rules of behavioral logic, but it shares much with standard rewriting, including notions like termination and confluence. We describe an efficient implementation of behavioral rewriting that uses standard rewriting. Circular coinductive rewriting combines behavioral rewriting with circular coinduction, giving a surprisingly powerful proof method for behavioral properties; it is implemented in the BOBJ system, which is used in our examples. These include several lazy functional stream program equivalences and a behavioral refinement.", "num_citations": "12\n", "authors": ["1590"]}
{"title": "Runtime verification-17 years later\n", "abstract": " Runtime verification is the discipline of analyzing program executions using rigorous methods. The discipline covers such topics as specification-based monitoring, where single executions are checked against formal specifications; predictive runtime analysis, where properties about a system are predicted/inferred from single (good) executions; specification mining from execution traces; visualization of execution traces; and to be fully general: computation of any interesting information from execution traces. Finally, runtime verification also includes fault protection, where monitors actively protect a running system against errors. The paper is written as a response to the \u2018Test of Time Award\u2019 attributed to the authors for their 2001 paper [45]. The present paper provides a brief overview of what lead to that paper, what has happened since, and some perspectives on the future of the field.", "num_citations": "11\n", "authors": ["1590"]}
{"title": "Language definitions as rewrite theories\n", "abstract": " is a formal framework for defining the operational semantics of programming languages. It includes software tools for compiling language definitions to Maude rewrite theories, for executing programs in the defined languages based on the Maude rewriting engine, and for analyzing programs by adapting various Maude analysis tools. A recent extension to the tool suite is an automatic transformation of language definitions that enables the symbolic execution of programs, i.e., the execution of programs with symbolic inputs. In this paper we investigate the theoretical relationships between language definitions and their translations to Maude, between symbolic extensions of definitions and their Maude encodings, and how the relations between definitions and their symbolic extensions are reflected on their respective representations in Maude. These results show, in particular, how\u00a0\u2026", "num_citations": "11\n", "authors": ["1590"]}
{"title": "Scalable parametric runtime monitoring\n", "abstract": " Runtime monitoring is an effective means to improve the reliability of systems. In recent years, parametric monitoring, which is highly suitable for object-oriented systems, has gained significant traction.  Previous work on the performance of parametric runtime monitoring has focused on the performance of monitoring only one specification at a time.  A realistic system, however, has numerous properties that need to be monitored simultaneously.  This paper introduces scalable techniques to improve the performance of one of the fastest parametric monitoring systems, JavaMOP, in the presence of multiple simultaneous properties, resulting in average runtime overheads that are less than the summation of the overheads of the properties run in isolation.  An extensive evaluation shows that these techniques, which were derived following a thorough investigation and analysis of the current bottlenecks in JavaMOP, improve its runtime performance in the presence of multiple properties by up to two times and the memory usage by 34%", "num_citations": "11\n", "authors": ["1590"]}
{"title": "Towards semantics-based WCET analysis\n", "abstract": " Ideally, program analysis tools should be based on rigorous semantics of the employed programming languages. Unfortunately, giving a formal semantics (using conventional approaches) to a real language is a non-trivial matter; moreover, even when a semantics is available, it is often not easy to use it for program analysis. Recent research in rewriting logic semantics and in tool development based on such semantics [13, 3] shows encouraging results with respect to both expressiveness and scalability. Moreover, the application of these techniques in the context of real-world low-level languages such as Verilog [9] gives us hope that the theoretically ideal semantics-based approach to program analysis may be, after all, also practically feasible. We propose a general methodology for worst-case execution time (WCET) analysis centered around a formal executable semantics of the underlying language. We assert that the formal definition of a language has all the necessary information to be used for WCET program analysis and verification. We use the K rewrite-based semantic framework [13, 3] to define a formal executable semantics for a RISC assembly language, namely an integer restricted fragment of Simplescalar [1]. With it, we can take C programs and \u201cexecute\u201d them semantically as follows: first compile them into executables, then extract assembly programs from them using the Simplescalar disassembler [1], then execute the resulting assembly programs in our K semantics. The choice of the Simplescalar toolset is inspired from [8]. K is highly-modular, allowing us to start with a high-level semantics of the language and then plugging in\u00a0\u2026", "num_citations": "11\n", "authors": ["1590"]}
{"title": "Automating coinduction with case analysis\n", "abstract": " Coinduction is a major technique employed to prove behavioral properties of systems, such as behavioral equivalence. Its automation is highly desirable, despite the fact that most behavioral problems are -complete. Circular coinduction, which is at the core of the CIRC prover, automates coinduction by systematically deriving new goals and proving existing ones until, hopefully, all goals are proved. Motivated by practical examples, circular coinduction and CIRC have been recently extended with several features, such as special contexts, generalization and simplification. Unfortunately, none of these extensions eliminates the need for case analysis and, consequently, there are still many natural behavioral properties that CIRC cannot prove automatically. This paper presents an extension of circular coinduction with case analysis constructs and reasoning, as well as its implementation in CIRC. To\u00a0\u2026", "num_citations": "11\n", "authors": ["1590"]}
{"title": "A language-independent program verification framework\n", "abstract": " This invited paper describes an approach to language-independent deductive verification using the  semantics framework, in which an operational semantics of a language is defined and a program verifier together with other language tools are generated automatically, correct-by-construction.", "num_citations": "10\n", "authors": ["1590"]}
{"title": "Term-generic logic\n", "abstract": " Term-generic logic (TGL) is a first-order logic parameterized with terms defined axiomatically (rather than constructively), by requiring them to only provide generic notions of free variable and substitution satisfying reasonable properties. TGL has a complete Gentzen system generalizing that of first-order logic. A certain fragment of TGL, called Horn                 2, possesses a much simpler Gentzen system, similar to traditional typing derivation systems of \u03bb-calculi. Horn                 2 appears to be sufficient for defining a whole plethora of \u03bb-calculi as theories inside the logic. Within intuitionistic TGL, a Horn                 2 specification of a calculus is likely to be adequate by default. A bit of extra effort shows adequacy w.r.t. classic TGL as well, endowing the calculus with a complete loose semantics.", "num_citations": "10\n", "authors": ["1590"]}
{"title": "Mining parametric state-based specifications from executions\n", "abstract": " This paper presents an approach to mine parametric state-based specifications from execution traces, which can involve multiple components. We first discuss a general framework for mining parametric properties from execution traces, which allows one to apply non-parametric mining algorithms to infer parametric specifications without any modification. Then we propose a novel mining algorithm that extends the Probabilistic Finite State Automata (PFSA) approach to infer finite automata that describe system behaviors concisely and precisely from successful executions. The presented technique has been implemented in a prototype tool for Java, called jMiner, which has been applied to a number of real-life programs, including Java library classes and popular open source packages. Our experiments generated many meaningful specifications and revealed problematic behaviors in some programs, showing the effectiveness of our approach.", "num_citations": "10\n", "authors": ["1590"]}
{"title": "Formal Approaches to Software Testing and Runtime Verification: First Combined International Workshops FATES 2006 and RV 2006, Seattle, WA, USA, August 15-16, 2006, Revised\u00a0\u2026\n", "abstract": " Software validation is one of the most cost-intensive tasks in modern software production processes. The objective of FATES/RV 2006 was to bring scientists from both academia and industry together to discuss formal approaches to test and analyze programs and monitor and guide their executions. Formal approaches to test may cover techniques from areas like theorem proving, model checking, constraint resolution, static program analysis, abstract interpretation, Markov chains, and various others. Formal approaches to runtime verification use formal techniques to improve traditional ad-hoc monitoring techniques used in testing, debugging, performance monitoring, fault protection, etc. The FATES/RV 2006 workshop selected 14 high-quality papers out of 31 submissions. Each paper underwent at least three anonymous reviews by either PC members or external reviewers selected by them. In addition to the 14\u00a0\u2026", "num_citations": "10\n", "authors": ["1590"]}
{"title": "Axiomatizability in inclusive equational logics\n", "abstract": " A categorical framework for equational logics is presented, together with axiomatizability results in the style of Birkhoff. The distinctive categorical structures used are inclusion systems, which are an alternative to factorization systems in which factorization is required to be unique rather than unique \u2018up to an isomorphism\u2019. In this framework, models are any objects, and equations are special epimorphisms in [Cfr ], while satisfaction is injectivity. A first result says that equations-as-epimorphisms define exactly the quasi-varieties, suggesting that epimorphisms actually represent conditional equations. In fact, it is shown that the projectivity/freeness of the domain of epimorphisms is what makes the difference between unconditional and conditional equations, the first defining the varieties, as expected. An abstract version of the axiom of choice seems to be sufficient for free objects to be projective, in which case the\u00a0\u2026", "num_citations": "10\n", "authors": ["1590"]}
{"title": "Interpreting abstract interpretations in membership equational logic\n", "abstract": " We present a logical framework in which abstract interpretations can be naturally specified and then verified. Our approach is based on membership equational logic which extends equational logics by membership axioms, asserting that a term has a certain sort. We represent an abstract interpretation as a membership equational logic specification, usually as an overloaded order-sorted signature with membership axioms. It turns out that, for any term, its least sort over this specification corresponds to its most concrete abstract value. Maude implements membership equational logic and provides mechanisms to calculate the least sort of a term efficiently. We first show how Maude can be used to get prototyping of abstract interpretations \u201cfor free.\u201d Building on the meta-logic facilities of Maude, we further develop a tool that automatically checks an abstract interpretation against a set of user-defined properties. This can\u00a0\u2026", "num_citations": "10\n", "authors": ["1590"]}
{"title": "The institution of order-sorted equational logic\n", "abstract": " The paper provides an organisation of order-sorted equational logic as an Institution. 1 Introduction It is well known that Institution are a very strong tool for abstract model theory. Among many other models, order-sorted logic proved itself as a particularily useful formalism for logic-based programming. It is, therefore, important to include it into a general frame, where powerful methods are available. This paper organizes order-sorted equational logic as an Institution. For all the necesary background from category theory and many-sorted algebra, the reader is referred to [2]. 2 Preliminaries In the following we will present the basic conventions and notations we are dealing with. Let S be a\" sort set\". An S-sorted set A is just a family of sets A s for every sort s 2 S; we will write fA sjs 2 Sg. Similarly, given S-sorted sets A and B, an S-sorted function f: A! B is an S-sorted family ff s: A s! bsjs 2 Sg. If f: A! B and g: B! C are two S-sorted functions then...", "num_citations": "10\n", "authors": ["1590"]}
{"title": "A general approach to define binders using matching logic\n", "abstract": " We propose a novel definition of binders using matching logic, where the binding behavior of object-level binders is directly inherited from the built-in exists binder of matching logic. We show that the behavior of binders in various logical systems such as lambda-calculus, System F, pi-calculus, pure type systems, can be axiomatically defined in matching logic as notations and logical theories. We show the correctness of our definitions by proving conservative extension theorems, which state that a sequent/judgment is provable in the original system if and only if it is provable in matching logic, in the corresponding theory. Our matching logic definition of binders also yields models to all binders, which are deductively complete with respect to formal reasoning in the original systems. For lambda-calculus, we further show that the yielded models are representationally complete, a desired property that is not enjoyed by\u00a0\u2026", "num_citations": "9\n", "authors": ["1590"]}
{"title": "Towards a verified model of the Algorand consensus protocol in Coq\n", "abstract": " The Algorand blockchain is a secure and decentralized public ledger based on pure proof of stake rather than proof of work. At its core it is a novel consensus protocol with exactly one block certified in each round: that is, the protocol guarantees that the blockchain does not fork. In this paper, we report on our effort to model and formally verify the Algorand consensus protocol in the Coq proof assistant. Similar to previous consensus protocol verification efforts, we model the protocol as a state transition system and reason over reachable global states. However, in contrast to previous work, our model explicitly incorporates timing issues (e.g., timeouts and network delays) and adversarial actions, reflecting a more realistic environment faced by a public blockchain. Thus far, we have proved asynchronous safety of the protocol: two different blocks cannot be certified in the same round, even when the adversary has\u00a0\u2026", "num_citations": "9\n", "authors": ["1590"]}
{"title": "On safety properties and their monitoring\n", "abstract": " Various definitions of safety properties as sets of execution traces have been introduced in the literature, some over finite traces, others over infinite traces, yet others over both finite and infinite traces. By employing cardinality arguments, this paper first shows that these notions of safety are ultimately equivalent, by showing each of them to have the cardinal of the continuum. It is then shown that all safety properties can be characterized as ``always past'' properties, and then that the problem of monitoring a safety property can be arbitrarily hard. Finally, two decidable specification formalisms for safety properties are discussed, namely extended regular expressions and past time LTL. It is shown that monitoring the former requires non-elementary space. An optimal monitor synthesis algorithm is given for the latter; the generated monitors run in space linear with the number of temporal operators and in time linear with the size of the formula.", "num_citations": "9\n", "authors": ["1590"]}
{"title": "A semantic approach to interpolation\n", "abstract": " Interpolation results are investigated for various types of formulae. By shifting the focus from syntactic to semantic interpolation, we generate, prove and classify a series of interpolation results for first-order logic. A few of these results non-trivially generalize known interpolation results. All the others are new.", "num_citations": "9\n", "authors": ["1590"]}
{"title": "Predicting concurrency errors at runtime using sliced causality\n", "abstract": " A runtime analysis technique is presented, which can predict errors in multi-threaded systems by examining event traces generated by executions of these systems even when they are successful. The technique is based on a novel partial order relation on relevant events, called sliced causality, which loosens the obvious but strict ``happens-before'' relation by considering static structural information about the multi-threaded program, such as control-flow and data-flow dependence, and dynamic synchronization information, such as lock-sets. A vector clock based algorithm to encode the sliced causality is given, together with a procedure for generating all potential runs that are consistent with this partial order in a memory effective way. Then violations of properties can be ``predicted'' by running the corresponding monitor against potential runs that are consistent with the observed execution, i.e., permutations of (abstract) events that do not violate the sliced causal partial order. The monitors can be manually implemented or automatically synthesized from the desired properties, which can be given in any formalism that allows monitor synthesis algorithms. Our runtime analysis technique is sound, in the sense that it reports no false alarms. As expected, it is not complete; indeed, it cannot say anything about code that was not reached during the observed execution. A prototype system, called jPredictor, has been implemented and evaluated on several Java applications with promising results.", "num_citations": "9\n", "authors": ["1590"]}
{"title": "Monitoring-oriented programming: A tool-supported methodology for higher quality object-oriented software\n", "abstract": " This paper presents a tool-supported methodological paradigm for object-oriented software development, called monitoring-oriented programming and abbreviated MOP, in which runtime monitoring is a basic software design principle. The general idea underlying MOP is that software developers insert specifications in their code via annotations. Actual monitoring code is automatically synthesized from these annotations before compilation and integrated at appropriate places in the program, according to user-defined configuration attributes. This way, the specification is checked at runtime against the implementation. Moreover, violations and/or validations of specifications can trigger user-defined code at any points in the program, in particular recovery code, outputting or sending messages, or raising exceptions.   The MOP paradigm does not promote or enforce any specific formalism to specify requirements: it allows the users to plug-in their favorite or domain-specific specification formalisms via logic plug-in modules. There are two major technical challenges that MOP supporting tools unavoidably face: monitor synthesis and monitor integration. The former is heavily dependent on the specification formalism and comes as part of the corresponding logic plug-in, while the latter is uniform for all specification formalisms and depends only on the target programming language. An experimental prototype tool, called Java-MOP, is also discussed, which currently supports most but not all of the desired MOP features. MOP aims at reducing the gap between formal specification and implementation, by integrating the two and allowing them together to\u00a0\u2026", "num_citations": "9\n", "authors": ["1590"]}
{"title": "Towards behavioral Maude: Behavioral membership equational logic\n", "abstract": " How can algebraic and coalgebraic specifications be integrated? How can behavioral equivalence be addressed in an algebraic specification language? The hidden-sorted approach, originating in work of Goguen and Meseguer in the early 80's, and further developed into the hidden-sorted logic approach by researchers at Oxford, UC San Diego, and Kanazawa offers some attractive answers, and has been implemented in both BOBJ and CafeOBJ. In this work we investigate both further extensions of hidden logic, and an extension of the Maude specification language called BMaude supporting this extended hidden-sorted semantics.Maude's underlying equational logic, membership equational logic, generalizes and increases the expressive power of many-sorted and order-sorted equational logics. We develop a hidden-sorted extension of membership equational logic, and give conditions under which theories\u00a0\u2026", "num_citations": "9\n", "authors": ["1590"]}
{"title": "End-to-end formal verification of ethereum 2.0 deposit smart contract\n", "abstract": " We report our experience in the formal verification of the deposit smart contract, whose correctness is critical for the security of Ethereum 2.0, a new Proof-of-Stake protocol for the Ethereum blockchain. The deposit contract implements an incremental Merkle tree algorithm whose correctness is highly nontrivial, and had not been proved before. We have verified the correctness of the compiled bytecode of the deposit contract to avoid the need to trust the underlying compiler. We found several critical issues of the deposit contract during the verification process, some of which were due to subtle hidden bugs of the compiler.", "num_citations": "8\n", "authors": ["1590"]}
{"title": "Maximizing concurrency bug detection in multithreaded software programs\n", "abstract": " Disclosed systems and methods incorporate a sound and maximal causal model with control flow information for maximum concurrency error detection in general multithreaded programs. The maximal causal model may be based on or integrated with the sequential consistency model, and form the basis for a formula including branch and order variables as first-order logical constraints solvable by an SMT solver for detection or prediction of concurrency errors. The disclosed systems and methods also relate to predictive trace analysis (PTA) for predicting generic concurrency properties using local traces (as opposed to a global trace) through the threads of a multithreaded program. By uniformly modeling violations of concurrency properties and the thread causality as constraints over events, and using an SMT solver, the systems and methods predict property violations allowed by the causal model.", "num_citations": "8\n", "authors": ["1590"]}
{"title": "Matching logic: A logic for structural reasoning\n", "abstract": " Matching logic is a first-order logic (FOL) variant to reason about structure. Its sentences, called patterns, are constructed using variables, symbols, connectives and quantifiers, but no dif ference is made between function and predicate symbols. In models, a pattern evaluates into a power-set domain (the set of values that match it), in contrast to FOL where functions, predicates and connectives map into a domain. Matching logic generalizes several logical frameworks important for program analysis, such as: propositional logic, algebraic specification, FOL with equality, and separation logic. Patterns allow for specifying separation requirements at any level in any program configuration, not only in the heaps or stores, without any special logical constructs for that: the very nature of pattern matching is that if two structures are matched as part of a pattern, then they can only be spatially separated. Like FOL, matching logic can also be translated into pure predicate logic with equality, but it also admits its own sound and complete proof system.", "num_citations": "8\n", "authors": ["1590"]}
{"title": "A semantic approach to interpolation\n", "abstract": " Craig interpolation is investigated for various types of formulae. By shifting the focus from syntactic to semantic interpolation, we generate, prove and classify a series of interpolation results for first-order logic. A few of these results non-trivially generalize known interpolation results; all the others are new. We also discuss some applications of our results to the theory of institutions and of algebraic specifications, and a Craig\u2013Robinson version of these results.", "num_citations": "8\n", "authors": ["1590"]}
{"title": "Matching Logic-Extended Report\n", "abstract": " Hoare logics rely on the fact that logic formulae can encode, or specify, program states, including environments, stacks, heaps, path conditions, data constraints, and so on. Such formula encodings tend to lose the structure of the original program state and thus to be complex in practice, making it difficult to relate formal systems and program correctness proofs to the original programming language and program, respectively. Worse, since programs often manipulate mathematical objects such as lists, trees, graphs, etc., one needs to also encode, as logical formulae, the process of identifying these objects in the encoded program state. This paper proposes matching logic, an alternative to Hoare logics in which the state structure plays a crucial role. Program states are represented as algebraic datatypes called (concrete) configurations, and program state specifications are represented as configuration terms with variables and constraints on them, called (configuration) patterns. A pattern specifies those configurations thatmatch it. Patterns can bind variables to their scope, allowing both for pattern abstraction and for expressing loop invariants. Matching logic is tightly connected to rewriting logic semantics (RLS): matching logic formal systems can systematically be obtained from executable RLS of languages. This relationship allows to prove soundness of matching logic formal systems w.r.t. complementary, testable semantics. All notions are exemplified using KernelC, a fragment of C with dynamic memory allocation/deallocation.", "num_citations": "8\n", "authors": ["1590"]}
{"title": "Pluggable Policies for C\n", "abstract": " Many programs make implicit assumptions about data. Common assumptions include whether a variable has been initialized or can only contain non-null references. Domain-specific examples are also common, with many scientific programs manipulating values with implicit units of measurement. However, in languages like C, there is no language facility for representing these assumptions, making violations of these implicit program policies challenging to detect. In this paper, we present a framework for pluggable policies for the C language. The core of the framework is a shared rewrite-logic semantics of C designed for symbolic execution of C programs, and an annotation engine allowing for annotations across multiple policies to be added to C programs. Policies are created by providing a policy specific language, usable in annotations, and policy specific values and semantics. This provides a method to quickly develop new policies, taking advantage of the existing framework components. To illustrate the use of the framework, two case studies in policy development are presented: a basic null pointer analysis, and a more comprehensive analysis to verify unit of measurement safety.", "num_citations": "8\n", "authors": ["1590"]}
{"title": "A rewriting approach to the design and evolution of object-oriented languages\n", "abstract": " Rewriting logic semantics provides an environment for defining new and existing languages. These language definitions are formal and executable, providing language interpreters almost for free while also providing a framework for building analysis tools, such as type checkers, model checkers, and abstract interpreters. Large subsets of several existing object-oriented languages have been defined, while a new research language, KOOL, has been created as a platform for experimenting with language features and type systems. At the same time, new tools and formalisms aimed specifically at programming languages are being developed.", "num_citations": "8\n", "authors": ["1590"]}
{"title": "Rewriting logic systems\n", "abstract": " We present an overview of rewriting-based systems that were presented at the workshop.", "num_citations": "8\n", "authors": ["1590"]}
{"title": "A Rewrite Logic Approach to Semantic Definition, Design and Analysis of Object-Oriented Languages\n", "abstract": " This paper introduces a framework for rapid prototyping of object oriented programming languages and corresponding analysis tools. It is based on formal definitions of language features in rewrite logic, a simple and intuitive logic for concurrency with powerful tool support. A domain-specific front-end consisting of a notation and a technique, called K, allows for compact, modular, expressive and easy to understand and change definitions of language features. The framework is illustrated by first defining KOOL, an experimental concurrent object-oriented language with exceptions, and then by discussing the definition of JAVA. Generic rewrite logic tools, such as efficient rewrite engines and model checkers, can be used on language definitions and yield interpreters and corresponding formal program analyzers at no additional cost.", "num_citations": "8\n", "authors": ["1590"]}
{"title": "KOOL: A K-based Object-Oriented Language\n", "abstract": " This paper documents KOOL, a dynamic, object-oriented language designed using the K framework. The KOOL language includes many features available in mainstream object-oriented languages, including such features as runtime type inspection and exceptions. Since the language is currently changing, this should be seen as a snapshot of the language at this point in time.", "num_citations": "8\n", "authors": ["1590"]}
{"title": "Towards a unified proof framework for automated fixpoint reasoning using matching logic\n", "abstract": " Automation of fixpoint reasoning has been extensively studied for various mathematical structures, logical formalisms, and computational domains, resulting in specialized fixpoint provers for heaps, for streams, for term algebras, for temporal properties, for program correctness, and for many other formal systems and inductive and coinductive properties. However, in spite of great theoretical and practical interest, there is no unified framework for automated fixpoint reasoning. Although several attempts have been made, there is no evidence that such a unified framework is possible, or practical. In this paper, we propose a candidate based on matching logic, a formalism recently shown to theoretically unify the above mentioned formal systems. Unfortunately, the (Knaster-Tarski) proof rule of matching logic, which enables inductive reasoning, is not syntax-driven. Worse, it can be applied at any step during a proof\u00a0\u2026", "num_citations": "7\n", "authors": ["1590"]}
{"title": "Applicative matching logic\n", "abstract": " This paper proposes a logic for programming languages, which is both simple and expressive, to serve as a foundation for language semantics frameworks.  Matching mu-logic has been recently proposed as a unifying foundation for programming languages, specification and verification.  It has been shown to capture several logics important for programming languages, including first-order logic with least fixpoints, separation logic, temporal logics, modal mu-logic, and importantly, reachability logic, a language-independent logic for program verification that subsumes Hoare logic.  This paper identifies a fragment of matching mu-logic called applicative matching logic (AML), which is much simpler and thus more appealing from a foundational perspective, yet as expressive as matching mu-logic. Several additional logical frameworks fundamental for programming languages are shown to be faithfully captured by AML, including many- and order-sorted algebras, lambda-calculus, (dependent) type systems, evaluation contexts, and rewriting.  Finally, it is shown how all these make AML an appropriate underlying logic foundation for complex language semantics frameworks, such as K.", "num_citations": "7\n", "authors": ["1590"]}
{"title": "RV-ECU: Maximum Assurance In-Vehicle Safety Monitoring\n", "abstract": " The Runtime Verification ECU (RV-ECU) is a new development platform for checking and enforcing the safety of automotive bus communications and software systems. RV-ECU uses runtime verification, a formal analysis subfield geared at validating and verifying systems as they run, to ensure that all manufacturer and third-party safety specifications are complied with during the operation of the vehicle. By compiling formal safety properties into code using a certifying compiler, the RV-ECU executes only provably correct code that checks for safety violations as the system runs. RV-ECU can also recover from violations of these properties, either by itself in simple cases or together with safe message-sending libraries implementable on third-party control units on the bus. RV-ECU can be updated with new specifications after a vehicle is released, enhancing the safety of vehicles that have already been sold and\u00a0\u2026", "num_citations": "7\n", "authors": ["1590"]}
{"title": "Matching logic rewriting: Unifying operational and axiomatic semantics in a practical and generic framework\n", "abstract": " Matching logic allows to specify structural properties about program configurations by means of special formulae, called patterns, and to reason about them by means of pattern matching. This paper proposes rewriting over matching logic formulae, which generalizes both term rewriting and Hoare triples, as a unified framework for operational semantics and for program verification. A programming language is formally defined as a set of rewrite rules.  A language-independent nine-rule proof system then can be used either to derive any operational program behavior, or to verify programs against arbitrary properties specified also as rewrite rules, thus reducing the gap between operational semantics and axiomatic semantics to zero. The proof system is proved both sound and complete for operational semantics and partially correct for program verification. All these proofs are language-independent. A matching logic verifier for a fragment of C, called MatchC, has been implemented and evaluated with encouraging results on a series of non-trivial programs, attesting to the practicality of the approach.", "num_citations": "7\n", "authors": ["1590"]}
{"title": "From rewriting logic executable semantics to matching logic program verification\n", "abstract": " Rewriting logic semantics (RLS) is a definitional framework in which a programming language is defined as a rewrite theory: the algebraic signature defines the program configurations, the equations define structural identities on configurations, and the rewrite rules define the irreversible computational steps. RLS language definitions are efficiently executable using conventional rewrite engines, yielding interpreters for the defined languages for free.  Matching logic is a program verification logic inspired by RLS. Matching logic specifications are particular first-order formulae with constrained algebraic structure, called patterns. Configurations satisfy patterns iff they match their algebraic structure and satisfy their constraints. Patterns can naturally specify data separation and require no special support from the underlying logic.  Using HIMP, a C-like language with dynamic memory allocation/deallocation and pointer arithmetic, this paper shows how one can derive an executable matching logic verifier from HIMP\u2019s RLS. It is shown that the derived verifier is sound, that is every verified formula holds in the original, complementary RLS of HIMP, and complete, that is every verified formula is provable using HIMP\u2019s sound matching logic proof system. In passing, this paper also shows that, for the restriction of HIMP without a heap called IMP for which one can give a conventional Hoare logic proof system, a restricted use of the matching logic proof system is equivalent to the Hoare logic proof system, in that any proof derived using any of the proof systems can be turned into a proof using the other. The encoding from Hoare logic into matching logic is\u00a0\u2026", "num_citations": "7\n", "authors": ["1590"]}
{"title": "Parametric Trace Slicing and Monitoring\n", "abstract": " Trace analysis plays a fundamental role in many program analysis approaches, such as runtime verification, testing, monitoring, and specification mining. Recent research efforts bring empirical evidence that execution traces are frequently comprised of many meaningful trace slices merged together, each slice corresponding to instances of relevant parameters. Several current trace analysis techniques and systems allow the specification of parametric properties, and the analysis of execution traces with respect to each instance of the parameters. However, the current solutions have limitations: some in the specification formalism, others in the type of trace they support; moreover, they share common notions, intuitions, even techniques and algorithms, suggesting that a fundamental study and understanding of parametric trace analysis is needed.  This foundational paper gives the first solution to parametric trace analysis that is unrestricted by the type of parametric property or trace that can be analyzed. First, a general purpose parametric trace slicing technique is discussed, which takes each event in the parametric trace and distributes it to its corresponding trace slices. This parametric trace slicing technique can be used in combination with any conventional, non-parametric trace analysis technique, by applying the later on each trace slice. As an instance, a parametric property monitoring technique is then presented, which processes each trace slice online. Thanks to the generality of parametric trace slicing, the parametric property monitoring technique reduces to encapsulating and indexing unrestricted and well-understood non-parametric\u00a0\u2026", "num_citations": "7\n", "authors": ["1590"]}
{"title": "A Rewriting Logic Approach to Operational Semantics\u2013Extended Abstract\n", "abstract": " This paper shows how rewriting logic semantics (RLS) can be used as a computational logic framework for operational semantic definitions of programming languages. Several operational semantics styles are addressed: big-step and small-step structural operational semantics (SOS), modular SOS, reduction semantics with evaluation contexts, and continuation-based semantics. Each of these language definitional styles can be faithfully captured as an RLS theory, in the sense that there is a one-to-one correspondence between computational steps in the original language definition and computational steps in the corresponding RLS theory. A major goal of this paper is to show that RLS does not force or pre-impose any given language definitional style, and that its flexibility and ease of use makes RLS an appealing framework for exploring new definitional styles.", "num_citations": "7\n", "authors": ["1590"]}
{"title": "Mop: Reliable software development using abstract aspects\n", "abstract": " Monitoring-Oriented Programming (MOP) is a formal framework for software development and analysis. It aims at reducing the gap between formal specification and implementation via runtime monitoring. In MOP, the developer specifies desired properties using definable specification formalisms, along with code to execute when properties are violated or validated, which can be used not only to report, but especially to recover from errors. The MOP framework automatically generates monitors from the specified properties and then integrates them together with the recovery code into the original system. Since the recovery code typically is executed infrequently and can be validated more easily than the actual system, MOP is expected to increase software reliability at little amortized runtime overhead. This paper presents MOP from a pragmatic, rather than foundational perspective, as an instance of aspect-oriented programming (AOP) where one defines abstract aspects using logics; one is relieved from providing unnecessary implementation details, because these are generated and integrated automatically. Existing AOP tools provide crucial support: an MOP frontend for Java, called JavaMOP and also discussed in the paper, is implemented using AspectJ. A series of examples illustrate the strengths of MOP from different perspectives.", "num_citations": "7\n", "authors": ["1590"]}
{"title": "The AutoBayes Program Synthesis System| System Description|\n", "abstract": " AUTOBAYES is a fully automatic program synthesis system for the statistical data analysis domain. Its input is a concise description of a data analysis problem in the form of a statistical model; its output is optimized and fully documented C/C++ code which can be linked dynamically into the Matlab and Octave environments. AUTOBAYES synthesizes code by a schema-guided deductive process. Schemas (i.e., code templates with associated semantic constraints) are applied to the original problem and recursively to emerging subproblems. AUTOBAYES complements this approach by symbolic computation to derive closed-form solutions whenever possible. In this paper, we concentrate on the interaction between the symbolic computations and the deductive synthesis process. A statistical model specifies for each problem variable (i.e., data or parameter) its properties and dependencies in the form of a probability distribution, A typical data analysis task is to estimate the best possible parameter values from the given observations or measurements. The following example models normal-distributed data but takes prior information (e.g., from previous experiments) on the data's mean value and variance into account.", "num_citations": "7\n", "authors": ["1590"]}
{"title": "Matching logic explained\n", "abstract": " Matching logic was recently proposed as a unifying logic for specifying and reasoning about static structure and dynamic behavior of programs. In matching logic, patterns and specifications are used to uniformly represent mathematical domains (such as numbers and Boolean values), datatypes, and transition systems, whose properties can be reasoned about using one fixed matching logic proof system. In this paper we give a tutorial of matching logic. We use a suite of examples to explain the basic concepts of matching logic and show how to capture many important mathematical domains, datatypes, and transition systems using patterns and specifications. We put emphasis on the general principles of induction and coinduction in matching logic and show how to do inductive and coinductive reasoning about datatypes and codatatypes. To encourage the future tools development for matching logic, we propose\u00a0\u2026", "num_citations": "6\n", "authors": ["1590"]}
{"title": "All-path reachability logic\n", "abstract": " This paper presents a language-independent proof system for reachability properties of programs written in non-deterministic (e.g., concurrent) languages, referred to as all-path reachability logic. It derives partial-correctness properties with all-path semantics (a state satisfying a given precondition reaches states satisfying a given postcondition on all terminating execution paths). The proof system takes as axioms any unconditional operational semantics, and is sound (partially correct) and (relatively) complete, independent of the object language. The soundness has also been mechanized in Coq. This approach is implemented in a tool for semantics-based verification as part of the K framework (http://kframework.org)", "num_citations": "6\n", "authors": ["1590"]}
{"title": "Application Assurance for Open Platform In-Vehicle Infotainment System\n", "abstract": " The disclosure includes a system, method and tangible memory for providing application assurance for an open platform in-vehicle infotainment system. The system includes an open platform in-vehicle infotainment system including a tangible memory, a processor and an untrusted application including a monitor module. The untrusted application is stored in the tangible memory. The monitor module included in the untrusted application is generated based on a set of one or more runtime rules describing desired behavior of the untrusted application that complies with a specification for an application programming interface of the open platform in-vehicle infotainment system. The untrusted application runs on the open platform in-vehicle infotainment system. The monitor module causes the processor to observe, based on the one or more runtime rules, behavior of the untrusted application to determine whether the\u00a0\u2026", "num_citations": "6\n", "authors": ["1590"]}
{"title": "Runtime verification at work: A tutorial\n", "abstract": " We present a suite of runtime verification tools developed by Runtime Verification Inc.: RV-Match, RV-Predict, and RV-Monitor. RV-Match is a tool for checking C programs for undefined behavior and other common programmer mistakes. It is extracted from the most complete formal semantics of the C11 language and beats many similar tools in its ability to catch a broad range of undesirable behaviors. RV-Predict is a dynamic data race detector for Java and C/C++ programs. It is perhaps the only tool that is both sound and maximal: it only reports real races and it can find all races that can be found by any other sound data race detector analyzing the same execution trace. RV-Monitor is a runtime monitoring tool that checks and enforces safety and security properties during program execution. Our tools focus on reporting no false positives and are free for non-commercial use.", "num_citations": "6\n", "authors": ["1590"]}
{"title": "Rewriting Logic and Its Applications: 10th International Workshop, WRLA 2014, Held as a Satellite Event of ETAPS, Grenoble, France, April 5-6, 2014, Revised Selected Papers\n", "abstract": " This book constitutes the thoroughly refereed post-workshop proceedings of the 10th International Workshop on Rewriting Logic and its Applications, WRLA 2014, held as a satellite event of ETAPS 2014, in Grenoble, France, in March 2014. The 13 revised full papers presented together with 3 invited papers were carefully reviewed and selected from 21 submissions. The papers address a great diversity of topics in the fields of foundations and models of RL; languages based on RL; RL as a logical framework; RL as a semantic framework; use of RL to provide rigorous support for model-based software engineering; formalisms related to RL; verification techniques for RL specifications; comparisons of RL with existing formalisms having analogous aims; application of RL to specification and analysis of distributed systems and physical systems.", "num_citations": "6\n", "authors": ["1590"]}
{"title": "Specifying languages and verifying programs with k\n", "abstract": " K is a rewrite-based executable semantic framework for defining languages. The K framework is designed to allow implementing a variety of generic tools that can be used with any language defined in K, such as parsers, interpreters, symbolic execution engines, semantic debuggers, test-case generators, state-space explorers, model checkers, and even deductive program verifiers. The latter are based on matching logic for expressing static properties, and on reachability logic for expressing dynamic properties. Several large languages have been already defined or are being defined in K, including C, Java, Python, Javascript, and LLVM.", "num_citations": "6\n", "authors": ["1590"]}
{"title": "A rewriting logic approach to defining type systems\n", "abstract": " We show how programming language semantics and definitions of their corresponding type systems can both be written in a single framework amenable to proofs of soundness. The framework is based on full rewriting logic (not to be confused with context reduction or term rewriting), where rules can match anywhere in a term (or configuration).  We present an extension of the syntactic approach to proving type system soundness presented by Wright and Felleisen [1994] that works in the above described semantics-based domain. As before, the properties of preservation and progress are crucial. We use an abstraction function to relate semantic configurations in the language domain to semantic configurations in the type domain, and then proceed to use the preservation and progress properties as usual. We also develop an abstract type system, which is a type system modulo certain structural characteristics.  To demonstrate the method, we give examples of five languages and corresponding type systems. They include two imperative languages and three functional languages, and three type checkers and two type inferencers. We then proceed to prove that preservation holds for each.", "num_citations": "6\n", "authors": ["1590"]}
{"title": "Inductive behavioral proofs by unhiding\n", "abstract": " We show that for any behavioral \u03a3-specification \u00df there is an ordinary algebraic specification \u00df\u0303 over a larger signature, such that a model behaviorally satisfies \u00df iff it satisfies, in the ordinary sense, the \u2211-theorems of \u00df\u0303. The idea is to add machinery for contexts and experiments (sorts, operations and equations), use it, and then hide it. We develop a procedure, called unhiding, which takes a finite \u00df and produces a finite \u00df\u0303. The practical aspect of this procedure is that one can use any standard equational inductive theorem prover to derive behavioral theorems, even if neither equational reasoning nor induction is sound for behavioral satisfaction.", "num_citations": "6\n", "authors": ["1590"]}
{"title": "Abstract semantics for module composition\n", "abstract": " Technology is evolving unexpectedly fast. \u00cboftware requirements tend to be exponentially higher every year and thus huge software systems are needed. Failure-safe systems are required not only in armies or governmental institutions, but also in many branches of industry. It is not a new thing that most failures in big systems are due either to flaws in requirements and incipient formal or informal specifications, or to exceptions which are not treated in implementations. In this light, good specification languages and automatic code generators seem to be indispensable, and modularization is becoming a crucial methodology. Programmers and software engineers agree in unanimity that a useful characteristic of the programming languages they use for implementations (C++, Java, etc.) is their support for both public and private features (types, functions). The public features are often called interfaces. The private part is not visible outside the module (class, package) that declares it, but it can be used internally to define the visible part. \u00cbuch distinction helps software engineers abstract their work and ignore details which are a main source of confusion and errors.We claim that the distinction between private and public features might also be a desirable characteristic of formal specifications, not only from the practical point of view, because of the increased level of abstraction, but also because of at least two important theoretical reasons. One is the possibility to specify finitely some theories which do not admit finite standard presentations. For example, Bergstra and Tucker [2] showed that any recursive \u03a3-algebra can be specified as the \u03a3-restriction of\u00a0\u2026", "num_citations": "6\n", "authors": ["1590"]}
{"title": "Towards a Trustworthy Semantics-Based Language Framework via Proof Generation\n", "abstract": " We pursue the vision of an ideal language framework, where programming language designers only need to define the formal syntax and semantics of their languages, and all language tools are automatically generated by the framework. Due to the complexity of such a language framework, it is a big challenge to ensure its trustworthiness and to establish the correctness of the autogenerated language tools. In this paper, we propose an innovative approach based on proof generation. The key idea is to generate proof objects as correctness certificates for each individual task that the language tools conduct, on a case-by-case basis, and use a trustworthy proof checker to check the proof objects. This way, we avoid formally verifying the entire framework, which is practically impossible, and thus can make the language framework both practical and trustworthy. As a first step, we formalize program execution as mathematical proofs and generate their complete proof objects. The experimental result shows that the performance of our proof object generation and proof checking is very promising.", "num_citations": "5\n", "authors": ["1590"]}
{"title": "Verification of casper in the coq proof assistant\n", "abstract": " This report describes our effort to model and verify the Casper blockchain finality system in the Coq proof assistant. We outline the salient details on blockchain systems using Casper, describe previous verification efforts we used as a starting point, and give an overview of the formal definitions and properties proved. The Coq source files are available at: https://github.com/runtimeverification/casper-proofs", "num_citations": "5\n", "authors": ["1590"]}
{"title": "Formal design, implementation and verification of blockchain languages (invited talk)\n", "abstract": " This invited paper describes recent, ongoing and planned work on the use of the rewrite-based semantic framework K to formally design, implement and verify blockchain languages and virtual machines. Both academic and commercial endeavors are discussed, as well as thoughts and directions for future research and development.", "num_citations": "5\n", "authors": ["1590"]}
{"title": "A theoretical foundation for programming languages aggregation\n", "abstract": " Programming languages should be formally specified in order to reason about programs written in them. We show that, given two formally specified programming languages, it is possible to construct the formal semantics of an aggregated language, in which programs consist of pairs of programs from the initial languages. The construction is based on algebraic techniques and it can be used to reduce relational properties (such as equivalence of programs) to reachability properties (in the aggregated language).", "num_citations": "5\n", "authors": ["1590"]}
{"title": "Making Maude definitions more interactive\n", "abstract": " This paper presents an interface for achieving interactive executions of Maude terms by allowing console and file input/output\u00a0(I/O) operations. This interface consists of a Maude API for I/O operations, a Java-based server offering I/O capabilities, and a communication protocol between the two implemented using the external objects concept and Maude\u2019s TCP sockets. This interface was evaluated as part of the  framework, providing interactive interpreter capabilities for executing and testing programs for multiple language definitions.", "num_citations": "5\n", "authors": ["1590"]}
{"title": "Security models in rewriting logic for cryptographic protocols and browsers\n", "abstract": " This dissertation tackles crucial issues of web browser security. Web browsers are now a central part of the trusted code base of any end-user computer system, as more and more usage shifts to services provided by web sites that are accessed through those browsers. Towards this goal we identify three key aspects of web browser security:(i) the machine-to-user communication,(ii) internal browser security concerns and (iii) machine-to-machine communication.", "num_citations": "5\n", "authors": ["1590"]}
{"title": "Monitoring IVHM systems using a monitor-oriented programming framework\n", "abstract": " We describe a runtime verification approach to increase the safety of IVHM systems by an integration of TEAMS models and MOP (Monitor-Oriented Programming). The TEAMS model is used to automatically extract relevant runtime information from the controlled system by means of events. This information is passed on-line to the MOP engine, allowing to verify complex temporal properties and to discover running patterns which are of interest in detecting and preventing faulty behaviors.", "num_citations": "5\n", "authors": ["1590"]}
{"title": "Complete categorical deduction for satisfaction as injectivity\n", "abstract": " Birkhoff (quasi-)variety categorical axiomatizability results have fascinated many scientists by their elegance, simplicity and generality. The key factor leading to their generality is that equations, conditional or not, can be regarded as special morphisms or arrows in a special category, where their satisfaction becomes injectivity, a simple and abstract categorical concept. A natural and challenging next step is to investigate complete deduction within the same general and elegant framework. We present a categorical deduction system for equations as arrows and show that, under appropriate finiteness requirements, it is complete for satisfaction as injectivity. A straightforward instantiation of our results yields complete deduction for several equational logics, in which conditional equations can be derived as well at no additional cost, as opposed to the typical method using the theorems of constants and of\u00a0\u2026", "num_citations": "5\n", "authors": ["1590"]}
{"title": "Behavioral extensions of institutions\n", "abstract": " We show that any institution  satisfying some reasonable conditions can be transformed into another institution, , which captures formally and abstractly the intuitions of adding support for behavioral equivalence and reasoning to an existing, particular algebraic framework. We call our transformation an \u201cextension\u201d because  has the same sentences as  and because its entailment relation includes that of . Many properties of behavioral equivalence in concrete hidden logics follow as special cases of corresponding institutional results. As expected, the presented constructions and results can be instantiated to other logics satisfying our requirements as well, thus leading to novel behavioral logics, such as partial or infinitary ones, that have the desired properties.", "num_citations": "5\n", "authors": ["1590"]}
{"title": "An executable semantic definition of the Beta language using rewriting logic\n", "abstract": " In this paper, we present an overview of our method of specifying the semantics of programming languages using rewriting logic. This method, which we refer to as the \"continuation-based style\", relies on an explicit representation of a program's control context, allowing flexibility in defining complex, control-intensive features of languages while still allowing simple definitions of simple language constructs. To illustrate this technique, we present a definition of a significant subset of the object-oriented language Beta running in the Maude rewriting engine. This specification gives us an executable platform for running Beta programs and for experimenting with new language features. We illustrate this by extending the language with super calls. We also touch upon some features of the underlying framework, including the ability to model check Beta programs running on our framework with rewriting-based tools.", "num_citations": "5\n", "authors": ["1590"]}
{"title": "Proofs on safety for untrusted code\n", "abstract": " Proof-carrying code is a technique that can be used to execute untrusted code safely. A code consumer speci es requirements and safety rules which de ne the safe behavior of a system, and a code producer packages each program with a formal proof that the program satis es the requirements. The consumer uses a fast proof validator to check that the proof is correct, and hence the program is safe. In this report, we discuss applications for which proof-carrying code is appropriate, explain the mechanics of proof-carrying code, compare it with other techniques and suggest two research directions for the method. 1", "num_citations": "5\n", "authors": ["1590"]}
{"title": "Initial algebra semantics in matching logic\n", "abstract": " Matching logic is a unifying foundational logic for defining formal programming language semantics, which adopts a minimalist design with few primitive constructs that are enough to express all properties within a variety of logical systems, including FOL, separation logic, (dependent) type systems, modal mu-logic, and more. In this paper, we consider initial algebra semantics and show how to capture it by matching logic specifications. Formally, given an algebraic specification E that defines a set of sorts (of data) and a set of operations whose behaviors are defined by a set of equational axioms, we define a corresponding matching logic specification, denoted INITIALALGEBRA(E), whose models are exactly the initial algebras of E. Thus, we reduce initial E-algebra semantics to the matching logic specifications INITIALALGEBRA(E), and reduce extrinsic initial E-algebra reasoning, which includes inductive reasoning, to generic, intrinsic matching logic reasoning.", "num_citations": "4\n", "authors": ["1590"]}
{"title": "Systematic Concurrency Testing with Maximal Causality\n", "abstract": " We propose the first systematic concurrent program testing approach that is able to cover the entire scheduling space with a provably minimal number of test runs. Each run corresponds to a distinct maximal causal model extracted from a given execution trace, which captures the largest possible set of causally equivalent legal executions. The maximal causal models can be represented using first-order logic constraints, and testing all the executions comprised by a maximal causal model reduces to offline constraint solving. Based on the same constraint model, we also develop a schedule generation algorithm that iteratively generates new casually different schedules. The core idea is to systematically force previous read operations to read different values, thus enumerating all the causal models. We have implemented our approach in an explicit stateless model checker, and our eval- uation showed that our technique is able to 1) find concurrency bugs faster; 2) finish state space exploration with much fewer schedules than previous techniques.", "num_citations": "4\n", "authors": ["1590"]}
{"title": "On the complexity of stream equality\n", "abstract": " We study the complexity of deciding the equality of streams specified by systems of equations. There are several notions of stream models in the literature, each generating a different semantics of stream equality. We pinpoint the complexity of each of these notions in the arithmetical or analytical hierarchy. Their complexity ranges from low levels of the arithmetical hierarchy such as \u03a002 for the most relaxed stream models, to levels of the analytical hierarchy such as \u03a011 and up to subsuming the entire analytical hierarchy for more restrictive but natural stream models. Since all these classes properly include both the semi-decidable and co-semi-decidable classes, it follows that regardless of the stream semantics employed, there is no complete proof system or algorithm for determining equality or inequality of streams. We also discuss several related problems, such as the existence and uniqueness of stream\u00a0\u2026", "num_citations": "4\n", "authors": ["1590"]}
{"title": "Bus-MOP: a runtime monitoring framework for PCI peripherals\n", "abstract": " COTS peripherals are heavily used in the embedded market, but their unpredictability is a threat for high-criticality real-time systems: it is hard or impossible to formally verify COTS components. Instead, we propose to monitor the runtime behavior of COTS peripherals against their assumed specifications. If violations are detected, then an appropriate recovery measure can be taken. Our monitoring solution is decentralized: a monitoring device is plugged in on a peripheral bus and monitors the peripheral behavior by examining read and write transactions on the bus. Provably correct (wrt given specifications) hardware monitors are synthesized from high level specifications, and executed on FPGAs, resulting in zero runtime overhead on the system CPU. The proposed technique, called BusMOP, has been implemented as an instance of a generic runtime verification framework, called MOP, which until now has only been used for software monitoring. We experimented with our technique using a COTS data acquisition board.", "num_citations": "4\n", "authors": ["1590"]}
{"title": "Static analysis to enforce safe value flow in embedded control systems\n", "abstract": " Embedded control systems consist of multiple components with different criticality levels interacting with each other. For example, in a passenger jet, the navigation system interacts with the passenger entertainment system in providing passengers the distance-to-destination information. It is imperative that failures in the non-critical subsystem should not compromise critical functionality. This architectural principle for robustness can, however, be easily compromised by implementation-level errors. We describe SafeFlow, which statically analyzes core components in the system to ensure that they use non-core values communicated through shared memory only if they are run-time monitored for safety or recoverability. Using simple, local annotations and semantic restrictions on shared memory usage in the core component, SafeFlow precisely identifies accesses to unmonitored non-core values. With a few false\u00a0\u2026", "num_citations": "4\n", "authors": ["1590"]}
{"title": "Computational logical frameworks and generic program analysis technologies\n", "abstract": " The technologies developed to solve the verifying compiler grand challenge should be generic, that is, not tied to a particular language but widely applicable to many languages. Such technologies should also be semantics-based, that is, based on a rigorous formal semantics of the languages.               For this, a computational logical framework with efficient executability and a spectrum of meta-tools can serve as a basis on which to: (1) define the formal semantics of any programming language; and (2) develop generic program analysis techniques and tools that can be instantiated to generate powerful analysis tools for each language of interest.               Not all logical frameworks can serve such purposes well. We first list some specific requirements that we think are important to properly address the grand challenge. Then we present our experience with rewriting logic as supported by the Maude system\u00a0\u2026", "num_citations": "4\n", "authors": ["1590"]}
{"title": "Automatic and Precise Dimensional Analysis\n", "abstract": " The loss of NASA's Mars climate orbiter is evidence of the importance of units of measurement as a safety policy for software in general and for scientific applications in particular. In this paper we present a static analysis technique that detects violations of the unit policy. The technique relies on domain-specific unit annotations inserted in the code, either manually or automatically with the support of a tool, which are verified conservatively, i.e., all runtime unit errors are detected statically using an automatic theorem prover. This paper informally compares our approach with others, describes the technique in detail, and evaluates a benchmark built of standard programs for unit analysis and important fragments of NASA's SCRover project code.", "num_citations": "4\n", "authors": ["1590"]}
{"title": "Certifying Kalman Filters\n", "abstract": " Formal code certification is a rigorous approach to demonstrate software quality. Its basic idea is to require that code producers provide formal certificates, or proofs, that their code satisfies certain quality properties. In this paper, we focus on certifying software developed for state estimation of dynamic systems, which is an important problem found in spacecraft, aircraft, geophysical, and in many other applications. The most common way of solving state estimation problems consists of using Kalman filters, which are stochastic, recursive algorithms providing statistically optimal estimates of the state of a system based on noisy sensor measurements. We present a certifier for Kalman filter programs, which is a program that takes as input a program claiming to implement a Kalman filter together with a specification of that Kalman filter, as well as a certificate under the form of assertions and proof scripts merged within the program via annotations, and tells whether the code correctly implements the state estimation problem specified. We tested our certifier on three Kalman filters: simple Kalman filter, information filter and extended Kalman filter. So far we have played both the role of the producer of these programs and the role of the consumer, but our next step is to merge the presented technology with AutoFilter, a state estimation program sysnthesis system developed by researchers at NASA Ames, the idea being to have AutoFilter synthesize the correctness certificates together with the code.", "num_citations": "4\n", "authors": ["1590"]}
{"title": "Kan extensions of institutions\n", "abstract": " Institutions were introduced by Goguen and Burstall [GB84, GB85, GB86, GB92] to formally capture the notion of logical system. Interpreting institutions as functors, and morphisms and representations of institutions as natural transformations, we give elegant proofs for the completeness of the categories of institutions with morphisms and representations, respectively, show that the duality between morphisms and representations of institutions comes from an adjointness between categories of functors, and prove the cocompleteness of the categories of institutions over small signatures with morphisms and representations, respectively.", "num_citations": "4\n", "authors": ["1590"]}
{"title": "Language-parametric compiler validation with application to LLVM\n", "abstract": " We propose a new design for a Translation Validation (TV) system geared towards practical use with modern optimizing compilers, such as LLVM. Unlike existing TV systems, which are custom-tailored for a particular sequence of transformations and a specific, common language for input and output programs, our design clearly separates the transformation-specific components from the rest of the system, and generalizes the transformation-independent components. Specifically, we present Keq, the first program equivalence checker that is parametric to the input and output language semantics and has no dependence on the transformation between the input and output programs. The Keq algorithm is based on a rigorous formalization, namely cut-bisimulation, and is proven correct. We have prototyped a TV system for the Instruction Selection pass of LLVM, being able to automatically prove equivalence for\u00a0\u2026", "num_citations": "3\n", "authors": ["1590"]}
{"title": "Formalizing Correct-by-Construction Casper in Coq\n", "abstract": " Correct-by-Construction Casper (CBC Casper) is an Ethereum candidate consensus protocol undergoing active design and development. We present a formalization of CBC Casper using the Coq proof assistant that includes a model of the consensus protocol and proofs of safety and non-triviality protocol properties. We leverage Coq's type classes to model CBC Casper at various levels of abstraction. In doing so, we 1) illuminate the assumptions that each protocol property depends on, and 2) reformulate the protocol in general, mathematical terms. We highlight two advantages of our approach: 1) from a proof engineering perspective, it enables a clean separation of concerns between theory and implementation; 2) from a protocol engineering perspective, it provides a rigorous, foundational understanding of the protocol conducive to finding and proving stronger properties. We detail one such new property\u00a0\u2026", "num_citations": "3\n", "authors": ["1590"]}
{"title": "Dealing With C's Original Sin\n", "abstract": " In the very early days of C, the compiler written by Dennis Ritchie and supplied with the UNIX operating system entirely defined the language. As the number of users and C implementations grew, however, so too did the need for a language standard-a contract between users and implementers about what should and should not count as C. This effort began in 1983 with the formation of a committee tasked with producing \"an unambiguous and machine-independent definition of the language C\" and led to the ANSI C Standard in 1989.1 In retrospect, it was not until this date, 17 years after the first compiler, when C's most notorious language feature slithered into the world: <;italic>undefined behavior<;/italic>.", "num_citations": "3\n", "authors": ["1590"]}
{"title": "Matching mu-Logic: Foundation of K Framework\n", "abstract": " K framework is an effort in realizing the ideal language framework where programming languages must have formal semantics and all languages tools are automatically generated from the formal semantics in a correct-by-construction manner at no additional costs. In this extended abstract, we present matching mu-logic as the foundation of K and discuss some of its applications in defining constructors, transition systems, modal mu-logic and temporal logic variants, and reachability logic.", "num_citations": "3\n", "authors": ["1590"]}
{"title": "Formal design, implementation and verification of blockchain languages\n", "abstract": " This invited paper describes recent, ongoing and planned work on the use of the rewrite-based semantic framework K to formally design, implement and verify blockchain languages and virtual machines. Both academic and commercial endeavors are discussed, as well as thoughts and directions for future research and development.", "num_citations": "3\n", "authors": ["1590"]}
{"title": "From Hoare logic to matching logic\n", "abstract": " Matching logic has been recently proposed as an alternative program verification approach. Unlike Hoare logic, where one defines a language-specific proof system that needs to be proved sound for each language separately, matching logic provides a language-independent and sound proof system that directly uses the trusted operational semantics of the language as axioms. Matching logic thus has a clear practical advantage: it eliminates the need for an additional semantics of the same language in order to reason about programs, and implicitly eliminates the need for tedious soundness proofs. What is not clear, however, is whether matching logic is as powerful as Hoare logic. This paper introduces a technique to mechanically translate Hoare logic proof derivations into equivalent matching logic proof derivations. The presented technique has two consequences: first, it suggests that matching logic has no theoretical limitation over Hoare logic; and second, it provides a new approach to prove Hoare logics sound.", "num_citations": "3\n", "authors": ["1590"]}
{"title": "Reachability logic\n", "abstract": " This paper introduces *reachability logic*, a language-independent seven-rule proof system for deriving reachability properties of systems. The key ingredients of *reachability logic* are its sentences, which are called reachability rules and generalize the transitions of operational semantics and the Hoare triples of axiomatic semantics, and the *Circularity* proof rule, which generalizes invariant proof rules for iterative and recursive constructs in axiomatic semantics. The target transition system is described as a set of reachability rules, which are taken as axioms in a reachability logic proof. Typical definition styles which can be read as collections of reachability rules include conventional small-step and big-step operational semantics. The reachability logic proof system is shown sound (in the sense of partial correctness) and relatively complete. The soundness result has also been formalized in Coq, allowing to convert reachability logic proofs into proof certificates depending only on the operational semantics and the unavoidable domain reasoning. Reachability logic thus eliminates the need to independently define an axiomatic and an operational semantics for each language, and the non-negligible effort to prove the former sound and complete w.r.t the latter.", "num_citations": "3\n", "authors": ["1590"]}
{"title": "Formalizing operator task analysis\n", "abstract": " Human operators are unique in their decision making capability, judgment and nondeterminism. Their sense of judgment, unpredictable decision procedures, susceptibility to environmental elements can cause them to erroneously execute a given task description to operate a computer system. Usually, a computer system is protected against some erroneous human behaviors by having necessary safeguard mechanisms in place. But some erroneous human operator behaviors can lead to severe or even fatal consequences especially in safety critical systems. A generalized methodology that can allow modeling and analyzing the interactions between computer systems and human operators where the operators are allowed to deviate from their prescribed behaviors will provide a formal understanding of the robustness of a computer system against possible aberrant behaviors by its human operators.", "num_citations": "3\n", "authors": ["1590"]}
{"title": "A meta-language for functional verification\n", "abstract": " This dissertation perceives a similarity between two activities: that of coordinating the search for simulation traces toward reaching verification closure, and that of coordinating the search for a proof within a theorem prover. The programmatic coordination of simulation is difficult with existing tools for digital circuit verification because stimuli generation, simulation execution, and analysis of simulation results are all decoupled. A new programming language to address this problem, analogous to the mechanism for orchestrating proof search tactics within a theorem prover, is defined wherein device simulation is made a first-class notion. This meta-language for functional verification is first formalized in a parametric way over hardware description languages using rewriting logic, and subsequently a more richly featured software tool for Verilog designs, implemented as an embedded domain-specific language in Haskell\u00a0\u2026", "num_citations": "3\n", "authors": ["1590"]}
{"title": "Mining parametric specifications\n", "abstract": " Mining formal specifications from program executions has numerous applications in software analysis, from program understanding and modeling to testing and bug detection.  Parametric specifications carry parameters that are bound to concrete values at runtime. They are useful for specifying system behaviors involving multiple components. Runtime monitoring of parametric specifications is relatively well-understood, with several performant runtime monitoring systems available. The main challenge underlying such parametric monitoring systems is to slice parametric execution traces into smaller, non-parametric traces, each relevant for a particular parameter instance; then each of the trace slices is monitored against a non-parametric monitor.  This paper presents a novel technique to automatically mine parametric specifications from execution traces, which builds upon the observation that there is an inherent duality between parametric specification monitoring and parametric specification mining: they both rely on an online parametric trace slicing process, followed either by monitoring the resulting trace slices against given specifications in the first case, or by inferring the specifications that best explain the observed trace slices in the second case.  A blind use of off-the-shelf parametric trace slicing techniques from monitoring leads to inefficient and \"noisy\" slicers for mining.  The first contribution of this paper is a mining-specific parametric trace slicer, which makes slicing feasible and precise for mining. The obtained trace slices can then be passed to any non-parametric property learner. A blind use of the off-the-shelf Probabilistic Finite State\u00a0\u2026", "num_citations": "3\n", "authors": ["1590"]}
{"title": "P systems with control nuclei\n", "abstract": " We describe an extension of P-systems where each membrane has an associated control nucleus responsible with the generation of the rules to be applied in that membrane. The nucleus exports a set of rules which are applied in the membrane region (only for one step, but in the usual maximal-parallel way), then the rules are removed and a new iteration of this process takes place. This way, powerful control mechanisms may be included in P-systems themselves, as opposed to using the level of \u201cstrategies\u201d previously exploited for simulating P-systems. The nuclei may contain general programs for generating rules, ranging from those using information on the full system, to more restricted programs where only local information in the nuclei themselves is used. The latter approach mixed with a particular mechanism for the representation of the control programs, the rules, and the export procedure is engaged to develop a model for cell growth and division in normal and abnormal (tumoral) evolution of biological systems.", "num_citations": "3\n", "authors": ["1590"]}
{"title": "Effective predictive runtime analysis using sliced causality and atomicity\n", "abstract": " Predictive runtime analysis has been proposed to improve the effectiveness of concurrent program analysis and testing. Observing an execution, predictive runtime analysis extracts causality which is then used as the model of the program and checked against desired properties. This way, one can predict concurrent errors without actually hitting them. The causality constructed during the analysis determines the prediction ability of this approach. This paper presents an efficient and sound approach to computing sliced causality and atomicity which significantly but soundly improves existing causalities by removing irrelevant causal partial orders using dependence, relevance, and atomicity information of the program. Algorithms presented in this paper have been implemented and extensively evaluated. The results show that the technique is effective and sound: we found all the previously known bugs as well as unknown errors in popular systems, like the Tomcat webserver and the Apache FTP server, without any false alarms.", "num_citations": "3\n", "authors": ["1590"]}
{"title": "\u2014A Semantic Framework for Programming Languages and Formal Analysis\n", "abstract": " We give an overview on the applications and foundations of the  language framework, a semantic framework for programming languages and formal analysis tools.  represents a 20-year effort in pursuing the ideal language framework vision, where programming languages must have formal definitions, and tools for a given language, such as parsers, interpreters, compilers, semantic-based debuggers, state-space explorers, model checkers, deductive program verifiers, etc., can be derived from just one reference formal definition of the language, which is executable, and no other semantics for the same language should be needed. The correctness of the language tools is guaranteed on a case-by-case basis by proof objects, which encode rigorous mathematical proofs as certificates for every individual task that the tools do and can be mechanically checked by third-party proof checkers.", "num_citations": "2\n", "authors": ["1590"]}
{"title": "Statistical model checking of RANDAO\u2019s resilience against pre-computed reveal strategies\n", "abstract": " Decentralized (pseudo-)random number generation (RNG) is a core process of many emerging distributed systems, including perhaps most prominently, the upcoming Ethereum 2.0 (a.k.a. Serenity) protocol. To ensure security and proper operation, the randomness beacon must be unpredictable and hard to manipulate. A commonly accepted implementation scheme for decentralized RNG is a commit-reveal scheme, known as RANDAO, coupled with a reward system that incentivizes successful participation. However, this approach may still be susceptible to look-ahead attacks, in which an attacker (controlling a certain subset of participants) may attempt to pre-compute the outcomes of (possibly many) reveal strategies, and thus may bias the generated random number to his advantage. To formally evaluate resilience of RANDAO against such attacks, we develop a probabilistic model in rewriting logic of the RANDAO scheme (in the context of Serenity), and then apply statistical model checking and quantitative verification algorithms (using Maude and PVeStA) to analyze two different properties that provide different measures of bias that the attacker could potentially achieve using pre-computed strategies. We show through this analysis that unless the attacker is already controlling a sizable portion of validators and is aggressively attempting to maximize the number of last compromised proposers in the proposers list, the expected achievable bias is quite limited. The full specification of the models developed in this work are available online at https://github.com/runtimeverification/rdao-smc.", "num_citations": "2\n", "authors": ["1590"]}
{"title": "Runtime Verification\n", "abstract": " Portal - Seminar 07011 Dagstuhl Seminar Proceedings 07011 Runtime Verification B. Finkbeiner, K. Havelund, G. Rosu, O. Sokolsky (Eds.) published by LZI Host ISSN 1862 - 4405 Dagstuhl Seminar 07011, 02.01. - 06.01.2007 Additional Information Seminar Homepage License Search Publication Server Authors Finkbeiner, Bernd Havelund, Klaus Lee, Insup Regehr, John Rosu, Grigore Sammapun, Usa Sokolsky, Oleg Tripakis, Stavros 07011 Abstracts Collection -- Runtime Verification Authors: Finkbeiner, Bernd ; Havelund, Klaus ; Rosu, Grigore ; Sokolsky, Oleg Abstract | Document (192 KB) | BibTeX 07011 Executive Summary -- Runtime Verification Authors: Finkbeiner, Bernd ; Havelund, Klaus ; Rosu, Grigore ; Sokolsky, Oleg Abstract | Document (99 KB) | BibTeX Monitoring, Fault Diagnosis and Testing Real-time Systems using Analog and Digital Clocks Authors: Tripakis, Stavros Abstract | Document (76 KB) | \u2026", "num_citations": "2\n", "authors": ["1590"]}
{"title": "A Language-Independent Proof System for Mutual Program Equivalence (revisited)\n", "abstract": " Two programs are mutually equivalent if they both diverge or they end up in similar states. Mutual equivalence is an adequate notion of equivalence for programs written in deterministic languages. It is useful in many contexts, such as capturing the correctness of program transformations within the same language, or capturing the correctness of compilers between two different languages. In this paper we introduce a language-independent proof system for mutual equivalence, which is parametric in the operational semantics of two languages and in a statesimilarity relation. The proof system is sound: if it terminates then it establishes the mutual equivalence of the programs given to it as input. We illustrate it on two programs in two different languages (an imperative one and a functional one), that both compute the Collatz sequence.", "num_citations": "2\n", "authors": ["1590"]}
{"title": "Behavioral rewrite systems and behavioral productivity\n", "abstract": " This paper introduces behavioral rewrite systems, where rewriting is used to evaluate experiments, and behavioral productivity, which says that each experiment can be fully evaluated, and investigates some of their properties. First, it is shown that, in the case of (infinite) streams, behavioral productivity generalizes and may bring to a more basic rewriting setting the existing notion of stream productivity defined in the context of infinite rewriting and lazy strategies; some arguments are given that in some cases one may prefer the behavioral approach. Second, a behavioral productivity criterion is given, which reduces the problem to conventional term rewrite system termination, so that one can use off-the-shelf termination tools and techniques for checking behavioral productivity in general, not only for streams. Finally, behavioral productivity is shown to be equivalent to a proof-theoretic (rather than model-theoretic\u00a0\u2026", "num_citations": "2\n", "authors": ["1590"]}
{"title": "Monitoring Oriented Programming and Analysis\n", "abstract": " This thesis proposes runtime monitoring as a central principle in developing reliable software. Two major research directions are investigated. The first, called monitoring-oriented programming (MOP), aims at detecting and recovering from requirements violations at runtime. In MOP, a user develops specifications together with code; specifications are synthesized into monitors at compile time, and the monitors are then weaved within the application resulting in a system that is aware of its own execution and can correct itself. The second major monitoring-based research direction addressed in this thesis is predictive runtime analysis (PRA), which aims at detecting errors in programs before deployment. In PRA, the program is executed and a causal model is extracted from the observed execution; the causal model is then exhaustively analyzed for potential violations, this way PRA is able to detect errors that did not necessarily occur in the observed execution but that could appear in other executions. MOP and PRA are intrinsically related, both using the same specification formalisms and monitor synthesis algorithms; the difference is that MOP detects errors contemporaneously with their occurrence, and can thus also recover from them, while PRA detects potential errors that can take place in other\u2013unobserved, but causally possible\u2013executions of the system, and thus the system designer can fix them before deployment. Both techniques are sound, in that all reported errors are real.Two prototype systems have been implemented that prove the feasibility of the proposed techniques. JavaMOP is a monitoring-oriented programming system for Java\u00a0\u2026", "num_citations": "2\n", "authors": ["1590"]}
{"title": "C Policy Framework\n", "abstract": " \u201cNASA lost a $125 million Mars orbiter because one engineering team used metric units while another used English units for a key spacecraft operation... For that reason, information failed to transfer between the Mars Climate Orbiter spacecraft team at Lockheed Martin in Colorado and the mission navigation team in California.\u201d(picture and text from CNN. com, http://www. cnn. com/TECH/space/9909/30/mars. metric/)", "num_citations": "2\n", "authors": ["1590"]}
{"title": "Efficient Formalism-Independent Monitoring of Parametric Properties (Extended Version\n", "abstract": " Efficient monitoring of parametric properties, in spite of increasingly growing interest thanks to applications such as testing and security, imposes a highly non-trivial challenge on monitoring approaches due to the potentially huge number of parameter instances. A few solutions have been proposed, but most of them compromise their expressiveness for performance or vice versa. In this paper, we propose a generic, in terms of specification formalisms, yet efficient, solution to monitoring parametric specifications. Our approach is based on a general semantics for slicing parametric traces and makes use of knowledge about the property to monitor. The needed knowledge is not specific to the underlying formalism and can be easily computed when generating monitoring code from the property. An extensive evaluation shows that the monitoring code generated by our algorithm is still faster than other state-of-art techniques optimized for particular logics or properties. 1", "num_citations": "2\n", "authors": ["1590"]}
{"title": "A Rewriting Based Approach to OO Language Prototyping and Design\n", "abstract": " This paper introduces a framework for the rapid prototyping of object oriented programming languages. This framework is based on specifying the semantics of a language using term rewriting and a continuation-based representation of control. The notation used, called K, has been developed specifically for programming languages to overcome limitations in more general rewriting notation, and provides for more compact and modular language definitions. The K notation is used to define KOOL, a dynamic object-oriented language with many features found in mainstream object-oriented languages. The ability to rapidly prototype language features is shown both in the definition of KOOL and in the creation of a concurrent extension to the language.", "num_citations": "2\n", "authors": ["1590"]}
{"title": "Extensional theories and rewriting\n", "abstract": " This paper is an attempt to develop a unifying algebraic framework for extensional theories capturing formally the informal concept of extensionality, as well as a generic automated proving technique, called extensional rewriting, that can be instantiated and then used to prove equational properties in various particular extensional theories.", "num_citations": "2\n", "authors": ["1590"]}
{"title": "On implementing behavioral rewriting\n", "abstract": " Behavioral specification is an important algebraic method in software technology. A subtle aspect of behavioral specification is that operations may not be compatible with the behavioral (or observational) equivalence, meaning that the typical congruence inference rule may not be sound and, implicitly, that standard term rewriting cannot be used as is to execute behavioral specifications. Behavioral rewriting is an appropriate generalization of term rewriting which is already internally implemented in two behavioral specification and verification systems, CafeOBJ and BOBJ. In this paper we propose two alternative solutions to implement behavioral rewriting, on top of almost any standard term rewriting system, without modifying it internally.", "num_citations": "2\n", "authors": ["1590"]}
{"title": "Towards Certifying Domain-Specific Properties of Synthesized Code\u2013Extended Abstract\u2013\n", "abstract": " We present a technique for certifying domain-specific properties of code generated using program synthesis technology. Program synthesis is a maturing technology that generates code from high-level specifications in particular domains. For acceptance in safety-critical applications, the generated code must be thoroughly tested which is a costly process. We show how the program synthesis system AUT-OFILTER can be extended to generate not only code but also proofs that properties hold in the code. This technique has the potential to reduce the costs of testing generated code.", "num_citations": "2\n", "authors": ["1590"]}
{"title": "Trustworthy program verification via proof generation\n", "abstract": " In an ideal language framework, language designers only need to define the formal semantics of their languages. Deductive program verifiers and other language tools are automatically generated by the framework. In this paper, we propose a novel approach to establishing the correctness of these autogenerated verifiers via proof generation. Our approach is based on the K language framework and its logical foundation, matching logic. Given a formal language semantics in K, we translate it into a corresponding matching logic theory. Then, we encode formal verification tasks as reachability formulas in matching logic. The correctness of one verification task is then established, on a case-by-case basis, by automatically generating a rigorous, machine-checkable mathematical proof of the associated reachability formula. Experiments with our proof generation prototype on various verification tasks in different programming languages show promising performance and attest to the feasibility of the proposed approach.", "num_citations": "1\n", "authors": ["1590"]}
{"title": "Connecting constrained constructor patterns and matching logic\n", "abstract": " Constrained constructor patterns are pairs of a constructor term pattern and a quantifier-free first-order logic constraint, built from conjunction and disjunction. They are used to express state predicates for reachability logic defined over rewrite theories. Matching logic has been recently proposed as a unifying foundation for programming languages, specification and verification. It has been shown to capture several logical systems and/or models that are important for programming languages, including first-order logic with fixpoints and order-sorted algebra. In this paper, we investigate the relationship between constrained constructor patterns and matching logic. The comparison result brings us a mutual benefit for the two approaches. Matching logic can borrow computationally efficient proofs for some equivalences, and the language of the constrained constructor patterns can get a more logical flavor and\u00a0\u2026", "num_citations": "1\n", "authors": ["1590"]}
{"title": "Formal Design, Implementation and Verification of Blockchain Languages Using K (Invited Talk)\n", "abstract": " The usual post-mortem approach to formal language semantics and verification, where the language is firstly implemented and used in production for many years before a need for formal semantics and verification tools naturally arises, simply does not work anymore. New blockchain languages or virtual machines are proposed at an alarming rate, followed by new versions of them every few weeks, together with programs (or smart contracts) in these languages that are responsible for financial transactions of potentially significant value. Formal analysis and verification tools are therefore needed immediately for such languages and virtual machines. We will present recent academic and commercial results in developing blockchain languages and virtual machines that come directly equipped with formal analysis and verification tools. The main idea is to generate all these automatically, correct-by-construction from a formal language specification.", "num_citations": "1\n", "authors": ["1590"]}
{"title": "Statistical Model Checking of RANDAO\u2019s Resilience to Pre-computed Reveal Strategies\n", "abstract": " RANDAO is a commit-reveal scheme for generating pseudo-random numbers in a decentralized fashion. The scheme is used in emerging blockchain systems as it is widely believed to provide randomness that is unpredictable and hard to manipulate by maliciously behaving nodes. However, RANDAO may still be susceptible to look-ahead attacks, in which an attacker (controlling a subset of nodes in the network) may attempt to pre-compute the outcomes of (possibly many) reveal strategies, and thus may bias the generated random number to his advantage. In this work, we formally evaluate resilience of RANDAO against such attacks. We first develop a probabilistic model in rewriting logic of RANDAO, and then apply statistical model checking and quantitative verification algorithms (using Maude and PVeStA) to analyze two different properties that provide different measures of bias that the attacker could\u00a0\u2026", "num_citations": "1\n", "authors": ["1590"]}
{"title": "Efficient over-the-air software update for a connected vehicle\n", "abstract": " The disclosure includes a system and method for providing a wireless software update for a connected vehicle. The connected vehicle includes a processor, an engine, a battery, a non-transitory memory storing a vehicle application and a wireless antenna that is powered by the battery and operable to receive a monitor module from a wireless network while the battery is not being charged. The processor is communicatively coupled to the battery, the wireless antenna and the non-transitory memory. The monitor module is written in an aspect language and includes a software patch for the vehicle application. The wireless antenna receives the monitor module from the wireless network while leaving a sufficient charge in the battery to enable the battery to start the engine. The processor installs the monitor module in the vehicle application stored in the non-transitory memory. Installation of the monitor module\u00a0\u2026", "num_citations": "1\n", "authors": ["1590"]}
{"title": "SETSS\u201919 Lecture Notes on K\u22c6\n", "abstract": " We give an overview on the applications and foundations of the K language framework, a semantic framework for programming languages and formal analysis tools. K represents a 20-year effort in pursuing the ideal language framework vision, where programming languages must have formal definitions, and tools for a given language, such as parsers, interpreters, compilers, semantic-based debuggers, state-space explorers, model checkers, deductive program verifiers, etc., can be derived from just one reference formal definition of the language, which is executable, and no other semantics for the same language should be needed. The correctness of the languages tools is guaranteed on a case-by-case basis by proof objects, which encode rigorous mathematical proofs as certificates for every individual tasks that the tools do and can be mechanically checked by third-party proof checkers.", "num_citations": "1\n", "authors": ["1590"]}
{"title": "Verifying Finality for Blockchain Systems\n", "abstract": " I was able to port many [HOL Light] proofs that I did not understand: despite the huge difierences between the two proof languages, it was usually possible to guess what had to be proved from the HOL Light text, along with many key reasoning steps. Isabelle\u2019s automation was generally able to fill the gaps.", "num_citations": "1\n", "authors": ["1590"]}
{"title": "Towards a ool Future\n", "abstract": " The  framework was successfully used for defining formal semantics for several practical languages, e.g. C, Java, Java Script, but no language with distributed concurrent objects was defined in  up\u00a0to now. In this paper we investigate how the model of asynchronous method calls, using the so-called futures for handling the return values, can be added to an existing  definition using the ideas from the Complete Guide to the Future paper. As the running example we use the  definition of KOOL, a pedagogical and research language that captures the essence of the object-oriented programming paradigm. This is a first step toward a generic methodology for modularly adding future-based mechanisms to allow asynchronous method calls.", "num_citations": "1\n", "authors": ["1590"]}
{"title": "Abstract Semantics for K Module Composition\n", "abstract": " A structured K definition is easier to write, understand and debug than one single module containing the whole definition. Furthermore, modularization makes it easy to reuse work between definitions that share some principles or features. Therefore, it is useful to have a semantics for module composition operations that allows the properties of the resulting modules to be well understood at every step of the composition process. This paper presents an abstract semantics for a module system proposal for the K framework. It describes K modules and module transformations in terms of institution-based model theory introduced by Goguen and Burstall.", "num_citations": "1\n", "authors": ["1590"]}
{"title": "Low-Level Program Verification using Matching Logic Reachability\n", "abstract": " Restore the previously executing code from the stack cell, which also contains the previously-executing priority to restore to the priority cell. Interrupts are assigned numeric priority in the order they are scheduled by the program, and can interrupt only code running at a lower priority. The main program begins executing at priority 0.", "num_citations": "1\n", "authors": ["1590"]}
{"title": "Reachability Logic in K\n", "abstract": " This paper presents a language-independent proof system for reachability properties of programs written in non-deterministic (concurrent) languages, referred to as reachability logic. The proof system derives partial-correctness properties with either all-path or one-path semantics, i.e., that states satisfying a given precondition reach states satisfying a given postcondition on all execution paths, respectively on one execution path. Reachability logic takes as axioms any unconditional operational semantics, and is sound (i.e., partially correct) and (relatively) complete, independent of the object language; the soundness has also been mechanized. The proof system is implemented in a tool for semantics-based verification as part of the K framework, and evaluated on a few examples.", "num_citations": "1\n", "authors": ["1590"]}
{"title": "Towards a unified theory of operational and axiomatic semantics\u2014extended abstract\n", "abstract": " This paper presents a nine-rule language-independent proof system that takes an operational semantics as axioms and derives program properties, including ones corresponding to Hoare triples. This eliminates the need for language-specific Hoare-style proof rules in order to verify programs, and, implicitly, the tedious step of proving such proof rules sound for each language separately. The key proof rule is Circularity, which is coinductive in nature and allows for reasoning about constructs with repetitive behaviors (eg, loops). The generic proof system is shown sound and has been implemented in the MatchC program verifier.", "num_citations": "1\n", "authors": ["1590"]}
{"title": "Formal Techniques for Distributed Systems\n", "abstract": " This volume contains the proceedings of the FMOODS/FORTE 2012 conference, a joint conference combining the 14th IFIP International Conference on Formal Methods for Open Object-Based Distributed Systems (FMOODS) and the 32nd IFIP International Conference on Formal Techniques for Networked and Distributed Systems (FORTE) held during June 13\u201314, 2012, in Stockholm. FMOODS/FORTE was hosted together with the 14th International Conference on Coordination Models and Languages (COORDINATION) and the 12th IFIP International Conference on Distributed Applications and Interoperable Systems (DAIS) by the federated conference event DisCoTec 2012, devoted to distributed computing techniques and sponsored by the International Federation for Information Processing (IFIP).FMOODS/FORTE provides a forum for fundamental research on the theory and applications of distributed systems\u00a0\u2026", "num_citations": "1\n", "authors": ["1590"]}
{"title": "An overview of monitoring oriented programming\n", "abstract": " This article gives an overview of Monitoring Oriented Programming (MOP). In MOP, runtime monitoring is supported and encouraged as a fundamental principle for building reliable systems. Monitors are automatically synthesized from specified properties and are used in conjunction with the original system to check its dynamic behaviors. When a specification is violated or validated at runtime, user-defined actions will be triggered, which can be any code, such as information logging or runtime recovery. Two instances of MOP are presented: JavaMOP (for Java programs) and BusMOP (for monitoring PCI bus traffic). The architecture of MOP is discussed, and an explanation of parametric trace monitoring and its implementation is given. A comprehensive evaluation of JavaMOP attests to its efficiency, especially with respect to similar systems. The implementation of BusMOP is discussed in detail. In general, BusMOP\u00a0\u2026", "num_citations": "1\n", "authors": ["1590"]}
{"title": "Towards semantics-based WCET analysis\n", "abstract": " This paper proposes the use of formal semantics as a basis for worst-case execution time (WCET) analysis. Specifically, the semantics of a RISC assembly language is formally defined using recent advances in rewrite-based semantics, and then is used to discover and eliminate erroneous execution paths in the context ofWCET analysis. This paper makes two novel contributions:(1) it shows that using a formal semantics of the employed language can be practically feasible in WCET analysis (not only theoretically desirable); and (2) it shows that the discovery and elimination of erroneous execution paths can not only improve the WCET estimation, but can also be achieved using off-the-shelf technology for rewrite-based semantics.", "num_citations": "1\n", "authors": ["1590"]}
{"title": "On compiling rewriting logic language definitions into competitive interpreters\n", "abstract": " This paper describes a completely automated method for generating efficient and competitive interpreters from formal semantics expressed in Rewriting Logic. The semantics are compiled into OCaml code, which then acts as the interpreter for the language being defined. This automatic translation is tested on the semantics of an imperative as well as a functional language, and these generated interpreters are then benchmarked across a number of programs. In all cases the compiled interpreter is faster than directly executing the definition in a Rewriting system with improvements of several orders of magnitude.", "num_citations": "1\n", "authors": ["1590"]}
{"title": "Algebraic Methodology and Software Technology: 12th International Conference, AMAST 2008 Urbana, IL, USA, July 28-31, 2008, Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the 12th International Conference on Algebraic Methodology and Software Technology, AMAST 2008, held in Urbana, IL, USA, in July 2008. The 28 revised full papers presented together with 3 invited talks were carefully reviewed and selected from 58 submissions. Among the topics covered are all current issues in formal methods related to algebraic and logical foundations, software technology, and to programming methodology including concurrent and reactive systems, evolutionary software/adaptive systems, logic and functional programming, object paradigms, constraint programming and concurrency, program verification and transformation, programming calculi, specification languages and tools, formal specification and development case studies, logic, category theory, relation algebra, computational algebra, algebraic foundations for languages and systems, coinduction, theorem proving and logical frameworks for reasoning, logics of programs, as well as algebra and coalgebra.", "num_citations": "1\n", "authors": ["1590"]}
{"title": "K: a Rewrite-based Framework for Modular Language Design, Semantics, Analysis and Implementation-Version 2\n", "abstract": " K is an algebraic framework for defining programming languages. It consists of a technique and of a specialized and highly optimized notation. The K-technique, which can be best explained in terms of rewriting modulo equations or in terms of rewriting logic, is based on a first-order representation of continuations with intensive use of matching modulo associativity, commutativity and identity. The K-notation consists of a series of high-level conventions that make the programming language definitions intuitive, easy to understand, to read and to teach, compact, modular and scalable. One important notational convention is based on context transformers, allowing one to automatically synthesize concrete rewrite rules from more abstract definitions once the concrete structure of the state is provided, by .completing. the contexts in which the rules should apply. The K framework is introduced by defining FUN, a concurrent higher-order programming language with parametric exceptions. A rewrite logic definition of a programming language can be executed on rewrite engines, thus providing an interpreter for the language for free, but also gives an initial model semantics, amenable to formal analysis such as model checking and inductive theorem proving. Rewrite logic definitions in K can lead to automatic, correct-by-construction generation of interpreters, compilers and analysis tools.", "num_citations": "1\n", "authors": ["1590"]}
{"title": "Formally defining and verifying master/slave speculative parallelization\n", "abstract": " Master/Slave Speculative Parallelization (MSSP) is a new execution paradigm that decouples the issues of performance and correctness in microprocessor design and implementation. MSSP uses a fast, not necessarily correct, master processor to speculatively split a program into tasks, which are executed independently and concurrently on slower, but correct, slave processors. This work reports on the first steps in our efforts to formally validate that overall correctness can be achieved in MSSP despite a lack of correctness guarantees in its performance-critical parts. We describe three levels of an abstract model for MSSP, each refining the next and each preserving equivalence to a sequential machine. Equivalence is established in terms of a jumping refinement, a notion we introduce to describe equivalence at specific places of interest in the code. We also report on experiences and insights gained from\u00a0\u2026", "num_citations": "1\n", "authors": ["1590"]}
{"title": "Preface: Volume 70, issue 4\n", "abstract": " Runtime Verification 2002This volume contains the Proceedings of the Second Workshop on Runtime Verification (RV'02). The Workshop was held in Copenhagen, Denmark, on 26 July 2002, as a satellite event to CAV'02. The First Workshop on Runtime Verification (RV'01) was held in Paris, France, on 23 July 2001, as a satellite event to CAV'01.The objective of the RV workshops is to bring scientists from both academia and industry together to debate on how to monitor, analyze and guide the execution of programs. The ultimate longer term goal is to investigate whether the use of lightweight formal methods applied during the execution of programs is a viable complement to the current heavyweight methods proving programs correct always before their execution, such as model checking and theorem proving. Dynamic program monitoring and analysis can occur during testing or during operation. The subject\u00a0\u2026", "num_citations": "1\n", "authors": ["1590"]}
{"title": "Program Instrumentation and Trace Analysis\n", "abstract": " Several attempts have been made recently to apply techniques such as model checking and theorem proving to the analysis of programs. This shall be seen as a current trend to analyze real software systems instead of just their designs. This includes our own effort to develop a model checker for Java, the Java PathFinder 1, one of the very first of its kind in 1998. However, model checking cannot handle very large programs without some kind of abstraction of the program. This paper describes a complementary scalable technique to handle such large programs. Our interest is turned on the observation part of the equation: How much information can be extracted about a program from observing a single execution trace? It is our intention to develop a technology that can be applied automatically and to large full-size applications, with minimal modification to the code. We present a tool, Java PathExplorer (JPaX), for exploring execution traces of Java programs. The tool prioritizes scalability for completeness, and is directed towards detecting errors in programs, not to prove correctness. One core element in JPaX is an instrumentation package that allows to instrument Java byte code files to log various events when executed. The instrumentation is driven by a user provided script that specifies what information to log. Examples of instructions that such a script can contain are:'report name and arguments of all called methods defined in class C, together with a timestamp';'report all updates to all variables'; and'report all acquisitions and releases of locks'. In more complex instructions one can specify that certain expressions should be evaluated and\u00a0\u2026", "num_citations": "1\n", "authors": ["1590"]}
{"title": "Synthesizing dynamic programming algorithms fromlinear temporal logic formulae\n", "abstract": " The problem of testing a linear temporal logic (LTL) formula on a finite execution trace of events, generated by an executing program, occurs naturally in runtime analysis of software. We present an algorithm which takes an LTL formula and generates an efficient dynamic programming algorithm. The generated algorithm tests whether the LTL formula is satisfied by a finite trace of events given as input. The generated algorithm runs in linear time, its constant depending on the size of the LTL formula. The memory needed is constant, also depending on the size of the formula.}", "num_citations": "1\n", "authors": ["1590"]}
{"title": "Weak inclusion systems\n", "abstract": " We dene weak inclusion systems as a natural extension of inclusion systems. We provethatseveralpropertiesoffactorisation systems and inclusion systems remainvalidunderthisextension and we obtain new properties as algebraic tools in abstract model theory.", "num_citations": "1\n", "authors": ["1590"]}
{"title": "Statically Resolving Dynamic Includes in PHP\n", "abstract": " The PHP language includes a number of dynamic features that impact the correctness and precision of static analysis tools. One of these features is the include expression: evaluated at runtime, include accepts a single argument, an arbitrary expression, that is evaluated to compute the path of the file to include. Because the correct file to include is chosen at runtime, it may not be possible to determine the program source that should be analyzed up front.In prior work, we determined that many of these dynamic includes are static in practice, ie, that it is possible to resolve the path expression statically to a single intended file. In this paper we present an algorithm, implemented using the Rascal meta-programming language, for resolving these dynamic includes. To evaluate the effectiveness of the algorithm, we have applied it to a corpus of 39 open-source systems, totaling more than 5 million lines of PHP. Our results\u00a0\u2026", "num_citations": "1\n", "authors": ["1590"]}
{"title": "CS422\u2013Formal Methods in System Design: A Monitor Synthesis Algorithm for Past LTL\n", "abstract": " A monitor synthesis algorithm from linear temporal logic (LTL) safety formulae of the form D\u03d5 where \u03d5 is a past time LTL formula was presented in [3]. The generated monitors implemented the recursive semantics of past-time LTL using a dynamic programming technique, and needed O (| \u03d5|) time to process each new event and O (| \u03d5|) total space. Some compiler-like optimizations of the generated monitors were also proposed in [3], which would further reduce the required space. It is not clear how much the required space could be reduced by applying those optimizations.", "num_citations": "1\n", "authors": ["1590"]}