{"title": "Fizzy: feature subset selection for metagenomics\n", "abstract": " Some of the current software tools for comparative metagenomics provide ecologists with the ability to investigate and explore bacterial communities using \u03b1\u2013 & \u03b2\u2013diversity. Feature subset selection \u2013 a sub-field of machine learning \u2013 can also provide a unique insight into the differences between metagenomic or 16S phenotypes. In particular, feature subset selection methods can obtain the operational taxonomic units (OTUs), or functional features, that have a high-level of influence on the condition being studied. For example, in a previous study we have used information-theoretic feature selection to understand the differences between protein family abundances that best discriminate between age groups in the human gut microbiome. We have developed a new Python command line tool, which is compatible with the widely adopted BIOM format, for microbial ecologists that implements information-theoretic\u00a0\u2026", "num_citations": "38\n", "authors": ["1077"]}
{"title": "A novelty detector and extreme verification latency model for nonstationary environments\n", "abstract": " Safe and reliable operation of systems relies on the use of online condition monitoring and diagnostic systems that aim to take immediate actions upon the occurrence of a fault. Model-based solutions are often not practical in nonstationary environments. Thus, the evolving data stream requires the data-driven model to be adaptive. In this paper, we propose a framework for the fault detection and classification that is accomplished on the data stream with both the gradual and abrupt drifts. The framework is only provided with prior information about the possible faults at the initial step; however, despite this, the framework can still detect the novel faults without receiving any update. Furthermore, an efficient fault classification algorithm is presented to maximize the efficiency of the proposed framework. Finally, the proposed framework is applied for diagnosing bearing defects in the induction motors to demonstrate its\u00a0\u2026", "num_citations": "35\n", "authors": ["1077"]}
{"title": "Convolutional neural networks for pavement roughness assessment using calibration\u2010free vehicle dynamics\n", "abstract": " Road roughness is a measure of how uncomfortable a ride is, and provides an important indicator for the needs of roadway maintenance or repavement, which is closely tied to the state and federal budget prioritization. As such, accurate and timely monitoring of deteriorating road conditions and following maintenance are essential to improve the overall ride quality on the road. Various technologies, including vehicle\u2010mounted laser profiling systems, have been developed and adopted for road roughness (e.g., IRI\u2014International Roughness Index) measurement; however, their high cost limits their use. While recent advances in smartphone technologies allow us to use their embedded accelerometers for road roughness monitoring, the complicated process of necessary vehicle calibration hinders the widespread use of the technology in the actual practices. In this work, a deep learning IRI estimation method is\u00a0\u2026", "num_citations": "18\n", "authors": ["1077"]}
{"title": "Fraud analysis approaches in the age of big data-A review of state of the art\n", "abstract": " Fraud is a criminal practice for illegitimate gain of wealth or tampering information. Fraudulent activities are of critical concern because of their severe impact on organizations, communities as well as individuals. Over the last few years, various techniques from different areas such as data mining, machine learning, and statistics have been proposed to deal with fraudulent activities. Unfortunately, the conventional approaches display several limitations, which were addressed largely by advanced solutions proposed in the advent of Big Data. In this paper, we present fraud analysis approaches in the context of Big Data. Then, we study the approaches rigorously and identify their limits by exploiting Big Data analytics.", "num_citations": "11\n", "authors": ["1077"]}
{"title": "Advances in machine learning for processing and comparison of metagenomic data\n", "abstract": " Recent advances in next-generation sequencing have enabled high-throughput determination of biological sequences in microbial communities, also known as microbiomes. The large volume of data now presents the challenge of how to extract knowledge\u2013recognize patterns, find similarities, and find relationships\u2013from complex mixtures of nucleic acid sequences currently being examined. In this chapter we review basic concepts as well as state-of-the-art techniques to analyze hundreds of samples which each contain millions of DNA and RNA sequences. We describe the general character of sequence data and some of the processing steps that prepare raw sequence data for inference. We then describe the process of extracting features from the data, in our case assigning taxonomic and gene labels to the sequences. Then we review methods for cross-sample comparisons: 1) using similarity measures and ordination techniques to visualize and measure distances between samples and 2) feature selection and classification to select the most relevant features for discriminating between samples. Finally, in conclusion, we outline some open research problems and challenges left for future research.", "num_citations": "10\n", "authors": ["1077"]}
{"title": "A semi-parallel framework for greedy information-theoretic feature selection\n", "abstract": " Feature selection (FS) is a well-studied area that avoids issues related the curse of dimensionality and overfitting. FS is a preprocessing procedure that identifies the feature subset that is both relevant and non-redundant. Although FS has been driven by the exploration of \u201cbig data\u201d and the development of high-performance computing, the implementation of scalable information-theoretic FS remains an under-explored topic. In this contribution, we revisit the greedy optimization procedure of information-theoretic filter FS and propose a semi-parallel optimizing paradigm that can provide an equivalent feature set as the greedy FS algorithms in a fraction of the time. We focus on greedy selection algorithms due to their larger computational complexity associated with a rapidly growing number of features. Our framework is benchmarked against twelve datasets, including one extremely large dataset that has more than a\u00a0\u2026", "num_citations": "9\n", "authors": ["1077"]}
{"title": "The impact of encoding\u2013decoding schemes and weight normalization in spiking neural networks\n", "abstract": " Abstract Spike-timing Dependent Plasticity (STDP) is a learning mechanism that can capture causal relationships between events. STDP is considered a foundational element of memory and learning in biological neural networks. Previous research efforts endeavored to understand the functionality of STDP\u2019s learning window in spiking neural networks (SNNs). In this study, we investigate the interaction among different encoding/decoding schemes, STDP learning windows and normalization rules for the SNN classifier, trained and tested on MNIST, NIST and ETH80-Contour datasets. The results show that when no normalization rules are applied, classical STDP typically achieves the best performance. Additionally, first-spike decoding classifiers require much less decoding time than a spike count decoding classifier. Thirdly, when no normalization rule is applied, the classifier accuracy decreases as the encoding\u00a0\u2026", "num_citations": "7\n", "authors": ["1077"]}
{"title": "High performance machine learning (HPML) framework to support DDDAS decision support systems: design overview\n", "abstract": " This paper presents a design for a High Performance Machine Learning (HPML) framework to support DDDAS decision processes. The HPML framework can provide a high performance computing environment to implement large scale machine learning algorithms that leverages Big Data tools (e.g., SPARK, Hadoop), parallel algorithms, and MapReduce programming paradigm. The framework provides the following capabilities: \u00b7 High Performance Parallel Algorithms: For a suite of important ML, we will develop three parallel implementations of each algorithm that are based on Message Passing Interface (MPI), Shared Memory (SM) and MapReduce programming model. \u00b7 High Performance and Scalable Platforms: This will enable us to identify the best high performance platform that maximizes performance and scalability of the parallel ML methods. We will experiment with and evaluate the performance and\u00a0\u2026", "num_citations": "7\n", "authors": ["1077"]}
{"title": "A fast information-theoretic approximation of joint mutual information feature selection\n", "abstract": " Feature selection is an important step in data analysis to address the curse of dimensionality. Such dimensionality reduction techniques are particularly important when if a classification is required and the model scales in polynomial time with the size of the feature (e.g., some applications include genomics, life sciences, cyber-security, etc.). Feature selection is the process of finding the minimum subset of features that allows for the maximum predictive power. Many of the state-of-the-art information-theoretic feature selection approaches use a greedy forward search; however, there are concerns with the search in regards to the efficiency and optimality. A unified framework was recently presented for information-theoretic feature selection that tied together many of the works in over the past twenty years. The work showed that joint mutual information maximization (JMI) is generally the best options; however, the\u00a0\u2026", "num_citations": "6\n", "authors": ["1077"]}
{"title": "Speeding up joint mutual information feature selection with an optimization heuristic\n", "abstract": " Feature selection is an important pre-processing stage for nearly all data analysis pipelines that impact applications, such as genomics, life & biomedical sciences, and cyber-security. These application-driven fields rely on feature selection to not only build classifiers but also understand the data by trying to discover knowledge. Furthermore, these dimensionality reduction methods address the curse of dimensionality as well if classification is the end objective. Feature selection is formally defined as the process of optimizing the minimum subset of features that allow for the maximum predictive power. In this work, we address the problem of feature selection by using information theory. Many of the information-theoretic approaches perform a greedy forward optimization, however, the overall efficacy is a concern since most of the existing greedy algorithms only work in a centralized fashion (e.g. single sequential\u00a0\u2026", "num_citations": "6\n", "authors": ["1077"]}
{"title": "Data poisoning attacks against mrmr\n", "abstract": " Many machine learning models lack the consideration that an adversary can alter data at the time of training or testing. Over the past decade, the machine learning models' vulnerability has been a concern and more secure algorithms are needed. Unfortunately, the security of feature selection (FS) remains an under-explored area. There are only a few works that address data poisoning algorithms that are targeted at embedded FS; however, data poisoning techniques targeted at information-theoretic FS do not exist. In this contribution, a novel data poisoning algorithm is proposed that targets failures in minimum Redundancy Maximum Relevance (mRMR) . We demonstrate that mRMR can be easily poisoned to select features that would not normally have been selected.", "num_citations": "5\n", "authors": ["1077"]}
{"title": "Online reconfigurable antenna state selection based on thompson sampling\n", "abstract": " Reconfigurable antennas (RAs) are capable of dynamically and swiftly changing their radiation patterns, which enables them to adapt to channel variations and enhance link capacity. To fully exploit the benefits of RAs, the antenna states need to be optimally selected on-the-fly. The main challenges are two-fold: uncertainty of channel over time, and a large number of candidate antenna states. Previous approaches can only deal with a small number of antenna states, or suffer from slow convergence. In this paper, we propose an optimal online antenna state selection framework for SISO and MISO wireless links, based on the Thompson sampling algorithm for general stochastic bandits. In order to enhance the convergence rate for large antenna state sets, we propose two novel antenna state pruning strategies and integrate them with Thompson sampling, which exploit the relationship between antenna radiation\u00a0\u2026", "num_citations": "5\n", "authors": ["1077"]}
{"title": "Self-supervised correlational monocular depth estimation using ResVGG network\n", "abstract": " Self-supervised monocular depth estimation (SMDE) has recently received significant attention in computer vision. Leveraging the development of deep learning approaches, SMDE provides a solution to the applications of automation, navigation, and scene understanding. In this paper, we propose a novel training objective and learning network to perform a single image depth estimation in our convolutional neural network without the ground truth depth data. The proposed training objective enables the learning network to learn the stereo image correlation in training and estimates the image depth from a single input image in prediction. The proposed learning network ResVGG is a hybrid structure of Resnet50 and VGG-16. The proposed ResVGG has a similar performance as Resnet50 but needs much less computational costs. We demonstrate that our proposed method has competitive accuracy comparing to the current state-of-the-art on KITTI dataset and achieves the frame rates of 32 frame per second (FPS) in prediction using a single NVIDIA GTX 1080 GPU. Furthermore, the proposed method can potentially support visual odometry depth estimation.", "num_citations": "5\n", "authors": ["1077"]}
{"title": "Autonomic management of 3d cardiac simulations\n", "abstract": " Large scale scientific applications in general and especially cardiac simulations experience different execution phases at runtime and each phase has different computational and communication requirements. An optimal solution or numerical scheme for one execution phase might not be appropriate for the next phase of the application execution. We propose an autonomic management framework, which is built on the physics aware programming (PAP) paradigm for accelerating the cardiac simulations further beyond what can be achieved through traditional parallelization efforts. This approach effectively exploits the physical properties of the cardiac simulation by being smart in the development of simulation algorithms. The cardiac simulation phase is periodically monitored and analyzed to identify its current execution phase. We apply machine learning techniques to detect the phase of the simulation during\u00a0\u2026", "num_citations": "5\n", "authors": ["1077"]}
{"title": "Fine tuning lasso in an adversarial environment against gradient attacks\n", "abstract": " Machine learning and data mining algorithms typically assume that the training and testing data are sampled from the same fixed probability distribution; however, this violation is often violated in practice. The field of domain adaptation addresses the situation where this assumption of a fixed probability between the two domains is violated; however, the difference between the two domains (training/source and testing/target) may not be known a priori. There has been a recent thrust in addressing the problem of learning in the presence of an adversary, which we formulate as a problem of domain adaption to build a more robust classifier. This is because the overall security of classifiers and their preprocessing stages have been called into question with the recent findings of adversaries in a learning setting. Adversarial training (and testing) data pose a serious threat to scenarios where an attacker has the opportunity\u00a0\u2026", "num_citations": "5\n", "authors": ["1077"]}
{"title": "Adversarial filters for secure modulation classification\n", "abstract": " Modulation Classification (MC) refers to the problem of classifying the modulation class of a wireless signal. In the wireless communications pipeline, MC is the first operation performed on the received signal and is critical for reliable decoding. This paper considers the problem of secure modulation classification, where a transmitter (Alice) wants to maximize MC accuracy at a legitimate receiver (Bob) while minimizing MC accuracy at an eavesdropper (Eve). The contribution of this work is to design novel adversarial learning techniques for secure MC. In particular, we present adversarial filtering based algorithms for secure MC, in which Alice uses a carefully designed adversarial filter to mask the transmitted signal, that can maximize MC accuracy at Bob while minimizing MC accuracy at Eve. We present two filtering based algorithms, namely gradient ascent filter (GAF), and a fast gradient filter method (FGFM), with varying levels of complexity. Our proposed adversarial filtering based approaches significantly outperform additive adversarial perturbations (used in the traditional ML community and other prior works on secure MC) and also have several other desirable properties. In particular, GAF and FGFM algorithms are a) computational efficient (allow fast decoding at Bob), b) power-efficient (do not require excessive transmit power at Alice); and c) SNR efficient (i.e., perform well even at low SNR values at Bob).", "num_citations": "4\n", "authors": ["1077"]}
{"title": "Fraud data analytics tools and techniques in big data era\n", "abstract": " Fraudulent activities (e.g., suspicious credit card transaction, financial reporting fraud, and money laundering) are critical concerns to various entities including bank, insurance companies, and public service organizations. Typically, these activities lead to detrimental effects on the victims such as a financial loss. Over the years, fraud analysis techniques underwent a rigorous development. However, lately, the advent of Big data led to vigorous advancement of these techniques since Big Data resulted in extensive opportunities to combat financial frauds. Given that the massive amount of data that investigators need to sift through, massive volumes of data integrated from multiple heterogeneous sources (e.g., social media, blogs) to find fraudulent patterns is emerging as a feasible approach.", "num_citations": "4\n", "authors": ["1077"]}
{"title": "The akron-kalman filter for tracking time-varying networks\n", "abstract": " We propose the AKRON-Kalman filter for the problem of inferring sparse dynamic networks from a noisy undersampled set of measurements. Unlike the Lasso-Kalman filter, which uses regularization with the l 1 -norm to find an approximate sparse solution, the AKRON-Kalman tracker uses the l 1  approximation to find the location of a \u201csufficient number\u201d of zero entries that guarantees the existence of the optimal sparsest solution. This sufficient number of zeros can be shown to be exactly equal to the dimension of the kernel of an under-determined system. The AKRON-Kalman tracker then iteratively refines this solution of the l 1  problem by ensuring that the observed reconstruction error does not exceed the measurement noise level. The AKRON solution is sparser, by construction, than the Lasso solution while the Kalman tracking ensures that all past observations are taken into account to estimate the network in\u00a0\u2026", "num_citations": "4\n", "authors": ["1077"]}
{"title": "Incremental learning of concept drift from imbalanced data\n", "abstract": " Learning data sampled from a nonstationary distribution has been shown to be a very challenging problem in machine learning, because the joint probability distribution between the data and classes evolve over time. Thus learners must adapt their knowledge base, including their structure or parameters, to remain as strong predictors. This phenomenon of learning from an evolving data source is akin to learning how to play a game while the rules of the game are changed, and it is traditionally referred to as learning concept drift. Climate data, financial data, epidemiological data, spam detection are examples of applications that give rise to concept drift problems. An additional challenge arises when the classes to be learned are not represented (approximately) equally in the training data, as most machine learning algorithms work well only when the class distributions are balanced. However, rare categories are commonly faced in real-world applications, which leads to skewed or imbalanced datasets. Fraud detection, rare disease diagnosis, anomaly detection are examples of applications that feature imbalanced datasets, where data from category are severely underrepresented. Concept drift and class imbalance are traditionally addressed separately in machine learning, yet data streams can experience both phenomena. This work introduces Learn++. NIE (nonstationary & imbalanced environments) and Learn++. CDS (concept drift with SMOTE) as two new members of the Learn++ family of incremental learning algorithms that explicitly and simultaneously address the aforementioned phenomena. The former addresses concept drift and\u00a0\u2026", "num_citations": "4\n", "authors": ["1077"]}
{"title": "AKRON: An algorithm for approximating sparse kernel reconstruction\n", "abstract": " Exact reconstruction of a sparse signal for an under-determined linear system using the \u21130-measure is, in general, an NP-hard problem. The most popular approach is to relax the \u21130-optimization problem to an \u21131-approximation. However, the strength of this convex approximation relies upon rigid properties on the system, which are not verifiable in practice. Greedy algorithms have been proposed in the past to speed up the optimization of the \u21131 problem, but their computational efficiency comes at the expense of a larger error. In an effort to control error and complexity, this paper goes beyond the \u21131-approximation by growing neighborhoods of the \u21131-solution that moves towards the optimal solution. The size of the neighborhood is tunable depending on the computational resources. The proposed algorithm, termed Approximate Kernel RecONstruction (AKRON), yields significantly smaller errors than current greedy\u00a0\u2026", "num_citations": "3\n", "authors": ["1077"]}
{"title": "Analysis methods for shotgun metagenomics\n", "abstract": " The development of whole metagenome shotgun sequencing (WGS) has enabled the precise characterization of taxonomic diversity and functional capabilities of microbial communities in situ while obviating organism isolation and cultivation procedures. WGS created with second- and third-generation sequencing technologies will generate millions of reads and tens (or hundreds) of gigabytes of information about the organisms under investigation. Despite containing an immense amount of information, the reads are unorganized and unlabeled, leading to a significant challenge in discerning from which genome a read originated. Thus, analysis of WGS data necessitates first determining community structure and function from the raw reads before the focus can shift to making multi-sample comparisons. A typical WGS workflow consists of read assignment (taxonomic binning and classification\u00a0\u2026", "num_citations": "3\n", "authors": ["1077"]}
{"title": "Scalable subset selection with filters and its applications\n", "abstract": " Increasingly many applications of machine learning are encountering large data that were almost unimaginable just a few years ago, and hence, many of the current algorithms cannot handle, ie, do not scale to, today's extremely large volumes of data. The data are made up of a large set of features describing each observation, and the complexity of the models for making predictions tend to increase not only with the number of observations, but also the number of features. Fortunately, not all of the features that make up the data carry meaningful information about making the predictions. Thus irrelevant features should be filtered from the data prior to building a model. Such a process of removing features to produce a subset is commonly referred to as feature subset selection. In this work, we present two new filter-based feature subset selection algorithms that are scalable to large data sets that address:(i) potentially\u00a0\u2026", "num_citations": "3\n", "authors": ["1077"]}
{"title": "Data poisoning against information-theoretic feature selection\n", "abstract": " A typical assumption made in machine learning is that a learning model does not consider an adversary\u2019s existence that can subvert a classifier\u2019s objective. As a result, machine learning pipelines exhibit vulnerabilities in an adversarial environment. Feature Selection (FS) is an essential preprocessing stage in data analytics and has been widely used in security-sensitive machine learning applications; however, FS research in adversarial machine learning has been largely overlooked. Recently, empirical works demonstrated that the FS is also vulnerable in an adversarial environment. In the past decade, although the research community has made extensive efforts to promote the classifiers\u2019 robustness and develop countermeasures against adversaries, only a few contributions investigated FS\u2019s behavior in a malicious environment. Given that machine learning pipelines increasingly rely on FS to combat the \u201ccurse\u00a0\u2026", "num_citations": "2\n", "authors": ["1077"]}
{"title": "Detecting adversarial audio via activation quantization error\n", "abstract": " The robustness and vulnerability of Deep Neural Networks (DNN) are quickly becoming a critical area of interest since these models are in widespread use across real-world applications (i.e., image and audio analysis, recommendation system, natural language analysis, etc.). A DNN\u2019s vulnerability is exploited by an adversary to generate data to attack the model; however, the majority of adversarial data generators have focused on image domains with far fewer work on audio domains. More recently, audio analysis models were shown to be vulnerable to adversarial audio examples (e.g., speech command classification, automatic speech recognition, etc.). Thus, one urgent open problem is to detect adversarial audio reliably. In this contribution, we incorporate a separate and yet related DNN technique to detect adversarial audio, namely model quantization. Then we propose an algorithm to detect adversarial\u00a0\u2026", "num_citations": "2\n", "authors": ["1077"]}
{"title": "Nonlinear Brain Tumor Model Estimation with Long Short-Term Memory Neural Networks\n", "abstract": " Gliomas are malignant brain tumors that are associated with high neurological morbidity and poor outcomes. Patients diagnosed with low-grade gliomas are typically followed by a sequence of measurements of the tumor size. Here, we show the promise of Long Short-Term Memory Neural Networks (LSTMs) to address two important clinical questions in low-grade gliomas: 1) classification and prediction of future behavior; and 2) early detection of dedifferentiation to a higher grade or more aggressive growth. We use a system of partial differential equations (PDEs), from our earlier work, to generate simulated growth of low-grade gliomas with different clinical parameters. We design an LSTM network to solve the inverse problem of PDE parameter estimation. We find that accuracy increases as a function of the number of tumor measurements and perplexity can also be used to detect a change in tumor grade. These\u00a0\u2026", "num_citations": "2\n", "authors": ["1077"]}
{"title": "A self-protection agent using error correcting output codes to secure computers and applications\n", "abstract": " The human immune system is incredibly efficient at identifying self- and non-self entities in our bodies. A non-self entity (malicious), once identified, is attacked by particular types of cells to remove the intruder before it can cause damage. Our immune system has components that identify not only non-self entities but also recall old entities that may not have been encountered for a very long time, but it is still essential that these entities be correctly classified as malicious. The domain of cybersecurity can significantly benefit from having a framework that can identify, react and adapt to malicious behaviors. Such a model for cyber protection should draw a parallel to our immune system, at least at a high level. In this work, we present a flexible framework that leverages machine learning to identify malicious behaviors that are threats to users, computers, and applications in a network. The proposed framework relies on the\u00a0\u2026", "num_citations": "2\n", "authors": ["1077"]}
{"title": "A study of an incremental spectral meta-learner for nonstationary environments\n", "abstract": " Incrementally learning from large volumes of streaming data over time is a problem that is of crucial importance to the computational intelligence community, especially in scenarios where it is impractical or simply unfeasible to store all historical data. Learning becomes a particularly challenging problem when the probabilistic properties of the data are changing with time (i.e., gradual, abrupt, etc.), and there is scarce availability of class labels. Many existing strategies for learning in nonstationary environments use the most recent batch of training data to tune their parameters (e.g., calculate classifier voting weights), and never reassess these parameters when the unlabeled test data arrive. Making a limited drift assumption is generally one way to justify not needing to re-evaluate the parameters of a classifiers; however, labeled data that have already been learned if presented to the classifier for testing could be\u00a0\u2026", "num_citations": "2\n", "authors": ["1077"]}
{"title": "Edge-Guided Occlusion Fading Reduction for a Light-Weighted Self-Supervised Monocular Depth Estimation\n", "abstract": " Self-supervised monocular depth estimation methods generally suffer the occlusion fading issue due to the lack of supervision by the per pixel ground truth. Although a post-processing method was proposed by Godard et. al. to reduce the occlusion fading, the compensated results have a severe halo effect. In this paper, we propose a novel Edge-Guided post-processing to reduce the occlusion fading issue for self-supervised monocular depth estimation. We further introduce Atrous Spatial Pyramid Pooling (ASPP) into the network to reduce the computational costs and improve the inference performance. The proposed ASPP-based network is lighter, faster, and better than current commonly used depth estimation networks. This light-weight network only needs 8.1 million parameters and can achieve up to 40 frames per second for  input in the inference stage using a single nVIDIA GTX1080 GPU. The proposed network also outperforms the current state-of-the-art on the KITTI benchmarks. The ASPP-based network and Edge-Guided post-processing produce better results either quantitatively and qualitatively than the competitors.", "num_citations": "1\n", "authors": ["1077"]}
{"title": "Malicious HTML File Prediction: A Detection and Classification Perspective with Noisy Data\n", "abstract": " Cybersecurity plays a critical role in protecting sensitive information and the structural integrity of networked systems. As networked systems continue to expand in numbers as well as in complexity, so does the threat of malicious activity and the necessity for advanced cybersecurity solutions. Furthermore, both the quantity and quality of available data on malicious content as well as the fact that malicious activity continuously evolves makes automated protection systems for this type of environment particularly challenging. Not only is the data quality a concern, but the volume of the data can be quite small for some of the classes. This creates a class imbalance in the data used to train a classifier; however, many classifiers are not well equipped to deal with class imbalance. One such example is detecting malicious HMTL files from static features. Unfortunately, collecting malicious HMTL files is extremely difficult and\u00a0\u2026", "num_citations": "1\n", "authors": ["1077"]}
{"title": "Feature subset selection for inferring relative importance of taxonomy\n", "abstract": " Examining the bacterial or functional differences between multiple habitats/populations/phenotypes plays an important role in making inferences about the roles that the taxonomy and functional profiles can take on in microbial ecology. It is therefore important to the field of comparative metagenomics, using \u03b1-& \u03b2-diversity, that methods or algorithms can detect the importance of particular subsets of variables that best differentiate the multiple phenotypes in the data. Given todays genomic data deluge efficient methods that can carry out these inferences cannot be understated enough. We assume observations are collected from a multitude of different environments (eg, males vs. females, control vs. stimulus, etc.), and each observation is comprised of hundreds or thousands of different taxa/functional features (ie, 16S or whole genome shotgun). Our goal in this work is to examine the role, assumptions, and\u00a0\u2026", "num_citations": "1\n", "authors": ["1077"]}
{"title": "Variable Selection to Improve Classification of Metagenomes\n", "abstract": " Metagenomics is the study of DNA extracted from the microbial communities in an environment, in comparison to traditional genomics, which studies the nucleic acids from single organisms (Wooley et al., 2010). In a metagenomic study, a sample is collected directly from the environment, which can be a gram of soil (Rousk et al., 2010; Bowers et al., 2011), milliliter of ocean (Williamson et al., 2008), swab from an object (Caporaso et al., 2011), or a sample of the microbes associated with a host organism, such as humans (Caporaso et al., 2011; Costello et al., 2009). The microbial content of an environmental sample is termed its \u201cmicrobiome\u201d. There are several questions that are of particular importance when the microbiome is being examined. In particular, who is there, how much of each species is there, and what are they doing overall? Some of these questions can be addressed using DNA/RNA sequencing followed by homology and taxonomic classification; however, usually hypotheses focus on answering: which organisms and/or their functions (eg, metabolisms) best differentiate multiple phenotypes in a collection of samples? Consider a collection of gut microbiome samples that were collected from patients with inflammatory bowel disease (IBD), and a control set that do not have IBD. A natural question to ask when examining the differences between the gut microbiomes of the two phenotypes is what organisms or genes can distinguish patients with IBD and healthy controls? Knowing the answers to such question can be useful in developing a better understanding about a disease, and aid in developing medicines to target a disease\u00a0\u2026", "num_citations": "1\n", "authors": ["1077"]}