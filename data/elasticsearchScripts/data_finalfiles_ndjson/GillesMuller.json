{"title": "Entropy: a consolidation manager for clusters\n", "abstract": " Clusters provide powerful computing environments, but in practice much of this power goes to waste, due to the static allocation of tasks to nodes, regardless of their changing computational requirements. Dynamic consolidation is an approach that migrates tasks within a cluster as their computational requirements change, both to reduce the number of nodes that need to be active and to eliminate temporary overload situations. Previous dynamic consolidation strategies have relied on task placement heuristics that use only local optimization and typically do not take migration overhead into account. However, heuristics based on only local optimization may miss the globally optimal solution, resulting in unnecessary resource usage, and the overhead for migration may nullify the benefits of consolidation.", "num_citations": "690\n", "authors": ["518"]}
{"title": "Think: A software framework for component-based operating system kernels\n", "abstract": " Building a flexible kernel from components is a promising solution for supporting various embedded systems. The use of components encourages code re-use and reduces development time. Flexibility permits the system to be configured at various stages of the design, up to run time. In this paper, we propose a software framework, called Think, for implementing operating system kernels from components of arbitrary sizes. A unique feature of Think is that it provides a uniform and highly flexible binding model to help OS architects assemble operating system components in varied ways. An OS architect can build an OS kernel from components using Think without being forced into a predefined kernel design (eg exo-kernel, micro-kernel or classical OS kernel). To evaluate the Think framework, we have implemented Kortex, a library of commonly used kernel components. We have used Kortex to implement several\u00a0\u2026", "num_citations": "288\n", "authors": ["518"]}
{"title": "Faults in Linux: Ten years later\n", "abstract": " In 2001, Chou et al. published a study of faults found by applying a static analyzer to Linux versions 1.0 through 2.4. 1. A major result of their work was that the drivers directory contained up to 7 times more of certain kinds of faults than other directories. This result inspired a number of development and research efforts on improving the reliability of driver code. Today Linux is used in a much wider range of environments, provides a much wider range of services, and has adopted a new development and release model. What has been the impact of these changes on code quality? Are drivers still a major problem?", "num_citations": "279\n", "authors": ["518"]}
{"title": "Documenting and automating collateral evolutions in Linux device drivers\n", "abstract": " The internal libraries of Linux are evolving rapidly, to address new requirements and improve performance. These evolutions, however, entail a massive problem of collateral evolution in Linux device drivers: for every change that affects an API, all dependent drivers must be updated accordingly. Manually performing such collateral evolutions is time-consuming and unreliable, and has lead to errors when modifications have not been done consistently. In this paper, we present an automatic program transformation tool Coccinelle, for documenting and automating device driver collateral evolutions. Because Linux programmers are accustomed to manipulating program modifications in terms of patch files, this tool uses a language based on the patch syntax to express transformations, extending patches to semantic patches. Coccinelle preserves the coding style of the original driver, as would a human programmer\u00a0\u2026", "num_citations": "276\n", "authors": ["518"]}
{"title": "Remote Core Locking: Migrating Critical-Section Execution to Improve the Performance of Multithreaded Applications\n", "abstract": " The scalability of multithreaded applications on current multicore systems is hampered by the performance of lock algorithms, due to the costs of access contention and cache misses. In this paper, we propose a new lock algorithm, Remote Core Locking (RCL), that aims to improve the performance of critical sections in legacy applications on multicore architectures. The idea of RCL is to replace lock acquisitions by optimized remote procedure calls to a dedicated server core. RCL limits the performance collapse observed with other lock algorithms when many threads try to acquire a lock concurrently and removes the need to transfer lock-protected shared data to the core acquiring the lock because such data can typically remain in the server core\u2019s cache.", "num_citations": "189\n", "authors": ["518"]}
{"title": "Harissa: a Flexible and Efficient Java Environment Mixing Bytecode and Compiled Code\n", "abstract": " The Java language provides a promising solution to the design of safe programs, with an application spectrum ranging from Web services to operating system components. The well-known tradeo of Java's portability is the ine ciency of its basic execution model, which relies on the interpretation of an object-based virtual machine. Many solutions have been proposed to overcome this problem, such as just-in-time (JIT) and oline bytecode compilers. However, most compilers trade e ciency for either portability or the ability to dynamically load bytecode. In this paper, we present an approach which reconciles portability and e ciency, and preserves the ability to dynamically load bytecode. We have designed and implemented an e cient environment for the execution of Java programs, named Harissa. Harissa permits the mixing of compiled and interpreted methods. Harissa's compiler translates Java bytecode to C, incorporating aggressive optimizations such as virtualmethod call optimization based on the Class Hierarchy Analysis. To evaluate the performance of Harissa, we have conducted an extensive experimental study aimed at comparing the various existing alternatives to execute Java programs. The C code produced by Harissa's compiler is more e cient than all other alternative ways of executing Java programs (that were available to us): it is up to 140 times faster than the JDK interpreter, up to 13 times faster than the Softway Guava JIT, and 30% faster than the Toba bytecode to C compiler.", "num_citations": "184\n", "authors": ["518"]}
{"title": "Devil: An IDL for hardware programming\n", "abstract": " To keep up with the frantic pace at which devices come out, drivers need to be quickly developed, debugged and tested. Although a driver is a critical system component, the driver development process has made little (if any) progress. The situation is particularly disastrous when considering the hardware operating code (ie, the layer interacting with the device). Writing this code often relies on inaccurate or incomplete device documentation and involves assembly-level operations. As a result, hardware operating code is tedious to write, prone to errors, and hard to debug and maintain.", "num_citations": "178\n", "authors": ["518"]}
{"title": "Tempo: Specializing systems applications and beyond\n", "abstract": " Permission to make digital/hard copy of part or all of this work for personal or classroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title of the publication, and its date appear, and notice is given that copying is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.", "num_citations": "152\n", "authors": ["518"]}
{"title": "Java bytecode compression for low-end embedded systems\n", "abstract": " A program executing on a low-end embedded system, such as a smart-card, faces scarce memory resources and fixed execution time constraints. We demonstrate that factorization of common instruction sequences in Java bytecode allows the memory footprint to be reduced, on average, to 85% of its original size, with a minimal execution time penalty. While preserving Java compatibility, our solution requires only a few modifications which are straightforward to implement in any JVM used in a low-end embedded system.", "num_citations": "126\n", "authors": ["518"]}
{"title": "Specialization tools and techniques for systematic optimization of system software\n", "abstract": " Specialization has been recognized as a powerful technique for optimizing operating systems. However, specialization has not been broadly applied beyond the research community because current techniques based on manual specialization, are time-consuming and error-prone. The goal of the work described in this paper is to help operating system tuners perform specialization more easily. We have built a specialization toolkit that assists the major tasks of specializing operating systems. We demonstrate the effectiveness of the toolkit by applying it to three diverse operating system components. We show that using tools to assist specialization enables significant performance optimizations without error-prone manual modifications. Our experience with the toolkit suggests new ways of designing systems that combine high performance and clean structure.", "num_citations": "121\n", "authors": ["518"]}
{"title": "Declarative specialization of object-oriented programs\n", "abstract": " Designing and implementing generic software components is encouraged by languages such as object-oriented ones and commonly advocated in most application areas. Generic software components have many advantages among which the most important is reusability. However, it comes at a price: genericity often incurs a loss of efficiency. This paper presents an approach aimed at reconciling genericity and efficiency. To do so, we introduce declarations to the Java language to enable a programmer to specify how generic programs should be specialized for a particular usage pattern. Our approach has been implemented as a compiler from our extended language into standard Java.", "num_citations": "110\n", "authors": ["518"]}
{"title": "Understanding collateral evolution in Linux device drivers\n", "abstract": " In a modern operating system (OS), device drivers can make up over 70% of the source code. Driver code is also heavily dependent on the rest of the OS, for functions and data structures defined in the kernel and driver support libraries. These properties pose a significant problem for OS evolution, as any changes in the interfaces exported by the kernel and driver support libraries can trigger a large number of adjustments in dependent drivers. These adjustments, which we refer to as collateral evolutions, may be complex, entailing substantial code reorganizations. As to our knowledge there exist no tools to help in this process, collateral evolution is thus time consuming and error prone. In this paper, we present a qualitative and quantitative assessment of collateral evolution in Linux device driver code. We provide a taxonomy of evolutions and collateral evolutions, and use an automated patch-analysis tool that we\u00a0\u2026", "num_citations": "105\n", "authors": ["518"]}
{"title": "Towards automatic specialization of Java programs\n", "abstract": " Automatic program specialization can derive effcient implementations from generic components, thus reconciling the often opposing goals of genericity and efficiency. This technique has proved useful within the domains of imperative, functional, and logical languages, but so far has not been explored within the domain of object-oriented languages. We present experiments in the specialization of Java programs. We demonstrate how to construct a program specializer for Java programs from an existing specializer for C programs and a Java-to-C compiler. Specialization is managed using a declarative approach that abstracts over the optimization process and masks implementation details. Our experiments show that program specialization provides a four-time speedup of an image-filtering program. Based on these experiments, we identify optimizations of object-oriented programs that can be carried out by\u00a0\u2026", "num_citations": "105\n", "authors": ["518"]}
{"title": "Fast, optimized Sun RPC using automatic program specialization\n", "abstract": " Fast remote procedure call (RPC) is a major concern for distributed systems. Many studies aimed at efficient RPC consist of either new implementations of the RPC paradigm or manual optimization of critical sections of the code. This paper presents an experiment that achieves automatic optimization of an existing, commercial RPC implementation, namely the Sun RPC. The optimized Sun RPC is obtained by using an automatic program specializer. It runs up to 1.5 times faster than the original Sun RPC. Close examination of the specialized code does not reveal further optimization opportunities which would lead to significant improvements without major manual restructuring. The contributions of this work are: the optimized code is safely produced by an automatic tool and thus does not entail any additional maintenance; to the best of our knowledge this is the first successful specialization of mature, commercial\u00a0\u2026", "num_citations": "95\n", "authors": ["518"]}
{"title": "Scheduling support for transactional memory contention management\n", "abstract": " Transactional Memory (TM) is considered as one of the most promising paradigms for developing concurrent applications. TM has been shown to scale well on >multiple cores when the data access pattern behaves \"well,\" i.e., when few conflicts are induced. In contrast, data patterns with frequent write sharing, with long transactions, or when many threads contend for a smaller number of cores, result in numerous conflicts. Until recently, TM implementations had little control of transactional threads, which remained under the supervision of the kernel's transaction-ignorant scheduler. Conflicts are thus traditionally resolved by consulting an STM-level contention manager. Consequently, the contention managers of these \"conventional\" TM implementations suffer from a lack of precision and often fail to ensure reasonable performance in high-contention workloads. Recently, scheduling-based TM contention\u00a0\u2026", "num_citations": "87\n", "authors": ["518"]}
{"title": "I-JVM: a Java virtual machine for component isolation in OSGi\n", "abstract": " The OSGi framework is a Java-based, centralized, component oriented platform. It is being widely adopted as an execution environment for the development of extensible applications. However, current Java Virtual Machines are unable to isolate components from each other. For instance, a malicious component can freeze the complete platform by allocating too much memory or alter the behavior of other components by modifying shared variables. This paper presents I-JVM, a Java Virtual Machine that provides a lightweight approach to isolation while preserving compatibility with legacy OSGi applications. Our evaluation of I-JVM shows that it solves the 8 known OSGi vulnerabilities that are due to the Java Virtual Machine and that the overhead of I-JVM compared to the JVM on which it is based is below 20%.", "num_citations": "84\n", "authors": ["518"]}
{"title": "Safe and efficient active network programming\n", "abstract": " Active networks are aimed at incorporating programmability into the network to achieve extensibility. One approach to obtaining extensibility is to download router programs into network nodes. This programmability is critical to allow multipoint distributed systems to adapt to network conditions and individual clients' needs. Although promising, this approach raises critical issues such as safety to achieve reliability despite the complexity of a distributed system, security to protect shared resources, and efficiency to maximize usage of bandwidth. This paper proposes the use of a domain-specific language, PLAN-P, to address all of the above issues. To address safety and security, we give examples of properties of PLAN-P programs that can be automatically checked due to the use of a restricted language. For efficiency, we show that an automatically generated run-time compiler for PLAN-P produces code which\u00a0\u2026", "num_citations": "83\n", "authors": ["518"]}
{"title": "A foundation for flow-based program matching: using temporal logic and model checking\n", "abstract": " Reasoning about program control-flow paths is an important functionality of a number of recent program matching languages and associated searching and transformation tools. Temporal logic provides a well-defined means of expressing properties of control-flow paths in programs, and indeed an extension of the temporal logic CTL has been applied to the problem of specifying and verifying the transformations commonly performed by optimizing compilers. Nevertheless, in developing the Coccinelle program transformation tool for performing Linux collateral evolutions in systems code, we have found that existing variants of CTL do not adequately support rules that transform subterms other than the ones matching an entire formula. Being able to transform any of the subterms of a matched term seems essential in the domain targeted by Coccinelle.", "num_citations": "80\n", "authors": ["518"]}
{"title": "WYSIWIB: A declarative approach to finding API protocols and bugs in Linux code\n", "abstract": " Eliminating OS bugs is essential to ensuring the reliability of infrastructures ranging from embedded systems to servers. Several tools based on static analysis have been proposed for finding bugs in OS code. They have, however, emphasized scalability over usability, making it difficult to focus the tools on specific kinds of bugs and to relate the results to patterns in the source code. We propose a declarative approach to bug finding in Linux OS code using a control-flow based program search engine. Our approach is WYSIWIB (What You See Is Where It Bugs), since the programmer expresses specifications for bug finding using a syntax close to that of ordinary C code. The key advantage of our approach is that search specifications can be easily tailored, to eliminate false positives or catch more bugs. We present three case studies that have allowed us to find hundreds of potential bugs.", "num_citations": "76\n", "authors": ["518"]}
{"title": "VMKit: a substrate for managed runtime environments\n", "abstract": " Managed Runtime Environments (MREs), such as the JVM and the CLI, form an attractive environment for program execution, by providing portability and safety, via the use of a bytecode language and automatic memory management, as well as good performance, via just-in-time (JIT) compilation. Nevertheless, developing a fully featured MRE, including e.g. a garbage collector and JIT compiler, is a herculean task. As a result, new languages cannot easily take advantage of the benefits of MREs, and it is difficult to experiment with extensions of existing MRE based languages. This paper describes and evaluates VMKit, a first attempt to build a common substrate that eases the development of high-level MREs. We have successfully used VMKit to build two MREs: a Java Virtual Machine and a Common Language Runtime. We provide an extensive study of the lessons learned in developing this infrastructure, and\u00a0\u2026", "num_citations": "75\n", "authors": ["518"]}
{"title": "The performance of consistent checkpointing in distributed shared memory systems\n", "abstract": " This paper presents the design and implementation of a consistent checkpointing scheme for distributed shared memory (DSM) systems. Our approach relies on the integration of checkpoints within synchronization barriers already existing in applications; this avoids the need to introduce an additional synchronization mechanism. The main advantage of our checkpointing mechanism is that performance degradation arises only when a checkpoint is being taken; hence, the programmer can adjust the trade-off between the cost of checkpointing and the cost of longer rollbacks by adjusting the time between two successive checkpoints. The paper compares several implementations of the proposed consistent checkpointing mechanism (incremental, non-blocking, and pre-flushing) on the Intel Paragon multicomputer for several parallel scientific applications. Performance measures show that a careful optimization of the\u00a0\u2026", "num_citations": "74\n", "authors": ["518"]}
{"title": "Hector: Detecting resource-release omission faults in error-handling code for systems software\n", "abstract": " Omitting resource-release operations in systems error handling code can lead to memory leaks, crashes, and deadlocks. Finding omission faults is challenging due to the difficulty of reproducing system errors, the diversity of system resources, and the lack of appropriate abstractions in the C language. To address these issues, numerous approaches have been proposed that globally scan a code base for common resource-release operations. Such macroscopic approaches are notorious for their many false positives, while also leaving many faults undetected. We propose a novel microscopic approach to finding resource-release omission faults in systems software. Rather than generalizing from the entire source code, our approach focuses on the error-handling code of each function. Using our tool, Hector, we have found over 370 faults in six systems software projects, including Linux, with a 23% false positive\u00a0\u2026", "num_citations": "67\n", "authors": ["518"]}
{"title": "Bossa: a language-based approach to the design of real-time schedulers\n", "abstract": " CiNii \u8ad6\u6587 - A Language-based Approach for the Design of Real Time Schedulers CiNii \u56fd\u7acb \u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e \u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 A Language-based Approach for the Design of Real Time Schedulers BARRETO L. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 BARRETO L. \u53ce\u9332\u520a\u884c\u7269 Proc. 10th International Conference on Real-Time Systems (RTS'2002) Proc. 10th International Conference on Real-Time Systems (RTS'2002), 2002 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u30dd\u30ea\u30b7\u968e\u5c64\u5316\u65b9\u5f0f\u306b\u3088\u308b\u30b9\u30c8\u30ec\u30fc\u30b8\u30b7\u30b9\u30c6\u30e0\u306e \u5b9f\u73fe\u3068\u8a55\u4fa1 \u5c0f\u67f3 \u9806\u88d5 , \u7530\u80e1 \u548c\u54c9 , \u5e02\u6751 \u54f2 , \u677e\u4e0b \u6e29 \u60c5\u5831\u51e6\u7406\u5b66\u4f1a\u8ad6\u6587\u8a8c\u30b3\u30f3\u30d4\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u30b7\u30b9\u30c6\u30e0 \uff08ACS\uff09 48(SIG8(ACS18)), 179-191, 2007-05-15 \u53c2\u8003\u6587\u732e20\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(\u2026", "num_citations": "65\n", "authors": ["518"]}
{"title": "Web cache prefetching as an aspect: towards a dynamic-weaving based solution\n", "abstract": " Given the high proportion of HTTP traffic in the Internet, Web caches are crucial to reduce user access time, network latency, and bandwidth consumption. Prefetching in a Web cache can further enhance these benefits. For the best performance, however, the prefetching policy must match user and Web application characteristics. Thus, new prefetching policies must be loaded dynamically as needs change. Most Web caches are large C programs, and thus adding one or more prefetching policies to an existing Web cache is a daunting task. The main problem is that prefetching concerns crosscut the cache structure. Aspect-oriented programming is a natural technique to address this issue. Nevertheless, existing approaches either do not provide dynamic weaving, incur a high overhead for invocation of dynamically loaded code, or do not target C applications. In this paper we present \u03bc-Dyner, which addresses\u00a0\u2026", "num_citations": "63\n", "authors": ["518"]}
{"title": "Harissa: A hybrid approach to Java execution\n", "abstract": " Java provides portability and safety but falls short on efficiency. To resolve this problem, the authors developed Harissa, an execution environment that offers efficiency without sacrificing portability or dynamic class loading.", "num_citations": "59\n", "authors": ["518"]}
{"title": "Scaling up partial evaluation for optimizing the Sun commercial RPC protocol\n", "abstract": " We report here a successful experiment in using partial evaluation on a realistic program, namely the Sun commercial RPC (Remote Procedure Call) protocol. The Sun RPC is implemented in a highly generic way that offers multiple opportunities of specialization.Our study also shows the incapacity of traditional binding-time analyses to treat real system programs. Our experiment has been made with Tempo, a partial evaluator for C programs targeted towards system software. Tempo's binding-time analysis had to be improved to integrate partially static data structures (interprocedurally), context sensitivity, use sensitivity and return sensitivity.The Sun RPC experiment files, including the specialized implementation, are publicly available upon request to the authors.", "num_citations": "58\n", "authors": ["518"]}
{"title": "Finding Error Handling Bugs in OpenSSL Using Coccinelle\n", "abstract": " OpenSSL is a library providing various functionalities relating to secure network communication. Detecting and fixing bugs in OpenSSL code is thus essential, particularly when such bugs can lead to malicious attacks. In previous work, we have proposed a methodology for finding API usage protocols in Linux kernel code using the program matching and transformation engine Coccinelle. In this work, we report on our experience in applying this methodology to OpenSSL, focusing on API usage protocols related to error handling. We have detected over 30 bugs in a recent OpenSSL snapshot, and in many cases it was possible to correct the bugs automatically. Our patches correcting these bugs have been accepted by the OpenSSL developers. This work furthermore confirms the applicability of our methodology to user-level code.", "num_citations": "54\n", "authors": ["518"]}
{"title": "Efficient incremental checkpointing of Java programs\n", "abstract": " We investigate the optimization of language-level checkpointing of Java programs. First, we describe how to systematically associate incremental checkpoints with Java classes. While being safe, the genericness of this solution induces substantial execution overhead. Second, to solve the dilemma of genericness versus performance, we use automatic program specialization to transform the generic checkpointing methods into highly optimized ones. Specialization exploits two kinds of information: structural properties about the program classes; knowledge of unmodified data structures in specific program phases. The latter information allows us to generate phase-specific checkpointing methods. We evaluate our approach on two benchmarks, a realistic application which consists of a program analysis engine, and a synthetic program which can serve as a metric. Specialization gives a speedup proportional to the\u00a0\u2026", "num_citations": "54\n", "authors": ["518"]}
{"title": "A declarative approach for designing and developing adaptive components\n", "abstract": " An adaptive component is a component that is able to adapt its behavior to different execution contexts. Building an adaptive application is difficult because of component dependencies and the lack of language support. As a result, code that implements adaptation is often tangled, hindering maintenance and evolution. To overcome this problem, we propose a declarative approach to program adaptation. This approach makes the specific issues of adaptation explicit. The programmer can focus on the basic features of the application, and separately provide clear and concise adaptation information. Concretely, we propose adaptation classes, which enrich Java classes with adaptive behaviors. A dedicated compiler automatically generates Java code that implements the adaptive features. Moreover, these adaptation declarations can be checked for consistency to provide additional safety guarantees. As a working\u00a0\u2026", "num_citations": "53\n", "authors": ["518"]}
{"title": "Ensuring data security and integrity with a fast stable storage\n", "abstract": " An active, RAM-based stable storage board is described. The stable storage board is intended to make it relatively fast to access data structures, and contains features to guard against incorrect access from a faulty processor or from an errant program. Basic performance data are presented and potential applications are described.<>", "num_citations": "53\n", "authors": ["518"]}
{"title": "Static and dynamic program compilation by interpreter specialization\n", "abstract": " Interpretation and run-time compilation techniques are increasingly important because they can support heterogeneous architectures, evolving programming languages, and dynamically-loaded code. Interpretation is simple to implement, but yields poor performance. Run-time compilation yields better performance, but is costly to implement. One way to preserve simplicity but obtain good performance is to apply program specialization to an interpreter in order to generate an efficient implementation of the program automatically. Such specialization can be carried out at both compile time and run time.               Recent advances in program-specialization technology have significantly improved the performance of specialized interpreters. This paper presents and assesses experiments applying program specialization to both bytecode and structured-language interpreters. The results show that for some general\u00a0\u2026", "num_citations": "52\n", "authors": ["518"]}
{"title": "Continuously measuring critical section pressure with the free-lunch profiler\n", "abstract": " Today, Java is regularly used to implement large multi-threaded server-class applications that use locks to protect access to shared data. However, understanding the impact of locks on the performance of a system is complex, and thus the use of locks can impede the progress of threads on configurations that were not anticipated by the developer, during specific phases of the execution. In this paper, we propose Free Lunch, a new lock profiler for Java application servers, specifically designed to identify, in-vivo, phases where the progress of the threads is impeded by a lock. Free Lunch is designed around a new metric, critical section pressure (CSP), which directly correlates the progress of the threads to each of the locks. Using Free Lunch, we have identified phases of high CSP, which were hidden with other lock profilers, in the distributed Cassandra NoSQL database and in several applications from the DaCapo\u00a0\u2026", "num_citations": "48\n", "authors": ["518"]}
{"title": "SmPL: A domain-specific language for specifying collateral evolutions in Linux device drivers\n", "abstract": " Collateral evolutions are a pervasive problem in large-scale software development. Such evolutions occur when an evolution that affects the interface of a generic library entails modifications, i.e., collateral evolutions, in all library clients. Performing these collateral evolutions requires identifying the affected files and modifying all of the code fragments in these files that in some way depend on the changed interface.We have studied the collateral evolution problem in the context of Linux device drivers. Currently, collateral evolutions in Linux are mostly done manually using a text editor, possibly with the help of tools such as grep. The large number of Linux drivers, however, implies that this approach is time-consuming and unreliable, leading to subtle errors when modifications are not done consistently.In this paper, we propose a transformation language, SmPL, to specify collateral evolutions. Because Linux\u00a0\u2026", "num_citations": "45\n", "authors": ["518"]}
{"title": "Design decisions for the FTM: a general purpose fault tolerant machine\n", "abstract": " Until now, fault tolerance has been reserved to specialized areas. However, due to the generalization of micro-computers and workstations in distributed environment, more users are concerned with reliability. For instance. diskless workstations are disabled by a failure of a \ufb01le server. Consequently there is a need for a general purpose fault tolerant system which can support a wide range of applications. This is our goal in the design of the Fault Tolerant Multiprocessor machine.The FTM hardware architecture is built from standard open machines connected by an interconnection sub-system. In such architecture processing elements are built using dynamic redundancy. When a processor fails the interconnection sub-system enables another processor to recover and restan computations from a non-enoneous state.", "num_citations": "45\n", "authors": ["518"]}
{"title": "RelaxDHT: A churn-resilient replication strategy for peer-to-peer distributed hash-tables\n", "abstract": " DHT-based P2P systems provide a fault-tolerant and scalable means to store data blocks in a fully distributed way. Unfortunately, recent studies have shown that if connection/disconnection frequency is too high, data blocks may be lost. This is true for most of the current DHT-based systems' implementations. To deal with this problem, it is necessary to build more efficient replication and maintenance mechanisms. In this article, we study the effect of churn on PAST, an existing DHT-based P2P system. We then propose solutions to enhance churn tolerance and evaluate them through discrete event simulation.", "num_citations": "44\n", "authors": ["518"]}
{"title": "A framework for simplifying the development of kernel schedulers: Design and performance evaluation\n", "abstract": " Writing a new scheduler and integrating it into an existing OS is a daunting task, requiring the understanding of multiple low-level kernel mechanisms. Indeed, implementing a new scheduler is outside the expertise of application programmers, even though they are the ones who understand best the scheduling needs of their applications. To address these problems, we present the design of Bossa, a language targeted toward the development of scheduling policies. Bossa provides high-level abstractions that are specific to the domain of scheduling. These constructs simplify the task of specifying a new scheduling policy and facilitate the static verification of critical safety properties. We illustrate our approach by presenting an implementation of the EDF scheduling policy. The overhead of Bossa is acceptable. Overall, we have found that Bossa simplifies scheduler development to the point that kernel expertise is not\u00a0\u2026", "num_citations": "41\n", "authors": ["518"]}
{"title": "Automatic generation of network protocol gateways\n", "abstract": " The emergence of networked devices in the home has made it possible to develop applications that control a variety of household functions. However, current devices communicate via a multitude of incompatible protocols, and thus gateways are needed to translate between them. Gateway construction, however, requires an intimate knowledge of the relevant protocols and a substantial understanding of low-level network programming, which can be a challenge for many application programmers.               This paper presents a generative approach to gateway construction, z2z, based on a domain-specific language for describing protocol behaviors, message structures, and the gateway logic. Z2z includes a compiler that checks essential correctness properties and produces efficient code. We have used z2z to develop a number of gateways, including SIP to RTSP, SLP to UPnP, and SMTP to SMTP via\u00a0\u2026", "num_citations": "39\n", "authors": ["518"]}
{"title": "Capturing OS expertise in an event type system: the Bossa experience\n", "abstract": " Emerging applications have increasingly specialized scheduling requirements. Changing the scheduling policy of an existing OS is, however, often difficult because scheduling code is typically deeply intertwined with the rest of the kernel. We have recently introduced the Bossa framework to facilitate the implementation and integration of new scheduling policies. While the use of Bossa simplifes the problem of implementing a new scheduler, knowledge of the control and data flow through the scheduling actions of the kernel is still needed to ensure that the behavior of the provided scheduling policy matches kernel expectations. In this paper, we propose a modular type system that provides a high-level characterization of the aspects of kernel behavior that affect the correctness of a scheduling policy. These types guide policy development and are linked with the compiler to enable static verification of correctness\u00a0\u2026", "num_citations": "39\n", "authors": ["518"]}
{"title": "Fast and portable locking for multicore architectures\n", "abstract": " The scalability of multithreaded applications on current multicore systems is hampered by the performance of lock algorithms, due to the costs of access contention and cache misses. The main contribution presented in this article is a new locking technique, Remote Core Locking (RCL), that aims to accelerate the execution of critical sections in legacy applications on multicore architectures. The idea of RCL is to replace lock acquisitions by optimized remote procedure calls to a dedicated server hardware thread. RCL limits the performance collapse observed with other lock algorithms when many threads try to acquire a lock concurrently and removes the need to transfer lock-protected shared data to the hardware thread acquiring the lock, because such data can typically remain in the server\u2019s cache. Other contributions presented in this article include a profiler that identifies the locks that are the bottlenecks in\u00a0\u2026", "num_citations": "36\n", "authors": ["518"]}
{"title": "Lessons from FTM: an experiment in design and implementation of a low-cost fault tolerant system\n", "abstract": " This paper describes an experiment in the design of a general purpose fault tolerant system, FTM. The main objective of the FTM design was to implement a low-cost fault-tolerant system that could be used on standard workstations. At the operating system level, the authors' goal was to offer fault-tolerance transparency to user applications. In other words, porting an application to FTM need only require compiling the source code without having to modify it. These objectives were achieved using the Mach micro-kernel and a modular set of reliable servers which implement application checkpoints and provide continuous system functions despite machine crashes. At the architectural level, their approach relies on a high-performance stable storage implementation, called stable transactional memory (STM), which can be implemented either by hardware or software. The authors first motivate their design choices, then\u00a0\u2026", "num_citations": "36\n", "authors": ["518"]}
{"title": "Semantic patches for documenting and automating collateral evolutions in Linux device drivers\n", "abstract": " Developing and maintaining drivers is known to be one of the major challenges in creating a general-purpose, practically-useful operating system [1, 3]. In the case of Linux, device drivers make up, by far, the largest part of the kernel source code, and many more drivers are available outside the standard kernel source tree. New drivers are needed all the time, to give access to the latest devices. To ease driver development, Linux provides a set of driver support libraries, each devoted to a particular bus or device type. These libraries encapsulate much of the complexity of interacting with the device and the Linux kernel, and impose a uniform structure on device-specific code within a given bus or device type.", "num_citations": "35\n", "authors": ["518"]}
{"title": "Coccinelle: 10 Years of Automated Evolution in the Linux Kernel\n", "abstract": " The Coccinelle C-program matching and transformation tool was first released in 2008 to facilitate specification and automation in the evolution of Linux kernel code. The novel contribution of Coccinelle was that it allows software developers to write code manipulation rules in terms of the code structure itself, via a generalization of the patch syntax. Over the years, Coccinelle has been extensively used in Linux kernel development, resulting in over 6000 commits to the Linux kernel, and has found its place as part of the Linux kernel development process. This paper studies the impact of Coccinelle on Linux kernel development and the features of Coccinelle that have made it possible. It can provide guidance on how other research-based tools can achieve practical impact in the open-source development community.", "num_citations": "34\n", "authors": ["518"]}
{"title": "Adapting distributed applications using extensible networks\n", "abstract": " Active networks have been proposed to allow the dynamic extension of network behavior by downloading application-specific protocols (ASPs) into network routers. We demonstrate the feasibility of the use of ASPs in an active network for the adaptation of distributed software components. We have implemented three examples which show that ASPs can be used to easily extend distributed applications, and furthermore, that such adaptation can be safe, portable and efficient. Safety and efficiency is obtained by implementing the ASPs in PLAN-P, a domain-specific language and run-time system for active networking. The presented examples illustrate three different applications: audio broadcasting with bandwidth adaptation in routers; an extensible HTTP server with load-balancing facilities; and a multipoint MPEG server derived from a point-to-point server.", "num_citations": "33\n", "authors": ["518"]}
{"title": "Faults in Linux 2.6\n", "abstract": " In August 2011, Linux entered its third decade. Ten years before, Chou et al. published a study of faults found by applying a static analyzer to Linux versions 1.0 through 2.4.1. A major result of their work was that the drivers directory contained up to 7 times more of certain kinds of faults than other directories. This result inspired numerous efforts on improving the reliability of driver code. Today, Linux is used in a wider range of environments, provides a wider range of services, and has adopted a new development and release model. What has been the impact of these changes on code quality? To answer this question, we have transported Chou et al.\u2019s experiments to all versions of Linux 2.6; released between 2003 and 2011. We find that Linux has more than doubled in size during this period, but the number of faults per line of code has been decreasing. Moreover, the fault rate of drivers is now below that of other\u00a0\u2026", "num_citations": "32\n", "authors": ["518"]}
{"title": "Tracking code patterns over multiple software versions with Herodotos\n", "abstract": " An important element of understanding a software code base is to identify the repetitive patterns of code it contains and how these evolve over time. Some patterns are useful to the software, and may be modularized. Others are detrimental to the software, such as patterns that represent defects. In this case, it is useful to study the occurrences of such patterns, to identify properties such as when and why they are introduced, how long they persist, and the reasons why they are corrected.", "num_citations": "32\n", "authors": ["518"]}
{"title": "Formal methods meet domain specific languages\n", "abstract": " In this paper, we relate an experiment whose aim is to study how to combine two existing approaches for ensuring software correctness: Domain Specific Languages (DSLs) and formal methods. As examples, we consider the Bossa DSL and the B formal method. Bossa is dedicated to the development of process schedulers and has been used in the context of Linux and Chorus. B is a refinement based formal method which has especially been used in the domain of railway systems. In this paper, we use B to express the correctness of a Bossa specification. Furthermore, we show how B can be used as an alternative to the existing Bossa tools for the production of certified schedulers.", "num_citations": "32\n", "authors": ["518"]}
{"title": "A DSL approach to improve productivity and safety in device drivers development\n", "abstract": " Although new peripheral devices are emerging at a frantic pace and require the fast release of drivers, little progress has been made to improve the development of such device drivers. Too often, this development consists of decoding hardware intricacies, based on inaccurate documentation. Then, assembly-level operations need to be used to interact with the device. These low-level operations reduce the readability of the driver and prevent safety properties from being checked. This paper presents an approach based on domain-specific languages (DSLs) to overcome these problems. We define a language, named Devil (DEVice Interaction Language), dedicated to defining the basic communication with a device. Unlike a general-purpose language, Devil allows a description to be checked for consistency. This not only improves the safety of the interaction with the device but also uncovers bugs early in the\u00a0\u2026", "num_citations": "31\n", "authors": ["518"]}
{"title": "Mely: Efficient Workstealing for Multicore Event-Driven Systems\n", "abstract": " Many high-performance communicating systems are designed using the event-driven paradigm. As multicore platforms are now pervasive, it becomes crucial for such systems to take advantage of the available hardware parallelism. Event-coloring is a promising approach in this regard. First, it allows programmers to simply and progressively inject support for the safe, parallel execution of multiple event handlers through the use of annotations. Second, it relies on a workstealing algorithm to dynamically balance the execution of event handlers on the available cores. This paper studies the impact of the workstealing algorithm on the overall system performance. We first show that the only existing workstealing algorithm designed for event-coloring runtimes is not always efficient: for instance, it causes a 33% performance degradation on a Web server. We then introduce several enhancements to improve the workstealing behavior. An evaluation using both microbenchmarks and real applications, a Web server and the Secure File Server (SFS), shows that our system consistently outperforms a state-of-the-art runtime (Libasync-smp), with or without workstealing. In particular, our new workstealing improves performance by up to +25% compared to Libasync-smp without workstealing and by up to +73% compared to the Libasync-smp workstealing algorithm, in the Web server case.", "num_citations": "30\n", "authors": ["518"]}
{"title": "FastLane: improving performance of software transactional memory for low thread counts\n", "abstract": " Software transactional memory (STM) can lead to scalable implementations of concurrent programs, as the relative performance of an application increases with the number of threads that support it. However, the absolute performance is typically impaired by the overheads of transaction management and instrumented accesses to shared memory. This often leads STM-based programs with low thread counts to perform worse than a sequential, non-instrumented version of the same application.", "num_citations": "29\n", "authors": ["518"]}
{"title": "Zebu: A Language-Based Approach for Network Protocol Message Processing\n", "abstract": " A network application communicates with other applications according to a set of rules known as a protocol. This communication is managed by the part of the application known as the protocol-handling layer, which enables the manipulation of protocol messages. The protocol-handling layer is a critical component of a network application since it represents the interface between the application and the outside world. It must thus satisfy two constraints: It must be efficient to be able to treat a large number of messages and it must be robust to face various attacks targeting the application itself or the underlying platform. Despite these constraints, the development process of this layer still remains rudimentary and requires a high level of expertise. It includes translating the protocol specification written in a high-level formalism such as ABNF toward low-level code such as C. The gap between these abstraction levels can\u00a0\u2026", "num_citations": "29\n", "authors": ["518"]}
{"title": "Churn-resilient replication strategy for peer-to-peer distributed hash-tables\n", "abstract": " DHT-based P2P systems provide a fault-tolerant and scalable mean to store data blocks in a fully distributed way. Unfortunately, recent studies have shown that if connection/disconnection frequency is too high, data blocks may be lost. This is true for most current DHT-based system\u2019s implementations. To avoid this problem, it is necessary to build really efficient replication and maintenance mechanisms. In this paper, we study the effect of churn on an existing DHT-based P2P system such as DHash or PAST. We then propose solutions to enhance churn tolerance and evaluate them through discrete event simulations.", "num_citations": "29\n", "authors": ["518"]}
{"title": "Safe operating system specialization: the RPC case study\n", "abstract": " Adaptive operating systems allow one to optimize system functionalities with respect to common situations. We present an experiment aimed at optimizing the RPC implementation in Chorus by manual specialization. We show that there exist numerous opportunities for specialization and that they can lead to great improvements. Then, we discuss how this optimization can be reproduced automatically with a specializer for C programs.", "num_citations": "29\n", "authors": ["518"]}
{"title": "Deadline-aware scheduling for software transactional memory\n", "abstract": " Software Transactional Memory (STM) is an optimistic concurrency control mechanism that simplifies the development of parallel programs. Still, the interest of STM has not yet been demonstrated for reactive applications that require bounded response time for some of their operations. We propose to support such applications by allowing the developer to annotate some transaction blocks with deadlines. Based on previous execution statistics, we adjust the transaction execution strategy by decreasing the level of optimism as the deadlines near through two modes of conservative execution, without overly limiting the progress of concurrent transactions. Our implementation comprises a STM extension for gathering statistics and implementing the execution mode strategies. We have also extended the Linux scheduler to disable preemption or migration of threads that are executing transactions with deadlines. Our\u00a0\u2026", "num_citations": "28\n", "authors": ["518"]}
{"title": "Improving driver robustness: an evaluation of the Devil approach\n", "abstract": " To keep up with the frantic pace at which devices come out, drivers need to be quickly developed, debugged and tested. We have recently introduced a new approach to improve driver robustness based on an Interface Definition Language, named Devil. Devil allows a high-level definition of the communication of a device. A computer automatically checks the consistency of a Devil specification and generates stubs that include run-time checks. We use mutation analysis to evaluate the improvement in driver robustness offered by Devil. To do so, we have injected programming errors using mutation analyses into Devil based Linux drivers and the original C drivers. We assess how early errors can be caught in the development process, by measuring whether errors are detected either at compile time or at run time. The results of our experiments on the IDE Linux disk driver show that nearly 3 times more errors are\u00a0\u2026", "num_citations": "28\n", "authors": ["518"]}
{"title": "Invited application paper: language design for implementing process scheduling hierarchies\n", "abstract": " Standard operating systems provide only a single fixed scheduler, which does not meet all possible application scheduling needs. More flexibility can be achieved using a hierarchy of schedulers, allowing multiple schedulers to coexist in a single operating system (OS). Bossa is a framework for facilitating the implementation and deployment of OS process schedulers. In this paper, we describe the features of Bossa that enable the creation and management of a scheduling hierarchy. These features include a domain-specific language for implementing schedulers and a type system for describing requirements on scheduler behavior. The use of the domain-specific language eases scheduler development and enables scheduler verification. We have found that the approach allows programmers, even students who are not kernel or scheduling experts, to easily and safely implement and deploy schedulers that meet\u00a0\u2026", "num_citations": "27\n", "authors": ["518"]}
{"title": "An approach to improving the structure of error-handling code in the linux kernel\n", "abstract": " The C language does not provide any abstractions for exception handling or other forms of error handling, leaving programmers to devise their own conventions for detecting and handling errors. The Linux coding style guidelines suggest placing error handling code at the end of each function, where it can be reached by gotos whenever an error is detected. This coding style has the advantage of putting all of the error-handling code in one place, which eases understanding and maintenance, and reduces code duplication. Nevertheless, this coding style is not always applied. In this paper, we propose an automatic program transformation that transforms error-handling code into this style. We have applied our transformation to the Linux 2.6. 34 kernel source code, on which it reorganizes the error handling code of over 1800 functions, in about 25 minutes.", "num_citations": "24\n", "authors": ["518"]}
{"title": "Performance of consistent checkpointing in a modular operating system: results of the FTM experiment\n", "abstract": " This paper presents an evaluation of the performance of a consistent checkpointing mechanism that has been integrated into a modular Mach microkernel based operating system. We have measured the performance overhead of checkpointing for several workstation-typical applications: number crunching and office tools. This has been done using specific servers which were added to a standard Mach 3.0/BSD system. Measurements are performed for failure-free executions by varying the number of checkpoints and thus the amount of computation lost in the event of a crash. Our initial results showed a time overhead of about 3% for up to 20% work lost in the event of a crash. while we get an overhead between 16% and 23% for up to 1% computation lost. Also, when porting interactive office tools such as the micro-emacs text editor, we get a maximal checkpoint duration of 1.4 second on our prototype\u00a0\u2026", "num_citations": "23\n", "authors": ["518"]}
{"title": "FT-NFS: An efficient fault-tolerant NFS server designed for off-the-shelf workstations\n", "abstract": " In most modern local area network environments, NFS is used to provide remote file storage on a particular server machine. A consequence of this distributed architecture is that the failure of the server results in paralysis or a loss of work for users. The paper presents the design of a low cost fault tolerant NFS server which can be installed on most Unix networking environments. FT-NFS runs as a user process and does not necessitate any underlying specific operating system functionality. The originality of our approach relies on the use of a stable cache which provides data availability and resiliency to a single failure. The main benefits of the stable cache are first to allow disk write operations to be safely performed in the back ground and second to permit the gathering of small files in large containers. The latter technique permits disk I/Os to be improved by reducing their number and increasing their length. Under\u00a0\u2026", "num_citations": "21\n", "authors": ["518"]}
{"title": "A language-based approach for improving the robustness of network application protocol implementations\n", "abstract": " The secure and robust functioning of a network relies on the defect-free implementation of network applications. As network protocols have become increasingly complex, however, hand-writing network message processing code has become increasingly error-prone. In this paper, we present a domain-specific language, Zebu, for generating robust and efficient message processing layers. A Zebu specification, based on the notation used in RFCs, describes protocol message formats and related processing constraints. Zebu-based applications are efficient, since message fragments can be specified to be processed on demand. Zebu-based applications are also robust, as the Zebu compiler automatically checks specification consistency and generates parsing stubs that include validation of the message structure. Using a message torture suite in the context of SIP and RTSP, we show that Zebu-generated code is\u00a0\u2026", "num_citations": "20\n", "authors": ["518"]}
{"title": "Tarantula: Killing driver bugs before they hatch\n", "abstract": " The Linux operating system is undergoing continual evolution. Evolution in the kernel and generic driver modules often triggers the need for corresponding evolutions in specific device drivers. Such collateral evolutions are tedious, because of the large number of device drivers, and error-prone, because of the complexity of the code modifications involved. We propose an automatic tool, Tarantula, to aid in this process. In this paper, we examine some recent evolutions in Linux and the collateral evolutions they trigger, and assess the corresponding requirements on Tarantula.", "num_citations": "20\n", "authors": ["518"]}
{"title": "Partial Evaluation for Software Engineering.\n", "abstract": " Permission to make digital/hard copy of part or all of this work for personal or classroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title of the publication, and its date appear, and notice is given that copying is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.", "num_citations": "20\n", "authors": ["518"]}
{"title": "Matching micro-kernels to modern applications using fine-grained memory protection\n", "abstract": " Scalable distributed systems, systems whose processing power remains proportional to the number of component processors, require a programming methodology where an application developer may take existing software modules and plug them together to form a new application. To allow mistrusting modules to interact, the underlying kernel support must offer protection barriers which do not impede performance. The wide-ranging nature of modern applications used on larger scale systems means that existing kernel functions may not necessarily be the most efficient for an application. The kernel must therefore allow an application to dynamically install a function an the kernel; this is one aspect of customization. This paper argues that customization support is one aspect of fine-grained protection for modules needing CPU supervisor privilege. We describe the kernel support required for fine grained protection\u00a0\u2026", "num_citations": "20\n", "authors": ["518"]}
{"title": "The Battle of the Schedulers: FreeBSD {ULE} vs. Linux {CFS}\n", "abstract": " This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.", "num_citations": "19\n", "authors": ["518"]}
{"title": "Towards robust oses for appliances: A new approach based on domain-specific languages\n", "abstract": " Appliances represent a quickly growing domain that raises new challenges in OS design and development. First, new products appear at a rapid pace to satisfy emerging needs. Second, the nature of these markets makes these needs unpredictable. Lastly, given the competitiveness of such markets, there exists tremendous pressure to deliver new products. In fact, innovation is a requirement in emerging markets to gain commercial success. The embedded nature of appliances makes upgrading and fixing bugs difficult (and sometimes impossible) to achieve. Consequently, there must be a high level of confidence in the software. Additionally, the pace of innovation requires rapid OS development so as to match ever changing needs of new appliances. To offer confidence, software must be highly robust. That is, for a given type of appliance, critical behavioral properties must be determined and guaranteed (eg\u00a0\u2026", "num_citations": "19\n", "authors": ["518"]}
{"title": "Maximizing parallelism without exploding deadlines in a mixed criticality embedded system\n", "abstract": " Complex embedded systems today commonly involve a mix of real-time and best-effort applications. The recent emergence of low-cost multicore processors raises the possibility of running both kinds of applications on a single machine, with virtualization ensuring isolation. Nevertheless, memory contention can introduce other sources of delay, that can lead to missed deadlines. In this paper, we present a combined offline/online memory bandwidth monitoring approach. Our approach estimates and limits the impact of the memory contention incurred by the best-effort applications on the execution time of the real-time application. We show that our approach is compatible with the hardware counters provided by current small commodity multicore processors. Using our approach, the system designer can limit the overhead on the real-time application to under 5% of its expected execution time, while still enabling\u00a0\u2026", "num_citations": "18\n", "authors": ["518"]}
{"title": "WYSIWIB: exploiting fine\u2010grained program structure in a scriptable API\u2010usage protocol\u2010finding process\n", "abstract": " Bug\u2010finding tools rely on specifications of what is correct or incorrect code. As it is difficult for a tool developer or user to anticipate all possible specifications, strategies for inferring specifications have been proposed. These strategies obtain probable specifications by observing common characteristics of code or execution traces, typically focusing on sequences of function calls. To counter the observed high rate of false positives, heuristics have been proposed for ranking or pruning the results. These heuristics, however, can result in false negatives, especially for rarely used functions. In this paper, we propose an alternate approach to specification inference, in which the user guides the inference process using patterns of code that reflect the user's understanding of the conventions and design of the targeted software project. We focus on specifications describing the correct usage of API functions, which we refer to\u00a0\u2026", "num_citations": "17\n", "authors": ["518"]}
{"title": "Energy adaptation for multimedia information kiosks\n", "abstract": " Video kiosks increasingly contain powerful PC-like embedded processors, allowing them to display video at a high level of quality. Such video display, however, entails significant energy consumption. This paper presents an approach to reducing energy consumption by adapting the CPU clock frequency. In contrast to previous approaches, we exploit the specific behavior of a video kiosk. Because a kiosk plays the same set of movies over and over, we choose a CPU frequency for a given frame based on the computational requirements of the frame that were observed on earlier iterations. We have implemented our approach in the legacy video player MPlayer. On a PC like those that can be found in kiosks, we observe increases in battery lifetime of up to 2 times as compared to running at the maximum CPU frequency on a set of high resolution divx movies.", "num_citations": "17\n", "authors": ["518"]}
{"title": "On designing a target-independent DSL for safe OS process-scheduling components\n", "abstract": " Developing new process-scheduling components for multiple OSes is challenging because of the tight interdependence between an OS and its scheduler and because of the stringent safety requirements that OS code must satisfy. In this context, a domain-specific language (DSL), designed by a scheduling expert, can encapsulate scheduling expertise and thus facilitate scheduler programming and verification. Nevertheless, designing a DSL that is target-independent and provides safety guarantees requires expertise not only in scheduling but also in the structure of various OSes. To address these issues, we propose the introduction of an OS expert into the DSL design process and the use of a type system to enable the OS expert to express relevant OS properties.               This paper instantiates our approach in the context of the Bossa process-scheduling framework and describes how the types provided\u00a0\u2026", "num_citations": "17\n", "authors": ["518"]}
{"title": "Java bytecode compression for embedded systems\n", "abstract": " A program executing on an embedded system or similar environment faces limited memory resources and fixed time constrains. We demonstrate how factorization of common instruction sequences can be automatically applied to Java bytecode programs. Based on a series of experiments, we argue that program size is reduced by 30% on the average, typically with an execution time penalty of less than 30%. The one-time, minor modifications necessary to make a standard Java interpreter compatible with this factorized code are presented on the Harissa virtual machine, together with an algorithm for performing the factorization of Java bytecode.", "num_citations": "17\n", "authors": ["518"]}
{"title": "Semantic patches considered helpful\n", "abstract": " Modern software development is characterized by the use of libraries and interfaces. This software architecture carries down even to the operating system level. Linux, for example, is organized as a small kernel, complemented with libraries providing generic functionalities for use in implementing network access, file management, access to physical devices, etc. Much of the Linux source code then consists of service-specific files that use these libraries. These libraries are also used by the many OS-level services that are maintained outside of the Linux source tree.", "num_citations": "16\n", "authors": ["518"]}
{"title": "Distributing MPEG movies over the internet using programmable networks\n", "abstract": " Distributing video over the Internet is an increasingly important application. Nevertheless, the real-time and high bandwidth requirements of video make video distribution over today's Internet a challenge. Adaptive approaches can be used to respond to changes in bandwidth availability while limiting the effect of such changes on perceptual quality and resource consumption. Nevertheless, most existing adaptation mechanisms have limited scalability and do not effectively exploit the heterogeneity of the Internet. In this paper, we describe the design and implementation of a MPEG video broadcasting service based on active networks. In an active network, routers can be programmed to make routing decisions based on local conditions. Because decisions are made locally, adaptation reacts rapidly to changing conditions and is unaffected by conditions elsewhere in the network. Programmability allows the\u00a0\u2026", "num_citations": "16\n", "authors": ["518"]}
{"title": "Towards easing the diagnosis of bugs in OS code\n", "abstract": " The rapid detection and treatment of bugs in operating systems code is essential to maintain the overall security and dependability of a computing system. A number of techniques have been proposed for detecting bugs, but little has been done to help developers analyze and treat them. In this paper we propose to combine bug-finding rules with transformations that automatically introduce bug-fixes or workarounds when a possible bug is detected. This work builds on our previous work on the Coccinelle tool, which targets device driver evolution.", "num_citations": "15\n", "authors": ["518"]}
{"title": "Efficient treatment of failures in RPC systems\n", "abstract": " This paper addresses extensions to be made to a basic remote procedure call system for the integration of primitive fault tolerance measures. Our main design goal is to not introduce performance penalty for remote procedure calls executing in the absence of failures, and to not impose significant overhead by the treatment of failures. Basically, extensions include a simple algorithm that finds and eliminates orphans, and a mechanism that detects abnormally terminated remote calls. Our solution for orphan detection as based on the extermination approach, its efficiency coming from a minor addition to the system architecture that allows the implementation of high speed stable storage. Performance measures given by the implementation of our reliability mechanisms on top of the Mach 3.0/BSD UX36 operating system show that the mechanisms are responsible for adding only 1% overhead on the operating system's\u00a0\u2026", "num_citations": "15\n", "authors": ["518"]}
{"title": "Data compaction method for an intermediate object code program executable in an onboard system provided with data processing resources and corresponding onboard system with\u00a0\u2026\n", "abstract": " The invention concerns a data compaction method and system for an intermediate program. The method consists in searching the program (1000) for identical sequences (Si) and counting Ni number of occurrences of each sequence (Si), a comparison test (1001) to find the superiority of a function f (Ni) to a reference value enables to generate (1003) a specific instruction of a specific code (Ci) with which the sequence (Si) is associated, replacing (1004) each occurrence in the sequence (Si) by the specific code (Ci) in the intermediate program to create a compacted intermediate program (FCC) with which an executing file (FEX) is associated. The invention is applicable to multiple application portable objects such as microprocessor cards, onboard systems of the like.", "num_citations": "14\n", "authors": ["518"]}
{"title": "Accurate program analyses for successful specialization of legacy system software\n", "abstract": " Choosing the accuracy of program analyses is a crucial issue when designing and developing a partial evaluator capable of treating realistic programs, and in particular legacy software. In this paper, we investigate the degree of accuracy of alias and binding-time analyses that is required to successfully exploit the specialization opportunities present in the Sun commercial implementation of the remote procedure call protocol (RPC). The Sun RPC implementation consists of a stack of small parameterized layers. This structure is representative of a certain programming style in operating system and network development. The analysis features that we have explored have been implemented in Tempo, a partial evaluator for C. After automatic specialization of the RPC using Tempo, we measured speedups up to 1.5 for complete remote procedure calls (including network transport) and up to 3.7 for local buffer encoding\u00a0\u2026", "num_citations": "14\n", "authors": ["518"]}
{"title": "The devil language\n", "abstract": " Devil is a Domain-specific language dedicated to defining the basic communica- tion with a device. Unlike a general-purpose language, Devil allows a specification to be checked for consistency. This not only improves the safety of the interaction with the device but also uncovers bugs early in the development process. A compiler automatically generates from a Devil specification efficient low-level code to communicate with the device. This report presents the reference manual of the Devil language.", "num_citations": "14\n", "authors": ["518"]}
{"title": "Dynamic Consolidation of Highly Available Web Applications\n", "abstract": " Datacenters provide an economical and practical solution for hosting large scale n-tier Web applications. When scalability and high availability are required, each tier can be implemented as multiple replicas, which can absorb extra load and avoid a single point of failure. Realizing these benefits in practice, however, requires that replicas be assigned to datacenter nodes according to certain placement constraints. To provide the required quality of service to all of the hosted applications, the datacenter must consider of all of their specific constraints. When the constraints are not satisfied, the datacenter must quickly adjust the mappings of applications to nodes, taking all of the applications' constraints into account. This paper presents Plasma, an approach for hosting highly available Web applications, based on dynamic consolidation of virtual machines and placement constraint descriptions. The placement constraint descriptions allow the data- center administrator to describe the datacenter infrastructure and each appli- cation administrator to describe his requirements on the VM placement. Based on the descriptions, Plasma continuously optimizes the placement of the VMs in order to provide the required quality of service. Experiments on simulated configurations show that the Plasma reconfiguration algorithm is able to man- age a datacenter with up to 2000 nodes running 4000 VMs with 800 placement constraints. Real experiments on a small cluster of 8 working nodes running 3 instances of the RUBiS benchmarks with a total of 21 VMs show that con- tinuous consolidation is able to reach 85% of the load of a 21 working nodes cluster.", "num_citations": "13\n", "authors": ["518"]}
{"title": "Enforcing the use of API functions in Linux code\n", "abstract": " In the Linux kernel source tree, header files typically define many small functions that have a simple behavior but are critical to ensure readability, correctness, and maintainability. We have observed, however, that some Linux code does not use these functions systematically. In this paper, we propose an approach combining rule-based program matching and transformation with generative programming to generate rules for finding and fixing code fragments that should use the functions defined in header files. We illustrate our approach using an in-depth study based on four typical functions defined in the header file include/linux/usb. h.", "num_citations": "13\n", "authors": ["518"]}
{"title": "A uniform and automatic approach to copy elimination in system extensions via program specialization\n", "abstract": " Most operating systems heavily rely on intermediate data structures for modularity or portability reasons. This paper extends program specialization to eliminate these intermediate data structures in a uniform manner. Our transformation process is fully automatic and is based on a specializer for C programs, named Tempo. The key advantage of our approach is that the degree of safety of the source program is preserved by the optimization. As a result, mature system code can be reused without requiring additional verification. Our preliminary results on the automatically optimized RPC code are very promising in that they are identical to the results we obtained by manual specialization of the same code. In this last experiment, performance measurement of the specialized RPC fragments shows a minimal speedup of 30% compared to the non-specialized code. Elimination of intermediate data structures is part of our research effort towards optimizing operating system components via program specialization. It improves on our previous work in that optimizations are now carried out automatically using our specialization tool. Furthermore, it shows how generic subsystems can be automatically specialized into specific system extensions by exploiting application constraints.", "num_citations": "13\n", "authors": ["518"]}
{"title": "Semantic Patches for Java Program Transformation (Experience Report)\n", "abstract": " Developing software often requires code changes that are widespread and applied to multiple locations. There are tools for Java that allow developers to specify patterns for program matching and source-to-source transformation. However, to our knowledge, none allows for transforming code based on its control-flow context. We prototype Coccinelle4J, an extension to Coccinelle, which is a program transformation tool designed for widespread changes in C code, in order to work on Java source code. We adapt Coccinelle to be able to apply scripts written in the Semantic Patch Language (SmPL), a language provided by Coccinelle, to Java source files. As a case study, we demonstrate the utility of Coccinelle4J with the task of API migration. We show 6 semantic patches to migrate from deprecated Android API methods on several open source Android projects. We describe how SmPL can be used to express several API migrations and justify several of our design decisions.", "num_citations": "12\n", "authors": ["518"]}
{"title": "Automatic Android deprecated-API usage update by learning from single updated example\n", "abstract": " Due to the deprecation of APIs in the Android operating system, developers have to update usages of the APIs to ensure that their applications work for both the past and current versions of Android. Such updates may be widespread, non-trivial, and time-consuming. Therefore, automation of such updates will be of great benefit to developers. AppEvolve, which is the state-of-the-art tool for automating such updates, relies on having before-and after-update examples to learn from. In this work, we propose an approach named CocciEvolve that performs such updates using only a single after-update example. CocciEvolve learns edits by extracting the relevant update to a block of code from an after-update example. From preliminary experiments, we find that CocciEvolve can successfully perform 96 out of 112 updates, with a success rate of 85%.", "num_citations": "11\n", "authors": ["518"]}
{"title": "A Framework for the Design Configuration of Accountable Selfish-Resilient Peer-to-Peer Systems\n", "abstract": " A challenge in designing a peer-to-peer (P2P) system is to ensure that the system is able to tolerate selfish nodes that strategically deviate from their specification whenever doing so is convenient. In this paper, we propose RACOON, a framework for the design of P2P systems that are resilient to selfish behaviours. While most existing solutions target specific systems or types of selfishness, RACOON proposes a generic and semi-automatic approach that achieves robust and reusable results. Also, RACOON supports the system designer in the performance-oriented tuning of the system, by proposing a novel approach that combines Game Theory and simulations. We illustrate the benefits of using RACOON by designing two P2P systems: a live streaming and an anonymous communication system. In simulations and a real deployment of the two applications on a testbed comprising 100 nodes, the systems designed\u00a0\u2026", "num_citations": "11\n", "authors": ["518"]}
{"title": "Finding resource-release omission faults in Linux\n", "abstract": " The management of the releasing of allocated resources is a continual problem in ensuring the robustness of systems code. Missing resource-releasing operations lead to memory leaks and deadlocks. A number of approaches have been proposed to detect such problems, but they often have a high rate of false positives, or focus only on commonly used functions. In this paper we observe that resource-releasing operations are often found in error-handling code, and that the choice of resource-releasing operation may depend on the context in which it is to be used. We propose an approach to finding resource-release omission faults in C code that takes into account these issues. We use our approach to find over 100 faults in the drivers directory of Linux 2.6.34, with a false positive rate of only 16%, well below the 30% that has been found to be acceptable to developers.", "num_citations": "11\n", "authors": ["518"]}
{"title": "Position Summary Bossa: a DSL framework for Application-Specific Scheduling Policies\n", "abstract": " Emerging computing models and applications are continuously challenging the operating system scheduler. Multimedia applications require predictable performance and stringent timing guarantees. Embedded systems need to minimize power consumption. Network routers demand isolated execution of active network programs. Meeting all these requirements requires specialized scheduling policies, which traditional scheduling infrastructures are unable to provide.While it is clear the need for customized scheduling policies, there is a lack of tools that capture the design singularities of schedulers to ease the development process. Moreover, writing schedulers requires deep OS knowledge and involves the development of low-level OS code, which frequently crosscuts multiple kernel mechanisms (eg, process synchronization, file system and device driver operations, system calls).", "num_citations": "11\n", "authors": ["518"]}
{"title": "An experience in the design of a reliable object based system\n", "abstract": " The design and implementation of a reliable object-based system on top of a fault-tolerant multiprocessor machine based on stable storage technology, the FTM, are described. Each reliable object is characterized by its persistent state and methods. It is mapped onto a server which manages the object's persistent state and method calls. In order to recover a global consistent state of objects in the event of failure, a solution based on dynamic atomic actions was implemented. Servers are running on a fault tolerant version of an extended MACH microkernel.< >", "num_citations": "11\n", "authors": ["518"]}
{"title": "When extended para-virtualization (xpv) meets numa\n", "abstract": " This paper addresses the problem of efficiently virtualizing NUMA architectures. The major challenge comes from the fact that the hypervisor regularly reconfigures the placement of a virtual machine (VM) over the NUMA topology. However, neither guest operating systems (OSes) nor system runtime libraries (eg, Hotspot) are designed to consider NUMA topology changes at runtime, leading end user applications to unpredictable performance. This paper presents eXtended Para-Virtualization (XPV), a new principle to efficiently virtualize a NUMA architecture. XPV consists in revisiting the interface between the hypervisor and the guest OS, and between the guest OS and system runtime libraries (SRL) so that they can dynamically take into account NUMA topology changes. The paper presents a methodology for systematically adapting legacy hypervisors, OSes, and SRLs. We have applied our approach with less\u00a0\u2026", "num_citations": "10\n", "authors": ["518"]}
{"title": "Usuba: Optimizing & Trustworthy Bitslicing Compiler\n", "abstract": " Bitslicing is a programming technique commonly used in cryptography that consists in implementing a combinational circuit in software. It results in a massively parallel program immune to cache-timing attacks by design.", "num_citations": "10\n", "authors": ["518"]}
{"title": "Efficient locking for multicore architectures\n", "abstract": " The scalability of multithreaded applications on current multicore systems is hampered by the performance of critical sections, due in particular to the costs of access contention and cache misses. In this paper, we propose a new locking technique, Remote Core Locking (RCL) that aims to improve the performance of critical sections in legacy applications on multicore architectures. The idea of RCL is to replace lock acquisitions by optimized remote procedure calls to a dedicated server core. RCL limits the performance collapse observed with regular locks when many threads try to acquire a lock concurrently and removes the need to transfer lock-protected shared data to the core acquiring the lock: such data can typically remain in the server core's cache. Our microbenchmark shows that under high contention, RCL is always more efficient than the other state-of-the-art lock mechanisms, and a preliminary macrobenchmark evaluation shows performance gains on SPLASH-2 benchmarks (speedup up to 4.85) and on the Web cacheapplication memcached (speedup up to 2.62).", "num_citations": "10\n", "authors": ["518"]}
{"title": "Experiments in program compilation by interpreter specialization\n", "abstract": " Interpretation and run-time compilation techniques are becoming increasingly important due to the need to support heterogeneous architectures, evolving programming languages, and dynamically downloaded code. Although interpreters are easy to write and maintain, they are inefficient. On the other hand, run-time compilation provides efficient execution, but is costly to implement. One way to get the best of both approaches is to apply program specialization to an interpreter in order to generate an efficient implementation automatical- ly. Recent advances in program specialization technology have resulted in important improvements in the performance of specialized interpreters. This paper presents and assesses experimental results for the application of program specialization to both bytecode and structured-language interpreter- s. The results show that for general-purpose bytecode, program specialization can yield speedups of up to a factor of four, while specializing certain structured-language interpreters can yield performance equivalent to code compiled by a general-purpose compiler.", "num_citations": "10\n", "authors": ["518"]}
{"title": "JIT vs. Offline Compilers: Limits and Benefits of Bytecode Compilation\n", "abstract": " Just-in-time and o ine compilers are solutions which have been proposed to overcome Java's ine cient interpretation scheme. However, most compilers trade e ciency for portability.In this paper, we present an approach which reconciles portability and e ciency. We have designed and implemented a Java bytecode to C compiler, named Salsa, which incorporates both novel features and aggressive optimizations. To evaluate our compiler, we have conducted an extensive experimental study aimed at comparing the various existing alternatives to execute Java programs. The C code produced by Salsa is more e cient than all other alternative ways of executing Java programs (that were available to us): it is up to 140 times faster than JDK interpreter, up to 13 times faster than the Softway Guava JIT, and 30% faster than the Juice bytecode to C compiler. The compilation process does virtual-method call optimization based on the Class Hierarchy Analysis. Finally, in contrast to existing o ine compilers, the runtime system of Salsa includes an interpreter allowing to preserve the ability to dynamically load bytecode.", "num_citations": "10\n", "authors": ["518"]}
{"title": "RACOON++: A semi-automatic framework for the selfishness-aware design of cooperative systems\n", "abstract": " A challenge in designing cooperative distributed systems is to develop feasible and cost-effective mechanisms to foster cooperation among selfish nodes, i.e., nodes that strategically deviate from the intended specification to increase their individual utility. Finding a satisfactory solution to this challenge may be complicated by the intrinsic characteristics of each system, as well as by the particular objectives set by the system designer. Our previous work addressed this challenge by proposing RACOON, a general and semi-automatic framework for designing selfishness-resilient cooperative systems. RACOON relies on classical game theory and a custom built simulator to predict the impact of a fixed set of selfish behaviours on the designer's objectives. In this paper, we present RACOON++, which extends the previous framework with a declarative model for defining the utility function and the static behaviour of selfish\u00a0\u2026", "num_citations": "9\n", "authors": ["518"]}
{"title": "Fast and precise retrieval of forward and back porting information for Linux device drivers\n", "abstract": " Porting Linux device drivers to target more recent and older Linux kernel versions to compensate for the everchanging kernel interface is a continual problem for Linux device driver developers. Acquiring information about interface changes is a necessary, but tedious and error prone, part of this task. In this paper, we propose two tools, Prequel and gcc-reduce, to help the developer collect the needed information. Prequel provides language support for querying git commit histories, while gcc-reduce translates error messages produced by compiling a driver with a target kernel into appropriate Prequel queries. We have used our approach in porting 33 device driver files over up to 3 years of Linux kernel history, amounting to hundreds of thousands of commits. In these experiments, for 3/4 of the porting issues, our approach highlighted commits that enabled solving the porting task. For many porting issues, our approach retrieves relevant commits in 30 seconds or less.", "num_citations": "9\n", "authors": ["518"]}
{"title": "Oops! where did that code snippet come from?\n", "abstract": " A kernel oops is an error report that logs the status of the Linux kernel at the time of a crash. Such a report can provide valuable first-hand information for a Linux kernel maintainer to conduct postmortem debugging. Recently, a repository has been created that systematically collects kernel oopses from Linux users. However, debugging based on only the information in a kernel oops is difficult. We consider the initial problem of finding the offending line, ie, the line of source code that incurs the crash. For this, we propose a novel algorithm based on approximate sequence matching, as used in bioinformatics, to automatically pinpoint the offending line based on information about nearby machine-code instructions, as found in a kernel oops. Our algorithm achieves 92% accuracy compared to 26% for the traditional approach of using only the oops instruction pointer.", "num_citations": "9\n", "authors": ["518"]}
{"title": "Zimp: Efficient intercore communications on manycore machines\n", "abstract": " Modern computers have an increasing number of cores and, as exemplified by the recent Barrelfish operating system, the software they execute increasingly resembles distributed, message-passing systems. To support this evolution, there is a need for very efficient inter-core communication mechanisms. Current operating systems provide various inter-core communication mechanisms, but it is not clear yet how they behave on manycore processors. In this report, we study seven mechanisms, that are considered state-of-the-art. We show that these mechanisms have two main drawbacks that limit their efficiency: they perform costly memory copy operations and they do not provide efficient support for one-to-many communications. We do thus propose ZIMP, a new inter-core communication mechanism that implements zero-copy inter-core message communications and that efficiently handles one-to-many communications. We evaluate ZIMP on three manycore machines, having respectively 8, 16 and 24 cores, using both microand macro-benchmarks (a consensus and a checkpointing protocol). Our evaluation shows that ZIMP consistently improves the performance of existing mechanisms, by up to one order of magnitude.", "num_citations": "9\n", "authors": ["518"]}
{"title": "Dealing with hardware in embedded software: A general framework based on the Devil language\n", "abstract": " Writing code that talks to hardware is a crucial part of any embedded project. Both productivity and quality are needed, but some flaws in the traditional development process make these requirements difficult to meet.", "num_citations": "9\n", "authors": ["518"]}
{"title": "Faster run-time specialized code using data specialization\n", "abstract": " Run-time specialization is a technique that optimizes a program based on run-time information. In this context, specialization time must be constrained, limiting the possibility to further optimize the specialized code. We present a low-cost methodology for improving the code generated by a run-time specializer. This result is acheived by combining run-time specialization with another form of automatic specialization, data specializat- ion. We show how to use our approach to implement compaction of run-time specialized code in the framework of the Tempo specializer for C programs. We find that the compaction optimization can improve the performance of the specialized code by up to a factor of 4, while adding only about 10% to the cost of run-time specialization on most of our examples.", "num_citations": "9\n", "authors": ["518"]}
{"title": "Towards safe and efficient customization in distributed systems\n", "abstract": " Progress in microprocessor and communication system technology is leading to the emergence of new large scale distributed architectures whose power increases proportionally to the number of processor elements. Such architectures support execution of traditional distributed applications based on the client-server paradigm as well as parallel applications. Among other consequences, this framework requires the design of distributed operating systems that exhibit good performance for a wide range of applications. One way to achieve this consists of weakening the semantics of system functions, and to customize them to implement stronger semantics that match the applications' needs. Customization in distributed systems must be achieved in a safe and efficient way. The safety property relates to the fact that a customized function must implement a stronger semantics than that of the base function. Efficiency of a\u00a0\u2026", "num_citations": "9\n", "authors": ["518"]}
{"title": "Automated Deprecated-API Usage Update for Android Apps: How Far are We?\n", "abstract": " As the Android API evolves, some API methods may be deprecated, to be eventually removed. App developers face the challenge of keeping their apps up-to-date, to ensure that the apps work in both older and newer Android versions. Currently, AppEvolve is the state-of-the-art approach to automate such updates, and it has been shown to be quite effective. Still, the number of experiments reported is moderate, involving only API usage updates in 41 usage locations. In this work, we replicate the evaluation of AppEvolve and assess whether its effectiveness is generalizable. Given the set of APIs on which AppEvolve has been evaluated, we test AppEvolve on other mobile apps that use the same APIs. Our experiments show that AppEvolve fails to generate applicable updates for 81% of our dataset, even though the relevant knowledge for correct API updates is available in the examples. We first categorize the\u00a0\u2026", "num_citations": "8\n", "authors": ["518"]}
{"title": "Bossa nova: Introducing modularity into the bossa domain-specific language\n", "abstract": " Domain-specific languages (DSLs) have been proposed as a solution to ease the development of programs within a program family. Sometimes, however, experience with the use of a DSL reveals the presence of subfamilies within the family targeted by the language. We are then faced with the question of how to capture these subfamilies in DSL abstractions. A solution should retain features of the original DSL to leverage existing expertise and support tools.               The Bossa DSL is a language targeted towards the development of kernel process scheduling policies. We have encountered the issue of program subfamilies in using this language to implement an encyclopedic, multi-OS library of scheduling policies. In this paper, we propose that introducing certain kinds of modularity into the language can furnish abstractions appropriate for implementing scheduling policy subfamilies. We present the\u00a0\u2026", "num_citations": "7\n", "authors": ["518"]}
{"title": "A framework for simplifying the development of kernel schedulers: design and performance evaluation\n", "abstract": " CiteSeerX \u2014 A framework for simplifying the development of kernel schedulers: design and performance evaluation Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA A framework for simplifying the development of kernel schedulers: design and performance evaluation Cached Download as a PDF Download Links [www.diku.dk] Other Repositories/Bibliography DBLP Save to List Add to Collection Correct Errors Monitor Changes by Luciano Porto Barreto , Gilles Muller , Julia L. Lawall , Kenji Kono Citations: 4 - 2 self Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract Keyphrases kernel scheduler performance evaluation Powered by: Apache Solr About CiteSeerX Submit and \u2026", "num_citations": "7\n", "authors": ["518"]}
{"title": "Provable multicore schedulers with Ipanema: application to work conservation\n", "abstract": " Recent research and bug reports have shown that work conservation, the property that a core is idle only if no other core is overloaded, is not guaranteed by Linux's CFS or FreeBSD's ULE multicore schedulers. Indeed, multicore schedulers are challenging to specify and verify: they must operate under stringent performance requirements, while handling very large numbers of concurrent operations on threads. As a consequence, the verification of correctness properties of schedulers has not yet been considered.", "num_citations": "6\n", "authors": ["518"]}
{"title": "Analysing Selfishness Flooding with SEINE\n", "abstract": " Selfishness is one of the key problems that confronts developers of cooperative distributed systems (e.g., file-sharing networks, voluntary computing). It has the potential to severely degrade system performance and to lead to instability and failures. Current techniques for understanding the impact of selfish behaviours and designing effective countermeasures remain manual and time-consuming, requiring multi-domain expertise. To overcome these difficulties, we propose SEINE, a simulation framework for rapid modelling and evaluation of selfish behaviours in a cooperative system. SEINE relies on a domain-specific language (SEINE-L) for specifying selfishness scenarios, and provides semi-automatic support for their implementation and study in a state-of-the-art simulator. We show in this paper that (1) SEINE-L is expressive enough to specify fifteen selfishness scenarios taken from the literature, (2) SEINE is\u00a0\u2026", "num_citations": "6\n", "authors": ["518"]}
{"title": "Application-level optimizations on numa multicore architectures: the apache case study\n", "abstract": " Application-Level Optimizations on NUMA Multicore Architectures: the Apache Case Study Page 1 Application-Level Optimizations on NUMA Multicore Architectures: the Apache Case Study Fabien Gaud1, Renaud Lachaize2, Baptiste Lepers3, Gilles Muller1, Vivien Qu\u00e9ma3 1INRIA, 2Universit\u00e9 de Grenoble, 3CNRS May 12, 2010 1 / 26 Application-Level Optimizations on NUMA Multicore Architectures: the Apache Case Study Page 2 Objectives \u25b6 NUMA multicore architectures are becoming commonplace \u25b6 Application domain: Data servers, aka networked services \u25b6 Goal: Improve the performance of data servers on multicore architectures \u25b6 Contribution: Scaling the Apache Web server on NUMA multicore systems using application-level optimizations 2 / 26 Application-Level Optimizations on NUMA Multicore Architectures: the Apache Case Study Page 3 Problem 0 2000 4000 6000 8000 10000 1 2 3 4 # of \u2026", "num_citations": "6\n", "authors": ["518"]}
{"title": "How Often do Experts Make Mistakes?\n", "abstract": " Large open-source software projects involve developers with a wide variety of backgrounds and expertise. Such software projects furthermore include many internal APIs that developers must understand and use properly. According to the intended purpose of these APIs, they are more or less frequently used, and used by developers with more or less expertise. In this paper, we study the impact of usage patterns and developer expertise on the rate of defects occurring in the use of internal APIs. For this preliminary study, we focus on memory management APIs in the Linux kernel, as the use of these has been shown to be highly error prone in previous work. We study defect rates and developer expertise, to consider e.g., whether widely used APIs are more defect prone because they are used by less experienced developers, or whether defects in widely used APIs are more likely to be fixed.", "num_citations": "6\n", "authors": ["518"]}
{"title": "Automatic verification of bossa scheduler properties\n", "abstract": " Bossa is a development environment for operating-system process schedulers that provides numerous safety guarantees. In this paper, we show how to automate the checking of safety properties of a scheduling policy developed in this environment. We find that most of the relevant properties can be considered as invariant or refinement properties. In order to automate the related proof obligations, we use the WS1S logic for which a decision procedure is implemented by Mona. The proof techniques are implemented using the FMona tool.", "num_citations": "6\n", "authors": ["518"]}
{"title": "A stable transactional memory for building robust object oriented programs\n", "abstract": " The purpose of the Fault Tolerant Multiprocessor project (FTM) is to design a fault tolerant machine based on a Stable Transactional Memory (STM). The STM allows manipulation of stable objects within atomic transactions. The building of the operating system has led us to define a C++ interface to the STM which provides stable classes. Robust object oriented programs which resist processor failures can be written in C++ using stable objects and transactions.", "num_citations": "6\n", "authors": ["518"]}
{"title": "{SPINFER}: Inferring Semantic Patches for the Linux Kernel\n", "abstract": " In a large software system such as the Linux kernel, there is a continual need for large-scale changes across many source files, triggered by new needs or refined design decisions. In this paper, we propose to ease such changes by suggesting transformation rules to developers, inferred automatically from a collection of examples. Our approach can help automate large-scale changes as well as help understand existing large-scale changes, by highlighting the various cases that the developer who performed the changes has taken into account. We have implemented our approach as a tool, Spinfer. We evaluate Spinfer on a range of challenging large-scale changes from the Linux kernel and obtain rules that achieve 86% precision and 69% recall on average.", "num_citations": "5\n", "authors": ["518"]}
{"title": "On the design of a domain-specific language for OS process-scheduling extensions\n", "abstract": " Archive ouverte HAL - On the design of a domain-specific language for OS process-scheduling extensions Acc\u00e9der directement au contenu Acc\u00e9der directement \u00e0 la navigation Toggle navigation CCSD HAL HAL HALSHS TEL M\u00e9diHAL Liste des portails AUR\u00e9HAL API Data Documentation Episciences.org Episciences.org Revues Documentation Sciencesconf.org Support hal Accueil D\u00e9p\u00f4t Consultation Les derniers d\u00e9p\u00f4ts Par type de publication Par discipline Par ann\u00e9e de publication Par structure de recherche Les portails de l'archive Recherche Documentation hal-00457190, version 1 Communication dans un congr\u00e8s On the design of a domain-specific language for OS process-scheduling extensions Julia Lawall 1, 2 Gilles Muller 1 Anne Fran\u00e7oise Le Meur D\u00e9tails 1 LINA - Laboratoire d'Informatique de Nantes Atlantique 2 Department of Computer Science R\u00e9sum\u00e9 : no abstract Type de document : \u2026", "num_citations": "5\n", "authors": ["518"]}
{"title": "\u00abExtensibilit\u00e9e Dynamique dans les Caches Web: une Approche par Aspects\u00bb\n", "abstract": " Les caches Web sont une solution \u00e9prouv\u00e9e pour am\u00e9liorer les performances du r\u00e9seau Internet. Cependant, la multiplication des services et la g\u00e9n\u00e9ration dynamique des pages conduisenta une diversification des besoins vis-a-vis du cache. Il devient important de pouvoir sp\u00e9cialisera la vol\u00e9e le cache au service acc\u00e9d\u00e9. Le t\u00e9l\u00e9chargement dynamique de modules n\u2019apporte qu\u2019une r\u00e9ponse partielle: dans ce cas, l\u2019interaction entre le cache et les modules est limit\u00e9e aux interfaces pr\u00e9vues par ce dernier. Une fois le cache d\u00e9velopp\u00e9, ces interfaces sont fig\u00e9es tandis que les services se multiplient et que de nouveaux besoins continuenta se faire jour.Dans cet article, nous proposons d\u2019utiliser la programmation par aspect afin de pouvoir \u00e9tendrea la vol\u00e9e les fonctionnalit\u00e9s offertes par les caches. Pour appliquer notre approche, nous avons con\u00e7u notre propre systemea aspects, \u03bcDYNER, et d\u00e9riv\u00e9 du cache Web libre le plus r\u00e9pandu, SQUID, un cache Web dynamiquement extensible. Dans le cas d\u2019un module de pr\u00e9chargement des acces, le co\u00fbt de l\u2019extensibilit\u00e9 dynamique apport\u00e9e est n\u00e9gligeable.", "num_citations": "5\n", "authors": ["518"]}
{"title": "A domain-specific language approach to programmable networks\n", "abstract": " Active networks present significant safety, security, and efficiency challenges. Domain-specific languages, i.e., languages providing only constructs relevant to a particular domain, provide a solution that balances these constraints. Safety and security can be ensured using verification techniques that exploit the restricted nature of such languages. Strategies have been developed for the compilation of domain-specific languages that provide both portability and efficiency. This paper presents a synthesis of work on the PLAN-P domain-specific language for programmable routers. We present the language design, representative experiments that have been carried out using the language, and new compilation strategies. End-to-end performance is typically comparable to that of hand-coded C implementations.", "num_citations": "5\n", "authors": ["518"]}
{"title": "Bossa: une approche langage \u00e1 la conception d\u2019ordonnanceurs de processus\n", "abstract": " Il existea l\u2019heure actuelle de nombreux travaux visanta proposer de nouvelles politiques d\u2019ordonnancement de processus. Toutefois, d \u00e9velopper un ordonnanceur reste une t\u00e2che difficile qui requierta la fois une bonne expertise du noyau et une programmation bas niveau. Dans ce papier, nous pr\u00e9sentons Bossa, un langage dedi\u00e9 au d\u00e9veloppement de politiques d\u2019ordonnancement. Bossa fournit des abstractions de haut-niveau sp\u00e9cifiques au domaine de l\u2019ordonnancement qui simplifient le d\u00e9veloppement et permettent la v\u00e9rification des propri\u00e9t\u00e9s de s\u00fbret\u00e9 sur une politique d\u2019ordonnancement. Nous pr\u00e9sentons Bossaa partir de la description de la politique d\u2019ordonnancement de Linux.", "num_citations": "5\n", "authors": ["518"]}
{"title": "Automatic optimization of the Sun RPC protocol implementation via partial evaluation\n", "abstract": " We report here an experiment of using partial evaluation on a realistic program, namely the Sun commercial RPC protocol. RPC is a highly generic software that offers several opportunities of specialization. We used Tempo, a partial evaluator for C programs targeted towards system software.", "num_citations": "5\n", "authors": ["518"]}
{"title": "Stable transactional memories and fault tolerant architectures\n", "abstract": " In order to provide powerful and reliable computers, many fault tolerant multiprocessor architectures have been developped in recent years. These architectures can be subdivided into two categories:\u2022 loosely coupled category in which each processor has private ressources and communicates with other processors through message passing protocols,\u2022 tightly coupled category in which each processor directly accesses all the memory and I/O ressources and communicate with other processors through shared memory.Apart from N-modular redundancy techniques like the one proposed in System/88 [Harr87] or FTMP [Hopk78] computers which are valid for all types of multiprocessors, different means to tolerate processor failures are adopted dependent upon the type of architecture. One method of achieving fault tolerance on loosely coupled architectures uses a process-pair scheme. Every process running in the\u00a0\u2026", "num_citations": "5\n", "authors": ["518"]}
{"title": "Architecture of fault-tolerant multiprocessor workstations\n", "abstract": " A fault-tolerant multiprocessor architecture that is based on standard processors associated with stable storage boards is presented. The hardware architecture of the stable storage board and its software interface are briefly described. The hardware organization of the fault-tolerant multiprocessor is detailed, and some functionalities of a secure kernel are examined. The current status of the project is indicated.< >", "num_citations": "5\n", "authors": ["518"]}
{"title": "Towards Proving Optimistic Multicore Schedulers\n", "abstract": " Operating systems have been shown to waste machine resources by leaving cores idle while work is ready to be scheduled. This results in suboptimal performance for user applications, and wasted power.", "num_citations": "4\n", "authors": ["518"]}
{"title": "Memory monitoring in a multi-tenant OSGi execution environment\n", "abstract": " Smart Home market players aim to deploy component-based and service-oriented applications from untrusted third party providers on a single OSGi execution environment. This creates the risk of resource abuse by buggy and malicious applications, which raises the need for resource monitoring mechanisms. Existing resource monitoring solutions either are too intrusive or fail to identify the relevant resource consumer in numerous multi-tenant situations. This paper proposes a system to monitor the memory consumed by each tenant, while allowing them to continue communicating directly to render services. We propose a solution based on a list of configurable resource accounting rules between tenants, which is far less intrusive than existing OSGi monitoring systems. We modified an experimental Java Virtual Machine in order to provide the memory monitoring features for the multi-tenant OSGi environment. Our\u00a0\u2026", "num_citations": "4\n", "authors": ["518"]}
{"title": "Herodotos: A tool to expose bugs' lives\n", "abstract": " Software is continually evolving, to improve performance, correct errors, and add new features. Code modifications, however, inevitably lead to the introduction of defects. To prevent the introduction of defects, one has to understand why they occur. Thus, it is important to develop tools and practices that aid in defect finding, tracking and prevention. In this paper, we propose a methodology and associated tool, Herodotos, to study defects over time. Herodotos semi-automatically tracks defects over multiple versions of a software project, independent of other changes in the source files. It builds a graphical history of each defect and gives some statistics based on the results. We have evaluated this approach on the history of a representative range of open source projects over the last three years. For each project, we explore several kinds of defects that have been found by static code analysis. We analyze the generated results to compare the selected software projects and defect kinds.", "num_citations": "4\n", "authors": ["518"]}
{"title": "Automating the port of Linux to the VirtualLogix hypervisor using semantic patches\n", "abstract": " Virtualization is a promising technology for running multiple operating systems (OS\u2019s) on a single processor. Preparing an OS for use with virtualization, however, involves making some changes to the OS code, which must be repeated for each version, whether a new release or a client customization. Typically such changes are expressed as patches, but patches are often not portable from one version to another, and thus manual adjustments are needed as well. In this paper, we consider the use of the automated transformation system Coccinelle to perform the changes required to port several versions of Linux to the VLX hypervisor. Coccinelle provides a notion of semantic patches, which are more abstract than standard patches, and thus are potentially applicable to a wider range of OS versions. We have applied this approach in the context of Linux versions 2.6.13, 2.6.14, and 2.6.15, for the ARM architecture.", "num_citations": "4\n", "authors": ["518"]}
{"title": "Automating the Porting of Linux to the VirtualLogix Hypervisor using Semantic Patches\n", "abstract": " Virtualization is a promising technology for running multiple operating systems (OS\u2019s) on a single processor. Preparing an OS for use with virtualization, however, involves making some changes to the OS code, which must be repeated for each version, whether a new release or a client customization. Typically such changes are expressed as patches, but patches are often not portable from one version to another, and thus manual adjustments are needed as well. In this paper, we consider the use of the automated transformation system Coccinelle to perform the changes required to port several versions of Linux to the VLX hypervisor. Coccinelle provides a notion of semantic patches, which are more abstract than standard patches, and thus are potentially applicable to a wider range of OS versions. We have applied this approach in the context of Linux versions 2.6.13, 2.6.14, and 2.6.15, for the ARM architecture.", "num_citations": "4\n", "authors": ["518"]}
{"title": "Conception et r\u00e9alisation d'une machine multiprocesseur s\u00fbre de fonctionnement\n", "abstract": " Etude de la conception d'une nouvelle architecture de machines sures de fonctionnement. La caracteristique principale de cette architecture est la possibilite de concevoir une machine sure de fonctionnement a partir d'une ou plusieurs machines standards et d'un composant sur de fonctionnement appele memoire stable", "num_citations": "4\n", "authors": ["518"]}
{"title": "Improving Prediction Accuracy of Memory Interferences for Multicore Platforms\n", "abstract": " Memory interferences may introduce important slowdowns in applications running on COTS multi-core processors. They are caused by concurrent accesses to shared hardware resources of the memory system. The induced delays are difficult to predict, making memory interferences a major obstacle to the adoption of COTS multi-core processors in real-time systems. In this article, we propose an experimental characterization of applications' memory consumption to determine their sensitivity to memory interferences. Thanks to a new set of microbenchmarks, we show the lack of precision of a purely quantitative characterization. To improve accuracy, we define new metrics quantifying qualitative aspects of memory consumption and implement a profiling tool using the VALGRIND framework. In addition, our profiling tool produces high resolution profiles allowing us to clearly distinguish the various phases in\u00a0\u2026", "num_citations": "3\n", "authors": ["518"]}
{"title": "JMake: Dependable Compilation for Kernel Janitors\n", "abstract": " The Linux kernel is highly configurable, and thus, in principle, any line of code can be included or excluded from the compiled kernel based on configuration operations. Configurability complicates the task of a kernel janitor, who cleans up faults across the code base. A janitor may not be familiar with the configuration options that trigger compilation of a particular code line, leading him to believe that a fix has been compile-checked when this is not the case. We propose JMake, a mutation-based tool for signaling changed lines that are not subjected to the compiler. JMake shows that for most of the 12,000 file-modifying commits between Linux v4.3 and v4.4 the configuration chosen by the kernel allyesconfig option is sufficient, once the janitor chooses the correct architecture. For most commits, this check requires only 30 seconds or less. We then characterize the situations in which changed code is not subjected to\u00a0\u2026", "num_citations": "3\n", "authors": ["518"]}
{"title": "Prequel: A Patch-Like Query Language for Commit History Search\n", "abstract": " The commit history of a code base such as the Linux kernel is a gold mine of information on how evolutions should be made, how bugs should be fixed, etc.  Nevertheless, the high volume of commits available and the rudimentary filtering tools provided mean that it is often necessary to wade through a lot of irrelevant information before finding example commits that can help with a specific software development problem.  To address this issue, we propose Prequel (Patch Query Language), which brings the descriptive power of code matching to the problem of querying a commit history.  We show in particular how Prequel can be used in understanding how to eliminate uses of deprecated functions.", "num_citations": "3\n", "authors": ["518"]}
{"title": "Understanding the Memory Consumption of the MiBench Embedded Benchmark\n", "abstract": " Complex embedded systems today commonly involve a mix of real-time and best-effort applications. The recent emergence of small low-cost commodity multi-core processors raises the possibility of running both kinds of applications on a single machine, with virtualization ensuring that the best-effort applications cannot steal CPU cycles from the real-time applications. Nevertheless, memory contention can introduce other sources of delay, that can lead to missed deadlines. In this paper, we analyze the sources of memory consumption for the real-time applications found in the MiBench embedded benchmark suite.", "num_citations": "3\n", "authors": ["518"]}
{"title": "Incinerator\u2013Eliminating Stale References in Dynamic OSGi Applications\n", "abstract": " Java class loaders are commonly used in application servers to load, unload and update a set of classes as a unit. However, unloading or updating a class loader can introduce stale references to the objects of the outdated class loader. A stale reference leads to a memory leak and, for an update, to an inconsistency between the outdated classes and their replacements. To detect and eliminate stale references, we propose Incinerator, a Java virtual machine extension that introduces the notion of an outdated class loader. Incinerator detects stale references and sets them to null during a garbage collection cycle. We evaluate Incinerator in the context of the OSGi framework and show that Incinerator correctly detects and eliminates stale references, including a bug in Knopflerfish. We also evaluate the performance of Incinerator with the DaCapo benchmark on VMKit and show that Incinerator has an overhead of at\u00a0\u2026", "num_citations": "3\n", "authors": ["518"]}
{"title": "Supporting Time-Based QoS Requirements in Software Transactional Memory\n", "abstract": " Software transactional memory (STM) is an optimistic concurrency control mechanism that simplifies parallel programming. However, there has been little interest in its applicability to reactive applications in which there is a required response time for certain operations. We propose supporting such applications by allowing programmers to associate time with atomic blocks in the form of deadlines and quality-of-service (QoS) requirements. Based on statistics of past executions, we adjust the execution mode of transactions by decreasing the level of optimism as the deadline approaches. In the presence of concurrent deadlines, we propose different conflict resolution policies. Execution mode switching mechanisms allow the meeting of multiple deadlines in a consistent manner, with potential QoS degradations being split fairly among several threads as contention increases, and avoiding starvation. Our\u00a0\u2026", "num_citations": "3\n", "authors": ["518"]}
{"title": "Oops! What about a Million Kernel Oopses?\n", "abstract": " When a failure occurs in the Linux kernel, the kernel emits an \"oops\", summarizing the execution context of the failure. Kernel oopses describe real Linux errors, and thus can help prioritize debugging efforts and motivate the design of tools to improve the reliability of Linux code. Nevertheless, the information is only meaningful if it is representative and can be interpreted correctly. In this paper, we study a repository of kernel oopses collected over 8 months by Red Hat. We consider the overall features of the data, the degree to which the data reflects other information about Linux, and the interpretation of features that may be relevant to reliability. We find that the data correlates well with other information about Linux, but that it suffers from duplicate and missing information. We furthermore identify some potential pitfalls in studying features such as the sources of common faults and common failing applications.", "num_citations": "3\n", "authors": ["518"]}
{"title": "Optimisations applicatives pour multi-c\u0153urs numa: un cas d\u2019\u00e9tude avec le serveur web apache\n", "abstract": " Les machines multi-c\u0153urs \u00e0 acc\u00e8s m\u00e9moire non uniforme (NUMA) sont de plus en plus r\u00e9pandues. Il est donc n\u00e9cessaire de comprendre comment les exploiter efficacement. La plupart des travaux men\u00e9s dans ce domaine choisissent de r\u00e9soudre ce probl\u00e8me au niveau du noyau du syst\u00e8me d\u2019exploitation, en am\u00e9liorant les abstractions fournies aux applications ou en proposant de nouvelles architectures logicielles pour am\u00e9liorer le passage \u00e0 l\u2019\u00e9chelle des applications sur ce type de machines. Dans ce papier, nous adoptons un point de vue compl\u00e9mentaire: nous \u00e9tudions comment am\u00e9liorer le couple d\u2019applications Apache-PHP au dessus d\u2019un noyau Linux standard. Nous mettons en lumi\u00e8re trois diff\u00e9rents probl\u00e8mes de performance \u00e0 diff\u00e9rents niveaux du syst\u00e8me:(i) un nombre d\u2019acc\u00e8s m\u00e9moire distants trop important,(ii) un \u00e9quilibrage de charge inefficace entre les processeurs et (iii) de la contention sur des structures de donn\u00e9es du noyau. Nous proposons et implantons des solutions au niveau applicatif pour chacun de ces probl\u00e8mes. Notre version optimis\u00e9e des applications Apache et PHP a un d\u00e9bit 33% plus important que ces applications non modifi\u00e9es sur une machine \u00e0 16 c\u0153urs. Nous concluons en explicitant les le\u00e7ons tir\u00e9es lors de ces travaux sur l\u2019am\u00e9lioration de la performance des serveurs sur les architectures multi-c\u0153urs NUMA.", "num_citations": "3\n", "authors": ["518"]}
{"title": "Applying the B formal method to the Bossa domain-specific language\n", "abstract": " Domain-specific languages (DSLs) are used in both industry and research, in complex areas as varied as data mining [5], graphics [7], and device driver development [10]. A DSL provides high-level domain-specific abstractions that encapsulate domain expertise, thus making programming easier and less error-prone. Such languages also promise to be more amenable to verification, as the set of language abstractions can be designed to be easy to relate to desired properties and can be constrained to avoid problematic constructs such as unbounded loops. Nevertheless, many DSLs provide no verification, and those that do typically either rely on verification provided by a general-purpose host language or use ad hoc analyzers. The former approach is, however, limited to the facilities of the host language, which are rarely adequate for expressing and checking domain-specific properties, while the latter puts a huge burden on the DSL developer. We observe that many powerful verification systems have been developed in the formal methods community. Our goal is to realize the potential for verification of DSLs by harnessing these resources in designing and implementing DSL verifiers.To begin to bridge the gap between existing approaches to program verification and DSLs, we are applying the B formal method [1] to the Bossa DSL [9]. B is a refinementbased formal method that has been used for the development of safety critical software, especially in the domain of railway systems [2, 3]. The main feature of a B development process is that it proves that the final code implements its formal specification. Bossa is a DSL for specifying operating\u00a0\u2026", "num_citations": "3\n", "authors": ["518"]}
{"title": "Towards a Scheduling Framework for Dynamically Downloaded Multimedia Applications\n", "abstract": " Multimedia applications are well-known to have specific scheduling requirements. To address this issue, we propose that a multimedia application that is made available over the Internet should be accompanied by an appropriate scheduling policy. In previous work, we have introduced the Bossa framework, which facilitates the implementation of new scheduling policies, making it possible for an application developer to implement an appropriate scheduling policy concomitantly with an application. The machine independence and security guarantees provided by .NET are ideal for distributing such a coupled application and scheduling policy over the Internet.", "num_citations": "3\n", "authors": ["518"]}
{"title": "Experience with Building Distributed Systems on top of the Mach Microkernel\n", "abstract": " Three alternative design approaches may be taken by research groups wishing to build a distributed system prototype. The first one, notably adopted by the Clouds project [Dasgupta 88] consists of building the prototype on a bare machine. The second one is to use an existing monolithic operating system (Unix in most cases). This approach was chosen in Emerald [Black 86], Argus [Liskov 87] and Guide-1 [Balter 91]. Finally, the third approach is to build the prototype on top of a micro-kernel, like for instance the V-Kernel [Cheriton 84], Mach [Accetta 86] or Chorus [Rozier 88]. Several research prototypes have been built following this last approach since the early nineties (Munin [Carter 91], Opal [Chase 92], Cricket [Shekita 91] in the United States and EOS [Gruber 92a], COOL [Lea 93], Gothic [Ban\u00e2tre 94], FTM [Ban\u00e2tre 91a], Guide-2 [Hagimont 94a] in Europe).The success of this third approach can be explained by the additional flexibility and efficiency of micro-kernels compared to monolithic systems like Unix, while keeping Unix functionalities available. This explains why microkernels, which were primarily designed for defining an easily portable minimal abstract machine on top of which operating systems can be implemented, were selected to support experimental systems designed to assess the value of new \u201cmiddleware\u201d architectures.", "num_citations": "3\n", "authors": ["518"]}
{"title": "BMC: Accelerating Memcached using Safe In-kernel Caching and Pre-stack Processing\n", "abstract": " In-memory key-value stores are critical components that help scale large internet services by providing low-latency access to popular data. Memcached, one of the most popular key-value stores, suffers from performance limitations inherent to the Linux networking stack and fails to achieve high performance when using high-speed network interfaces. While the Linux network stack can be bypassed using DPDK based solutions, such approaches require a complete redesign of the software stack and induce high CPU utilization even when client load is low.To overcome these limitations, we present BMC, an inkernel cache for Memcached that serves requests before the execution of the standard network stack. Requests to the BMC cache are treated as part of the NIC interrupts, which allows performance to scale with the number of cores serving the NIC queues. To ensure safety, BMC is implemented using eBPF. Despite the safety constraints of eBPF, we show that it is possible to implement a complex cache service. Because BMC runs on commodity hardware and requires modification of neither the Linux kernel nor the Memcached application, it can be widely deployed on existing systems. BMC optimizes the processing time of Facebook-like small-size requests. On this target workload, our evaluations show that BMC improves throughput by up to 18x compared to the vanilla Memcached application and up to 6x compared to an optimized version of Memcached that uses the SO_REUSEPORT socket flag. In addition, our results also show that BMC has negligible overhead and does not deteriorate throughput when treating non-target workloads.", "num_citations": "2\n", "authors": ["518"]}
{"title": "AndroEvolve: Automated Android API Update with Data Flow Analysis and Variable Denormalization\n", "abstract": " The Android operating system is frequently updated, with each version bringing a new set of APIs. New versions may involve API deprecation; Android apps using deprecated APIs need to be updated to ensure the apps' compatibility withold and new versions of Android. Updating deprecated APIs is a time-consuming endeavor. Hence, automating the updates of Android APIs can be beneficial for developers. CocciEvolve is the state-of-the-art approach for this automation. However, it has several limitations, including its inability to resolve out-of-method-boundary variables and the low code readability of its update due to the addition of temporary variables. In an attempt to further improve the performance of automated Android API update, we propose an approach named AndroEvolve, which addresses the limitations of CocciEvolve through the addition of data flow analysis and variable name denormalization. Data flow analysis enables AndroEvolve to resolve the value of any variable within the file scope. Variable name denormalization replaces temporary variables that may present in the CocciEvolve update with appropriate values in the target file. We have evaluated the performance of AndroEvolve and the readability of its updates on 360 target files. AndroEvolve produces 26.90% more instances of correct updates compared to CocciEvolve. Moreover, our manual and automated evaluation shows that AndroEvolve updates are more readable than CocciEvolve updates.", "num_citations": "2\n", "authors": ["518"]}
{"title": "Memory virtualization in virtualized systems: segmentation is better than paging\n", "abstract": " The utilization of paging for virtual machine (VM) memory management is the root cause of memory virtualization overhead. This paper shows that paging is not necessary in the hypervisor. In fact, memory fragmentation, which explains paging utilization, is not an issue in virtualized datacenters thanks to VM memory demand patterns. Our solution Compromis, a novel Memory Management Unit, uses direct segment for VM memory management combined with paging for VM's processes. The paper presents a systematic methodology for implementing Compromis in the hardware, the hypervisor and the datacenter scheduler. Evaluation results show that Compromis outperforms the two popular memory virtualization solutions: shadow paging and Extended Page Table by up to 30% and 370% respectively.", "num_citations": "2\n", "authors": ["518"]}
{"title": "Fork/Wait and Multicore Frequency Scaling: a Generational Clash\n", "abstract": " The complexity of computer architectures has risen since the early years of the Linux kernel: Simultaneous Multi-Threading (SMT), multicore processing, and frequency scaling with complex algorithms such as Intel\u00ae Turbo Boost have all become omnipresent. In order to keep up with hardware innovations, the Linux scheduler has been rewritten several times, and many hardware-related heuristics have been added. Despite this, we show in this paper that a fundamental problem was never identified: the POSIX process creation model, ie, fork/wait, can behave inefficiently on current multicore architectures due to frequency scaling. We investigate this issue through a simple case study: the compilation of the Linux kernel source tree. To do this, we develop SchedLog, a low-overhead scheduler tracing tool, and SchedDisplay, a scriptable tool to graphically analyze SchedLog's traces efficiently.", "num_citations": "2\n", "authors": ["518"]}
{"title": "Automatic Code Generation for Iterative Multi-dimensional Stencil Computations\n", "abstract": " We present a source-to-source auto-generating framework that enables a large programmer community to easily and safely implement parallel stencil codes within the framework of Ordered Read-Write Locks (ORWL). It meets the specific needs of the application at a high level of abstraction. ORWL is an inter-task synchronization model for iterative data-oriented parallel and distributed algorithms that uses strict FIFO ordering for the access to all resources. It guarantees equity, liveness and efficiency for a wide range of applications. The main hurdle for using ORWL lies in its initialization phase, where the programmer has to specify the access scheme between tasks and resources and the initial positions of the tasks in the FIFOs. We provide a user-friendly interface based on a Domain-Specific Language (DSL) that captures domain semantics and automatically generates ORWL parallel high-performance stencil\u00a0\u2026", "num_citations": "2\n", "authors": ["518"]}
{"title": "Fastlane: Streamlining transactions for low thread counts\n", "abstract": " Software transactional memory (STM) can lead to scalable implementations of concurrent programs, as the relative performance of an application increases with the number of threads that support it. However, the absolute performance is typically impaired by the overheads of transaction management and instrumented accesses to shared memory. This often leads STM-based programs with low thread counts to perform worse than a sequential, non-instrumented version of the same application.In this paper, we propose FASTLANE, a new STM system that bridges the performance gap between sequential execution and classical STM algorithms when running on few cores. FASTLANE seeks to reduce instrumentation costs and thus performance degradation in its target operation range. We introduce a family of algorithms that differentiate between two types of threads: One thread (the master) is allowed to commit transactions without aborting, thus with minimal instrumentation and management costs, while other threads (the helpers) can commit transactions only when they do not conflict with the master. Helpers thus contribute to the application progress without impairing on the performance of the master.", "num_citations": "2\n", "authors": ["518"]}
{"title": "Improving the Security of Infrastructure Software using Coccinelle.\n", "abstract": " Finding and fixing programming errors in deployed software is often a slow, painstaking, and expensive process. In order to minimise this problem, static analysis is increasingly being adopted as a way to find programming errors before the software application is released. Coccinelle is a program matching and transfor-mation tool that makes it easy for developers to express static analysis-based software defect-finding rules and scan software source code for potential defects.", "num_citations": "2\n", "authors": ["518"]}
{"title": "Transaction Activation scheduling Support for Transactional Memory\n", "abstract": " Transactional Memory (TM) is considered as one of the most promising paradigms for developing concurrent applications. TM has been shown to scale well on multiple cores when the data access pattern behaves \u201cwell,\u201d i.e., when few conflicts are induced. In contrast, data patterns with frequent write sharing, with long transactions, or when many threads contend for a smaller number of cores, produce numerous aborts. These problems are traditionally addressed by application-level contention managers, but they suffer from a lack of precision and provide unpredictable benefits on many workloads. In this paper, we propose a system approach where the scheduler tries to avoid aborts by preventing conflicting transactions from running simultaneously. We use a combination of several techniques to help reduce the odds of conflicts, by (1) avoiding preempting threads running a transaction until the transaction completes, (2) keeping track of conflicts and delaying the restart of a transaction until conflicting transactions have committed, and (3) keeping track of conflicts and only allowing a thread with conflicts to run at low priority. Our approach has been implemented in Linux for Software Transactional Memory (STM) using a shared memory segment to allow fast communication between the STM library and the scheduler. It only requires small and contained modifications to the operating system. Experimental evaluation demonstrates that our approach significantly reduces the number of aborts while improving transaction throughput on various workloads.", "num_citations": "2\n", "authors": ["518"]}
{"title": "Towards documenting and automating collateral evolutions in linux device drivers\n", "abstract": " Collateral evolutions are a pervasive problem in Linux device driver development, due to the frequent evolution of Linux driver support libraries and APIs. Such evolutions are needed when an evolution in a driver support library affects the library's interface, entailing modifications in all dependent device-specific code. Currently, collateral evolutions in Linux are done nearly manually. The large number of Linux drivers, however, implies that this approach is time-consuming and unreliable, leading to subtle errors when modifications are not done consistently. In this paper, we describe the development of a language-based infrastructure, Coccinelle, with the goal of documenting and automating the kinds of collateral evolutions that occur in device driver code. Because Linux programmers are accustomed to manipulating program modifications in terms of patch files, we base our language on the patch syntax, extending patches to semantic patches.", "num_citations": "2\n", "authors": ["518"]}
{"title": "Devil: un IDL pour les contr\u00f4leurs de p\u00e9riph\u00e9riques\n", "abstract": " Pour suivre la cadence erenee a laquelle de nouveaux peripheriques sortent sur le marche, les pilotes correspondants doivent souvent^ etre developpes dans l\u2019urgence. Bien que les pilotes de peripheriques soient des composants critiques des systemes d\u2019exploitation, leur processus de developpement est rudimentaire. Cela est particulierement agrant en ce qui concerne la couche basse des pilotes, chargee de la communication directe avec les contr^ oleurs de peripheriques. D\u2019une part, developper cette couche necessite l\u2019etude de documentations trop souvent imprecises ou incompletes. D\u2019autre part, cette couche utilise surtout des operateurs bit a bit, pour lesquels le langage C n\u2019ore pas plus de s^ urete que l\u2019assembleur. Il en ressort que la couche basse des pilotes est dicile a ecrire, et souvent source d\u2019erreurs. Cet article presente une nouvelle approche du developpement de la couche basse des pilotes\u00a0\u2026", "num_citations": "2\n", "authors": ["518"]}
{"title": "A reliable client-server model on top of a micro-kernel\n", "abstract": " The recently emerged micro-kernel technology is now well recognized as a base mechanism for building distributed systems. This paper addresses the problem of designing a fault tolerant operating system while keeping the advantages of the micro-kernel technology. We introduce a solution based on standard workstations and on global consistent state computation using dynamic atomic actions. The advantages of our solution are that it does not introduce RPC performance degradation and that it avoids complete duplication of workstations, thus offering a satisfactory performance/cost ratio.", "num_citations": "2\n", "authors": ["518"]}
{"title": "AndroEvolve: Automated Update for Android Deprecated-API Usages\n", "abstract": " The Android operating system (OS) is often updated, where each new version may involve API deprecation. Usages of deprecated APIs in Android apps need to be updated to ensure the apps\u2019 compatibility with the old and new versions of the Android OS. In this work, we propose AndroEvolve, an automated tool to update usages of deprecated Android APIs, that addresses the limitations of the state-of-the-art tool, CocciEvolve. AndroEvolve utilizes data flow analysis to solve the problem of out-of-method-boundary variables, and variable denormalization to remove the temporary variables introduced by CocciEvolve. We evaluated the accuracy of AndroEvolve using a dataset of 360 target files and 20 deprecated Android APIs, where AndroEvolve is able to produce 319 correct updates, compared to CocciEvolve which only produces 249 correct updates. We also evaluated the readability of AndroEvolve\u2019s update\u00a0\u2026", "num_citations": "1\n", "authors": ["518"]}
{"title": "(No) Compromis: Paging Virtualization Is Not a Fatality\n", "abstract": " Nested/Extended Page Table (EPT) is the current hardware solution for virtualizing memory in virtualized systems. It induces a significant performance overhead due to the 2D page walk it requires, thus 24 memory accesses on a TLB miss (instead of 4 memory accesses in a native system). This 2D page walk constraint comes from the utilization of paging for managing virtual machine (VM) memory. This paper shows that paging is not necessary in the hypervisor. Our solution Compromis, a novel Memory Management Unit, uses direct segments for VM memory management combined with paging for VM's processes. This is the first time that a direct segment based solution is shown to be applicable to the entire VM memory while keeping applications unchanged. Relying on the 310 studied datacenter traces, the paper shows that it is possible to provision up to 99.99% of the VMs using a single memory segment. The paper presents a systematic methodology for implementing Compromis in the hardware, the hypervisor and the datacenter scheduler. Evaluation results show that Compromis outperforms the two popular memory virtualization solutions: shadow paging and EPT by up to 30% and 370% respectively.", "num_citations": "1\n", "authors": ["518"]}
{"title": "Fewer Cores, More Hertz: Leveraging High-Frequency Cores in the {OS} Scheduler for Improved Application Performance\n", "abstract": " In modern server CPUs, individual cores can run at different frequencies, which allows for fine-grained control of the performance/energy tradeoff. Adjusting the frequency, however, incurs a high latency. We find that this can lead to a problem of frequency inversion, whereby the Linux scheduler places a newly active thread on an idle core that takes dozens to hundreds of milliseconds to reach a high frequency, just before another core already running at a high frequency becomes idle.", "num_citations": "1\n", "authors": ["518"]}
{"title": "The future depends on the low-level stuff\n", "abstract": " Device drivers are essential to modern computing, to provide applications with access, via the operating system (OS), to devices such as keyboards, disks, networks, and cameras. Development of new computing paradigms, such as the internet of things, is hampered because device driver development is challenging and error-prone, requiring a high level of expertise in both the targeted OS and the specific device. Furthermore, implementing just one driver is often not sufficient; today\u2019s computing landscape is characterized by a number of OSes, eg, Linux, Windows, MacOS, and BSD, and each is found in a wide range of variants and versions. All of these factors make the development, porting, backporting, and maintenance of device drivers a critical problem for device manufacturers, industry that requires specific devices, and even ordinary users. Recent years have seen a number of approaches directed towards easing device driver development. Merillon et al. propose Devil [10], a domain-specific language for describing the low-level interface of a device. Chipounov et al. propose RevNic [3], a template-based approach for porting device drivers from one OS to another. Ryzhyk et al. propose Termite [14], an approach for synthesizing device driver code from a specification of an OS and a device. Currently, these approaches have been successfully applied to only a small number of toy drivers. Indeed, Kadav and Swift [5] observe that these approaches make assumptions that are not satisfied by many drivers; for example, that a driver involves little computation other than the direct interaction between the OS and the device. At the same time, a\u00a0\u2026", "num_citations": "1\n", "authors": ["518"]}
{"title": "Understanding the genetic makeup of Linux device drivers\n", "abstract": " Attempts have been made to understand driver development in terms of code clones. In this paper, we propose an alternate view, based on the metaphor of a gene. Guided by this metaphor, we study the structure of Linux 3.10 ethernet platform driver probe functions.", "num_citations": "1\n", "authors": ["518"]}
{"title": "REICoM: Robust and Efficient Inter-core Communications on Manycore Machines\n", "abstract": " Manycore machines are becoming an alternative to physically distributed systems. The software running on these machines more and more resembles distributed, message-passing applications. Consequently, these machines require robust and efficient inter-core communication mechanisms. In this paper, we study ten communication mechanisms that are considered state-ofthe-art. We show that only two communication mechanisms are robust, but that, unfortunately, they are not efficient. We do thus propose REICoM, a new intercore communication mechanism that is both robust and efficient. Using two macro-benchmarks (a consensus and a snapshot protocol), we show that REICoM consistently improves the performance over existing mechanisms.", "num_citations": "1\n", "authors": ["518"]}
{"title": "Report on 5th workshop on programming languages and operating systems (PLOS)\n", "abstract": " This report summarizes the Fifth Workshop on Programming Languages and Operating Systems (PLOS 2009), which was held in conjunction with the SOSP 2009 conference. This report presents the motivation for holding the workshop and summarizes the workshop contributions.", "num_citations": "1\n", "authors": ["518"]}
{"title": "Towards Class-Based Dynamic Voltage Scaling for Multimedia Applications\n", "abstract": " Video display has significant, but highly variable, CPU requirements. As such, it is an attractive target for power management via dynamic voltage scaling. In previous work, we have proposed a dynamic voltage scaling algorithm directed to the context of video kiosks, in which a minimal frequency for each frame can be determined experimentally based on observations taken during the first few iterations of the video. In this paper, we review that work, and begin to consider how such an approach can be adapted to the more common case where a video is only played once, on hardware that is not known in advance.", "num_citations": "1\n", "authors": ["518"]}
{"title": "V\u00e9rification automatique de propri\u00e9t\u00e9s d'ordonnanceurs Bossa\n", "abstract": " Archive ouverte HAL - V\u00e9rification automatique de propri\u00e9t\u00e9s d'ordonnanceurs Bossa Acc\u00e9der directement au contenu Acc\u00e9der directement \u00e0 la navigation Toggle navigation CCSD HAL HAL HALSHS TEL M\u00e9diHAL Liste des portails AUR\u00e9HAL API Data Documentation Episciences.org Episciences.org Revues Documentation Sciencesconf.org Support hal Accueil D\u00e9p\u00f4t Consultation Les derniers d\u00e9p\u00f4ts Par type de publication Par discipline Par ann\u00e9e de publication Par structure de recherche Les portails de l'archive Recherche Documentation hal-00457181, version 1 Communication dans un congr\u00e8s V\u00e9rification automatique de propri\u00e9t\u00e9s d'ordonnanceurs Bossa Jean-Paul Bodeveix 1 Mamoun Filali 1 Julia Lawall 2 Gilles Muller 3, 4 D\u00e9tails 1 IRIT-ACADIE - Assistance \u00e0 la Certification d\u2019Applications DIstribu\u00e9es et Embarqu\u00e9es IRIT - Institut de recherche en informatique de Toulouse 2 Department of Computer \u2026", "num_citations": "1\n", "authors": ["518"]}
{"title": "The Bossa Framework for Scheduler Development\n", "abstract": " Writing a new scheduler and integrating it into an existing OS is a daunting task, requiring the understanding of multiple low-level kernel mechanisms. Indeed, implementing a new scheduler is outside the expertise of application programmers, even though they are the ones who understand best the scheduling needs of their applications.We propose a framework, Bossa, to allow application programmers to implement kernel schedulers easily and safely. This framework defines a scheduling interface that is instantiated in a version of the Linux kernel by an Linux expert using an approach based on a variant of Aspect-Oriented Programming. Schedulers are written using a domain-specific language (DSL) that provides high-level scheduling-specific abstractions to simplify the programming of scheduling policies. The use of a DSL both eases scheduler programming and enables verification that a scheduling policy is compatible with OS requirements. We have found that Bossa gives good performance in practice. In this talk, we present the Bossa DSL, its implementation in Linux 2.4, and its use in the context of multimedia applications.", "num_citations": "1\n", "authors": ["518"]}
{"title": "Dealing with hardware in embedded software: a retargetable framework based on the devil language\n", "abstract": " Writing code that talks to hardware is a crucial part of any embedded project. Both productivity and quality are needed, but some flaws in the traditional development process make these requirements difficult to meet. We have recently introduced a new approach of dealing with hardware, based on the Devil language. Devil allows to write a high-level, formal definition of the programming interface of a peripheral circuit. A compiler automatically checks the consistency of a Devil specification, from which it generates the low-level, hardware-operating code. In our original framework, the generated code is dependent of the host architecture (CPU, buses, and bridges). Consequently, any variation in the hardware environment requires a specific tuning of the compiler. Considering the variability of embedded architectures, this is a serious drawback. In addition, this prevents from mixing different buses in the same circuit interface. In this paper, we remove those limitations by improving our framework in two ways. (i) We propose a better isolation between the Devil compiler and the host architecture. (ii) We introduce Trident, a language extension aimed at mapping one or several buses to each peripheral circuit.", "num_citations": "1\n", "authors": ["518"]}
{"title": "Towards verifiable device drivers: An approach based on domain-specific languages\n", "abstract": " Although peripheral devices come out at a frantic pace and require fast releases of drivers, little progress has been made to improve the development of drivers. Too often, this development consists of decoding hardware intricacies, based on ambiguous or incomplete documentation, to determine how to operate a device. Then, assembly-level operations need to be used to interact with the device. These low-level operations make the device driver fairly unreadable and prevent safety properties from being checked. This paper presents a language, named Devil, dedicated to defining the functional interface of a device. More precisely, Devil aims at specifying the access mechanisms, the type and layout of data, and behavioral properties involved in operating a device. The benefit of our approach is that, once compiled, a Devil description implements an interface which models an idealized device and abstracts the hardware intricacies. Unlike a general-purp- ose language, Devil allows a description to be thoroughly verified; this verification greatly improves the safety of the communications with the device. The design of Devil is based on key concepts we identified in analyzing the domain of device drivers. Our language has been used to specify a large number of PC devices including Ethernet, video, sound, interrupt, DMA and mouse controllers.", "num_citations": "1\n", "authors": ["518"]}
{"title": "An active stable storage and its integration in a multiprocessor architecture\n", "abstract": " La m\u00e9moire stable est un support d'information \ufb01able qui est utilis\u00e9 pour le stokage de structures de donn\u00e9es permanentes (\ufb01chiers) ou comme celles qui sont n\u00e9cessaires \u00e0 la mise en \u0153uvre des protocoles de terminaison des actions atomiques. Cet article d\u00e9crit une m\u00e9moire stable active adapt\u00e9e a la gestion d'objets structur\u00e9s. Elle poss\u00e8de des m\u00e9canismes de protection internes pour survivre aux comportements anormaux du processeur qui l'acc\u00e8de. Une analyse comparative des temps d'acc\u00e8s de cette m\u00e9moire d\u00e9montre la possibilit\u00e9 de l \u2018utiliser comme le composant de base d'une machine multiprocesseur tol\u00e9rante aux fautes.", "num_citations": "1\n", "authors": ["518"]}
{"title": "Implementing Dynamic Atomic Actions Using Reliable Servers\n", "abstract": " Providing fault tolerance in a distributed system requires ensuring two properties: availability and global system consistency. Availability means that data can be accessed and modified at any time despite crashes of nodes managing the data. In other words, service availability is necessary to offer continuous operation to users.Global system consistency means that the system is able to deal with either the failure of a server, or the failure of a client calling one or more services shared by several clients. In such a situation, the system must be able either to transparently mask the failure, or to roll-back to a previous consistent state.", "num_citations": "1\n", "authors": ["518"]}