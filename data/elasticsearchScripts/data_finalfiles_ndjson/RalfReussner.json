{"title": "Elasticity in cloud computing: What it is, and what it is not\n", "abstract": " Originating from the field of physics and economics, the term elasticity is nowadays heavily used in the context of cloud computing. In this context, elasticity is commonly understood as the ability of a system to automatically provision and deprovision computing resources on demand as workloads change. However, elasticity still lacks a precise definition as well as representative metrics coupled with a benchmarking methodology to enable comparability of systems. Existing definitions of elasticity are largely inconsistent and unspecific, which leads to confusion in the use of the term and its differentiation from related terms such as scalability and efficiency; the proposed measurement methodologies do not provide means to quantify elasticity without mixing it with efficiency or scalability aspects. In this short paper, we propose a precise definition of elasticity and analyze its core properties and requirements explicitly distinguishing from related terms such as scalability and efficiency. Furthermore, we present a set of appropriate elasticity metrics and sketch a new elasticity tailored benchmarking methodology addressing the special requirements on workload design and calibration.", "num_citations": "608\n", "authors": ["637"]}
{"title": "Toward trustworthy software systems\n", "abstract": " Organizations such as Microsoft's Trusted Computing Group and Sun Microsystems' Liberty Alliance are currently leading the debate on \"trustworthy computing.\" However, these and other initiatives primarily focus on security, and trustworthiness depends on many other attributes. To address this problem, the University of Oldenburg's TrustSoft Graduate School aims to provide a holistic view of trustworthiness in software - one that considers system construction, evaluation/analysis, and certification - in an interdisciplinary setting. Component technology is the foundation of our research program. The choice of a component architecture greatly influences the resulting software systems' nonfunctional properties. We are developing new methods for the rigorous design of trustworthy software systems with predictable, provable, and ultimately legally certifiable system properties. We are well aware that it is impossible to\u00a0\u2026", "num_citations": "204\n", "authors": ["637"]}
{"title": "Using genetic search for reverse engineering of parametric behavior models for performance prediction\n", "abstract": " In component-based software engineering, existing components are often reused in new applications. Correspondingly, the response time of an entire component-based application can be predicted from the execution durations of individual component services. These execution durations depend on the runtime behavior of a component which itself is influenced by three factors: the execution platform, the usage profile, and the component wiring. To cover all relevant combinations of these influencing factors, conventional prediction of response times requires repeated deployment and measurements of component services for all such combinations, incurring a substantial effort. This paper presents a novel comprehensive approach for reverse engineering and performance prediction of components. In it, genetic programming is utilized for reconstructing a behavior model from monitoring data, runtime bytecode\u00a0\u2026", "num_citations": "93\n", "authors": ["637"]}
{"title": "Towards self-aware performance and resource management in modern service-oriented systems\n", "abstract": " Modern service-oriented systems have increasingly complex loosely-coupled architectures that often exhibit poor performance and resource efficiency and have high operating costs. This is due to the inability to predict at run-time the effect of dynamic changes in the system environment (e.g., varying service workloads) and adapt the system configuration accordingly. In this paper, we describe a long-term vision and approach for designing systems with built-in self-aware performance and resource management capabilities. We advocate the use of architecture-level performance models extracted dynamically from the evolving system configuration and maintained automatically during operation. The models will be exploited at run-time to adapt the system to changes in the environment ensuring that resources are utilized efficiently and performance requirements are continuously satisfied.", "num_citations": "90\n", "authors": ["637"]}
{"title": "Handbuch der Software-Architektur\n", "abstract": " Handbuch der Software-Architektur Page 1 Ralf Reussner \u2022 Wilhelm Hasselbring (Hrsg.) Handbuch der Software-Architektur m HOCHSCHUU m LIECHTENSTEIN Bibliothek Pin dpunkt.verlag Page 2 IX Inhaltsverzeichnis 1 Einleitung 1 2 Die Rolle der Software-Architekten 9 2.1 Wer wird als Software-Architekt angesehen? 9 2.1.1 Der Plattformspezialist als Software-Architekt 10 2.1.2 Der Entwurfsspezialist als Software-Architekt.......... 11 2.1.3 Der Stratege als Software-Architekt 11 2.2 Probleme der technologiebezogenen Sichtweisen 12 2.2:1 Der Software-Architekt als Wunderheiler ... 13 2.2.2 Der Software-Architekt als kleiner K\u00f6nig 13 2.2.3 Resultierende Konflikte 14 2.2.4 Die generische Flexibilisierungsfalle : 15 2.3 Aufgaben von Architekten -....-...-..\u2022.,.. 16 2.3.1 Entwurf \u2022 17 2.3.2 Planung und Organisation .'...\u2022 18 2.3.3 Bau\u00fcberwachung 19 2.4 Wer sollte Architekten beauftragen? .' 19 2.4.1 Die Bauherrenrolle : .\u2022 \u2026", "num_citations": "88\n", "authors": ["637"]}
{"title": "SKaMPI: A comprehensive benchmark for public benchmarking of MPI\n", "abstract": " The main objective of the MPI communication library is to enable {\\it portable parallel programming} with high performance within the message-passing paradigm. Since the MPI standard has no associated performance model, and makes no performance guarantees, comprehensive, detailed and accurate performance figures for different hardware platforms and MPI implementations are important for the application programmer, both for understanding and possibly improving the behavior of a given program on a given platform, as well as for assuring a degree of predictable behavior when switching to another hardware platform and/or MPI implementation. We term this latter goal {\\it performance portability}, and address the problem of attaining performance portability by benchmarking. We describe the SKaMPI benchmark which covers a large fraction of MPI, and incorporates well-accepted mechanisms for\u00a0\u2026", "num_citations": "86\n", "authors": ["637"]}
{"title": "Parametrisierte Vertr\u00e4ge zur Protokolladaption bei Software-Komponenten\n", "abstract": " Parametrisierte Vertr\u00e4ge fordern vom Schnittstellenmodell lediglich die explizite Modellierung der Angebots-und der Bedarfsschnittstelle, sind aber ansonsten unabh\u00e4ngig von der Information, die in diesen Schnittstellen modelliert wird. ProtokollbehafteteSchnittstellen beschreiben im Vergleich zu herk \u00f6mmlichen Schnittstellen (Signaturlisten) zus\u00e4tzlich eine relevante Klasse von Zusammenh\u00e4ngen zwischen Methoden. In der Praxis sind bei der \u00fcberwiegenden Zahl von Komponenten nicht alle Dienste in jedem Zustand verf\u00fcgbar. Dies f\u00fchrt zu illegalen Aufrufreihenfolgen von Diensten, die durch die Kompatibilit\u00e4tspr\u00fcfung herk\u00f6mmlicher Schnittstellen nicht erkannt werden k\u00f6nnen. Die legalen Aufrufreihenfolgen bilden das Protokoll einer Komponente. Allein Dokumentation von Protokollen in der Schnittstelle ist nicht ausreichend; erw\u00fcnscht ist eine automatische Uberpr\u00fcfung der Protokollkompatibilit\u00e4t zweier\u00a0\u2026", "num_citations": "69\n", "authors": ["637"]}
{"title": "Performance prediction for black-box components using reengineered parametric behaviour models\n", "abstract": " In component-based software engineering, the response time of an entire application is often predicted from the execution durations of individual component services. However, these execution durations are specific for an execution platform (i.e. its resources such as CPU) and for a usage profile. Reusing an existing component on different execution platforms up to now required repeated measurements of the concerned components for each relevant combination of execution platform and usage profile, leading to high effort. This paper presents a novel integrated approach that overcomes these limitations by reconstructing behaviour models with platform-independent resource demands of bytecode components. The reconstructed models are parameterised over input parameter values. Using platform-specific results of bytecode benchmarking, our approach is able to translate the platform-independent\u00a0\u2026", "num_citations": "61\n", "authors": ["637"]}
{"title": "Parametric performance completions for model-driven performance prediction\n", "abstract": " Performance prediction methods can help software architects to identify potential performance problems, such as bottlenecks, in their software systems during the design phase. In such early stages of the software life-cycle, only a little information is available about the system\u2019s implementation and execution environment. However, these details are crucial for accurate performance predictions. Performance completions close the gap between available high-level models and required low-level details. Using model-driven technologies, transformations can include details of the implementation and execution environment into abstract performance models. However, existing approaches do not consider the relation of actual implementations and performance models used for prediction. Furthermore, they neglect the broad variety of possible implementations and middleware platforms, possible configurations, and\u00a0\u2026", "num_citations": "59\n", "authors": ["637"]}
{"title": "The dublo architecture pattern for smooth migration of business information systems: An experience report\n", "abstract": " While the importance of multi-tier architectures for enterprise information systems is widely accepted and their benefits are well published, the systematic migration from monolithic legacy systems toward multi-tier architectures is known to a much lesser extent. In this paper we present a pattern on how to re-use elements of legacy systems within multi-tier architectures, which also allows for a smooth migration path. We report on experience we made with migrating existing municipal information systems towards a multitier architecture. The experience is generalized by describing the underlying pattern such that it can be re-used for similar architectural migration tasks. The emerged Dublo pattern is based on the partial duplication of business logic among legacy system and newly deployed application server. While this somehow contradicts the separation-of-concerns principle, it offers a high degree of flexibility in the\u00a0\u2026", "num_citations": "56\n", "authors": ["637"]}
{"title": "Automatic component protocol adaptation with the CoConut/J tool suite\n", "abstract": " While industrial middleware platforms such as CORBA, EJB, or .NET facilitate the development of distributed applications by providing certain infra-structural services required in many distributed systems (such as name services, remote method calls, parameter marshalling, etc.), these industrial platforms fail to support the development of distributed systems with independent components. In particular, their component models do not provide sufficient information for component interoperability checks or automated component adaptation. Especially when considering the personal and institutional separation between component developer and component vendor as one of the prerequisites of an independent component market, finding automatically as many component interoperability errors as possible is crucial. Hence, it is of practical concern that component interfaces not only model the correct way of calling the\u00a0\u2026", "num_citations": "56\n", "authors": ["637"]}
{"title": "Predictive performance modeling of virtualized storage systems using optimized statistical regression techniques\n", "abstract": " Modern virtualized environments are key for reducing the operating costs of data centers. By enabling the sharing of physical resources, virtualization promises increased resource efficiency with decreased administration costs. With the increasing popularity of I/O-intensive applications, however, the virtualized storage used in such environments can quickly become a bottleneck and lead to performance and scalability issues. Performance modeling and evaluation techniques applied prior to system deployment help to avoid such issues. In current practice, however, virtualized storage and its performance-influencing factors are often neglected or treated as a black-box. In this paper, we present a measurement-based performance prediction approach for virtualized storage systems based on optimized statistical regression techniques. We first propose a general heuristic search algorithm to optimize the parameters\u00a0\u2026", "num_citations": "55\n", "authors": ["637"]}
{"title": "Performance modeling in industry: a case study on storage virtualization\n", "abstract": " In software engineering, performance and the integration of performance analysis methodologies gain increasing importance, especially for complex systems. Well-developed methods and tools can predict non-functional performance properties like response time or resource utilization in early design stages, thus promising time and cost savings. However, as performance modeling and performance prediction is still a young research area, the methods are not yet well-established and in wide-spread industrial use. This work is a case study of the applicability of the Palladio Component Model as a performance prediction method in an industrial environment. We model and analyze different design alternatives for storage virtualization on an IBM* system. The model calibration, validation and evaluation is based on data measured on a System z9* as a proof of concept. The results show that performance predictions\u00a0\u2026", "num_citations": "54\n", "authors": ["637"]}
{"title": "Enhanced component interfaces to support dynamic adaption and extension\n", "abstract": " Current component systems offer the possibility to integrate different enterprise systems, e.g., by wrapping legacy components or integrating several object protocols such as RMI, Corba-IIOP, etc. A disadvantage of todays component systems is that the interface descriptions of their component model do not give alone sufficient information to deploy a component correctly and reliably. Therefore the definition of new interface models, which are enhanced by (semantic) applicability information play an important role in enterprise application integration. We describe a new model of software component interfaces, using an extension of finite state machines to describe the protocol to use a component's offered services, and the sequences of calls to external services the component requires to fulfil its offered services. Our model concentrates on protocol issues of interoperability. We present a description of our new\u00a0\u2026", "num_citations": "54\n", "authors": ["637"]}
{"title": "Architecture-based assessment and planning of change requests\n", "abstract": " Software architecture reflects important decisions on structure, used technology and resources. Architecture decisions influence to a large extent requirements on software quality. During software evolution change requests have to be implemented in a way that the software maintains its quality, as various potential implementations of a specific change request influence the quality properties differently. Software development processes involve various organisational and technical roles. Thus, for sound decision making it is important to understand the consequences of the decisions on the various software engineering artefacts (eg architecture, code, test cases, build, or deployments) when analysing the impact of a change request. However, existing approaches do not use sufficient architecture descriptions or are limited to software development without taking management tasks into account. In this paper, we\u00a0\u2026", "num_citations": "53\n", "authors": ["637"]}
{"title": "On a software architecture description supporting component deployment and system runtime reconfiguration\n", "abstract": " Architecture description languages (ADLs) can be used for describing architectures of component-based software systems. Typical ADLs provide explicit support for specifying components, connectors and configurations as well as for building hierarchical system configurations. All of them allow to specify structural dependencies among components, thus describing static configurations. This may be sufficient for an initial system composition, but does not provide enough information for post-deployment and runtime reconfiguration. Only a few ADLs provide some support for dynamics, usually without a clear differentiation between a possible behaviour of component descriptions and a runtime behaviour of component instances. Even XML-based ADLs such as xADL 2.0, which clearly distinguishes between the design-time and run-time, only defines structural instance schemata. In our approach, we observe the \u201duse\u201d dependencies among instances of components (called \u201dlive components\u201d) of an already deployed and running system. The life components are constrained by specified structural dependencies (defined in \u201dcomponent descriptions\u201d). Live components are hosted in containers. The \u201dService- Connector-Container\u201d view of our model provides a way to describe the runtime behaviour of a system. Thus, it supports dynamic reconfiguration of component-based software systems. We use \u201dservice effect automata\u201d for runtime behaviour specification and intend to extend them using the timing and liveness concepts of \u201dlive sequence charts\u201d.", "num_citations": "52\n", "authors": ["637"]}
{"title": "A pattern-based performance completion for message-oriented middleware\n", "abstract": " Details about the underlying Message-oriented Middleware (MOM) are essential for accurate performance predictions of software systems using message-based communication. The MOM's configuration and usage strongly influence its throughput, resource utilisation and timing behaviour. Prediction models need to reflect these effects and allow software architects to evaluate the performance influence of MOM configured for their needs. Performance completions [31, 32] provide the general concept to include low-level details of execution environments in abstract performance models. In this paper, we extend the Palladio Component Model (PCM)[4] by a performance completion for Message-oriented Middleware. With our extension to the model, software architects can specify and configure message-based communication using a language based on messaging patterns. For performance evaluation, a model-to\u00a0\u2026", "num_citations": "48\n", "authors": ["637"]}
{"title": "Dependability Metrics: GI-Dagstuhl Research Seminar, Dagstuhl Castle, Germany, October 5-November 1, 2005, Advanced Lectures\n", "abstract": " This tutorial book gives an overview of the current state of the art in measuring the different aspects of dependability of systems: reliability, security and performance.", "num_citations": "45\n", "authors": ["637"]}
{"title": "Classifying software component interoperability errors to support component adaption\n", "abstract": " This paper discusses various classifications of component interoperability errors. These classifications aim at supporting the automation of component adaptation. The use of software components will only demonstrate beneficial, if the costs for component deployment (i.e., acquisition and composition) are considerably lower than those for custom component development. One of the main reasons for the moderate progress in component-based software engineering are the high costs for component deployment. These costs are mainly caused by adapting components to bridge interoperability errors between unfitting components. One way to lower the costs of component deployment is to support component adaptation by tools, i.e., for interoperability checks of (semi-)automated adaptor generation. This automation of component adaptation requires a deep understanding of component interoperability errors\u00a0\u2026", "num_citations": "45\n", "authors": ["637"]}
{"title": "On benchmarking collective MPI operations\n", "abstract": " This article concentrates on recent work on benchmarking collective operations with SKaMPI. The goal of the SKaMPI project is the creation of a database containing performance measurements of parallel computers in terms of MPI operations. These data support software developers in creating portable and fast programs. Existing algorithms for measuring the timing of collective operations are discussed and a new algorithm is presented, taking into account the differences of local clocks. Results of measurements on a Cray T3E/900 and an IBM RS 6000 SP are presented.", "num_citations": "44\n", "authors": ["637"]}
{"title": "Defining and quantifying elasticity of resources in cloud computing and scalable platforms\n", "abstract": " CiteSeerX \u2014 Defining and Quantifying Elasticity of Resources in Cloud Computing and Scalable Platforms Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA Defining and Quantifying Elasticity of Resources in Cloud Computing and Scalable Platforms Cached Download as a PDF Download Links [digbib.ubka.uni-karlsruhe.de] Save to List Add to Collection Correct Errors Monitor Changes by Michael Kuperberg , Nikolas Herbst , Joakim Von Kistowski , Ralf Reussner Citations: 1 - 0 self Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract This Report has been published on the Internet under the following Keyphrases cloud computing scalable platform Powered by: Apache Solr \u2026", "num_citations": "40\n", "authors": ["637"]}
{"title": "The use of parameterised contracts for architecting systems with software components\n", "abstract": " We discuss relations between component design and software architecture and how CASE tools and generators can support software architecural design with components. More specific, we present the concept of parameterised contracts and show how they influence the granularity of software components. Parameterised contracts link the providesand the requires-interface of a component. This allows to perform automatically a certain class of component adaptations without changing the code. These adaptations also do not have to be foreseen and programmed by the component developer in advance. We show how parameterised contracts can be used to enhance the reusability of software components in software architectures.", "num_citations": "39\n", "authors": ["637"]}
{"title": "Design for future: managed software evolution\n", "abstract": " Innovative software engineering methodologies, concepts and tools which focus on supporting the ongoing evolution of complex software, in particular regarding its continuous adaptation to changing functional and quality requirements as well as platforms over a long period are required. Supporting such a co-evolution of software systems along with their environment represents a very challenging undertaking, as it requires a combination or even integration of approaches and insights from different software engineering disciplines. To meet these challenges, the Priority Programme 1593 Design for Future\u2014Managed Software Evolution has been established, funded by the German Research Foundation, to develop fundamental methodologies and a focused approach for long-living software systems, maintaining high quality and supporting evolution during the whole life cycle. The goal of the priority\u00a0\u2026", "num_citations": "36\n", "authors": ["637"]}
{"title": "Software als Institution und ihre Gestaltbarkeit\n", "abstract": " Software regelt immer mehr zwischenmenschliche Interaktionen. \u00dcblicherweise werden die Funktionsmechanismen, Wirkungen und Gestaltungsoptionen von Regeln in der Institutionenforschung behandelt. In diesem Artikel soll beleuchtet werden, inwieweit sich Ans\u00e4tze der Institutionenforschung auf Software anwenden lassen und was sich aus dieser Forschungsperspektive zu den Regelungswirkungen und Gestaltungsoptionen von Software ableiten l\u00e4sst.", "num_citations": "36\n", "authors": ["637"]}
{"title": "Validation of predictions with measurements\n", "abstract": " This chapter discusses ways to validate metrics and raises awareness for possible caveats if metrics are used in a social environment.", "num_citations": "35\n", "authors": ["637"]}
{"title": "A Platform for Empirical Research on Information System Evolution.\n", "abstract": " Software-intensive systems are subject to continuous change due to modification of the systems themselves and their environment. Methods for supporting evolution are a competitive edge in software engineering as software is operated over decades. Empirical research is useful to validate the effectiveness of these methods. However, empirical studies on software evolution are rarely comprehensive and hardly replicable. Collaboration in empirical studies may prevent these shortcomings. We analyzed the support for such collaboration and examined existing studies in a literature review. Based on our findings, we designed CoCoMEP\u2013a platform for supporting collaboration in empirical research on software evolution by shared knowledge. We report lessons learned from the application of the platform in a large research programme.", "num_citations": "32\n", "authors": ["637"]}
{"title": "Trusted cloud computing\n", "abstract": " The research projects of the Trusted Cloud Initiative of the Federal Ministry for Economic Affairs and Energy are developing novel cloud services and making them, as well as already existing services, usable for trustable IT applications. Cloud services have a number of advantages over in-house services, starting with cost efficiency and flexible payment models, where customers only pay for resources and services they actually use. Combined with the elasticity of service provision, that is the dynamic adaptation to seasonal or task-depending resource demands of a service\u2019s operation, it makes Cloud Computing an attractive model for service provisioning on many layers (as application platform, programming model, or for domain-specific services).However, the user gives up some control over the operation of the IT services to the service provider. This particularly concerns non-functional aspects of service\u00a0\u2026", "num_citations": "32\n", "authors": ["637"]}
{"title": "ByCounter: Portable runtime counting of bytecode instructions and method invocations\n", "abstract": " For bytecode-based applications, runtime instruction counts can be used as a platform-independent application execution metric, and also can serve as the basis for bytecode-based performance prediction. However, different instruction types have different execution durations, so they must be counted separately, and method invocations should be identified and counted because of their substantial contribution to the total application performance. For Java bytecode, most JVMs and profilers do not provide such functionality at all, and existing bytecode analysis frameworks require expensive JVM instrumentation for instruction-level counting. In this paper, we present ByCounter, a lightweight approach for exact runtime counting of executed bytecode instructions and method invocations. ByCounter significantly reduces total counting costs by instrumenting only the application bytecode and not the JVM, and it can be used without modifications on any JVM. We evaluate the presented approach by successfully applying it to multiple Java applications on different JVMs, and discuss the runtime costs of applying ByCounter to these cases. Key words: Java, bytecode, counting, portable, fine-grained", "num_citations": "30\n", "authors": ["637"]}
{"title": "Architecture-based change impact analysis in information systems and business processes\n", "abstract": " Business processes as well as software systems face various changes during their lifetime. As they mutually influence each other, business processes and software systems have to be modified in co-evolution. Thus, to adequately predict the change impact, it is important to consider the complex mutual dependencies of both domains. However, existing approaches are limited to analyzing the change propagation in software systems or business processes in isolation. In this paper, we present a tool-supported approach to estimate the change propagation caused by a change request in business processes or software systems based on the software architecture and the process design. We focus on the mutual dependencies regarding the change propagation between both domains. In the evaluation, we apply our approach to a community case study to demonstrate the quality of results in terms of precision, recall\u00a0\u2026", "num_citations": "26\n", "authors": ["637"]}
{"title": "Parametric performance contracts for software components and their compositionality\n", "abstract": " The performance of a software component heavily depends on the environment of the component. As a software component only justifies its investment when deployed in several environments, one can not specify the performance of a component as a constant (eg, as a single value or distribution of values in its interface). Hence, classical component contracts allowing to state the component\u2019s performance as a post-condition, if the environment realises a specific performance stated in the precondition, do not help. This fixed pair of pre-and postcondition do not model that a component can have very different performance figures depending on its context. Instead of that, parametric contracts are needed for specifying the environmental dependency of the component\u2019s provided performance. In this paper we discuss the specification of such dependencies for the performance metric response time. We model the statistical distribution of response time in dependency of the distribution of response times of environmental services.", "num_citations": "25\n", "authors": ["637"]}
{"title": "Data-driven software architecture for analyzing confidentiality\n", "abstract": " Preservation of confidentiality has become a crucial quality property of software systems that software vendors have to consider in each development phase. Especially, neglecting confidentiality constraints in the software architecture leads to severe issues in later phases that often are hard to correct. In contrast to the implementation phase, there is no support for systematically considering confidentiality in architectural design phases by means of data processing descriptions. To fill this gap, we introduce data flows in an architectural description language to enable simple definition of confidentiality constraints. Afterwards, we transform the software architecture specification to a logic program to find violated confidentiality constraints. In a case study-based evaluation, we apply the analysis to sixteen scenarios to show the accuracy of the approach.", "num_citations": "24\n", "authors": ["637"]}
{"title": "Investigating performance metrics for scaling microservices in cloudiot-environments\n", "abstract": " A CloudIoT solution typically connects thousands of IoT things with cloud applications in order to store or process sensor data. In this environment, the cloud applications often consist of microservices which are connected to each other via message queues and must reliably handle a large number of messages produced by the IoT things. The state of a message queue in such a system can be a challenge if the rate of incoming messages continuously exceeds the rate of outgoing messages. This can lead to performance and reliability degradations due to overloaded queues and result in the unavailability of the cloud application. In this paper we present a case study to investigate which performance metrics to be used by a threshold-based auto-scaler for scaling consuming microservices of a message queue in order to prevent overloaded queues and to avoid SLA violations. We evaluate the suitability of each\u00a0\u2026", "num_citations": "24\n", "authors": ["637"]}
{"title": "The CoCoME platform for collaborative empirical research on information system evolution\n", "abstract": " In industrial practice, many information systems are operated over decades. During operation they face various modi cations, eg due to emerging requirements, bug xes, and environmental changes, such as legal constraint or technology stack updates. In consequence, the systems change continually which is referred to as software evolution [13]. Supporting software evolution is a competitive advantage in software engineering. A variety of methods aim at supporting di erent aspects of software evolution. However, it is hard to assess their e ectiveness and to compare them due to divergent characteristics. Empirical research in terms of case studies and controlled experiments is useful to validate these methods. However, empirical studies on software evolution are rarely comprehensive. They only cover few of the many aspects needed to study evolution such as (i) long time-frames of observation are required to analyze changes,(ii) large amount of artifacts and (iii) various types of artifacts are a ected by evolution,(iv) artifacts repeatedly change,(v) changes partly build upon each other,(vi) various stakeholders are involved,(vii) access to relevant project data,(viii) relevant project data must be documented over long time spans,(ix) relevant context knowledge must be documented beyond the code base and issue trackers. To study evolution comprehensively we believe it is important to collaborate by joint research in order to increase coverage of the aspects. Joint research supports sharing of knowledge and resources [16]. In particular, this allows replicating studies which in general is important to con rm and to strengthen results of empirical\u00a0\u2026", "num_citations": "24\n", "authors": ["637"]}
{"title": "Challenges and opportunities of cloud computing\n", "abstract": " In recent years, Cloud Computing has become an emerging technology that gains wide influence on IT systems. Cloud Computing is a distributed computing model for enabling service-oriented, on-demand network access to rapidly scalable resources [9]. Such resources include infrastructure as a service (IaaS), development and runtime platforms as a service (PaaS), and software and business applications as a service (SaaS). Clients do not own the resources, yet applications and data are guaranteed to be available and ubiquitously accessible by means of Web services and Web APIs \u201cin the Cloud\u201d.", "num_citations": "24\n", "authors": ["637"]}
{"title": "Modelling layered component execution environments for performance prediction\n", "abstract": " Software architects often use model-based techniques to analyse performance (e.g. response times), reliability and other extra-functional properties of software systems. These techniques operate on models of software architecture and execution environment, and are applied at design time for early evaluation of design alternatives, especially to avoid implementing systems with insufficient quality. Virtualisation (such as operating system hypervisors or virtual machines) and multiple layers in execution environments (e.g. RAID disk array controllers on top of hard disks) are becoming increasingly popular in reality and need to be reflected in the models of execution environments. However, current component meta-models do not support virtualisation and cannot model individual layers of execution environments. This means that the entire monolithic model must be recreated when different implementations\u00a0\u2026", "num_citations": "24\n", "authors": ["637"]}
{"title": "A benchmark for MPI derived datatypes\n", "abstract": " We present an extension of the SKaMPI benchmark for MPI implementations to cover the derived datatype mechanism of MPI. All MPI constructors for derived datatypes are covered by the benchmark, and varied along different dimensions. This is controlled bya set of predened patterns which can be instantiated bypa rameters given bythe user in a configurationsle. We classifythe patterns intofixed types, dynamic types, nested types, and special types. We show results from the SKaMPI ping-pong measurement with the fixed and special types on three platforms: CrayT3E/900, IBM RS 6000SP, NEC SX-5. The machines show quite some difference in handling datatypes, with typically a significant penaltyfor nested types for the Cray (up to a factor of 16) and the IBM (up to a factor of 8), whereas the NEC treats these types very uniformly (overhead of between 2 and 4). Such results illustrate the need for a\u00a0\u2026", "num_citations": "24\n", "authors": ["637"]}
{"title": "I/O performance modeling of virtualized storage systems\n", "abstract": " Server virtualization is a key technology to share physical resources efficiently and flexibly. With the increasing popularity of I/O-intensive applications, however, the virtualized storage used in shared environments can easily become a bottleneck and cause performance and scalability issues. Performance modeling and evaluation techniques applied prior to system deployment help to avoid such issues. In current practice, however, virtualized storage and its effects on the overall system performance are often neglected or treated as a black-box. In this paper, we present a systematic I/O performance modeling approach for virtualized storage systems based on queueing theory. We first propose a general performance model building methodology. Then, we demonstrate our methodology creating I/O queueing models of a real-world representative environment based on IBM System z and IBM DS8700 server\u00a0\u2026", "num_citations": "23\n", "authors": ["637"]}
{"title": "A controlled experiment to evaluate how styles affect the understandability of requirements specifications\n", "abstract": " This paper presents a controlled experiment in which two different requirements specification styles (white-box and black-box) were compared concerning the understandability of two requirements specifications from the viewpoint of a customer. The results of the experiment confirm the common belief that black-box requirements specifications (e.g., documented with SCR) are easier to understand from a customer point of view than white-box specifications (e.g., documented with UML). Questions about particular functions and behavior of the specified system were answered faster and more correctly by the participants. This result suggests that using a black-box specification style when communicating with customers is beneficial.", "num_citations": "23\n", "authors": ["637"]}
{"title": "Workload-aware system monitoring using performance predictions applied to a large-scale e-mail system\n", "abstract": " Offering services in the internet requires a dependable operation of the underlying software systems with guaranteed quality of service. The workload of such systems typically significantly varies throughout a day and thus leads to changing resource utilisations. Existing system monitoring tools often use fixed threshold values to determine if a system is in an unexpected state. Especially in low load situations, deviations from the system's expected behaviour are detected too late if fixed value thresholds (leveled for peak loads) are used. In this paper, we present our approach of a workload-aware performance monitoring process based on performance prediction techniques. This approach allows early detections of performance problems before they become critical. We applied our approach to the e-mail system operated by Germany's largest e-mail provider, the 1&1 Internet AG. This case study demonstrates the\u00a0\u2026", "num_citations": "22\n", "authors": ["637"]}
{"title": "Using internal domain-specific languages to inherit tool support and modularity for model transformations\n", "abstract": " Model-driven engineering (MDE) has proved to be a useful approach to cope with today\u2019s ever-growing complexity in the development of software systems; nevertheless, it is not widely applied in industry. As suggested by multiple studies, tool support is a major factor for this lack of adoption. In particular, the development of model transformations lacks good tool support. Additionally, modularization techniques are inevitable for the development of larger model transformations to keep them maintainable. Existing tools for MDE, in particular model transformation approaches, are often developed by small teams and cannot keep up with advanced tool support for mainstream general-purpose programming languages, such as IntelliJ or Visual Studio. Internal DSLs are a promising solution to these problems. In this paper, we investigate the impact of design decisions of an internal DSL to the reuse of tool\u00a0\u2026", "num_citations": "21\n", "authors": ["637"]}
{"title": "On the appropriate rationale for using design patterns and pattern documentation\n", "abstract": " Software design patterns are proven solutions for recurring design problems. Decisions on the use of a pattern in a software design form a specific but important class of design decisions. However, despite their importance, these design decisions are often mistaken and rarely documented. In our survey, about 90% of the participants confirmed to have experienced such problems. Therefore, we propose an approach that supports the appropriate use of design patterns and documentation of such decisions. The main idea is to create a pattern catalogue, where a pattern (as part of its catalogue entry) is annotated with general questions on the appropriateness of the use of the pattern. The envisioned benefits of this approach are a more appropriate use of design patterns, and documented design decisions on the use of patterns with positive effects on evolution. In this paper, we present the enriched pattern catalogue\u00a0\u2026", "num_citations": "21\n", "authors": ["637"]}
{"title": "Experimental evaluation of the performance-influencing factors of virtualized storage systems\n", "abstract": " Virtualized cloud environments introduce an additional abstraction layer on top of physical resources enabling their collective use by multiple systems to increase resource efficiency. In I/O-intensive applications, however, the virtualized storage of such shared environments can quickly become a bottleneck and lead to performance and scalability issues. In software performance engineering, application performance is analyzed to assess the non-functional properties taking into account the many performance-influencing factors. In current practice, however, virtualized storage is either modeled as a black-box or tackled with full-blown and fine-granular simulations. This paper presents a systematic performance analysis approach of I/O-intensive applications in virtualized environments. First, we systematically identify storageperformance- influencing factors in a representative storage environment. Second\u00a0\u2026", "num_citations": "21\n", "authors": ["637"]}
{"title": "A case study evaluation of maintainability and performance of persistency techniques\n", "abstract": " Efforts for software evolution supersede any other part of the software life cycle. Technological decisions have a major impact on the maintainability, but are not well reflected by existing code or architecture based metrics. The way the persistency of object structures with relational databases is solved affects the maintainability of the overall system. Besides maintainability other quality attributes of the software are of interest, in particular performance metrics. However, a systematic evaluation of the benefits and drawback of different persistency frameworks is lacking. In this paper we systematically evaluate the maintainability and performance of different technological approaches for this mapping. The paper presents a testbed and an evaluation process with specifically designed metrics to evaluate persistency techniques regarding their maintainability and performance. In the second part we present and discuss the\u00a0\u2026", "num_citations": "21\n", "authors": ["637"]}
{"title": "Constructing performance model of JMS middleware platform\n", "abstract": " Middleware performance models are useful building blocks in the performance models of distributed software applications. We focus on performance models of messaging middleware implementing the Java Message Service standard, showing how certain system design properties--including pipelined processing and message coalescing--interact to create performance behavior that the existing models do not capture accurately. We construct a performance model of the ActiveMQ messaging middleware that addresses the outlined issues and discuss how the approach extends to other middleware implementations.", "num_citations": "20\n", "authors": ["637"]}
{"title": "Kamp: Karlsruhe architectural maintainability prediction\n", "abstract": " In their lifetime software systems usually need to be adapted in order to fit in a changing environment or to cover new required functionality. The effort necessary for implementing changes is related to the maintainability of the software system. Therefore, maintainability is an important quality aspect of software systems. Today Software Architecture plays an important role in achieving software quality goals. Therefore, it is useful to evaluate software architectures regarding their impact on the quality of the program. However, unlike other quality attributes, such as performance or reliability, there is relatively less work on the impact of the software architecture on maintainability in a quantitative manner. In particular, the cost of software evolution not only stems from software-development activities, such as reimplementation, but also from software management activities, such as re-deployment, upgrade installation, etc. Most metrics for software maintainability base on code of object-oriented designs, but not on architectures, and do not consider costs from software management activities. Likewise, existing current architectural maintainability evaluation techniques manually yield just qualitative (and often subjective) results and also do concentrate on software (re-) development costs. In this paper, we present KAMP, the Karlsruhe Architectural Maintainability Prediction Method, a quantitative approach to evaluate the maintainability of software architectures. Our approach estimates the costs of change requests for a given architecture and takes into account re-implementation costs as well as re-deployment and upgrade activities. We combine several\u00a0\u2026", "num_citations": "20\n", "authors": ["637"]}
{"title": "Managed Software Evolution\n", "abstract": " This open access book presents the outcomes of the \u201cDesign for Future \u2013 Managed Software Evolution\u201d priority program 1593, which was launched by the German Research Foundation (\u201cDeutsche Forschungsgemeinschaft (DFG)\u201d) to develop new approaches to software engineering with a specific focus on long-lived software systems. The different lifecycles of software and hardware platforms lead to interoperability problems in such systems. Instead of separating the development, adaptation and evolution of software and its platforms, as well as aspects like operation, monitoring and maintenance, they should all be integrated into one overarching process. Accordingly, the book is split into three major parts, the first of which includes an introduction to the nature of software evolution, followed by an overview of the specific challenges and a general introduction to the case studies used in the project. The second part of the book consists of the main chapters on knowledge carrying software, and cover tacit knowledge in software evolution, continuous design decision support, model-based round-trip engineering for software product lines, performance analysis strategies, maintaining security in software evolution, learning from evolution for evolution, and formal verification of evolutionary changes. In turn, the last part of the book presents key findings and spin-offs. The individual chapters there describe various case studies, along with their benefits, deliverables and the respective lessons learned. An overview of future research topics rounds out the coverage. The book was mainly written for scientific researchers and advanced professionals with\u00a0\u2026", "num_citations": "19\n", "authors": ["637"]}
{"title": "Modelling parametric contracts and the state space of composite components by graph grammars\n", "abstract": " Modeling the dependencies between provided and required services within a software component is necessary for several reasons, such as automated component adaptation and architectural dependency analysis. Parametric contracts for software components specify such dependencies and were successfully used for automated protocol adaptation and quality of service prediction. In this paper, a novel model for parametric contracts based on graph grammars is presented and a first definition of the compositionality of parametric contracts is given. Compared to the previously used finite state machine based formalism, the graph grammar formalism allows a more elegant formulation of parametric contract applications and considerably simpler implementations.", "num_citations": "18\n", "authors": ["637"]}
{"title": "Palladio\u2013Prediction of performance properties\n", "abstract": " Palladio is a component modelling approach with a focus on performance (i.e. response time, throughput, resource utilisation) analysis to enable early design-time evaluation of software architectures. It targets modelling business information systems. The Palladio approach includes                                      a meta-model called \u201cPalladio Component Model\u201d for structural views, component behaviour specifications, resource environment, component allocation and the modelling of system usage and                                                     multiple analysis techniques ranging from process algebra analysis to discrete event simulation.                                                     Additionally, the Palladio approach is aligned with a development process model tailored for component-based software systems.                                                          Early design-time predictions avoid costly redesigns and reimplementation. Palladio enables software\u00a0\u2026", "num_citations": "17\n", "authors": ["637"]}
{"title": "Architecture-based change impact analysis in cross-disciplinary automated production systems\n", "abstract": " Maintaining an automated production system is a challenging task as it comprises artifacts from multiple disciplines \u2013 namely mechanical, electrical, and software engineering. As the artifacts mutually affect each other, even small modifications may cause extensive side effects. Consequently, estimating the maintenance effort for modifications in an automated production system precisely is time consuming and often nearly as complicated as implementing the modifications. In this paper, we present the KAMP4aPS approach for architecture-based change impact analysis in production automation. We propose metamodels to specify the various artifacts of the system and modifications to them, as well as algorithms and rules for change propagation analysis based on the models. We evaluate KAMP4aPS for three different change scenarios based on the established xPPU community case study on production\u00a0\u2026", "num_citations": "16\n", "authors": ["637"]}
{"title": "Automated modeling of I/O performance and interference effects in virtualized storage systems\n", "abstract": " Modern IT systems frequently employ virtualization technology to maximize resource efficiency. By sharing physical resources, however, the virtualized storage used in such environments can quickly become a bottleneck. Performance modeling and evaluation techniques applied prior to system deployment help to avoid performance issues. In current practice, however, modeling I/O performance is usually avoided due to the increasing complexity of modern virtualized storage systems. In this paper, we present an automated modeling approach based on statistical regression techniques to analyze I/O performance and interference effects in the context of virtualized storage systems. We demonstrate our approach in three case studies creating performance models with two I/O benchmarks. The case studies are conducted in a real-world environment based on IBM System z and IBM DS8700 server hardware. Using\u00a0\u2026", "num_citations": "16\n", "authors": ["637"]}
{"title": "Ginpex: deriving performance-relevant infrastructure properties through goal-oriented experiments\n", "abstract": " In software performance engineering, the infrastructure on which an application is running plays a crucial role when predicting the performance of the application. Thus, to yield accurate prediction results, performance-relevant properties and behaviour of the infrastructure have to be integrated into performance models. However, capturing these properties is a cumbersome and error-prone task, as it requires carefully engineered measurements and experiments. Existing approaches for creating infrastructure performance models require manual coding of these experiments, or ignore the detailed properties in the models. The contribution of this paper is the Ginpex approach, which introduces goal-oriented and model-based specification and generation of executable performance experiments for detecting and quantifying performance relevant infrastructure properties. Ginpex provides a metamodel for experiment\u00a0\u2026", "num_citations": "16\n", "authors": ["637"]}
{"title": "Single Underlying Models for Projectional, Multi-View Environments.\n", "abstract": " Multi-view environments provide different views of software systems optimized for different stakeholders. One way of ensuring consistency of overlapping and inter-dependent information contained in such views is to project them \u201con demand\u201d from a Single Underlying Model (SUM). However, there are various ways of building and evolving such SUMs. This paper presents criteria to distinguish them, describes three archetypical approaches for building SUMs, and analyzes their advantages and disadvantages. From these criteria, guidelines for choosing which approach to use in specific application areas are derived.", "num_citations": "15\n", "authors": ["637"]}
{"title": "Maintenance effort estimation with KAMP4aPS for cross-disciplinary automated PLC-based Production Systems-a collaborative approach\n", "abstract": " Automated production systems (aPSs) are often in operation for several decades. Due to a multiplicity of reasons, these assets have to be maintained and modified over the time multiple times and with respect to multiple engineering domains. An increased economic pressure demands to perform these tasks in an optimized way. Therefore, it is necessary to estimate change effects with respect to multidisciplinary interdependences, required surrounding non-functional tasks and the effort and costs included in each step. This paper outlines available cost estimation methods for PLC-based automation and Information Systems (ISs). We introduce Karlsruhe Architectural Maintainability Prediction for aPS (KAMP4aPS), an approach to estimate the necessary maintenance tasks to be performed and their related costs for the domain of aPSs by extending KAMP, which is limited to change propagation analysis on ISs\u00a0\u2026", "num_citations": "15\n", "authors": ["637"]}
{"title": "A framework for coupled simulations of robots and spiking neuronal networks\n", "abstract": " Bio-inspired robots still rely on classic robot control although advances in neurophysiology allow adaptation to control as well. However, the connection of a robot to spiking neuronal networks needs adjustments for each purpose and requires frequent adaptation during an iterative development. Existing approaches cannot bridge the gap between robotics and neuroscience or do not account for frequent adaptations. The contribution of this paper is an architecture and domain-specific language (DSL) for connecting robots to spiking neuronal networks for iterative testing in simulations, allowing neuroscientists to abstract from implementation details. The framework is implemented in a web-based platform. We validate the applicability of our approach with a case study based on image processing for controlling a four-wheeled robot in an experiment setting inspired by Braitenberg vehicles.", "num_citations": "15\n", "authors": ["637"]}
{"title": "Modeling of i/o performance interference in virtualized environments with queueing petri nets\n", "abstract": " Virtualization technology allows to share the physical resources used in IT infrastructures for efficient and flexible system operation. Sharing of physical resources, however, comes usually at the cost of performance and poses significant challenges to respect the Quality-of-Service of consolidated data-intensive applications due to the mutual performance interference among the applications. The non-trivial impact of workload consolidation on the I/O performance can be anticipated using explicit performance analysis techniques. In current practice, however, explicit modeling of I/O performance interference effects in virtualized environments is usually avoided due to their complexity. In this paper, we present an explicit performance modeling approach of I/O performance interference in virtualized environments with queueing Petri nets (QPNs). More specifically, we first highlight major challenges when modeling I/O\u00a0\u2026", "num_citations": "15\n", "authors": ["637"]}
{"title": "Towards architecture-centric evolution of long-living systems (the ADVERT approach)\n", "abstract": " Although an intensive research attention has been paid to software evolution, there is no established approach which supports a software development and evolution round-trip between requirements, design decisions, architectural elements, and code. The ADVERT approach shall provide support for software evolution on an architectural level. ADVERT is based on two core ideas:(1) Maintaining trace links between requirements, design decisions, and architecture elements, and (2) explicitly integrating software architecture information into the code. The expected benefits of the approach are:(1) Eased understanding of the relationship between requirements and design, and (2) assured compliance between architectural design and implementation. In this position paper we explain our envisioned approach and demonstrate it on a CoCoME-based example, which is a benchmark for component-based modelling\u00a0\u2026", "num_citations": "15\n", "authors": ["637"]}
{"title": "A Modular Reference Structure for Component-based Architecture Description Languages.\n", "abstract": " Metamodels are used to define languages, code generation and they serve as data structures for metamodel-centric software systems. In software engineering, these metamodels are crafted, evolved and extended, eg, by further quality dimensions or structural features. However, an ad-hoc modeling approach does not properly support metamodel reuse by extension or composition. Nor does it enforce a proper modularization which helps with tackling complexity. We present an approach to design and extend metamodels for component-based architecture description languages in a modular way. The information which is to be metamodeled is divided into paradigm, domain, quality and analysis content. We constrain the usage of dependencies and give instructions how to modularize in accordance to concerns. Related approaches try to modularize and compose transformations, generators, and tools in general. However, in the field of metamodels, little support is given. Our approach is applied to several concerns of the Palladio Component Model and an extension thereof.", "num_citations": "14\n", "authors": ["637"]}
{"title": "Achieving Performance Portability with SKaMPI for High-Performance MPI Programs\n", "abstract": " Current development processes for parallel software often fail to deliver portable software. This is because these processes usually require a tedious tuning phase to deliver software of good performance. This tuning phase often is costly and results in machine specific tuned (i.e., less portable) software. Designing software for performance and portability in early stages of software design requires performance data for all targeted parallel hardware platforms. In this paper we present a publicly available database, which contains data necessary for software developers to design and implement portable and high performing MPI software.", "num_citations": "14\n", "authors": ["637"]}
{"title": "Towards Performance Prediction for Cloud Computing Environments based on Goal-oriented Measurements.\n", "abstract": " Scalability and performance are critical quality attributes of applications developed for the cloud. Many of these applications have to support hundreds or thousands of concurrent users with strongly fluctuating workloads. Existing approaches for software performance evaluation do not address the new challenges that arise for applications executed in cloud computing environments. The effects of virtualization on response times, throughput, and resource utilisation as well as the massive number of resources available require new platform and resource models for software performance evaluation. Modelling cloud environments using established approaches for software performance prediction is a cumbersome task that requires a detailed understanding of virtualization techniques and their effect on software performance. Additional complexity comes from the fact that cloud environments may combine multiple virtualization platforms which differ in implementation and performance properties.In this position paper, we propose an approach to infer performance models of cloud computing environments automatically through goal-oriented measurements. The resulting performance models can be directly combined with established model-driven performance prediction approaches. We outline the research challenges that have to be addressed in order to employ the approach for design-time performance predictions of software systems running in cloud computing environments.", "num_citations": "13\n", "authors": ["637"]}
{"title": "A prediction model for software performance in symmetric multiprocessing environments\n", "abstract": " The broad introduction of multi-core processors made symmetric multiprocessing (SMP) environments mainstream. The additional cores can significantly increase software performance. However, their actual benefit depends on the operating system scheduler's capabilities, the system's workload, and the software's degree of concurrency. The load distribution on the available processors (or cores) strongly influences response times and throughput of software applications. Hence, understanding the operating system scheduler's influence on performance and scalability is essential for the accurate prediction of software performance (response time, throughput, and resource utilisation). Existing prediction approaches tend to approximate the influence of operating system schedulers by abstract policies such as processor sharing and its more sophisticated extensions. However, these abstractions often fail to\u00a0\u2026", "num_citations": "13\n", "authors": ["637"]}
{"title": "Formal foundations of dynamic types for software components\n", "abstract": " In this work we describe the foundations of a type system for software components, which supports (1) error checking during composition,(2) automatic adaption of a component's services according to the resources of its surrounding environment, and (3) controlled extension of components by plug-ins. Since the type information is described at the interface of a component, this type system can also be regarded as an new notion for to enhance classical interfaces. This document gives a specification of a component type and describes algorithms performing the above actions (1)-(3).", "num_citations": "13\n", "authors": ["637"]}
{"title": "Monitor overhead measurement with SKaMPI\n", "abstract": " The activities testing and tuning of the software lifecycle are concerned with analyzing program executions. Such analysis relies on state information that is generated by monitoring tools during program runs. Unfortunately the monitor overhead imposes intrusion onto the observed program. The resulting influences are manifested as different temporal behavior and possible reordering of nondeterministic events, which is called the probe effect. Consequently correct analysis data requires to keep the perturbation a minimum, which defines the need for monitors with small overhead. Measuring the actual overhead of monitors for MPI programs can be done with the benchmarking suite SKaMPI. It\u2019s results serve as a main characteristic for the quality of the applied tool, and additionally increase the user\u2019s awareness of the monitoring crisis. Besides that, the measurements of SKaMPI can be used in correction\u00a0\u2026", "num_citations": "13\n", "authors": ["637"]}
{"title": "TimerMeter: Quantifying properties of software timers for system analysis\n", "abstract": " To analyse runtime behaviour and performance of software systems, accurate time measurements are obtained using timer methods. The underlying hardware timers and counters are read and processed by several software layers, which introduce overhead and delays that impact accuracy and statistical validity of fine-granular measurements. To understand and to control these impacts, the resulting accuracy of timer methods and their invocation costs must be quantified. However, quantitative properties of timer methods are usually not specified as they are platform-specific due to differences in hardware, operating systems and virtual machines. Also, no algorithm exists for precisely quantifying the timer methods' properties, so programmers have to work with coarse estimates and cannot evaluate and compare different timer methods and timer APIs. In this paper, we present TimerMeter, a novel algorithm for\u00a0\u2026", "num_citations": "12\n", "authors": ["637"]}
{"title": "Dynamic types for software components\n", "abstract": " Component based software development requires run-time coupling of binary software components. We present a type system for software components, which allows to check for certain errors when coupling binary components. A unique feature of our type system is the dynamic adaption of a component's type according to the environment the component is plugged in.", "num_citations": "12\n", "authors": ["637"]}
{"title": "A comparative case study with industrial requirements engineering methods\n", "abstract": " Numerous requirements engineering methods have been proposed to improve the quality of requirements documents as well as the developed software and to increase customer satisfaction with the final product. In this paper, we report on an explorative case study in the area of reactive systems with eight requirements engineering methods from a wide spectrum, namely, BSM, OCTOPUS, ROOM, SA/RT, SCR, SDL, UML (with OMT process), and Z. Our major finding is that the structuring mechanisms provided by a requirements engineering method, like hierarchies (eg, ROOM) or views (eg, UML), are directly related (1) to the number of problems found in the informal requirements during the creation of a requirements specification as well as (2) to the understandability of the final requirements specification.", "num_citations": "12\n", "authors": ["637"]}
{"title": "A survey on the state and future of automotive software release and configuration management\n", "abstract": " In modern vehicles, embedded software is mainly the driving force for the realisation of new functionality. Spread on more than hundred electronic control units, software parts work together in a network to fulfil certain tasks of safety, comfort, energy management and of course vehicle dynamics. Currently, this software is flashed at the end of the assembly line. In regular terms (normally six months), a software baseline of all the electronic control units is released. As a consequence, a distinct variety of releases is deployed on vehicles on the road for each vehicle type. Combined with the alternatives of engines, chassis and customer wishes, the variants of vehicles rise to a number beyond millions. Opening now the chance for over the air updates demands for restrictive and rigorous consistency checks before any update is spread among all the vehicles of one brand or type in the field.In an empirical study based on a survey, we collected and interpreted the answers of participants from different automotive institutions concerning the current state of practice and the faced challenges during release development and management. The outcome of the survey revealed that field updates are getting more and more important due to the rising software part inside vehicles and that over the air communication is an efficient way to realise them in the future. The shortening release and update cycles together with increasing number of variants and the multidisciplinarity in the automotive field were identified as the main challenges in the current development of automotive engineering.", "num_citations": "11\n", "authors": ["637"]}
{"title": "Challenges in secure software evolution-the role of software architecture\n", "abstract": " Achieving quality properties for software systems and maintaining them during evolution is challenging. Especially, security properties often degrade during software evolution. This is often not noticed and can lead to monetary loss and serious damage to the company\u2019s image. Approaches for maintaining security properties exist but fail to exploit the knowledge of the architectural design phase. This results in high effort and slow reactions on evolutionary changes. In this paper, we describe five key challenges in maintaining security properties during software evolution and show how architecture supports mastering them.", "num_citations": "11\n", "authors": ["637"]}
{"title": "Deriving performance-relevant infrastructure properties through model-based experiments with Ginpex\n", "abstract": " To predict the performance of an application, it is crucial to consider the performance of the underlying infrastructure. Thus, to yield accurate prediction results, performance-relevant properties and behaviour of the infrastructure have to be integrated into performance models. However, capturing these properties is a cumbersome and error-prone task, as it requires carefully engineered measurements and experiments. Existing approaches for creating infrastructure performance models require manual coding of these experiments, or ignore the detailed properties in the models. The contribution of this paper is the Goal-oriented INfrastructure Performance EXperiments (Ginpex) approach, which introduces goal-oriented and model-based specification and generation of executable performance experiments for automatically detecting and quantifying performance-relevant infrastructure properties. Ginpex\u00a0\u2026", "num_citations": "11\n", "authors": ["637"]}
{"title": "Position paper: approach for architectural design and modelling with documented design decisions (admd3)\n", "abstract": " Documented design decisions simplify the evolution of software systems. However, currently design decisions are often either badly documented or are not documented at all. Relations between requirements, decisions, and architectural elements are missing, and architecture alternatives are not preserved. As a consequence it is hard to identify deprecated design solutions when requirements change In this position paper, we present an approach to document software architecture design decisions, together with related requirements and related architectural elements, through the goal-driven elicitation of those requirements needed to make a design decision. Therefore, we propose a process model and supporting meta-models, including a meta-model for a pattern catalogue. The speciality of this pattern catalogue is the inclusion of questions to drive requirements engineering to validate pattern selections, and to\u00a0\u2026", "num_citations": "11\n", "authors": ["637"]}
{"title": "An enhanced model for component interfaces to support automatic and dynamic adaption\n", "abstract": " This paper presents a new model of software component interfaces, using an extension of finite states machines to describe (a) the protocol to use a component\u2019s offered services, and (b) the sequences of calls to the external services the component requires to fulfill its offered services. With this model we integrate information into the interface of a software component to:(a) Check whether a component will be used correctly in its environment during system integration (ie, before the component is actually used).(b) Adapt the interface of a component which describes the component\u2019s offered services, in case the environment does not offer all resources the component requires to offer all its services. In this case the adapted component still offers a subset of its services, to the contrary of todays component systems, which do not allow any integration in this case at all.", "num_citations": "11\n", "authors": ["637"]}
{"title": "A Meta-protocol and Type system for the Dynamic Coupling of Binary Components\n", "abstract": " We introduce a new type system, where the type of a component consists of two protocols\u2014a call and a use protocol. We model these protocols by finite automata and show how those reflect component enhancement and adaption. Coupling is controlled by a meta-protocol, which calls the adaption and enhancement algorithms. These algorithms require type information of the components involved. This type information is provided by the meta-protocol using reflection. This mechanism allows automatic adaption of component types in changing environments.", "num_citations": "11\n", "authors": ["637"]}
{"title": "SKaMPI: the Special Karlsruher MPI-benchmark: User Manual\n", "abstract": " SKaMPI is the Special Karlsruher MPI-Benchmark. SKaMPI measures the performance of MPI 3] 1] implementations, and of course of the underlying hardware. It performs various measurements of several MPI functions. SKaMPI's primary goal is giving support to software developers. The knowledge of MPI function's performance has several benefits: The software developer knows the right way of implementing a program for a given machine, without (or with shortening) the tedious time costly tuning, which usually has to take place. The developer has not to wait until the code is written, performance issues can also be considered during the design stage. Developing for performance even can take place, also if the considered target machine is not accessible. MPI performance knowledge is especially important, when developing portable parallel programs. So the code can be developed for all considered target platforms in an optimal manner. So we achieve performance portability, which means that code runs without time consuming tuning after recompilation on a new platform.", "num_citations": "11\n", "authors": ["637"]}
{"title": "A layered reference architecture for metamodels to tailor quality modeling and analysis\n", "abstract": " Nearly all facets of our everyday life strongly depend on software-intensive systems. Besides correctness, highly relevant quality properties of these systems include performance, as directly perceived by the user, and maintainability, as an important decision factor for evolution. These quality properties strongly depend on architectural design decisions. Hence, to ensure high quality, research and practice is interested in approaches to analyze the system architecture for quality properties. Therefore, models of the system architecture are created and used for analysis. Many different languages (often defined by metamodels) exist to model the systems and reason on their quality. Such languages are mostly specific to quality properties, tools or development paradigms. Unfortunately, the creation of a specific model for any quality property of interest and any different tool used is simply infeasible. Current metamodels\u00a0\u2026", "num_citations": "10\n", "authors": ["637"]}
{"title": "Karlsruher Thesen zur Digitalen Souver\u00e4nit\u00e4t Europas\n", "abstract": " Die Digitale Souver\u00e4nit\u00e4t stellt die Wirtschaft, Politik und Forschung Europas vor gro\u00dfe Herausforderungen. Diese und m\u00f6gliche L\u00f6sungen stellen f\u00fchrende Forscher im Bereich IT-Sicherheit der Karlsruher Forschungseinrichtungen, dem Karlsruher Institut f\u00fcr Technologie (KIT), FZI Forschungszentrum Informatik und Fraunhofer Institut f\u00fcr Optronik, Systemtechnik und Bildauswertung (IOSB) in den folgenden Thesen vor.", "num_citations": "10\n", "authors": ["637"]}
{"title": "Performance certification of software components\n", "abstract": " Non-functional properties of software should be specified early in the development process. In a distributed process of software development, this means that quality requirements must be made explicit in the specification, and the developing party of a commissioned component needs to deliver not only the implemented component, but also a description of its non-functional properties. Based on these artefacts, a conformance check guarantees that the implemented component fulfills the performance requirements.We extend the notion of model refinement to non-functional properties of software and propose a refinement calculus for conformance checking between abstract performance descriptions of components. The calculus is based on a refinement notion that covers the performance-relevant aspects of components. The approach is applied to the Palladio Component Model as a description language for\u00a0\u2026", "num_citations": "10\n", "authors": ["637"]}
{"title": "Automatic derivation of performance prediction models for load-balancing properties based on goal-oriented measurements\n", "abstract": " In symmetric multiprocessing environments, the performance of a software system heavily depends on the application's parallelism, the scheduling and load-balancing policies of the operating system, and the infrastructure it is running on. The scheduling of tasks can influence the response time of an application by several orders of magnitude. Thus, detailed models of the operating system scheduler are essential for accurate performance predictions. However, building such models for schedulers and including them into performance prediction models involves a lot of effort. For this reason, simplified scheduler models are used for the performance evaluation of business information systems in general. In this work, we present an approach to derive load-balancing properties of general-purpose operating system (GPOS) schedulers automatically. Our approach uses goal-oriented measurements to derive\u00a0\u2026", "num_citations": "10\n", "authors": ["637"]}
{"title": "Design for Future\u2013Legacy-Probleme von morgen vermeidbar?\n", "abstract": " Eine weit verbreitete Annahme ist, dass Software nicht altert, da Software als immaterielles Gut keinerlei Verschlei\u00dferscheinungen unterliegt. Diese Annahme ist aber falsch\u2013Software altert. Die Umgebung, in der Software eingesetzt wird, ver\u00e4ndert sich kontinuierlich und die dazugeh\u00f6rige Hardware und die Infrastruktur ver\u00e4ndern sich. Wird die Software nicht ebenfalls kontinuierlich weiterentwickelt, so altert sie rasch relativ zu ihrer Umgebung. Dieses Problem ist vor allem im Bereich der gro\u00dfen betrieblichen Informationssysteme unter dem Oberbegriff Legacy bekannt. Es sind gro\u00dfe Systeme im industriellen Einsatz, teilweise noch in FORTRAN oder COBOL geschrieben, die trotz suboptimaler Funktionalit\u00e4t und Qualit\u00e4t weiter unangepasst im Einsatz bleiben, da eine Optimierung an der Komplexit\u00e4t des Systems und dem Aufwand f\u00fcr eine Einarbeitung durch die heutigen Entwickler scheitert. Eine Neuerstellung\u00a0\u2026", "num_citations": "10\n", "authors": ["637"]}
{"title": "Variants and versions management for models with integrated consistency preservation\n", "abstract": " Modern software systems are often developed and maintained by describing them in several modeling and programming languages. To reduce complexity and improve understandability of such systems, models represent specific views on the system. These views have semantic interrelations (eg, by sharing common or dependent information) that need to be kept consistent during evolution of the system. Apart from that, modern systems need to run in many different contexts and be highly configurable to satisfy the demand for fully customizable products. Such variable systems often comprise various dependencies from which inconsistencies may arise. Combining solutions for consistency management with variants and versions management, however, comes with many challenges.", "num_citations": "9\n", "authors": ["637"]}
{"title": "Analysing the fidelity of measurements performed with hardware performance counters\n", "abstract": " Performance evaluation requires accurate and dependable measurements of timing values. Such measurements are usually made using timer methods, but these methods are often too coarse-grained and too inaccurate. Thus, direct usage of hardware performance counters is frequently used for fine-granular measurements due to higher accuracy. However, direct access to these counters may be misleading on multicore computers because cores can be paused or core affinity changed by the operating system, resulting in misleading counter values. The contribution of this paper is the demonstration of an additional, significant flaw arising from the direct use of hardware performance counters. We demonstrate that using JNI and assembler instructions to access the Timestamp Counter from Java applications can result in grossly wrong values, even in single-threaded scenarios.", "num_citations": "9\n", "authors": ["637"]}
{"title": "Dependability metrics\n", "abstract": " With the growing ubiquity of computing systems it is essential that we can place reliance on the services they deliver. This is particularly obvious and important in areas like aircraft avionics, global financial transaction processing, or nuclear power plant control where human lives or large financial values are at stake. But also the worldwide daily nuisances of computer viruses or data corruptions caused by crashing operating systems collectively impose high costs on society, which are beginning to become economically relevant. Within computer science, the term dependability has been introduced as a general term to cover all critical quality aspects of computing systems. Following the terminology of Laprie [26, 293], a system is dependable if trust can justifiably be placed in the service it delivers (we will define dependability and related terms more precisely later in this book). In the early days of computer science\u00a0\u2026", "num_citations": "9\n", "authors": ["637"]}
{"title": "Dynamic coupling of binary components and its technical support\n", "abstract": " Consider a framework for a mail user agent with a text\u2212 reader, a sound\u2212 player and a video\u2212 player as pre\u2212 compiled (binary) components to be added dynamically, ie, at run\u2212 time. Note, that the framework component, ie the mail user agent, should not need to know in advance which kinds of mails it has to handle, so it leaves some aspects open. But nevertheless, we want it in compiled form, because the user should not need to recompile the mail user agent only because he receives an unknown kind of mail. The current solution of MIME\u2212 types is not satisfying, because the functionality of the required components (plug\u2212 ins) for handling the different MIME\u2212 types is not properly integrated into the mail user agent.", "num_citations": "9\n", "authors": ["637"]}
{"title": "Enabling consistency in view-based system development\u2014The Vitruvius approach\n", "abstract": " During the development of large software-intensive systems, developers use several modeling languages and tools to describe a system from different viewpoints. Model-driven and view-based technologies have made it easier to define domain-specific languages and transformations.Nevertheless, using several languages leads to fragmentation of information, to redundancies in the system description, and eventually to inconsistencies. Inconsistencies have negative impacts on the system\u2019s quality and are costly to fix. Often, there is no support for consistency management across multiple languages. Using a single language is no practicable solution either, as it is overly complex to define, use, and evolve such a language. View-based development is a suitable approach to deal with complex systems, and is widely used in other engineering disciplines. Still, we need to cope with the problems of fragmentation and\u00a0\u2026", "num_citations": "8\n", "authors": ["637"]}
{"title": "A Categorization of Interoperability Issues in Networks of Transformations.\n", "abstract": " Bidirectional transformations (BX) are a common approach for keeping two types of models consistent, but consistency preservation between more than two types of models is not researched well. One solution is the composition of BX to networks of transformations. Nevertheless, such networks are prone to failures due to interoperability issues between the individual BX, which are independently developed by various experts. We therefore systematically identify and categorize such issues. First, we structure the process of consistency specification into different conceptual levels. Then we develop a catalog of potential mistakes, which we derive from those levels, and consequential failure types. Finally, we discuss strategies to avoid mistakes at the different levels. This catalog is beneficial for transformation developers and transformation language developers. It improves awareness in developers of potential mistakes and consequential failures, enables the development of techniques to avoid specific mistakes by construction, and eases the identification of reasons for failures.", "num_citations": "8\n", "authors": ["637"]}
{"title": "A generic approach for Architecture-level performance modeling and prediction of virtualized storage systems\n", "abstract": " Virtualized environments introduce an additional abstraction layer on top of physical resources to enable the collective resource usage by multiple systems. With the rise of I/O-intensive applications, however, the virtualized storage of such shared environments can quickly become a bottleneck and lead to performance and scalability issues. The latter can be avoided through careful design of the application architecture and systematic capacity planning throughout the system life cycle. In current practice, however, virtualized storage and its performance-influencing design decisions are often neglected or treated as a black-box. In this work-in-progress paper, we propose a generic approach for performance modeling and prediction of virtualized storage systems at the software architecture level. More specifically, we propose two performance modeling approaches of virtualized systems. Furthermore, we propose two\u00a0\u2026", "num_citations": "8\n", "authors": ["637"]}
{"title": "Managing product line variability by patterns\n", "abstract": " Software product lines have a demonstrated potential for cost-effective development of software families. Product lines have to support and coordinate variabilities between the different members of the product family. However, it is also known that the management of these variabilities and the concurrent evolution of product line architecture and single products are still challenging tasks [1]. This organizational overhead often prevents small and medium enterprizes with limited software development staff from adopting product lines. This paper introduces three classes of product line variability and discusses their impacts to product line architectures. In particular, we discuss the management of these variabilities, by introducing a pattern-based product line architecture and an associated pattern language for statistical analysis software.", "num_citations": "8\n", "authors": ["637"]}
{"title": "Exploiting protocol information for speeding up runtime reconfiguration of component-based systems\n", "abstract": " To reduce the down-time of software systems and maximise the set of available services during reconfiguration, we propose exploiting component protocol information. This is achieved by knowing the state of a running system and determining the component dependencies for the time interval from receiving a reconfiguration request until reconfiguration completion. For this forecast we use the architectural descriptions that specify static dependencies, as well as component protocol information. By considering only component interactions for the time interval of reconfiguration we can exclude past and future dependencies from our runtime dependency graphs. We show that such change-request-specific runtime dependency graphs may be considerably smaller than the corresponding static architecture based dependency graphs; this way, we are speeding up runtime reconfiguration of component-based systems while maximising the set of available services.", "num_citations": "8\n", "authors": ["637"]}
{"title": "Identifying Needs for a Holistic Modelling Approach to Privacy Aspects in Enterprise Software Systems.\n", "abstract": " Modelling is a common method for both Business Architecture Management and for Software Architecture Management. In general, there is a gap in the model continuity between business models and software models. Especially when modelling compliance driven requirements like privacy traceability is important for compliance checks and helps to build the models in an efficient way. In this paper, approaches for modelling privacy from business and software engineering perspective are examined. A key finding is that there is currently no comprehensive modelling approach covering the needed aspects and perspectives.", "num_citations": "7\n", "authors": ["637"]}
{"title": "Collaborative Modeling Enabled By Version Control\n", "abstract": " Model-Driven Software Development is a key field in software development activities which is well-suited to design and develop large-scale software systems. Developing and maintaining large-scale model-driven software systems entail a need for collaborative modeling by a large number of software designers and developers. As long as software models are constantly changed during development and maintenance, collaborative modeling requires frequently sharing of model changes between collaborators. Thereby, a solid change representation support for model changes plays an essential role for collaborative modeling systems. This paper focuses on the problem of model change representation for collaborative modeling. It introduces a meta-model generic, operation-based and textual difference language to represent model changes in collaborative modeling. This paper also demonstrates a collaborative modeling application Kotelett.", "num_citations": "7\n", "authors": ["637"]}
{"title": "Architecture-based analysis of changes in information system evolution\n", "abstract": " Software is subject to continuous change. Software quality is determined by large extent through architecture which reflects important decisions, eg on structure and technology. For sound decision making during evolution change impacts on various system artifacts must be understood. In this paper, we introduce a new evolution scenario (replacing the database) to an established demonstrator for information system evolution. We demonstrate the application of an architecture-based approach for change impact analysis to identify artifacts affected by the scenario.", "num_citations": "7\n", "authors": ["637"]}
{"title": "Enriching software architecture models with statistical models for performance prediction in modern storage environments\n", "abstract": " Model-based performance prediction approaches on the software architecture-level provide a powerful tool for capacity planning due to their high abstraction level. To process the increasing amount of data produced by today's applications, modern storage systems are becoming increasingly complex having multiple tiers and intricate optimization strategies. Current software architecture-level modeling approaches, however, struggle to account for this development and are not well-suited in complex storage environments due to overly simplistic storage assumptions, which consequently leads to inaccurate performance predictions. To address this problem, in this paper we present a novel approach to combine software architecture-level performance models with statistical models that capture the complex behavior of modern storage systems. More specifically, we first propose a general methodology for enriching\u00a0\u2026", "num_citations": "7\n", "authors": ["637"]}
{"title": "An approach for integrated lifecycle management for business processes and business software\n", "abstract": " The lifecycles of business processes and business software interact with each other since business software is used to support business processes, and requirements on business software are derived from business processes. By integrating these lifecycles, it is possible to test if the business software meets the process-based requirements as well as to identify which impacts changes of the software product have on the business process. In this chapter, the authors give an introduction into these interdependencies. Foundations of lifecycle management, business process modeling, and performance engineering are presented, followed by the description of a framework for an integrated lifecycle management for business processes and business software. This framework is based on business process simulation and software performance prediction. The evaluation of the framework is described by applying it to an\u00a0\u2026", "num_citations": "7\n", "authors": ["637"]}
{"title": "Architectural concepts in programming languages\n", "abstract": " The deterioration of architectural structure and the lack of modularity have resulted in major deficiencies that will require more support from components and their interfaces. Programming language concepts also must explicitly reflect this support.", "num_citations": "7\n", "authors": ["637"]}
{"title": "Technical report: Secure cloud computing through a separation of duties\n", "abstract": " Cloud Computing offers many opportunities but also introduces new risks. A user who outsources a database into the Cloud loses control over his data. While the data can be secured against external threats using standard techniques, the service providers themselves have to be trusted to ensure privacy. This work proposes a novel approach to provide security for database services without the need to trust the provider. We suggest employing a separation of duties by distributing critical information and services among multiple providers in a way that the secrecy of a database can only be compromised if corrupted providers work together. We present a formal security model to evaluate distribution procedures and apply it to our approach. A result of independent interest shows that our novel security property implies k-anonymity for deterministic anonymization procedures.", "num_citations": "7\n", "authors": ["637"]}
{"title": "Performance evaluation of scheduling policies in symmetric multiprocessing environments\n", "abstract": " The shift of hardware architecture towards parallel execution led to a broad usage of multi-core processors in desktop systems and in server systems. The benefit of additional processor cores for software performance depends on the software's parallelism as well as the operating system scheduler's capabilities. Especially, the load on the available processors (or cores) strongly influences response times and throughput of software applications. Hence, a sophisticated understanding of the mutual influence of software behaviour and operating system schedulers is essential for accurate performance evaluations. Multi-core systems pose new challenges for performance analysis and developers of operating systems. For example, an optimal scheduling policy for multi-server systems, such as shortest remaining processing time (SRPT) for single-server systems, is not yet known in queueing theory. In this paper, we\u00a0\u2026", "num_citations": "7\n", "authors": ["637"]}
{"title": "Funktionsgetriebene Integration von Legacy-Systemen mit Web Services\n", "abstract": " In diesem Beitrag wird eine Architektur vorgestellt, auf deren Grundlage die Kommunale Datenverarbeitung Oldenburg (KDO) ihre umfangreichen Legacy-Anwendungssysteme in eine moderne Softwarearchitektur integriert. Grundlage dieser Architektur ist das Dublo-Architekturmuster, das die sanfte Integration und Migration von Legacy-Anwendungssystemen unterst\u00fctzt. Das betrachtete kommunale Anwendungssystem wird \u00fcber Web Services in eine J2EE-Architektur integriert, wobei die zustandsabh\u00e4ngige Verf\u00fcgbarkeit einzelner Funktionen anhand von Protokollautomaten kontrolliert wird.", "num_citations": "7\n", "authors": ["637"]}
{"title": "Empirical research on similarity metrics for software component interfaces\n", "abstract": " In this article we define metrics for measuring component interface similarity. These metrics were applied in an empirical study investigating the relationship between the functionality and the interfaces of software components. The main contributions of this article are:(i) the introduction of software component interface refactorings, ie, transformations working on IDL-style (ie, signature-list based) component interfaces. These refactorings are used to define a similarity metric for signature-list based component interfaces,(ii) the definition of efficiently computable metrics measuring the similarity of software component protocols (ie, valid call sequences to component services), and,(iii) the discussion of first insights from the metrics' application in the context of our empirical study.", "num_citations": "7\n", "authors": ["637"]}
{"title": "Benchmarking collective operations with SKaMPI\n", "abstract": " This article concentrates on recent work on benchmarking collective operations with SKaMPI. The goal of the SKaMPI project is the creation of a database containing performance measurements of parallel computers in terms of MPI operations. Its data support software developers in creating portable and fast programs. Existing algorithms for measuring the timing of collective operations are discussed and a new algorithm is presented, taking into account the differences of local clocks. Results of measurements on the HLRS Cray T3E/900 are presented and compared with other machines.", "num_citations": "7\n", "authors": ["637"]}
{"title": "Adapting components and predicting architectural properties with parameterised contracts\n", "abstract": " \u00bd While interoperability tests check the inclusion of a component's requires interface in the environmental provides interfaces, parameterised contracts link the provides-and the requires-interface of a component. This allows to perform automatically a certain class of component adaptations without changing the code. These adaptations also do not have to be foreseen and programmed by the component developer in advance. We show how parameterised contracts can be used to enhance the reusability of software components in software architectures. Combining parameterised contracts themselves results again in a parameterised contract, describing the properties of a component assembly. We present to common connection styles, which can be used to conclude from local component properties to global architectural properties.", "num_citations": "7\n", "authors": ["637"]}
{"title": "Skalib: Skampi as a library-technical reference manual\n", "abstract": " SKaLib is a library to support the development of benchmarks. It osprings from the SKaMPI-project [2]. SKaMPI is a benchmark to measure the performance of MPI-operations [6]. Many mechanisms and function of the SKaMPI-benchmark program are also useful when benchmarking other functions than MPI's. The goal of SKaLib is to oer the benchmarking mechanisms of SKaMPI to a broader range of applications. The mechanisms are: precision adjustable measurement of time, controlled standard error, automatic parameter renement, and merging results of several benchmarking runs. This documents fullls two purposes: on the one hand it should be a manual to use the library SKaLib and explains how to benchmark an operation. On the other hand this report complements the SKaMPI-user manual [4]. The latter report explains the congurations and the output of SKaMPI, whereas this reports gives a detailed description of the internal data structures and operations used in the SKaMPI-benchmark. There is...", "num_citations": "7\n", "authors": ["637"]}
{"title": "Towards classes of architectural dependability assurance for machine-learning-based systems\n", "abstract": " Advances in Machine Learning (ML) have brought previously hard to handle problems within arm's reach. However, this power comes at the cost of unassured reliability and lacking transparency. Overcoming this drawback is very hard due to the probabilistic nature of ML. Current approaches mainly tackle this problem by developing more robust learning procedures. Such algorithmic approaches, however, are limited to certain types of uncertainties and cannot deal with all of them, eg, hardware failure. This paper discusses how this problem can be addressed at architectural rather than algorithmic level to assess systems dependability properties in early development stages. Moreover, we argue that Self-Adaptive Systems (SAS) are more suited to safeguard ML wrt various uncertainties. As a step towards this we propose classes of dependability in which ML-based systems may be categorized and discuss which\u00a0\u2026", "num_citations": "6\n", "authors": ["637"]}
{"title": "An extensible approach to implicit incremental model analyses\n", "abstract": " As systems evolve, analysis results based on models of the system must be updated, in many cases as fast as possible. Since usually only small parts of the model change, large parts of the analysis\u2019 intermediate results could be reused in an incremental fashion. Manually invalidating these intermediate results at the right places in the analysis is a non-trivial and error-prone task that conceals the codes intention. A possible solution for this problem is implicit incrementality, i.e., an incremental algorithm is derived from the batch specification, aiming for an increased performance without the cost of degraded maintainability. Current approaches are either specialized to a subset of analyses or require explicit state management. In this paper, we propose an approach to implicit incremental model analysis capable of integrating custom dynamic algorithms. For this, we formalize incremental derivation using\u00a0\u2026", "num_citations": "6\n", "authors": ["637"]}
{"title": "Classifying Approaches for Constructing Single Underlying Models\n", "abstract": " Multi-view environments for software development allow different views of a software system to be defined to cover the requirements of different stakeholders. One way of ensuring consistency of overlapping information often contained in such views is to project them \u201con demand\u201d from a Single Underlying Model (SUM). However, there are several ways to construct and adapt such SUMs. This paper presents four archetypal approaches and analyses their advantages and disadvantages based on several new criteria. In addition, guidelines are presented for selecting a suitable SUM construction approach for a specific project.", "num_citations": "6\n", "authors": ["637"]}
{"title": "A cross-disciplinary language for change propagation rules\n", "abstract": " Automated production systems are in operation for a long time and are continuously being changed. Therefore, for these systems it is important to have the ability to react efficiently to changes. Change propagation analysis approaches allow predicting the effects of changes before they are actually implemented. Such approaches often use predefined change propagation rules that indicate how the change propagates in a system. However, the change propagation rules used by these approaches are limited to a discipline such as information systems, to the structure of system elements in a discipline, or to a programming language such as Java. In this paper, we present a cross-disciplinary language to specify change propagation rules. The proposed language is independent of a particular discipline, structure of system elements, or programming languages. To show the improvement of the readability and the\u00a0\u2026", "num_citations": "6\n", "authors": ["637"]}
{"title": "The current state of the holistic privacy and security modelling approach in business process and software architecture modelling\n", "abstract": " Modelling is central for business process and software architecture documentation and analysis. However, business processes and software architectures are specified with their own highly developed languages, methods and tools. There are approaches in the literature for modelling privacy and security issues using existing business process or architecture modelling languages to express different requirements by enriching these languages with annotations. Nevertheless, there is a lack of formalization and therefore the potential use for tool-based analyses are limited. In addition, the continuity between business and software models is not granted, but when modelling compliance requirements like privacy, traceability is very important, e.g. for compliance checks. In this contribution, approaches for modelling security and privacy in business and software models are examined. One key finding is that there\u00a0\u2026", "num_citations": "6\n", "authors": ["637"]}
{"title": "The CoCoME platform: A research note on empirical studies in information system evolution\n", "abstract": " Methods for supporting evolution of software-intensive systems are a competitive edge in software engineering as software is often operated over decades. Empirical research is useful to validate the effectiveness of these methods. However, empirical studies on software evolution are rarely comprehensive and hardly replicable. Collaboration may prevent these shortcomings. We designed CoCoMEP\u00a0\u2014 a platform for supporting collaboration in empirical research on software evolution by shared knowledge. We report lessons learned from the application of the platform in a large research programme.", "num_citations": "6\n", "authors": ["637"]}
{"title": "Architecture-based assessment and planning of software changes in information and automated production systems state of the art and open issues\n", "abstract": " Information and automated production systems are long-living, evolvable systems. Consequently, modifications are performed to correct, improve or adapt the respective system. We introduce different approaches to analyze maintainability of software-intensive systems and propose two different case studies from the information and from the automated production systems domain as a basis to validate approaches on system evolution.", "num_citations": "6\n", "authors": ["637"]}
{"title": "Reverse Engineering of Parametric Behavioural Service Performance Models from Black-Box Components\n", "abstract": " Integrating heterogeneous software systems becomes increasingly important. It requires combining existing components to form new applications. Such new applications are required to satisfy non-functional properties, such as performance. Design-time performance prediction of new applications built from existing components helps to compare design decisions before actually implementing them to the full, avoiding costly prototype and glue code creation. But design-time performance prediction requires understanding and modeling of data flow and control flow accross component boundaries, which is not given for most black-box components. If, for example one component processes and forwards files to other components, this effect should be an explicit model parameter to correctly capture its performance impact. This impact should also be parameterised over data, but no reverse engineering approach exists to recover such dependencies. In this paper, we present an approach that allows reverse engineering of such behavioural models, which is applicable for blackbox components. By runtime monitoring and application of genetic programming, we recover functional dependencies in code, which then are expressed as parameterisation in the output model. We successfully validated our approach in a case study on a file sharing application, showing that all dependencies could correctly be reverse engineered from black-box components.", "num_citations": "6\n", "authors": ["637"]}
{"title": "Counter-constrained finite state machines: A new model for component protocols with resource-dependencies\n", "abstract": " This paper deals with the specification of software component protocols (i. e., the set of service call sequences). The contribution of this paper is twofold: (a) We discuss specific requirements of realworld protocols, especially in the presence of components which make use of limited resources. (b) We define counter-constrained finite state machines, a novel extension of finite state machines, specifically created to model protocols containing dependencies between services due to their access to shared resources. Opposed to other approaches like classical finite state machines, this newmo del combines two valuable properties: (a) it is powerful enough to model realistic component protocols with resource allocation, -usage, and -deallocation dependencies between methods (as occurring in common abstract data-types such as stacks or queues) and (b) allows effcient checking of interoperability and\u00a0\u2026", "num_citations": "6\n", "authors": ["637"]}
{"title": "Portable Leistungsmessung des Message Passing Interfaces\n", "abstract": " Der 1994 geschaffene Message-Passing-Interface-Standard (MPI) 9] stellt eine herstellerunabh angige Programmierschnittstelle f ur nachrichtengekoppelte Rechnersysteme vom Ethernet-LAN bis zum H ochstleistungsrechner dar. Heute gibt es f ur eine Vielzahl von verschiedenen Hardwareplattformen einsetzbare Implementierungen.", "num_citations": "6\n", "authors": ["637"]}
{"title": "Using SKaMPI for developing high-performance MPI programs with performance portability\n", "abstract": " The current practice of developing high-performance software for parallel computers includes a tuning phase where the software\u2019s performance is optimised for a specific hardware platform. This tuning phase often is costly and results in machine-specific, hence, less portable software. In this paper we present a publicly available database providing performance data for operations of the message-passing-interface (MPI) measured on several different platforms. This allows to design MPI programs for performance and portability in early stages of software development. Considering the performance of MPI operations while designing programmes allows the software developer (a) to select the fastest implementation alternative, (b) to write performance portable software (i.e., software showing high performance on several platforms without platform-specific tuning), if possible, and (c) to quantify the tradeoff between\u00a0\u2026", "num_citations": "5\n", "authors": ["637"]}
{"title": "A controlled experiment on the understandability of different requirements specifications styles\n", "abstract": " In this paper, we report on a controlled experiment, in which we compared two different requirements specification styles. Following the traditional black-box style, a system is described by its externally visible behavior, any design detail is omitted from the requirements. Following the white-box style, which was popularized by object-oriented analysis, a system is described by the behavior of its constituent entities, eg, objects. In the experiment, we compared the understandability of two requirements specifications of the same system each written in a different style.", "num_citations": "5\n", "authors": ["637"]}
{"title": "Continuous design decision support\n", "abstract": " Continuous Software Engineering (CSE) is a software engineering process in which developers continuously change the software while keeping it in a releasable state [KB17]. CSE means to develop, release, and learn from software in very short rapid cycles [Bos14]. It incorporates agile practices and involves activities such as continuous integration, delivery, and deployment [SAZ17, Joh+ 18b]. The emergence of CSE is driven by a growing need for flexibility and rapid adaption in the current software environment [FS17].Software developers and architects continuously make design decisions while they develop and evolve software. They make decisions on the requirements to be addressed, the design artefacts (eg architectural components, packages, interfaces, classes, and methods) to be created or the design patterns to be applied. For example, it is a design decision to apply an adapter design pattern instead of changing an existing interface when adding new features to a software. The knowledge of the developers on the design decisions they make is called decision knowledge. In particular, decision knowledge comprises the knowledge about the problems, the decisions they address, solution approaches, their context, and rationale in terms of arguments, criteria, and the assessment of solution alternatives. Decision knowledge should be communicated within a development team so that every developer knows and considers existing decisions [Bru+ 14]. When developers evolve software, it is important for them to reflect and build on former decisions. Otherwise, they might make inconsistent decisions and are likely to contribute to the\u00a0\u2026", "num_citations": "4\n", "authors": ["637"]}
{"title": "An approach to requirement analysis in automated production systems\n", "abstract": " Automated production systems (aPS) involve different disciplines, like mechanical and software engineering. Evolution has to be seen as a repetitive activity in these systems. Complexity of hardware and especially software is constantly rising and demands for automated solutions, as change propagation analysis by hand is slow and error-prone. In this paper, we present an approach to automatically calculate change propagation based on requirement changes in aPS. 1 IntroductionAutomation has become a crucial success factor for manufacturing industries. aPS are software-controlled mechanical systems, which are under operation for several decades [8]. During their lifetime aPS are subject of evolution due to new technological developments or changing requirements [7]. aPS involve multiple disciplines, as they comprise both hardware and software. The hardware further includes mechanical (eg fixtures) and electrical parts (eg sensors). Mutual dependencies between these disciplines cause evolution and change management to be challenging [7]. But, changes are often implemented ad hoc by responsible employees. Thus, change management in aPS is not well documented [7]. This paper presents an approach to automate change propagation analysis by extending an existing maintainability framework to support requirements in aPS. Automating change analysis allows profound documentation. Furthermore, the consideration of requirements allows to work on a higher level of abstraction and increases usability.", "num_citations": "4\n", "authors": ["637"]}
{"title": "Architecture-driven Reduction of Specification Overhead for Verifying Confidentiality in Component-based Software Systems.\n", "abstract": " Code verification techniques can be used to guarantee that some of the information processed in software systems remains confidential. For this, allowed information flows have to be specified for the system under analysis. Reducing the specification overhead could render code verification feasible where verification was considered too complex or costly so far. In this paper, we introduce a model-driven approach to reduce the overhead for creating and maintaining such specifications. Independent of the verification input format, developers can specify confidentiality for componentbased architecture models, which are kept consistent with object-oriented code. They are supported in adapting the specifications to evolving systems in order to detect information leaks with less effort and in earlier development stages.I. Introduction and Motivation For many software systems, the confidentiality of processed data is important. To guarantee this non-functional property, it is possible to verify the flow of information in the code. For this, confidentiality requirements have to be specified for every system under analysis. The effort required to specify and verify confidentiality represents a design and development overhead. A part of this effort is inevitable, and another part is due to accidental complexity. As a result, it is either more costly than necessary to develop secure software, or the software is less secure than required. In this paper, we present an approach for reducing the overhead for creating and maintaining confidentiality specifications using the component-based development paradigm and model-driven techniques. With this approach 1.) confidentiality\u00a0\u2026", "num_citations": "4\n", "authors": ["637"]}
{"title": "The OMPCM simulator for model-based software performance prediction\n", "abstract": " Software performance models play an important role in early stage quality evaluations. Performance models in particular allow for comparing architectural alternatives before unfavourable design decisions are made that need to be revised in a costly procedure. The Palladio component model (PCM) is a modelling language for component-based software architectures. Instances of the PCM can already be analysed for their performance using analytical or simulative approaches. It is, however, difficult to obtain accurate performance predictions for network-intensive distributed systems. This is mainly due to the simplistic network model used so far. In this paper, we present the OMPCM simulator, which integrates OMNeT++ network simulation with architecture-level software performance prediction. OMPCM models can be automatically created from PCM models using a chain of model transformations.", "num_citations": "4\n", "authors": ["637"]}
{"title": "Metric-based selection of timer methods for accurate measurements\n", "abstract": " Performance measurements are often concerned with accurate recording of timing values, which requires timer methods of high quality. Evaluating the quality of a given timer method or performance counter involves analysing several properties, such as accuracy, invocation cost and timer stability. These properties are metrics with platform-dependent values, and ranking and selecting timer methods requires comparisons using multidimensional metric sets, which make the comparisons ambiguous and unnecessary complex. To solve this problem, this paper proposes a new unified metric that allows for a simpler comparison. The one-dimensional metric is designed to capture fine-granular differences between timer methods, and normalises accuracy and other quality attributes by using CPU cycles instead of time units. The proposed metric is evaluated on all timer methods provided by Java and .NET platform APIs.", "num_citations": "4\n", "authors": ["637"]}
{"title": "Automated Benchmarking of Java APIs\n", "abstract": " Performance is an extra-functional property of software systems which is often critical for achieving sufficient scalability or efficient resource utilisation. As many applications are built using application programmer interfaces (APIs) of execution platforms and external components, the performance of the used API implementations has a strong impact on the performance of the application itself. Yet the sheer size and complexity of today's APIs make it hard to manually benchmark them, while many semantical constraints and requirements (on method parameters, etc.) make it complicated to automate the creation of API benchmarks. Additionally, modern execution platforms such as the Java Virtual Machine perform extensive nondeterministic runtime optimisations, which need to be considered and quantified for realistic benchmarking. In this paper, we present an automated solution for benchmarking any large APIs that are written in the Java programming language, not just the Java Platform API. Our implementation induces the optimisations of the Just-In-Time compiler to obtain realistic benchmarking results. We evaluate the approach on a large subset of the Java Platform API exposed by the base libraries of the Java Virtual Machine.", "num_citations": "4\n", "authors": ["637"]}
{"title": "Evaluation of maintainability of model-driven persistency techniques\n", "abstract": " Although the original OMG Model-Driven Architecture Approach is not concerned with software evolution, modeldriven techniques may be good candidates to ease software evolution. However, a systematic evaluation of the benefits and drawback of model-driven approaches compared to other approaches are lacking. Besides maintainability other quality attributes of the software are of interest, in particular performance metrics. One specific area where model driven approaches are established in the area of software evolution are the generation of adapters to persist modern object oriented business models with legacy software and databases. This paper presents a testbed and an evaluation process with specifically designed metrics to evaluate model-driven techniques regarding their maintainability and performance against established persistency frameworks.", "num_citations": "4\n", "authors": ["637"]}
{"title": "Towards more realistic component protocol modelling with finite state machines\n", "abstract": " In this paper we discuss the shortcomings of current approaches to software component protocol specification. While protocol specifications with a finite state space allow powerful analysis, the expressiveness of such descriptions is rather limited in several ways. In this paper, we discuss in particular the role of parameter values and return values of method names in provides and requires protocols of software components. We present a novel state-based approach where we handle explicitly (a) different outcomes of functions (returned values or exceptions thrown) and (b) different parameter values. We discuss this model in terms of practical specification possibilities, interoperability and substitutability checks and apply our model to specify as an example the FileStream class from Microsoft\u2019s .NET framework class library.", "num_citations": "4\n", "authors": ["637"]}
{"title": "Testing of component-based systems and software quality\n", "abstract": " Quality is generally an important issue in the development of products. Various methods and techniques have been developed to ensure high quality. One of these methods is the use of components. Prefabricated components can also be used in the development of software systems. Even though they can contribute to a quality increase, they do not obviate quality management. Therefore, testing of component-based systems still plays an important role. Testing can also be considered for other types of systems. Additionally, various types of tests can also be considered, such as robustness and performance tests. However, what information does a test yield? That depends on the quality of the test. Metrics, therefore, must not be forgotten, which can be applied to test suites as well as software. And in the case tests can not be applied, static quality assurance methods such as inspections may be used. When problems have been detected through tests or inspections, these may lead to refactorings which also need assistance.", "num_citations": "4\n", "authors": ["637"]}
{"title": "Applying patterns to develop a product line architecture for statistical analysis software\n", "abstract": " This paper discusses the role of patterns in product line design by introducing a pattern based product line architecture for statistical analysis software. Associated to the architecture is a pattern language describing the instantiation of concrete software products from the product line. As patterns document design decisions better than mere code, patterns hinder the architectural drift. Since stability and organized evolution is of high importance for long-term assets, the use of patterns and pattern languages is particularly interesting for product line architectures.", "num_citations": "4\n", "authors": ["637"]}
{"title": "Using Parameterized Contracts for Component Reliability Prediction\n", "abstract": " Using Parameterized Contracts for Component Reliability Prediction - Research Portal, King's College, London King's College London King's main site Research portal Home Researchers Research Groups Research Outputs Research Funding Internal Research Outputs Theses . Journals Publishers Using Parameterized Contracts for Component Reliability Prediction Research output: Chapter in Book/Report/Conference proceeding \u203a Conference paper Ralf Reussner, Heinz Schmidt, Iman Poernomo Overview Citation formats Original language English Title of host publication Conference of Integrated Design and Process Science Publisher SDPS Press Pages 241 - 252 Number of pages 12 Published 2002 Event Conference of Integrated Design and Process Science - Pasadena, United States Duration: 1 Jan 2002 \u2192 \u2026 Conference Conference Conference of Integrated Design and Process Science Country United \u2026", "num_citations": "4\n", "authors": ["637"]}
{"title": "A Model-Based Approach to Calculate Maintainability Task Lists of PLC Programs for Factory Automation\n", "abstract": " As long-living systems, automated Production Systems (aPS) have to be adapted due to optimization and inclusion of new features in their life cycle over decades. aPS consist of electrical, mechanical, and software components, which have a complex interaction and mutual dependencies. Consequently, these heterogeneous components have to be maintained together. Thus, the change propagation analysis in aPS is a challenging task. Existing approaches to change impact analysis lack tool-support and require expert knowledge in the aPS, as well as in the machine under study and its environment. This paper presents a tool-supported approach to change propagation analysis in aPS based on initial change requests. Our approach calculates a list of maintainability tasks to implement change requests in control programs deployed on Programmable Logic Controllers (PLC). To evaluate the quality and coverage\u00a0\u2026", "num_citations": "3\n", "authors": ["637"]}
{"title": "Towards a common classification of changes for information and automated production systems as precondition for maintenance effort estimation\n", "abstract": " Both information and automated production systems (aPS) evolve during their lifetime, e.g. due to changes in requirements and infrastructure. In order to estimate maintenance effort in information systems the KAMP method is applied. This paper discusses the necessary classification of changes as a prerequisite to apply such a method. Aggravating aPS consist not only of software but also include mechanics and electric/automation hardware. Therefore, the classification has to be enlarged to a multi-disciplinary one. The limitations of this approach for aPS are discussed in detail and demonstrated using three scenarios of a lab size pick and place unit. The paper closes delivering first ideas to cope with these.", "num_citations": "3\n", "authors": ["637"]}
{"title": "The Storage Performance Analyzer: Measuring, Monitoring, and Modeling of I/O Performance in Virtualized Environments (Invited Demonstration Paper)\n", "abstract": " The ever-increasing I/O resource demands pose significant challenges for today's system environments to meet performance requirements. The resource demand effects are even magnified in modern virtualized environments where workloads are consolidated to save hardware and operating costs. Tool-supported analysis approaches can help to understand I/O performance characteristics and avoid I/O performance and interference issues. In this demo paper, we present the Storage Performance Analyzer (SPA)-a tool for automated I/O performance analysis. SPA is equipped with tailored features for virtualized environments allowing to measure, monitor, and model both I/O performance and interference effects in modern environments. SPA is open-source and available for common operating systems.", "num_citations": "3\n", "authors": ["637"]}
{"title": "Domain-specific heuristics for automated improvement of PCM-based architectures\n", "abstract": " When designing a software architecture, a software architect faces multiple tradeoffs between non-functional requirements. This is due to the fact that some quality properties interfere with each other [RH06]. If we take performance and costs, for example, usually a faster CPU costs more than a slower CPU. So the architect has to decide whether this performance benefit is worth the extra costs. Another example is the allocation of components to hardware resources.", "num_citations": "3\n", "authors": ["637"]}
{"title": "Quality of Software Architectures Models and Architectures: 4th International Conference on the Quality of Software Architectures, QoSA 2008, Karlsruhe, Germany, October 14-17\u00a0\u2026\n", "abstract": " Models are used in all kinds of engineering disciplines to abstract from the various details of the modelled entity in order to focus on a specific aspect. Like a blueprint in civil engineering, a software architecture provides an abstraction from the full software system\u2019s complexity. It allows software designers to get an overview on the system under development and to analyze its properties. In this sense, models are the foundation needed for software development to become a true engineering discipline. Especially when reasoning on a software system\u2019s extra-functional properties, its software architecture carries the necessary information for early, design-time analyses. These analyses take the software architecture as input and can be used to direct the design process by allowing a systematic evaluation of different design alternatives. For example, they can be used to cancel out decisions which would lead to\u00a0\u2026", "num_citations": "3\n", "authors": ["637"]}
{"title": "Software industrialization and architecture certification\n", "abstract": " The industrialization of software development induces several changes to the development process as software development becomes distributed over company borders. They cooperatively develop individual components that are later assembled to software systems. This division of responsibilities requires a stricter quality assurance and in fact, creates a setting where the certification of software products becomes increasingly interesting. Until now, there are a few software product certification approaches, as in non-component-based software development processes, the considerable effort of software certification was only rarely justified. Therefore, existing certification approaches do not consider and support the requirements posed by industrialization, namely the separation of component development (by various providers) and system development. This paper presents a software certification approach which takes these requirements into account and allows certifying individual components as well as system architectures.", "num_citations": "3\n", "authors": ["637"]}
{"title": "User Manual of SKaMPI (Special Karlsruher MPI-Benchmark)\n", "abstract": " SKaMPI is the Special Karlsruher MPI-Benchmark. SKaMPI measures the performance of MPI-Implementations, and of course the used hardware. It performs various measurements of several MPI functions. The results are stored in a text le, from which a report can be generated automatically. The results are applicable for two things:", "num_citations": "3\n", "authors": ["637"]}
{"title": "A unified model to detect information flow and access control violations in software architectures\n", "abstract": " Software architectures allow identifying confidentiality issues early and in a cost-efficient way. Information Flow (IF) and Access Control (AC) are established confidentiality mechanisms, so modeling and analysis approaches should support them. Because confidentiality issues often trace back to data usage, data-oriented approaches are promising. However, we could not identify a data-oriented approach handling both, IF and AC. Therefore, we present a unified data-oriented modeling and analysis approach supporting both, IF and AC, within the same model in this paper. We demonstrate the integration into an existing architectural description language and evaluate the resulting expressiveness and accuracy by a case study considering 22 cases.", "num_citations": "2\n", "authors": ["637"]}
{"title": "Preface to the 1st Workshop on View-Oriented Software Engineering (VoSE)\n", "abstract": " ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,!,\",# $,%, &,',%,#,#,\",',%, & (,\",,),*,+,%,',,,#,\",,',,.,,/!,%,%, 0,#,', 1, 2,,,,,, 34, 5,,, 3,, 6,,,,,,,,, 7,,,,, 3,,,, 8, 9,,,, 3,,,,,, 38,,,,,,:, 9,,,,,,,,,,!,\",#;,#,,', &,%!,%,<=,),!,>,',,.(,\",,),*,?,!,\",>,,\",), 0,#@,%,,,',,),,#,, A,#, B, 0,%,,>,, &,., C,,,,,, 9,,, 4, 5,,, 3,, 6, D, 9,,,,, 7,,, 8,,, 9, E,,,,,,,, 2,,,,,,,,,!,\",# $,%, &,',%,#,#,\",',%, & (,\",,),*, F,!,\",>,,,,% G,,,',#,, H, I,.+,%,',,,#,\",,',,.,,,,,, D, 9,,, 4, 5,,, 3,, 6, 3,,,, 7,, 8, 9,,,,,,,,, D, 9,,, 8,,, J,,,, J,, 9,,,,,,,,,,,!,\",#;,#,,', &,%!,%,<=,),!,>,',,.(,\",,),*,?,!,\",>,,\",), 0,#@,%,,,',,),,#,, A,#, B, 0,%,,>,, &,., C,,,,,, 9,,, 4, 5,,, 3,, 6,,,,, 8,,, 9,,,,,, 7,,, 8,,, 9,,,,,,,,,,,,,,,,,,!,\",# $,%, &,',%,#,#,\",',%, & (,\",,),*, F,!,\",>,,,,% G,,,',#,, H, I,.+,%,',,,#,\",,',,.,,,,,, D, 9,,, 4, 5,,, 3,, 6,,,,,,, 7,, 8, 9,,,,,,,,, D, 9,,, 8,,, K, J,, J, L, 2,,,,,,,,,,,, 6,,,, 3,,,,,,, M,, 3,,, N,,,, M,, 3,,,,,,,, O,,,, M,,,,,,,,,,,,, 9,, M,,,,,,,,, 6,,, D,, 6,,,,,,,,,,,,,,,,,,, O,, M,,,, D,,,,,,,,,,,,,,,,,,,,,,,, 3,,,,,,,,,,,,, M,,,,,,,,,, 3,,,, 8,, 8,, 6,,,, 3,,,,, 8, P,,,,,,,,, 4,,,,,,,, O,,,,,, 6,,,,,,,,,,,, 9,,,,,,,,,,,,,,,,, 8,, 8,,,, M,, Q, M,,,,,,, 4, 3,,,,, 4,,,,,, 3,, M, 8,,,, 6,,, M,,,, 69,,,,,, O,,,,, 9,,,,,,,,,,,,,,,,,, O,,,,, 3\u00a0\u2026", "num_citations": "2\n", "authors": ["637"]}
{"title": "Maintaining security in software evolution\n", "abstract": " The engineering of security-critical software systems faces special challenges regarding evolution. Even if a substantial effort went into ensuring security during the system\u2019s initial development, it is uncertain if the system remains secure when changes to the software, the execution platform, or the system environment occur. Relevant changes that might endanger security include new or evolving system requirements, changing laws, or updated knowledge regarding attacks and mitigations. Failure to keep up with such changes can lead to substantial breaches and losses, highlighting the need to actively maintain an established level of security [And08].For preserving security in long-living systems, ongoing and systematic support for the evolution of knowledge and software is required. Reflecting the guiding theme Methods and processes for evolution of the priority program, there is a need for techniques, tools, and processes to support the evolution of systems in order to ensure lifelong compliance with security requirements. These techniques, tools, and processes need to address two main challenges, as outlined in the chapter \u201cChallenges\u201d of this book:", "num_citations": "2\n", "authors": ["637"]}
{"title": "Supplementary material for the evaluation of the layered reference architecture for metamodels to tailor quality modeling and analysis\n", "abstract": " This technical report contains supplementary information to the evaluation of the Layered Reference Architecture for Metamodels. Chapter 2 provides detailed descriptions of the case study metamodels (original and modular version). Chapter 3 provides installation instructions for the evaluation tool. Section 4.1 presents the evolution scenarios in detail. Section 4.2 contains information about the models. All metamodels, Modular EMF Designer diagrams, the evaluation tool, as well as the input and output data can be found online: https://github. com/kit-sdq/Metamodel-Reference-Architecture-Validation", "num_citations": "2\n", "authors": ["637"]}
{"title": "An Industry 4.0 Case Study: The Integration of CoCoME and xPPU\n", "abstract": " 2.1 Empirical approaches and community case studies................ 9 2.1. 1 CoCoME................................... 10 2.1. 2 xPPU..................................... 12 2.2 Contributions in Industry 4.0............................ 13 2.2. 1 MyJoghurt.................................. 13", "num_citations": "2\n", "authors": ["637"]}
{"title": "Ecoreification: making arbitrary Java code accessible to metamodel-based tools\n", "abstract": " Models are used in software engineering to describe parts of a system that are relevant for the computation of specific analyses, or the provision of specific functionality. Metamodeling languages such as Ecore make it possible to realize analyses and functionality with model-driven technology, such as transformation engines. If models conform to a metamodel that was expressed using Ecore, numerous Eclipse-based tools can be reused to directly analyze, display, or transform models. In many software projects, models are, however, realized with objects of plain-old Java classes rather than an explicit metamodel, so these popular toolscannot be used.In this new ideas paper, we present an Ecoreification approach, which can be used to automatically extract Ecore-conforming metamodels from Java code, and a code generator that combines the benefits of both worlds. The resulting code can be used exactly as\u00a0\u2026", "num_citations": "2\n", "authors": ["637"]}
{"title": "Praxis der Forschung: Eine Lehrveranstaltung des forschungsnahen Lehrens und Lernens in der Informatik am KIT\n", "abstract": " Der neue Lehrveranstaltungstyp Praxis der Forschung wurde 2012 im Master-Studiengang Informatik des Karlsruher Instituts f\u00fcr Technologie (KIT) eingef\u00fchrt. Zentrales Konzept dieser Veranstaltung ist das forschungsnahe Lehren und Lernen: Studierende erwerben im Rahmen eines eigenen Forschungsprojekts sowohl Fachwissen als auch methodische Kompetenz zu wissenschaftlicher Arbeit. Die konkrete Ausgestaltung folgt den Grunds\u00e4tzen der Forschungsn\u00e4he und der integrierten Vermittlung methodischer Kompetenzen. Die Studierenden sollen insbesondere auch erfahren, dass es ein wesentlicher Aspekt der wissenschaftlichen Arbeit ist, Forschungsergebnisse sicht-und wahrnehmbar zu machen.", "num_citations": "2\n", "authors": ["637"]}
{"title": "Migration eines Altsystems zu einer Java-Enterprise-Architektur\n", "abstract": " In diesem Kapitel berichten wir \u00fcber Erfahrungen mit der Migration kommunaler Informationssysteme hin zu einer SOA. In einer Kooperation zwischen der Forschung und einem kommunalen IT-Dienstleister wurde eine zukunftsf\u00e4hige Software-Architektur erarbeitet, die aktuellen Standards entspricht und die die speziellen Randbedingungen kommunaler Verwaltungen ber\u00fccksichtigt. Die selbst entwickelten Produkte des Software-Hauses sind bei ca. 200 kommunalen Kunden im Einsatz, haben lange und kostenintensive Phasen der qualitativen Stabilisierung und Funktionsanreicherung (was wir mit IQTF f\u00fcr Increasing Quality Times Functionality abk\u00fcrzen) hinter sich und erfordern st\u00e4ndige Pflege, h\u00e4ufig bedingt durch \u00c4nderungen gesetzgeberischer Verwaltungsvorgaben. Bei der Einf\u00fchrung einer neuen Software-Architektur in das Produktportfolio darf der Aufwand nicht untersch\u00e4tzt werden. Insbesondere ist ein Konzept f\u00fcr eine Architekturmigration notwendig. Unsere Erfahrungen f\u00fchrten zu dem in Kapitel 10 vorgestellten Dublo-Muster. Dieses Beispielkapitel beschreibt den Rahmen des Projektes, das zur Beschreibung des Musters gef\u00fchrt hat. Bei der Umsetzung des Dublo-Architekturmusters mit den vorhandenen, erfahrenen Software-Entwicklern entstanden neue Probleme und Verz\u00f6gerungen, deren Ursachen h\u00e4ufig im nicht technischen Bereich lagen und die man bei Beachtung bestimmter organisatorischer, psychologischer und betriebswirtschaftlicher Aspekte h\u00e4tte verhindern k\u00f6nnen. Aus den gemachten Erfahrungen werden diese Aspekte isoliert und aufgez\u00e4hlt. Ein derartiger interdisziplin\u00e4rer Ansatz, der bereits bei Definition und\u00a0\u2026", "num_citations": "2\n", "authors": ["637"]}
{"title": "Basic and dependent metrics\n", "abstract": " Besides the distinction between empirical and analytical metrics, metrics can also be distinguished whether they are basic or dependent.", "num_citations": "2\n", "authors": ["637"]}
{"title": "Migration der Architektur von Altsystemen\n", "abstract": " In Abschnitt 10.1 werden wir zun\u00e4chst allgemeine Aspekte der sanften Migration behandeln. Aus den im sp\u00e4teren Kapitel 24 berichteten Erfahrungen mit der Migration kommunaler Informationssysteme wurde das in Abschnitt 10.2 beschriebene Dublo-Muster [HRJ+04] abgeleitet. Unsere Erfahrungen werden durch die Beschreibung des zugrunde liegenden Musters verallgemeinert, sodass sie f\u00fcr \u00e4hnliche Aufgaben der Migration von Architekturen wiederverwendet werden k\u00f6nnen. Das Dublo-Muster basiert auf der teilweisen Duplikation der Gesch\u00e4ftslogik zwischen Altsystem und neuer Mittelschicht. Auch wenn dadurch zu einem gewissen Grad das Prinzip\u00bb Separation of Concerns \u00abverletzt wird, erh\u00e4lt", "num_citations": "2\n", "authors": ["637"]}
{"title": "Detecting violations of access control and information flow policies in data flow diagrams\n", "abstract": " The security of software-intensive systems is frequently attacked. High fines or loss in reputation are potential consequences of not maintaining confidentiality, which is an important security objective. Detecting confidentiality issues in early software designs enables cost-efficient fixes. A Data Flow Diagram (DFD) is a modeling notation, which focuses on essential, functional aspects of such early software designs. Existing confidentiality analyses on DFDs support either information flow control or access control, which are the most common confidentiality mechanisms. Combining both mechanisms can be beneficial but existing DFD analyses do not support this. This lack of expressiveness requires designers to switch modeling languages to consider both mechanisms, which can lead to inconsistencies. In this article, we present an extended DFD syntax that supports modeling both, information flow and access control\u00a0\u2026", "num_citations": "1\n", "authors": ["637"]}
{"title": "Design-Time Validation of Runtime Reconfiguration Strategies: An Environmental-Driven Approach\n", "abstract": " Validating the effectiveness of reconfiguration strategies of Self-Adaptive Systems (SAS) regarding their impact on runtime quality properties is a challenging problem at design time. Since quality properties, such as performance or reliability, are effectively observable at runtime, it is inherently difficult to validate reconfiguration strategies at design-time during their design (e.g., during the definition of the software architecture). Furthermore, engineering and validating SAS at design-time involves uncertainties that are difficult to manage due to a dynamic operating environment. Therefore, we propose a novel model-based analysis approach that is driven by a temporal probabilistic model which captures the stochastic nature of the operating environment. The sampled trajectories through the state space serve as a basis for validation. Software engineers benefit from the framework by validating their reconfiguration\u00a0\u2026", "num_citations": "1\n", "authors": ["637"]}
{"title": "A Formal Approach to Prove Compatibility in Transformation Networks\n", "abstract": " The increasing complexity of software and cyberphysical systems is handled by dividing the description of the system under construction into different models or views, each with an appropriate abstraction for the needs of specific roles. Since all such models describe the same system, they usually share an overlap of information, which can lead to inconsistencies if overlapping information is not modified uniformly in all models. A well-researched approach to make these overlaps explicit and resolve inconsistencies are incremental, bidirectional model transformations. They specify the constraints between two metamodels and the restoration of consistency between their instances. Relating more than two metamodels can be achieved by combining bidirectional transformations to a network. However, such a network may contain cycles of transformations, whose consistency constraints can be contradictory if they are not aligned with each other and thus cannot be fulfilled at the same time. Such transformations are considered incompatible. In this article, we provide a formal definition of consistency and compatibility of transformations and propose an inductive approach to prove compatibility of a given network of transformations. We prove correctness of the approach based on these formal definitions. Furthermore, we present an operationalization of the approach at the example of QVT-R. It detects contradictions between relations by transforming them into first-order logic formulae and evaluating them with an SMT solver. The approach operates conservatively, ie, it is not able to prove compatibility in all cases, but it identifies transformations as\u00a0\u2026", "num_citations": "1\n", "authors": ["637"]}
{"title": "TCP-Inspired Congestion Avoidance for Cloud-IoT Applications\n", "abstract": " Cloud-loT Applications consist of thousands of smart devices sending sensor data to processing cloud applications. If the processing rate of the cloud application is limited it may be unable to cope with an increasing number of connected devices. If such a situation is not addressed, the cloud application is overloaded with messages, resulting in a high processing delay or loss of data. For this reason we propose a TCP-inspired congestion avoidance which reconfigures the send rate of devices at runtime aiming for a low processing delay and a high throughput. We show, that it is able to avoid congestions by adapting the send rate of devices to a fair share of the processing rate of the cloud application.", "num_citations": "1\n", "authors": ["637"]}
{"title": "A Metamodel-Based Approach to Calculate Maintainability Task Lists of PLC Programs for Factory Automation\n", "abstract": " As long-living systems, automated Production Systems (aPS) have to be adapted due to optimization and inclusion of new features in their life cycle over decades. aPS consist of electrical, mechanical, and software components, which have a complex interaction and mutual dependencies. Consequently, these heterogeneous components have to be maintained together. Thus, the change propagation analysis in aPS is a challenging task. However, calculating the change propagation by hand is time-consuming a...\u00bb", "num_citations": "1\n", "authors": ["637"]}
{"title": "Automated Analysis of the Co-evolution of Software Systems and Business Processes\n", "abstract": " Software systems are an essential part of business processes. As business processes and the corresponding software systems mutually affect each other, they co-evolve during their life cycle. Thus, to adequately predict the impact of a change, their mutual dependencies have to be considered. However, existing approaches to change propagation analysis consider one domain in isolation and neglect the mutual dependencies between the domains. In this paper, we propose the Karlsruhe Architectural Maintainability Prediction for Business Processes (KAMP4BP) to analyze the change propagation in business processes and the corresponding software systems.", "num_citations": "1\n", "authors": ["637"]}
{"title": "SensIDL: Ein Werkzeug zur Vereinfachung der Schnittstellenimplementierung intelligenter Sensoren\n", "abstract": " {Die allgegenw {\\\" a} rtige mobile Nutzung des Internets sowie die zunehmende Integration von Kommunikationsf {\\\" a} higkeiten in Alltagsgegenst {\\\" a} nde sowohl im Heimbereich als auch im industriellen Umfeld, besser bekannt als das Internet der Dinge, f {\\\" u} hren zu einer zunehmenden Vernetzung verschiedenster Systeme. Im Heimbereich werden Fernseher, Smartphones, aber auch Licht-, Fenster-und Heizungssteuerungen, K {\\\" u} hlschr {\\\" a} nke und ganze Hausautomatisierungssysteme vernetzt. Im Industrieumfeld wird die Vernetzung als Teil der vierten industriellen Revolution stark intensiviert. Die Bandbreite der eingesetzten Systeme reicht von hochleistungsf {\\\" a} higen Server-und PC-Systemen {\\\" u} ber Cloud-Dienste und mobile Endger {\\\" a} te, wie Smartphones und Tablets, bis zu intelligenten eingebetteten mobilen oder station {\\\" a} ren heterogenen Sensorsystemen mit eingeschr {\\\" a} nkter Energieversorgung und begrenzten Rechenkapazit {\\\" a} ten.},", "num_citations": "1\n", "authors": ["637"]}
{"title": "Change Impact Analysis by Architecture-based Assessment and Planning\n", "abstract": " Software architecture presents the main artifact of software systems reflecting design decisions and thus influence their quality attributes. Furthermore, during software evolution each architecture decision also influences technical artifacts (e.g., test cases) and the corresponding orga- nizational responsibilities (e.g., tester). Thus, it is important to predict the impact of a change request (e.g., changing an interface) on the software architecture and other software artifacts for decision- making. Hence, a software architect can estimate the effort of the implementation of a change request due to corresponding implementation tasks. However, existing approaches are limited to artifacts of the software development process or do not use formal architecture descriptions. We present the Karlsruhe Architectural Maintainability Prediction (KAMP), that enables software architects to ana- lyze the propagation of change requests in software architecture models. Our approach is not limited to the technical operations but as well considers the organizational tasks. KAMP supports softwa- re architects by automatically generated task lists to implement changes. In an empirical study, we showed, that KAMP improves the scalability, precision, and completeness of change propagation analysis.", "num_citations": "1\n", "authors": ["637"]}
{"title": "Modellierung 2016\n", "abstract": " Modelle stellen ein der wichtiges Hilfsmittel zur Beherrschung komplexer Systeme dar. Die Themenbereiche der Entwicklung, Nutzung, Kommunikation und Verarbeitung von Modellen sind so vielf\u00e4ltig wie die Informatik mit all ihren Anwendungen.", "num_citations": "1\n", "authors": ["637"]}
{"title": "Towards Reliability Estimation of Large Systems-of-Systems with the Palladio Component Model\n", "abstract": " The component paradigm aims at accelerating the construction of large scale systems-of-systems by facilitating the integration of third-party components. The size and the complexity of such large software make the reliability assessment process challenging. The different components building such large systems can be developed and maintained by multiple parties. In addition, the reliability testing process should focus on the integration logic connecting the components. The standard approach is to perform integration testing to uncover interaction faults between the different components building the software. However, integration testing of large systems-of-systems is in the most of the cases intractable. The multiplicity of the potential interactions between the different subsystems can be hardly systematically tested. In addition, standard integration test cases cannot be used to estimate the reliability of the\u00a0\u2026", "num_citations": "1\n", "authors": ["637"]}
{"title": "Using queuing models for large system migration scenarios\u2013an industrial case study with IBM system z\n", "abstract": " Large IT organizations exchange their computer infrastructure on a regular time basis. When planning such an environment exchange, it is required to explicitly consider the impact on the Quality-of-Service of the applications to avoid violations of Service Level Agreements. In current practice, however, using explicit performance models for such estimations is frequently avoided due to scepticism towards their practical usability and benefits for complex environments. In this paper, we present a real-world case study to demonstrate that a queuing model-based approach can be effectively used to predict performance impact when migrating to a new environment in an industrial context. We first present a general modeling methodology and explain how we apply it for system migration scenarios. Then, we present a real-world industrial case study and show how the performance models can be used. The\u00a0\u2026", "num_citations": "1\n", "authors": ["637"]}
{"title": "Performance-Aware design of web application front-ends\n", "abstract": " The responsiveness of web applications directly affects customer satisfaction and, as a consequence, business-critical metrics like revenue and conversion rates. However, building web applications with low response times is a challenging task. The heterogeneity of browsers and client devices as well as the complexity of today\u2019s web applications lead to high development and test efforts. Measuring front-end performance requires a deep understanding of measurement tools and techniques as well as a lot of manual effort. With our approach, developers and designers can assess front-end performance for different scenarios without measuring. We use prediction models derived by a series of automated, systematic experiments to give early feedback about the expected performance. Our approach predicts the front-end performance of real-world web applications with an average error of 11% across all\u00a0\u2026", "num_citations": "1\n", "authors": ["637"]}
{"title": "Qualit\u00e4t von Gesch\u00e4ftsprozessen und Unternehmenssoftware\u2013Eine Thesensammlung\n", "abstract": " In diesem Positionspapier betrachten wir die Integration von Gesch\u00e4ftsprozess- und Unternehmenssoftware-Gestaltung unter dem Aspekt der Qualit\u00e4t. Wichtige Fragen sind dabei z.B. An welcher Stelle im Unternehmen und im Software-Entwicklungszyklus entstehen Qualit\u00e4tsanforderungen? Wie beeinflussen sich diese gegenseitig? Wie kann man sie am besten umsetzen? Wie kann man sie am besten \u00fcberpr\u00fcfen? Dieses Positionspapier beschreibt die aus Sicht der AutorInnen wichtigsten Herausforderungen bei dieser Integration.", "num_citations": "1\n", "authors": ["637"]}
{"title": "Introduction to overlapping attributes\n", "abstract": " Research on software quality attributes is often focused on one attribute. For example, the software reliability community is rarely concerned with memory footprint and researchers dealing with performance are seldom interested in availability metrics. This leads to metrics and prediction models focusing on a single attribute.", "num_citations": "1\n", "authors": ["637"]}
{"title": "Quality of Software Architectures\n", "abstract": " Although the quality of a system\u2019s software architecture is one of the critical factors in its overall quality, the architecture is simply a means to an end, the end being the implemented system. Thus the ultimate measure of the quality of the software architecture lies in the implemented system, in how well it satisfies the system and project requirements and constraints and whether it can be maintained and evolved successfully. In order to treat design as a science rather than an art, we need to be able to address the quality of the software architecture directly, not simply as it is reflected in the implemented system. Therefore, QoSA is concerned with software architecture quality directly by addressing the problems of:", "num_citations": "1\n", "authors": ["637"]}
{"title": "Migration von Altsystemen zu dienstorientierten Architekturen\n", "abstract": " Die KDO (Zweckverband Kommunale Datenverarbeitung Oldenburg) ist ein erfolgreicher IT-Dienstleister, der dom\u00e4nenspezifische Software-L\u00f6sungen f\u00fcr kommunale Verwaltungen anbietet (http://www. kdo. de). Bisher basieren die Client-Server-L\u00f6sungen der KDO haupts\u00e4chlich auf Informix-Datenbanken mit Informix 4GL [IBM] und dem 4Js-Laufzeitsystem (serverseitig) inklusive dem 4Js-Windows-Frontend f\u00fcr Clients [Fou]. Die L\u00f6sungen der KDO k\u00f6nnen wahlweise dezentral oder zentral mit der KDO als ASP betrieben werden. Die KDO entschied sich, von der (monolithischen) Zwei-Schichten-Architektur hin zu einer standardbasierten und zukunftsorientierten Multi-Schichten-Architektur zu migrieren, die den Einsatz moderner Software-Engineering-Methoden erm\u00f6glicht. Bei der Ver\u00e4nderung von traditioneller hin zu komponentenbasierter Software-Entwicklung galt es, besonders auf die sich ergebenden Risiken und neuen Herausforderungen zu achten [RR03, Vit03]. Um den \u00dcbergang zu neuen Technologien zu bew\u00e4ltigen, entschied sich die KDO f\u00fcr eine Zusammenarbeit mit OFFIS, dem Oldenburger Forschungsund Entwicklungsinstitut f\u00fcr Informatik-Werkzeuge und-systeme. OFFIS ist ein An-Institut des Department f\u00fcr Informatik der Universit\u00e4t Oldenburg (http://www. offis. de). Die hier vorgestellte Arbeit ist das Ergebnis gemeinsamer Anstrengungen der Software-Engineering-Gruppe der Universit\u00e4t, der Abteilung Betriebliches Informations-und Wissensmanagement von OFFIS und der KDO. Die Kooperation wird begleitet von einer Schulung der KDO-Mitarbeiter in objektorientierter Modellierung und Enterprise-Java-Technologien, die\u00a0\u2026", "num_citations": "1\n", "authors": ["637"]}
{"title": "Architects for buildings and software\n", "abstract": " In software engineering and especially in software architecture often terms from other scientific and engineering disciplines are used. This paper describes the analogy between the role of an building architect and a software architect considering the similarities as well as the differences regarding their tasks in a building and a software developing project.. Topics such as specialization and education are considered as well.", "num_citations": "1\n", "authors": ["637"]}
{"title": "Formal Foundations of Embedded Software and Component-Based Software Architectures (FESCA) Workshop\n", "abstract": " Formal Foundations of Embedded Software and Component-Based Software Architectures (FESCA) Workshop - Research Portal, King's College, London King's College London King's main site Research portal Home Researchers Research Groups Research Outputs Research Funding Internal Research Outputs Theses . Journals Publishers Formal Foundations of Embedded Software and Component-Based Software Architectures (FESCA) Workshop Research output: Contribution to journal \u203a Editorial \u203a peer-review Juliana K\u00fcster-Filipe (Editor), Iman Poernomo (Editor), Ralf Reussner (Editor), Sandeep Shukla (Editor) Overview Citation formats Original language English Journal Electronic Notes in Theoretical Computer Science Volume 108 Published 2004 King's Authors Iman Poernomo (Editor) (Informatics) Research Groups King's College London Post to Twitter Post to FaceBook Post to Digg View graph of \u2026", "num_citations": "1\n", "authors": ["637"]}
{"title": "Model-based software reuse\n", "abstract": " This report summarises the presentations and discussions of the First Workshop on Model-based Software Reuse, held in conjunction with the 16th European Conference on Object-Oriented Programming (ECOOP) Malaga, Spain June 10, 2002. This workshop was motivated by the observation that convenient models are very useful to understand the mechanisms of reuse. Models may help to define the interoperability between components, to detect feature interaction and to increase the traceability. They have the potential to define the essential aspects of the compositionality of the assets (i.e., components, aspects, views, etc.). Common problems discussed were how to reason about the properties of composed assets (with the focus on invalid asset compositions detection) and how to model assets to enable such reasoning. Eleven papers have been accepted and presented. Discussion groups in the\u00a0\u2026", "num_citations": "1\n", "authors": ["637"]}
{"title": "Counter-constrained Finite State Machines: Modelling Component Protocols with Resource-dependencies\n", "abstract": " This report deals with the specication of software component protocols (ie, the set of service call sequences). The contribution of this report is twofold:(a) We discuss specic requirements of real-world protocols, especially in the presence of components wich make use of limited resources.(b) We dene counter-constrained nite state machines (CC-FSMs), a novel extension of nite state machines, specifically created to model protocols having dependencies between services due to their access to shared resources. We provide a theoretical framework for reasoning and analysing CC-FSMs. Opposed to nite state machines and other approaches, CC-FSMs combine two valuable properties:(a) CC-FSMs are powerful enough to model realistic component protocols with resource allocation, usage, and de-allocation dependencies between methods (as occurring in common abstract datatypes such as stacks or queues) and\u00a0\u2026", "num_citations": "1\n", "authors": ["637"]}
{"title": "Recent Advances of SKaMPI\n", "abstract": " The goal of SKaMPI is the creation of a database containing performance measurements of parallel computers in terms of MPI. This data supports software developers in creating portable and fast programs. To meet this goal port ability is a crucial property of our this benchmark. A large number of platforms to test on and to examine is important for the realization of our vision. Since access to the Stuttgart Cray T3E in October 1998 we were able to improve the code\u2019s portability, to solve memory alignment problems, to compare the quality Cray\u2019s implemented MPI gather algorithm to the one used in IBM\u2019s SP MPI library, and to investigate the blocking behavior of collective MPI operations. Recently we added the parameterization of measurements suites with (even user defined) datatypes.", "num_citations": "1\n", "authors": ["637"]}