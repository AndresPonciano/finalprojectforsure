{"title": "An empirical study of the bad smells and class error probability in the post-release object-oriented system evolution\n", "abstract": " Bad smells are used as a means to identify problematic classes in object-oriented systems for refactoring. The belief that the bad smells are linked with problematic classes is largely based on previous metric research results. Although there is a plethora of empirical studies linking software metrics to errors and error proneness of classes in object-oriented systems, the link between the bad smells and class error probability in the evolution of object-oriented systems after the systems are released has not been explored. There has been no empirical evidence linking the bad smells with class error probability so far. This paper presents the results from an empirical study that investigated the relationship between the bad smells and class error probability in three error-severity levels in an industrial-strength open source system. Our research, which was conducted in the context of the post-release system evolution\u00a0\u2026", "num_citations": "265\n", "authors": ["90"]}
{"title": "The effectiveness of software metrics in identifying error-prone classes in post-release software evolution process\n", "abstract": " Many empirical studies have found that software metrics can predict class error proneness and the prediction can be used to accurately group error-prone classes. Recent empirical studies have used open source systems. These studies, however, focused on the relationship between software metrics and class error proneness during the development phase of software projects. Whether software metrics can still predict class error proneness in a system\u2019s post-release evolution is still a question to be answered. This study examined three releases of the Eclipse project and found that although some metrics can still predict class error proneness in three error-severity categories, the accuracy of the prediction decreased from release to release. Furthermore, we found that the prediction cannot be used to build a metrics model to identify error-prone classes with acceptable accuracy. These findings suggest that as a\u00a0\u2026", "num_citations": "206\n", "authors": ["90"]}
{"title": "A quantitative investigation of the acceptable risk levels of object-oriented metrics in open-source systems\n", "abstract": " Object-oriented metrics have been validated empirically as measures of design complexity. These metrics can be used to mitigate potential problems in the software complexity. However, there are few studies that were conducted to formulate the guidelines, represented as threshold values, to interpret the complexity of the software design using metrics. Classes can be clustered into low and high risk levels using threshold values. In this paper, we use a statistical model, derived from the logistic regression, to identify threshold values for the Chidamber and Kemerer (CK) metrics. The methodology is validated empirically on a large open-source system-the Eclipse project. The empirical results indicate that the CK metrics have threshold effects at various risk levels. We have validated the use of these thresholds on the next release of the Eclipse project-Version 2.1-using decision trees. In addition, the selected\u00a0\u2026", "num_citations": "164\n", "authors": ["90"]}
{"title": "Finding software metrics threshold values using ROC curves\n", "abstract": " An empirical study of the relationship between object\u2010oriented (OO) metrics and error\u2010severity categories is presented. The focus of the study is to identify threshold values of software metrics using receiver operating characteristic curves. The study used the three releases of the Eclipse project and found threshold values for some OO metrics that separated no\u2010error classes from classes that had high\u2010impact errors. Although these thresholds cannot predict whether a class will definitely have errors in the future, they can provide a more scientific method to assess class error proneness and can be used by engineers easily. Copyright \u00a9 2009 John Wiley & Sons, Ltd.", "num_citations": "144\n", "authors": ["90"]}
{"title": "An empirical assessment of refactoring impact on software quality using a hierarchical quality model\n", "abstract": " Software refactoring is a collection of reengineering activities that aims to improve software quality. Refactorings are commonly used in agile software processes to improve software quality after a significant software development or evolution. There is belief that refactoring improves quality factors such as understandability, flexibility, and reusability. However, there is limited empirical evidence to support such assumptions. The aim of this study is to confirm such claims using a hierarchal quality model. We study the effect of software refactoring on software quality. We provide details of our findings as heuristics that can help software developers make more informed decisions about what refactorings to perform in regard to improve a particular quality factor. We validate the proposed heuristics in an empirical setting on two open-source systems. We found that the majority of refactoring heuristics do improve quality; however some heuristics do not have a positive impact on all software quality factors. In addition, we found that the impact analysis of refactorings divides software measures into two categories: high and low impacted measures. These categories help in the endeavor to know the best measures that can be used to identify refactoring candidates. We validated our findings on two open-source systems\u2014Eclipse and Struts. For both systems, we found consistency between the heuristics and the actual refactorings.", "num_citations": "65\n", "authors": ["90"]}
{"title": "Improving software fault-prediction for imbalanced data\n", "abstract": " Fault-proneness has been studied extensively as a quality factor. The prediction of fault-proneness of software modules can help software engineers to plan evolutions of the system. This plan can be compromised in case prediction models are biased or do not have high prediction performance. One major issue that can impact the prediction performance is the fault distributions such as the data imbalance, i.e., the majority of modules are faultless whereas the minority of modules is only faulty. In this paper, we propose to use the fault content (i.e., the number of faults in a module) to oversample the minority. We applied this technique on a large object-oriented system - Eclipse. The proposed oversampling is tested on three classifiers. The results have shown a better prediction performance than other traditional oversampling techniques. The oversampling technique is more convenient than other sampling\u00a0\u2026", "num_citations": "46\n", "authors": ["90"]}
{"title": "Deriving metrics thresholds using log transformation\n", "abstract": " Software metrics are surrogates of many software quality factors such as fault proneness, reusability, and maintenance effort. Software metrics are numbers collected from software code to assess and evaluate where problems are more probable to happen. These numbers are used to flag warnings of the problematic parts of software code using threshold values. However, the proposed techniques did not consider the data distribution and skewness in data. In this research, we aim to propose a methodology based on log transformation to improve the metrics quality. To explore the effect of log transformation on data analysis, we conduct analysis of using software metrics after transformation in identifying fault\u2010prone areas on multireleases of 11 products (41 releases). The results show that the log transformation can be used to derive threshold values for all metrics under investigation. The results of the\u00a0\u2026", "num_citations": "40\n", "authors": ["90"]}
{"title": "An investigation of bad smells in object-oriented design\n", "abstract": " Bad smells are used to identify problematic classes in object-oriented design. Although intuitively making sense, the promise that bad smells can indicate the quality of design has not been validated by empirical evidence. This paper presents the results from an investigation that explored the relationship between the bad smells and the errors in an object-oriented system. The investigation found that some bad smells are positively associated with class errors", "num_citations": "32\n", "authors": ["90"]}
{"title": "The application of ROC analysis in threshold identification, data imbalance and metrics selection for software fault prediction\n", "abstract": " Software engineers have limited resources and need metrics analysis tools to investigate software quality such as fault-proneness of modules. There are a large number of software metrics available to investigate quality. However, not all metrics are strongly correlated with faults. In addition, software fault data are imbalanced and affect quality assessment tools such as fault prediction or threshold values that are used to identify risky modules. Software quality is investigated for three purposes. First, the receiver operating characteristics (ROC) analysis is used to identify threshold values to identify risky modules. Second, the ROC analysis is investigated for imbalanced data. Third, the ROC analysis is considered for feature selection. This work validated the use of ROC to identify thresholds for four metrics (WMC, CBO, RFC and LCOM). The ROC results after sampling the data are not significantly different from\u00a0\u2026", "num_citations": "28\n", "authors": ["90"]}
{"title": "An empirical study of the effect of power law distribution on the interpretation of oo metrics\n", "abstract": " Context. Software metrics are surrogates of software quality. Software metrics can be used to find possible problems or chances for improvements in software quality. However, software metrics are numbers that are not easy to interpret. Previous analysis of software metrics has shown fat tails in the distribution. The skewness and fat tails of such data are properties of many statistical distributions and more importantly the phenomena of the power law. These statistical properties affect the interpretation of software quality metrics. Objectives. The objective of this research is to validate the effect of power laws on the interpretation of software metrics. Method. To investigate the effect of power law properties on software quality, we study five open-source systems to investigate the distribution and their effect on fault prediction models. Results. Study shows that power law behavior has an effect on the interpretation and usage of software metrics and in particular the CK metrics. Many metrics have shown a power law behavior. Threshold values are derived from the properties of the power law distribution when applied to open-source systems. Conclusion. The properties of a power law distribution can be effective in improving the fault-proneness models by setting reasonable threshold values.", "num_citations": "23\n", "authors": ["90"]}
{"title": "Medical volume segmentation using 3d multiresolution analysis\n", "abstract": " 3D volume segmentation aims at partitioning the voxels into 3D objects (sub-volumes) which represent meaningful physical entities. Multi-resolution analysis (MRA) allows for the preservation of an image according to certain levels of resolution or blurring. The quality of this approach makes it useful in image compression, de-noising, and classification or segmentation. This paper focuses on the implementation of a medical volume segmentation technique using 3D discrete wavelet transform (DWT). A comparison study has been carried out to evaluate 2D and 3D techniques which reveal that 3D approaches can accurately detect the region of interest in both phantom and real data.", "num_citations": "21\n", "authors": ["90"]}
{"title": "Building a smart academic advising system using association rule mining\n", "abstract": " In an academic environment, student advising is considered a paramount activity for both advisors and student to improve the academic performance of students. In universities of large numbers of students, advising is a time-consuming activity that may take a considerable effort of advisors and university administration in guiding students to complete their registration successfully and efficiently. Current systems are traditional and depend greatly on the effort of the advisor to find the best selection of courses to improve students performance. There is a need for a smart system that can advise a large number of students every semester. In this paper, we propose a smart system that uses association rule mining to help both students and advisors in selecting and prioritizing courses. The system helps students to improve their performance by suggesting courses that meet their current needs and at the same time improve their academic performance. The system uses association rule mining to find associations between courses that have been registered by students in many previous semesters. The system successfully generates a list of association rules that guide a particular student to select courses registered by similar students.", "num_citations": "20\n", "authors": ["90"]}
{"title": "An investigation of ck metrics thresholds\n", "abstract": " There is a dearth of studies that identified thresholds values of Chidamber and Kemerer (CK) metrics. In an empirical study on open-source software, Eclipse project\u2014Version 3.0, we identified the thresholds values for CBO, RFC and WMC at two levels of risks using a quantitative methodology based on the logistic regression curve. These threshold values can be used to identify the most error-prone classes.", "num_citations": "12\n", "authors": ["90"]}
{"title": "Empirical study of fault prediction for open-source systems using the Chidamber and Kemerer metrics\n", "abstract": " Software testers are usually provoked with projects that have faults. Predicting a class's fault-proneness is vital for minimising cost and improving the effectiveness of the software testing. Previous research on software metrics has shown strong relationships between software metrics and faults in object-oriented systems using a binary variable. However, these models do not consider the history of faults in classes. In this work, a dependent variable is proposed that uses fault history to rate classes into four categories (none, low risk, medium risk and high risk) and to improve the predictive capability of fault models. The study is conducted on many releases of four open-source systems. The study tests the statistical differences in seven machine learning algorithms to find whether the proposed variable can be used to build better prediction models. The performance of the classifiers using the four categories is\u00a0\u2026", "num_citations": "11\n", "authors": ["90"]}
{"title": "Generating a Language-Independent Graphical User Interfaces from UML Models\n", "abstract": " The cost of the software development is high and there is a need to automate parts or all activities of the software development to reduce the development costs. In this work, the User Interface (UI) design is automated and UIs are generated for language-independent code from Unified Modeling Language (UML) diagrams. These diagrams are used to generate both the content of the UIs and the navigation through the use interfaces. Based on end-user feedback, the UML diagrams and the UI prototype can be iteratively refined. To demonstrate this work, a tool that automates the generation of UI prototype is built. The tool generates a prototype that is coded using an eXtensable Markup Language (XML) called the UI Markup Language (UIML). The proposed approach is validated and UIs are generated for two case studies.", "num_citations": "7\n", "authors": ["90"]}
{"title": "Predicting Error Probability in the Eclipse Project.\n", "abstract": " Object-oriented software metrics have been shown to be able to predict various software quality factors. This paper investigated whether the metrics could predict class error probability and whether the predicted probability could group classes in the object-oriented design. We found, in an open source system, that a set of object-oriented metrics could predict class error probability and the probability could be used to group classes into error and no-error categories with reasonable accuracies.", "num_citations": "7\n", "authors": ["90"]}
{"title": "A verification of the correspondence between design and implementation quality attributes using a hierarchal quality model\n", "abstract": " software verification is very important activity to prolong software quality. Many software systems deviate from their design when implemented. Typically, software engineers expect a high correspondence between design and implementation artifacts to ensure the quality of the final product. In this paper, we validate the use of a quality model to verify the correspondence between the artifacts of a software design and implementation. The model uses software metrics to measure the differences between the design graphical models (UML diagrams) and the source code for three external quality attributes: reusability, extendibility and understandability. The significance of the differences is verified using inferential and descriptive statistical tests. The proposed model is validated on a real open-source system that was developed in C++. The proposed model can be used to investigate the differences in a software quality either at the system or component levels. Many differences in quality attributes have been identified in the case study. The correspondence model has shown many characteristics; it is flexible, extendible and accepts different forms of design (UML diagrams) and code notations.", "num_citations": "5\n", "authors": ["90"]}
{"title": "Classification of application reviews into software maintenance tasks using data mining techniques\n", "abstract": " Mobile application reviews are considered a rich source of information for software engineers to provide a general understanding of user requirements and technical feedback to avoid main programming issues. Previous researches have used traditional data mining techniques to classify user reviews into several software maintenance tasks. In this paper, we aim to use associative classification (AC) algorithms to investigate the performance of different classifiers to classify reviews into several software maintenance tasks. Also, we proposed a new AC approach for review mining (ACRM). Review classification needs preprocessing steps to apply natural language preprocessing and text analysis. Also, we studied the influence of two feature selection techniques (information gain and chi-square) on classifiers. Association rules give a better understanding of users\u2019 intent since they discover the hidden patterns in\u00a0\u2026", "num_citations": "2\n", "authors": ["90"]}
{"title": "Exploring trends in the evolution of open-source systems\n", "abstract": " Software evolution is the costliest process in software project. Successful software projects tend to evolve longer for high quality software. To keep the software quality under control, software engineers need to know the trends in software growth to help in allocating appropriate resources in future releases. How does software evolve and in what pace is very important to understand software evolution? Knowing the evolution of software as a whole is not enough to make decisions. Software engineers need to understand the class evolution in object-oriented systems. The evolution of classes in five open-source systems are empirically studied using the growth rate using linear and nonlinear models. The work analyzes the evolution of classes for logarithmic, exponential and quadratic models. The results show that that most classes follow the logarithmic and quadratic models. While the linear model was the best fit in\u00a0\u2026", "num_citations": "2\n", "authors": ["90"]}
{"title": "Synergies and Conflicts among Software Quality Attributes and Bug Fixes\n", "abstract": " Quality has great effect on acceptance and behaviour of software products under operation. Quality has many perspectives for users, engineers and managers. These perspectives are represented formally in a quality model that is composed of both external and internal measures. External quality attributes can be measured indirectly using internal properties of the software product. Many external attributes are of interest to manager of software production and are combined in a framework or a quality model. However, these attributes may have interrelationships, positive or negative (conflicts). In addition, external quality attributes may have relationships with bugs in a component or system. In this research, we aim to find empirical evidence of the interrelationships among six quality attributes (reusability, flexibility, understandability, functionality, extendibility and effectiveness) and with bugs. We also demonstrate the\u00a0\u2026", "num_citations": "2\n", "authors": ["90"]}
{"title": "An empirical investigation of predicting fault count, fix cost and effort using software metrics\n", "abstract": " Software fault prediction is important in software engineering field. Fault prediction helps engineers manage their efforts by identifying the most complex parts of the software where errors concentrate. Researchers usually study the faultproneness in modules because most modules have zero faults, and a minority have the most faults in a system. In this study, we present methods and models for the prediction of fault-count, fault-fix cost, and fault-fix effort and compare the effectiveness of different prediction models. This research proposes using a set of procedural metrics to predict three fault measures: fault count, fix cost and fix effort. Five regression models are used to predict the three fault measures. The study reports on three data sets published by NASA. The models for each fault are evaluated using the Root Mean Square Error. A comparison amongst fault measures is conducted using the Relative Absolute Error. The models show promising results to provide a practical guide to help software engineers in allocating resources during software testing and maintenance. The cost fix models show equal or better performance than fault count and effort models.", "num_citations": "2\n", "authors": ["90"]}
{"title": "The validation and threshold values of object-oriented metrics\n", "abstract": " Controlling software quality has always been one of the goals in software engineering. Software design metrics provide a quantitative means to measure software design quality and may be the basis of techniques that can help control software quality. There have been a plethora of studies on software metrics in both the procedural and object-oriented (OO) paradigms [1][3][4][6][7][11][14-17][19][22-25][30][36][37][41][46][47][54][56][58-63][66][74]. We often use software metrics to predict the behavior of a system or the components of a system. In the OO paradigm, empirical studies have shown that metrics could predict quality factors such as class error proneness [7][15-17][22][29][74], maintenance effort [3][11][37][58][81], maintenance performance [6][21][43][52], design effort [24][44][65] and project progress [4][40][60]. However, the data used in these studies were collected during the development phases of the systems. As systems continue to evolve after their release, significant amounts of resources must be dedicated to maintain the quality of the systems as they evolve. It is not at all clear whether the predictive capabilities of the metrics still exist in the post-release evolution of a system. One study conducted by Alshayeb and Li [3] raised doubt that the metrics\u2019 predictive capabilities would carry through the post-", "num_citations": "2\n", "authors": ["90"]}
{"title": "A Student Advising System Using Association Rule Mining\n", "abstract": " Academic advising is a time-consuming activity that takes a considerable effort in guiding students to improve student performance. Traditional advising systems depend greatly on the effort of the advisor to find the best selection of courses to improve student performance in the next semester. There is a need to know the associations and patterns among course registration. Finding associations among courses can guide and direct students in selecting the appropriate courses that leads to performance improvement. In this paper, the authors propose to use association rule mining to help both students and advisors in selecting and prioritizing courses. Association rules find dependences among courses that help students in selecting courses based on their performance in previous courses. The association rule mining is conducted on thousands of student records to find associations between courses that have been\u00a0\u2026", "num_citations": "1\n", "authors": ["90"]}
{"title": "Identifying threshold values of change-prone modules\n", "abstract": " Software changes frequently during the lifetime of a project. These changes increase the total cost of software development. Knowing where changes are more likely help in allocating appropriate resources in different activities of software lifecycle. The change-proneness of modules is defined as likelihood of change in a module. Change-proneness can be predicted using software metrics such as Chidamber and Kemerer metrics. However, the use of prediction models requires knowledge in advanced techniques such as data mining and regression analysis. Hence, there is a need to a more direct and simple technique to have more information about change-prone modules. Threshold values are proposed as such technique to identify the modules that are more change-prone. We propose the ROC analysis to identify thresholds. The ROC analysis suggests that large values of software metrics are indicators of\u00a0\u2026", "num_citations": "1\n", "authors": ["90"]}
{"title": "Identifying and eliminating less complex instances from software fault data\n", "abstract": " Software quality is costly to achieve for large systems. Developers and testers need to investigate a large number of software modules to ensure software quality. This investigation is time consuming and formal models such as machine learning techniques are used to predict the fault-proneness of software modules to predict where faults are more probable. However, many modules are small in either implementation or design size and have low priority to investigate their quality. In this research, machine learners are used to classify the most complex parts of software. The less complex software modules are filtered out using two size measures: number of public parameters in a software construct and line of code (LOC) as a surrogate of implementation size. Modules at lowest 10, 20 and 30% of each measure are filtered out of the trained and tested data sets. The remaining modules are used to build four\u00a0\u2026", "num_citations": "1\n", "authors": ["90"]}
{"title": "A Graph-Based Representation of Object-Oriented Designs.\n", "abstract": " This article proposes a general graph-based representation for modeling Object Oriented (OO) software designs. The advantages of the representation over the Unified Modeling Language (UML) are: it formalizes the link between primitive OO design features and primitive entities of graphs; it models all the design features of an OO design in one cohesive graph; it provides us a convenient platform for formal analysis and reasoning. Thus, the representation allows us to investigate various properties of OO designs, through their graph representations. These properties include software metrics, link predications and small world phenomenon etc.", "num_citations": "1\n", "authors": ["90"]}
{"title": "A comparison of modern object-oriented programming languages and compilers using standard gang of four design patterns\n", "abstract": " The design patterns from the widely used Gang of Four book provide flexible, reusable designs that can be employed in many disparate application domains. Since these patterns employ the object-oriented techniques in standard ways, they can serve as a good mechanism for comparing different object-oriented languages, and for comparing the compilers for these languages.", "num_citations": "1\n", "authors": ["90"]}