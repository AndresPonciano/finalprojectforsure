{"title": "Automatically improving accuracy for floating point expressions\n", "abstract": " Scientific and engineering applications depend on floating point arithmetic to approximate real arithmetic. This approximation introduces rounding error, which can accumulate to produce unacceptable results. While the numerical methods literature provides techniques to mitigate rounding error, applying these techniques requires manually rearranging expressions and understanding the finer details of floating point arithmetic. We introduce Herbie, a tool which automatically discovers the rewrites experts perform to improve accuracy. Herbie's heuristic search estimates and localizes rounding error using sampled points (rather than static error analysis), applies a database of rules to generate improvements, takes series expansions, and combines improvements for different input regions. We evaluated Herbie on examples from a classic numerical methods textbook, and found that Herbie was able to improve\u00a0\u2026", "num_citations": "172\n", "authors": ["1712"]}
{"title": "Planning for change in a formal verification of the Raft consensus protocol\n", "abstract": " We present the first formal verification of state machine safety for the Raft consensus protocol, a critical component of many distributed systems. We connected our proof to previous work to establish an end-to-end guarantee that our implementation provides linearizable state machine replication. This proof required iteratively discovering and proving 90 system invariants. Our verified implementation is extracted to OCaml and runs on real networks. The primary challenge we faced during the verification process was proof maintenance, since proving one invariant often required strengthening and updating other parts of our proof. To address this challenge, we propose a methodology of planning for change during verification. Our methodology adapts classical information hiding techniques to the context of proof assistants, factors out common invariant-strengthening patterns into custom induction principles, proves\u00a0\u2026", "num_citations": "102\n", "authors": ["1712"]}
{"title": "RoboFlow: A flow-based visual programming language for mobile manipulation tasks\n", "abstract": " General-purpose robots can perform a range of useful tasks in human environments; however, programming them to robustly function in all possible environments that they might encounter is unfeasible. Instead, our research aims to develop robots that can be programmed by its end-users in their context of use, so that the robot needs to robustly function in only one particular environment. This requires intuitive ways in which end-users can program their robot. To that end, this paper contributes a flow-based visual programming language, called RoboFlow, that allows programming of generalizable mobile manipulation tasks. RoboFlow is designed to (i) ensure a robust low-level implementation of program procedures on a mobile manipulator, and (ii) restrict the high-level programming as much as possible to avoid user errors while enabling expressive programs that involve branching, looping, and nesting. We\u00a0\u2026", "num_citations": "86\n", "authors": ["1712"]}
{"title": "Relay: A new ir for machine learning frameworks\n", "abstract": " Machine learning powers diverse services in industry including search, translation, recommendation systems, and security. The scale and importance of these models require that they be efficient, expressive, and portable across an array of heterogeneous hardware devices. These constraints are often at odds; in order to better accommodate them we propose a new high-level intermediate representation (IR) called Relay. Relay is being designed as a purely-functional, statically-typed language with the goal of balancing efficient compilation, expressiveness, and portability. We discuss the goals of Relay and highlight its important design constraints. Our prototype is part of the open source NNVM compiler framework, which powers Amazon's deep learning framework MxNet.", "num_citations": "57\n", "authors": ["1712"]}
{"title": "Jitk: A trustworthy in-kernel interpreter infrastructure\n", "abstract": " Modern operating systems run multiple interpreters in the kernel, which enable user-space applications to add new functionality or specialize system policies. The correctness of such interpreters is critical to the overall system security: bugs in interpreters could allow adversaries to compromise user-space applications and even the kernel.", "num_citations": "57\n", "authors": ["1712"]}
{"title": "Scalable verification of border gateway protocol configurations with an SMT solver\n", "abstract": " Internet Service Providers (ISPs) use the Border Gateway Protocol (BGP) to announce and exchange routes for de-livering packets through the internet. ISPs must carefully configure their BGP routers to ensure traffic is routed reli-ably and securely. Correctly configuring BGP routers has proven challenging in practice, and misconfiguration has led to worldwide outages and traffic hijacks. This paper presents Bagpipe, a system that enables ISPs to declaratively express BGP policies and that automatically verifies that router configurations implement such policies. The novel initial network reduction soundly reduces policy verification to a search for counterexamples in a finite space. An SMT-based symbolic execution engine performs this search efficiently. Bagpipe reduces the size of its search space using predicate abstraction and parallelizes its search using symbolic variable hoisting. Bagpipe's policy specification\u00a0\u2026", "num_citations": "50\n", "authors": ["1712"]}
{"title": "Verified peephole optimizations for CompCert\n", "abstract": " Transformations over assembly code are common in many compilers. These transformations are also some of the most bug-dense compiler components. Such bugs could be elim-inated by formally verifying the compiler, but state-of-the-art formally verified compilers like CompCert do not sup-port assembly-level program transformations. This paper presents Peek, a framework for expressing, verifying, and running meaning-preserving assembly-level program trans-formations in CompCert. Peek contributes four new com-ponents: a lower level semantics for CompCert x86 syntax, a liveness analysis, a library for expressing and verifying peephole optimizations, and a verified peephole optimiza-tion pass built into CompCert. Each of these is accompanied by a correctness proof in Coq against realistic assumptions about the calling convention and the system memory alloca-tor. Verifying peephole optimizations in\u00a0\u2026", "num_citations": "42\n", "authors": ["1712"]}
{"title": "\u0152uf: minimizing the Coq extraction TCB\n", "abstract": " Verifying systems by implementing them in the programming language of a proof assistant (eg, Gallina for Coq) lets us directly leverage the full power of the proof assistant for verifying the system. But, to execute such an implementation requires extraction, a large complicated process that is in the trusted computing base (TCB).", "num_citations": "41\n", "authors": ["1712"]}
{"title": "Toward a standard benchmark format and suite for floating-point analysis\n", "abstract": " We introduce FPBench, a standard benchmark format for validation and optimization of numerical accuracy in floating-point computations. FPBench is a first step toward addressing an increasing need in our community for comparisons and combinations of tools from different application domains. To this end, FPBench provides a basic floating-point benchmark format and accuracy measures for comparing different tools. The FPBench format and measures allow comparing and composing different floating-point tools. We describe the FPBench format and measures and show that FPBench expresses benchmarks from recent papers in the literature, by building an initial benchmark suite drawn from these papers. We intend for FPBench to grow into a standard benchmark suite for the members of the floating-point tools research community.", "num_citations": "36\n", "authors": ["1712"]}
{"title": "Synthesizing structured CAD models with equality saturation and inverse transformations\n", "abstract": " Recent program synthesis techniques help users customize CAD models (eg, for 3D printing) by decompiling low-level triangle meshes to Constructive Solid Geometry (CSG) expressions. Without loops or functions, editing CSG can require many coordinated changes, and existing mesh decompilers use heuristics that can obfuscate high-level structure.", "num_citations": "27\n", "authors": ["1712"]}
{"title": "Verifying that web pages have accessible layout\n", "abstract": " Usability and accessibility guidelines aim to make graphical user interfaces accessible to all users, by, say, requiring that text is sufficiently large, interactive controls are visible, and heading size corresponds to importance. These guidelines must hold on the infinitely many possible renderings of a web page generated by differing screen sizes, fonts, and other user preferences. Today, these guidelines are tested by manual inspection of a few renderings, because 1) the guidelines are not expressed in a formal language, 2) the semantics of browser rendering are not well understood, and 3) no tools exist to check all possible renderings of a web page. VizAssert solves these problems. First, it introduces visual logic to precisely specify accessibility properties. Second, it formalizes a large fragment of the browser rendering algorithm using novel finitization reductions. Third, it provides a sound, automated tool for\u00a0\u2026", "num_citations": "21\n", "authors": ["1712"]}
{"title": "Software verification with ITPs should use binary code extraction to reduce the TCB\n", "abstract": " LCF-style provers emphasise that all results are secured by logical inference, and yet their current facilities for code extraction or code generation fall short of this high standard. This paper argues that extraction mechanisms with a small trusted computing base (TCB) ought to be used instead, pointing out that the recent CakeML and \u0152uf projects show that this is possible in HOL and within reach in Coq.", "num_citations": "18\n", "authors": ["1712"]}
{"title": "Egg: Fast and extensible equality saturation\n", "abstract": " An e-graph efficiently represents a congruence relation over many expressions. Although they were originally developed in the late 1970s for use in automated theorem provers, a more recent technique known as equality saturation repurposes e-graphs to implement state-of-the-art, rewrite-driven compiler optimizations and program synthesizers. However, e-graphs remain unspecialized for this newer use case. Equality saturation workloads exhibit distinct characteristics and often require ad-hoc e-graph extensions to incorporate transformations beyond purely syntactic rewrites.   This work contributes two techniques that make e-graphs fast and extensible, specializing them to equality saturation. A new amortized invariant restoration technique called rebuilding takes advantage of equality saturation's distinct workload, providing asymptotic speedups over current techniques in practice. A general mechanism called\u00a0\u2026", "num_citations": "17\n", "authors": ["1712"]}
{"title": "Functional programming for compiling and decompiling computer-aided design\n", "abstract": " Desktop-manufacturing techniques like 3D printing are increasingly popular because they reduce the cost and complexity of producing customized objects on demand. Unfortunately, the vibrant communities of early adopters, often referred to as \"makers,\" are not well-served by currently available software pipelines. Users today must compose idiosyncratic sequences of tools which are typically repurposed variants of proprietary software originally designed for expert specialists.   This paper proposes fundamental programming-languages techniques to bring improved rigor, reduced complexity, and new functionality to the computer-aided design (CAD) software pipeline for applications like 3D-printing. Compositionality, denotational semantics, compiler correctness, and program synthesis all play key roles in our approach, starting from the perspective that solid geometry is a programming language.   Specifically, we\u00a0\u2026", "num_citations": "17\n", "authors": ["1712"]}
{"title": "Investigating safety of a radiotherapy machine using system models with pluggable checkers\n", "abstract": " Formal techniques for guaranteeing software correctness have made tremendous progress in recent decades. However, applying these techniques to real-world safety-critical systems remains challenging in practice. Inspired by goals set out in prior work, we report on a large-scale case study that applies modern verification techniques to check safety properties of a radiotherapy system in current clinical use. Because of the diversity and complexity of the system\u2019s components (software, hardware, and physical), no single tool was suitable for both checking critical component properties and ensuring that their composition implies critical system properties. This paper describes how we used state-of-the-art approaches to develop specialized tools for verifying safety properties of individual components, as well as an extensible tool for composing those properties to check the safety of the system as a whole\u00a0\u2026", "num_citations": "17\n", "authors": ["1712"]}
{"title": "Carpentry compiler\n", "abstract": " Traditional manufacturing workflows strongly decouple design and fabrication phases. As a result, fabrication-related objectives such as manufacturing time and precision are difficult to optimize in the design space, and vice versa. This paper presents HL-HELM, a high-level, domain-specific language for expressing abstract, parametric fabrication plans; it also introduces LL-HELM, a low-level language for expressing concrete fabrication plans that take into account the physical constraints of available manufacturing processes. We present a new compiler that supports the real-time, unoptimized translation of high-level, geometric fabrication operations into concrete, tool-specific fabrication instructions; this gives users immediate feedback on the physical feasibility of plans as they design them. HELM offers novel optimizations to improve accuracy and reduce fabrication time as well as material costs. Finally\u00a0\u2026", "num_citations": "14\n", "authors": ["1712"]}
{"title": "Formal semantics and automated verification for the border gateway protocol\n", "abstract": " Traffic is routed across the Internet by Autonomous Systems, or ASes, such as ISPs, corporations, and universities. To route traffic reliably and securely, ASes must configure their Border Gateway Protocol (BGP) routers to implement policies restricting how routing announcements can be used and exchanged with other ASes.It is challenging to correctly implement BGP policies in low-level configuration languages. Large ASes maintain millions of lines of frequently-changing configurations that run distributed across hundreds of routers [8, 16]. Router misconfigurations are common and have led to highly visible failures affecting ASes and their billions of users. For example, in 2009 YouTube was inaccessible worldwide for several hours due to a misconfiguration in Pakistan [2], and in 2010 and 2014 China Telecom hijacked significant but unknown fractions of international traffic for extended periods [4, 15, 11, 10]. Goldberg surveys several additional major outages and their causes [7]. We present the first mechanized formal semantics of the BGP specification RFC 4271 [14], and we show how to use this semantics to develop reliable tools and guidelines that help BGP administrators avoid router misconfiguration. In contrast to previous semantics [6, 3, 17], our semantics is fully formal (it is implemented in the Coq proof assistant), and it models all required features of the BGP specification modulo low-level details such as bit representation of update messages and TCP.", "num_citations": "12\n", "authors": ["1712"]}
{"title": "Dynamic tensor rematerialization\n", "abstract": " Checkpointing enables the training of deep learning models under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand. Current checkpointing techniques statically plan these recomputations offline and assume static computation graphs. We demonstrate that a simple online algorithm can achieve comparable performance by introducing Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. We prove that DTR can train an -layer linear feedforward network on an  memory budget with only  tensor operations. DTR closely matches the performance of optimal static checkpointing in simulated experiments. We incorporate a DTR prototype into PyTorch merely by interposing on tensor allocations and operator calls and collecting lightweight metadata on tensors.", "num_citations": "8\n", "authors": ["1712"]}
{"title": "Teaching rigorous distributed systems with efficient model checking\n", "abstract": " Writing correct distributed systems code is difficult, especially for novice programmers. The inherent asynchrony and need for fault-tolerance make errors almost inevitable. Industrial-strength testing and model checking have been shown to be effective at uncovering bugs, but they come at a cost---in both time and effort---that is far beyond what students can afford. To address this, we have developed an efficient model checking framework and visual debugger for distributed systems, with the goal of helping students find and fix bugs in near real-time. We identify two novel techniques for reducing the search state space to more efficiently find bugs in student implementations. We report our experiences using these tools to help over two hundred students build a correct, linearizable, fault-tolerant, dynamically-sharded key--value store.", "num_citations": "8\n", "authors": ["1712"]}
{"title": "Combining tools for optimization and analysis of floating-point computations\n", "abstract": " Recent renewed interest in optimizing and analyzing floating-point programs has lead to a diverse array of new tools for numerical programs. These tools are often complementary, each focusing on a distinct aspect of numerical programming. Building reliable floating point applications typically requires addressing several of these aspects, which makes easy composition essential. This paper describes the composition of two recent floating-point tools: Herbie, which performs accuracy optimization, and Daisy, which performs accuracy verification. We find that the combination provides numerous benefits to users, such as being able to use Daisy to check whether Herbie\u2019s unsound optimizations improved the worst-case roundoff error, as well as benefits to tool authors, including uncovering a number of bugs in both tools. The combination also allowed us to compare the different program rewriting techniques\u00a0\u2026", "num_citations": "8\n", "authors": ["1712"]}
{"title": "SpaceSearch: A library for building and verifying solver-aided tools\n", "abstract": " Many verification tools build on automated solvers. These tools reduce problems in a specific application domain (e.g., compiler optimization validation) to queries that can be discharged with a highly optimized solver. But the correctness of the reductions themselves is rarely verified in practice, limiting the confidence that the solver's output establishes the desired domain-level property.   This paper presents SpaceSearch, a new library for developing solver-aided tools within a proof assistant. A user builds their solver-aided tool in Coq against the SpaceSearch interface, and the user then verifies that the results provided by the interface are sufficient to establish the tool's desired high-level properties. Once verified, the tool can be extracted to an implementation in a solver-aided language (e.g., Rosette), where SpaceSearch provides an efficient instantiation of the SpaceSearch interface with calls to an underlying\u00a0\u2026", "num_citations": "8\n", "authors": ["1712"]}
{"title": "Theia: automatically generating correct program state visualizations\n", "abstract": " Program state visualizations (PSVs) help programmers understand hidden program state like objects, references, and closures. Unfortunately, existing PSV tools do not support custom language semantics, which educators often use to introduce programming languages gradually. They also fail to visualize key pieces of program state, which can lead to incorrect and confusing visualizations.", "num_citations": "7\n", "authors": ["1712"]}
{"title": "Relay: A high-level compiler for deep learning\n", "abstract": " Frameworks for writing, compiling, and optimizing deep learning (DL) models have recently enabled progress in areas like computer vision and natural language processing. Extending these frameworks to accommodate the rapidly diversifying landscape of DL models and hardware platforms presents challenging tradeoffs between expressivity, composability, and portability. We present Relay, a new compiler framework for DL. Relay's functional, statically typed intermediate representation (IR) unifies and generalizes existing DL IRs to express state-of-the-art models. The introduction of Relay's expressive IR requires careful design of domain-specific optimizations, addressed via Relay's extension mechanisms. Using these extension mechanisms, Relay supports a unified compiler that can target a variety of hardware platforms. Our evaluation demonstrates Relay's competitive performance for a broad class of models and devices (CPUs, GPUs, and emerging accelerators). Relay's design demonstrates how a unified IR can provide expressivity, composability, and portability without compromising performance.", "num_citations": "7\n", "authors": ["1712"]}
{"title": "Bagpipe: Verified BGP configuration checking\n", "abstract": " To reliably and securely route traffic across the Internet, Internet Service Providers (ISPs) must configure their Border Gateway Protocol (BGP) routers to implement policies restricting how routing information can be exchanged with other ISPs. Correctly implementing these policies in lowlevel router configuration languages, with configuration code distributed across all of an ISP\u2019s routers, has proven challenging in practice, and misconfiguration has led to extended worldwide outages and traffic hijacks. We present Bagpipe, a system that enables ISPs to concisely express their policies and automatically check that router configurations adhere to these policies. To check policies efficiently, Bagpipe introduces the initial network reduction, exploits modern satisfiability solvers by building on the Rosette framework for solver-aided tools, and parallelizes configuration checking across many nodes. To ensure Bagpipe correctly checks configurations, we verified its implementation in Coq, which required developing both a new framework for verifying solver-aided tools and also the first formal semantics for BGP based on RFC 4271. To validate the effectiveness of our verified checker, we ran it on the router configurations of Internet2, a nationwide ISP. Bagpipe revealed 19 violations of standard BGP router policies without issuing any false positives. To validate our BGP semantics, we performed random differential testing against C-BGP, a popular BGP simulator. We found no bugs in our semantics and one bug in C-BGP.", "num_citations": "7\n", "authors": ["1712"]}
{"title": "Nimble: Efficiently compiling dynamic neural networks for model inference\n", "abstract": " Modern deep neural networks increasingly make use of features such as control flow, dynamic data structures, and dynamic tensor shapes. Existing deep learning systems focus on optimizing and executing static neural networks which assume a pre-determined model architecture and input data shapes\u2014assumptions that are violated by dynamic neural networks. Therefore, executing dynamic models with deep learning systems is currently both inflexible and sub-optimal, if not impossible. Optimizing dynamic neural networks is more challenging than static neural networks; optimizations must consider all possible execution paths and tensor shapes. This paper proposes Nimble, a high-performance and flexible system to optimize, compile, and execute dynamic neural networks on multiple platforms. Nimble handles model dynamism by introducing a dynamic type system, a set of dynamism-oriented optimizations, and a light-weight virtual machine runtime. Our evaluation demonstrates that Nimble outperforms existing solutions for dynamic neural networks by up to 20x on hardware platforms including Intel CPUs, ARM CPUs, and Nvidia GPUs.", "num_citations": "6\n", "authors": ["1712"]}
{"title": "Icing: Supporting fast-math style optimizations in a verified compiler\n", "abstract": " Verified compilers like CompCert and CakeML offer increasingly sophisticated optimizations. However, their deterministic source semantics and strict IEEE 754 compliance prevent the verification of \u201cfast-math\u201d style floating-point optimizations. Developers often selectively use these optimizations in mainstream compilers like GCC and LLVM to improve the performance of computations over noisy inputs or for heuristics by allowing the compiler to perform intuitive but IEEE 754-unsound rewrites.                 We designed, formalized, implemented, and verified a compiler for Icing, a new language which supports selectively applying fast-math style optimizations in a verified compiler. Icing\u2019s semantics provides the first formalization of fast-math in a verified compiler. We show how the Icing compiler can be connected to the existing verified CakeML compiler and verify the end-to-end translation by a sequence of\u00a0\u2026", "num_citations": "6\n", "authors": ["1712"]}
{"title": "Combining precision tuning and rewriting\n", "abstract": " Precision tuning and rewriting can improve both the accuracy and speed of floating point expressions, yet these techniques are typically applied separately. This paper explores how finer-grained interleaving of precision tuning and rewriting can help automatically generate a richer set of Pareto-optimal accuracy versus speed trade-offs. We introduce Pherbie (Pareto Herbie), a tool providing both precision tuning and rewriting, and evaluate interleaving these two strategies at different granularities. Our results demonstrate that finer-grained interleavings improve both the Pareto curve of candidate implementations and overall optimization time. On a popular set of tests from the FPBench suite, Pherbie finds both implementations that are significantly more accurate for a given cost and significantly faster for a given accuracy bound compared to baselines using precision tuning and rewriting alone or in sequence.", "num_citations": "4\n", "authors": ["1712"]}
{"title": "Toward multi-precision, multi-format numerics\n", "abstract": " Recent research has provided new, domain-specific number systems that accelerate modern workloads. Using these number systems effectively requires analyzing subtle multiprecision, multi-format (MPMF) code. Ideally, recent programming tools that automate numerical analysis tasks could help make MPMF programs both accurate and fast. However, three key challenges must be addressed: existing automated tools are difficult to compose due to subtle incompatibilities; there is no \u201cgold standard\u201d for correct MPMF execution; and no methodology exists for generalizing existing, IEEE-754-specialized tools to support MPMF. In this paper we report on recent work towards mitigating these related challenges. First, we extend the FPBench standard to support multi-precision, multi-format (MPMF) applications. Second, we present Titanic, a tool which provides reference results for arbitrary MPMF computations. Third\u00a0\u2026", "num_citations": "3\n", "authors": ["1712"]}
{"title": "Modular verification of web page layout\n", "abstract": " Automated verification can ensure that a web page satisfies accessibility, usability, and design properties regardless of the end user's device, preferences, and assistive technologies. However, state-of-the-art verification tools for layout properties do not scale to large pages because they rely on whole-page analyses and must reason about the entire page using the complex semantics of the browser layout algorithm.   This paper introduces and formalizes modular layout proofs. A modular layout proof splits a monolithic verification problem into smaller verification problems, one for each component of a web page. Each component specification can use rely/guarantee-style preconditions to make it verifiable independently of the rest of the page and enabling reuse across multiple pages. Modular layout proofs scale verification to pages an order of magnitude larger than those supported by previous approaches.   We\u00a0\u2026", "num_citations": "3\n", "authors": ["1712"]}
{"title": "Rewrite rule inference using equality saturation\n", "abstract": " Many compilers, synthesizers, and theorem provers rely on rewrite rules to simplify expressions or prove equivalences. Developing rewrite rules can be difficult: rules may be subtly incorrect, profitable rules are easy to miss, and rulesets must be rechecked or extended whenever semantics are tweaked. Large rulesets can also be challenging to apply: redundant rules slow down rule-based search and frustrate debugging. This paper explores how equality saturation, a promising technique that uses e-graphs to apply rewrite rules, can also be used to infer rewrite rules. E-graphs can compactly represent the exponentially large sets of enumerated terms and potential rewrite rules. We show that equality saturation efficiently shrinks both sets, leading to faster synthesis of smaller, more general rulesets. We prototyped these strategies in a tool dubbed ruler. Compared to a similar tool built on CVC4, ruler synthesizes 5.8X smaller rulesets 25X faster without compromising on proving power. In an end-to-end case study, we show ruler-synthesized rules which perform as well as those crafted by domain experts, and addressed a longstanding issue in a popular open source tool.", "num_citations": "2\n", "authors": ["1712"]}
{"title": "Pure tensor program rewriting via access patterns (representation pearl)\n", "abstract": " Tensor kernels in machine learning (ML) often correspond to pure mathematical expressions, making term rewriting an attractive strategy for optimization and mapping to specialized hardware accelerators. However, existing ML intermediate representations (IRs) tend to either be \\textit{pure but high-level}, making low-level rewrites to hardware targets inexpressible, or \\textit{low-level but impure}, hampering the use of term rewriting altogether. This paper introduces Glenside, a pure IR whose core abstraction -- the \\textit{access pattern} -- enables low-level, layout-aware, hardware-centric program rewrites. We demonstrate how term rewriting in Glenside can be used to map program fragments to hardware accelerator invocations and automatically discover classic data layout transformations like \\texttt{im2col}. Glenside establishes a new foundation for exploring further term rewriting techniques in optimizing low-level tensor programs.", "num_citations": "2\n", "authors": ["1712"]}
{"title": "A graphical interactive debugger for distributed systems\n", "abstract": " Designing and debugging distributed systems is notoriously difficult. The correctness of a distributed system is largely determined by its handling of failure scenarios. The sequence of events leading to a bug can be long and complex, and it is likely to include message reorderings and failures. On single-node systems, interactive debuggers enable stepping through an execution of the program, but they lack the ability to easily simulate failure scenarios and control the order in which messages are delivered. Oddity is a graphical, interactive debugger for distributed systems. It brings the power of traditional step-through debugging---fine-grained control and observation of a program as it executes---to distributed systems. It also enables exploratory testing, in which an engineer examines and perturbs the behavior of a system in order to better understand it, perhaps without a specific bug in mind. A programmer can directly control message and failure interleaving. Oddity supports time travel, allowing a developer to explore multiple branching executions of a system within a single debugging session. Above all, Oddity encourages distributed systems thinking: rather than assuming the normal case and attaching failure handling as an afterthought, distributed systems should be developed around the certainty of message loss and node failure. Graduate and undergraduate students used Oddity in two distributed systems classes. Usage tracking and qualitative surveys showed that students found Oddity useful for both debugging and exploratory testing.", "num_citations": "2\n", "authors": ["1712"]}
{"title": "Verification of Implementations of Distributed Systems Under Churn\n", "abstract": " In order to provide high availability and operate in unreliable environments, many critical applications are implemented as distributed systems. Unfortunately, the need to handle packet drops, machine crashes, and churn\u2014the spontaneous arrival and departure of nodes to and from the network\u2014has made these systems difficult to implement correctly in practice. Recent work provides mechanisms to formally verify implementations that tolerate packet drops and machine crashes; however, no extant verification framework supports reasoning about churn. To address this challenge, we introduce support for reasoning about churn to the Coq-based Verdi framework. Churn makes it difficult for systems to always uphold desired safety properties. Instead, systems promise to progress towards full safety when networks are quiescent, and do their best to fight entropy the rest of the time. We term this type of guarantee punctuated safety. Such properties are challenging to verify because their proofs involve both invariance arguments over system actions and well-foundedness arguments over infinite sequences of system states under fairness hypotheses.", "num_citations": "2\n", "authors": ["1712"]}
{"title": "Formal Semantics and Verification for the Border Gateway Protocol\n", "abstract": " Internet Service Providers (ISPs) use the Border Gateway Protocol (BGP) to exchange routing information. ISPs use a variety of formalisms, checkers, and simulators to avoid BGP configuration errors. However, these tools are based on simplified semantics of BGP or no semantics at all, and therefore they cannot guarantee the absence of router misconfigurations. Meanwhile, BGP router misconfiguration has led to worldwide outages and traffic hijacks. To enable tools that provide formal guarantees, and to provide a foundation for future work on BGP, we present the first mechanized formal semantics of the BGP specification RFC 4271. The semantics is implemented in Coq. The semantics models all required features of the BGP specification modulo low-level details such as bit representation of update messages and TCP. Three case studies show how to use our semantics to develop reliable proofs, checkers, and simulators; and provide evidence for the correctness of our semantics. 1) We formalized and extended the seminal pen-and-paper proof by Gao & Rexford on the convergence of BGP, revealing necessary extensions to Gao & Rexford\u2019s original assumptions. 2) We verified the soundness of the Bagpipe tool which automatically checks that BGP configurations adhere to given specifications. 3) We tested the popular BGP simulator C-BGP against our semantics, revealing a bug in C-BGP.", "num_citations": "2\n", "authors": ["1712"]}
{"title": "A roadmap towards parallel printing for desktop 3d printers\n", "abstract": " 3D printers with multiple extruders (or multiheaded printers) are common in the desktop fabrication community, but are primarily used for multi-color or multi-material printing, using only one extruder at a time. What if these multiheaded desktop printers could also be used for simultaneous parallel printing? While this is a relatively unexplored direction, we argue that it deserves further investigation: a flexible, robust, and affordable parallel printing ecosystem could significantly reduce fabrication time for many applications and further enhance the value of desktop rapid prototyping. We propose a research agenda to explore the development of a parallel printing pipeline, and summarize our observations from a preliminary investigation of simultaneous extrusion. We hope this vision will encourage and guide future research in developing hardware, firmware, and slicers to facilitate parallel 3D printing.", "num_citations": "1\n", "authors": ["1712"]}
{"title": "Learning to Adapt: Analyses for Configurable Software\n", "abstract": " Configurations are powerful tools for end-user customization of software. For example, non-expert software users may customize the behavior of programs via option menus, system administrators may tune server behavior by editing configuration files, and software developers may specialize generic frameworks to suit their purposes with software annotations and configuration files. Although configurations are extremely powerful tools that increase software flexibility, they increase implementation complexity causing subtle bugs and confounding analysis tools. This dissertation considers the challenges posed by highly-configurable software and proposes that specialized program analyses can overcome these challenges. In particular, this dissertation explores two main topics. The first direction is creating program analyses to find defects in software that supports configuration changes at runtime. The second is the development of principled techniques for analyzing applications built on highly-configurable frameworks. These two research efforts are supported with formal proofs and empirical evaluations of proof-of-concept implementations.", "num_citations": "1\n", "authors": ["1712"]}
{"title": "Automatic formal verification for EPICS\n", "abstract": " We built an EPICS-based radiation therapy machine control program and are using it to treat patients at our hospital. To help ensure safety, the control program uses a restricted subset of EPICS constructs and programming techniques, and we developed several new automated formal verification tools for this subset.", "num_citations": "1\n", "authors": ["1712"]}
{"title": "Peek: A formally verified peephole optimization framework for x86\n", "abstract": " Peek is a first step toward adding support for assembly-level program analyses, transformations, and optimizations in CompCert. Currently, Peek focuses on x86 peephole transformations implemented and verified in Coq. Peek is designed to provide a modular interface requiring that each peephole optimization satisfy only local correctness properties. Our primary result establishes that, assuming the C calling convention, any peephole optimization satisfying these local properties preserves global program meaning.", "num_citations": "1\n", "authors": ["1712"]}
{"title": "Reducing the Costs of Proof Assistant Based Formal Verification or: Conviction without the Burden of Proof\n", "abstract": " This thesis considers the challenge of fully formal software verification in the demanding and foundational context of mechanical proof assistants. While this approach offers the strongest guarantees for software correctness, it has traditionally imposed tremendous costs to manually construct proofs. In this work, I explore techniques to mitigate this proof burden through careful system design. In particular, I demonstrate how formal shim verification and extensible compiler techniques can radically reduce the proof burden for realistic implementations of critical modern infrastructure", "num_citations": "1\n", "authors": ["1712"]}
{"title": "Parameterized program equivalence checking\n", "abstract": " In the previous chapter we discussed an approach to verify if two programs are equivalent, thereby proving that the translation (performed by an HLS tool) from high-level design to low-level design is correct. In this chapter, we discuss another approach that guarantees correctness of the translation from high-level design to low-level design, by proving the HLS tool itself correct. Unlike translation validation, this approach proves the correctness of an HLS tool once and for all, before it is ever run. In the following sections we describe in details an approach called Parametrized Equivalence Checking\u00a0[120] (PEC ) that generalizes the translation validation approach discussed in the previous chapter to automatically establish the correctness of semantics preserving transformations once and for all.", "num_citations": "1\n", "authors": ["1712"]}