{"title": "LiteRace: Effective sampling for lightweight data-race detection\n", "abstract": " Data races are one of the most common and subtle causes of pernicious concurrency bugs. Static techniques for preventing data races are overly conservative and do not scale well to large programs. Past research has produced several dynamic data race detectors that can be applied to large programs. They are precise in the sense that they only report actual data races. However, dynamic data race detectors incur a high performance overhead, slowing down a program's execution by an order of magnitude.", "num_citations": "280\n", "authors": ["1626"]}
{"title": "A randomized scheduler with probabilistic guarantees of finding bugs\n", "abstract": " This paper presents a randomized scheduler for finding concurrency bugs. Like current stress-testing methods, it repeatedly runs a given test program with supplied inputs. However, it improves on stress-testing by finding buggy schedules more effectively and by quantifying the probability of missing concurrency bugs. Key to its design is the characterization of the depth of a concurrency bug as the minimum number of scheduling constraints required to find it. In a single run of a program with n threads and k steps, our scheduler detects a concurrency bug of depth d with probability at least 1/nkd-1. We hypothesize that in practice, many concurrency bugs (including well-known types such as ordering errors, atomicity violations, and deadlocks) have small bug-depths, and we confirm the efficiency of our schedule randomization by detecting previously unknown and known concurrency bugs in several production-scale\u00a0\u2026", "num_citations": "272\n", "authors": ["1626"]}
{"title": "Effective Data-Race Detection for the Kernel.\n", "abstract": " Data races are an important class of concurrency errors where two threads erroneously access a shared memory location without appropriate synchronization. This paper presents DataCollider, a lightweight and effective technique for dynamically detecting data races in kernel modules. Unlike existing data-race detection techniques, DataCollider is oblivious to the synchronization protocols (such as locking disciplines) the program uses to protect shared memory accesses. This is particularly important for low-level kernel code that uses a myriad of complex architecture/device specific synchronization mechanisms. To reduce the runtime overhead, DataCollider randomly samples a small percentage of memory accesses as candidates for data-race detection. The key novelty of DataCollider is that it uses breakpoint facilities already supported by many hardware architectures to achieve negligible runtime overheads. We have implemented DataCollider for the Windows 7 kernel and have found 25 confirmed erroneous data races of which 12 have already been fixed.", "num_citations": "226\n", "authors": ["1626"]}
{"title": "Line-up: a complete and automatic linearizability checker\n", "abstract": " Modular development of concurrent applications requires thread-safe components that behave correctly when called concurrently by multiple client threads. This paper focuses on linearizability, a specific formalization of thread safety, where all operations of a concurrent component appear to take effect instantaneously at some point between their call and return. The key insight of this paper is that if a component is intended to be deterministic, then it is possible to build an automatic linearizability checker by systematically enumerating the sequential behaviors of the component and then checking if each its concurrent behavior is equivalent to some sequential behavior.", "num_citations": "163\n", "authors": ["1626"]}
{"title": "Effective program verification for relaxed memory models\n", "abstract": " Program verification for relaxed memory models is hard. The high degree of nondeterminism in such models challenges standard verification techniques. This paper proposes a new verification technique for the most common relaxation, store buffers. Crucial to this technique is the observation that all programmers, including those who use low-lock techniques for performance, expect their programs to be sequentially consistent. We first present a monitor algorithm that can detect the presence of program executions that are not sequentially consistent due to store buffers while only exploring sequentially consistent executions. Then, we combine this monitor with a stateless model checker that verifies that every sequentially consistent execution is correct. We have implemented this algorithm in a prototype tool called Sober and present experiments that demonstrate the precision and scalability of our method\u00a0\u2026", "num_citations": "159\n", "authors": ["1626"]}
{"title": "Yinyang k-means: A drop-in replacement of the classic k-means with consistent speedup\n", "abstract": " This paper presents Yinyang K-means, a new algorithm for K-means clustering. By clustering the centers in the initial stage, and leveraging efficiently maintained lower and upper bounds between a point and centers, it more effectively avoids unnecessary distance calculations than prior algorithms. It significantly outperforms classic K-means and prior alternative K-means algorithms consistently across all experimented data sets, cluster numbers, and machine configurations. The consistent, superior performance\u2014plus its simplicity, user-control of overheads, and guarantee in producing the same clustering results as the standard K-means does\u2014makes Yinyang K-means a drop-in replacement of the classic K-means with an order of magnitude higher performance.", "num_citations": "128\n", "authors": ["1626"]}
{"title": "DRFx: A simple and efficient memory model for concurrent programming languages\n", "abstract": " The most intuitive memory model for shared-memory multithreaded programming is sequential consistency (SC), but it disallows the use of many compiler and hardware optimizations thereby impacting performance. Data-race-free (DRF) models, such as the proposed C++ 0x memory model, guarantee SC execution for datarace-free programs. But these models provide no guarantee at all for racy programs, compromising the safety and debuggability of such programs. To address the safety issue, the Java memory model, which is also based on the DRF model, provides a weak semantics for racy executions. However, this semantics is subtle and complex, making it difficult for programmers to reason about their programs and for compiler writers to ensure the correctness of compiler optimizations.", "num_citations": "106\n", "authors": ["1626"]}
{"title": "End-to-end sequential consistency\n", "abstract": " Sequential consistency (SC) is arguably the most intuitive behavior for a shared-memory multithreaded program. It is widely accepted that language-level SC could significantly improve programmability of a multiprocessor system. However, efficiently supporting end-to-end SC remains a challenge as it requires that both compiler and hardware optimizations preserve SC semantics. While a recent study has shown that a compiler can preserve SC semantics for a small performance cost, an efficient and complexity-effective SC hardware remains elusive. Past hardware solutions relied on aggressive speculation techniques, which has not yet been realized in a practical implementation. This paper exploits the observation that hardware need not enforce any memory model constraints on accesses to thread-local and shared read-only locations. A processor can easily determine a large fraction of these safe accesses\u00a0\u2026", "num_citations": "105\n", "authors": ["1626"]}
{"title": "Retro: Targeted resource management in multi-tenant distributed systems\n", "abstract": " In distributed systems shared by multiple tenants, effective resource management is an important pre-requisite to providing quality of service guarantees. Many systems deployed today lack performance isolation and experience contention, slowdown, and even outages caused by aggressive workloads or by improperly throttled maintenance tasks such as data replication. In this work we present Retro, a resource management framework for shared distributed systems. Retro monitors per-tenant resource usage both within and across distributed systems, and exposes this information to centralized resource management policies through a high-level API. A policy can shape the resources consumed by a tenant using Retro\u2019s control points, which enforce sharing and ratelimiting decisions. We demonstrate Retro through three policies providing bottleneck resource fairness, dominant resource fairness, and latency guarantees to high-priority tenants, and evaluate the system across five distributed systems: HBase, Yarn, MapReduce, HDFS, and Zookeeper. Our evaluation shows that Retro has low overhead, and achieves the policies\u2019 goals, accurately detecting contended resources, throttling tenants responsible for slowdown and overload, and fairly distributing the remaining cluster capacity.", "num_citations": "103\n", "authors": ["1626"]}
{"title": "Data-parallel finite-state machines\n", "abstract": " A finite-state machine (FSM) is an important abstraction for solving several problems, including regular-expression matching, tokenizing text, and Huffman decoding. FSM computations typically involve data-dependent iterations with unpredictable memory-access patterns making them difficult to parallelize. This paper describes a parallel algorithm for FSMs that breaks dependences across iterations by efficiently enumerating transitions from all possible states on each input symbol. This allows the algorithm to utilize various sources of data parallelism available on modern hardware, including vector instructions and multiple processors/cores. For instance, on benchmarks from three FSM applications: regular expressions, Huffman decoding, and HTML tokenization, the parallel algorithm achieves up to a 3x speedup over optimized sequential baselines on a single core, and linear speedups up to 21x on 8 cores.", "num_citations": "94\n", "authors": ["1626"]}
{"title": "Kahawai: High-quality mobile gaming using gpu offload\n", "abstract": " This paper presents Kahawai 1, a system that provides high-quality gaming on mobile devices, such as tablets and smartphones, by offloading a portion of the GPU computation to server-side infrastructure. In contrast with previous thin-client approaches that require a server-side GPU to render the entire content, Kahawai uses collaborative rendering to combine the output of a mobile GPU and a server-side GPU into the displayed output. Compared to a thin client, collaborative rendering requires significantly less network bandwidth between the mobile device and the server to achieve the same visual quality and, unlike a thin client, collaborative rendering supports disconnected operation, allowing a user to play offline-albeit with reduced visual quality.", "num_citations": "93\n", "authors": ["1626"]}
{"title": "A case for an SC-preserving compiler\n", "abstract": " The most intuitive memory consistency model for shared-memory multi-threaded programming is sequential consistency (SC). However, current concurrent programming languages support a relaxed model, as such relaxations are deemed necessary for enabling important optimizations. This paper demonstrates that an SC-preserving compiler, one that ensures that every SC behavior of a compiler-generated binary is an SC behavior of the source program, retains most of the performance benefits of an optimizing compiler. The key observation is that a large class of optimizations crucial for performance are either already SC-preserving or can be modified to preserve SC while retaining much of their effectiveness. An SC-preserving compiler, obtained by restricting the optimization phases in LLVM, a state-of-the-art C/C++ compiler, incurs an average slowdown of 3.8% and a maximum slowdown of 34% on a set of\u00a0\u2026", "num_citations": "89\n", "authors": ["1626"]}
{"title": "Finding protocol manipulation attacks\n", "abstract": " We develop a method to help discover manipulation attacks in protocol implementations. In these attacks, adversaries induce honest nodes to exhibit undesirable behaviors by misrepresenting their intent or network conditions. Our method is based on a novel combination of static analysis with symbolic execution and dynamic analysis with concrete execution. The former finds code paths that are likely vulnerable, and the latter emulates adversarial actions that lead to effective attacks. Our method is precise (ie, no false positives) and we show that it scales to complex protocol implementations. We apply it to four diverse protocols, including TCP, the 802.11 MAC, ECN, and SCTP, and show that it is able to find all manipulation attacks that have been previously reported for these protocols. We also find a previously unreported attack for SCTP. This attack is a variant of a TCP attack but must be mounted differently in\u00a0\u2026", "num_citations": "85\n", "authors": ["1626"]}
{"title": "CHET: an optimizing compiler for fully-homomorphic neural-network inferencing\n", "abstract": " Fully Homomorphic Encryption (FHE) refers to a set of encryption schemes that allow computations on encrypted data without requiring a secret key. Recent cryptographic advances have pushed FHE into the realm of practical applications. However, programming these applications remains a huge challenge, as it requires cryptographic domain expertise to ensure correctness, security, and performance.", "num_citations": "75\n", "authors": ["1626"]}
{"title": "GAMBIT: effective unit testing for concurrency libraries\n", "abstract": " As concurrent programming becomes prevalent, software providers are investing in concurrency libraries to improve programmer productivity. Concurrency libraries improve productivity by hiding error-prone, low-level synchronization from programmers and providing higher-level concurrent abstractions. Testing such libraries is difficult, however, because concurrency failures often manifest only under particular scheduling circumstances. Current best testing practices are often inadequate: heuristic-guided fuzzing is not systematic, systematic schedule enumeration does not find bugs quickly, and stress testing is neither systematic nor fast. To address these shortcomings, we propose a prioritized search technique called GAMBIT that combines the speed benefits of heuristic-guided fuzzing with the soundness, progress, and reproducibility guarantees of stateless model checking. GAMBIT combines known techniques\u00a0\u2026", "num_citations": "65\n", "authors": ["1626"]}
{"title": "Multicore acceleration of priority-based schedulers for concurrency bug detection\n", "abstract": " Testing multithreaded programs is difficult as threads can interleave in a nondeterministic fashion. Untested interleavings can cause failures, but testing all interleavings is infeasible. Many interleaving exploration strategies for bug detection have been proposed, but their relative effectiveness and performance remains unclear as they often lack publicly available implementations and have not been evaluated using common benchmarks. We describe NeedlePoint, an open-source framework that allows selection and comparison of a wide range of interleaving exploration policies for bug detection proposed by prior work.", "num_citations": "64\n", "authors": ["1626"]}
{"title": "What\u2019s decidable about weak memory models?\n", "abstract": " We investigate the decidability of the state reachability problem in finite-state programs running under weak memory models. In [3], we have shown that this problem is decidable for TSO and its extension with the write-to-write order relaxation, but beyond these models nothing is known to be decidable. Moreover, we have shown that relaxing the program order by allowing reads or writes to overtake reads leads to undecidability. In this paper, we refine these results by sharpening the (un)decidability frontiers on both sides. On the positive side, we introduce a new memory model NSW (for non-speculative writes) that extends TSO with the write-to-write relaxation, the read-to-read relaxation, and support for partial fences. We present a backtrack-free operational model for NSW, and prove that it does not allow causal cycles (thus barring pathological out-of-thin-air effects). On the negative side, we show that\u00a0\u2026", "num_citations": "52\n", "authors": ["1626"]}
{"title": "Efficient processor support for DRFx, a memory model with exceptions\n", "abstract": " A longstanding challenge of shared-memory concurrency is to provide a memory model that allows for efficient implementation while providing strong and simple guarantees to programmers. The C++ 0x and Java memory models admit a wide variety of compiler and hardwareoptimizations and provide sequentially consistent (SC) semantics for data-race-free programs. However, they either do not provide any semantics (C++ 0x) or provide a hard-to-understand semantics (Java) for racy programs, compromising the safety and debuggability of such programs.", "num_citations": "45\n", "authors": ["1626"]}
{"title": "Parallelizing dynamic programming through rank convergence\n", "abstract": " This paper proposes an efficient parallel algorithm for an important class of dynamic programming problems that includes Viterbi, Needleman-Wunsch, Smith-Waterman, and Longest Common Subsequence. In dynamic programming, the subproblems that do not depend on each other, and thus can be computed in parallel, form stages or wavefronts. The algorithm presented in this paper provides additional parallelism allowing multiple stages to be computed in parallel despite dependences among them. The correctness and the performance of the algorithm relies on rank convergence properties of matrix multiplication in the tropical semiring, formed with plus as the multiplicative operation and max as the additive operation. This paper demonstrates the efficiency of the parallel algorithm by showing significant speed ups on a variety of important dynamic programming problems. In particular, the parallel Viterbi\u00a0\u2026", "num_citations": "44\n", "authors": ["1626"]}
{"title": "Server GPU assistance for mobile GPU applications\n", "abstract": " Various technologies described herein pertain to performing collaborative rendering. A GPU of a mobile device can generate a mobile-rendered video stream based on a first instance of an application executed on the mobile device. A GPU of a server can generate one or more server-rendered video streams based on instance (s) of the application executed on the server. Based on the one or more server-rendered video streams, the server can generate a compressed server-manipulated video stream. The mobile device can further combine the mobile-rendered video stream and the compressed server-manipulated video stream to form a collaborative video stream, and a display screen of the mobile device can be caused to display the collaborative video stream. The mobile-rendered video stream can have a first level of a quality attribute and the collaborative video stream can have a second level of the quality\u00a0\u2026", "num_citations": "43\n", "authors": ["1626"]}
{"title": "An incremental heap canonicalization algorithm\n", "abstract": " The most expensive operation in explicit state model checking is the hash computation required to store the explored states in a hash table. One way to reduce this computation is to compute the hash incrementally by only processing those portions of the state that are modified in a transition. This paper presents an incremental heap canonicalization algorithm that aids in such an incremental hash computation. Like existing heap canonicalization algorithms, the incremental algorithm reduces the state space explored by detecting heap symmetries. On the other hand, the algorithm ensures that for small changes in the heap the resulting canonical representations differ only by relatively small amounts. This reduces the amount of hash computation a model checker has to perform after every transition, resulting in significant speedup of state space exploration. This paper describes the algorithm and its\u00a0\u2026", "num_citations": "41\n", "authors": ["1626"]}
{"title": "Bounded partial-order reduction\n", "abstract": " Eliminating concurrency errors is increasingly important as systems rely more on parallelism for performance. Exhaustively exploring the state-space of a program's thread interleavings finds concurrency errors and provides coverage guarantees, but suffers from exponential state-space explosion. Two prior approaches alleviate state-space explosion. (1) Dynamic partial-order reduction (DPOR) provides full coverage and explores only one interleaving of independent transitions. (2) Bounded search provides bounded coverage by enumerating interleavings that do not exceed a bound. In particular, we focus on preemption-bounding. Combining partial-order reduction with preemption-bounding had remained an open problem. We show that preemption-bounded search explores the same partial orders repeatedly and consequently explores more executions than unbounded DPOR, even for small bounds. We\u00a0\u2026", "num_citations": "37\n", "authors": ["1626"]}
{"title": "Modular arithmetic decision procedure\n", "abstract": " All integer data types in programs (such as int, short, byte) have an underlying finite representation in hardware. This finiteness can result in subtle integer-overflow errors that are hard to reason about both for humans and analysis tools alike. As a first step towards finding such errors automatically, we will describe two modular arithmetic decision procedures for reasoning about bounded integers.We show how to deal with modular arithmetic operations and inequalities for both linear and non-linear problems. Both procedures are suitable for integration with Nelson-Oppen framework [1, 2, 3]. The linear solver is composed of M\u00fcller-Seidl algorithm [4] and an arbitrary integer solver for solving preprocessed congruences and inequalities.", "num_citations": "37\n", "authors": ["1626"]}
{"title": "Concurrency software testing with probabilistic bounds on finding bugs\n", "abstract": " Described is a probabilistic concurrency testing mechanism for testing a concurrent software program that provides a probabilistic guarantee of finding any concurrent software bug at or below a bug depth (that corresponds to a complexity level for finding the bug). A scheduler/algorithm inserts priority lowering points into the code and runs the highest priority thread based upon initially randomly distributed priorities. When that thread reaches a priority lowering point, its priority is lowered to a value associated (eg, by random distribution) with that priority lowering point, whereby a different thread now has the currently highest priority. That thread is run until its priority is similarly lowered, and so on, whereby all schedules needed to find a concurrency bug are run.", "num_citations": "33\n", "authors": ["1626"]}
{"title": "Data race detection\n", "abstract": " The claimed subject matter provides a method for detecting a data race. The method includes inserting a plurality of breakpoints into a corresponding plurality of program locations. Each of the program locations accesses a plurality of memory locations. Each of the program locations is selected randomly. The method also includes detecting one or more data races for the memory locations in response to one or more of the breakpoints firing. Additionally, the method includes generating a report describing the one or more data races.", "num_citations": "30\n", "authors": ["1626"]}
{"title": "EVA: An encrypted vector arithmetic language and compiler for efficient homomorphic computation\n", "abstract": " Fully-Homomorphic Encryption (FHE) offers powerful capabilities by enabling secure offloading of both storage and computation, and recent innovations in schemes and implementations have made it all the more attractive. At the same time, FHE is notoriously hard to use with a very constrained programming model, a very unusual performance profile, and many cryptographic constraints. Existing compilers for FHE either target simpler but less efficient FHE schemes or only support specific domains where they can rely on expert-provided high-level runtimes to hide complications.", "num_citations": "29\n", "authors": ["1626"]}
{"title": "2dfq: Two-dimensional fair queuing for multi-tenant cloud services\n", "abstract": " In many important cloud services, different tenants execute their requests in the thread pool of the same process, requiring fair sharing of resources. However, using fair queue schedulers to provide fairness in this context is difficult because of high execution concurrency, and because request costs are unknown and have high variance. Using fair schedulers like WFQ and WF\u00b2Q in such settings leads to bursty schedules, where large requests block small ones for long periods of time. In this paper, we propose Two-Dimensional Fair Queueing (2DFQ), which spreads requests of different costs across di erent threads and minimizes the impact of tenants with unpredictable requests. In evaluation on production workloads from Azure Storage, a large-scale cloud system at Microsoft, we show that 2DFQ reduces the burstiness of service by 1-2 orders of magnitude. On workloads where many large requests compete with\u00a0\u2026", "num_citations": "28\n", "authors": ["1626"]}
{"title": "Amplification of dynamic checks through concurrency fuzzing\n", "abstract": " The subject disclosure relates to effective dynamic monitoring of an application executing in a computing system by increasing concurrency coverage. A set of dynamic checks are linked to an application by mechanisms that enable the dynamic checks to monitor behavior of the application at runtime. As additionally described herein, concurrency fuzzing is applied to the application to randomize thread schedules of the application, thus increasing a number of disparate concurrency scenarios of the application observed by the plurality of dynamic checks.", "num_citations": "26\n", "authors": ["1626"]}
{"title": "Progress guarantee for parallel programs via bounded lock-freedom\n", "abstract": " Parallel platforms are becoming ubiquitous with modern computing systems. Many parallel applications attempt to avoid locks in order to achieve high responsiveness, aid scalability, and avoid deadlocks and livelocks. However, avoiding the use of system locks does not guarantee that no locks are actually used, because progress inhibitors may occur in subtle ways through various program structures. Notions of progress guarantee such as lock-freedom, wait-freedom, and obstruction-freedom have been proposed in the literature to provide various levels of progress guarantees.", "num_citations": "26\n", "authors": ["1626"]}
{"title": "Sampling techniques for dynamic data-race detection\n", "abstract": " This document describes a dynamic data race detector that utilizes adaptive sampling techniques. The adaptive sampling techniques include locating threads during execution of a multi-threaded program and identifying thread-specific hot paths, thread-specific cold paths and lockset paths during execution of the program. Once these paths are identified, they are sampled, potentially at different rates. Any information gained during the sampling may be stored in a data race log, which a developer may use to correct any identified program bugs.", "num_citations": "24\n", "authors": ["1626"]}
{"title": "Computer implemented methods for solving difference and non-difference linear constraints\n", "abstract": " A computer implemented method for solving linear arithmetic constraints that combines a solver for difference constraints with a general linear arithmetic constraint solver. When used to solve sparse linear arithmetic constraints, the time and space complexity of the process is determined by the difference constraint component.", "num_citations": "22\n", "authors": ["1626"]}
{"title": "White-box testing of big data analytics with complex user-defined functions\n", "abstract": " Data-intensive scalable computing (DISC) systems such as Google\u2019s MapReduce, Apache Hadoop, and Apache Spark are being leveraged to process massive quantities of data in the cloud. Modern DISC applications pose new challenges in exhaustive, automatic testing because they consist of dataflow operators, and complex user-defined functions (UDF) are prevalent unlike SQL queries. We design a new white-box testing approach, called BigTest to reason about the internal semantics of UDFs in tandem with the equivalence classes created by each dataflow and relational operator. Our evaluation shows that, despite ultra-large scale input data size, real world DISC applications are often significantly skewed and inadequate in terms of test coverage, leaving 34% of Joint Dataflow and UDF (JDU) paths untested. BigTest shows the potential to minimize data size for local testing by 10^ 5 to 10^ 8 orders of\u00a0\u2026", "num_citations": "17\n", "authors": ["1626"]}
{"title": "Systematic concurrency testing using CHESS\n", "abstract": " Concurrency testing should aim for systematic coverage of thread interleavings. The most common method used today is stress testing, where the program is run under load with lots of threads. While this indirectly increases the variety of thread interleavings, the coverage is neither sufficient and nor predictable---stories are legend of the so-called\" Heisenbugs\" that rarely surface during testing and are very hard to debug.", "num_citations": "17\n", "authors": ["1626"]}
{"title": "Parallelizing wfst speech decoders\n", "abstract": " The performance-intensive part of a large-vocabulary continuous speech-recognition system is the Viterbi computation that determines the sequence of words that are most likely to generate the acoustic-state scores extracted from an input utterance. This paper presents an efficient parallel algorithm for Viterbi. The key idea is to partition the per-frame computation among threads to minimize inter-thread communication despite traversing a large irregular acoustic and language model graphs. Together with a per-thread beam search, load balancing language-model lookups, and memory optimizations, we achieve a 6.67\u00d7 speedup over an highly-optimized production-quality WFST-based speech decoder. On a 200,000 word vocabulary and a 59 million ngram model, our decoder runs at 0.27\u00d7 real time while achieving a word-error rate of 14.81% on 6214 labeled utterances from Voice Search data.", "num_citations": "16\n", "authors": ["1626"]}
{"title": "A volatile-by-default JVM for server applications\n", "abstract": " A *memory consistency model* (or simply *memory model*) defines the possible values that a shared-memory read may return in a multithreaded programming language. Choosing a memory model involves an inherent performance-programmability tradeoff. The Java language has adopted a *relaxed* (or *weak*) memory model that is designed to admit most traditional compiler optimizations and obviate the need for hardware fences on most shared-memory accesses. The downside, however, is that programmers are exposed to a complex and unintuitive semantics and must carefully declare certain variables as `volatile` in order to enforce program orderings that are necessary for proper behavior.   This paper proposes a simpler and stronger memory model for Java through a conceptually small change: *every* variable has `volatile` semantics by default, but the language allows a programmer to tag certain\u00a0\u2026", "num_citations": "15\n", "authors": ["1626"]}
{"title": "Systematically exploring the behavior of control programs\n", "abstract": " Many networked systems today, ranging from home automation networks to global wide-area networks, are operated using centralized control programs. Bugs in such programs pose serious risks to system security and stability. We develop a new technique to systematically explore the behavior of control programs. Because control programs depend intimately on absolute and relative timing of inputs, a key challenge that we face is to systematically handle time. We develop an approach that models programs as timed automata and incorporates novel mechanisms to enable scalable and comprehensive exploration. We implement our approach in a tool called DeLorean and apply it to real control programs for home automation and software-defined networks. DeLorean is able to finds bugs in these programs as well as provide significantly better code coverage\u2014up to 94% compared to 76% for existing techniques.", "num_citations": "15\n", "authors": ["1626"]}
{"title": "A two-tier technique for supporting quantifiers in a lazily proof-explicating theorem prover\n", "abstract": " Lazy proof explication is a theorem-proving architecture that allows a combination of Nelson-Oppen-style decision procedures to leverage a SAT solver\u2019s ability to perform propositional reasoning efficiently. The SAT solver finds ways to satisfy a given formula propositionally, while the various decision procedures perform theory reasoning to block propositionally satisfied instances that are not consistent with the theories. Supporting quantifiers in this architecture poses a challenge as quantifier instantiations can dynamically introduce boolean structure in the formula, requiring a tighter interleaving between propositional and theory reasoning.               This paper proposes handling quantifiers by using two SAT solvers, thereby separating the propositional reasoning of the input formula from that of the instantiated formulas. This technique can then reduce the propositional search space, as the paper\u00a0\u2026", "num_citations": "15\n", "authors": ["1626"]}
{"title": "CHET: compiler and runtime for homomorphic evaluation of tensor programs\n", "abstract": " Fully Homomorphic Encryption (FHE) refers to a set of encryption schemes that allow computations to be applied directly on encrypted data without requiring a secret key. This enables novel application scenarios where a client can safely offload storage and computation to a third-party cloud provider without having to trust the software and the hardware vendors with the decryption keys. Recent advances in both FHE schemes and implementations have moved such applications from theoretical possibilities into the realm of practicalities. This paper proposes a compact and well-reasoned interface called the Homomorphic Instruction Set Architecture (HISA) for developing FHE applications. Just as the hardware ISA interface enabled hardware advances to proceed independent of software advances in the compiler and language runtimes, HISA decouples compiler optimizations and runtimes for supporting FHE applications from advancements in the underlying FHE schemes. This paper demonstrates the capabilities of HISA by building an end-to-end software stack for evaluating neural network models on encrypted data. Our stack includes an end-to-end compiler, runtime, and a set of optimizations. Our approach shows generated code, on a set of popular neural network architectures, is faster than hand-optimized implementations.", "num_citations": "14\n", "authors": ["1626"]}
{"title": "Modeling software behavior using learned predicates\n", "abstract": " The described implementations relate to analysis of computing programs. One implementation provides a technique that can include accessing values of input variables that are processed by test code and runtime values that are produced by the test code while processing the input variables. The technique can also include modeling relationships between the runtime values and the values of the input variables. The relationships can reflect discontinuous functions of the input variables.", "num_citations": "14\n", "authors": ["1626"]}
{"title": "Can you fool me? towards automatically checking protocol gullibility\n", "abstract": " We consider the task of automatically evaluating protocol gullibility, that is, the ability of some of the participants to subvert the protocol without the knowledge of the others. We explain how this problem can be formalized as a game between honest and manipulative participants. We identify the challenges underlying this problem and outline several techniques to address them. Finally, we describe the design of a preliminary prototype for checking protocol gullibility and show that it can uncover vulnerabilities in the ECN protocol. 1.", "num_citations": "14\n", "authors": ["1626"]}
{"title": "An efficient Nelson-Oppen decision procedure for difference constraints over rationals\n", "abstract": " Nelson and Oppen provided a methodology for modularly combining decision procedures for individual theories to construct a decision procedure for a combination of theories. In addition to providing a check for satisfiability, the individual decision procedures need to provide additional functionalities, including equality generation. In this paper, we propose a decision procedure for a conjunction of difference constraints over rationals (where the atomic formulas are of the form x\u2a7d y+ c or x< y+ c). The procedure extends any negative cycle detection algorithm (like the Bellman-Ford algorithm) to generate (1) equalities between all pair of variables,(2) produce proofs and (3) generates models that can be extended by other theories in a Nelson-Oppen framework. All the operations mentioned above can be performed with only a linear overhead to the cycle detection algorithm, in the average case.", "num_citations": "14\n", "authors": ["1626"]}
{"title": "Modular difference logic is hard\n", "abstract": " In connection with machine arithmetic, we are interested in systems of constraints of the form x + k \\leq y + k'. Over integers, the satisfiability problem for such systems is polynomial time. The problem becomes NP complete if we restrict attention to the residues for a fixed modulus N.", "num_citations": "13\n", "authors": ["1626"]}
{"title": "Efficient parallelization using rank convergence in dynamic programming algorithms\n", "abstract": " This paper proposes an efficient parallel algorithm for an important class of dynamic programming problems that includes Viterbi, Needleman--Wunsch, Smith--Waterman, and Longest Common Subsequence. In dynamic programming, the subproblems that do not depend on each other, and thus can be computed in parallel, form stages, or wavefronts. The algorithm presented in this paper provides additional parallelism allowing multiple stages to be computed in parallel despite dependences among them. The correctness and the performance of the algorithm relies on rank convergence properties of matrix multiplication in the tropical semiring, formed with plus as the multiplicative operation and max as the additive operation. This paper demonstrates the efficiency of the parallel algorithm by showing significant speedups on a variety of important dynamic programming problems. In particular, the parallel Viterbi\u00a0\u2026", "num_citations": "12\n", "authors": ["1626"]}
{"title": "Fusing effectful comprehensions\n", "abstract": " List comprehensions provide a powerful abstraction mechanism for expressing computations over ordered collections of data declaratively without having to use explicit iteration constructs. This paper puts forth effectful comprehensions as an elegant way to describe list comprehensions that incorporate loop-carried state. This is motivated by operations such as compression/decompression and serialization/deserialization that are common in log/data processing pipelines and require loop-carried state when processing an input stream of data. We build on the underlying theory of symbolic transducers to fuse pipelines of effectful comprehensions into a single representation, from which efficient code can be generated. Using background theory reasoning with an SMT solver, our fusion and subsequent reachability based branch elimination algorithms can significantly reduce the complexity of the fused pipelines. Our\u00a0\u2026", "num_citations": "11\n", "authors": ["1626"]}
{"title": "Troubleshooting transiently-recurring errors in production systems with blame-proportional logging\n", "abstract": " Many problems in production systems are transiently recurring\u2014they occur rarely, but when they do, they recur for a short period of time. Troubleshooting these problems is hard as they are rare enough to be missed by sampling techniques and traditional postmortem analyses of runtime logs suffers either from low-fidelity of logging too little or from the overhead of logging too much.", "num_citations": "10\n", "authors": ["1626"]}
{"title": "Sc-haskell: Sequential consistency in languages that minimize mutable shared heap\n", "abstract": " A core, but often neglected, aspect of a programming language design is its memory (consistency) model. Sequential consistency~(SC) is the most intuitive memory model for programmers as it guarantees sequential composition of instructions and provides a simple abstraction of shared memory as a single global store with atomic read and writes. Unfortunately, SC is widely considered to be impractical due to its associated performance overheads.  Perhaps contrary to popular opinion, this paper demonstrates that SC is achievable with acceptable performance overheads for mainstream languages that minimize mutable shared heap. In particular, we modify the Glasgow Haskell Compiler to insert fences on all writes to shared mutable memory accessed in nonfunctional parts of the program. For a benchmark suite containing 1,279 programs, SC adds a geomean overhead of less than 0.4\\% on an x86 machine\u00a0\u2026", "num_citations": "10\n", "authors": ["1626"]}
{"title": "Low-rank methods for parallelizing dynamic programming algorithms\n", "abstract": " This article proposes efficient parallel methods for an important class of dynamic programming problems that includes Viterbi, Needleman-Wunsch, Smith-Waterman, and Longest Common Subsequence. In dynamic programming, the subproblems that do not depend on each other, and thus can be computed in parallel, form stages or wavefronts. The methods presented in this article provide additional parallelism allowing multiple stages to be computed in parallel despite dependencies among them. The correctness and the performance of the algorithm relies on rank convergence properties of matrix multiplication in the tropical semiring, formed with plus as the multiplicative operation and max as the additive operation. This article demonstrates the efficiency of the parallel algorithm by showing significant speedups on a variety of important dynamic programming problems. In particular, the parallel Viterbi decoder\u00a0\u2026", "num_citations": "10\n", "authors": ["1626"]}
{"title": "Accelerating sequential consistency for Java with speculative compilation\n", "abstract": " A memory consistency model (or simply a memory model) specifies the granularity and the order in which memory accesses by one thread become visible to other threads in the program. We previously proposed the volatile-by-default (VBD) memory model as a natural form of sequential consistency (SC) for Java. VBD is significantly stronger than the Java memory model (JMM) and incurs relatively modest overheads in a modified HotSpot JVM running on Intel x86 hardware. However, the x86 memory model is already quite close to SC. It is expected that the cost of VBD will be much higher on the other widely used hardware platform today, namely ARM, whose memory model is very weak.", "num_citations": "9\n", "authors": ["1626"]}
{"title": "Top: A framework for enabling algorithmic optimizations for distance-related problems\n", "abstract": " This paper introduces an abstraction to enable unified treatment to distance-related problems. It offers the first set of principled understanding to automatic algorithmic optimizations to such problems. It describes TOP, the first software framework that is able to automatically produce optimized algorithms either matching or outperforming manually designed algorithms for solving distance-related problems.", "num_citations": "9\n", "authors": ["1626"]}
{"title": "Improving the responsiveness of internet services with automatic cache placement\n", "abstract": " The backends of today's Internet services rely heavily on caching at various layers both to provide faster service to common requests and to reduce load on back-end components. Cache placement is especially challenging given the diversity of workloads handled by widely deployed Internet services. This paper presents TOOL, an analysis technique that automatically optimizes cache placement. Our experiments have shown that near-optimal cache placements vary significantly based on input distribution.", "num_citations": "9\n", "authors": ["1626"]}
{"title": "CMC: A model checker for network protocol implementations\n", "abstract": " Network protocols are hard to test due to the many nondeterministic events they handle. Protocol implementers must carefully handle all possible events (timeouts, packet losses, network failures) in all possible protocol and network states. Conventional testing can explore only a minute fraction of the exponential number of such combinations, leaving a residue of errors even in well-tested systems.", "num_citations": "9\n", "authors": ["1626"]}
{"title": "Determining a likelihood of a resource experiencing a problem based on telemetry data\n", "abstract": " Described herein is a system that transmits and combines local models, that individually comprise a set of local parameters computed via stochastic gradient descent (SGD), into a global model that comprises a set of global model parameters. The local models are computed in parallel at different geographic locations along with symbolic representations. Network transmission of the local models and the symbolic representations, rather than transmission of the large training data subsets processed to compute the local models and symbolic representations, conserves resources and decreases latency. The global model can then be used as a model to determine a likelihood of a monitored resource or a user of the monitored resource experiencing a problem with respect to performance or completion of one or more operations. The system can also implement an action to assist in resolving or avoiding the problem.", "num_citations": "8\n", "authors": ["1626"]}
{"title": "Determining a likelihood of a user interaction with a content element\n", "abstract": " Described herein is a system that transmits and combines local models, that individually comprise a set of local parameters computed via stochastic gradient descent (SGD), into a global model that comprises a set of global model parameters. The local models are computed in parallel at different geographic locations along with symbolic representations. The symbolic representations can be used to combine the local models. The global model can determine a likelihood, given a new data instance of a feature set, that a user performs a computer interaction with the content element. For instance, the system can use the model to provide search results in response to a search query submitted by a user. Or, the system can use the model to make a recommendation or suggestion to a user in response to a request for content (eg, display a targeted advertisement, suggest a news story, etc.).", "num_citations": "7\n", "authors": ["1626"]}
{"title": "Implementing network security measures in response to a detected cyber attack\n", "abstract": " Described herein is a system transmits and combines local models, that individually include a set of local parameters computed via stochastic gradient descent (SGD), into a global model that includes a set of global model parameters. The local models are computed in parallel at different geographic locations (eg, different instances of computing infrastructure) along with symbolic representations. Network transmission of the local models and the symbolic representations, rather than transmission of the large training data subsets processed to compute the local models and symbolic representations, conserves resources and decreases latency. The global model can then be used as a model to determine a likelihood that at least a portion of current and/or recently received data traffic is illegitimate data traffic that is associated with a cyber attack. In some instances, the system can implement a remedial action to\u00a0\u2026", "num_citations": "7\n", "authors": ["1626"]}
{"title": "Static analysis for optimizing big data queries\n", "abstract": " Query languages for big data analysis provide user extensibility through a mechanism of user-defined operators (UDOs). These operators allow programmers to write proprietary functionalities on top of a relational query skeleton. However, achieving effective query optimization for such languages is extremely challenging since the optimizer needs to understand data dependencies induced by UDOs. SCOPE, the query language from Microsoft, allows for hand coded declarations of UDO data dependencies. Unfortunately, most programmers avoid using this facility since writing and maintaining the declarations is tedious and error-prone. In this work, we designed and implemented two sound and robust static analyses for computing UDO data dependencies. The analyses can detect what columns of an input table are never used or pass-through a UDO unchanged. This information can be used to significantly\u00a0\u2026", "num_citations": "7\n", "authors": ["1626"]}
{"title": "Niijima: sound and automated computation consolidation for efficient multilingual data-parallel pipelines\n", "abstract": " Multilingual data-parallel pipelines, such as Microsoft's Scope and Apache Spark, are widely used in real-world analytical tasks. While the involvement of multiple languages (often including both managed and native languages) provides much convenience in data manipulation and transformation, it comes at a performance cost---managed languages need a managed runtime, incurring much overhead. In addition, each switch from a managed to a native runtime (and vice versa) requires marshalling or unmarshalling of an ocean of data objects, taking a large fraction of the execution time. This paper presents Niijima, an optimizing compiler for Microsoft's Scope/Cosmos, which can consolidate C#-based user-defined operators (UDOs) across SQL statements, thereby reducing the number of dataflow vertices that require the managed runtime, and thus the amount of C# computations and the data marshalling cost\u00a0\u2026", "num_citations": "6\n", "authors": ["1626"]}
{"title": "Semantics-preserving parallelization of stochastic gradient descent\n", "abstract": " Stochastic gradient descent (SGD) is a well-known method for regression and classification tasks. However, it is an inherently sequential algorithm - at each step, the processing of the current example depends on the parameters learned from previous examples. Prior approaches to parallelizing linear learners using SGD, such as Hogwild! and AllReduce, do not honor these dependencies across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SymSGD, a parallel SGD algorithm that, to a first-order approximation, retains the sequential semantics of SGD. Each thread learns a local model in addition to a model combiner, which allows local models to be combined to produce the same result as what a sequential SGD would have produced. This paper evaluates SymSGD's accuracy and performance on 6 datasets on a shared-memory machine shows up-to\u00a0\u2026", "num_citations": "6\n", "authors": ["1626"]}
{"title": "Failure sketches: A better way to debug\n", "abstract": " One of the main reasons debugging is hard and time consuming is that existing debugging tools do not provide an explanation for the root causes of failures. Additionally, existing techniques either rely on expensive runtime recording or assume existence of a given program input that reliably reproduces the failure, which makes them hard to apply in production scenarios. Consequently, developers spend precious time chasing elusive bugs, resulting in productivity loss.", "num_citations": "6\n", "authors": ["1626"]}
{"title": "Developing and Maintaining High Performance Network Services\n", "abstract": " A network service runtime module executing on a processor is configured to accept a directed acyclic service graph representing elements of a network service application. During execution of the service graph, runtime events are stored. The service graph may by optimized by generating alternate service graphs, and simulating performance of the alternate service graphs in a simulator using the stored runtime events. A hill climber algorithm may be used in conjunction with the simulator to vary alternate service graphs and determine which alternate service graphs provide the greatest utility. Once determined, an alternate service graph with the greatest utility may be loaded into the network service runtime module for execution.", "num_citations": "6\n", "authors": ["1626"]}
{"title": "Parallel stochastic gradient descent with sound combiners\n", "abstract": " Stochastic gradient descent (SGD) is a well known method for regression and classification tasks. However, it is an inherently sequential algorithm at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing linear learners using SGD, such as HOGWILD! and ALLREDUCE, do not honor these dependencies across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SYMSGD, a parallel SGD algorithm that, to a first-order approximation, retains the sequential semantics of SGD. Each thread learns a local model in addition to a model combiner, which allows local models to be combined to produce the same result as what a sequential SGD would have produced. This paper evaluates SYMSGD's accuracy and performance on 6 datasets on a shared-memory machine shows upto 11x speedup over our heavily optimized sequential baseline on 16 cores and 2.2x, on average, faster than HOGWILD!.", "num_citations": "5\n", "authors": ["1626"]}
{"title": "Data collisions in concurrent programs\n", "abstract": " Described are techniques for detecting data collisions between a first portion and a second portion of an application executing on a computer, the first portion and the second portions executing concurrently with respect to each other. While the first portion and second portion are executing, before the first portion accesses a memory location shared by the first portion and the second portion, a value stored in the memory location is captured and the first portion is delayed. While the second portion continues to execute the first portion is delayed. After a period of the first portion having been paused or slowed, the current content of the memory location is compared with the captured content to determine if there is a data collision. The first and second portions may be threads, and the capturing, delaying, and determining may be performed by code inserted to the application after it has been compiled.", "num_citations": "5\n", "authors": ["1626"]}
{"title": "Synthesizing optimal collective algorithms\n", "abstract": " Collective communication algorithms are an important component of distributed computation. Indeed, in the case of deep-learning, collective communication is the Amdahl's bottleneck of data-parallel training.", "num_citations": "4\n", "authors": ["1626"]}
{"title": "Fluxo: a system for internet service programming by non-expert developers\n", "abstract": " Over the last 10-15 years, our industry has developed and deployed many large-scale Internet services, from e-commerce to social networking sites, all facing common challenges in latency, reliability, and scalability. Over time, a relatively small number of architectural patterns have emerged to address these challenges, such as tiering, caching, partitioning, and pre-or post-processing compute intensive tasks. Unfortunately, following these patterns requires developers to have a deep understanding of the trade-offs involved in these patterns as well as an end-to-end understanding of their own system and its expected workloads. The result is that non-expert developers have a hard time applying these patterns in their code, leading to low-performing, highly suboptimal applications.", "num_citations": "4\n", "authors": ["1626"]}
{"title": "Memory model safety of programs\n", "abstract": " Concurrency is pervasive in all systems software, including operating systems, databases, and web servers. With the future hardware performance improvements coming mainly from additional parallelism in the hardware, system designers will be forced make their programs more concurrent to exploit this trend. A particular problem that programmers face when writing concurrent programs is to ensure correctness in the presence of memory reordering caused by the underlying hardware or the compiler. Such ordering relaxations are invisible to a single-threaded program. However, a concurrent program may exhibit more executions on a relaxed model than on a sequentially consistent (SC) machine. This additional behavior can result in subtle bugs that are very hard to find, understand, and debug.One way to shield a programmer from these relaxations is to use appropriate concurrency abstractions, such as locks and transactional memory, provided either by a library or the compiler. Most programs will (and should) use such high-level concurrency abstractions. However, it is our position that a class of programs will still bypass these abstractions and use ad-hoc synchronization techniques. Such ad-hoc techniques include making direct use of hardware primitives for atomic operations (such as interlocked exchange, or compare-and-swap) and employing regular loads and stores for synchronization purposes. As such, these programs will be exposed to the effects of the relaxed memory model. We foresee three kinds of programs that use such ad-hoc synchronization techniques. First, the libraries that implement highlevel concurrency\u00a0\u2026", "num_citations": "4\n", "authors": ["1626"]}
{"title": "Determining a course of action based on aggregated data\n", "abstract": " Described herein is a system that transmits and combines local models, that individually comprise a set of local parameters computed via stochastic gradient descent (SGD), into a global model that comprises a set of global model parameters. The local models are computed in parallel at different geographic locations along with symbolic representations. Network transmission of the local models and the symbolic representations, rather than transmission of the large training data subsets processed to compute the local models and symbolic representations, conserves resources and decreases latency. The global model can then be used as a model to determine a likelihood of a course of action being successful for an organization. For example, the course of action can be a purchase of a security or a business operation strategy. In another example, the course of action can be a type of medical treatment for a patient.", "num_citations": "3\n", "authors": ["1626"]}
{"title": "System and methods for optimal error detection in programmatic environments\n", "abstract": " System and methods are provided for optimal error detection in programmatic environments through the utilization of at least one user-defined condition. Illustratively, the conditions can include one or more triggers initiating the collection of log data for methods associated with the provided at least one condition. Operatively, the disclosed systems and methods observe the run-time of the programmatic environment and initiate the collection of log data based on the occurrence of a condition trigger. A rank score can also be calculated to rank the methods associated with the defined condition to isolate those methods that have higher probability of causing the defined condition. Dynamic instrumentation of the methods associated with the user defined conditions during run time are used to calculate the rank score, which is used for ranking the methods.", "num_citations": "3\n", "authors": ["1626"]}
{"title": "Parallel dynamic programming through rank convergence\n", "abstract": " The techniques and/or systems described herein implement parallel processing of a dynamic programming problem across stages and/or clusters by breaking dependencies between stages and/or clusters. For instance, the techniques and/or systems may identify dependencies between sub-problems of the dynamic programming problem and group the sub-problems into stages. The techniques and/or systems may also group the stages into clusters (eg, at least two clusters to be parallel processed). Then, the techniques and/or systems generate one or more solutions to use instead of actual solutions so that the dynamic programming problem can be parallel processed across stages and/or clusters.", "num_citations": "3\n", "authors": ["1626"]}
{"title": "Dynamic analyses for data-race detection\n", "abstract": " Data races caused by unsynchronized accesses to shared data have long been the source of insidious errors in concurrent software. They are hard to identify during testing, reproduce, and debug. Recent advances in race detection tools show great promise for improving the situation, however, and can enable programmers to find and eliminate race conditions more effectively. This tutorial explores dynamic analysis techniques to efficiently find data races in large-scale software. It covers the theoretical underpinnings, implementation techniques, and reusable infrastructure used to build state-of-the-art data-race detectors (as well as analyses targeting other types of concurrency errors). The tutorial provides industrial case studies on finding data races and closes with a discussion of open research questions in this area.", "num_citations": "3\n", "authors": ["1626"]}
{"title": "Back to the future: Forecasting program behavior in automated homes\n", "abstract": " Networked devices such as locks and thermostats are now cheaply available, which is accelerating the adoption of home automation. But unintended behaviors of programs that control automated homes can pose severe problems, including security risks (eg, accidental disarming of alarms). We facilitate predictable control of automated homes by letting users \u201cfast forward\u201d their program and observe its possible future behaviors under different sequences of inputs and environmental conditions. A key challenge that we face, which is not addressed by existing program exploration techniques, is to systematically handle time because home automation programs depend intimately on absolute and relative timing of inputs. We develop an approach that models programs as timed automata and incorporates novel mechanisms to enable scalable and comprehensive exploration of their behavior. We implement our approach in a tool called DeLorean and apply it to 10 real home automation programs. We find that it can fast forward these programs 3.6 times to 36K times faster than real time and uncover unintended behaviors in them.", "num_citations": "3\n", "authors": ["1626"]}
{"title": "Fluxo: A simple service compiler\n", "abstract": " In this paper, we propose FLUXO, a system that separates an Internet service's logical functionality from the architectural decisions made to support performance, scalability, and reliability. FLUXO achieves this separation through three mechanisms: 1) a coarse-grained dataflow-based programming model; 2) detailed runtime request tracing to capture workload distributions, performance behavior, and resource requirements; and 3) a set of analysis techniques that determine how to apply simple, parameterized dataflow transformations to optimize the service architecture for performance, scalability, and reliability. In this paper, we describe our vision for how to make Internet services easier to construct, and show how a variety of Internet service performance optimizations may be expressed as transformations applied to FLUXO programs.", "num_citations": "3\n", "authors": ["1626"]}
{"title": "FLUXO: A Simple Service Compiler\n", "abstract": " In this paper, we propose FLUXO, a system that separates an Internet service\u2019s logical functionality from the architectural decisions made to support performance, scalability, and reliability. FLUXO achieves this separation through three mechanisms: 1) a coarse-grained dataflowbased programming model; 2) detailed runtime request tracing to capture workload distributions, performance behavior, and resource requirements; and 3) a set of analysis techniques that determine how to apply simple, parameterized dataflow transformations to optimize the service architecture for performance, scalability, and reliability. In this paper, we describe our vision for how to make Internet services easier to construct, and show how a variety of Internet service performance optimizations may be expressed as transformations applied to FLUXO programs.", "num_citations": "3\n", "authors": ["1626"]}
{"title": "Towards general-purpose resource management in shared cloud services\n", "abstract": " In distributed services shared by multiple tenants, managing resource allocation is an important pre-requisite to providing dependability and quality of service guarantees. Many systems deployed today experience contention, slowdown, and even system outages due to aggressive tenants and a lack of resource management. Improperly throttled background tasks, such as data replication, can overwhelm a system; conversely, high-priority background tasks, such as heartbeats, can be subject to resource starvation. In this paper, we outline five design principles necessary for e ffective and effi cient resource management policies that could provide guaranteed performance, fairness, or isolation. We present Retro, a resource instrumentation framework that is guided by these principles. Retro instruments all system resources and exposes detailed, real-time statistics of pertenant resource consumption, and could serve as a base for the implementation of such policies.", "num_citations": "2\n", "authors": ["1626"]}
{"title": "Safe-by-default Concurrency for Modern Programming Languages\n", "abstract": " Modern \u201csafe\u201d programming languages follow a design principle that we call safety by default and performance by choice. By default, these languages enforce important programming abstractions, such as memory and type safety, but they also provide mechanisms that allow expert programmers to explicitly trade some safety guarantees for increased performance. However, these same languages have adopted the inverse design principle in their support for multithreading. By default, multithreaded programs violate important abstractions, such as program order and atomic access to individual memory locations to admit compiler and hardware optimizations that would otherwise need to be restricted. Not only does this approach conflict with the design philosophy of safe languages, but very little is known about the practical performance cost of providing a stronger default semantics. In this article, we propose a safe\u00a0\u2026", "num_citations": "1\n", "authors": ["1626"]}
{"title": "Machine learning through parallelized stochastic gradient descent\n", "abstract": " Systems, methods, and computer media for machine learning through a symbolic, parallelized stochastic gradient descent (SGD) analysis are provided. An initial data portion analyzer can be configured to perform, using a first processor, SGD analysis on an initial portion of a training dataset. Values for output model weights for the initial portion are initialized to concrete values. Local model builders can be configured to perform, using an additional processor for each local model builder, symbolic SGD analysis on an additional portion of the training dataset. The symbolic SGD analysis uses a symbolic representation as an initial state for output model weights for the corresponding portions of the training dataset. The symbolic representation allows the SGD analysis and symbolic SGD analysis to be performed in parallel. A global model builder can be configured to combine outputs of the local model builders and the\u00a0\u2026", "num_citations": "1\n", "authors": ["1626"]}
{"title": "BigTest: a symbolic execution based systematic test generation tool for Apache spark\n", "abstract": " Data-intensive scalable computing (DISC) systems such as Google\u2019s MapReduce, Apache Hadoop, and Apache Spark are prevalent in many production services. Despite their popularity, the quality of DISC applications suffers due to a lack of exhaustive and automated testing. Current practices of testing DISC applications are limited to using a small random sample of the entire input dataset which merely exposes any program faults. Unlike SQL queries, testing DISC applications has new challenges due to a composition of both dataflow and relational operators, and user-defined functions (UDF) that could be arbitrarily long and complex.To address this problem, we demonstrate a new white-box testing framework called BigTest that takes an Apache Spark program as input and automatically generates synthetic, concrete data for effective and efficient testing. BigTest combines the symbolic execution of UDFs with\u00a0\u2026", "num_citations": "1\n", "authors": ["1626"]}
{"title": "Cross-language optimizations in big data systems: A case study of scope\n", "abstract": " Building scalable big data programs currently requires programmers to combine relational (SQL) with non-relational code (Java, C#, Scala). Relational code is declarative-a program describes what the computation is and the compiler decides how to distribute the program. SQL query optimization has enjoyed a rich and fruitful history, however, most research and commercial optimization engines treat non-relational code as a black-box and thus are unable to optimize it.", "num_citations": "1\n", "authors": ["1626"]}
{"title": "DRFx An Understandable, High Performance, and Flexible Memory Model for Concurrent Languages\n", "abstract": " The most intuitive memory model for shared-memory multi-threaded programming is sequential consistency (SC), but it disallows the use of many compiler and hardware optimizations and thus affects performance. Data-race-free (DRF) models, such as the C++11 memory model, guarantee SC execution for data-race-free programs. But these models provide no guarantee at all for racy programs, compromising the safety and debuggability of such programs. To address the safety issue, the Java memory model, which is also based on the DRF model, provides a weak semantics for racy executions. However, this semantics is subtle and complex, making it difficult for programmers to reason about their programs and for compiler writers to ensure the correctness of compiler optimizations. We present the drfx memory model, which is simple for programmers to understand and use while still supporting many common\u00a0\u2026", "num_citations": "1\n", "authors": ["1626"]}
{"title": "Testing components for thread safety\n", "abstract": " A checking system is described for determining whether a component is thread safe in the course of interacting with two or threads in a client environment. The checking system uses a manual, automatic, or semi-automatic technique to generate a test. The checking system then defines a set of coarse-grained observations for the test, in which the component is assumed to exhibit linearizability when interacting with threads. The set of coarse-grained observations may include both complete and \u201cstuck\u201d histories. The checking system then generates a set of fine-grained observations for the tests; here, the checking system makes no assumptions as to the linearizability of the component. The checking system identifies potential linearizability errors as those entries in the set of fine-grained observations that have no counterpart entries in the set of coarse-grained observations. The checking system may rely on a\u00a0\u2026", "num_citations": "1\n", "authors": ["1626"]}
{"title": "A safety-first approach to memory models\n", "abstract": " Recent efforts to standardize concurrency semantics for programming languages require programmers to explicitly annotate all memory accesses that can participate in a data race (\"unsafe\" accesses). This requirement allows the compiler and hardware to aggressively optimize unannotated accesses, which are assumed to be data-race-free (\"safe\" accesses), while still preserving the intuitive thread interleaving semantics known as sequential consistency (SC). However, unannotated data races are easy for programmers to accidentally introduce and difficult to detect, and thus the safety and correctness of programs can be significantly compromised. The authors argue instead for a safety-first approach, whereby the compiler and hardware treat every memory access as potentially unsafe unless it is proven otherwise. In this way, SC semantics is guaranteed for all programs, whether data-race-free or not. The authors\u00a0\u2026", "num_citations": "1\n", "authors": ["1626"]}
{"title": "Model Checking Software: 18th International SPIN Workshop, Snowbird, UT, USA, July 14-15, 2011, Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the 18th International SPIN workshop on Model Checking Software, SPIN 2011, held in Snowbird, UT, USA, in July 2011. The 10 revised full papers presented together with 2 tool demonstration papers and 1 invited contribution were carefully reviewed and selected from 29 submissions. The papers are organized in topical sections on abstractions and state-space reductions; search strategies; PROMELA encodings and extensions; and applications of model checking.", "num_citations": "1\n", "authors": ["1626"]}