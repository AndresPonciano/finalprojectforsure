{"title": "Design science research in information systems\n", "abstract": " Design Science Research in Information SystemsAcknowledgements: We would like to thank Allen Lee, Ron Weber, and Gordon Davis who in different ways each contributed to our thinking about design science in the Information Systems profession and encouraged us to pursue this line of research. We would also like to acknowledge the efforts of Rosann Collins who provided insightful comments and perspectives on the nature of the relationship between behavioral and design science research.", "num_citations": "16298\n", "authors": ["37"]}
{"title": "Positioning and presenting design science research for maximum impact\n", "abstract": " Design science research (DSR) has staked its rightful ground as an important and legitimate Information Systems (IS) research paradigm. We contend that DSR has yet to attain its full potential impact on the development and use of information systems due to gaps in the understanding and application of DSR concepts and methods. This essay aims to help researchers (1) appreciate the levels of artifact abstractions that may be DSR contributions, (2) identify appropriate ways of consuming and producing knowledge when they are preparing journal articles or other scholarly works, (3) understand and position the knowledge contributions of their research projects, and (4) structure a DSR article so that it emphasizes significant contributions to the knowledge base. Our focal contribution is the DSR knowledge contribution framework with two dimensions based on the existing state of knowledge in both the problem and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2824\n", "authors": ["37"]}
{"title": "Design science research in information systems\n", "abstract": " Design activities are central to most applied disciplines. Research in design has a long history in many fields including architecture, engineering, education, psychology, and the fine arts (Cross 2001). The computing and information technology (CIT) field since its advent in the late 1940s has appropriated many of the ideas, concepts, and methods of design science that have originated in these other disciplines. However, information systems (IS) as composed of inherently mutable and adaptable hardware, software, and human interfaces provide many unique and challenging design problems that call for new and creative ideas.", "num_citations": "2648\n", "authors": ["37"]}
{"title": "A three cycle view of design science research\n", "abstract": " As a commentary to Juhani Iivari\u0393\u00c7\u00d6s insightful essay, I briefly analyze design science research as an embodiment of three closely related cycles of activities. The Relevance Cycle inputs requirements from the contextual environment into the research and introduces the research artifacts into environmental field testing. The Rigor Cycle provides grounding theories and methods along with domain experience and expertise from the foundations knowledge base into the research and adds the new knowledge generated by the research to the growing knowledge base. The central Design Cycle supports a tighter loop of research activity for the construction and evaluation of design artifacts and processes. The recognition of these three cycles in a research project clearly positions and differentiates design science from other research paradigms. The commentary concludes with a claim to the pragmatic nature of design\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2454\n", "authors": ["37"]}
{"title": "The category concept: An extension to the entity-relationship model\n", "abstract": " An enhanced version of the Entity-Relationship (ER) data model called the Entity-Category-Relationship (ECR) data model is presented. The principal extension is the introduction of the concept of a category. Categories permit the grouping of entities from different entity types according to the roles they play in a relationship, as well as the representation of ISA and generalization hierarchies. The structures of the ECR data model are defined, and a graphic representation technique for their display is presented. Language operations to define and use an ECR database are defined. Two realistic examples of the use of the ECR model for database design are demonstrated. The examples show how ECR structures can be directly mapped into relational and network structures. The definition of derived relationships on an ECR database gives the power to phrase higher order recursive queries in a first order query\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "415\n", "authors": ["37"]}
{"title": "Integrated decision support systems: A data warehousing perspective\n", "abstract": " Successfully supporting managerial decision-making is critically dependent upon the availability of integrated, high quality information organized and presented in a timely and easily understood manner. Data warehouses have emerged to meet this need. They serve as an integrated repository for internal and external data\u0393\u00c7\u00f6intelligence critical to understanding and evaluating the business within its environmental context. With the addition of models, analytic tools, and user interfaces, they have the potential to provide actionable information resources\u0393\u00c7\u00f6business intelligence that supports effective problem and opportunity identification, critical decision-making, and strategy formulation, implementation, and evaluation. Four themes frame our analysis: integration, implementation, intelligence, and innovation.", "num_citations": "410\n", "authors": ["37"]}
{"title": "Optimization algorithms for distributed queries\n", "abstract": " The efficiency of processing strategies for queries in a distributed database is critical for system performance. Methods are studied to minimize the response time and the total time for distributed queries. A new algorithm (Algorithm GENERAL) is presented to derive processing strategies for arbitrarily complex queries. Three versions of the algorithm are given: one for minimizing response time and two for minimizing total time. The algorithm is shown to provide optimal solutions under certain conditions.", "num_citations": "368\n", "authors": ["37"]}
{"title": "Query processing in distributed database system\n", "abstract": " Query processing in a distributed system requires the transmission f data between computers in a network. The arrangement of data transmissions and local data processing is known as a distribution strategy for a query. Two cost measures, response time and total time are used to judge the quality of a distribution strategy. Simple algorithms are presented that derive distribution strategies which have minimal response time and minimal total time, for a special class of queries. These optimal algorithms are used as a basis to develop a general query processing algorithm. Distributed query examples are presented and the complexity of the general algorithm is analyzed. The integration of a query processing subsystem into a distributed database management system is discussed.", "num_citations": "322\n", "authors": ["37"]}
{"title": "Focus groups for artifact refinement and evaluation in design research\n", "abstract": " Focus groups to investigate new ideas are widely used in many research fields. The use of focus groups in design research poses interesting opportunities and challenges. Traditional focus group methods must be adapted to meet two specific goals of design research. For the refinement of an artifact design, exploratory focus groups (EFGs) study the artifact to propose improvements in the design. The cycle of build and evaluate using EFGs continues until the artifact is released for field test in the application environment. Then, the field test of the design artifact may employ confirmatory focus groups (CFGs) to establish the utility of the artifact in field use. Rigorous investigation of the artifact requires multiple CFGs to be run with opportunities for quantitative and qualitative data collection and analyses across the multiple CFGs. In this paper, we discuss the adaptation of focus groups to design research projects. We demonstrate the use of both EFGs and CFGs in a design research project in the health care field.", "num_citations": "309\n", "authors": ["37"]}
{"title": "Design science research contributions: Finding a balance between artifact and theory\n", "abstract": " With the rising interest in Design Science Research (DSR), it is crucial to engage in the ongoing debate on what constitutes an acceptable contribution for publishing DSR-the design artifact, the design theory, or both. In this editorial, we provide some constructive guidance across different positioning statements with actionable recommendations for DSR authors and reviewers. We expect this editorial to serve as a foundational step towards clarifying misconceptions about DSR contributions and to pave the way for the acceptance of more DSR papers to top IS journals.", "num_citations": "264\n", "authors": ["37"]}
{"title": "Principles of information systems analysis and design\n", "abstract": " 1.1 Business Information Systems 1.1. 1 Business Systems and Information Systems 1.1. 2 Categories of Information Systems 1.1. 3 People in Information Systems 1.1. 4 Problems of Logic and People in Information Systems 1.2 Box Structures of Information Systems 1. 2.1 Historical Perspective 1. 2.2 System Structures 1.2. 3 Box Structures in Business Operations 1. 2.4 Box Structure Descriptions 1. 3 The US Navy Supply System Reorder Policy 1.3. 1 The Clear Box Formulation 1.3. 2 The State Machine Derivation 1. 3.3 The Black Box Derivation 1. 3.4 Analysis of the Reorder Policy 1.4 Managing Information Systems Development 1.4. 1 Box Structure Hierarchies 1. 4.2 Box Structure Derivation and Expansion 1.4. 3 The System Development Process 1.4. 4 Information Systems Integrity Exercises", "num_citations": "195\n", "authors": ["37"]}
{"title": "Research commentary: An agenda for information technology research in heterogeneous and distributed environments\n", "abstract": " Application-driven, technology-intensive research is critically needed to meet the challenges of globalization, interactivity, high productivity, and rapid adaptation faced by business organizations. Information systems researchers are uniquely positioned to conduct such research, combining computer science, mathematical modeling, systems thinking, management science, cognitive science, and knowledge of organizations and their functions. We present an agenda for addressing these challenges as they affect organizations in heterogeneous and distributed environments. We focus on three major capabilities enabled by such environments: Mobile Computing, Intelligent Agents, and Net-Centric Computing. We identify and define important unresolved problems in each of these areas and propose research strategies to address them.", "num_citations": "193\n", "authors": ["37"]}
{"title": "Control of flexible software development under uncertainty\n", "abstract": " When should software development teams have the flexibility to modify their directions and how do we balance that flexibility with controls essential to produce acceptable outcomes? We use dynamic capabilities theory and an extension of control theory to understand these questions. This work is examined in a case study. Our results demonstrate that flexibility may be needed when the starting conditions are uncertain and that effective control in these situations requires use of traditional controls plus a new type of control we term emergent outcome control.", "num_citations": "190\n", "authors": ["37"]}
{"title": "The information systems research cycle\n", "abstract": " What distinguishes information systems from closely aligned disciplines such as computer science, organizational science, management science, economics, or systems engineering? How does IS research balance the demands of relevance and rigor to make effective contributions to both theory and practice? As senior researchers in IS, the authors have engaged in many debates on these questions and have come to some conclusions about what makes this burgeoning field unique and how to properly plan, execute, and evaluate IS research as well as transition it into practice.", "num_citations": "186\n", "authors": ["37"]}
{"title": "Towards a NeuroIS research methodology: intensifying the discussion on methods, tools, and measurement\n", "abstract": " The genesis of the Neuro-Information Systems (NeuroIS) field took place in 2007. Since then, a considerable number of IS scholars and academics from related disciplines have started to use theories, methods, and tools from neuroscience and psychophysiology to better understand human cognition, emotion, and behavior in IS contexts, and to develop neuro-adaptive information systems (ie, systems that recognize the physiological state of the user and that adapt, based on that information, in real-time). However, because the NeuroIS field is still in a nascent stage, IS scholars need to become familiar with the methods, tools, and measurements that are used in neuroscience and psychophysiology. Against the background of the increased importance of methodological discussions in the NeuroIS field, the Journal of the Association for Information Systems published a special issue call for papers entitled \u0393\u00c7\u00a3Methods, tools, and measurement in NeuroIS research\u0393\u00c7\u00a5 in 2012. We, the special issue\u0393\u00c7\u00d6s guest editors, accepted three papers after a stringent review process, which appear in this special issue. In addition to these three papers, we hope to intensify the discussion on NeuroIS research methodology, and to this end we present the current paper. Importantly, our observations during the review process (particularly with respect to methodology) and our own reading of the literature and the scientific discourse during conferences served as input for this paper. Specifically, we argue that six factors, among others that will become evident in future discussions, are critical for a rigorous NeuroIS research methodology; namely, reliability, validity, sensitivity\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "160\n", "authors": ["37"]}
{"title": "Using function abstraction to understand program behavior\n", "abstract": " The authors explain how to understand programs by abstracting program function. This abstraction is made possible by the algebraic structure and mathematical properties of structured programs. They describe an abstraction algorithm that provides a basis for a tool for automatic abstraction of program functions. They also explore what the goals of a program-abstraction tool should be. A miniature Cobol program for a banking application is used as an example.< >", "num_citations": "160\n", "authors": ["37"]}
{"title": "FORMANAGER: An office forms management system\n", "abstract": " The form has become an important abstraction for data management in an office application environment. Structured office forms present data to users in an easily understood and easily manipulated manner. In this paper we classify forms systems in terms of three dimensions: data structuring, user interfaces, and programming interfaces. Current forms systems are analyzed under these dimensions. We have designed a comprehensive forms management system, FORMANAGER, that includes facilities for form specification, form processing, and form control. The system transforms data from a relational database into a hierarchical data structure which defines the form. The design and algorithms for implementation of the system are described, and future extensions to enhance the capabilities of forms systems are proposed.", "num_citations": "124\n", "authors": ["37"]}
{"title": "Healthcare data warehousing and quality assurance\n", "abstract": " Healthcare data warehousing presents unique challenges. The industry is rife with often incompatible medical standards and coding schemes that require careful translation. Healthcare data comes from many sources and is delivered in many forms, including published books, individual spreadsheets, and several tape or data formats. Results derived from a healthcare data warehouse must be delivered in accessible form to diverse stakeholders, including healthcare regulators, physicians, hospital administrators, consumers, community activists, and members of the popular press. The industry's widely decentralized and largely autonomous data collection efforts make data quality a significant challenge. Finally, the sensitivity of healthcare data makes privacy and security issues paramount. Healthcare data warehousing will make rigorous, quantitative information available to healthcare decision makers. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "122\n", "authors": ["37"]}
{"title": "A fitness-utility model for design science research\n", "abstract": " Current thinking in design science research (DSR) defines the usefulness of the design artifact in a relevant problem environment as the primary research goal. Here we propose a complementary evaluation model for DSR. Drawing from evolutionary economics, we define a fitness-utility model that better captures the evolutionary nature of design improvements and the essential DSR nature of searching for a satisfactory design across a fitness landscape. Our goal is to move DSR to more meaningful evaluations of design artifacts for sustainable impacts. A key premise of this new thinking is that the evolutionary fitness of a design artifact is more valuable than its immediate usefulness. We conclude with a discussion of the strengths and challenges of the fitness-utility model for the performance of rigorous and relevant DSR.", "num_citations": "116\n", "authors": ["37"]}
{"title": "The Catch data warehouse: support for community health care decision-making\n", "abstract": " The measurement and assessment of health status in communities throughout the world is a massive information technology challenge. Comprehensive Assessment for Tracking Community Health (CATCH) provides systematic methods for community-level assessment that is invaluable for resource allocation and health care policy formulation. CATCH is based on health status indicators from multiple data sources, using an innovative comparative framework and weighted evaluation process to produce a rank-ordered list of critical community health care challenges. The community-level focus is intended to empower local decision-makers by providing a clear methodology for organizing and interpreting relevant health care data. Extensive field experience with the CATCH methods, in combination with expertise in data warehousing technology, has led to an innovative application of information technology in the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "111\n", "authors": ["37"]}
{"title": "Reviewing software diagrams: A cognitive study\n", "abstract": " Reviews and inspections of software artifacts throughout the development life cycle are effective techniques for identifying defects and improving software quality. While review methods for text-based artifacts (e.g., code) are well understood, very little guidance is available for performing reviews of software diagrams, which are rapidly becoming the dominant form of software specification and design. Drawing upon human cognitive theory, we study how 12 experienced software developers perform individual reviews on a software design containing two types of diagrams: entity-relationship diagrams and data flow diagrams. Verbal protocol methods are employed to describe and analyze defect search patterns among the software artifacts, both text and diagrams, within the design. Results indicate that search patterns that rapidly switch between the two design diagrams are the most effective. These findings support\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "110\n", "authors": ["37"]}
{"title": "The use of focus groups in design science research\n", "abstract": " Focus groups to investigate new ideas are widely used in many research fields. The use of focus groups in design science research poses interesting opportunities and challenges. Traditional focus group methods must be adapted to meet two specific goals of design research. For the evaluation of an artifact design, exploratory focus groups (EFGs) study the artifact to propose improvements in the design. The results of the evaluation are used to refine the design and the cycle of build and evaluate using EFGs continues until the artifact is released for field test in the application environment. Then, the field test of the design artifact may employ confirmatory focus groups (CFGs) to establish the utility of the artifact in field use. Rigorous investigation of the artifact requires multiple CFGs to be run with opportunities for quantitative and qualitative data collection and analyses across the multiple CFGs. In this chapter\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "105\n", "authors": ["37"]}
{"title": "An elaborated action design research process model\n", "abstract": " This research essay proposes an elaborated process model for applying the action design research (ADR) approach to immersive industry-based projects. Building on the original ADR concepts, we identify four distinct types of ADR cycles for diagnosis, design, implementation, and evolution of the growing artefact-based solution. Each ADR cycle moves through activities of problem formulation, artefact creation, evaluation, reflection, and learning. Rapid iterations of ADR cycles provide a well-defined process map for managing and performing an emergent ADR project. The proposed model supports multiple entry points based on the current state of the problem environment and the goals of the ADR project. The elaborated ADR process model provides a more flexible yet disciplined inquiry into the initiation, conduct, reflection, and presentation of rigorous and relevant ADR projects.", "num_citations": "91\n", "authors": ["37"]}
{"title": "The impacts of quality and productivity perceptions on the use of software process improvement innovations\n", "abstract": " Numerous software process improvement (SPI) innovations have been proposed to improve software development productivity and system quality; however, their diffusion in practice has been disappointing. This research investigates the adoption of the Personal Software Process on industrial software projects. Quantitative and qualitative analyses reveal that perceived increases in software quality and development productivity, project management benefits, and innovation fit to development tasks, enhance the usefulness of the innovation to developers. Results underscore the need to enrich current technology acceptance models with these constructs, and serve to encourage project managers to adopt formal SPI methods if developers perceive the methods will have positive impacts on their productivity and system quality.", "num_citations": "86\n", "authors": ["37"]}
{"title": "Quality attributes in telemedicine video conferencing\n", "abstract": " Video conferencing is used increasingly in many telemedicine applications, including medical personnel education, peer consultation, patient education, and direct patient care. Advances in technology and changes in medical care delivery have enhanced the ability to develop effective telemedicine video conferencing systems. Measures of effectiveness for technology systems rely on identified requirements,for system quality. In this research, we propose a comprehensive model of quality attributes for telemedicine video conferencing systems. The quality attribute model is developed from an extensive literature review, direct observations of telemedicine encounters, and structured interviews with telemedicine experts. The model contains four quality attribute groups: Technical, Usability, Physical Environment, and Human Element. Interview citations are used to justify the importance of these individual quality\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "85\n", "authors": ["37"]}
{"title": "The sciences of design: observations on an emerging field\n", "abstract": " The boundaries and contours of design sciences continue to undergo definition and refinement. In many ways, the sciences of design defy disciplinary characterization. They demand multiple epistemologies, theoretical orientations (eg construction, analysis or intervention) and value considerations. As our understanding of this emerging field of study grows, we become aware that the sciences of design require a systemic perspective that spans disciplinary boundaries. The Doctoral Consortium at the Design Science Research Conference in Information Sciences and Technology (DESRIST) was an important milepost in their evolution. It provided a forum where students and leading researchers in the design sciences challenged one another to tackle topics and concerns that are similar across different disciplines. This paper reports on the consortium outcomes and insights from mentors who took part in it. We\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "81\n", "authors": ["37"]}
{"title": "It's more than just use: An exploration of telemedicine use quality\n", "abstract": " \u0393\u00c7\u00a3Simply saying that more use will yield more benefits without considering the nature of this use (and context) is clearly insufficient\u0393\u00c7\u00a5 [W.H. DeLone, E.R. McLean, The DeLone and McLean model of information system success: a ten-year update, Journal of Management Information Systems 19 (4) (2003) 9\u0393\u00c7\u00f430, p. 16]. Our research specifies the use quality construct in the context of a mission critical system deployment\u0393\u00c7\u00f6namely, the use of medical video conferencing for patient exams. The product of this field study is a socio-technical framework for use quality in telemedicine service encounters. We also propose generalized categories (which may extend across domains) for identified attributes, provide a comparative overview of patient and provider perspectives, and discuss the effects of and remedies for selected attribute deficiencies.", "num_citations": "78\n", "authors": ["37"]}
{"title": "Box structured information systems\n", "abstract": " The box structure methodology for information systems development is based on a usage hierarchy of data abstractions, in which each abstraction is defined in three distinct forms, called its black box, its state machine, and its clear box. Each of these three box structures defines identical external behavior, but with increasing internal visibility, to provide a hierarchical structure which supports the systems development principles of referential transparency, transaction closure, state migration, and common services. This hierarchy of box structures provides, in turn, a basis for orderly management of information systems development by a finite set of analysis and design tasks in a spiral development process. The methodology and its use are described.", "num_citations": "75\n", "authors": ["37"]}
{"title": "Introduction to the special issue on design science\n", "abstract": " The rigorous application of design science in information and communications technology (ICT) research is growing rapidly and producing exciting results. The five papers published in this special issue reflect some of the most recent ideas and research projects in ICT design science research (DSR). This introduction begins with concise summaries of the published papers. We then reflect on three key design science issues, using the published papers to illustrate our views. The three issues are: (1) the nature of the artifacts/problems studied in DSR in ICT disciplines; (2) the research approaches that are used; and (3) the nature of the research contributions that are made. We explain why we believe that these issues are interdependent and why thinking about these three issues as a whole can support an improved understanding of the goals and processes of design science research.", "num_citations": "71\n", "authors": ["37"]}
{"title": "Special Issue Editorial\u0393\u00c7\u00f4Accumulation and Evolution of Design Knowledge in Design Science Research: A Journey Through Time and Space\n", "abstract": " Sir Isaac Newton (1676) famously said,\u0393\u00c7\u00a3If I have seen further, it is by standing on the shoulders of giants.\u0393\u00c7\u00a5 Research is a collaborative, evolutionary endeavor\u0393\u00c7\u00f6and it is no different with design science research (DSR), which builds upon existing design knowledge and creates new design knowledge to pass on to future projects. However, despite the vast, growing body of DSR contributions, scant evidence of the accumulation and evolution of design knowledge has been articulated in an organized DSR body of knowledge. Most contributions rather stand on their own feet than on the shoulders of giants, and this continues to limit how far we can see, curtailing the extent of the broader impacts that can be made through DSR. In this editorial, we aim at providing guidance on how to position design knowledge contributions in wider problem and solution spaces. We propose (1) a model conceptualizing design knowledge as a resilient relationship between problem and solution spaces,(2) a model that demonstrates how individual DSR projects consume and produce design knowledge,(3) a map to position a design knowledge contribution in problem and solution spaces, and (4) principles on how to use this map in a DSR project. We show how fellow researchers, readers, editors, and reviewers, as well as the IS community as a whole, can make use of these proposals, and also illustrate future research opportunities.", "num_citations": "70\n", "authors": ["37"]}
{"title": "A four-cycle model of IS design science research: capturing the dynamic nature of IS artifact design\n", "abstract": " We propose to extend the well-known three-cycle view for design science research (DSR) with a fourth cycle (change and impact cycle) that captures the dynamic nature of IS artifact design for volatile environments. The appropriation of in-innovative designs results in organizational changes that happen outside the new artifacts' immediate application contexts. The intention behind introducing the fourth cycle is to integrate recent advances in the DSR discourse conceptually within the DSR cycle model. We critically review such recent advances and integrate them into an extended model. We show how this change and impact (CI) cycle adds an important facet to DSR to cope with dynamic application contexts as well as artifact-induced organizational change and the resulting need for follow-up design efforts. Iterations of the CI cycle represent the continuous design evolution required to keep up with changing organizational environments.", "num_citations": "69\n", "authors": ["37"]}
{"title": "A fitness-utility model for design science research\n", "abstract": " Current thinking in design science research (DSR) defines the usefulness of the design artifact in a relevant problem environment as the primary research goal. Here we propose a complementary evaluation model for DSR. Drawing from evolutionary economics, we define a fitness-utility model that better captures the evolutionary nature of design improvements and the essential DSR nature of searching for a satisfactory design across a fitness landscape. We conclude with a discussion of the strengths and challenges of the fitness-utility model for performing rigorous DSR.", "num_citations": "60\n", "authors": ["37"]}
{"title": "The successful diffusion of innovations: guidance for software development organizations\n", "abstract": " Using the personal software process (PSP) as an example of an innovative information technology, the authors performed a field study of developers using the PSP approach on software development projects in industry. Their analysis of the results offers practical guidance on how software development organizations should support the diffusion of innovations into successful practice.", "num_citations": "54\n", "authors": ["37"]}
{"title": "The optimization of query processing on distributed database systems\n", "abstract": " Hevner, Alan-Raymond. Ph. D., Purdue. Un! vers i ty, December 1979. The Opt 1mlzationrof Query Processing on-Distributed Database Systems. Major Professor: S. Bing Yaov", "num_citations": "53\n", "authors": ["37"]}
{"title": "Perceived control and the diffusion of software process innovations\n", "abstract": " Emphasis on quality, productivity, and repeatable processes has led to the introduction of innovative software development tools and techniques. Evidence from use of these innovations shows significant gains in developer productivity and software quality. However, many potentially beneficial innovations are not widely diffused. We examine why this is so, focusing on how a software developer's perceived control over use of a software development innovation impacts the successful diffusion of the innovation. A proposed research model shows that a developer's perception of freedom in deciding whether to adopt an innovation and in deciding when and how to apply the innovation impacts IT diffusion success. Survey data were collected from practicing developers trained in the Personal Software ProcessSM (PSPSM) and are analyzed using structural equation modeling. Results suggest that creating a perception\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "52\n", "authors": ["37"]}
{"title": "Phase containment metrics for software quality improvement\n", "abstract": " The development of high-quality software is an essential business activity in many organizations. Improving software quality requires the effective use of a software development process with well-defined phases of development and a metrics program to define and verify product and process quality. A key objective in software development is phase containment of defects. It is accepted knowledge that identifying and correcting defects as close to their source as possible produces higher quality software with enhanced development productivity. The essential goal is that a defect should not escape the phase in which it is introduced. This paper presents a formal model of phase containment metrics. We report on the implementation of phase containment metrics in a real software development project. Data tables and charts are proposed as effective means of collecting and reporting the metrics. We demonstrate how\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "52\n", "authors": ["37"]}
{"title": "Conflict in collaborative software development\n", "abstract": " Pair Programming is an innovative collaborative software development methodology. Anecdotal and empirical evidence suggests that this agile development method produces better quality software in reduced time with higher levels of developer satisfaction. To date, little explanation has been offered as to why these improved performance outcomes occur. In this qualitative study, we focus on how individual differences, and specifically task conflict, impact results of the collaborative software development process and related outcomes. We illustrate that low to moderate levels of task conflict actually enhance performance, while high levels mitigate otherwise anticipated positive results.", "num_citations": "51\n", "authors": ["37"]}
{"title": "Distributed data allocation strategies\n", "abstract": " Publisher SummaryThis chapter focuses on the distributed data allocation strategies. Four significant design decisions compose the data allocation design for distributed systems: data partitioning, data placement, data replication, and dynamic data allocation. The decision models used in distributed data allocation employ several different modeling techniques. While most models use mathematical programming\u0393\u00c7\u00f6such as linear programming\u0393\u00c7\u00f6as a modeling basis, other models use queueing, Petri networks, simulation, or analytical modeling. For each type of model, there exists a group of solution techniques. The selection of a solution technique depends primarily on the size and complexity of the model and the degree of solution performance required. The initial step of a distributed data allocation is determining the most effective partitions of data to be units of distribution, referred to as the data partitioning\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "50\n", "authors": ["37"]}
{"title": "Decision insight systems for microcomputers: A critical evaluation\n", "abstract": " In this study, we introduce the term \u0393\u00c7\u00a3Decision Insight System\u0393\u00c7\u00a5 (DIS) to describe a new class of decision support systems for microcomputers. These packages are intended to help users make complex decisions. We define what we mean by DIS and describe thirteen commercially available DIS packages. These packages are evaluated with respect to a comprehensive set of criteria and recommendations are made with regard to these packages.", "num_citations": "50\n", "authors": ["37"]}
{"title": "Roles of digital innovation in design science research\n", "abstract": " There has been a surge of interest in the design science research (DSR) paradigm as central to information systems (IS) studies in the past 20 years. The goal of a DSR research project is to extend the boundaries of human and organizational capabilities by designing new and innovative artifacts represented by constructs, models, methods, and instantiations (Hevner et al. 2004; Peffers et al. 2007; Gregor and Hevner 2013). Broadly speaking, DSR aims to add to knowledge of how things can and should be constructed or arranged (ie, designed), usually by human agency, to achieve a desired set of goals. For example, design knowledge in the IS discipline includes knowledge of how to structure and construct a database system, how to model business processes, how to align IS with organizational strategy, and how to deliver data analytics for effective decision making (eg, Becker et al. 2015). DSR results in IS\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "46\n", "authors": ["37"]}
{"title": "A community health report card: comprehensive assessment for tracking community health (CATCH).\n", "abstract": " A systematic method for assessing the health status of communities has been under development at the University of South Florida since 1991. The system, known as CATCH, draws 226 indicators from multiple sources and uses an innovative comparative framework and weighted evaluation criteria to produce a rank-ordered community problem list. The CATCH results from II Floridian counties have focused attention on high priority health problems and provided a framework for measuring the impact of health expenditures on community health status outcomes. The method and plans to create an automated data warehouse to support its expansion and enrichment are described.", "num_citations": "46\n", "authors": ["37"]}
{"title": "Box-structured methods for systems development with objects\n", "abstract": " Box structures provide a rigorous and systematic process for performing systems development with objects. Box structures represent data abstractions as objects in three system views and combine the advantages of structured development with the advantages of object orientation. As data abstractions become more complex, the box structure usage hierarchy allows stepwise refinement of the system design with referential transparency and verification at every step. An integrated development environment based on box structures supports flexible object-based systems development patterns. We present a classic example of object-based systems development using box structures.", "num_citations": "44\n", "authors": ["37"]}
{"title": "Design of an information volatility measure for health care decision making\n", "abstract": " Health care decision makers and researchers often use reporting tools (e.g. Online Analytical Processing (OLAP)) that present data aggregated from multiple medical registries and electronic medical records to gain insights into health care practices and to understand and improve patient outcomes and quality of care. An important limitation is that the data are usually displayed as point estimates without full description of the instability of the underlying data, thus decision makers are often unaware of the presence of outliers or data errors. To manage this problem, we propose an Information Volatility Measure (IVM) to complement business intelligence (BI) tools when considering aggregated data (intra-cell) or when observing trends in data (inter-cell). The IVM definitions and calculations are drawn from volatility measures found in the field of finance, since the underlying data in both arenas display similar behaviors\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "41\n", "authors": ["37"]}
{"title": "Controlled experimentation on adaptations of pair programming\n", "abstract": " The use of agile methods is growing in industrial practice due to the documented benefits of increased software quality, shared programmer expertise, and user satisfaction. These methods include pair programming (two programmers working side-by-side producing the code) and test-driven approaches (test cases written first to prepare for coding). In practice, software development organizations adapt agile methods to their environment. The purpose of this research is to understand better the impacts of adapting these methods. We perform a set of controlled experiments to investigate how adaptations, or variations, to the pair programming method impact programming performance and user satisfaction. We find that method variations do influence programming results. In particular, better performance and satisfaction outcomes are achieved when the pair programming is performed in face-to-face versus\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "41\n", "authors": ["37"]}
{"title": "Controls in flexible software development\n", "abstract": " Control and flexibility may appear an unlikely pair. However, we propose that effective flexible software development processes must still provide clear control mechanisms to manage the progress and quality of the resulting software products. This paper presents our initial research to understand the types of control found in flexible software development processes. Control theory is used as a lens to study the control mechanisms found in plan-driven and flexible processes. We extend current thinking to include emergent outcome controls and clan controls for team coordination in our taxonomy of control mechanisms. Several popular flexible processes are analyzed for control mechanisms. We conclude with a brief discussion of our future research directions.", "num_citations": "41\n", "authors": ["37"]}
{"title": "The Knowledge Innovation Matrix (KIM): A clarifying lens for innovation\n", "abstract": " Innovation is often understood in terms such as radical versus incremental, or exploratory versus exploitative, yet these terms are used loosely with little precision as to the type or amount of \u0393\u00c7\u00ffnewness\u0393\u00c7\u00d6 found in the innovation. We suggest that innovations be judged on the basis of original knowledge contribution and needs addressed. Based on this fundamental definition, we propose a formal typology for categorizing innovations and the levels of both new knowledge contribution and real-world impact. The Knowledge Innovation Matrix (KIM) results from a classification of innovations and knowledge contributions on the two dimensions of application knowledge maturity and domain maturity. KIM provides a clarifying lens through which stakeholders can strategically manage innovation in multiple contexts. The matrix has four distinct quadrants termed (1) Invention,(2) Improvement,(3) Exaptation, and (4) Exploitation. We position this research in relation to existing innovation perspectives and briefly analyze the value propositions for innovations found in each of the quadrants in relation to the goals of the various innovation stakeholders in academia, industry, and government. Our aim is to produce a practical guide for developing a common understanding and a common language among academic researchers aiming at advancing knowledge, industrial managers aiming at new product and services, and government officials aiming at increasing the public welfare.", "num_citations": "39\n", "authors": ["37"]}
{"title": "The flow-service-quality framework: unified engineering for large-scale, adaptive systems\n", "abstract": " Modern enterprises are irreversibly dependent on large-scale, adaptive, component-based information systems whose complexity frequently exceeds current engineering capabilities for intellectual control, resulting in persistent difficulties in system development, management, and evolution. We propose an innovative framework of engineering representation and reasoning methods for developing these complex systems: the flow-service-quality (FSQ) framework. In dynamic network information systems with constantly varying function and usage, workflows and their corresponding traces of system services act as stable foundations for functional and non functional (quality attribute) specification, design, and operational control. Our objective is to provide theoretical foundations, language representations, and rigorous yet practical unified engineering methods to represent and reason about system flows as essential\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "39\n", "authors": ["37"]}
{"title": "An iterative method for distributed database design\n", "abstract": " The development of a distributed database system requires effective solutions to many complex and interrelated design problems. The cost dependencies between query optimization and data allocation on distrihuled systems are well recognized but little understood. We investigate these dependencies by proposing and analyzing an iterative heuristic which provides an integrated solution lo the query optimization and data allocation problems, The optimization heuristic itcrates between finding minimum cost query slrategies and minimum cost data allocations until a local minimum for the combined problem is found. A search from convergence efficiently scans the optimization search space for lower cost solutions. Parametric studies within a simple query environment demonstrate nearoptimal performance for the iterative method when minimizing lolal time and response cost of queries. The iterative method provides clear improvements over alternative solution methods. The paper concludes with the practical implications of this research and its future directions.", "num_citations": "39\n", "authors": ["37"]}
{"title": "Utilizing, producing, and contributing design knowledge in DSR projects\n", "abstract": " We distinguish several design knowledge types in IS research and examine different modes of utilizing and contributing design knowledge that can take place during design science research (DSR) projects. DSR projects produce project design knowledge, which is project-specific, possibly untested, conjectural, and temporary; thus, distinct from the more stable contributions to the propositional and prescriptive human knowledge bases. We also identify solution design knowledge as distinct from solution design entities in the prescriptive knowledge base. Each of the six modes of utilizing or contributing knowledge (i.e. design theorizing modes) we examine draws on different knowledge types in a different way to inform the production of project design knowledge (including artifact design) in a DSR project or to grow the human knowledge bases in return. Design science researchers can draw on our design\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "38\n", "authors": ["37"]}
{"title": "Entering action design research\n", "abstract": " In the execution of an Action Design Research (ADR) project, we experienced challenges in how to enter into the ADR research stages effectively. In this paper, we present how we addressed these challenges by extending the ADR model with two expanded up-front activities and multiple entry points. Our research on inter-organizational social networks is briefly described as the project context for application of the extended ADR model.", "num_citations": "36\n", "authors": ["37"]}
{"title": "Box-structured requirements determination methods\n", "abstract": " Requirements determination is an iterative process of eliciting, gathering, modeling, specifying, and analyzing system requirements information. It is the most critical, yet least understood, phase of systems development. This paper presents a rigorous approach for performing requirements determination with box-structured methods. By capturing requirements information in black box transactions and transaction hierarchies, intellectual control is maintained over large amounts of requirements information. The results of the box-structured requirements determination methods provide the basis for formal system design techniques. A concise example of box-structured requirements determination is included in an appendix.", "num_citations": "36\n", "authors": ["37"]}
{"title": "Query optimization on local area networks\n", "abstract": " Local area networks are becoming widely used as the database communication framework for sophisticated information systems. Databases can be distributed among stations on a network to achieve the advantages of performance, reliability, availability, and modularity. Efficient distributed query optimization algorithms are presented here for two types of local area networks: address ring networks and broadcast networks. Optimal algorithms are designed for simple queries. Optimization principles from these algorithms guide the development of effective heuristic algorithms for general queries on both types of networks. Several examples illustrate distributed query processing on local area networks.", "num_citations": "34\n", "authors": ["37"]}
{"title": "The incremental development process in cleanroom software engineering\n", "abstract": " The objective of this paper is to present the theoretical basis and practical application of incremental development in the Cleanroom software engineering process. Incremental development is based on the mathematical principle of referential transparency. Cleanroom uses incremental development to build systems in a succession of cumulative subsets of user function. The increments accumulate top-down into the final product in a development and certification pipeline. Increment planning occurs after top-level specification, and results in a construction plan for the software. Factors determining the composition of increments include clarity of requirements, usage probability of user functions, reliability requirements for subsystems, coordination with the hardware development schedule, dependencies between functions, complexity, reuse, or other factors that pose risks to the project. Each increment involves a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["37"]}
{"title": "CATCH/IT: a data warehouse to support comprehensive assessment for tracking community health.\n", "abstract": " A systematic methodology, Comprehensive Assessment for Tracking Community Health (CATCH), for analyzing the health status of communities has been under development at the University of South Florida since the early 1990s. CATCH draws 226 health status indicators from multiple data sources and uses an innovative comparative framework and weighted evaluation criteria to produce a rank-ordered list of community health problems. CATCH has been applied successfully in many Florida counties; focusing attention on high priority health issues and measuring the impact of health expenditures on community health status outcomes. Previously performed manually, we are using information technology (IT) to automate the CATCH methodology with a full-scale data warehouse, user-friendly forms and reports, and extended analysis and data mining capabilities. The automated system, CATCH/IT, will reduce\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "32\n", "authors": ["37"]}
{"title": "Patient perspectives of telemedicine quality\n", "abstract": " BackgroundThe purpose of this study was to explore the quality attributes required for effective telemedicine encounters from the perspective of the patient.MethodsWe used a multi-method (direct observation, focus groups, survey) field study to collect data from patients who had experienced telemedicine encounters. Multi-perspectives (researcher and provider) were used to interpret a rich set of data from both a research and practice perspective.ResultsThe result of this field study is a taxonomy of quality attributes for telemedicine service encounters that prioritizes the attributes from the patient perspective. We identify opportunities to control the level of quality for each attribute (ie, who is responsible for control of each attribute and when control can be exerted in relation to the encounter process). This analysis reveals that many quality attributes are in the hands of various stakeholders, and all attributes can be\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["37"]}
{"title": "Exploring the relationship between design science research and innovation: a case study of innovation at Chevron\n", "abstract": " What is the relationship between design science research and innovation? Our industry-academic collaboration poses this intriguing question and suggests a context and an experimental design for its study. We wish to understand the synergies between the active research areas of DSR and innovation by exploring their overlapping concepts and identifying unique ideas in each that have the potential to inform the other. We present a case study of an actual innovation process in Chevron as a source of empirical data for the exploration and subsequent analysis of how the application of DSR guidelines might inform the practical implementation of innovation processes.", "num_citations": "31\n", "authors": ["37"]}
{"title": "The front end of Innovation: perspectives on creativity, knowledge and design\n", "abstract": " While the importance of innovation as a dominant driver of societal and economic progress is well established, the processes and outcomes of innovation remain distressingly ad-hoc and unpredictable. In particular, the Front End of Innovation (FEI) provides many open questions as innovators are challenged to understand the opportunity context, generate novel ideas, and evaluate these ideas for the implementation of solutions. We propose an original model of the FEI with a nascent theory base drawn from recent perspectives in the areas of innovation, creativity, knowledge, and design science. A key insight is the application of a knowledge maturity lens to distinguish four categories of innovation \u0393\u00c7\u00f4 invention, exaptation, advancement, and exploitation. We conclude with an agenda for future research to extend innovation theories and with actionable advice for improving current practices of innovation.", "num_citations": "30\n", "authors": ["37"]}
{"title": "Introduction to design science research\n", "abstract": " Since the dawn of the digital revolution, information technologies have changed the way we live, work, play, and entertain. Designers of IT-based digital technology products play a critical role in ensuring that their designed artifacts are not just beautiful but provide value to their users. Users are increasingly interacting with a digital world. Designing interactions in this new world is a challenging task. The experiences we have when we browse the web, or visit amazon.com, sell/buy stuff on eBay or play amusing games on our mobile cell phones do have a tremendous impact on how we live our lives. Designing information systems is even more challenging.", "num_citations": "30\n", "authors": ["37"]}
{"title": "Design science research frameworks\n", "abstract": " The founding father of design science was Herbert E. Simon. Well known for his work on AI, decision making, and economics, Simon wrote a thought-provoking book called Sciences of the Artificial in the 1960s (Simon 1996). His profound insight was that certain phenomena or entities are \u0393\u00c7\u00a3artificial\u0393\u00c7\u00a5 in the sense that they are contingent to the goals or purposes of their designer. In other words, they could have been different had the goals been different (as opposed to natural phenomena which are necessarily evolved given natural laws). He further posits: Since artifacts are contingent, how is a science of the artificial possible? How to study artifacts empirically? On the other hand, Simon also deals with the notion of complexity. This is necessary because artificiality and complexity are inextricably interwoven.", "num_citations": "30\n", "authors": ["37"]}
{"title": "The impact of function extraction technology on next-generation software engineering\n", "abstract": " Currently, software engineers lack practical means to determine the full functional behavior of complex programs. This gap in intellectual control is the source of many long-standing and intractable problems in security, software engineering, and systems engineering. Function Extraction FX technology is directed to automated computation of full program behavior. FX is based on function-theoretic mathematical foundations of software that illuminate algorithmic methods for behavior computation. FX holds promise to replace resource-intensive, error-prone analysis of program behavior in human time scale with fast and correct analysis in computer time scale. The CERT trademark organization of the Software Engineering Institute is conducting research and development in FX technology and is developing a Function Extraction for Malicious Code system to rapidly determine the behavior of malicious code expressed in Assembler Language. FX technology has the potential for transformational impact across the software engineering life cycle, from specification and design to implementation, testing, and evolution. This study investigates these impacts and, based on a survey of software professionals, defines a strategy for FX evolution that addresses high-leverage opportunities first. FX is an initial step in developing next-generation software engineering as a computational discipline.Descriptors:", "num_citations": "29\n", "authors": ["37"]}
{"title": "Function-theoretic principles of program understanding\n", "abstract": " The authors propose a comprehensive methodology for automated program abstraction of computer programs. The theoretical foundations that support program abstraction are functional abstraction, data analysis, program slicing, and pattern matching. The theory of functional abstraction is applied to an example program. The localization of data scope is described as a step to facilitate the abstraction process. Techniques are presented for automatically abstracting the functions of both nonlooping and looping control structures.<>", "num_citations": "29\n", "authors": ["37"]}
{"title": "A neurodesign model for IS research.\n", "abstract": " The constructive (ie Build) activity in Design Research is where human cognitive (eg, complexity, creativity, control) and social (eg, collaboration) activities contribute to the design of novel artifacts that improve the human condition. In this essay, we model the design activity as an iterative process with flows connecting external and internal environments and problem and solution spaces. The design team performs within this process through cognitive interactions at critical points in the flow in order to structure the design problem, produce novel design candidates, manage the refinement of the best candidates into use artifacts, and achieve consensus among the design team as well as stakeholders. The model provides a basis to \u0393\u00c7\u00ffbroker\u0393\u00c7\u00d6and align neuroscientific theory and design research in the Information Systems (IS) field and, by doing so, within the broader informing science transdiscipline. The emphasis in the model on the interplay of \u0393\u00c7\u00ffdoing\u0393\u00c7\u00d6tasks and \u0393\u00c7\u00ffmaking\u0393\u00c7\u00d6sense focuses directly on the task at hand and in mind. These iterations are manifest in four interactions, each of which has a set of important cognitive challenges which we explore. Use of the model to guide NeuroDesign research presents a number of fruitful opportunities to extend the use of neuroimaging techniques in design research beyond the evaluation of information technology (IT) artifacts. The model also highlights the potential of design as an empirical context to identify, frame, and address some of the limitations of prior studies of complexity, creativity, control, and collaboration that, to date, have stymied mainstream neuroscience.", "num_citations": "28\n", "authors": ["37"]}
{"title": "The role of data warehousing in bioterrorism surveillance\n", "abstract": " The development of an effective bioterrorism surveillance system requires effective solutions to several critical challenges. The system must support multidimensional historical data, provide real-time surveillance of sensor data, have the capability for pattern recognition to quickly identify abnormal situations, and provide an analytic environment that accelerates investigations by epidemiologists and other responders. The use of real-time or flash data warehousing provides the essential ability to compare unfolding health events with historical patterns of key surveillance indicators. To explore the role of data warehousing in surveillance systems, we study naturally occurring incidents, Florida wildfires from 1996 through 2001, as reasonable facsimiles of bioterrorism attacks. Hospital admissions data on respiratory illnesses during that period are analyzed to uncover patterns that might resemble an airborne\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "28\n", "authors": ["37"]}
{"title": "Strategic information systems planning with box structures\n", "abstract": " Strategic information systems planning (SISP) is the process of aligning an organization's business strategy with effective computer based information systems to achieve critical business objectives. SISP is a top concern of major executives and considerable resources (time and money) are spent in SISP activities. Many SISP initiatives are not successful due to the difficulty of implementing the recommendations. A significant problem is the specification gap between the description of the recommended systems and the detail needed for actual system implementation. Existing SISP methods do not provide sufficiently rigorous representations to specify detailed system recommendations. Box structures are proposed as a solution to this problem and a SISP process with embedded box structure methods is presented. We have used this innovative process in two SISP projects with large organizations. Partial results\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "28\n", "authors": ["37"]}
{"title": "Telemedicine encounter quality: comparing patient and provider perspectives of a socio-technical system\n", "abstract": " The effectiveness of the telemedicine encounter is dependent on the use of state-of-the-art technology and the quality of the technology-based interactions. We take a socio-technical approach to understanding quality during telemedicine encounters. This approach has not been well studied in telemedicine service encounter research. To enrich understanding, we use a multimethod (direct observation, interview, focus group, survey) field study to collect and interpret a rich set of data. We conduct this study from two perspectives. First, we focus on the perceptions of the medical providers (e.g. physicians) who directly use the technology and are accountable for patient care. We then compare provider perspectives to those of patients, who act as indirect users of telemedicine technology and are the ultimate consumers of health care services provided via telemedicine. The result of this field study is a comparative\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["37"]}
{"title": "Advancing a NeuroIS research agenda with four areas of societal contributions\n", "abstract": " On the 10th anniversary of the NeuroIS field, we reflect on accomplishments but, more importantly, on the future of the field. This commentary presents our thoughts on a future NeuroIS research agenda with the potential for high impact societal contributions. Four key areas for future information systems (IS) research are: (1) IS design, (2) IS use, (3) emotion research, and (4) neuro-adaptive systems. We reflect on the challenges of each area and provide specific research questions that serve as important directions for advancing the NeuroIS field. The research agenda supports fellow researchers in planning, conducting, publishing, and reviewing high impact studies that leverage the potential of neuroscience knowledge and tools to further information systems research.", "num_citations": "26\n", "authors": ["37"]}
{"title": "Bioterrorism surveillance with real-time data warehousing\n", "abstract": " This paper discusses several technical challenges in the development of an effective bioterrorism surveillance system. Three factors are critical:                                             1.                                                 It must be multidimensional.                                                                                        2.                                                 It must accelerate the transmission of findings and data to most closely approximate real time surveillance so as to provide sufficient warning.                                                                                        3.                                                 It must have the capability for pattern recognition that will quickly identify an alarm or alert threshold value.                                                                                                         We build on our on-going health care data warehousing research to provide solutions to these challenges. The innovative use of flash data warehousing provides the essential ability to compare real-time healthcare data with\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["37"]}
{"title": "The CERT function extraction experiment: Quantifying FX impact on software comprehension and verification\n", "abstract": " Function Extraction FX is a new, theory-based technology for automated calculation of the functional behavior of software. The CERT Function Extraction experiment was conducted so as to better understand the impact of FX on human comprehension and verification of software and to rigorously quantify the business case for FX technology. This report describes the results of the controlled experiment that was performed to compare traditional manual methods of comprehension with automated behavior computation using an FX prototype. The results of the experiment show a substantial increase in human capabilities for software comprehension and verification using FX technology.Descriptors:", "num_citations": "22\n", "authors": ["37"]}
{"title": "Perceived Control of Software Developers and Its Impact on the Successful Diffusion of Information Technology.\n", "abstract": " Why are beneficial software engineering practices not being used effectively in the development of software systems This question has intrigued researchers in software engineering for many years Pamas 85. Billions of dollars per year are spent, and a large proportion wasted, on building and maintaining software systems that are either never completed or, if completed, are of poor quality. This state of software development has led to the introduction of innovative tools and techniques to support the software development process. Initial evidence from use of these tools and techniques shows significant improvements in development productivity and software quality. However, many of these potentially beneficial tools and techniques have not been widely adopted or diffused. This research seeks to examine the reason why this is so What factors explain the successful diffusion of new software development techniques into practiceDescriptors:", "num_citations": "22\n", "authors": ["37"]}
{"title": "Analysis of database system architectures using benchmarks\n", "abstract": " Database machine architectures have been proposed as a promising alternative to improve database system performance, control, and flexibility. While many claims have been made for the database machine concept, few studies have been made to test the performance advantages and disadvantages of a database machine in an application environment. A comprehensive benchmark study comparing the performance of database systems on a conventional computer system and a database machine is reported in this paper. The results show the database machine architecture to have superior performance in most cases. The performance advantage is sensitive to the communication line speed between the host computer and the database machine. The effects of line speed are studied and displayed in the benchmark results. A summary of the similarities and differences between the architectures based upon our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["37"]}
{"title": "Introduction to the ais thci special issue on design research in human-computer interaction\n", "abstract": " Design Research (DR) creates, builds, and evaluates innovative artifacts such as constructs, models, methods, and instantiations as well as operational information systems. It also investigates approaches, methods, behaviors, and processes related to design. Although the design research paradigm as an engineering approach in Information Systems (IS) research has been actively discussed in recent years (Hevner et al., 2004), comparatively little design related research has made its way into the IS community by means of widely recognized and outstanding publications. Human-Computer Interaction (HCI) Research is concerned with the ways humans interact with information, technologies, and tasks; especially in business, managerial, organizational, and cultural contexts (Zhang et al., 2002). Despite the realization that it is important for HCI research to focus on all issues that occur along the lifecycles of any information and communication technology (ICT) artifacts, IS scholars have traditionally put less effort into the design and development stage and more effort into the use and impact stage (Zhang and Li, 2005; Zhang et al., 2009).", "num_citations": "19\n", "authors": ["37"]}
{"title": "Introducing function extraction into software testing\n", "abstract": " Software testing can benefit from technologies that enable evolution toward increased engineering discipline. In current practice, software developers lack practical means to determine the full functional behavior of programs under development, and even the most thorough testing can provide only partial knowledge of behaviors. Thus, effective scientific principles and engineering technology for revealing software behavior should have a positive impact on software testing. This paper describes the emerging technology of function extraction (FX) for computing the behavior of programs to the maximum extent possible with mathematical precision. We explore how the use of FX technologies can transform methods for functional verification of software. An example illustrates the value of full behavior knowledge for complete and confident assessment of software function and fitness for use. We conclude by describing a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "18\n", "authors": ["37"]}
{"title": "System test planning of software: An optimization approach\n", "abstract": " This paper extends an exponential reliability growth model to determine the optimal number of test cases to be executed for various use case scenarios during the system testing of software. An example demonstrates a practical application of the optimization model for system test planning", "num_citations": "18\n", "authors": ["37"]}
{"title": "Applying agile software development processes to global virtual teams: A study of communication modalities\n", "abstract": " Today's information technology (IT) environment dictates speed, flexibility and a people-centered focus. Many of the new emerging agile software development processes accomplish these goals by calling for team collaboration in face-to-face settings. Globalization and the pressure to lower development costs have increased the utilization of virtual teams. These virtual teams represent a new organizational form that does not provide for same proximity collaboration. We propose an experiment that explores the impacts of different communication modalities in face-to-face and global virtual software development environments.", "num_citations": "18\n", "authors": ["37"]}
{"title": "Effectual application development on digital platforms\n", "abstract": " The development of novel software applications on digital platforms differs radically from traditional software development. In this position paper, we posit that software development managers and teams face unique challenges in platform environments and require new development approaches to be successful. While traditional software development approaches have focused on achieving application-market match, platform-based applications must also achieve application-platform match, application-market match, value propositions exceeding platform\u0393\u00c7\u00d6s core value propositions, and novelty. We argue that these desired properties support a new vision of the software development team as entrepreneurs. To support this positioning insight, we discuss the limitations of existing software development approaches and introduce an innovative approach for application development on digital platforms that is\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["37"]}
{"title": "The impacts of function extraction technology on program comprehension: A controlled experiment\n", "abstract": " Program comprehension is a critical, time-consuming, and highly error-prone task for software developers. Function extraction (FX) is a theory and technology that automates and supports program comprehension by calculating the semantic behaviors of programs at many levels of abstraction and displaying those behaviors in a standard, readable format in terms of the \u0393\u00c7\u00a3as-built\u0393\u00c7\u00a5 specification of the program. In this experimental study, developers using an FX prototype tool to assist them in determining the behavior of software modules have significantly more effective program comprehension, in both increased accuracy of understanding and reduced time on task. Moreover, developers have a positive reaction toward the use of the FX technology, and use of FX does not reduce their overall comprehension of the program.", "num_citations": "17\n", "authors": ["37"]}
{"title": "Requirements-driven database systems benchmark method\n", "abstract": " Benchmarks are the vital tools in the performance measurement, evaluation, and comparison of relational database management systems (RDBMS). Standard benchmarks such as the TP1, TPC-A, TPC-B, TPC-C, TPC-D, TPC-H, TPC-R, TPC-W, Wisconsin, and AS3AP benchmarks have been used to assess the performance of relational database management systems. These benchmarks are synthetic and domain-specific. Test results from these benchmarks are estimates of possible system performance for certain pre-determined application types. Database system performance on actual database domain may vary significantly from those in the standard benchmarks. In this paper, we describe a new benchmark method that is computer-assisted and developed from the perspective of the user's requirements.", "num_citations": "17\n", "authors": ["37"]}
{"title": "Product and project challenges in electronic commerce software development\n", "abstract": " Electronic commerce (E-commerce) software development organizations face unique challenges based on rapidly changing markets, demanding customers with ill-defined requirements, and resulting priority conflicts between product line development and customer projects. A model of this unique development environment is identified with important linkages among the product function, the project function, and the underlying software development function within an organization. Guided by this model of the E-commerce development environment, a case study of a medium-sized E-commerce company was conducted. Based on this study, eight critical challenges to the successful development of top quality software systems are identified. From these challenges a research model and propositions are presented. As each challenge is discussed unique impacts of the E-commerce environment are reinforced by\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["37"]}
{"title": "Flow-service-quality (FSQ) engineering: Foundations for network system analysis and development\n", "abstract": " Modern society could hardly function without the large-scale, network-centric information systems that pervade government, defense, and industry. As a result, serious failures or compromises carry far-reaching consequences. These systems are characterized by changing and often unknown boundaries and components, constantly varying function and usage, and complexities of pervasive asynchronous operations. Their complexity challenges human intellectual control, and their survivability has become an urgent priority. Engineering methods based on solid foundations and the realities of network systems are required to manage complexity and ensure survivability. Flow-Service-Quality FSQ engineering is an emerging technology for management, acquisition, analysis, development, evolution, and operation of large-scale, network-centric systems. FSQ engineering is based on Flow Structures, Computational Quality Attributes, and Flow Management Architectures. These technologies can help provide stable engineering foundations for the dynamic and often unpredictable world of large-scale, network-centric systems. Flow Structures define enterprise mission task flows and their refinements into uses of system services in network traversals. Flows are deterministic for human understanding, despite the underlying asynchronism of network operations. They can be refined, abstracted, and verified with precision, and deal explicitly with Uncertainty Factors, including uncertain commercial off-the-shelf functionality and system failures and compromises. Computational Quality Attributes go beyond static, a priori estimates to treat quality attributes\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["37"]}
{"title": "An iterative method for distributed database optimization\n", "abstract": " The development of a distributed database system requires effective solutions to many complex and interrelated design optimization problems. The cost dependencies between query optimization and data allocation on distributed systems are well recognized but little understood. We investigate these dependencies by proposing and analysing an iterative heuristic which provides an integrated solution to the query optimization and data allocation problems. The optimization heuristic iterates between finding minimum-cost query strategies and minimum-cost data allocations until a local minimum for the combined problem is found. A search from convergence efficiently scans the optimization search space for lower-cost solutions. In this paper, we apply the iterative heuristic to a realistic distributed database system model and a general class of queries and obtain very significant performance benefits. Experimental\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["37"]}
{"title": "Querying distributed databases on local area networks\n", "abstract": " Distributed databases on local area networks present additional considerations for query optimization over databases on geographically distributed, point-to-point networks. This paper surveys and evaluates the state of current research on distributed query optimization for local area networks. A classification taxonomy is presented and used to analyze the proposed query-optimization algorithms. The unique features of each algorithm are highlighted and a qualitative comparison of the algorithms is given. Future research directions are discussed.", "num_citations": "16\n", "authors": ["37"]}
{"title": "Local query translation and optimization in a distributed system\n", "abstract": " A new query translation and optimization algorithm is presented. The algorithm is being implemented as the local query translation and optimization technique of Honeywell's Distributed Database Testbed System (DDTS). The algorithm translates local queries expressed in representational schemas (relational) to their equivalent internal schemas (network). The technique is new in that it does not translate each relational command in isolation, but rather attempts to find a collection of relational commands for which an optimized sequence of CODASYL DML commands can be generated. The optimization minimizes the number of disk accesses by taking advantage of the access paths available to the CODASYL local database management systems and the relationship information of the variables used in the relational commands.", "num_citations": "16\n", "authors": ["37"]}
{"title": "From information to operations: Service quality and customer retention\n", "abstract": " In business, information is abundant. Yet, effective use of that information to inform and drive business operations is a challenge. Our industry-university collaborative project draws from a rich dataset of commercial demographics, transaction history, product features, and Service Quality Index (SQI) factors on shipping transactions at FedEx. We apply inductive methods to understand and predict customer churn in a noncontractual setting. Results identify several SQI variables as important determinants of churn across a variety of analytic approaches. Building on this we propose the design of a Business Intelligence (BI) dashboard as an innovative approach for increasing customer retention by identifying potential churners based on combinations of predictor variables such as demographics and SQI factors. This empirical study contributes to BI research and practice by demonstrating the application of data analytics\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["37"]}
{"title": "Beyond rigor and relevance: Exploring artifact resonance\n", "abstract": " We explore the research resonance concept in the context of the Information Systems (IS) design science research (DSR) literature. Resonance is a concept that complements research rigor and relevance and addresses how researchers can effectively communicate research outcomes to suitable audiences and, thus, achieve practical impact. We operationalize the resonance concept for the IS DSR context and analyze the resonance potential of mature and nascent artifacts that are documented in recent DSR papers. We find that papers consider artifact relevance aspects well, but show considerable room for improvement concerning communicating artifacts to achieve a high level of resonance with practitioner audiences. On this foundation, we see opportunities for further research to develop concrete guidelines for IS design researchers on how to increase their artifacts' resonance potential to the fullest extent.", "num_citations": "14\n", "authors": ["37"]}
{"title": "Effectuation and its implications for socio-technical design science research in information systems\n", "abstract": " We study the implications of the effectuation concept for socio-technical artifact design as part of the design science research (DSR) process in information systems (IS). Effectuation logic is the opposite of causal logic. Ef-fectuation does not focus on causes to achieve a particular effect, but on the possibilities that can be achieved with extant means and resources. Viewing so-cio-technical IS DSR through an effectuation lens highlights the possibility to design the future even without set goals. We suggest that effectuation may be a useful perspective for design in dynamic social contexts leading to a more dif-ferentiated view on the instantiation of mid-range artifacts for specific local ap-plication contexts. Design science researchers can draw on this paper\u0393\u00c7\u00d6s conclu-sions to view their DSR projects through a fresh lens and to reexamine their re-search design and execution. The paper also offers avenues for future research to develop more concrete application possibilities of effectuation in socio-technical IS DSR and, thus, enrich the discourse.", "num_citations": "14\n", "authors": ["37"]}
{"title": "Research in information systems analysis and design: introduction to the special theme papers\n", "abstract": " Information systems analysis and design are basic topics in the Information Systems (IS) curriculum. A large number of IS graduates are employed as information systems developers. However, research in the IS field pays relatively little attention to IS analysis and design topics. Few of the articles published in leading IS research journals in the last decade deal with these topics In response, CAIS and JAIS are jointly presenting Special Themes on Research in Information Systems Analysis and Design to begin to fill this void and to attract attention of researchers to this important area.", "num_citations": "14\n", "authors": ["37"]}
{"title": "Integrated CASE for cleanroom development\n", "abstract": " An integrated CASE environment for performing cleanroom systems engineering is proposed. The cleanroom integrated CASE environment is viewed as a series of iterative development activities. The fundamental integrating concepts and principles of cleanroom engineering provide a seamless development environment, with no information loss or artificial bridges between activities. It is shown that the integrated CASE environment for cleanroom systems development requires a formal methodology that covers the entire development process, which comprises cleanroom development concepts, a central repository of development information and a set of automated tools that work together transparently through the common repository. Three projects representative of experience in implementation of cleanroom CASE are discussed: a structuring facility for Cobol, a real-time system, and compiler components for a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["37"]}
{"title": "Envisioning entrepreneurship and digital innovation through a design science research lens: A matrix approach\n", "abstract": " Design Science Research (DSR) in the information systems (IS) field is, at its essence, about Digital Innovation (DI). Innovative sociotechnical design artifacts involve digital information technologies (IT) being used in ways that result in profound disruptions to traditional ways of doing business and to widespread societal changes. The pervasiveness of DI means that the individuals involved in bringing it about have diverse backgrounds, including application specialists, software engineers, data scientists, business managers, economists, venture capitalists, various user groups, and entrepreneurial leaders. This range of backgrounds means that DI, much more than traditional innovation, leads to varied perspectives on the methods and tools to be used in the development of effective and evolvable complex systems incorporating digital innovations. In this paper we present a new matrix approach to DI based on DSR\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["37"]}
{"title": "Design science research: Looking to the future\n", "abstract": " The previous chapters have taken you through the fundamentals of design science research, the problems, solutions space, design process, frameworks, outputs and artifacts, theories and dissemination of the research results. The design science research paradigm is highly relevant to information systems (IS) research because it directly addresses two of the key issues of the discipline: the central, albeit controversial, role of the IT artifact in IS research (Weber 1987; Orlikowski and Iacono 2001; Benbasat and Zmud 2003) and the lack of professional relevance of IS research (Benbasat and Zmud 1999; Hirschheim and Klein 2003). Design science, as conceptualized by Simon (1996), supports a pragmatic research paradigm that calls for the creation of innovative artifacts to solve real-world problems. Thus, design science research combines a focus on the IT artifact with a high priority on relevance in the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["37"]}
{"title": "ICIS 2008 Panel report: design science in information systems: Hegemony, bandwagon, or new wave?\n", "abstract": " In the past few years, design science has become a topic of increasing importance, especially in the North American academic IS community. Some observers see a new hegemony forming. Others dispute that but suggest that design science is merely the latest bandwagon rolling through the IS domain. A panel at the 2008 International Conference on Information Systems debated views of design science prevalent in the IS community. This paper reports on the panel discussion and attempts to position design science from various perspectives, including North American and European views, the latter with a long tradition of design-based IS scholarship.", "num_citations": "12\n", "authors": ["37"]}
{"title": "Semantic foundations for survivable system analysis and design\n", "abstract": " Survivability is the capability of an information system to support critical enterprise missions in adverse environments of attacks, failures, and accidents [Ellison et al 1999]. A research program in survivability must therefore address both systems and the environments within which they operate. Survivability is a combination of quality attributes, including security, reliability, safety, fault tolerance, dependability, and others [Mead et al 2000]. The SEI CERT Coordination Center, in cooperation with other researchers, has embarked on a multi-year, dual-thread research program, one thread to create engineering practices for survivable system design and development, the other to create engineering practices for analysis and definition of adverse environments. We believe that lack of theoretical foundations in both areas has been a serious impediment to survivable system development. In essence, we seek to move beyond natural language descriptions of survivability to a computational capability for engineering analysis of survivability properties. Accordingly, our agenda is to progress from theoretical foundations, to formal language representations, to engineering practices. We take it as an article of faith that to be effective, engineering practices must be based on rigorous foundations.At the same time, it is important to target foundations and engineering practices to the present reality and future evolution of information system architectures and technologies. Today\u0393\u00c7\u00d6s large-scale infrastructure systems are characterized by the behavioral complexity of asynchronous operations and the semantic complexity of data and service interoperability. Loss of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["37"]}
{"title": "Design Science Research: Cases\n", "abstract": " \u0393\u00c7\u00a3PROGRESS in IS\u0393\u00c7\u00a5 encompasses the various areas of Information Systems in theory and practice, presenting cutting-edge advances in the field. It is aimed especially at researchers, doctoral students, and advanced practitioners. The series features both research monographs that make substantial contributions to our state of knowledge and handbooks and other edited volumes, in which a team of experts is organized by one or more leading authorities to write individual chapters on various aspects of the topic.\u0393\u00c7\u00a3PROGRESS in IS\u0393\u00c7\u00a5 is edited by a global team of leading IS experts. The editorial board expressly welcomes new members to this group. Individual volumes in this series are supported by a minimum of two members of the editorial board, and a code of conduct mandatory for all members of the board ensures the quality and cutting-edge nature of the titles published under this series.", "num_citations": "11\n", "authors": ["37"]}
{"title": "It's more than just use: An investigation of telemedicine use quality\n", "abstract": " Many information systems (IS) studies portray use as an indicator of system success. However, \"simply saying that more use will yield more benefits without considering the nature of this use, is clearly insufficient\" [1 p. 16]. Researchers are also urged to address the study context in defining components of IS success. Our research specifies the use quality construct in the context of a mission critical system deployment, namely, the use of medical video conferencing for patient examinations. This type of telemedicine encounter provides an interesting context of system use as people in various roles interact with each other and with technology. We use a multi-method field study to collect and interpret a rich set of data on telemedicine encounters. We analyze the data from the perspectives of both patients and providers in the encounter. The result of this field study is a socio-technical framework of use quality for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["37"]}
{"title": "Design of a functionally distributed, multiprocessor database machine using data flow analysis\n", "abstract": " We propose a design methodology based on data flow analysis for a functionally distributed, multiprocessor database machine. We define a cost model of database processing with the objective cost being response time of a set of query strategies. Heuristic optimization techniques are proposed using the operations of grouping, decomposition, and replication. We apply the new optimization techniques in several realistic multiprocessor environments. The resulting configurations of multiprocessors demonstrate significant performance improvements over more traditional designs.", "num_citations": "11\n", "authors": ["37"]}
{"title": "Comparing alternative methods for composing community peer groups: a data warehouse application.\n", "abstract": " A method for assessing the health status of communities has been under development for a decade at the University of South Florida. Known as CATCH (Comprehensive Assessment for Tracking Community Health), the method utilizes health status indicators from multiple data sources. With federal grant support, a unique data warehouse has been created to automate CATCH assessments and to enhance online analytical processing for efficient data browsing, knowledge discovery, and model testing. A comparison of two peer grouping methods (population size versus predicted age-adjusted mortality) is reviewed to demonstrate the warehouse capabilities.", "num_citations": "10\n", "authors": ["37"]}
{"title": "Optimization of data access in distributed systems\n", "abstract": " The application of computer network technology~ o-database systems has produced much interest in distributed database systems. Query processing on a distributed system is seen to be quite a different problem from query processing on a centralized system. A query requiring data from two or more distinct nodes in the network must be solved by a distribution strategy that consists of a schedule of local data proceasing and data transmissions. Two cost measures, total time and response time, are used to judge the quality of a given distribution st~ ategy. Methods that find efficient distribution strategies for queries are proposed and snalyzed. Algo~ ithms that embody simple distribution tactics are shown to be optimal in the sense of minimizing response time and total time for a special class of queries. A method fs proposed to extend the optimal algorithms to derive efficient distribution strategies fo~ general query\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["37"]}
{"title": "Introduction to Design Science Research\n", "abstract": " Design Science Research (DSR) is a problem-solving paradigm that seeks to enhance human knowledge via the creation of innovative artifacts. Simply stated, DSR seeks to enhance technology and science knowledge bases via the creation of innovative artifacts that solve problems and improve the environment in which they are instantiated. The results of DSR include both the newly designed artifacts and design knowledge (DK) that provides a fuller understanding via design theories of why the artifacts enhance (or, disrupt) the relevant application contexts. The goal of this introduction chapter is to provide a brief survey of DSR concepts for better understanding of the following chapters that present DSR case studies.", "num_citations": "9\n", "authors": ["37"]}
{"title": "Agile methods: Fast-paced, but how fast?\n", "abstract": " What are the roles of time and time pressures in design and performance of agile processes for software development? How do we plan our rapid development activities given the constraints of due dates? What does it mean to be on \u0393\u00c7\u00ffinternet time\u0393\u00c7\u00d6? Agile methods are meant to be fast-paced, but are they fast in an effective way? How do time-pressures influence the productivity of a project team and how do they impact the motivations of developers? This paper considers the time issues in agile approaches to managing software projects and posits research propositions to guide further study of this area.", "num_citations": "9\n", "authors": ["37"]}
{"title": "Hospital discharge transactions: a data warehouse component\n", "abstract": " The Comprehensive Assessment for Tracking Community Health (CATCH) methodology, under development at the University of South Florida, provides a systematic framework for community level assessment that can be a valuable tool for resource allocation and healthcare policy formulation. The community-level focus employers local decision-makers and provides a clear methodology for organizing and interpreting the data. The CATCH methodology has been successfully used to produce many community reports, but each application requires time consuming data collection and analysis activities. A data warehouse is being constructed to support the current methodology as well as future initiatives. This paper reports on some aspects of this on-going data warehouse project, focusing on hospital discharge data. This data is used to derive a set of CATCH indicators, but is also a powerful data warehouse\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["37"]}
{"title": "Integrated CASE support for box structure systems development: a case study\n", "abstract": " The results are given of a project in which three computer aided systems engineering (CASE) tools are used in an integrated fashion to develop a box-structured system design of a system with real-time components. The box structure concepts provide the common basis for integrating the CASE tools. A prototype of the system has been implemented in Ada. The authors demonstrate the use and benefits of the CASE tools for developing systems via box structure methods.< >", "num_citations": "9\n", "authors": ["37"]}
{"title": "A white box analysis of concurrent system designs\n", "abstract": " An effective approach is demonstrated for the analysis of concurrent system design in box structures. Potential clear box designs at each level of the box structure usage hierarchy are termed white boxes and are evaluated for design correctness (ie consistency and closure) and estimated performance. A correct design with the best estimated performance can be selected as the final clear box. For deterministic concurrent designs, the authors present rigorous methods to evaluate design correctness. For non-deterministic design the authors resort to the use of a Petri-net model of the white box. By implementing the Petri-net model on an automated analysis tool, they evaluate correctness criteria of fairness, mutual exclusion, and liveness, and such performance criteria as resource utilization and throughput.<>", "num_citations": "9\n", "authors": ["37"]}
{"title": "A method for data re-engineering in structured programs\n", "abstract": " While the problem of restructuring control flow in software is fairly well understood, few methods exist for understanding and restructuring the data flow of software. A method of data re-engineering is proposed that combines the theories of data-usage abstractions for system redesign. The principal results of this re-engineering process are the elimination of data-flow anomalies, the reduction of data scope, and the construction of reusable data objects as common services.<>", "num_citations": "9\n", "authors": ["37"]}
{"title": "Design Science Research Opportunities in Health Care\n", "abstract": " Design science research (DSR) seeks to grow technology and science knowledge bases via the creation of innovative artifacts that solve problems and improve the environments in which they are instantiated. The field of healthcare offers many opportunities for the use of DSR to design innovations with a goal of improving the effectiveness and efficiencies of healthcare products, services, and delivery. We identify areas of opportunity via a qualitative survey of recent healthcare projects that apply DSR methods. Two cases are explored in more detail to demonstrate the use of DSR in challenging healthcare environments. Goals of purposefulness and innovation are identified in the projects. We conclude with a discussion of how DSR can have a broader impact on future research and practice in healthcare.", "num_citations": "8\n", "authors": ["37"]}
{"title": "Integrating joint application development (JAD) into cleanroom development with ICASE\n", "abstract": " The authors demonstrate the integration of joint application development (JAD), a process that permits customer/developer communications, into cleanroom software engineering. Integrated computer aided software engineering (CASE) is used in conjunction with JAD and cleanroom software engineering for the production of quality software systems. A detailed overview of JAD is provided, with emphasis on its use in the cleanroom system development process (CSDP).< >", "num_citations": "8\n", "authors": ["37"]}
{"title": "Concurrent system design with box structures\n", "abstract": " The author demonstrates the natural extension of the box structure theory and techniques of system development to concurrent system design. The semantics and syntax of box structures for designing concurrency are presented. Concurrent box structure designs for several concurrent system environments are shown. Extensions to more complex environments are suggested.< >", "num_citations": "8\n", "authors": ["37"]}
{"title": "Applying behavioral economics in predictive analytics for B2B churn: Findings from service quality data\n", "abstract": " Motivated by the long-standing debate on rationality in behavioral economics and the potential of theory-driven predictive analytics, this paper examines the link between service quality and B2B churn. Using longitudinal B2B transactional data with service quality indicators provided by a large company, we present evidence that both rationality and bounded-rationality assumptions play significant roles in predicting organizational decisions on churn. Specifically, variables that relate to the assumed rationality of organizations appear to provide accurate predictions while, at the same time, variables that capture boundedly rational decision rules appear to play a role through \u0393\u00c7\u00a3somatic states\u0393\u00c7\u00a5 that make organizations more sensitive to the rational variables. In addition to presenting a novel approach for predicting organizational decisions on churn, this paper offers theoretical and managerial insights as well as\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["37"]}
{"title": "Artifact and/or theory? Publishing design science research in IS\n", "abstract": " With the rising interest in Design Science Research (DSR), it has become crucial to understand what the acceptable components of a DSR paper are, in order to get published. Central to this is the ongoing debate on what constitutes an acceptable contribution in DSR-the artifact, the design theory or both? Two camps have emerged in this debate, and this panel is setup to engage thought leaders from both sides in a scholarly discourse. At the end, we aim to have moved a step towards collectively charting a path for the future of Design Science in the IS discipline.", "num_citations": "7\n", "authors": ["37"]}
{"title": "Provider perspectives of telemedicine encounter quality\n", "abstract": " Telemedicine systems have the potential to revolutionise healthcare by enhancing quality and efficiency through virtual access. To serve this goal, it is fundamental to understand the telemedicine encounter process. Holistic (sociotechnical) understanding of telemedicine encounter quality is still limited. We conduct this study from the position of the medical professionals who directly use the technology and are accountable for patient care. We use a field study comprising multiple methods (direct observation, interview, survey) and perspectives (researcher, provider) to collect and interpret data. Based on the data, we develop a rich model of telemedicine encounter quality attributes. We discuss the derived quality attributes (including the relevance of each attribute) and identify opportunities for their control in terms of who can apply control and when the control is best applied. Practical implications and future\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["37"]}
{"title": "Flow-Service-Quality (FSQ) Engineering for the Specification of Complex Systems\n", "abstract": " Modern enterprises are irreversibly dependent on large-scale, component-based information systems whose complexity frequently exceeds current engineering capabilities for intellectual control, resulting in persistent difficulties in system specification, development, management, and evolution. Flow-Service-Quality (FSQ)Engineering  provides an innovative solution for gaining control of complex systems development. In dynamic network information systems with constantly varying function and usage, flows and their corresponding traces of system services act as stable foundations for functional and non-functional (quality attribute) specification, design, and operational control. The objective of FSQ research and development is to provide theoretical foundations, language representations, and rigorous yet practical unified engineering methods to represent and reason about system flows as essential artifacts\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["37"]}
{"title": "A Guide to Performance Evaluation of Database Systems.\n", "abstract": " Benchmarking is one of several alternate methods of performance evaluation, which is a key aspect in the selection of database systems. The purpose of this report is to provide a performance evaluation methodology, or benchmarking framework, to assist in the design and implementation of a wide variety of benchmark experiments. The methodology, which identifies criteria to be utilized in the design, execution, and analysis of a database system benchmark, has been applied to three different database systems representative of current minicomputer, microcomputer, and database machine architectures. This generalized methodology can apply to most database system designs. In addition to presenting a wide variety of possible considerations in the design and implementation of the benchmark, this methodology can be applied to the evaluation of either a single system with several configurations, or to the comparison of several systems. A summary of the report identifies the three principal phases of a database system benchmark--benchmark design, execution, and analysis--and notes that no generalized methodology can provide a complete list of considerations for the design of an actual experiment. Seventy references are listed.(DJR)", "num_citations": "7\n", "authors": ["37"]}
{"title": "Citizen data scientist: A design science research method for the conduct of data science projects\n", "abstract": " Firms are seeking to gain greater understanding of and insights into more and more massive quantities of data collected and stored in disparate public and private databases. To effectively and efficiently deploy project resources to the data science search activity and to consequently build and evaluate innovative artifacts, firms are finding that a Design Science Research (DSR) approach can extend into the Data Science (DS) project domain through an iterative, evaluative project management method for the diagnosing, design, implementation, and evolution of data science artifacts. Importantly, DSR also provides a guided, emergent search paradigm that can be integral to finding hidden insights in massive data where the problem and solution domains are both frequently poorly understood at the outset of the DS inquiry. This article examines a case for using the elaborated action design research (eADR\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["37"]}
{"title": "A pragmatic approach for identifying and managing design science research goals and evaluation criteria\n", "abstract": " The effectiveness of a Design Science Research (DSR) project is judged both by the fitness of the designed artifact as a solution in the application environment and by the level of new research contributions. An important and understudied challenge is how to translate DSR project research goals into discrete and measurable evaluation criteria for use in the DSR processes. This position paper proposes an inclusive approach for articulating DSR goals and then identifying project evaluation criteria for these goals. The goals are organized hierarchically as utilitarian goals, safety goals, interaction and communication goals, cognitive and aesthetic goals, innovation goals, and evolution goals. Goals in a DSR project are identified pragmatically by considering the components of the context coupled with the hierarchy of goals. Based on the identified goals, the associated evaluation criteria are determined and organized along the same hierarchy. These criteria measure the ability of the artifact to meet its goals in itscontext (immediate fitness). Moreover, our approach also supports the innovation and research contributions of the project. The apex of the goal hierarchy addresses the identification of criteria measuring the fitness for evolution of the designed artifact, to accommodate for changes in goals or context.", "num_citations": "6\n", "authors": ["37"]}
{"title": "Neurophysiological analysis of visual syntax in design\n", "abstract": " Creative design activities in the development of software-intensive systems involve the wide use of visual tools, such as flowcharts and UML diagrams. In this research-in-progress paper, we explore the potential of eye fixation related potential (EFRP) as a method to assess the efficacy of visual notations used to build and evaluate IT artifacts. Drawing on past work in the areas of visual syntax and semantics, we ask whether selection of visual forms is a significant predictor of design artifact quality and utility. In particular, we propose a study that combines the use of EEG and EFRP methods to analyze the neurophysiological correlates of how designers employ visual syntax in the development of IT artifacts for software-intensive systems. Implications for both research and practice are discussed.", "num_citations": "6\n", "authors": ["37"]}
{"title": "Applying emergent outcome controls to mitigate time pressure in agile software development\n", "abstract": " Can agile software development methods handle time pressure effectively? In this research-in-progress paper we examine the sources and remedies for time pressure in an agile software development project. We draw upon research on emergent outcome controls to understand how they can be used effectively to handle time pressure. In particular, we use Extreme Programming (XP) as an agile development exemplar and propose 3 interesting research propositions. Further, we discuss the limitations, practical implications, and future research efforts on how emergent outcome controls can be used to balance aspects of quality, time, and cost in software development.", "num_citations": "6\n", "authors": ["37"]}
{"title": "Evaluating a disruptive innovation: function extraction technology in software development\n", "abstract": " Innovation in any field comes from disruption in normal behaviors, patterns, and methods of performing work. Software engineering can benefit from innovative technologies to improve quality and productivity in software development. However, it is often difficult to provide convincing evidence to key stakeholders, as a basis for understanding and investing resources in disruptive innovations. Function Extraction (FX) is an emerging technology for automated computation of software behavior. In this paper, we posit that FX is a potentially disruptive technology that can have a major impact on software development. We discuss the challenges of effectively evaluating FX to convince stakeholders that it is worth investment of time and resources. We request active participation of conference attendees to help evaluate the technology as a disruptive innovation.", "num_citations": "6\n", "authors": ["37"]}
{"title": "Guest editorial: Introduction to the special issue on modeling and implementation of service enterprise systems\n", "abstract": " The services industry is one of the most important and fast-growing fields in computing. Businesses are rapidly becoming aware of the ubiquitous presence and significant advantages of service-oriented computing and supply chains. However, the heterogeneity of online businesses and their applications makes interoperability, an essential requirement for business globalization, a huge challenge. Many businesses, such as Amazon. com, United Airlines, and Motorola, have implemented or transformed their online business applications into Web Services using a variety of service infrastructures provided by vendors including IBM, SAP, Microsoft, Oracle, and BEA. As a result, the Internet is shifting from a repository of data to a repository of services. Gartner predicts that the worldwide Software as a Service (SaaS) market will grow from $6.3 billion in 2006 to $19.3 billion by the end of 2011.Services Computing is a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["37"]}
{"title": "Data analytics for bioterrorism surveillance\n", "abstract": " Research on techniques for effective bioterrorism surveillance is limited by the availability of data from the few actual bioterrorism incidents and the difficulty of designing and executing simulated bioterrorism attacks for study. In this research, we describe a preliminary study of a naturally occurring incident, the Florida wildfires from January to June 2001, as a reasonable facsimile of a bioterrorism attack. Hospital admissions data on respiratory illnesses during that period are analyzed to uncover patterns that might be expected from an airborne terrorism attack. The principal contributions of this research are the online analytic processing (OLAP) techniques employed to study the adverse effects of this natural phenomenon. These techniques could provide important capabilities for epidemiologist-in-the-loop surveillance systems, enabling the rapid exploration of unusual situations and guidance for follow-up\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["37"]}
{"title": "Integrated decision support: A data warehousing perspective\n", "abstract": " Successfully supporting managerial decision making has become critically dependent upon the availability of integrated, high quality information organized and presented to managers in a timely and easily understood manner. Data warehouses have emerged to meet this need. Surrounded by analytical tools and models, data warehouses have the potential to transform operational data into business intelligence; enabling effective problem and opportunity identification and critical decision making, as well as strategy formulation, implementation, and evaluation (Simon 1977). In this paper, we explore the nature of data warehousing for integrated decision support focusing on research issues and their impact on practice. Conceptually a data warehouse is extremely simple. As popularized by Inmon (2002) and Inmon and Hackathorn (1994) it is a\" subject-oriented, integrated, time-invariant, nonupdatable collection of data used to support management decision-making processes and business intelligence.\" A data warehouse is a repository into which are placed all data relevant to the management of an organization and from which emerge the information and knowledge needed to effectively manage the organization. While this is clearly a simplistic and idealistic view of data warehousing, it allows us to begin the investigation of key challenges and research directions for this discipline.", "num_citations": "6\n", "authors": ["37"]}
{"title": "Data warehouse dissemination strategies for community health assessments\n", "abstract": " The Comprehensive Assessment for Tracking Community Health (CATCH) methodology provides a systematic framework for community-level assessment that can be a valuable tool for resource allocation and health care policy formulation. CATCH utilizes health status indicators from multiple data sources, using an innovative comparative framework and weighted evaluation process to produce a rank-ordered list of critical community health care challenges. The community-level focus is intended to empower local decision-makers and provide a clear methodology for organizing and interpreting relevant health care data.", "num_citations": "6\n", "authors": ["37"]}
{"title": "Community Healh Assessments: A Data Warehousing Approach\n", "abstract": " The measurement and assessment of health status in communities throughout the world is a massive information technology challenge. The Comprehensive Assessment for Tracking Community Health (CATCH) methodology provides a systematic framework for community-level assessment that can be a valuable tool for resource allocation and health care policy formulation. CATCH utilizes health status indicators from multiple data sources, using an innovative comparative framework and weighted evaluation process to produce a rank-ordered list of critical community health care challenges. The community-level focus is intended to empower local decision-makers and provide a clear methodology for organizing and interpreting relevant health care data. The effectiveness of the CATCH methodology is based on a data warehousing approach. The data warehouse allows a core set of reports to be produced at a reasonable cost for community use. In addition, online analytic processing (OLAP) functionality can be used to gain a deeper understanding of the health care issues. The data warehouse in conjunction with Internet-enabled dissemination methods will allow the information to be presented in a variety of formats and be distributed more widely in the decision-making community. On-going research directions in community health care decision making conclude the paper.", "num_citations": "6\n", "authors": ["37"]}
{"title": "Eras of business computing\n", "abstract": " The past half-century has seen amazing progress in the use of information technology and computer systems in business. Computerization and communication technologies have truly revolutionized the business organization of today. This chapter presents a structured overview of the evolution of business computing systems through six distinct eras:u\u0393\u00c7\u00f3 Era of Calculation\u0393\u00c7\u00f3 Era of Automation\u0393\u00c7\u00f3 Era of Integration and Innovation\u0393\u00c7\u00f3 Era of Decentralization\u0393\u00c7\u00f3 Era of Reengineering and Alignment\u0393\u00c7\u00f3 Era of the Internet and Ubiquitous ComputingAdvances in each of the major computing technologies\u0393\u00c7\u00f6Computational Platform, Communications, Software, and System Architecture\u0393\u00c7\u00f6are surveyed and placed in the context of the computing eras. The focus is on how technologies have enabled innovative strategic business directions based on new business system architectures. A key observation is that around 1975 the principal role\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["37"]}
{"title": "Executive workstations: Issues and requirements\n", "abstract": " An executive workstation is an integrated hardware/ software system that can provide business executives with powerful capabilities for making and implementing decisions. In this paper, relevant and sometimes controversial, issues and workstation requirements are discussed in the context of a scenario that portrays an executive using a workstation. The scenario anticipates fifth generation computer technology and it guides a specification of features and functions. The goal is to clarify what is possible and desirable in an executive workstation design.", "num_citations": "6\n", "authors": ["37"]}
{"title": "Transaction optimization in a distributed database testbed system\n", "abstract": " The design of the transaction optimization portion of the distributed database testbed system at the Honeywell Corporate Technology Center is described. In the design, data flow analysis and distributed query optimization techniques are applied to distributed transaction optimization. The theoretical foundations of this approach are presented, and a comprehensive example is given. 16 references.", "num_citations": "6\n", "authors": ["37"]}
{"title": "Flow semantics for intellectual control in IoT systems\n", "abstract": " Effective decision making is critical for operation and control of Internet of Things (IoT) systems. These systems are comprised of massive numbers of hardware, software and network components; all sensing, computing, communicating and controlling in dynamic architectures and state spaces of extraordinary complexity. Components, connections and users will come and go, quality and reliability will be unpredictable, and failures and intrusions will be ever-present. It is vital that these systems be rigorously designed, developed, and governed; but how can a rigorous engineering discipline be defined for an environment where the capability, connectivity, and integrity of components will vary from moment to moment? In this paper, we focus on Flow Semantics as a rigorous engineering foundation for analysis, development, evolution, operation, and governance of IoT systems in such unpredictable environments. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["37"]}
{"title": "The hows and whys of information markets\n", "abstract": " The use of information markets as a business intelligence (BI) technique for collecting dispersed intelligence and forming knowledge to support decision making is growing rapidly in many application fields. The objective of this chapter is to present a focused survey of how information markets work and why they produce accurate and actionable knowledge upon which effective decisions can be based. Numerous exemplars from the literature are described and key future research directions in information markets are highlighted.", "num_citations": "5\n", "authors": ["37"]}
{"title": "Design Science Research in Information Systems\n", "abstract": " Design activities are central to most applied disciplines. Research in design has a long history in many fields including architecture, engineering, education, psychology, and the fine arts (Cross 2001). The computing and information technology (CIT) field since its advent in the late 1940s has appropriated many of the ideas, concepts, and methods of design science that have originated in these other disciplines. However, information systems (IS) as composed of inherently mutable and adaptable hardware, software, and human interfaces provide many unique and challenging design problems that call for new and creative ideas. The design science research paradigm is highly relevant to information systems (IS) research because it directly addresses two of the key issues of the discipline: the central, albeit controversial, role of the IT artifact in IS research (Weber 1987; Orlikowski and Iacono 2001; Benbasat and Zmud 2003) and the perceived lack of professional relevance of IS research (Benbasat and Zmud 1999; Hirschheim and Klein 2003). Design science, as conceptualized by Simon (1996), supports a pragmatic research paradigm that calls for the creation of innovative artifacts to solve real-world problems. Thus, design science research combines a focus on the IT artifact with a high priority on relevance in the application domain. A tradition of design science research in the IS field has been slow to coalesce. Research in IS has been dominated by studies of the impacts of IT artifacts on organizations, teams, and individuals. Design research was considered the province of more technical disciplines such as computer science and electrical\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["37"]}
{"title": "On design theory\n", "abstract": " Science progresses because of advancement in theories. Dictionary definitions show that the word theory can take on many meanings, including \u0393\u00c7\u00a3a mental view\u0393\u00c7\u00a5 or \u0393\u00c7\u00a3contemplation,\u0393\u00c7\u00a5 \u0393\u00c7\u00a3a concept or mental scheme of something to be done, or the method of doing it; a systematic statement of rules or principles to be followed,\u0393\u00c7\u00a5 a \u0393\u00c7\u00a3system of ideas or statements held as an explanation or account of a group of facts or phenomena; a hypothesis that has been confirmed or established by observation or experiment and is propounded or accepted as accounting for the known facts; statements of what are held to be the general laws, principles, or causes of something known or observed,\u0393\u00c7\u00a5 a \u0393\u00c7\u00a3mere hypothesis, speculation, conjecture\u0393\u00c7\u00a5 (Gregor 2006).", "num_citations": "5\n", "authors": ["37"]}
{"title": "Design and Creativity\n", "abstract": " Abraham Maslow once said \u0393\u00c7\u00a3The key question isn\u0393\u00c7\u00d6t \u0393\u00c7\u00ffWhat fosters creativity?\u0393\u00c7\u00d6 But it is why in God\u0393\u00c7\u00d6s name isn\u0393\u00c7\u00d6t everyone creative? Where was the human potential lost? How was it crippled? I think therefore a good question might be not why do people create? But why do people not create or innovate? We have got to abandon that sense of amazement in the face of creativity, as if it were a miracle if anybody created anything.\u0393\u00c7\u00a5             Every designer is creative. In the world of software design, we also create artifacts. Where does this creativity come from? What exactly is meant to be creative? In this chapter we explore questions such as these. We also take a brief look at the creativity literature and discuss how information technology tools can help humans become more creative and vice versa.", "num_citations": "5\n", "authors": ["37"]}
{"title": "Feasible input domain partitioning in software testing: RCS case study\n", "abstract": " The efficiency of software testing can be improved by partitioning the input domain. In this paper we examine how partitions of the input domain can be established by utilizing the software\u0393\u00c7\u00d6s state design. The improvements are demonstrated with a case study.", "num_citations": "5\n", "authors": ["37"]}
{"title": "Eliminating non-traversable paths from structured programs\n", "abstract": " A formal procedure is given based on the use of regular expressions, to identify and eliminate nontraversable paths in a structured program. By reducing the use of control variables, the program is transformed into an unstructured variant and then restructured into a semantic equivalent of the original program with nontraversable paths eliminated. The authors demonstrate this procedure with an example and discuss its maintenance advantages.<>", "num_citations": "5\n", "authors": ["37"]}
{"title": "Attentional characteristics of anomaly detection in conceptual modeling\n", "abstract": " We use eye tracking to better understand the attentional characteristics specific to successful error detection in conceptual models. This phase of our multi-step research project describes the visual comportments associated with successful semantic and syntactic error identification and diagnosis. We test our predictions, based on prior studies on visual attention in an error detection task, or studies comparing experts and non-experts in diverse tasks, in a controlled experiment where participants are tasked with detecting and diagnosing errors in 75 BPMN\u252c\u00ab models. The results suggest that successful error diagnostics are linked with shorter total view time and shorter fixation duration, with a significant difference between semantic and syntactic errors. By identifying the visual attention differences and tendencies associated with successful detection tasks and the investigation of semantic and syntactic errors\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["37"]}
{"title": "Designing the Digital Transformation: 12th International Conference, DESRIST 2017, Karlsruhe, Germany, May 30\u0393\u00c7\u00f4June 1, 2017, Proceedings\n", "abstract": " This book constitutes the proceedings of the 12th International Conference on Design Science Research in Information Systems and Technology, DESRIST 2017, held in May/June 2017 in Karlsruhe, Germany. The 25 full and 11 short papers presented in this volume were carefully reviewed and selected from 66 full and 19 short papers. The contributions are organized in topical sections named: DSR in business process management; DSR in human computer interaction; DSR in data science and business analytics; DSR in service science; methodological contributions; domain-specific DSR applications; emerging themes and new ideas; and products and prototypes.", "num_citations": "4\n", "authors": ["37"]}
{"title": "The Digital Innovation Design Activities Wheel\n", "abstract": " While the importance of innovation as a dominant driver of societal and economic progress is well established, the processes and outcomes of the new phenomena of digital innovation (DI) are diverse. Managers and practitioners should benefit from a sense-making device that helps them better understand the opportunities and practices in the DI landscape. Thus, an original framework named the Digital Innovation Design Activities Wheel is proposed that builds on prior influential work on portfolio approaches to innovation (eg the ambidexterous approach) combined with recent thinking that modern strategic management of innovation should be based on innovative design activities. The Wheel framework relates DI design activities to different digital innovation outcomes in cyclic processes. It integrates work on portfolio approaches to innovation, the Knowledge-Innovation Matrix (KIM), theory of creativity, knowledge flows and innovation design activities in the context of DI. The framework is informed by analysis of case studies of DI of different types. We conclude with an agenda for future research to extend DI innovation theorizing and with actionable advice for improving current practices of innovation.", "num_citations": "4\n", "authors": ["37"]}
{"title": "Design science and innovation practices: A Delphi study\n", "abstract": " Current innovation practice demonstrates many of the ideas of design thinking. In this paper, we explore the readiness of innovation practice to include design science concepts and processes. At a recent conference, an initial round of a Delphi study engaged a number of innovation practitioners in a discussion around the question, \u0393\u00c7\u00a3Is design science the future of innovation?\u0393\u00c7\u00a5 The results of the study are presented as summaries of opportunities and challenges in bringing design science into innovation practice.", "num_citations": "4\n", "authors": ["37"]}
{"title": "Measuring information volatility in a health care information supply chain\n", "abstract": " We propose a measure of reliability called information volatility (IV) to complement Business Intelligence tools when considering aggregated data or when observing trends. Two types of information volatility are defined: intra-cell and inter-cell. For each, two types of distributions are considered: normal and lognormal, which is often the case for time series data. The IV measures are based on similar measures found in the finance literature, since there are similarities in the types of data. In order to understand the information volatility metrics, the notion of benchmarking is introduced with three propositions: numerical benchmarking, graphical benchmarking and categorical benchmarking. The IV metric is designed and evaluated using the design science research paradigm: first, the metric is developed and then it is evaluated through the use of focus groups (including several cycles for refinement of the design). The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["37"]}
{"title": "Management of Softwrare Product Development, Innovation and Adaptability\n", "abstract": " This inductive study develops a model of innovation and adaptability in software product development. It is based on a case study of a company that is transitioning from a custom development approach to a product-based solution. The emergent model represents a synthesis of the case study findings and the enfolding literature from traditional product development and software development. The goal of the emergent software product development model is to guide organizations in their selection of development processes.", "num_citations": "4\n", "authors": ["37"]}
{"title": "Editorial statement: Information technology and systems (ITS) department\n", "abstract": " This paper introduces the Information and Technology Systems (ITS) Department of CAIS. The department focuses on articles in design science. The goal of design-science research is the development and evaluation of technologies that extend the boundaries of human and organizational information-processing capabilities. Research must demonstrate the utility of such technologies to address problems or tasks not previously thought to be amenable to IT support. The article presents the objectives, concepts, and publication procedures for the ITS Department.", "num_citations": "4\n", "authors": ["37"]}
{"title": "Object-oriented design with box structures\n", "abstract": " There are many discussions in the literature today about object-oriented designs but few about systematic methods for object-oriented designing. There is a profound difference between a syntax correct object-oriented design and a sound engineering process for achieving one, often it is the difference between heuristic invention and systematic derivation. In either case the syntactic result is an object-oriented design, but practical results can vary widely in effectiveness.", "num_citations": "4\n", "authors": ["37"]}
{"title": "The analysis and design of embedded knowledge-based systems using box structure methods\n", "abstract": " Knowledge-based systems are widely used for decision support system and expert system applications. However, they are not a panacea for all business problems. Thorough requirements analysis methods must determine the need for knowledge-based systems vis-\u251c\u00ed-vis alternative information system designs. Furthermore, knowledge-based systems are often used as embedded subsystems within larger organizational information systems. Rigorous design methods must support the development of complex information systems with integrated, heterogeneous subsystems, including knowledge-based subsystems. In this paper, we demonstrate that the box structure methods of information systems development support both the requirements analysis of knowledge-based systems and the design of embedded knowledgebased systems. An extended example is presented to show the use of the box structure methods\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["37"]}
{"title": "Central repository data models for cleanroom systems development\n", "abstract": " An integrated cleanroom systems engineering environment must have at its foundation an effective central repository. The authors describe the cleanroom systems development process (CSDP) and discuss the necessary development information to be stored in the repository. Detailed data models for box structure requirements and design information are presented. They then discuss the use of the central repository in the cleanroom development process with supporting CASE tools. The paper concludes with future research directions for a improved cleanroom repository.< >", "num_citations": "4\n", "authors": ["37"]}
{"title": "Object-oriented system development methods\n", "abstract": " Publisher SummaryThis chapter explains what is meant by object-orientation, surveys current methods of object-oriented system development, and presents an integrated methodology for performing object-oriented system development. The chapter defines objects and explains object-oriented concepts as a basis for understanding the object-oriented system development methods. Armed with a basic understanding of object-oriented concepts, principles, and terminology, the chapter surveys the background of research and practice in the object-oriented field. The chapter surveys representations of the object-oriented system life cycle and discusses existing methods, techniques, and tools that support the development phases of object-oriented analysis, object-oriented design, and object-oriented implementation and testing. Brief surveys of research and development projects on object-oriented system\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["37"]}
{"title": "A box structured methodology for solving business problems\n", "abstract": " A box-structured methodology for solving business problems using available management science techniques is proposed. The procedure is free of bias caused by the choice of solution technique. Abstraction levels are developed for available management-science solution techniques. A mapping algorithm is defined that identifies what technique, or path of techniques, can be used to solve the problem. The abstraction categories are used in the decision process to narrow the search space of applicable techniques. The box-structured methodology supports the user evaluation of alternative solution techniques and provides a procedure for implementing the solution within an information system.< >", "num_citations": "4\n", "authors": ["37"]}
{"title": "Evaluation of optical disk systems for very large database applications\n", "abstract": " Optical Disk Systems have significant advantages over conventional magnetic mass storage media for very large database applications. Among other features, optical disk systems offer large capacity and high transfer rate. A critical problem is how to integrate the optical disk system into a total application system environment while maintaining the high performance capabilities of the optical disk. In this paper the performance of optical disk system configurations under realistic application environments is analyzed via queueing models. The results provide several important guidelines for the use of optical disk systems on large applications.", "num_citations": "4\n", "authors": ["37"]}
{"title": "Call for papers, issue 1/2019\n", "abstract": " There has been a surge of interest in design science research (DSR) in information systems (IS) in the last decade. DSR is now recognized as an important field of research in IS. The goal of the DSR paradigm is to extend the boundaries of human and organizational capabilities by designing new and innovative artifacts represented by constructs, models, methods, processes, and systems (Hevner et al. 2004; Peffers et al. 2007; Gregor and Hevner 2015). Broadly speaking, DSR aims to add to knowledge of how things can or should be constructed or arranged (ie, designed), usually by human agency, to achieve some desired goal. For example, design knowledge in the information systems (IS) discipline includes knowledge of how to structure and construct a database system, how to model business processes and how to align IS with organizational strategy, and how to employ data analytics for effective decision\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["37"]}
{"title": "Finding evidence for effectual application development on digital platforms\n", "abstract": " The development of novel software applications on digital platforms differs from traditional software development and provides unique challenges to software development managers and teams. Platform-based applications must achieve application-platform match, application-market match, value propositions exceeding platform\u0393\u00c7\u00d6s core value propositions, and novelty. These desired properties support a new vision of the software development team as entrepreneurs with a goal of developing novel applications on digital platforms. In this research study, we look for evidence on an open-source software development project \u0393\u00c7\u00f4 Apache Cordova \u0393\u00c7\u00f4 that development teams use effectual thinking. Over one thousand user stories are analyzed for the use of constructs from a proposed effectual software development research model. The findings provide an initial confirmation that effectual development methods\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["37"]}
{"title": "A social representations analysis of design science research\n", "abstract": " Compared to positivist survey and interpretive case study research, design science research (DSR) is a relatively novel and unfamiliar research paradigm within the computing field in South       Africa. In light of recent interest in the DSR paradigm, this study sought to investigate how local computing researchers familiarise themselves with an unfamiliar paradigm and what their       perspectives of DSR are. Key theoretical concepts from social representations theory (SRT), such as 'anchoring' and 'objectification', were used to explore how researchers constructed their       understanding of DSR. A visual approach was used to administer drawing and association tasks to two focus groups; each focus group comprised around 25 participants ranging from doctoral       students to experienced researchers. The focus group discussions invoked interesting complementary and distinctive associations about the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["37"]}
{"title": "Mathematical aspects of box structures\n", "abstract": " The box structures of black box, state machine, and clear box, which are fundamental objects for analysis and design of information systems in the box structure methodology, are considered. Mathematical properties of each of the box structures are presented. Derivation and expansion tasks provide a means of converting a system from one box structure representation to another. An actual case study involving the US Navy power system demonstrates the use of derivation and expansion in the analysis of a supply reorder policy.<>", "num_citations": "3\n", "authors": ["37"]}
{"title": "The duality of science: Knowledge in information systems research\n", "abstract": " Rigorous research in Information Systems requires an understanding of how scientific foundations drive both the process and outcome. This science duality addresses the effective application of the scientific method (process) as well as the advance of knowledge (outcome) from the research project. This commentary responds to Siponen and Klaavuniemi\u0393\u00c7\u00d6s paper with a focus on the uses of knowledge in Information Systems research.", "num_citations": "2\n", "authors": ["37"]}
{"title": "Design Science Research Modes in Human-Computer Interaction Projects\n", "abstract": " In this editorial, we introduce the special issue on design science research in human-computer interaction with four papers extended from the 2020 European Conference on Information Systems and propose a conceptual model for such research projects. Research in the interdisciplinary human-computer interaction (HCI) discipline advances knowledge of how humans interact with technologies, systems, information, and work structures. Design science research (DSR) methods support three distinct modes in HCI projects. In the interior mode, researchers build and evaluate novel technical solutions with a focus on improved system interfaces to support effective human use. Next, in the exterior mode, researchers build and evaluate novel behavioral solutions with a process focus on interactions that increase human capabilities. Lastly, in the gestalt mode, researchers build and evaluate novel composite solutions that improve synergies between technologies and human behaviors. We pose a comprehensive model for identifying the DSR modes of HCI research with related artifacts, evaluation techniques, design theories, and research impacts.", "num_citations": "2\n", "authors": ["37"]}
{"title": "What can be controlled: actionable ICT4D in the case of Palestine\n", "abstract": " A thriving information and communication technology (ICT) economy is an aspiration for developing countries. This research identifies the factors that can motivate or inhibit ICT opportunities in a developing country to grow a sustainable economy. We build an ICT4D decision framework that provides a three-dimensional view based on (1) key factors (e.g. infrastructure, policies), (2) the ICT supply chain, and (3) stakeholders (e.g. industry, government, academia). The framework is applied to the case of Palestine. We use secondary and primary data to understand how both controllable and non-controllable country characteristics have contributed to or inhibited the growth and development of an ICT sector. Results from extensive secondary data sources demonstrate the usability of the framework to analyze the current setting of the ICT sector, in addition to help investigate a range of possible opportunities for action\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["37"]}
{"title": "Control Portfolio Adaptation in Scrum: Initial Findings from a Practitioner Survey.\n", "abstract": " Software development processes are defined by control practices such as daily stand-ups or burndown charts. Although previous research has thoroughly investigated the antecedents of control selection and the consequences of control enactment, there is a lack of research into the dynamic changes that are made to a control portfolio throughout a project's life cycle. In preparation for a large, future field study, we conducted (i) a literature review to create a list of practices that are common in Scrum pro-jects and (ii) a survey with selected interviews of 16 Scrum practitioners. Our goals are to investigate whether these agile control practices are used, and if so, how intensively, and whether their use changes over a project\u0393\u00c7\u00d6s life cycle (ie, whether they are used throughout the entire project, and added or removed during the project). We find initial evidence for elements in the project environment, project outcomes, and experiences with control mechanisms to be main triggers of change to a control portfolio. Further, we identify different modes of adaptations (eg, control portfolio adaptation vs practice modification). Based on these findings we propose a research model and an outline for our future longitudinal field study.", "num_citations": "2\n", "authors": ["37"]}
{"title": "Designing the digital transformation: DESRIST 2017 research in progress proceedings\n", "abstract": " This volume contains selected research in progress papers at DESRIST 2017 \u0393\u00c7\u00f4 the 12th International Conference on Design Science Research in Information Systems and Technology held during May 30 \u0393\u00c7\u00f4 June 1, 2017, at Karlsruhe, Germany. This year\u0393\u00c7\u00d6s DESRIST conference continues the tradition of advancing and broadening design research within the information systems discipline. DESRIST brings together researchers and practitioners engaged in all aspects of Design Science Research (DSR), with a special emphasis on nurturing the symbiotic relationship between Design Science researchers and practitioners. As in previous years, scholars and design practitioners from various areas, such as information systems, business & operations research, computer science, and industrial design come together to discuss both challenges and opportunities of Design Science and to solve design problems through the innovative use of information technology and applications. The outputs of DESRIST, new and innovative constructs, models, methods, processes, and systems, provide the basis for novel solutions to design problems in many fields. The conference further builds on the foundation of eleven prior highly successful international conferences held in Claremont, Pasadena, Atlanta, Philadelphia, St. Gallen, Milwaukee, Las Vegas, Helsinki, Miami, Dublin, and St. Johns. The 12th DESRIST conference has the theme \u0393\u00c7\u00a3Designing the Digital Transformation\u0393\u00c7\u00a5 and emphasizes the contemporary challenge of transforming businesses and society using information technologies. The rapid digital transformation of businesses and society creates new\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["37"]}
{"title": "The DRIVES (Design research for innovation value, evaluation, and sustainability) model of innovation\n", "abstract": " We propose a new innovation model that leverages Design Science Research concepts and principles. The goal of this paper is to outline the six stages of DRIVES with brief descriptions of the activities performed during the stages. An industrial consortium provides some observations on the application of DRIVES for innovation.", "num_citations": "2\n", "authors": ["37"]}
{"title": "Dynamic systems security testing using function extraction\n", "abstract": " Extraction (FX) technology to the dynamic testing of security in large-scale, operational software systems. FX is used proactively as an intrusion detection and prevention system (IDPS) within a security infrastructure surrounding the operation of a critical software system. An innovative aspect of the FX approach is the concept of computational security attributes (CSA). The CSA approach to software security analysis provides theory-based foundations for precisely defining and computing security attribute values. The translation of a static security property expressed as an abstraction of external data to the dynamic behavior of a program expressed in terms of its data and functions is a key to the CSA approach for verification of behaviors that meet specific security properties. The paper concludes with a discussion of future research and development directions for applying FX to dynamic security testing.", "num_citations": "2\n", "authors": ["37"]}
{"title": "Meinung/Dialog\n", "abstract": " Attendance to and standards for relevance and rigour are critical for any research field. Any field should strive to be both highly relevant and highly rigorous in its research. If a field, especially an applied field like IS, does not study topics that have some promise of applicability to practice (relevance), it is unlikely to be funded or listened to. If a field does not produce research results that are correct (truthful) and credible (or worse, publishes work that is later proven to be false), then it risks becoming untrustworthy and also not funded or listened to. Ultimately, achieving relevance and rigour affect the reputation of a field and its long-term existence. Having and diligently applying appropriate standards and guidelines for decision-making about acceptable relevance and rigour is important for any field. But what standards should we have for the field of IS or Informatics? What contingent factors, if anything, about our field influence acceptable levels of relevance and rigour in our research?", "num_citations": "2\n", "authors": ["37"]}
{"title": "Next-generation software engineering introduction to minitrack\n", "abstract": " Societal dependence on largescale, networkcentric systems continues to grow. But cost and complexity are increasing, and errors and vulnerabilities persist despite best efforts. Evidence suggests that software engineering is reaching the limits of development technologies evolved in the first fifty years of computing. A need exists to create a next-generation software engineering (NGSE) for the next fifty years that will reduce cost and complexity for fast and correct development of the ultralargescale systems of the future. NGSE is envisioned as a computational engineering discipline, largely enabled by theorybased, semanticsdirected automation that permits intellectual control at a scope and scale unattainable with present methods.", "num_citations": "2\n", "authors": ["37"]}
{"title": "Rating the health status of US communities.\n", "abstract": " Although the existence of small-area variation in health care utilization and quality had been acknowledged decades ago, and the public release of data about the performance of hospitals and physicians is no longer controversial, the wide range of variability in the health status of US communities has received relatively little attention. The authors demonstrate (using Florida data) an empirically derived national system for rating the health status of communities, presented in a simplified consumer-type format, using a symbol-graded report card. This system is intended to keep the symbols of poor health status prominently in the minds and on the political agendas of community leaders.", "num_citations": "2\n", "authors": ["37"]}
{"title": "Achieving software quality through Cleanroom software engineering\n", "abstract": " Describes a success story in the use of modern technologies for software quality improvement. The Cleanroom software engineering process for zero-defect software has been successfully applied on development projects in a variety of environments with remarkable quality results. Cleanroom is based on formal, theory-based methods for software specification, design, correctness verification, and statistical quality certification. The authors survey a number of Cleanroom projects and demonstrate achievement of the following objectives: superior quality through Cleanroom software development; successful, cost-effective Cleanroom technology transfer to software development teams; and sharp reduction in effort to maintain and evolve Cleanroom software products.< >", "num_citations": "2\n", "authors": ["37"]}
{"title": "Network Database Design Methods.\n", "abstract": " The network data model is widely used as the data structure base of many popular database management systems. IDMS (10), IDS/II (19), DMS 1100 (24), andTOTAL (7) are among systems that use network structures for conceptual modeling. The network model (described more fully in Chapter 3) provides a flexible and general approach for the representation of relationships among entities. Many-to-many relationships, as well as one-to-many relationships, can be naturally represented as a network. Bachman diagrams (1) provide a convenient way of viewing network structures, as in Figure 8.1.", "num_citations": "2\n", "authors": ["37"]}
{"title": "Multiprocessor database machine architectures based on data flow analysis\n", "abstract": " A method for designing high performance multiprocessor database machine architectures based on data flow analysis is developed. The method consists of decomposing database functions onto replicated processors. This paper describes the decomposition process. Query evaluation strategies are decomposed into groups and mapped to an asynchronously pipelined multiprocessor system. An example illustrates this process in detail. A cost model is described to evaluate the effect of pipelining on the execution times of the strategy in terms of disc accesses, computations and data transmissions. Application of the cost model to various decompositions of the example query provides insights for the design of a multiprocessor database system. 14 references.", "num_citations": "2\n", "authors": ["37"]}
{"title": "Peak cubes in service operations: Bringing multidimensionality into decision support systems\n", "abstract": " Companies like Ritz Carlton, Disney and Verizon are among many who have invested in analytics to improve their customers' service experiences with the firms. Extensive data are collected on all aspects of how customers interact or experience the products or services. Research has shown the importance of the \u0393\u00c7\u00a3peak-end\u0393\u00c7\u00a5 rule in service design; that is, providing a customer with good \u0393\u00c7\u00a3peak\u0393\u00c7\u00a5 service levels and \u0393\u00c7\u00a3ending\u0393\u00c7\u00a5 the service experience with high quality can enhance customer satisfaction and build loyalty. However, previous studies have examined this phenomenon only in contexts with unidimensional service levels. We introduce peak cubes, which enable service designers and scholars to pinpoint prominent service levels in multidimensional service experience profiles\u0393\u00c7\u00f6thereby extending current research on behavioral economics and service design to more general settings. Results indicate the potential\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["37"]}
{"title": "Expertise as a mediating factor in conceptual modeling\n", "abstract": " We use eye-tracking to better understand the notion of expertise in conceptual modeling of complex systems. This research-in-progress paper describes an ongoing experiment to exploit the capacity of eye-tracking to explore the significance of expertise as a mediating factor in conceptual modeling. The proposed methodology highlights the applicability, validity, and potential of well-established eye-tracking methods to measure the effects of expertise. By identifying the differences in the strategies that novices and experts use to search, detect, and diagnose errors, we anticipate being able to help define training curricula appropriate for each level to improve performance and model result quality.", "num_citations": "1\n", "authors": ["37"]}
{"title": "Evaluation of a Commercial IoT Platform\n", "abstract": " Reference architectures for the development of software systems are used to bring clarity to complexity. By design, reference model architectures inform practice and help deliver superior solutions. A significant challenge in reference architecture design is to recreate in a model an approach that provides utility to the practitioner in the real world. Innovative and useful designs tend to either occur by adapting an existing reference architecture to a new context or by exaptation from an existing, successful commercial implementations of a complex system. In either case, we look to evaluate an extant reference architecture in context to inform researcher and practitioner alike. This research initiates a comparison of an existing, innovative commercial IoT platform-developed organically over a period of several years in multiple industry settings-against the Architectural Reference Model developed by IoT-A. The contributions of the paper include (1) an explicit description of the existing commercial IoT platform,(2) a description of the IoT-A reference model,(3) a mapping method for comparison and gap analysis between the two, and,(4) a discussion of a means to enrich IoT-A with the addition of features critical to a commercially viable IoT.", "num_citations": "1\n", "authors": ["37"]}
{"title": "Studying the creation of design artifacts\n", "abstract": " As software and information systems (IS) increase in functional sophistication, perceptions of IS quality are changing. Moving beyond issues of performance efficiency, essential qualities such as fitness for purpose, sustainability, and overall effectiveness become more complex. Creating software and information systems represents a highly interconnected locus in which both the generative processes of building design artifacts and articulating constructs used to evaluate their quality take place. We address this interconnectedness with an extended process-oriented research design enabling multi-modal neurophysiological data analyses. We posit that our research will provide more comprehensive assessments of the efficacy of design processes and the evaluation of the qualities of the resulting design artifacts.", "num_citations": "1\n", "authors": ["37"]}
{"title": "Flow Semantics for Intellectual Control in Smart City IoT Systems\n", "abstract": " Internet of Things (IoT) systems for Smart Cities will be comprised of massive numbers of hardware, software, and network components, all sensing, computing, communicating, and controlling in dynamic architectures and state spaces of extraordinary complexity. Nodes, connections, and users will come and go, quality and reliability will be unpredictable, and failures and intrusions will be ever-present. It is vital that these systems be rigorously designed, developed, and governed; but how can a rigorous engineering discipline be defined for an environment where the capability, connectivity, and integrity of components will vary from moment to moment? Current IoT reference architectures specify system structures, but provide little guidance for the semantic foundations required to create and verify system functionality, security, and quality attributes. In this paper, we focus on Flow Semantics as a rigorous engineering foundation for analysis, development, evolution, operation, and governance of IoT systems in such unpredictable environments. Flow Semantics are overarching engineering artifacts that exhibit desirable properties for IoT system development and operation. They are based on scale-free mathematical foundations of functions and relations, and capitalize on recent advances in automated computation of software behavior. The goal of this research is to develop engineering principles and practices for maintaining intellectual control in Smart City IoT systems.", "num_citations": "1\n", "authors": ["37"]}
{"title": "Business & Information Systems Engineering\n", "abstract": " The journal is receptive to research results from the field of business and information systems engineering as well as cutting-edge business practice findings. Specific solutions for application systems are published only if they serve as a model for other fields of application.The journal also covers important peripheral areas if developments in the narrower sphere of business and information systems engineering are substantially affected. Examples are the impact of computer science on business, individuals, and society as well as issues regarding training and further education.", "num_citations": "1\n", "authors": ["37"]}
{"title": "Beyond usefulness: A fitness-utility model for DSR\n", "abstract": " The Dependent Variable in Design Science Research               Current thinking in design science research (DSR) defines utility as the primary research goal (e.g. [1, p. 80]). In this context, the close relationship of utility to practical usefulness is emphasized. With a goal to increase the impacts of DSR, the search for the dependent variable in DSR requires some rethinking. A pair of alternative dependent variables can be considered: design fitness and design utility. In the case of fitness, we focus on its biological meaning\u0393\u00c7\u00f6the ability of an entity to reproduce itself and evolve from generation to generation. In the case of utility, rather than viewing it as being roughly equivalent to usefulness, we focus on its meaning in fields such as economics and decision sciences, where it serves as the basis for ranking decision alternatives. Naturally, usefulness plays an important role in determining both fitness and utility. Neither of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["37"]}
{"title": "Artifacts and knowledge in design science research\n", "abstract": " Design science research (DSR) has been an important component of Information Systems (IS) research for a considerable period of time and its general acceptance as a legitimate approach to research in IS is increasing. To further a better understanding of DSR in information and communication technology (ICT) disciplines, my presentation explores two important issues:(i) the nature of ICT artifacts and (ii) the role of knowledge for the grounding of DSR projects and the types of contributions that are made via DSR research. The goal is to provide assistance to researchers so that they are better able to identify appropriate ways of consuming and producing knowledge when they are preparing journal articles or other scholarly works and to show how a DSR article can be structured so that a significant contribution to the knowledge base is highlighted. The evolving contents of the presentation are drawn from on\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["37"]}
{"title": "A science of design for software-intensive systems\n", "abstract": " Future complex software-intensive systems (SIS) will be vastly different from the software systems that run today\u0393\u00c7\u00d6s world. Revolutionary advances in hardware, networking, information, and human interface technologies will require entirely new ways of thinking about how software-intensive systems are conceptualized, built, and evaluated. As we envision the future of tera1-computing and even peta2-computing environments, new science of design principles are needed to provide the foundations for managing issues of complexity, composition, quality, cost, and control of software-intensive systems.", "num_citations": "1\n", "authors": ["37"]}
{"title": "Flow-Service-Quality (FSQ) Systems Engineering\n", "abstract": " Modern enterprises are irreversibly dependent on large-scale information systems built from components whose function and quality attributes are not necessarily known a priori. The ad hoc and network-centric nature of these systems means that a complete static analysis of such systems is difficult or impossible. These systems grow and interconnect with other systems in ways that exceed current engineering techniques for intellectual control. We propose a new engineering frameworkfor reasoning about and developing such systems of systems: the Flow-Service-Quality (FSQ) framework. Our aim is to provide rigorous, practical engineering tools and methods to reason about system flows as first-class objects of specification, design, implementation, and operation. System flows are realized as traces of system services, and their quality attributes are treated as dynamic, changing quantities that are measured during system execution.", "num_citations": "1\n", "authors": ["37"]}
{"title": "Naturally occurring incidents as facsimiles for biochemical terrorist attacks\n", "abstract": " Research on techniques for effective bioterrorism surveillance is limited by the availability of data from actual bioterrorism incidents. This research explores the potential contribution of naturally occurring incidents, such as Florida wildfires, as reasonable facsimiles for airborne bioterrorist attacks. Hospital discharge data on respiratory illnesses are analyzed to uncover patterns that might resemble the effects of an aerosolized biological or chemical attack. Previous research [3] is extended by (1) utilizing Geographic Information Systems (GIS) to introduce appropriate spatial data and (2) increasing the sophistication of the spatial analysis by applying the retrospective space-time permutation model available through SaTScanTM. Initial results are promising and lead to a confirmation that Florida wildfires are potentially interesting surrogates for aerosolized biochemical terrorist attacks. Research implications\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["37"]}
{"title": "Flow-Service-Quality (FSQ) Requirements Engineering for High Assurance Systems\n", "abstract": " Modern society could hardly function without the largescale, network-centric information systems that pervade government, defense, and industry. These systems exhibit shifting boundaries, often-unknown component behavior, constantly varying function and usage, and pervasive asynchronous operations. Their complexity challenges intellectual control, and their survivability has become an urgent priority. Requirements engineering methods based on solid theoretical foundations and the realities of network systems are required to manage complexity and ensure survivability. Flow-Service-Quality (FSQ) engineering is an emerging technology for requirements specification, development, and evolution of network-centric systems. FSQ engineering is based on Flow Structures, Computational Quality Attributes, and Flow Management Architectures. These technologies can provide stable engineering foundations for the dynamic and unpredictable world of large-scale network systems. Flow Structure requirements define mission task flows and their refinements into uses of system services in network traversals. Flows are deterministic for human understanding, despite the asynchronism of network operations. They can be refined, abstracted, and verified with precision, and deal explicitly with uncertainty factors, including uncertain COTS functionality and system failures and compromises. Computational Quality Attributes go beyond static, a priori estimates to define requirements for quality attributes such as reliability and survivability as dynamic functions to be computed in system operation.", "num_citations": "1\n", "authors": ["37"]}
{"title": "The COR model for analyzing information systems change\n", "abstract": " Today's world is one of rapid and constant change. Computer-based information systems, as integral components of our businesses and personal lives, must change and adapt to maintain their effectiveness. IS professionals face the challenges of analyzing, enabling and controlling information systems changes. We present a formal model for viewing and analyzing information systems change. Drawing upon the biological model of punctuated equilibrium, we identify equilibriums in the IS life cycle at which system metrics can be collected and compared with previous equilibriums. Based upon one or more system metrics a set of COR measures are developed to study system core, obsolescence, and recency. We demonstrate the application of the COR model to two interesting cases of information systems change. Conclusions and future research directions complete the paper.", "num_citations": "1\n", "authors": ["37"]}
{"title": "An integrated systems development environment with box structures\n", "abstract": " An integrated systems development environment with box structures | Challenges and strategies for research in systems development ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksChallenges and strategies for research in systems developmentAn integrated systems development environment with box structures chapter An integrated systems development environment with box structures Share on Author: Alan R. Hevner View Profile Authors Info & Affiliations Publication: Challenges and strategies for research in systems developmentSeptember 1992 Pages 135\u0393\u00c7\u00f4150 0citation 0 Downloads Metrics Total Citations0 Total 0 Last .\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["37"]}
{"title": "Analysis of three database system architectures using benchmarks.\n", "abstract": " This report tests a newly designed benchmarking methodology, which evaluates the performance of database management systems, by applying it to three different database systems representative of current microcomputer, minicomputer, and database machine architectures. These experiments serve to demonstrate the viability of the methodology, and provide performance measures which characterize today's relational database systems under these environments.", "num_citations": "1\n", "authors": ["37"]}
{"title": "A Performance Benchmark of a Database Machine\n", "abstract": " This paper overviews a benchmark methodology for database systems. A standard benchmark methodology provides a means to make meaningful observations on database system performance and per formance comparisons between database sys-tems. We develop a benchmark for the IDM-500 database machine and present selected performance results. Our goal is to demonstrate the usefulness of benchmarking in making insightful obser-vations on database system per formance.Several recent benchmark studies of database systems have been performed. In (BITT 83] a benchmark was performed comparing the IDM-500 database machine to the INGRES database system both running on a VAX 11/750. The IDM-500 was run with and without the database accelerator. The benchmark testing was performed utilizing a synthetic database made up of randomly generated numbers. In [BORA 84], the authors extended the benchmark from a single user environment to a multiple user environment.", "num_citations": "1\n", "authors": ["37"]}
{"title": "Analysis of three database system architectures using benchmarks[Final Report]\n", "abstract": " This report tests a newly designed benchmaking methodology (see related document), which evaluates the performance of database management systems, by applying it to three different database systems representative of current microcomputer, minicomputer, and database machine architectures. These experiments serve to demonstrate the viability of the methodology, and provide performance measures which characterize today's relational database systems under these environments. Finally, this report reaches conclusions, based upon the results of the benchmark experiments, which span the three architectural classes. Observations are made about the performane of each type of system architecture, rather than comparing three commercial database systems.", "num_citations": "1\n", "authors": ["37"]}
{"title": "Performance evaluation of database systems: A benchmark methodology.\n", "abstract": " This report presents a generalized performance analysis methodology for the benchmarking of database systems. This methodology discusses criteria to be used in the design, execution, and analysis of a database system benchmark. This is a generalized methodology that can apply to any possible database system. By presenting a wide variety of possible consideration in the design and implementation of the benchmark, it is intended to make this methodology applicable to the evaluation, or to the comparison of several systems.", "num_citations": "1\n", "authors": ["37"]}