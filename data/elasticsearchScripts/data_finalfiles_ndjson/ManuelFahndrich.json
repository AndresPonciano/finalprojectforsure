{"title": "Enforcing high-level protocols in low-level software\n", "abstract": " The reliability of infrastructure software, such as operating systems and web servers, is often hampered by the mismanagement of resources, such as memory and network connections. The Vault programming language allows a programmer to describe resource management protocols that the compiler can statically enforce. Such a protocol can specify that operations must be performed in a certain order and that certain operations must be performed before accessing a given data object. Furthermore, Vault enforces statically that resources cannot be leaked. We validate the utility of our approach by enforcing protocols present in the interface between the Windows 2000 kernel and its device drivers.", "num_citations": "558\n", "authors": ["1882"]}
{"title": "Verification of Object-Oriented Programs with Invariants.\n", "abstract": " An object invariant defines what it means for an object\u2019s data to be in a consistent state. Object invariants are central to the design and correctness of objectoriented programs. This paper defines a programming methodology for using object invariants. The methodology, which enriches a program\u2019s state space to express when each object invariant holds, deals with owned object components, ownership transfer, and subclassing, and is expressive enough to allow many interesting object-oriented programs to be specified and verified. Lending itself to sound modular verification, the methodology also provides a solution to the problem of determining what state a method is allowed to modify.", "num_citations": "435\n", "authors": ["1882"]}
{"title": "Language support for fast and reliable message-based communication in Singularity OS\n", "abstract": " Message-based communication offers the potential benefits of providing stronger specification and cleaner separation between components. Compared with shared-memory interactions, message passing has the potential disadvantages of more expensive data exchange (no direct sharing) and more complicated programming. In this paper we report on the language, verification, and run-time system features that make messages practical as the sole means of communication between processes in the Singularity operating system. We show that using advanced programming language and verification techniques, it is possible to provide and enforce strong system-wide invariants that enable efficient communication and low-overhead software-based process isolation. Furthermore, specifications on communication channels help in detecting programmer mistakes early---namely at compile-time---thereby reducing the\u00a0\u2026", "num_citations": "371\n", "authors": ["1882"]}
{"title": "Adoption and focus: Practical linear types for imperative programming\n", "abstract": " A type system with linearity is useful for checking software protocols andresource management at compile time. Linearity provides powerful reasoning about state changes, but at the price of restrictions on aliasing. The hard division between linear and nonlinear types forces the programmer to make a trade-off between checking a protocol on an object and aliasing the object. Most onerous is the restriction that any type with a linear component must itself be linear. Because of this, checking a protocol on an object imposes aliasing restrictions on any data structure that directly or indirectly points to the object. We propose a new type system that reduces these restrictions with the adoption and focus constructs. Adoption safely allows a programmer to alias objects on which she is checking protocols, and focus allows the reverse. A programmer can alias data structures that point to linear objects and use focus for safe\u00a0\u2026", "num_citations": "361\n", "authors": ["1882"]}
{"title": "Typestates for objects\n", "abstract": " Today\u2019s mainstream object-oriented compilers and tools do not support declaring and statically checking simple pre- and postconditions on methods and invariants on object representations. The main technical problem preventing static verification is reasoning about the sharing relationships among objects as well as where object invariants should hold. We have developed a programming model of typestates for objects with a sound modular checking algorithm. The programming model handles typical aspects of object-oriented programs such as down-casting, virtual dispatch, direct calls, and subclassing. The model also permits subclasses to extend the interpretation of typestates and to introduce additional typestates. We handle aliasing by adapting our previous work on practical linear types developed in the context of the Vault system. We have implemented these ideas in a tool called Fugue for\u00a0\u2026", "num_citations": "302\n", "authors": ["1882"]}
{"title": "Declaring and checking non-null types in an object-oriented language\n", "abstract": " Distinguishing non-null references from possibly-null references at the type level can detect null-related errors in object-oriented programs at compile-time. This paper gives a proposal for retrofitting a language such as C# or Java with non-null types. It addresses the central complications that arise in constructors, where declared non-null fields may not yet have been initialized, but the partially constructed object is already accessible. The paper reports experience with an implementation for annotating and checking null-related properties in C# programs.", "num_citations": "251\n", "authors": ["1882"]}
{"title": "Static contract checking with abstract interpretation\n", "abstract": " We present an overview of Clousot, our current tool to statically check CodeContracts. CodeContracts enable a compiler and language-independent specification of Contracts (precondition, postconditions and object invariants).             Clousot checks every method in isolation using an assume/guarantee reasoning: For each method under analysis Clousot assumes its precondition and asserts the postcondition. For each invoked method, Clousot asserts its precondition and assumes the postcondition. Clousot also checks the absence of common runtime errors, such as null-pointer errors, buffer or array overruns, divisions by zero, as well as less common ones such as checked integer overflows or floating point precision mismatches in comparisons. At the core of Clousot there is an abstract interpretation engine which infers program facts. Facts are used to discharge the assertions. The use of abstract\u00a0\u2026", "num_citations": "198\n", "authors": ["1882"]}
{"title": "Scalable context-sensitive flow analysis using instantiation constraints\n", "abstract": " This paper shows that a type graph (obtained via polymorphic type inference) harbors explicit directional flow paths between functions. These flow paths arise from the instantiations of polymorphic types and correspond to call-return sequences in first-order programs. We show that flow information can be computed efficiently while considering only paths with well matched call-return sequences, even in the higher-order case. Furthermore, we present a practical algorithm for inferring type instantiation graphs and provide empirical evidence to the scalability of the presented techniques by applying them in the context of points-to analysis for C programs.", "num_citations": "168\n", "authors": ["1882"]}
{"title": "Type-base flow analysis: from polymorphic subtyping to CFL-reachability\n", "abstract": " We present a novel approach to scalable implementation of type-based flow analysis with polymorphic subtyping. Using a new presentation of polymorphic subytping with instantiation constraints, we are able to apply context-free language (CFL) reachability techniques to type-based flow analysis. We develop a CFL-based algorithm for computing flow-information in time O(n\u00b3), where n is the size of the typed program. The algorithm substantially improves upon the best previously known algorithm for flow analysis based on polymorphic subtyping with complexity O(n8). Our technique also yields the first demand-driven algorithm for polymorphic subtype-based flow-computation. It works directly on higher-order programs with structured data of finite type (unbounded data structures are incorporated via finite approximations), supports context-sensitive, global flow summariztion and includes polymorphic recursion.", "num_citations": "141\n", "authors": ["1882"]}
{"title": "Pentagons: a weakly relational abstract domain for the efficient validation of array accesses\n", "abstract": " Abstract We introduce Pentagons (Pntg), a weakly relational numerical abstract domain useful for the validation of array accesses in byte-code and intermediate languages (IL). This abstract domain captures properties of the form of x\u2208[a, b]\u2227 x< y. It is more precise than the well known Interval domain, but it is less precise than the Octagon domain. The goal of Pntg is to be a lightweight numerical domain useful for adaptive static analysis, where Pntg is used to quickly prove the safety of most array accesses, restricting the use of more precise (but also more expensive) domains to only a small fraction of the code. We implemented the Pntg abstract domain in Clousot, a generic abstract interpreter for. NET assemblies. Using it, we were able to validate 83% of array accesses in the core runtime library mscorlib. dll in a little bit more than 3 minutes.", "num_citations": "130\n", "authors": ["1882"]}
{"title": "Embedded contract languages\n", "abstract": " Specifying application interfaces (APIs) with information that goes beyond method argument and return types is a long-standing quest of programming language researchers and practitioners. The number of type system extensions or specification languages is a testament to that. Unfortunately, the number of such systems is also roughly equal to the number of tools that consume them. In other words, every tool comes with its own specification language.", "num_citations": "118\n", "authors": ["1882"]}
{"title": "Cloud types for eventual consistency\n", "abstract": " Mobile devices commonly access shared data stored on a server. To ensure responsiveness, many applications maintain local replicas of the shared data that remain instantly accessible even if the server is slow or temporarily unavailable. Despite its apparent simplicity and commonality, this scenario can be surprisingly challenging. In particular, a correct and reliable implementation of the communication protocol and the conflict resolution to achieve eventual consistency is daunting even for experts.               To make eventual consistency more programmable, we propose the use of specialized cloud data types. These cloud types provide eventually consistent storage at the programming language level, and thus abstract the numerous implementation details (servers, networks, caches, protocols). We demonstrate (1) how cloud types enable simple programs to use eventually consistent storage without\u00a0\u2026", "num_citations": "113\n", "authors": ["1882"]}
{"title": "Establishing object invariants with delayed types\n", "abstract": " Mainstream object-oriented languages such as C# and Java provide an initialization model for objects that does not guarantee programmer controlled initialization of fields. Instead, all fields are initialized to default values (0 for scalars and null for non-scalars) on allocation. This is in stark contrast to functional languages, where all parts of an allocation are initialized to programmer-provided values. These choices have a direct impact on two main issues: 1) the prevalence of null in object oriented languages (and its general absence in functional languages), and 2) the ability to initialize circular data structures. This paper explores connections between these differing approaches and proposes a fresh look at initialization. Delayed types are introduced to express and formalize prevalent initialization patterns in object-oriented languages.", "num_citations": "104\n", "authors": ["1882"]}
{"title": "Estimating the impact of scalable pointer analysis on optimization\n", "abstract": " This paper addresses the following question: Do scalable control-flow-insensitive pointer analyses provide the level of precision required to make them useful in compiler optimizations?               We first describe alias frequency, a metric that measures the ability of a pointer analysis to determine that pairs of memory accesses in C programs cannot be aliases. We believe that this kind of information is useful for a variety of optimizations, while remaining independent of a particular optimization. We show that control-flow and context insensitive analyses provide the same answer as the best possible pointer analysis on at least 95% of all statically generated alias queries. In order to understand the potential run-time impact of the remaining 5% queries, we weight the alias queries by dynamic execution counts obtained from profile data. Flow-insensitive pointer analyses are accurate on at least 95% of the\u00a0\u2026", "num_citations": "104\n", "authors": ["1882"]}
{"title": "Deconstructing process isolation\n", "abstract": " Most operating systems enforce process isolation through hardware protection mechanisms such as memory segmentation, page mapping, and differentiated user and kernel instructions. Singularity is a new operating system that uses software mechanisms to enforce process isolation. A software isolated process (SIP) is a process whose boundaries are established by language safety rules and enforced by static type checking. SIPs provide a low cost isolation mechanism that provides failure isolation and fast inter-process communication. To compare the performance of Singularity's SIPs against traditional isolation techniques, we implemented an optional hardware isolation mechanism. Protection domains are hardware-enforced address spaces, which can contain one or more SIPs. Domains can either run at the kernel's privilege level or be fully isolated from the kernel and run at the normal application privilege\u00a0\u2026", "num_citations": "99\n", "authors": ["1882"]}
{"title": "Sealing OS processes to improve dependability and safety\n", "abstract": " In most modern operating systems, a process is a hardware-protected abstraction for isolating code and data. This protection, however, is selective. Many common mechanisms---dynamic code loading, run-time code generation, shared memory, and intrusive system APIs---make the barrier between processes very permeable. This paper argues that this traditional open process architecture exacerbates the dependability and security weaknesses of modern systems. As a remedy, this paper proposes a sealed process architecture, which prohibits dynamic code loading, self-modifying code, shared memory, and limits the scope of the process API. This paper describes the implementation of the sealed process architecture in the Singularity operating system, discusses its merits and drawbacks, and evaluates its effectiveness. Some benefits of this sealed process architecture are: improved program analysis by tools\u00a0\u2026", "num_citations": "98\n", "authors": ["1882"]}
{"title": "The Spec# programming system: Challenges and directions\n", "abstract": " The Spec# programming system [4] is a new attempt to increase the quality of general purpose, industrial software. Using old wisdom, we propose the use of specifications to make programmer assumptions explicit. Using modern technology, we propose the use of tools to enforce the specifications. To increase its chances of having impact, we want to design the system so that it can be widely adopted.", "num_citations": "97\n", "authors": ["1882"]}
{"title": "On the relative completeness of bytecode analysis versus source code analysis\n", "abstract": " We discuss the challenges faced by bytecode analyzers designed for code verification compared to similar analyzers for source code. While a bytecode-level analysis brings many simplifications, e.g., fewer cases, independence from source syntax, name resolution, etc., it also introduces precision loss that must be recovered either via preprocessing, more precise abstract domains, more precise transfer functions, or a combination thereof.               The paper studies the relative completeness of a static analysis for bytecode compared to the analysis of the program source. We illustrate it through examples originating from the design and the implementation of Clousot, a generic static analyzer based on Abstract Interpretation for the analysis of MSIL.", "num_citations": "83\n", "authors": ["1882"]}
{"title": "Self-describing artifacts and application abstractions\n", "abstract": " Described herein is at least one implementation employing multiple self-describing software artifacts persisted on one or more computer-storage media of a software-based computer. In this implementation, each artifact is representative of at least part of the software components (eg, load modules, processes, applications, and operating system components) of the computing system and each artifact is described by at least one associated \u201cmanifest,\u201d which include metadata declarative descriptions of the associated artifact.", "num_citations": "67\n", "authors": ["1882"]}
{"title": "The Fugue protocol checker: Is your software baroque\n", "abstract": " Although today\u2019s safe languages, such as C\u266f and Java, automatically catch or prevent many programming errors through compile-time checks and automatic memory management, there remain many programming errors that are not caught until run time: forgetting to release a resource, such as a file or network connection; using a resource after release; calling methods in the wrong order; and making typos in literals that represent dynamic content, like a string that contains a sql query. Such programming errors cause run-time exceptions, which can be experienced by the software\u2019s customers, if the mistakes are not noticed during testing. These kinds of errors involve disobeying the rules for using an interface, called the interface\u2019s protocol. Interface protocols are typically recorded in informal documentation, where they are not useful for systematic checking. Fugue is a software checker that allows interface protocols to be specified as annotations in a library\u2019s source code or in Fugue\u2019s specification repository. Fugue ensures both that client code using an interface obeys the interface\u2019s protocol and that the interface\u2019s implementation is consistent with its protocol. Like a type checker, Fugue performs a static, modular analysis to produce a list of error messages and warnings. The analysis is static because it inspects the program\u2019s code, without any instrumentation to perform checks during execution. The analysis is modular because, at a method call site, the analysis inspects the callee\u2019s declaration and not its body. Fugue analyzes code in any language that compiles to the Common Language Runtime (clr)[12, 10], such as C\u266f, Visual Basic. net\u00a0\u2026", "num_citations": "64\n", "authors": ["1882"]}
{"title": "Global sequence protocol: A robust abstraction for replicated shared state\n", "abstract": " In the age of cloud-connected mobile devices, users want responsive apps that read and write shared data everywhere, at all times, even if network connections are slow or unavailable. The solution is to replicate data and propagate updates asynchronously. Unfortunately, such mechanisms are notoriously difficult to understand, explain, and implement. To address these challenges, we present GSP (global sequence protocol), an operational model for replicated shared data. GSP is simple and abstract enough to serve as a mental reference model, and offers fine control over the asynchronous update propagation (update transactions, strong synchronization). It abstracts the data model and thus applies both to simple key-value stores, and complex structured data. We then show how to implement GSP robustly on a client-server architecture (masking silent client crashes, server crash-recovery failures, and arbitrary network failures) and efficiently (transmitting and storing minimal information by reducing update sequences).", "num_citations": "63\n", "authors": ["1882"]}
{"title": "Verification modulo versions: Towards usable verification\n", "abstract": " We introduce Verification Modulo Versions (VMV), a new static analysis technique for reducing the number of alarms reported by static verifiers while providing sound semantic guarantees. First, VMV extracts semantic environment conditions from a base program P. Environmental conditions can either be sufficient conditions (implying the safety of P) or necessary conditions (implied by the safety of P). Then, VMV instruments a new version of the program, P', with the inferred conditions. We prove that we can use (i) sufficient conditions to identify abstract regressions of P' w.r.t. P; and (ii) necessary conditions to prove the relative correctness of P' w.r.t. P. We show that the extraction of environmental conditions can be performed at a hierarchy of abstraction levels (history, state, or call conditions) with each subsequent level requiring a less sophisticated matching of the syntactic changes between P' and P. Call\u00a0\u2026", "num_citations": "63\n", "authors": ["1882"]}
{"title": "Heap monotonic typestates\n", "abstract": " The paper defines the class of heap monotonic typestates. The monotonicity of such typestates enables sound checking algorithms without the need for nonaliasing regimes of pointers. The basic idea is that data structures evolve over time in a manner that only makes their representation invariants grow stronger, never weaker. This assumption guarantees that existing object references with particular typestates remain valid in all program futures, while still allowing objects to attain new stronger typestates. The system is powerful enough to establish properties of circular data structures.", "num_citations": "58\n", "authors": ["1882"]}
{"title": "Persisted specifications of method pre-and post-conditions for static checking\n", "abstract": " A system and method employing pre-and/or post-condition (s) specified at a source code level and persisted (eg, in associated object code and/or a specification repository) facilitating static checking of the object code is provided. The system and method are based, at least in part, upon a framework that employs rules for using an interface to be recorded as declarative specifications in an existing language. The system can employ a range of annotations that allow a developer to specify interface rule (s) with varying precision. At the simplest end of the range, a specifier can mark those methods that allocate and release resource (s). A specifier can also limit the order in which an object's methods may be called to the transitions of a finite state machine. At the more complex end of the range, a specifier can give a method a plug-in pre-and post condition, which is arbitrary code that examines an object's current state\u00a0\u2026", "num_citations": "51\n", "authors": ["1882"]}
{"title": "Reflective program generation with patterns\n", "abstract": " Runtime reflection facilities, as present in Java and .NET, are powerful mechanisms for inspecting existing code and metadata, as well as generating new code and metadata on the fly. Such power does come at a high price though. The runtime reflection support in Java and .NET imposes a cost on all programs, whether they use reflection or not, simply by the necessity of keeping all metadata around and the inability to optimize code because of future possible code changes. A second---often overlooked---cost is the difficulty of writing correct reflection code to inspect or emit new metadata and code and the risk that the emitted code is not well-formed. In this paper we examine a subclass of problems that can be addressed using a simpler mechanism than runtime reflection, which we call compile-time reflection. We argue for a high-level construct called a transform that allows programmers to write inspection and\u00a0\u2026", "num_citations": "51\n", "authors": ["1882"]}
{"title": "Plug-in pre-and postconditions for static program analysis\n", "abstract": " A system and method employing pre-and/or post-condition (s) specified at a source code level and persisted (eg, in associated object code and/or a specification repository) facilitating static checking of the object code is provided. The system and method are based, at least in part, upon a framework that employs rules for using an interface to be recorded as declarative specifications in an existing language.", "num_citations": "50\n", "authors": ["1882"]}
{"title": "Static verification for code contracts\n", "abstract": " The Code Contracts project\u00a0[3] at Microsoft Research enables programmers on the .NET platform to author specifications in existing languages such as C# and VisualBasic. To take advantage of these specifications, we provide tools for documentation generation, runtime contract checking, and static contract verification.             This talk details the overall approach of the static contract checker and examines where and how we trade-off soundness in order to obtain a practical tool that works on a full-fledged object-oriented intermediate language such as the .NET Common Intermediate Language.", "num_citations": "42\n", "authors": ["1882"]}
{"title": "Methods for enhancing flow analysis\n", "abstract": " Methods and structures are described that enhance flow analysis for programs. Whereas previous methods are complicated by the presence of function pointers, the present methods present a framework that abstracts function pointers as if they were any other program expressions so as to allow a desired level of analytical decision within a desired duration of analysis. One aspect of the present invention includes inferring types from a program, forming a type graph from the types, and forming a flow graph from the type graph to inhibit imprecise paths so as to enhance context-sensitivity of flow analysis. The methods may be used in any analysis tools such as code browsers and slicing tools.", "num_citations": "39\n", "authors": ["1882"]}
{"title": "Detecting races in relay ladder logic programs\n", "abstract": " Relay Ladder Logic (RLL) [4] is a programming language widely used for complex embedded control applications such as manufacturing and amusement park rides. The cost of bugs in RLL programs is extremely high, often measured in millions of dollars (for shutting down a factory) or human safety (for rides). In this paper, we describe our experience in applying constraint-based program analysis techniques to analyze production RLL programs. Our approach is an interesting combination of probabilistic testing and program analysis, and we show that our system is able to detect bugs with high probability, up to the approximations made by the conservative program analysis. We demonstrate that our analysis is useful in detecting some flaws in production RLL programs that are difficult to find by other techniques.", "num_citations": "39\n", "authors": ["1882"]}
{"title": "Flow-insensitive points-to analysis with term and set constraints\n", "abstract": " We describe new type systems for two kinds of flow-insensitive points-to analyses, one based on Andersen's algorithm and one based on Steensgaard's. The type systems are formulated using a mixed constraint framework. These systems can be seen as a straightforward axiomatization of an informal description of the algorithms, and we show this formally by proving soundness with respect to an operational semantics. Further, we show that these two systems are nearly identical, except that one uses subset constraints and one uses unification. We discuss an implementation of these systems and describe experiments that demonstrate that our general framework achieves running times within a small constant factor of a hand-coded solution. We conclude that a mixed constraint system provides a useful, practical framework for static semantic analyses.", "num_citations": "39\n", "authors": ["1882"]}
{"title": "Source code annotation language\n", "abstract": " Techniques and tools for implementing a source code annotation language are described. In one aspect, keywords are added to a function interface to define a contract for the function independent of function call context. In another aspect, annotations are inserted at global variables, formal parameters, return values, or user-defined types. The annotations include, for example, properties and qualifiers. A property can indicate, for example, a characteristic of a buffer. In another aspect, an annotation indicates that a value has usability properties sufficient to allow a function to rely on the value, where the usability properties depend on value type.", "num_citations": "36\n", "authors": ["1882"]}
{"title": "Safer unsafe code for. net\n", "abstract": " The .NET intermediate language (MSIL) allows expressing both statically verifiable memory and type safe code (typically called managed), as well as unsafe code using direct pointer manipulations. Unsafe code can be expressed in C# by marking regions of code as unsafe. Writing unsafe code can be useful where the rules of managed code are too strict. The obvious drawback of unsafe code is that it opens the door to programming errors typical of C and C++, namely memory access errors such as buffer overruns. Worse, a single piece of unsafe code may corrupt memory and destabilize the entire runtime or allow attackers to compromise the security of the platform. We present a new static analysis based on abstract interpretation to check memory safety for unsafe code in the .NET framework. The core of the analysis is a new numerical abstract domain, Strp, which is used to efficiently compute memory invariants\u00a0\u2026", "num_citations": "35\n", "authors": ["1882"]}
{"title": "Making set-constraint program analyses scale\n", "abstract": " this paper, it is sufficient to consider the core type language defined below, which avoids these irregularities. We still use the full type language in examples, however. The core type language distinguishes between\" left\" types \u00f8", "num_citations": "34\n", "authors": ["1882"]}
{"title": "Tracking down exceptions in standard ML programs\n", "abstract": " We describe our experiences with an exception analysis tool for Standard ML. Information about exceptions gathered by the analysis is visualized using pam, a program visualization tool for emacs. We study the results of the analysis of three well-known programs, classifying exceptions as assertion failures, error exceptions, control-flow exceptions, and pervasive exceptions. Even though the analysis is often conservative and reports many spurious exceptions, we have found it useful for checking the consistency of error and control-flow exceptions. Furthermore, using our tools, we have uncovered two minor exception-related bugs in the three programs we scrutinized.", "num_citations": "27\n", "authors": ["1882"]}
{"title": "Statically checkable pattern abstractions\n", "abstract": " Pattern abstractions increase the expressiveness of pattern matthing, enabling the programmer to describe a broader class of regular forests with patterns. Furthermore, pattern abstractions support code reuse and code factoring, features that facilitate maintenance and evolution of code. Past research on pattern abstractions has generally ignored the aspect of compile-time checks for exhaustiveness and redundancy. In this paper we propose a class of expressive patterns that admits these compile-time checks.", "num_citations": "26\n", "authors": ["1882"]}
{"title": "Eventually consistent storage and transactions in cloud based environment\n", "abstract": " An \u201cEventually Consistent Sharing Model\u201d provides various techniques for using \u201crevision diagrams\u201d to determine both arbitration and visibility of changes or updates to shared data (eg, data, databases, lists, etc.) without requiring a causally consistent partial order for visibility, and without requiring change or update timestamps for arbitration. In particular, the Eventually Consistent Sharing Model provides fork-join automata based on revision diagrams to track the forking and joining of data versions, thereby tracking updates made to replicas of that data by one or more sources.\u201cCloud types\u201d are used to define a structure of the shared data that enables fully automatic conflict resolution when updating the shared data. These concepts enable mobile devices (or other computing devices that may periodically go \u201coffline\u201d) to share structured data in cloud-based environments in a manner that provides local data replicas\u00a0\u2026", "num_citations": "24\n", "authors": ["1882"]}
{"title": "Bane: A library for scalable constraint-based program analysis\n", "abstract": " Program analysis is an important aspect of modern program development. Compilers use program analysis to prove the correctness of optimizing program transformations. Static error detection tools use program analysis to alert the programmer to the presence of potential errors. This dissertation focuses on the expressiveness and implementation of constraint-based program analyses, ie, analyses that are expressed as solutions to a system of constraints. We show that structuring the implementation of program analyses around a library of generic constraint solvers promotes reuse, gives control over precision-efficiency tradeoffs, and enables optimizations that yield orders of magnitude speedups over standard implementations.", "num_citations": "24\n", "authors": ["1882"]}
{"title": "Embedded contract languages\n", "abstract": " Specifying application interfaces (APIs) with information that goes beyond method argument and return types is a long-standing quest of programming language researchers and practitioners. The number of type system extensions or specification languages is roughly equal to the number of tools that consume them. In other words, every tool comes with its own specification language. In this paper we argue that for modern object-oriented languages, using an embedding of contracts as code is a better approach. We exemplify our embedding of Code Contracts on the Microsoft managed execution platform (.NET) using the C# programming language. The embedding works as well in Visual Basic. We discuss the numerous advantages of our approach and the technical challenges, as well as the status of tools that consume the embedded contracts", "num_citations": "23\n", "authors": ["1882"]}
{"title": "Annotations for (more) precise points-to analysis\n", "abstract": " We extend an existing points-to analysis for Java in two ways. First, we fully support .NET which has structs and parameter passing by reference. Second, we increase the precision for calls to nonanalyzable methods. A method is non-analyzable when its code is not available either because it is abstract (an interface method or an abstract class method), it is virtual and the callee cannot be statically resolved, or because it is implemented in native code (as opposed to managed bytecode). For such methods, we introduce extensions that model potentially affected heap locations. We also propose an annotation language that permits a modular analysis without losing too much precision. Our annotation language allows concise specification of points-to and read/write effects. Our analysis infers points-to and read/effect information from available code and also checks code against its annotation, when the latter is provided.", "num_citations": "22\n", "authors": ["1882"]}
{"title": "Contract programming for code error reduction\n", "abstract": " In one embodiment, a computer system provides an application programming interface (API) for augmenting an application API. A computer system receives software code written in a second programming language indicating a user's intention to augment an application API with contracts from a contract API written in a first programming language. The software code includes a reference to the contract API. The contracts include assertions indicating appropriate use of the application API. The computer system accesses portions of the contract API according to the reference in the software code and compiles the received software code and the referenced portions of the contract API into an intermediate language (IL) version of the software code. The IL version is in an intermediate language common to both the first programming language and the second programming language. The IL version includes the assertions\u00a0\u2026", "num_citations": "20\n", "authors": ["1882"]}
{"title": "State-based source code annotation\n", "abstract": " Techniques and tools relating to state-based source code annotation are described. For example, described techniques include flexible techniques for describing object states with annotations. In one aspect, properties of data structures in source code are described using state-defining code annotations. For example, specification structs can be used to describe an arbitrary set of states of objects, thereby improving the capabilities of the annotation language in terms of richness of program description. Specification structs also help to avoid annotating large numbers of individual fields in data structures by allowing several individual fields to be described by a single specification struct. Other aspects of a source code annotation language also are described.", "num_citations": "20\n", "authors": ["1882"]}
{"title": "Access-control permissions with inter-process message-based communications\n", "abstract": " Described herein are one or more implementations that facilitate message-passing over a communication conduit between software processes in a computing environment. More particularly, the implementations described restrict access of one process to another via messages passed over a particular conduit connecting the processes and the access-control restrictions are defined by a contract associated with that particular conduit.", "num_citations": "19\n", "authors": ["1882"]}
{"title": "Integrating a set of contract checking tools into visual studio\n", "abstract": " Integrating tools and extensions into existing languages, compilers, debuggers, and IDEs can be difficult, work-intensive, and often results in a one-off integration. In this paper, we report on our experience of building and integrating the CodeContract tool set into an existing programming environment. The CodeContract tools enable 1) authoring of contracts (preconditions, postconditions, and object invariants), 2) instrumenting contract checks into code, 3) statically checking code against contracts, and 4) visualizing contracts and results. We identify three characteristics of our integration that allowed us to reuse existing compilers and IDEs, increase the reach of our tools to multiple languages and target platforms, and maintain the tools over three consecutive versions of C# and Visual Studio with little effort. These principles are 1) use source embedding for new language features, 2) use target analysis and rewriting\u00a0\u2026", "num_citations": "17\n", "authors": ["1882"]}
{"title": "Prettier concurrency: purely functional concurrent revisions\n", "abstract": " This article presents an extension to the work of Launchbury and Peyton-Jones on the ST monad. Using a novel model for concurrency, called concurrent revisions [3,5], we show how we can use concurrency together with imperative mutable variables, while still being able to safely convert such computations (in the Rev monad) into pure values again. In contrast to many other transaction models, like software transactional memory (STM), concurrent revisions never use rollback and always deterministically resolve conflicts. As a consequence, concurrent revisions integrate well with side-effecting I/O operations. Using deterministic conflict resolution, concurrent revisions can deal well with situations where there are many conflicts between different threads that modify a shared data structure. We demonstrate this by describing a concurrent game with conflicting concurrent tasks.", "num_citations": "17\n", "authors": ["1882"]}
{"title": "Contract failure behavior with escalation policy\n", "abstract": " An error handling system is described herein that provides a facility for controlling the behavior of software when the software violates a contract condition. The system provides configurable runtime behavior that takes place when a contract fails. The error handling system provides an event that a hosting application or other software code can register to handle and that the system invokes upon detecting a contract failure. The application's response to the event determines how the system handles the failure. If the event is unhandled, the system triggers an escalation policy that allows an administrator or application to specify how the system handles contract failures. Thus, the error handling system provides increased control over the handling of contract failures within software code.", "num_citations": "14\n", "authors": ["1882"]}
{"title": "From polymorphic subtyping to CFL reachability: Context-sensitive flow analysis using instantiation constraints\n", "abstract": " We present a novel approach to computing context-sensitive flow of values through procedures and data structures. Our approach combines and extends techniques from two seemingly disparate areas: polymorphic subtyping and interprocedural dataflow analysis based on context-free language reachability. The resulting technique offers several advantages over previous approaches: it works directly on higher-order programs, provides demand-driven interprocedural queries, and improves the asymptotic complexity of a known algorithm based on polymorphic subtyping from O (n 8) to O (n 3) for computing all queries.", "num_citations": "13\n", "authors": ["1882"]}
{"title": "Kernel interface with categorized kernel objects\n", "abstract": " Described herein are one or more implementations that separate kernel interfaces functions into those that act on kernel objects owned by a process and accessed exclusively by that process\u2014described herein as local kernel objects\u2014from access to kernel objects owned by a process and accessible by other active processes.", "num_citations": "12\n", "authors": ["1882"]}
{"title": "Dynamic typing and subtype inference\n", "abstract": " Dynamic typing is aprogram analysis targeted at removing runtime tagging and untagging operations from programs written in dynamically typed languages. This paper compares dynamic typing with asubtyping system based onset constraints. Thepurposeis both to make precise the relationship between two superficially unrelated type systems andtoilhrstrate how the advantages ofdynamictyping and subtype inference can recombined. The central result is a theorem showing that a typing discipline at least as powerful as dynamic typing can be expressed using set constraints.", "num_citations": "12\n", "authors": ["1882"]}
{"title": "Cloud types: Robust abstractions for replicated shared state\n", "abstract": " Note: this publication is superseded by a newer report MSR-2015-11 (http://research. microsoft. com/apps/pubs/? id= 240462)", "num_citations": "8\n", "authors": ["1882"]}
{"title": "Inference of necessary field conditions with abstract interpretation\n", "abstract": " We present a new static analysis to infer necessary field conditions for object-oriented programs. A necessary field condition is a property that should hold on the fields of a given object, for otherwise there exists a calling context leading to a failure due to bad object state. Our analysis also infers the provenance of the necessary condition, so that if a necessary field condition is violated then an explanation containing the sequence of method calls leading to a failing assertion can be produced.               When the analysis is restricted to readonly fields, i.e., fields that can only be set in the initialization phase of an object, it infers object invariants. We provide empirical evidence on the usefulness of necessary field conditions by integrating the analysis into cccheck, our static analyzer for .NET. Robust inference of readonly object field invariants was the #1 request from cccheck users.", "num_citations": "8\n", "authors": ["1882"]}
{"title": "Inferring dataflow properties of user defined table processors\n", "abstract": " In SCOPE, a SQL style cloud-level data-mining scripting language, table processing capabilities are often provided by user defined .NET methods. The SCOPE compiler can optimize a query plan if it knows certain dataflow relations between the input and output tables, such as column independence, column equality, or that a column\u2019s values are non-null. This paper presents an automated analysis for inferring such relations from implementations of SCOPE table processing methods. Since most table processing methods are written as .NET iterators, our analysis must accurately deal with the resulting state-machine implementing such iterators. Other complications addressed are naming and estimating column numbers, aliasing and escaping, and the inference of universally quantified loop invariants.               We prototyped the analysis as Scooby, a static analyzer for .NET iterators. Scooby is able to\u00a0\u2026", "num_citations": "8\n", "authors": ["1882"]}
{"title": "Refined type inference for ML\n", "abstract": " Inclusion constraints over set-expressions [1, 4] provide a general formalism to express a large class of program analyses. Over the past two years, we have experimented with inclusion constraints to model dataflow in type-based analyses. One of our research goals is to determine how to structure and implement precise constraint-based analyses such that they scale to large programs. Program analyses with O (n3) complexity bounds often exhibit their worstcase complexity in practice and consequently do not scale beyond programs of a few thousand lines. As a result, coarser but faster analyses are usually used [2, 10]. Scaling behavior and precision are intimately connected and in an ideal formalism, one can be traded for the other. Unfortunately, inclusion constraints over set-expressions do not provide enough control over this precision-efficiency tradeoff. As an example, consider Hindley-Milner type inference. The equality constraints arising in the formulation of algorithm W [8] can be solved as symmetric inclusion constraints using a standard inclusion constraint solver. This approach results however in an algorithm with cubic time complexity, instead of the nearly linear time algorithm based on unification [7]. A key to the efficiency of Hindley-Milner type inference is that types are terms. Terms have unique head constructors, whereas set expressions generally do not. This property\u2014whether a quantity has a unique head constructor\u2014is a prime determinant of the cost of solving type constraints. We have designed an extended inclusion constraint formalism in which values with unique head constructors can be mixed with more general sets\u00a0\u2026", "num_citations": "8\n", "authors": ["1882"]}
{"title": "Non-null types in an object-oriented language\n", "abstract": " Non-null types can detect certain null-related errors in object-oriented programs earlier and avoid other such errors altogether. This paper gives a proposal for retrofitting a language like C# or Java with non-null types. It addresses the complications that arise in constructors, where non-null fields may not yet have been initialized.", "num_citations": "7\n", "authors": ["1882"]}
{"title": "Type-based flow analysis and context-free language reachability\n", "abstract": " We present a novel approach to computing the context-sensitive flow of values through procedures and data structures. Our approach combines and extends techniques from two seemingly disparate areas: polymorphic subtyping and interprocedural dataflow analysis based on context-free language reachability. The resulting technique offers several advantages over previous approaches: it works directly on higher-order programs; provides demand-driven interprocedural queries; and improves the asymptotic complexity of a known algorithm based on polymorphic subtyping from O(n8) to O(n3) for computing all queries. For intra-procedural flow restricted to equivalence classes, our algorithm yields linear inter-procedural flow queries.", "num_citations": "6\n", "authors": ["1882"]}
{"title": "Detecting races in relay ladder logic programs\n", "abstract": " Relay Ladder Logic (RLL) [5] is a programming language widely used for complex embedded control applications such as manufacturing and amusement park rides. The cost of bugs in RLL programs is extremely high, often measured in millions of dollars (for shutting down a factory) or human safety (for rides). In this paper, we describe our experience in applying constraint-based program analysis techniques to analyze production RLL programs. Our approach is an interesting combination of probabilistic testing and program analysis, and we show that our system is able to detect bugs with high probability, up to the approximations made by the conservative program analysis. We demonstrate that our analysis is useful in detecting some flaws in production RLL programs that are difficult to find by other techniques.", "num_citations": "6\n", "authors": ["1882"]}
{"title": "Lessons from a Web-based IDE and Runtime\n", "abstract": " At Microsoft Research, we have built a purely web-based IDE called TouchDevelop that enables anyone to pick up a device and start programming. The IDE is geared towards touch based devices without keyboards, ranging from phones, over tablets, to large display screens. Programs can be edited and run on the device without an auxiliary PC. Transitioning between programming on one device, and continuing on another device is seamless. The web application also works offline.", "num_citations": "4\n", "authors": ["1882"]}
{"title": "A Static Analysis to Detect Re-Entrancy in Object Oriented Programs.\n", "abstract": " This reasoning is only correct under certain conditions. In this paper we present sufficient conditions that make reasoning as above sound and show how these conditions can be checked separately, allowing us to divide the verification problem into two well-defined parts: 1) reasoning about object consistency of the receiver within a single method, and 2) reasoning about the absence of inconsistent re-entrant calls. In particular, when reasoning about the object consistency of the receiver within a method, our approach does not require proving invariants on other objects whose methods are called. We present a novel whole program analysis to determine the absence of inconsistent re-entrant calls. It warns developers when re-entrant calls are made on objects whose invariants may not hold. The analysis augments a points-to analysis to compute potential call chains in order to detect re-entrant calls.", "num_citations": "4\n", "authors": ["1882"]}
{"title": "A reentrancy analysis for object oriented programs\n", "abstract": " We are interested in object-oriented programming methodologies that enable static verification of object-invariants. Reasoning soundly and effectively about the consistency of objects is still one of the main stumbling blocks to pushing object-oriented program verification into the mainstream. In this paper we explore a simple model of invariants that is intuitive and allows us to divide the verification problem into two well-defined parts: 1) reasoning about object consistency within a single method, and 2) reasoning about the absence of inconsistent re-entrant calls. We delineate this division by specifying the assumptions and proof obligations of each part. Part one can be handled using well-established techniques in modular verification. This paper presents a novel program analysis to handle the second part. It warns developers when re-entrant calls are made on objects whose invariants may not hold. The analysis uses a points-to analysis to detect re-entrant calls and a simple dataflow analysis to decide whether the invariant of the receiver of a re-entrant call holds. Initial experimentation shows the analysis is able to recognize that many re-entrant calls can be safely performed as their receivers are in a consistent state.", "num_citations": "4\n", "authors": ["1882"]}
{"title": "Semantic baselining\n", "abstract": " Described herein are technologies pertaining to semantic baselining. Correctness conditions of a baseline program are inferred based upon a first static analysis undertaken over the baseline program. The correctness conditions are subsequently inserted into a revision to the baseline program. When a second static analysis is undertaken over the revised program with the correctness conditions inserted therein, warnings inherited from the baseline program are suppressed, while warnings caused by revisions are surfaced to a developer.", "num_citations": "3\n", "authors": ["1882"]}
{"title": "Cloud types for eventual consistency\n", "abstract": " Mobile devices commonly access shared data stored on a server. To ensure responsiveness, many applications maintain local replicas of the shared data that remain instantly accessible even if the server is slow or temporarily unavailable. Despite its apparent simplicity and commonality, this scenario can be surprisingly challenging. In particular, a correct and reliable implementation of the communication protocol and the conflict resolution to achieve eventual consistency is daunting even for experts.To make eventual consistency more programmable, we propose the use of specialized cloud data types. These cloud types provide eventually consistent storage at the programming language level, and thus abstract the numerous implementation details (servers, networks, caches, protocols). We demonstrate (1) how cloud types enable simple programs to use eventually consistent storage without introducing undue complexity, and (2) how to provide cloud types using a system and protocol comprised of multiple servers and clients.", "num_citations": "3\n", "authors": ["1882"]}
{"title": "It\u2019s Alive\n", "abstract": " It\u2019s Alive! Page 1 It\u2019s Alive! Continuous Feedback in UI Programming Sebastian Burckhardt Manuel Fahndrich Peli de Halleux Sean McDirmid Michal Moskal Nikolai Tillmann Microsoft Research Jun Kato The University of Tokyo Page 2 Live Programming : Archer Analogy [Hancock, 2003] \u2022 Archer: aim, shoot, inspect, repeat \u2022 Hose: aim & watch Page 3 \u2022 Archer: aim, shoot, inspect, repeat \u2022 edit, compile, test, repeat \u2022 Hose: aim & watch \u2022 edit & watch Live Programming : Archer Analogy [Hancock, 2003] Page 4 Quick Demo: What is Live Programming? What is TouchDevelop? Page 5 Question: How to do live programming? \u2022 Target: Event-driven apps with graphical user interfaces (GUI\u2019s) \u2022 User input events (tap button, edit text, ..) \u2022 I/O events (eg asynchronous web requests) \u2022 We can think of code editing as an event (replace old program with a new one) \u2022 What should we do in this situation? Page 6 on code changes, \u2026", "num_citations": "3\n", "authors": ["1882"]}
{"title": "Global sequence protocol\n", "abstract": " In the age of cloud-connected mobile devices, users want responsive apps that read and write shared data everywhere, at all times, even if network connections are slow or unavailable. The solution is to replicate data and propagate updates asynchronously. Unfortunately, such mechanisms are notoriously difficult to understand, explain, and implement. To address these challenges, we present GSP (global sequence protocol), an operational model for replicated shared data. GSP is simple and abstract enough to serve as a mental reference model, and offers fine control over the asynchronous update propagation (update transactions, strong synchronization). It abstracts the data model and thus applies both to simple key-value stores, and complex structured data. We then show how to implement GSP robustly on a clientserver architecture (masking silent client crashes, server crash-recovery failures, and arbitrary network failures) and efficiently (transmitting and storing minimal information by reducing update sequences).", "num_citations": "2\n", "authors": ["1882"]}
{"title": "A case for static analyzers in the cloud\n", "abstract": " A cloud-based static analyzer runs as service. Clients issue analysis requests through the local network or over the internet. The analysis takes advantage of the large computation resources offered by the cloud: the underlying infrastructure ensures scaling and unlimited storage. Cloud-based analyzers may relax performance-precision trade-offs usually associated with desktop-based analyzers. More cores enable more precise and responsive analyses. More storage enables perfect caching of the analysis results, shareable among different clients, and queryable off-line. To realize these advantages, cloud-based analyzers need to be architected differently than desktop ones. We describe our ongoing effort of moving a desktop analyzer, Clousot, into a cloud-based one, Cloudot.", "num_citations": "2\n", "authors": ["1882"]}
{"title": "Roll forward, not back: A case for deterministic conflict resolution\n", "abstract": " Enabling applications to execute various tasks in parallel is difficult if those tasks exhibit read and write conflicts. In recent work, we developed a programming model based on concurrent revisions that addresses this challenge: each forked task gets a conceptual copy of all locations that are declared to be shared. Each such location has a specific isolation type; on joins, state changes to each location are merged deterministically based on its isolation type. In this paper, we study how to specify isolation types abstractly using operation-based compensation functions rather than statebased merge functions. Using several examples including a list with insert, delete and modify operations, we propose compensation tables as a concise, general and intuitively accessible mechanism for determining how to merge arbitrary operation sequences. Finally, we provide sufficient conditions to verify that a state-based merge function correctly implements a compensation table.", "num_citations": "2\n", "authors": ["1882"]}
{"title": "Abstracting runtime heaps for program understanding\n", "abstract": " Modern programming environments provide extensive support for inspecting, analyzing, and testing programs based on the algorithmic structure of a program. Unfortunately, support for inspecting and understanding runtime data structures during execution is typically much more limited. This paper provides a general purpose technique for abstracting and summarizing entire runtime heaps. It shows how to take advantage of such heap abstractions for interactive debugging, visualization, and memory profiling. We describe the abstract heap model and the associated algorithms for transforming a concrete heap dump into the corresponding abstract model as well as algorithms for merging, comparing, model is designed to emphasize high-level concepts about heapbased data structures, such as shape and size, as well as relationships between heap structures, such as sharing and connectivity. The focus on high-level heap properties produces an abstraction that is useful for a range of applications. We demonstrate the utility and computational tractability of the abstract heap model by building a memory profiler. We then use this tool to check for, pinpoint, and correct sources of memory bloat from a test suite including programs from SPEC JVM98 and DaCapo. This evaluation shows that the tool is useful for both finding previously unknown memory problems and in providing additional context for understanding previously reported issues.", "num_citations": "2\n", "authors": ["1882"]}
{"title": "Clousot: Static Contract Checking with Abstract Interpretation\n", "abstract": " We present an overview of Clousot, our current tool to statically check CodeContracts. CodeContracts enable a compiler and languageindependent specification of Contracts (precondition, postconditions and object invariants).Clousot checks every method in isolation using an assume/guarantee reasoning: For each method under analysis Clousot assumes its precondition and asserts the postcondition. For each invoked method, Clousot asserts its precondition and assumes the postcondition. Clousot also checks the absence of common runtime errors, such as null-pointer errors, buffer or array overruns, divisions by zero, as well as less common ones such as checked integer overflows or floating point precision mismatches in comparisons. At the core of Clousot there is an abstract interpretation engine which infers program facts. Facts are used to discharge the assertions. The use of abstract interpretation (vs usual weakest precondition-based checkers) has two main advantages:(i) the checker automatically infers loop invariants letting the user focus only on boundary specifications;(ii) the checker is deterministic in its behavior (which abstractly mimics the flow of the program) and it can be tuned for precision and cost. Clousot embodies other techniques, such as iterative domain refinement, goal-directed backward propagation, precondition and postcondition inference, and message prioritization.", "num_citations": "2\n", "authors": ["1882"]}
{"title": "Checking compatibility of bit sizes in floating point comparison operations\n", "abstract": " We motivate, define and design a simple static analysis to check that comparisons of floating point values use compatible bit widths and thus compatible precision ranges. Precision mismatches arise due to the difference in bit widths of processor internal floating point registers (typically 80 or 64 bits) and their corresponding widths when stored in memory (64 or 32 bits). The analysis guarantees that floating point values from memory (i.e. array elements, instance and static fields) are not compared against floating point numbers in registers (i.e. arguments or locals).Without such an analysis, static symbolic verification is unsound and hence may report false negatives.The static analysis is fully implemented in Clousot, our static contract checker based on abstract interpretation.", "num_citations": "1\n", "authors": ["1882"]}
{"title": "Simple Contracts for C++(R1)\n", "abstract": " We present a minimal system for expressing interface requirements as contracts. They provide basic mitigation measures for early containment of undesired program behavior. The set of facilities suggested in this proposal is deliberately kept to the minimum of preconditions and post-conditions. Contracts are part of an operation\u2019s interface, but not part of its type. That is, while the expression of contracts is logically part of the operation\u2019s interface, the actual code verifying the conditions are part of the operation\u2019s implementation.", "num_citations": "1\n", "authors": ["1882"]}
{"title": "Effect Systems\n", "abstract": " This talk is an introduction to effect systems in general. I give examples to motivate the extension of types with effect information. I also present generic effect system rules, and domain specific ones for store effects and exceptions. Effect inference and soundness are only briefly mentioned. The talk concludes with a short history on research in effect systems.", "num_citations": "1\n", "authors": ["1882"]}