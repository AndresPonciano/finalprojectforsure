{"title": "Moa: Massive online analysis, a framework for stream classification and clustering\n", "abstract": " Massive Online Analysis (MOA) is a software environment for implementing algorithms and running experiments for online learning from evolving data streams. MOA is designed to deal with the challenging problem of scaling up the implementation of state of the art algorithms to real world dataset sizes. It contains collection of offline and online for both classification and clustering as well as tools for evaluation. In particular, for classification it implements boosting, bagging, and Hoeffding Trees, all with and without Naive Bayes classifiers at the leaves. For clustering, it implements StreamKM++, CluStream, ClusTree, Den-Stream, D-Stream and CobWeb. Researchers benefit from MOA by getting insights into workings and problems of different approaches, practitioners can easily apply and compare several algorithms to real world data set and settings. MOA supports bi-directional interaction with WEKA, the Waikato Environment for Knowledge Analysis, and is released under the GNU GPL license.", "num_citations": "1783\n", "authors": ["1069"]}
{"title": "Learning from time-changing data with adaptive windowing\n", "abstract": " We present a new approach for dealing with distribution change and concept drift when learning from data sequences that may vary with time. We use sliding windows whose size, instead of being fixed a priori, is recomputed online according to the rate of change observed from the data in the window itself. This delivers the user or programmer from having to guess a time-scale for change. Contrary to many related works, we provide rigorous guarantees of performance, as bounds on the rates of false positives and false negatives. Using ideas from data stream algorithmics, we develop a time- and memory-efficient version of this algorithm, called ADWIN2. We show how to combine ADWIN2 with the Na\u00efve Bayes (NB) predictor, in two ways: one, using it to monitor the error rate of the current model and declare when revision is necessary and, two, putting it inside the NB predictor to maintain up-to-date estimations of\u00a0\u2026", "num_citations": "1253\n", "authors": ["1069"]}
{"title": "Mining big data: current status, and forecast to the future\n", "abstract": " Big Data is a new term used to identify datasets that we can not manage with current methodologies or data mining software tools due to their large size and complexity. Big Data mining is the capability of extracting useful information from these large datasets or streams of data. New mining techniques are necessary due to the volume, variability, and velocity, of such data. The Big Data challenge is becoming one of the most exciting opportunities for the years to come. We present in this issue, a broad overview of the topic, its current status, controversy, and a forecast to the future. We introduce four articles, written by influential scientists in the field, covering the most interesting and state-of-the-art topics on Big Data mining.", "num_citations": "980\n", "authors": ["1069"]}
{"title": "Sentiment knowledge discovery in twitter streaming data\n", "abstract": " Micro-blogs are a challenging new source of information for data mining techniques. Twitter is a micro-blogging service built to discover what is happening at any moment in time, anywhere in the world. Twitter messages are short, and generated constantly, and well suited for knowledge discovery using data stream mining. We briefly discuss the challenges that Twitter data streams pose, focusing on classification problems, and then consider these streams for opinion mining and sentiment analysis. To deal with streaming unbalanced classes, we propose a sliding window Kappa statistic for evaluation in time-changing data streams. Using this statistic we perform a study on Twitter data using learning algorithms for data streams.", "num_citations": "735\n", "authors": ["1069"]}
{"title": "New ensemble methods for evolving data streams\n", "abstract": " Advanced analysis of data streams is quickly becoming a key area of data mining research as the number of applications demanding such processing increases. Online mining when such data streams evolve over time, that is when concepts drift or change completely, is becoming one of the core issues. When tackling non-stationary concepts, ensembles of classifiers have several advantages over single classifier methods: they are easy to scale and parallelize, they can adapt to change quickly by pruning under-performing parts of the ensemble, and they therefore usually also generate more accurate concept descriptions. This paper proposes a new experimental data stream framework for studying concept drift, and two new variants of Bagging: ADWIN Bagging and Adaptive-Size Hoeffding Tree (ASHT) Bagging. Using the new experimental framework, an evaluation study on synthetic and real-world datasets\u00a0\u2026", "num_citations": "689\n", "authors": ["1069"]}
{"title": "Adaptive learning from evolving data streams\n", "abstract": " We propose and illustrate a method for developing algorithms that can adaptively learn from data streams that drift over time. As an example, we take Hoeffding Tree, an incremental decision tree inducer for data streams, and use as a basis it to build two new methods that can deal with distribution and concept drift: a sliding window-based algorithm, Hoeffding Window Tree, and an adaptive method, Hoeffding Adaptive Tree. Our methods are based on using change detectors and estimator modules at the right places; we choose implementations with theoretical guarantees in order to extend such guarantees to the resulting adaptive learning algorithm. A main advantage of our methods is that they require no guess about how fast or how often the stream will drift; other methods typically have several user-defined parameters to this effect.               In our experiments, the new methods never do worse, and in\u00a0\u2026", "num_citations": "415\n", "authors": ["1069"]}
{"title": "Adaptive random forests for evolving data stream classification\n", "abstract": " Random forests is currently one of the most used machine learning algorithms in the non-streaming (batch) setting. This preference is attributable to its high learning performance and low demands with respect to input preparation and hyper-parameter tuning. However, in the challenging context of evolving data streams, there is no random forests algorithm that can be considered state-of-the-art in comparison to bagging and boosting based algorithms. In this work, we present the adaptive random forest (ARF) algorithm for classification of evolving data streams. In contrast to previous attempts of replicating random forests for data stream learning, ARF includes an effective resampling method and adaptive operators that can cope with different types of concept drifts without complex optimizations for different data sets. We present experiments with a parallel implementation of ARF which has no degradation in\u00a0\u2026", "num_citations": "332\n", "authors": ["1069"]}
{"title": "A survey on ensemble learning for data stream classification\n", "abstract": " Ensemble-based methods are among the most widely used techniques for data stream classification. Their popularity is attributable to their good performance in comparison to strong single learners while being relatively easy to deploy in real-world applications. Ensemble algorithms are especially useful for data stream learning as they can be integrated with drift detection algorithms and incorporate dynamic updates, such as selective removal or addition of classifiers. This work proposes a taxonomy for data stream ensemble learning as derived from reviewing over 60 algorithms. Important aspects such as combination, diversity, and dynamic updates, are thoroughly discussed. Additional contributions include a listing of popular open-source tools and a discussion about current data stream research challenges and how they relate to ensemble learning (big data streams, concept evolution, feature drifts, temporal\u00a0\u2026", "num_citations": "320\n", "authors": ["1069"]}
{"title": "Leveraging bagging for evolving data streams\n", "abstract": " Bagging, boosting and Random Forests are classical ensemble methods used to improve the performance of single classifiers. They obtain superior performance by increasing the accuracy and diversity of the single classifiers. Attempts have been made to reproduce these methods in the more challenging context of evolving data streams. In this paper, we propose a new variant of bagging, called leveraging bagging. This method combines the simplicity of bagging with adding more randomization to the input, and output of the classifiers. We test our method by performing an evaluation study on synthetic and real-world datasets comprising up to ten million examples.", "num_citations": "308\n", "authors": ["1069"]}
{"title": "Data stream mining a practical approach\n", "abstract": " CiteSeerX \u2014 DATA STREAM MINING A Practical Approach Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA DATA STREAM MINING A Practical Approach Cached Download as a PDF Download Links [www.cs.waikato.ac.nz] Save to List Add to Collection Correct Errors Monitor Changes by Albert Bifet , Richard Kirkby Citations: 10 - 1 self Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract Keyphrases data stream mining practical approach Powered by: Apache Solr About CiteSeerX Submit and Index Documents Privacy Policy Help Data Source Contact Us Developed at and hosted by The College of Information Sciences and Technology \u00a9 2007-2019 The Pennsylvania \u2026", "num_citations": "258\n", "authors": ["1069"]}
{"title": "SAMOA: scalable advanced massive online analysis.\n", "abstract": " Abstract samoa (Scalable Advanced Massive Online Analysis) is a platform for mining big data streams. It provides a collection of distributed streaming algorithms for the most common data mining and machine learning tasks such as classification, clustering, and regression, as well as programming abstractions to develop new algorithms. It features a pluggable architecture that allows it to run on several distributed stream processing engines such as Storm, S4, and Samza. samoa is written in Java, is open source, and is available at http://samoa-project. net under the Apache Software License version 2.0.", "num_citations": "203\n", "authors": ["1069"]}
{"title": "Mining big data in real time\n", "abstract": " Albert Bifet Yahoo! Research Barcelona Avinguda Diagonal 177, 8th floor Barcelona, 08018, Catalonia, Spain E-mail: abifet@ yahoo-inc. com", "num_citations": "195\n", "authors": ["1069"]}
{"title": "Adaptive stream mining: Pattern learning and mining from evolving data streams\n", "abstract": " This book is a significant contribution to the subject of mining time-changing data streams and addresses the design of learning algorithms for this purpose. It introduces new contributions on several different aspects of the problem, identifying research opportunities and increasing the scope for applications. It also includes an in-depth study of stream mining and a theoretical analysis of proposed methods and algorithms. The first section is concerned with the use of an adaptive sliding window algorithm (ADWIN). Since this has rigorous performance guarantees, using it in place of counters or accumulators, it offers the possibility of extending such guarantees to learning and mining algorithms not initially designed for drifting data. Testing with several methods, including Na\u00efve Bayes, clustering, decision trees and ensemble methods, is discussed as well. The second part of the book describes a formal study of connected acyclic graphs, or'trees', from the point of view of closure-based mining, presenting efficient algorithms for subtree testing and for mining ordered and unordered frequent closed trees. Lastly, a general methodology to identify closed patterns in a data stream is outlined. This is applied to develop an incremental method, a sliding-window based method, and a method that mines closed trees adaptively from data streams. These are used to introduce classification methods for tree data streams.", "num_citations": "187\n", "authors": ["1069"]}
{"title": "Scikit-multiflow: A multi-output streaming framework\n", "abstract": " Abstract scikit-multiflow is a framework for learning from data streams and multi-output learning in Python. Conceived to serve as a platform to encourage the democratization of stream learning research, it provides multiple state-of-the-art learning methods, data generators and evaluators for different stream learning problems, including single-output, multi-output and multi-label. scikit-multiflow builds upon popular open source frameworks including scikitlearn, MOA and MEKA. Development follows the FOSS principles. Quality is enforced by complying with PEP8 guidelines, using continuous integration and functional testing. The source code is available at https://github. com/scikit-multiflow/scikit-multiflow.", "num_citations": "170\n", "authors": ["1069"]}
{"title": "Machine learning for data streams: with practical examples in MOA\n", "abstract": " A hands-on approach to tasks and techniques in data stream mining and real-time analytics, with examples in MOA, a popular freely available open-source software framework. Today many information sources\u2014including sensor networks, financial markets, social networks, and healthcare monitoring\u2014are so-called data streams, arriving sequentially and at high speed. Analysis must take place in real time, with partial data and without the capacity to store the entire data set. This book presents algorithms and techniques used in data stream mining and real-time analytics. Taking a hands-on approach, the book demonstrates the techniques using MOA (Massive Online Analysis), a popular, freely available open-source software framework, allowing readers to try out the techniques after reading the explanations. The book first offers a brief introduction to the topic, covering big data mining, basic methodologies for mining data streams, and a simple example of MOA. More detailed discussions follow, with chapters on sketching techniques, change, classification, ensemble methods, regression, clustering, and frequent pattern mining. Most of these chapters include exercises, an MOA-based lab session, or both. Finally, the book discusses the MOA software, covering the MOA graphical user interface, the command line, use of its API, and the development of new methods within MOA. The book will be an essential reference for readers who want to use data stream mining as a tool, researchers in innovation or data stream mining, and programmers who want to create new algorithms for MOA.", "num_citations": "161\n", "authors": ["1069"]}
{"title": "Efficient online evaluation of big data stream classifiers\n", "abstract": " The evaluation of classifiers in data streams is fundamental so that poorly-performing models can be identified, and either improved or replaced by better-performing models. This is an increasingly relevant and important task as stream data is generated from more sources, in real-time, in large quantities, and is now considered the largest source of big data. Both researchers and practitioners need to be able to effectively evaluate the performance of the methods they employ. However, there are major challenges for evaluation in a stream. Instances arriving in a data stream are usually time-dependent, and the underlying concept that they represent may evolve over time. Furthermore, the massive quantity of data also tends to exacerbate issues such as class imbalance. Current frameworks for evaluating streaming and online algorithms are able to give predictions in real-time, but as they use a prequential setting\u00a0\u2026", "num_citations": "161\n", "authors": ["1069"]}
{"title": "Scalable and efficient multi-label classification for evolving data streams\n", "abstract": " Many challenging real world problems involve multi-label data streams. Efficient methods exist for multi-label classification in non-streaming scenarios. However, learning in evolving streaming scenarios is more challenging, as classifiers must be able to deal with huge numbers of examples and to adapt to change using limited time and memory while being ready to predict at any point.               This paper proposes a new experimental framework for learning and evaluating on multi-label data streams, and uses it to study the performance of various methods. From this study, we develop a multi-label Hoeffding tree with multi-label classifiers at the leaves. We show empirically that this method is well suited to this challenging task. Using our new framework, which allows us to generate realistic multi-label data streams with concept drift (as well as real data), we compare with a selection of baseline methods, as\u00a0\u2026", "num_citations": "139\n", "authors": ["1069"]}
{"title": "Batch-incremental versus instance-incremental learning in dynamic and evolving data\n", "abstract": " Many real world problems involve the challenging context of data streams, where classifiers must be incremental: able to learn from a theoretically-infinite stream of examples using limited time and memory, while being able to predict at any point. Two approaches dominate the literature: batch-incremental methods that gather examples in batches to train models; and instance-incremental methods that learn from each example as it arrives. Typically, papers in the literature choose one of these approaches, but provide insufficient evidence or references to justify their choice. We provide a first in-depth analysis comparing both approaches, including how they adapt to concept drift, and an extensive empirical study to compare several different versions of each approach. Our results reveal the respective advantages and disadvantages of the methods, which we discuss in detail.", "num_citations": "130\n", "authors": ["1069"]}
{"title": "Fast perceptron decision tree learning from evolving data streams\n", "abstract": " Mining of data streams must balance three evaluation dimensions: accuracy, time and memory. Excellent accuracy on data streams has been obtained with Naive Bayes Hoeffding Trees\u2014Hoeffding Trees with naive Bayes models at the leaf nodes\u2014albeit with increased runtime compared to standard Hoeffding Trees. In this paper, we show that runtime can be reduced by replacing naive Bayes with perceptron classifiers, while maintaining highly competitive accuracy. We also show that accuracy can be increased even further by combining majority vote, naive Bayes, and perceptrons. We evaluate four perceptron-based learning strategies and compare them against appropriate baselines: simple perceptrons, Perceptron Hoeffding Trees, hybrid Naive Bayes Perceptron Trees, and bagged versions thereof. We implement a perceptron that uses the sigmoid activation function instead of the threshold activation\u00a0\u2026", "num_citations": "124\n", "authors": ["1069"]}
{"title": "Adaptive learning and mining for data streams and frequent patterns\n", "abstract": " This thesis is devoted to the design of data mining algorithms for evolving data streams and for the extraction of closed frequent trees. First, we deal with each of these tasks separately, and then we deal with them together, developing classification methods for data streams containing items that are trees. In the data stream model, data arrive at high speed, and the algorithms that must process them have very strict constraints of space and time. In the first part of this thesis we propose and illustrate a framework for developing algorithms that can adaptively learn from data streams that change over time. Our methods are based on using change detectors and estimator modules at the right places. We propose an adaptive sliding window algorithm ADWIN for detecting change and keeping updated statistics from a data stream, and use it as a black-box in place or counters or accumulators in algorithms initially not\u00a0\u2026", "num_citations": "113\n", "authors": ["1069"]}
{"title": "An effective evaluation measure for clustering on evolving data streams\n", "abstract": " Due to the ever growing presence of data streams, there has been a considerable amount of research on stream mining algorithms. While many algorithms have been introduced that tackle the problem of clustering on evolving data streams, hardly any attention has been paid to appropriate evaluation measures. Measures developed for static scenarios, namely structural measures and ground-truth-based measures, cannot correctly reflect errors attributable to emerging, splitting, or moving clusters. These situations are inherent to the streaming context due to the dynamic changes in the data distribution. In this paper we develop a novel evaluation measure for stream clustering called Cluster Mapping Measure (CMM). CMM effectively indicates different types of errors by taking the important properties of evolving data streams into account. We show in extensive experiments on real and synthetic data that CMM is a\u00a0\u2026", "num_citations": "103\n", "authors": ["1069"]}
{"title": "Kalman filters and adaptive windows for learning in data streams\n", "abstract": " We study the combination of Kalman filter and a recently proposed algorithm for dynamically maintaining a sliding window, for learning from streams of examples. We integrate this idea into two well-known learning algorithms, the Na\u00efve Bayes algorithm and the k-means clusterer. We show on synthetic data that the new algorithms do never worse, and in some cases much better, than the algorithms using only memoryless Kalman filters or sliding windows with no filtering.", "num_citations": "97\n", "authors": ["1069"]}
{"title": "Mining frequent closed graphs on evolving data streams\n", "abstract": " Graph mining is a challenging task by itself, and even more so when processing data streams which evolve in real-time. Data stream mining faces hard constraints regarding time and space for processing, and also needs to provide for concept drift detection. In this paper we present a framework for studying graph pattern mining on time-varying streams. Three new methods for mining frequent closed subgraphs are presented. All methods work on coresets of closed subgraphs, compressed representations of graph sets, and maintain these sets in a batch-incremental manner, but use different approaches to address potential concept drift. An evaluation study on datasets comprising up to four million graphs explores the strength and limitations of the proposed methods. To the best of our knowledge this is the first work on mining frequent closed subgraphs in non-stationary data streams.", "num_citations": "95\n", "authors": ["1069"]}
{"title": "Efficient data stream classification via probabilistic adaptive windows\n", "abstract": " In the context of a data stream, a classifier must be able to learn from a theoretically-infinite stream of examples using limited time and memory, while being able to predict at any point. Many methods deal with this problem by basing their model on a window of examples. We introduce a probabilistic adaptive window (PAW) for data-stream learning, which improves this windowing technique with a mechanism to include older examples as well as the most recent ones, thus maintaining information on past concept drifts while being able to adapt quickly to new ones. We exemplify PAW with lazy learning methods in two variations: one to handle concept drift explicitly, and the other to add classifier diversity using an ensemble. Along with the standard measures of accuracy and time and memory use, we compare classifiers against state-of-the-art classifiers from the data-stream literature.", "num_citations": "88\n", "authors": ["1069"]}
{"title": "Detecting sentiment change in Twitter streaming data\n", "abstract": " MOA-TweetReader is a real-time system to read tweets in real time, to detect changes, and to find the terms whose frequency changed. Twitter is a micro-blogging service built to discover what is happening at any moment in time, anywhere in the world. Twitter messages are short, and generated constantly, and well suited for knowledge discovery using data stream mining. MOA-TweetReader is a software extension to the MOA framework. Massive Online Analysis (MOA) is a software environment for implementing algorithms and running experiments for online learning from evolving data streams. MOA-TweetReader is released under the GNU GPL license.", "num_citations": "81\n", "authors": ["1069"]}
{"title": "Improving adaptive bagging methods for evolving data streams\n", "abstract": " We propose two new improvements for bagging methods on evolving data streams. Recently, two new variants of Bagging were proposed: ADWIN  Bagging and Adaptive-Size Hoeffding Tree (ASHT) Bagging. ASHT Bagging uses trees of different sizes, and ADWIN  Bagging uses ADWIN  as a change detector to decide when to discard underperforming ensemble members. We improve ADWIN  Bagging using Hoeffding Adaptive Trees, trees that can adaptively learn from data streams that change over time. To speed up the time for adapting to change of Adaptive-Size Hoeffding Tree (ASHT) Bagging, we add an error change detector for each classifier. We test our improvements by performing an evaluation study on synthetic and real-world datasets comprising up to ten million examples.", "num_citations": "79\n", "authors": ["1069"]}
{"title": "Iot big data stream mining\n", "abstract": " The challenge of deriving insights from the Internet of Things (IoT) has been recognized as one of the most exciting and key opportunities for both academia and industry. Advanced analysis of big data streams from sensors and devices is bound to become a key area of data mining research as the number of applications requiring such processing increases. Dealing with the evolution over time of such data streams, ie, with concepts that drift or change completely, is one of the core issues in IoT stream mining. This tutorial is a gentle introduction to mining IoT big data streams. The first part introduces data stream learners for classification, regression, clustering, and frequent pattern mining. The second part deals with scalability issues inherent in IoT applications, and discusses how to mine data streams on distributed engines such as Spark, Flink, Storm, and Samza.", "num_citations": "72\n", "authors": ["1069"]}
{"title": "MOA-TweetReader: Real-Time Analysis in Twitter Streaming Data\n", "abstract": " Twitter is a micro-blogging service built to discover what is happening at any moment in time, anywhere in the world. Twitter messages are short, generated constantly, and well suited for knowledge discovery using data stream mining. We introduce MOA-TweetReader, a system for processing tweets in real time. We show two main applications of the new system for studying Twitter data: detecting changes in term frequencies and performing real-time sentiment analysis.", "num_citations": "69\n", "authors": ["1069"]}
{"title": "An analysis of factors used in search engine ranking.\n", "abstract": " This paper investigates the influence of different page features on the ranking of search engine results. We use Google (via its API) as our testbed and analyze the result rankings for several queries of different categories using statistical methods. We reformulate the problem of learning the underlying, hidden scores as a binary classification problem. To this problem we then apply both linear and non-linear methods. In all cases, we split the data into a training set and a test set to obtain a meaningful, unbiased estimator for the quality of our predictor. Although our results clearly show that the scoring function cannot be approximated well using only the observed features, we do obtain many interesting insights along the way and discuss ways of obtaining a better estimate and main limitations in trying to do so.", "num_citations": "67\n", "authors": ["1069"]}
{"title": "Extremely fast decision tree mining for evolving data streams\n", "abstract": " Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning.", "num_citations": "66\n", "authors": ["1069"]}
{"title": "Accurate ensembles for data streams: Combining restricted Hoeffding trees using stacking\n", "abstract": " The success of simple methods for classification shows that is is often not necessary to model complex attribute interactions to obtain good classification accuracy on practical problems. In this paper, we propose to exploit this phenomenon in the data stream context by building an ensemble of Hoeffding trees that are each limited to a small subset of attributes. In this way, each tree is restricted to model interactions between attributes in its corresponding subset. Because it is not known a priori which attribute subsets are relevant for prediction, we build exhaustive ensembles that consider all possible attribute subsets of a given size. As the resulting Hoeffding trees are not all equally important, we weigh them in a suitable manner to obtain accurate classifications. This is done by combining the log-odds of their probability estimates using sigmoid perceptrons, with one perceptron per class. We propose a mechanism for setting the perceptrons\u2019 learning rate using the ADWIN change detection method for data streams, and also use ADWIN to reset ensemble members (ie Hoeffding trees) when they no longer perform well. Our experiments show that the resulting ensemble classifier outperforms bagging for data streams in terms of accuracy when both are used in conjunction with adaptive naive Bayes Hoeffding trees, at the expense of runtime and memory consumption.", "num_citations": "66\n", "authors": ["1069"]}
{"title": "Big data stream learning with samoa\n", "abstract": " Big data is flowing into every area of our life, professional and personal. Big data is defined as datasets whose size is beyond the ability of typical software tools to capture, store, manage and analyze, due to the time and memory complexity. Velocity is one of the main properties of big data. In this demo, we present SAMOA (Scalable Advanced Massive Online Analysis), an open-source platform for mining big data streams. It provides a collection of distributed streaming algorithms for the most common data mining and machine learning tasks such as classification, clustering, and regression, as well as programming abstractions to develop new algorithms. It features a pluggable architecture that allows it to run on several distributed stream processing engines such as Storm, S4, and Samza. SAMOA is written in Java and is available at http://samoa-project.net under the Apache Software License version 2.0.", "num_citations": "63\n", "authors": ["1069"]}
{"title": "Machine learning for streaming data: state of the art, challenges, and opportunities\n", "abstract": " Incremental learning, online learning, and data stream learning are terms commonly associated with learning algorithms that update their models given a continuous influx of data without performing multiple passes over data. Several works have been devoted to this area, either directly or indirectly as characteristics of big data processing, i.e., Velocity and Volume. Given the current industry needs, there are many challenges to be addressed before existing methods can be efficiently applied to real-world problems. In this work, we focus on elucidating the connections among the current stateof- the-art on related fields; and clarifying open challenges in both academia and industry. We treat with special care topics that were not thoroughly investigated in past position and survey papers. This work aims to evoke discussion and elucidate the current research opportunities, highlighting the relationship of different\u00a0\u2026", "num_citations": "62\n", "authors": ["1069"]}
{"title": "Strip: stream learning of influence probabilities\n", "abstract": " Influence-driven diffusion of information is a fundamental process in social networks. Learning the latent variables of such process, ie, the influence strength along each link, is a central question towards understanding the structure and function of complex networks, modeling information cascades, and developing applications such as viral marketing.", "num_citations": "57\n", "authors": ["1069"]}
{"title": "MOA: a real-time analytics open source framework\n", "abstract": " Massive Online Analysis (MOA) is a software environment for implementing algorithms and running experiments for online learning from evolving data streams. MOA is designed to deal with the challenging problems of scaling up the implementation of state of the art algorithms to real world dataset sizes and of making algorithms comparable in benchmark streaming settings. It contains a collection of offline and online algorithms for classification, clustering and graph mining as well as tools for evaluation. For researchers the framework yields insights into advantages and disadvantages of different approaches and allows for the creation of benchmark streaming data sets through stored, shared and repeatable settings for the data feeds. Practitioners can use the framework to easily compare algorithms and apply them to real world data sets and settings. MOA supports bi-directional interaction with\u00a0\u2026", "num_citations": "55\n", "authors": ["1069"]}
{"title": "Vht: Vertical hoeffding tree\n", "abstract": " IoT big data requires new machine learning methods able to scale to large size of data arriving at high speed. Decision trees are popular machine learning models since they are very effective, yet easy to interpret and visualize. In the literature, we can find distributed algorithms for learning decision trees, and also streaming algorithms, but not algorithms that combine both features. In this paper we present the Vertical Hoeffding Tree (VHT), the first distributed streaming algorithm for learning decision trees. It features a novel way of distributing decision trees via vertical parallelism. The algorithm is implemented on top of Apache SAMOA, a platform for mining big data streams, and thus able to run on real-world clusters. Our experiments to study the accuracy and throughput of VHT prove its ability to scale while attaining superior performance compared to sequential decision trees.", "num_citations": "46\n", "authors": ["1069"]}
{"title": "Streamdm: Advanced data mining in spark streaming\n", "abstract": " Real-time analytics are becoming increasingly important due to the large amount of data that is being created continuously. Drawing from our experiences at Huawei Noah's Ark Lab, we present and demonstrate here StreamDM, a new open source data mining and machine learning library, designed on top of Spark Streaming, an extension of the core Spark API that enables scalable stream processing of data streams. StreamDM is designed to be easily extended and used, either practitioners, developers, or researchers, and is the first library to contain advanced stream mining algorithms for Spark Streaming.", "num_citations": "46\n", "authors": ["1069"]}
{"title": "Ensembles of restricted hoeffding trees\n", "abstract": " The success of simple methods for classification shows that is is often not necessary to model complex attribute interactions to obtain good classification accuracy on practical problems. In this article, we propose to exploit this phenomenon in the data stream context by building an ensemble of Hoeffding trees that are each limited to a small subset of attributes. In this way, each tree is restricted to model interactions between attributes in its corresponding subset. Because it is not known a priori which attribute subsets are relevant for prediction, we build exhaustive ensembles that consider all possible attribute subsets of a given size. As the resulting Hoeffding trees are not all equally important, we weigh them in a suitable manner to obtain accurate classifications. This is done by combining the log-odds of their probability estimates using sigmoid perceptrons, with one perceptron per class. We propose a mechanism for\u00a0\u2026", "num_citations": "46\n", "authors": ["1069"]}
{"title": "Mining adaptively frequent closed unlabeled rooted trees in data streams\n", "abstract": " Closed patterns are powerful representatives of frequent patterns, since they eliminate redundant information. We propose a new approach for mining closed unlabeled rooted trees adaptively from data streams that change over time. Our approach is based on an efficient representation of trees and a low complexity notion of relaxed closed trees, and leads to an on-line strategy and an adaptive sliding window technique for dealing with changes over time. More precisely, we first present a general methodology to identify closed patterns in a data stream, using Galois Lattice Theory. Using this methodology, we then develop three closed tree mining algorithms: an incremental one IncTreeNat, a sliding-window based one, WinTreeNat, and finally one that mines closed trees adaptively from data streams, AdaTreeNat. To the best of our knowledge this is the first work on mining frequent closed trees in streaming data\u00a0\u2026", "num_citations": "44\n", "authors": ["1069"]}
{"title": "Adaptive model rules from high-speed data streams\n", "abstract": " Decision rules are one of the most expressive and interpretable models for machine learning. In this article, we present Adaptive Model Rules (AMRules), the first stream rule learning algorithm for regression problems. In AMRules, the antecedent of a rule is a conjunction of conditions on the attribute values, and the consequent is a linear combination of the attributes. In order to maintain a regression model compatible with the most recent state of the process generating data, each rule uses a Page-Hinkley test to detect changes in this process and react to changes by pruning the rule set. Online learning might be strongly affected by outliers. AMRules is also equipped with outliers detection mechanisms to avoid model adaption using anomalous examples. In the experimental section, we report the results of AMRules on benchmark regression problems, and compare the performance of our system with other\u00a0\u2026", "num_citations": "37\n", "authors": ["1069"]}
{"title": "On learning guarantees to unsupervised concept drift detection on data streams\n", "abstract": " Motivated by the Statistical Learning Theory (SLT), which provides a theoretical framework to ensure when supervised learning algorithms generalize input data, this manuscript relies on the Algorithmic Stability framework to prove learning bounds for the unsupervised concept drift detection on data streams. Based on such proof, we also designed the Plover algorithm to detect drifts using different measure functions, such as Statistical Moments and the Power Spectrum. In this way, the criterion for issuing data changes can also be adapted to better address the target task. From synthetic and real-world scenarios, we observed that each data stream may require a different measure function to identify concept drifts, according to the underlying characteristics of the corresponding application domain. In addition, we discussed about the differences of our approach against others from literature, and showed illustrative\u00a0\u2026", "num_citations": "36\n", "authors": ["1069"]}
{"title": "Deep learning in partially-labeled data streams\n", "abstract": " Of the considerable research on data streams, relatively little deals with classification where only some of the instances in the stream are labeled. Most state-of-the-art data-stream algorithms do not have an effective way of dealing with unlabeled instances from the same domain. In this paper we explore deep learning techniques that provide important advantages such as the ability to learn incrementally in constant memory, and from unlabeled examples. We develop two deep learning methods and explore empirically via a series of empirical evaluations the application to several data streams scenarios based on real data. We find that our methods can offer competitive accuracy as compared with existing popular data-stream learners.", "num_citations": "35\n", "authors": ["1069"]}
{"title": "Mining frequent closed rooted trees\n", "abstract": " Many knowledge representation mechanisms are based on tree-like structures, thus symbolizing the fact that certain pieces of information are related in one sense or another. There exists a well-studied process of closure-based data mining in the itemset framework: we consider the extension of this process into trees. We focus mostly on the case where labels on the nodes are nonexistent or unreliable, and discuss algorithms for closure-based mining that only rely on the root of the tree and the link structure. We provide a notion of intersection that leads to a deeper understanding of the notion of support-based closure, in terms of an actual closure operator. We describe combinatorial characterizations and some properties of ordered trees, discuss their applicability to unordered trees, and rely on them to design efficient algorithms for mining frequent closed subtrees both in the ordered and the unordered\u00a0\u2026", "num_citations": "35\n", "authors": ["1069"]}
{"title": "Bitcoin volatility forecasting with a glimpse into buy and sell orders\n", "abstract": " Bitcoin is one of the most prominent decentralized digital cryptocurrencies. Ability to understand which factors drive the fluctuations of the Bitcoin price and to what extent they are predictable is interesting both from the theoretical and practical perspective. In this paper, we study the problem of the Bitcoin short-term volatility forecasting based on volatility history and order book data. Order book, consisting of buy and sell orders over time, reflects the intention of the market and is closely related to the evolution of volatility. We propose temporal mixture models capable of adaptively exploiting both volatility history and order book features. By leveraging rolling and incremental learning and evaluation procedures, we demonstrate the prediction performance of our model as well as studying the robustness, in comparison to a variety of statistical and machine learning baselines. Meanwhile, our temporal mixture model\u00a0\u2026", "num_citations": "34\n", "authors": ["1069"]}
{"title": "Clustering performance on evolving data streams: Assessing algorithms and evaluation measures within MOA\n", "abstract": " In today's applications, evolving data streams are ubiquitous. Stream clustering algorithms were introduced to gain useful knowledge from these streams in real-time. The quality of the obtained clusterings, i.e. how good they reflect the data, can be assessed by evaluation measures. A multitude of stream clustering algorithms and evaluation measures for clusterings were introduced in the literature, however, until now there is no general tool for a direct comparison of the different algorithms or the evaluation measures. In our demo, we present a novel experimental framework for both tasks. It offers the means for extensive evaluation and visualization and is an extension of the Massive Online Analysis (MOA) software environment released under the GNU GPL License.", "num_citations": "31\n", "authors": ["1069"]}
{"title": "Classifier concept drift detection and the illusion of progress\n", "abstract": " When a new concept drift detection method is proposed, a common way to show the benefits of the new method, is to use a classifier to perform an evaluation where each time the new algorithm detects change, the current classifier is replaced by a new one. Accuracy in this setting is considered a good measure of the quality of the change detector. In this paper we claim that this is not a good evaluation methodology and we show how a non-change detector can improve the accuracy of the classifier in this setting. We claim that this is due to the existence of a temporal dependence on the data and we propose not to evaluate concept drift detectors using only classifiers.", "num_citations": "30\n", "authors": ["1069"]}
{"title": "On dynamic feature weighting for feature drifting data streams\n", "abstract": " The ubiquity of data streams has been encouraging the development of new incremental and adaptive learning algorithms. Data stream learners must be fast, memory-bounded, but mainly, tailored to adapt to possible changes in the data distribution, a phenomenon named concept drift. Recently, several works have shown the impact of a so far nearly neglected type of drifcccct: feature drifts. Feature drifts occur whenever a subset of features becomes, or ceases to be, relevant to the learning task. In this paper we (i) provide insights into how the relevance of features can be tracked as a stream progresses according to information theoretical Symmetrical Uncertainty; and (ii) how it can be used to boost two learning schemes: Naive Bayesian and k-Nearest Neighbor. Furthermore, we investigate the usage of these two new dynamically weighted learners as prediction models in the leaves of the Hoeffding\u00a0\u2026", "num_citations": "30\n", "authors": ["1069"]}
{"title": "Distributed adaptive model rules for mining big data streams\n", "abstract": " Decision rules are among the most expressive data mining models. We propose the first distributed streaming algorithm to learn decision rules for regression tasks. The algorithm is available in SAMOA (Scalable Advanced Massive Online Analysis), an open-source platform for mining big data streams. It uses a hybrid of vertical and horizontal parallelism to distribute Adaptive Model Rules (AMRules) on a cluster. The decision rules built by AMRules are comprehensible models, where the antecedent of a rule is a conjunction of conditions on the attribute values, and the consequent is a linear combination of the attributes. Our evaluation shows that this implementation is scalable in relation to CPU and memory consumption. On a small commodity Samza cluster of 9 nodes, it can handle a rate of more than 30000 instances per second, and achieve a speedup of up to 4.7x over the sequential version.", "num_citations": "30\n", "authors": ["1069"]}
{"title": "Drift detection using stream volatility\n", "abstract": " Current methods in data streams that detect concept drifts in the underlying distribution of data look at the distribution difference using statistical measures based on mean and variance. Existing methods are unable to proactively approximate the probability of a concept drift occurring and predict future drift points. We extend the current drift detection design by proposing the use of historical drift trends to estimate the probability of expecting a drift at different points across the stream, which we term the expected drift probability. We offer empirical evidence that applying our expected drift probability with the state-of-the-art drift detector, ADWIN, we can improve the detection performance of ADWIN by significantly reducing the false positive rate. To the best of our knowledge, this is the first work that investigates this idea. We also show that our overall concept can be easily incorporated back onto incremental\u00a0\u2026", "num_citations": "29\n", "authors": ["1069"]}
{"title": "Kaggle LSHTC4 winning solution\n", "abstract": " Our winning submission to the 2014 Kaggle competition for Large Scale Hierarchical Text Classification (LSHTC) consists mostly of an ensemble of sparse generative models extending Multinomial Naive Bayes. The base-classifiers consist of hierarchically smoothed models combining document, label, and hierarchy level Multinomials, with feature pre-processing using variants of TF-IDF and BM25. Additional diversification is introduced by different types of folds and random search optimization for different measures. The ensemble algorithm optimizes macroFscore by predicting the documents for each label, instead of the usual prediction of labels per document. Scores for documents are predicted by weighted voting of base-classifier outputs with a variant of Feature-Weighted Linear Stacking. The number of documents per label is chosen using label priors and thresholding of vote scores. This document describes the models and software used to build our solution. Reproducing the results for our solution can be done by running the scripts included in the Kaggle package. A package omitting precomputed result files is also distributed. All code is open source, released under GNU GPL 2.0, and GPL 3.0 for Weka and Meka dependencies.", "num_citations": "29\n", "authors": ["1069"]}
{"title": "Random forests of very fast decision trees on GPU for mining evolving big data streams\n", "abstract": " Random Forest is a classical ensemble method used to improve the performance of single tree classifiers. It is able to obtain superior performance by increasing the diversity of the single classifiers. However, in the more challenging context of evolving data streams, the classifier has also to be adaptive and work under very strict constraints of space and time. Furthermore, the computational load of using a large number of classifiers can make its application extremely expensive.", "num_citations": "29\n", "authors": ["1069"]}
{"title": "Stream data mining using the MOA framework\n", "abstract": " Massive Online Analysis (MOA) is a software framework that provides algorithms and evaluation methods for mining tasks on evolving data streams. In addition to supervised and unsupervised learning, MOA has recently been extended to support multi-label classification and graph mining. In this demonstrator we describe the main features of MOA and present the newly added methods for outlier detection on streaming data. Algorithms can be compared to established baseline methods such as LOF and ABOD using standard ranking measures including Spearman rank coefficient and the AUC measure. MOA is an open source project and videos as well as tutorials are publicly available on the MOA homepage.", "num_citations": "29\n", "authors": ["1069"]}
{"title": "Streaming multi-label classification\n", "abstract": " This paper presents a new experimental framework for studying multi-label evolving stream classification, with efficient methods that combine the best practices in streaming scenarios with the best practices in multi-label classification. Many real world problems involve data which can be considered as multi-label data streams. Efficient methods exist for multi-label classification in non streaming scenarios. However, learning in evolving streaming scenarios is more challenging, as the learners must be able to adapt to change using limited time and memory. We present a new experimental software that extends the MOA framework. Massive Online Analysis (MOA) is a software environment for implementing algorithms and running experiments for online learning from evolving data streams. It is released under the GNU GPL license.", "num_citations": "28\n", "authors": ["1069"]}
{"title": "Adaptive random forests for data stream regression.\n", "abstract": " Data stream mining is a hot topic in the machine learning community that tackles the problem of learning and updating predictive models as new data becomes available over time. Even though several new methods are proposed every year, most focus on the classification task and overlook the regression task. In this paper, we propose an adaptation to the Adaptive Random Forest so that it can handle regression tasks, namely ARF-Reg. ARF-Reg is empirically evaluated and compared to the state-of-the-art data stream regression algorithms, thus highlighting its applicability in different data stream scenarios.", "num_citations": "27\n", "authors": ["1069"]}
{"title": "Multi-label classification with meta-labels\n", "abstract": " The area of multi-label classification has rapidly developed in recent years. It has become widely known that the baseline binary relevance approach can easily be outperformed by methods which learn labels together. A number of methods have grown around the label power set approach, which models label combinations together as class values in a multi-class problem. We describe the label-power set-based solutions under a general framework of meta-labels and provide some theoretical justification for this framework which has been lacking, explaining how meta-labels essentially allow a random projection into a space where non-linearities can easily be tackled with established linear learning algorithms. The proposed framework enables comparison and combination of related approaches to different multi-label problems. We present a novel model in the framework and evaluate it empirically against\u00a0\u2026", "num_citations": "25\n", "authors": ["1069"]}
{"title": "A streaming flow-based technique for traffic classification applied to 12+ 1 years of Internet traffic\n", "abstract": " The continuous evolution of Internet traffic and its applications makes the classification of network traffic a topic far from being completely solved. An essential problem in this field is that most of proposed techniques in the literature are based on a static view of the network traffic (i.e., they build a model or a set of patterns from a static, invariable dataset). However, very little work has addressed the practical limitations that arise when facing a more realistic scenario with an infinite, continuously evolving stream of network traffic flows. In this paper, we propose a streaming flow-based classification solution based on Hoeffding Adaptive Tree, a machine learning technique specifically designed for evolving data streams. The main novelty of our proposal is that it is able to automatically adapt to the continuous evolution of the network traffic without storing any traffic data. We apply our solution to a 12 + 1 year-long\u00a0\u2026", "num_citations": "23\n", "authors": ["1069"]}
{"title": "Efficient multi-label classification for evolving data streams\n", "abstract": " Many real world problems involve data which can be considered as multi-label data streams. Efficient methods exist for multi-label classification in non streaming scenarios. However, learning in evolving streaming scenarios is more challenging, as the learners must be able to adapt to change using limited time and memory. This paper proposes a new experimental framework for studying multi-label evolving stream classification, and new efficient methods that combine the best practices in streaming scenarios with the best practices in multi-label classification. We present a Multi-label Hoeffding Tree with multilabel classifiers at the leaves as a base classifier. We obtain fast and accurate methods, that are well suited for this challenging multi-label classification streaming task. Using the new experimental framework, we test our methodology by performing an evaluation study on synthetic and real-world datasets. In comparison to well-known batch multi-label methods, we obtain encouraging results.", "num_citations": "23\n", "authors": ["1069"]}
{"title": "Streaming random patches for evolving data stream classification\n", "abstract": " Ensemble methods are a popular choice for learning from evolving data streams. This popularity is due to (i) the ability to simulate simple, yet, successful ensemble learning strategies, such as bagging and random forests; (ii) the possibility of incorporating drift detection and recovery in conjunction to the ensemble algorithm; (iii) the availability of efficient incremental base learners, such as Hoeffding Trees. In this work, we introduce the Streaming Random Patches (SRP) algorithm, an ensemble method specially adapted to stream classification which combines random subspaces and online bagging. We provide theoretical insights and empirical results illustrating different aspects of SRP. In particular, we explain how the widely adopted incremental Hoeffding trees are not, in fact, unstable learners, unlike their batch counterparts, and how this fact significantly influences ensemble methods design and performance\u00a0\u2026", "num_citations": "22\n", "authors": ["1069"]}
{"title": "Data stream classification using random feature functions and novel method combinations\n", "abstract": " Big Data streams are being generated in a faster, bigger, and more commonplace. In this scenario, Hoeffding Trees are an established method for classification. Several extensions exist, including high-performing ensemble setups such as online and leveraging bagging. Also, k-nearest neighbors is a popular choice, with most extensions dealing with the inherent performance limitations over a potentially-infinite stream.At the same time, gradient descent methods are becoming increasingly popular, owing in part to the successes of deep learning. Although deep neural networks can learn incrementally, they have so far proved too sensitive to hyper-parameter options and initial conditions to be considered an effective \u2018off-the-shelf\u2019 data-streams solution.In this work, we look at combinations of Hoeffding-trees, nearest neighbor, and gradient descent methods with a streaming preprocessing approach in the form of a\u00a0\u2026", "num_citations": "22\n", "authors": ["1069"]}
{"title": "Recurring concept meta-learning for evolving data streams\n", "abstract": " When concept drift is detected during classification in a data stream, a common remedy is to retrain a framework\u2019s classifier. However, this loses useful information if the classifier has learnt the current concept well, and this concept will recur again in the future. Some frameworks retain and reuse classifiers, but it can be time-consuming to select an appropriate classifier to reuse. These frameworks rarely match the accuracy of state-of-the-art ensemble approaches. For many data stream tasks, speed is important: fast, accurate frameworks are needed for time-dependent applications. We propose the Enhanced Concept Profiling Framework (ECPF), which aims to recognise recurring concepts and reuse a classifier trained previously, enabling accurate classification immediately following a drift. The novelty of ECPF is in how it uses similarity of classifications on new data, between a new classifier and existing classifiers\u00a0\u2026", "num_citations": "20\n", "authors": ["1069"]}
{"title": "Change detection in categorical evolving data streams\n", "abstract": " Detecting change in evolving data streams is a central issue for accurate adaptive learning. In real world applications, data streams have categorical features, and changes induced in the data distribution of these categorical features have not been considered extensively so far. Previous work on change detection focused on detecting changes in the accuracy of the learners, but without considering changes in the data distribution.", "num_citations": "20\n", "authors": ["1069"]}
{"title": "Mining frequent closed trees in evolving data streams\n", "abstract": " We propose new algorithms for adaptively mining closed rooted trees, both labeled and unlabeled, from data streams that change over time. Closed patterns are powerful representatives of frequent patterns, since they eliminate redundant information. Our approach is based on an advantageous representation of trees and a low-complexity notion of relaxed closed trees, as well as ideas from Galois Lattice Theory. More precisely, we present three closed tree mining algorithms in sequence: an incremental one, IncTreeMiner, a sliding-window based one, WinTreeMiner, and finally one that mines closed trees adaptively from data streams, AdaTreeMiner. By adaptive we mean here that it presents at all times the closed trees that are frequent in the current state of the data stream. To the best of our knowledge this is the first work on mining closed frequent trees in streaming data varying with time. We give a first\u00a0\u2026", "num_citations": "20\n", "authors": ["1069"]}
{"title": "Use of ensembles of Fourier spectra in capturing recurrent concepts in data streams\n", "abstract": " In this research, we apply ensembles of Fourier encoded spectra to capture and mine recurring concepts in a data stream environment. Previous research showed that compact versions of Decision Trees can be obtained by applying the Discrete Fourier Transform to accurately capture recurrent concepts in a data stream. However, in highly volatile environments where new concepts emerge often, the approach of encoding each concept in a separate spectrum is no longer viable due to memory overload and thus in this research we present an ensemble approach that addresses this problem. Our empirical results on real world data and synthetic data exhibiting varying degrees of recurrence reveal that the ensemble approach outperforms the single spectrum approach in terms of classification accuracy, memory and execution time.", "num_citations": "19\n", "authors": ["1069"]}
{"title": "Adaptive XML tree classification on evolving data streams\n", "abstract": " We propose a new method to classify patterns, using closed and maximal frequent patterns as features. Generally, classification requires a previous mapping from the patterns to classify to vectors of features, and frequent patterns have been used as features in the past. Closed patterns maintain the same information as frequent patterns using less space and maximal patterns maintain approximate information. We use them to reduce the number of classification features. We present a new framework for XML tree stream classification. For the first component of our classification framework, we use closed tree mining algorithms for evolving data streams. For the second component, we use state of the art classification methods for data streams. To the best of our knowledge this is the first work on tree classification in streaming data varying with time. We give a first experimental evaluation of the proposed\u00a0\u2026", "num_citations": "19\n", "authors": ["1069"]}
{"title": "An efficient closed frequent itemset miner for the MOA stream mining system\n", "abstract": " Mining itemsets is a central task in data mining, both in the batch and the streaming paradigms. While robust, efficient, and well-tested implementations exist for batch mining, hardly any publicly available equivalent exists for the streaming scenario. The lack of an efficient, usable tool for the task hinders its use by practitioners and makes it difficult to assess new research in the area. To alleviate this situation, we review the algorithms described in the literature, and implement and evaluate the IncMine algorithm by Cheng, Ke and Ng [J. Intell. Inf. Syst. 31 (3)(2008), 191\u2013215] for mining frequent closed itemsets from data streams. Our implementation works on top of the MOA (Massive Online Analysis) stream mining framework to ease its use and integration with other stream mining tasks. We provide a PAC-style rigorous analysis of the quality of the output of IncMine as a function of its parameters; this type of analysis is\u00a0\u2026", "num_citations": "18\n", "authors": ["1069"]}
{"title": "Efficient exact and approximate algorithms for computing betweenness centrality in directed graphs\n", "abstract": " In this paper, first given a directed network G and a vertex , we propose a new exact algorithm to compute betweenness score of r. Our algorithm pre-computes a set , which is used to prune a huge amount of computations that do not contribute in the betweenness score of r. Then, for the cases where  is large, we present a randomized algorithm that samples from  and performs computations for only the sampled elements. We show that this algorithm provides an -approximation of the betweenness score of r. Finally, we empirically evaluate our algorithms and show that they significantly outperform the most efficient existing algorithms, in terms of both running time and accuracy. Our experiments also show that our proposed algorithms can effectively compute betweenness scores of all vertices in a set of vertices.", "num_citations": "17\n", "authors": ["1069"]}
{"title": "Inferring demographics and social networks of mobile device users on campus from AP-trajectories\n", "abstract": " Exploring demographics and social networks of Internet users are widely used for many applications such as recommendation systems. The popularity of mobile devices (eg, smartphones) and location-based Internet services (eg, Google Maps) facilitates the collection of users' locations over time. Despite recent efforts to predict users' attributes (eg, age and gender) and social networks based on utilizing the rich location context knowledge (eg, name, type, and description) of places of interest (eg, restaurants and hotels) they checked-in on location-based online social networks such as Foursqure and Gowalla, little attention has been given to inferring attributes and social networks of mobile device users based on their spatiotemporal trajectories with less/no location context knowledge. In this paper we collect logs of thousands of mobile devices' network connections to wireless access points (APs) of two campuses\u00a0\u2026", "num_citations": "17\n", "authors": ["1069"]}
{"title": "Boosting decision stumps for dynamic feature selection on data streams\n", "abstract": " Feature selection targets the identification of which features of a dataset are relevant to the learning task. It is also widely known and used to improve computation times, reduce computation requirements, and to decrease the impact of the curse of dimensionality and enhancing the generalization rates of classifiers. In data streams, classifiers shall benefit from all the items above, but more importantly, from the fact that the relevant subset of features may drift over time. In this paper, we propose a novel dynamic feature selection method for data streams called Adaptive Boosting for Feature Selection (ABFS). ABFS chains decision stumps and drift detectors, and as a result, identifies which features are relevant to the learning task as the stream progresses with reasonable success. In addition to our proposed algorithm, we bring feature selection-specific metrics from batch learning to streaming scenarios. Next, we\u00a0\u2026", "num_citations": "16\n", "authors": ["1069"]}
{"title": "Mining frequent closed unordered trees through natural representations\n", "abstract": " Many knowledge representation mechanisms consist of link-based structures; they may be studied formally by means of unordered trees. Here we consider the case where labels on the nodes are nonexistent or unreliable, and propose data mining processes focusing on just the link structure. We propose a representation of ordered trees, describe a combinatorial characterization and some properties, and use them to propose an efficient algorithm for mining frequent closed subtrees from a set of input trees. Then we focus on unordered trees, and show that intrinsic characterizations of our representation provide for a way of avoiding the repeated exploration of unordered trees, and then we give an efficient algorithm for mining frequent closed unordered trees.", "num_citations": "16\n", "authors": ["1069"]}
{"title": "Adaptive random forests with resampling for imbalanced data streams\n", "abstract": " The large volume of data generated by computer networks, smartphones, wearables and a wide range of sensors, which produce real-time data, are only useful if they can be efficiently processed so that individuals can make timely decisions based on them. In this context, machine learning techniques are widely used. While it performs better than humans in such tasks, every machine learning algorithm has a certain intrinsic bias, which means they assume that the data have specific characteristics, such as having a balanced distribution between classes. As many real-world applications present imbalanced traits in their data, this topic is gaining repercussion over time. In this work, we present the Adaptive Random Forest with Resampling (ARF RE ), which is a classifier designed to deal with imbalanced datasets. ARF RE  resample the instances based on the current class label distribution. We show through a set\u00a0\u2026", "num_citations": "15\n", "authors": ["1069"]}
{"title": "Large-scale learning from data streams with apache samoa\n", "abstract": " Apache SAMOA (Scalable Advanced Massive Online Analysis) is an open-source platform for mining big data streams. Big data is defined as datasets whose size is beyond the ability of typical software tools to capture, store, manage, and analyze, due to the time and memory complexity. Apache SAMOA provides a collection of distributed streaming algorithms for the most common data mining and machine learning tasks such as classification, clustering, and regression, as well as programming abstractions to develop new algorithms. It features a pluggable architecture that allows it to run on several distributed stream processing engines such as Apache Flink, Apache Storm, and Apache Samza. Apache SAMOA is written in Java and is available at                  https://samoa.incubator.apache.org                                 under the Apache Software License version 2.0.", "num_citations": "15\n", "authors": ["1069"]}
{"title": "Merit-guided dynamic feature selection filter for data streams\n", "abstract": " Learning from ephemeral data streams has garnered the interest of both researchers and practitioners towards adaptive learning techniques. Despite the convincing results obtained thus far, most of the current research still overlooks that the relevance of features may change throughout the learning process. Scenarios where features become - or cease to be - relevant to the learning task are called feature drifting data streams, and the identification of which features are relevant becomes even more challenging when the feature space is high-dimensional. To select relevant features during the progress of data streams, we propose a merit-guided and classifier-independent dynamic feature selection algorithm named DynamIc SymmetriCal Uncertainty Selection for Streams (DISCUSS). We evaluate our proposal on both synthetic and real-world datasets and show that DISCUSS can boost kNN and Naive Bayes\u00a0\u2026", "num_citations": "14\n", "authors": ["1069"]}
{"title": "Telemetry-based stream-learning of BGP anomalies\n", "abstract": " Recent technology evolution allows network equipments to continuously stream a wealth of\" telemetry\" information, which pertains to multiple protocols and layers of the stack, at a very fine spatialgrain and at high-frequency. Processing this deluge of telemetry data in real-time clearly offers new opportunities for network control and troubleshooting, but also poses serious challenges.", "num_citations": "14\n", "authors": ["1069"]}
{"title": "River: machine learning for streaming data in Python\n", "abstract": " River is a machine learning library for dynamic data streams and continual learning. It provides multiple state-of-the-art learning methods, data generators/transformers, performance metrics and evaluators for different stream learning problems. It is the result from the merger of two popular packages for stream learning in Python: Creme and scikitmultiflow. River introduces a revamped architecture based on the lessons learnt from the seminal packages. River\u2019s ambition is to be the go-to library for doing machine learning on streaming data. Additionally, this open source package brings under the same um-", "num_citations": "13\n", "authors": ["1069"]}
{"title": "Adaptive XGBoost for evolving data streams\n", "abstract": " Boosting is an ensemble method that combines base models in a sequential manner to achieve high predictive accuracy. A popular learning algorithm based on this ensemble method is eXtreme Gradient Boosting (XGB). We present an adaptation of XGB for classification of evolving data streams. In this setting, new data arrives over time and the relationship between the class and the features may change in the process, thus exhibiting concept drift. The proposed method creates new members of the ensemble from mini-batches of data as new data becomes available. The maximum ensemble size is fixed, but learning does not stop when this size is reached because the ensemble is updated on new data to ensure consistency with the current concept. We also explore the use of concept drift detection to trigger a mechanism to update the ensemble. We test our method on real and synthetic data with concept drift\u00a0\u2026", "num_citations": "13\n", "authors": ["1069"]}
{"title": "Low-latency multi-threaded ensemble learning for dynamic big data streams\n", "abstract": " Real-time mining of evolving data streams involves new challenges when targeting today's application domains such as the Internet of the Things: increasing volume, velocity and volatility requires data to be processed on-the-fly with fast reaction and adaptation to changes. This paper presents a high performance scalable design for decision trees and ensemble combinations that makes use of the vector SIMD and multicore capabilities available in modern processors to provide the required throughput and accuracy. The proposed design offers very low latency and good scalability with the number of cores on commodity hardware when compared to other state-of-the art implementations. On an Intel i7-based system, processing a single decision tree is 6\u00d7 faster than MOA (Java), and 7\u00d7 faster than StreamDM (C++), two well-known reference implementations. On the same system, the use of the 6 cores (and 12\u00a0\u2026", "num_citations": "13\n", "authors": ["1069"]}
{"title": "Incremental ensemble classifier addressing non-stationary fast data streams\n", "abstract": " Classification of data points in a data stream is a fundamentally different set of challenges than data mining on static data. While streaming data is often placed into the context of \"Big Data\" (or more specifically \"Fast Data\") wherein one-pass algorithms are used, true data streams offer additional hurdles due to their dynamic, evolving, and non-stationary nature. During the stream, the available labels (or concepts) often change, and a concept's definition in the feature space can also evolve (or drift) over time. The core issue is that the hidden generative function of the data is not a constant function, but rather evolves over time. This is known as a non-stationary distribution. In this paper, we describe a new approach to using ensembles for stream classification. While the core method is straightforward, it is specifically designed to adapt quickly with very little overhead to the dynamic and evolving nature of data streams\u00a0\u2026", "num_citations": "13\n", "authors": ["1069"]}
{"title": "Intersection algorithms and a closure operator on unordered trees\n", "abstract": " Link-based data may be studied formally by means of unordered trees. On a dataset formed by such link-based data, a natural notion of support-based closure can be immediately defined. Abstracting information from subsets of such data requires, first, a formal notion of intersection; second, deeper understanding of the notion of closure; and, third, efficient algorithms for computing intersections on unordered trees. We provide answers to these three questions.", "num_citations": "13\n", "authors": ["1069"]}
{"title": "Data stream analysis: Foundations, major tasks and tools\n", "abstract": " The significant growth of interconnected Internet\u2010of\u2010Things (IoT) devices, the use of social networks, along with the evolution of technology in different domains, lead to a rise in the volume of data generated continuously from multiple systems. Valuable information can be derived from these evolving data streams by applying machine learning. In practice, several critical issues emerge when extracting useful knowledge from these potentially infinite data, mainly because of their evolving nature and high arrival rate which implies an inability to store them entirely. In this work, we provide a comprehensive survey that discusses the research constraints and the current state\u2010of\u2010the\u2010art in this vibrant framework. Moreover, we present an updated overview of the latest contributions proposed in different stream mining tasks, particularly classification, regression, clustering, and frequent patterns. This article is categorized\u00a0\u2026", "num_citations": "11\n", "authors": ["1069"]}
{"title": "Measuring the Shattering coefficient of Decision Tree models\n", "abstract": " In spite of the relevance of Decision Trees (DTs), there is still a disconnection between their theoretical and practical results while selecting models to address specific learning tasks. A particular criterion is provided by the Shattering coefficient, a growth function formulated in the context of the Statistical Learning Theory (SLT), which measures the complexity of the algorithm bias as sample sizes increase. In attempt to establish the basis for a relative theoretical complexity analysis, this paper introduces a method to compute the Shattering coefficient of DT models using recurrence equations. Next, we assess the bias of models provided by DT algorithms while solving practical problems as well as their overall learning bounds in light of the SLT. As the main contribution, our results support other researchers to decide on the most adequate DT models to tackle specific supervised learning tasks.", "num_citations": "11\n", "authors": ["1069"]}
{"title": "Efficient frequent subgraph mining on large streaming graphs\n", "abstract": " We propose an efficient, approximate algorithm to solve the problem of finding frequent subgraphs in large streaming graphs. The graph stream is treated as batches of labeled nodes and edges. Our proposed algorithm finds the set of frequent subgraphs as the graph evolves after each batch. The computational complexity is bounded to linear limits by looking only at the changes made by the most recent batch, and the historical set of frequent subgraphs. As a part of our approach, we also propose a novel sampling algorithm that samples regions of the graph that have been changed by the most recent update to the graph. The performance of the proposed approach is evaluated using five large graph datasets, and our approach is shown to be faster than the state of the art large graph miners while maintaining their accuracy. We also compare our sampling algorithm against a well known sampling algorithm for\u00a0\u2026", "num_citations": "11\n", "authors": ["1069"]}
{"title": "A sketch-based naive bayes algorithms for evolving data streams\n", "abstract": " A well-known learning task in big data stream mining is classification. Extensively studied in the offline setting, in the streaming setting - where data are evolving and even infinite - it is still a challenge. In the offline setting, training needs to store all the data in memory for the learning task; yet, in the streaming setting, this is impossible to do due to the massive amount of data that is generated in real-time. To cope with these resource issues, this paper proposes and analyzes several evolving naive Bayes classification algorithms, based on the well-known count-min sketch, in order to minimize the space needed to store the training data. The proposed algorithms also adapt concept drift approaches, such as ADWIN, to deal with the fact that streaming data may be evolving and change over time. However, handling sparse, very high-dimensional data in such framework is highly challenging. Therefore, we include the\u00a0\u2026", "num_citations": "11\n", "authors": ["1069"]}
{"title": "EXAD: A system for explainable anomaly detection on big data traces\n", "abstract": " Big Data systems are producing huge amounts of data in real-time. Finding anomalies in these systems is becoming increasingly important, since it can help to reduce the number of failures, and improve the mean time of recovery. In this work, we present EXAD, a new prototype system for explainable anomaly detection, in particular for detecting and explaining anomalies in time-series data obtained from traces of Apache Spark jobs. Apache Spark has become the most popular software tool for processing Big Data. The new system contains the most well-known approaches to anomaly detection, and a novel generator of artificial traces, that can help the user to understand the different performances of the different methodologies. In this demo, we will show how this new framework works, and how users can benefit of detecting anomalies in an efficient and fast way when dealing with traces of jobs of Big Data systems.", "num_citations": "11\n", "authors": ["1069"]}
{"title": "Feat: A fairness-enhancing and concept-adapting decision tree classifier\n", "abstract": " Fairness-aware learning is increasingly important in socially-sensitive applications for the sake of achieving optimal and non-discriminative decision-making. Most of the proposed fairness-aware learning algorithms process the data in offline settings and assume that the data is generated by a single concept without drift. Unfortunately, in many real-world applications, data is generated in a streaming fashion and can only be scanned once. In addition, the underlying generation process might also change over time. In this paper, we propose and illustrate an efficient algorithm for mining fair decision trees from discriminatory and continuously evolving data streams. This algorithm, called FEAT (Fairness-Enhancing and concept-Adapting Tree), is based on using the change detector to learn adaptively from non-stationary data streams, that also accounts for fairness. We study FEAT\u2019s properties and\u00a0\u2026", "num_citations": "9\n", "authors": ["1069"]}
{"title": "Delayed labelling evaluation for data streams\n", "abstract": " A large portion of the stream mining studies on classification rely on the availability of true labels immediately after making predictions. This approach is well exemplified by the test-then-train evaluation, where predictions immediately precede true label arrival. However, in many real scenarios, labels arrive with non-negligible latency. This raises the question of how to evaluate classifiers trained in such circumstances. This question is of particular importance when stream mining models are expected to refine their predictions between acquiring instance data and receiving its true label. In this work, we propose a novel evaluation methodology for data streams when verification latency takes place, namely continuous re-evaluation. It is applied to reference data streams and it is used to differentiate between stream mining techniques in terms of their ability to refine predictions based on newly arriving instances. Our\u00a0\u2026", "num_citations": "9\n", "authors": ["1069"]}
{"title": "Correction to: Adaptive random forests for evolving data stream classification\n", "abstract": " The Publisher regrets an error in the spelling of the family name of the sixth author. The correct spelling is Bernhard Pfahringer, as it appears in the author list above.", "num_citations": "9\n", "authors": ["1069"]}
{"title": "Unsupervised real-time detection of BGP anomalies leveraging high-rate and fine-grained telemetry data\n", "abstract": " Recent technology evolution of network equipment allow to continuously stream a wealth of information, pertaining to multiple protocols and layers of the stack, at a very fine spatial-grain and at furthermore high-frequency. Processing this deluge of telemetry data in real-time clearly offers new opportunities for network control and troubleshooting, but also poses serious challenges. In this demonstration, we tackle this challenge by applying streaming machine-learning techniques to the continuous flow of control and data-plane telemetry data, with the purpose of real-time detection of BGP anomalies. In particular, we implement an anomaly detection engine that leverages DenStream, an unsupervised clustering technique, and apply it to telemetry features collected from a large-scale testbed comprising tens of routers traversed by 1 Terabit/s worth of real application traffic.", "num_citations": "9\n", "authors": ["1069"]}
{"title": "Subtree testing and closed tree mining through natural representations\n", "abstract": " Several classical schemes exist to represent trees as strings over a fixed alphabet; these are useful in many algorithmic and conceptual studies. Our previous work has proposed a representation of unranked trees as strings over a countable alphabet, and has shown how this representation is useful for canonizing unordered trees and for mining closed frequent trees, whether ordered or unordered. Here we propose a similar, simpler alternative and adapt some basic algorithmics to it; then we show empirical evidence of the usefulness of this representation for mining frequent closed unordered trees on real-life data.", "num_citations": "9\n", "authors": ["1069"]}
{"title": "An in-depth comparison of group betweenness centrality estimation algorithms\n", "abstract": " One of the important indices defined for a set of vertices in a graph is group betweenness centrality. While in recent years a number of approximate algorithms have been proposed to estimate this index, there is no comprehensive and in-depth analysis and comparison of these algorithms in the literature. In this paper, we first present a generic algorithm that is used to express different approximate algorithms in terms of probability distributions. Using this generic algorithm, we show that interestingly existing methods have the same theoretical accuracy. Then, we present an extension of distance-based sampling to group betweenness centrality, which is based on a new notion of distance between a single vertex and a set of vertices. In the end, to empirically evaluate efficiency and accuracy of different algorithms, we perform experiments over several real-world networks. Our extensive experiments reveal that those\u00a0\u2026", "num_citations": "8\n", "authors": ["1069"]}
{"title": "DyBED: An efficient algorithm for updating betweenness centrality in directed dynamic graphs\n", "abstract": " An important index widely used to analyze social and information networks is betweenness centrality. In this paper, given a dynamic and directed graph G and a vertex r in G, we present the DyBED algorithm that updates the (approximate) betweenness centrality of r, when an update operation (vertex/edge insertion/deletion) occurs in G. Our algorithm first during pre-processing computes two subsets of the vertex set of G, called \u03a0T(r) and \u03a0T(r). The Cartesian product of these two sets defines the sample space of our algorithm. In other words, each sample is a pair, whose first element belongs to \u03a0T(r) and second element belongs to \u03a0T(r). Then after each update operation, DyBED updates the sets \u03a0T(r) and \u03a0T(r), the sampled pairs, the information stored for each sample and accordingly, the betweenness centrality of r. We theoretically and empirically evaluate DyBED and show that it yields significant improvement\u00a0\u2026", "num_citations": "8\n", "authors": ["1069"]}
{"title": "Multi-label classification\n", "abstract": " BackgroundMulti-label classifications come naturally. For example, a movie can be assigned to both genres \u201cAction\u201d and \u201cComedy.\u201d In the physical world, this often involves physical duplication, for example, a video store has to put copies of such a movie in two different sections--\u201cAction\u201d and \u201cComedy\u201d--or create a single-category (\u201cAction-Comedy\u201d). In a digital environment there are no such restrictions, and it is likely for this reason that there has been a rapid rise in the popularity of the multi-label concept in everyday applications. Consider how the once-familiar \u2018folder\u2019metaphor is being replaced by the label/tag term in many everyday applications: email, picture, document, and media collections, and so forth.", "num_citations": "8\n", "authors": ["1069"]}
{"title": "Ensembles of sparse multinomial classifiers for scalable text classification\n", "abstract": " Machine learning techniques face new challenges in scalability to large-scale tasks. Many of the existing algorithms are unable to scale to potentially millions of features and structured classes encountered in web-scale datasets such as Wikipedia. The third Large Scale Hierarchical Text Classification evaluation (LSHTC3) evaluated systems for multi-label hierarchical categorization of Wikipedia articles. In this paper we present a broad overview of our system in the evaluation, performing among the top systems in the challenge. We describe the several new modeling ideas we used that make text classification systems both more effective and scalable. These are: reduction of inference time complexity for probabilistic classifiers using inverted indices, classifier modifications optimized with direct search algorithms, ensembles of diverse multi-label classifiers and a novel feature-regression based method for scalable ensemble combination.", "num_citations": "8\n", "authors": ["1069"]}
{"title": "Mining Implications from Lattices of Closed Trees.\n", "abstract": " We propose a way of extracting high-confidence association rules from datasets consisting of unlabeled trees. The antecedents are obtained through a computation akin to a hypergraph transversal, whereas the consequents follow from an application of the closure operators on unlabeled trees developed in previous recent works of the authors. We discuss in more detail the case of rules that always hold, independently of the dataset, since these are more complex than in itemsets due to the fact that we are no longer working on a lattice.", "num_citations": "8\n", "authors": ["1069"]}
{"title": "Sampling informative patterns from large single networks\n", "abstract": " The set of all frequent patterns that are extracted from a single network can be huge. A technique recently proposed for obtaining a compact, informative and useful set of patterns is output sampling, where a small set of frequent patterns is randomly chosen. However, existing output sampling algorithms work only in the transactional setting, where the database consists of a collection of relatively small graphs. In this paper, first we extend the output sampling framework to the single network setting where the database is a large single graph, counting supports of patterns is more complicated, and frequent patterns might be sampled based on any arbitrary target distribution. Then, we propose sampling techniques that are based on more interesting/informative measures or those that are specific to large single networks, such as product of the pattern size with its support, network compressibility, and pattern density\u00a0\u2026", "num_citations": "7\n", "authors": ["1069"]}
{"title": "Towards automated configuration of stream clustering algorithms\n", "abstract": " Clustering is an important technique in data analysis which can reveal hidden patterns and unknown relationships in the data. A common problem in clustering is the proper choice of parameter settings. To tackle this, automated algorithm configuration is available which can automatically find the best parameter settings. In practice, however, many of our today\u2019s data sources are data streams due to the widespread deployment of sensors, the internet-of-things or (social) media. Stream clustering aims to tackle this challenge by identifying, tracking and updating clusters over time. Unfortunately, none of the existing approaches for automated algorithm configuration are directly applicable to the streaming scenario. In this paper, we explore the possibility of automated algorithm configuration for stream clustering algorithms using an ensemble of different configurations. In first experiments, we demonstrate that our approach is able to automatically find superior configurations and refine them over time.", "num_citations": "7\n", "authors": ["1069"]}
{"title": "Ubiquitous artificial intelligence and dynamic data streams\n", "abstract": " Artificial Intelligence is leading to ubiquitous sources of Big Data arriving at high-velocity and in real-time. To effectively deal with it, we need to be able to adapt to changes in the distribution of the data being produced, and we need to do it using a minimum amount of time and memory. In this paper, we detail modern applications falling into this context, and discuss some state-of-the-art methodologies in mining data streams in real-time, and the open source tools that are available to do machine learning/data mining in real-time for this challenging setting.", "num_citations": "7\n", "authors": ["1069"]}
{"title": "Machine Learning for Data Streams\n", "abstract": " ML for DS-CiscoML for DS-Cisco Page 1 Machine Learning for Data Streams Albert Bifet (@abifet) Cisco-Ecole Polytechnique Symposium 2018, 10 April 2018 Page 2 Machine Learning \u2022 Machine learning is a type of artificial intelligence (AI) that provides computers with the ability to learn without being explicitly programmed. \u2022 Machine learning focuses on the development of computer programs that can teach themselves to grow and change when exposed to new data. 2 Page 3 Machine Learning Imperative Programming The programmer specifies an explicit sequences of steps to follow to produce a result. Decision: +, - Data Page 4 Machine Learning Machine Learning Algorithm Decision: +, - Data Page 5 AI Systems \u2022 According to Nikola Kasabov, AI systems should exhibit the following characteristics: \u2022 Accommodate new problem solving rules incrementally \u2022 Adapt online and in real time \u2022 Are able to analyze \u2026", "num_citations": "7\n", "authors": ["1069"]}
{"title": "Streaming data mining with massive online analytics (MOA)\n", "abstract": " Fast Big Data is being produced at high-velocity in real-time. To effectively deal with this type of streaming data produced in real time, we need to be able to adapt to changes on the distribution of the data being produced, and we need to do it using the minimum amount of time and memory. The Internet of Things (IoT) is a good example and motivation of this type of streaming data produced in real time. Massive Online Analytics (MOA) is a software environment for implementing algorithms and running experiments for online learning from evolving data streams. MOA is designed to deal with the challenging problem of scaling up the implementation of state of the art algorithms to real world dataset sizes. MOA includes classification and clustering methods. It contains collection of offline and online methods as well as tools for evaluation. MOA supports bi-directional interaction with WEKA, the Waikato Environment for\u00a0\u2026", "num_citations": "7\n", "authors": ["1069"]}
{"title": "Survey on feature transformation techniques for data streams\n", "abstract": " Mining high-dimensional data streams poses a fundamental challenge to machine learning as the presence of high numbers of attributes can remarkably degrade any mining task\u2019s performance. In the past several years, dimension reduction (DR) approaches have been successfully applied for different purposes (eg, visualization). Due to their highcomputational costs and numerous passes over large data, these approaches pose a hindrance when processing infinite data streams that are potentially high-dimensional. The latter increases the resource-usage of algorithms that could suffer from the curse of dimensionality. To cope with these issues, some techniques for incremental DR have been proposed. In this paper, we provide a survey on reduction approaches designed to handle data streams and highlight the key benefits of using these approaches for stream mining algorithms.", "num_citations": "6\n", "authors": ["1069"]}
{"title": "On ensemble techniques for data stream regression\n", "abstract": " An ensemble of learners tends to exceed the predictive performance of individual learners. This approach has been explored for both batch and online learning. Ensembles methods applied to data stream classification were thoroughly investigated over the years, while their regression counterparts received less attention in comparison. In this work, we discuss and analyze several techniques for generating, aggregating, and updating ensembles of regressors for evolving data streams. We investigate the impact of different strategies for inducing diversity into the ensemble by randomizing the input data (resampling, random subspaces and random patches). On top of that, we devote particular attention to techniques that adapt the ensemble model in response to concept drifts, including adaptive window approaches, fixed periodical resets and randomly determined windows. Extensive empirical experiments show\u00a0\u2026", "num_citations": "6\n", "authors": ["1069"]}
{"title": "Predicting attributes and friends of mobile users from AP-Trajectories\n", "abstract": " Exploring the demographic attributes and social networks of Internet users is widely employed by many applications, such as recommendation systems. The popularity of mobile devices (notably smartphones) and location-based Internet services (e.g., Google Maps) facilitates the collection of users\u2019 locations over time. There have been recent efforts to predict users\u2019 attributes (e.g., age and gender) from this data, and location-based social networks such as Foursquare and Gowalla are based on using the rich location context knowledge of points of interest (e.g., the name, type and description of restaurants and hotels) where users check-in online. However, little attention has been paid to inferring the attributes and social networks of mobile device users based on their spatiotemporal trajectories where there is little or no location context knowledge. In this paper, we collect logs of thousands of mobile devices\u00a0\u2026", "num_citations": "6\n", "authors": ["1069"]}
{"title": "Predicting over-indebtedness on batch and streaming data\n", "abstract": " Detecting over-indebtedness, the difficulties meeting household payment commitments, poses multiple Big Data challenges for banking institutions. We present a novel data-driven framework for predicting over-indebtedness on realworld data. A warning mechanism that generates predictions 6 months ahead, improving the chances of financial recovery. This framework is based on the combination of feature selection and supervised learning techniques, and uses data balancing for fine-tuning the predictive models. We propose two versions of the framework based on state-of-the-art batch and streaming learning techniques. To the best of our knowledge, the proposed framework is the first to cast over-indebtedness prediction as a stream learning problem. The appeal of stream learning rises from the large amount of data continuously generated, and the fact that batch models become obsolete over time as financial\u00a0\u2026", "num_citations": "6\n", "authors": ["1069"]}
{"title": "Droplet ensemble learning on drifting data streams\n", "abstract": " Ensemble learning methods for evolving data streams are extremely powerful learning methods since they combine the predictions of a set of classifiers, to improve the performance of the best single classifier inside the ensemble. In this paper we introduce the Droplet Ensemble Algorithm (DEA), a new method for learning on data streams subject to concept drifts which combines ensemble and instance based learning. Contrarily to state of the art ensemble methods which select the base learners according to their performances on recent observations, DEA dynamically selects the subset of base learners which is the best suited for the region of the feature space where the latest observation was received. Experiments on 25 datasets (most of which being commonly used as benchmark in the literature) reproducing different type of drifts show that this new method achieves excellent results on accuracy and\u00a0\u2026", "num_citations": "6\n", "authors": ["1069"]}
{"title": "Real-Time Big Data Stream Analytics.\n", "abstract": " Big Data is a new term used to identify datasets that we cannot manage with current methodologies or data mining software tools due to their large size and complexity. Big Data mining is the capability of extracting useful information from these large datasets or streams of data. New mining techniques are necessary due to the volume, variability, and velocity, of such data. MOA is a software framework with classification, regression, and frequent pattern methods, and the new APACHE SAMOA is a distributed streaming software for mining data streams.", "num_citations": "6\n", "authors": ["1069"]}
{"title": "A Survey on Spatio-temporal Data Analytics Systems\n", "abstract": " Due to the surge of spatio-temporal data volume, the popularity of location-based services and applications, and the importance of extracted knowledge from spatio-temporal data to solve a wide range of real-world problems, a plethora of research and development work has been done in the area of spatial and spatio-temporal data analytics in the past decade. The main goal of existing works was to develop algorithms and technologies to capture, store, manage, analyze, and visualize spatial or spatio-temporal data. The researchers have contributed either by adding spatio-temporal support with existing systems, by developing a new system from scratch for processing spatio-temporal data, or by implementing algorithms for mining spatio-temporal data. The existing ecosystem of spatial and spatio-temporal data analytics can be categorized into three groups, (1) spatial databases (SQL and NoSQL), (2) big spatio-temporal data processing infrastructures, and (3) programming languages and software tools for processing spatio-temporal data. Since existing surveys mostly investigated big data infrastructures for processing spatial data, this survey has explored the whole ecosystem of spatial and spatio-temporal analytics along with an up-to-date review of big spatial data processing systems. This survey also portrays the importance and future of spatial and spatio-temporal data analytics.", "num_citations": "5\n", "authors": ["1069"]}
{"title": "Adaptive Algorithms for Estimating Betweenness and k-path Centralities\n", "abstract": " Betweenness centrality and k-path centrality are two important indices that are widely used to analyze social, technological and information networks. In the current paper, first given a directed network G and a vertex , we present a novel adaptive algorithm for estimating betweenness score of r. Our algorithm first computes two subsets of the vertex set of G, called $\\mathcalRF (r) $ and $\\mathcalRT (r) $. They define the sample spaces of the start-points and the end-points of the samples. Then, it adaptively samples from $\\mathcalRF (r) $ and $\\mathcalRT (r) $ and stops as soon as some condition is satisfied. The stopping condition depends on the samples met so far, $|\\mathcalRF (r)| $ and $|\\mathcalRT (r)| $. We show that compared to the well-known existing algorithms, our algorithm gives a better -approximation. Then, we propose a novel algorithm for estimating k-path centrality of r. Our algorithm is\u00a0\u2026", "num_citations": "5\n", "authors": ["1069"]}
{"title": "Introduction to the special issue on big data, IoT Streams and Heterogeneous Source Mining\n", "abstract": " Recent years have witnessed a dramatic increase in our ability to collect data from sensors and devices, across different formats, from connected applications and enormous dynamic networks including social networks, as well as many other sources. This data flood has outpaced our capability to process, analyze, store and understand these datasets using traditional methods. In all these areas, we are facing significant challenges in leveraging the vast amount of data, and dealing with its speed of arrival and its heterogeneous and evolving nature. This includes challenges in system capabilities, storage and processing, algorithmic design and business models. Approaches for dealing with data in a streaming fashion are thus becoming increasingly relevant in many tasks, including mining and analysis, data representation and visualization, incremental learning and anomaly detection. We are pleased to introduce\u00a0\u2026", "num_citations": "5\n", "authors": ["1069"]}
{"title": "IDSA-IoT: An Intrusion Detection System Architecture for IoT Networks\n", "abstract": " The Internet of Things (IoT) allows large amounts and variety of devices to connect, interact and exchange data. The IoT network creates numerous opportunities for novel attacks that can compromise information and systems integrity. Intrusion detection systems have been studied over two decades, mostly employing traditional data mining and machine learning techniques that require an offline phase for model training on large amounts of data. This paper presents three data stream novelty detection techniques applied to the intrusion detection problem and proposes IDSA-IoT, a novel implementation architecture, which combines the use of resources at the edge of the network and a public cloud. After an extensive empirical evaluation, results show that it is possible to identify new attack patterns soon after their emergence and to adapt the models in an efficient way.", "num_citations": "5\n", "authors": ["1069"]}
{"title": "confstream: Automated algorithm selection and configuration of stream clustering algorithms\n", "abstract": " Machine learning has become one of the most important tools in data analysis. However, selecting the most appropriate machine learning algorithm and tuning its hyperparameters to their optimal values remains a difficult task. This is even more difficult for streaming applications where automated approaches are often not available to help during algorithm selection and configuration. This paper proposes the first approach for automated algorithm selection and configuration of stream clustering algorithms. We train an ensemble of different stream clustering algorithms and configurations in parallel and use the best performing configuration to obtain a clustering solution. By drawing new configurations from better performing ones, we are able to improve the ensemble performance over time. In large experiments on real and artificial data we show how our ensemble approach can improve upon default configurations\u00a0\u2026", "num_citations": "4\n", "authors": ["1069"]}
{"title": "Efficient Batch-Incremental Classification Using UMAP for Evolving Data Streams.\n", "abstract": " Learning from potentially infinite and high-dimensional data streams poses significant challenges in the classification task. For instance, k-Nearest Neighbors (kNN) is one of the most often used algorithms in the data stream mining area that proved to be very resourceintensive when dealing with high-dimensional spaces. Uniform Manifold Approximation and Projection (UMAP) is a novel manifold technique and one of the most promising dimension reduction and visualization techniques in the non-streaming setting because of its high performance in comparison with competitors. However, there is no version of UMAP that copes with the challenging context of streams. To overcome these restrictions, we propose a batch-incremental approach that pre-processes data streams using UMAP, by producing successive embeddings on a stream of disjoint batches in order to support an incremental kNN classification. Experiments conducted on publicly available synthetic and realworld datasets demonstrate the substantial gains that can be achieved with our proposal compared to state-of-the-art techniques.", "num_citations": "4\n", "authors": ["1069"]}
{"title": "Arbitrated dynamic ensemble with abstaining for time-series forecasting on data streams\n", "abstract": " A well-known challenge in mining temporal data streams is their dynamic nature where changes and recurrent concepts are likely to happen. Ensemble methods are powerful techniques to improve overall accuracy and tackle the aforementioned challenges by combining several classifiers. Dynamic Ensemble Selection allows selecting, on the fly, the most accurate classifiers only to contribute to the final output. This is motivated by the assumption that components of the ensemble have different degrees of expertise on different sub-spaces of the data. Existing Dynamic Ensemble Selection methods are tailored to batch learning or classification tasks but less suited to stream mining and forecasting tasks. In this paper, we propose a new Arbitrated Dynamic Ensemble Selection technique STREAMING-ADE for time-series forecasting on data streams that uses meta-learning to monitor the predictive power of ensemble\u00a0\u2026", "num_citations": "4\n", "authors": ["1069"]}
{"title": "Discriminative distance-based network indices with application to link prediction\n", "abstract": " In distance-based network indices, the distance between two vertices is measured by the length of shortest paths between them. A shortcoming of this measure is that when it is used in real-world networks, a huge number of vertices may have exactly the same closeness/eccentricity scores. This restricts the applicability of these indices as they cannot distinguish vertices. Furthermore, in many applications, the distance between two vertices not only depends on the length of shortest paths but also on the number of shortest paths between them. In this paper, first we develop a new distance measure, proportional to the length of shortest paths and inversely proportional to the number of shortest paths, that yields discriminative distance-based centrality indices. We present exact and randomized algorithms for computation of the proposed discriminative indices. Then, by performing extensive experiments, we first\u00a0\u2026", "num_citations": "4\n", "authors": ["1069"]}
{"title": "2 Big Data Stream Mining\n", "abstract": " In this chapter we give a gentle introduction to some basic methods for learning from data streams. In the next chapter, we show a practical example of how to use MOA with some of the methods briefly presented in this chapter. These and other methods are presented in more detail in part II of this book.", "num_citations": "4\n", "authors": ["1069"]}
{"title": "Echo state hoeffding tree learning\n", "abstract": " Nowadays, real-time classification of Big Data streams is becoming essential in a variety of application domains. While decision trees are powerful and easy-to-deploy approaches for accurate and fast learning from data streams, they are unable to capture the strong temporal dependences typically present in the input data. Recurrent Neural Networks are an alternative solution that include an internal memory to capture these temporal dependences; however their training is computationally very expensive and with slow convergence, requiring a large number of hyper-parameters to tune. Reservoir Computing was proposed to reduce the computation requirements of the training phase but still include a feed-forward layer which requires a large number of parameters to tune. In this work we propose a novel architecture for real-time classification based on the combination of a Reservoir and a decision tree. This combination reduces the number of hyper-parameters while still maintaining the good temporal properties of recurrent neural networks. The capabilities of the proposed architecture to learn some typical string-based functions with strong temporal dependences are evaluated in the paper. We show how the new architecture is able to incrementally learn these functions in real-time with fast adaptation to unknown sequences. And we study the influence of the reduced number of hyper-parameters in the behaviour of the proposed solution.", "num_citations": "4\n", "authors": ["1069"]}
{"title": "Tutorial 1. Introduction to MOA {M} assive {O} nline {A} nalysis\n", "abstract": " This tutorial is a basic introduction to MOA. Massive Online Analysis (MOA) is a software environment for implementing algorithms and running experiments for online learning from evolving data streams. We suppose that MOA is installed in your system. Start a graphical user interface for configuring and running tasks with the command: java-cp moa. jar-javaagent: sizeofag. jar moa. gui. GUI", "num_citations": "4\n", "authors": ["1069"]}
{"title": "Closed and maximal tree mining using natural representations\n", "abstract": " Mining frequent trees is becoming an important task, with broad applications including chemical informatics, computer vision, text retrieval, bioinformatics, and Web analysis. Many link-based structures may be studied formally by means of unordered trees.Closure-based mining on purely relational data, that is, itemset mining, is, by now, well-established, and there are interesting algorithmic developments. Sharing some of the attractive features of frequency-based summarization of subsets, it offers an alternative view with both downsides and advantages; among the latter, there are the facts that, first, by imposing closure, the number of frequent sets is heavily reduced and, second, the possibility appears of developing a mathematical foundation that connects closure-based mining with lattice-theoretic approaches like Formal Concept Analysis.", "num_citations": "4\n", "authors": ["1069"]}
{"title": "vertTIRP: Robust and efficient vertical frequent time interval-related pattern mining\n", "abstract": " Time-interval-related pattern (TIRP) mining algorithms find patterns such as \u201cA starts B\u201d or \u201cA overlaps B\u201d. The discovery of TIRPs is computationally highly demanding. In this work, we introduce a new efficient algorithm for mining TIRPs, called vertTIRP which combines an efficient representation of these patterns, using their temporal transitivity properties to manage them, with a pairing strategy that sorts the temporal relations to be tested, in order to speed up the mining process. Moreover, this work presents a robust definition of the temporal relations that eliminates the ambiguities with other relations when taking into account the uncertainty in the start and end time of the events (epsilon-based approach), and includes two constraints that enable the user to better express the types of TIRPs to be learnt. An experimental evaluation of the method was performed with both synthetic and real datasets, and the results\u00a0\u2026", "num_citations": "3\n", "authors": ["1069"]}
{"title": "Performance measures for evolving predictions under delayed labelling classification\n", "abstract": " For many streaming classification tasks, the ground truth labels become available with a non-negligible latency. Given this delayed labelling setting, after the instance data arrives and before its true label is known, the online classifier model may change. Hence, the initial prediction can be replaced with additional periodic predictions gradually produced before the true label becomes available. The quality of these predictions may largely vary. Thus, the question arises of how to summarise the performance of these models when multiple predictions for a single instance are made due to delayed labels.In this study, we aim to provide intuitive performance measures summarising the performance of multiple predictions made for individual instances before their true labels arrive. Particular attention is paid to the fact that under the delayed label setting, the emphasis placed on the quality of initial predictions can vary\u00a0\u2026", "num_citations": "3\n", "authors": ["1069"]}
{"title": "Semi-supervised Learning over Streaming Data using MOA\n", "abstract": " Machine learning algorithms for data streams usually suppose that all data examples available for learning are strictly labeled. Unfortunately, in real-world scenarios, data examples are not always labeled. Semi-supervised learning is a challenging task to learn using labeled and unlabeled data at the same time. It is especially relevant in the context of data streams, where the data is generated in real-time, and the labels may be missing due to various factors (e.g., network delay, errors during the communication between sensors, expensive labeling process, and others). In this paper, we present two novel approaches to handle missing labels for classification learning in data streams, namely cluster-and-label and self-training. We discuss the strengths and weaknesses of each solution to establish a baseline to evaluate semi-supervised learning techniques in data streams. These methods are implemented inside\u00a0\u2026", "num_citations": "3\n", "authors": ["1069"]}
{"title": "ORSUM 2019 2nd workshop on online recommender systems and user modeling\n", "abstract": " The ever-growing nature of user generated data in online systems poses obvious challenges on how we process such data. Typically, this issue is regarded as a scalability problem and has been mainly addressed with distributed algorithms able to train on massive amounts of data in short time windows. However, data is inevitably adding up at high speeds. Eventually one needs to discard or archive some of it. Moreover, the dynamic nature of data in user modeling and recommender systems, such as change of user preferences, and the continuous introduction of new users and items make it increasingly difficult to maintain up-to-date, accurate recommendation models. The objective of this workshop is to bring together researchers and practitioners interested in incremental and adaptive approaches to stream-based user modeling, recommendation and personalization, including algorithms, evaluation issues\u00a0\u2026", "num_citations": "3\n", "authors": ["1069"]}
{"title": "Recent trends in streaming data analysis, concept drift and analysis of dynamic data sets.\n", "abstract": " Today, many data are not any longer static but occur as dynamic data streams with high velocity, variability and volume. This leads to new challenges to be addressed by novel or adapted algorithms. In this tutorial we provide an introduction into the field of streaming data analysis summarizing its major characteristics and highlighting important research directions in the analysis of dynamic data.", "num_citations": "3\n", "authors": ["1069"]}
{"title": "Learning fast and slow: A unified batch/stream framework\n", "abstract": " Data ubiquity highlights the need of efficient and adaptable data-driven solutions. In this paper, we present FAST AND SLOW LEARNING (FSL), a novel unified framework that sheds light on the symbiosis between batch and stream learning. FSL works by employing Fast (stream) and Slow (batch) Learners, emulating the mechanisms used by humans to make decisions. We showcase the applicability of FSL on the task of classification by introducing the FAST AND SLOW CLASSIFIER (FSC). A Fast Learner provides predictions on the spot, continuously updating its model and adapting to changes in the data. On the other hand, the Slow Learner provides predictions considering a wider spectrum of seen data, requiring more time and data to create complex models. Once that enough data has been collected, FSC trains the Slow Learner and starts tracking the performance of both learners. A drift detection\u00a0\u2026", "num_citations": "3\n", "authors": ["1069"]}
{"title": "Mining Internet of Things (IoT) Big Data Streams.\n", "abstract": " Big Data and the Internet of Things (IoT) have the potential to fundamentally shift the way we interact with our surroundings. The challenge of deriving insights from the Internet of Things (IoT) has been recognized as one of the most exciting and key opportunities for both academia and industry. Advanced analysis of big data streams from sensors and devices is bound to become a key area of data mining research as the number of applications requiring such processing increases. Dealing with the evolution over time of such data streams, ie, with concepts that drift or change completely, is one of the core issues in stream mining. Dealing with this setting, MOA is a software framework with classification, regression, and frequent pattern methods, and the new APACHE SAMOA is a distributed streaming software for mining IoT data streams.", "num_citations": "3\n", "authors": ["1069"]}
{"title": "An analysis of factors used in search engine ranking\n", "abstract": " This paper investigates the influence of different page features on the ranking of search engine results. We use Google (via its API) as our testbed and analyze the result rankings for several queries of different categories using statistical methods. We reformulate the problem of learning the underlying, hidden scores as a binary classification problem. To this problem we then apply both linear and non-linear methods. In all cases, we split the data into a training set and a test set to obtain a meaningful, unbiased estimator for the quality of our predictor. Although our results clearly show that the scoring function cannot be approximated well using only the observed features, we do obtain many interesting insights along the way and discuss ways of obtaining a better estimate and main limitations in trying to do so.", "num_citations": "3\n", "authors": ["1069"]}
{"title": "Analyzing and repairing concept drift adaptation in data stream classification\n", "abstract": " Data collected over time often exhibit changes in distribution, or concept drift, caused by changes in factors relevant to the classification task, eg weather conditions. Incorporating all relevant factors into the model may be able to capture these changes, however, this is usually not practical. Data stream based methods, which instead explicitly detect concept drift, have been shown to retain performance under unknown changing conditions. These methods adapt to concept drift by training a model to classify each distinct data distribution. However, we hypothesize that existing methods do not robustly handle real-world tasks, leading to adaptation errors where context is misidentified. Adaptation errors may cause a system to use a model which does not fit the current data, reducing performance. We propose a novel repair algorithm to identify and correct errors in concept drift adaptation. Evaluation on synthetic data\u00a0\u2026", "num_citations": "2\n", "authors": ["1069"]}
{"title": "Improving parallel performance of ensemble learners for streaming data through data locality with mini-batching\n", "abstract": " Machine Learning techniques have been employed in virtually all domains in the past few years. New applications demand the ability to cope with dynamic environments like data streams with transient behavior. Such environments present new requirements like incrementally process incoming data instances in a single pass, under both memory and time constraints. Furthermore, prediction models often need to adapt to concept drifts observed in non-stationary data streams. Ensemble learning comprises a class of stream mining algorithms that achieved remarkable prediction performance in this scenario. Implemented as a set of (several) individual component classifiers whose predictions are combined to predict new incoming instances, ensembles are naturally amendable for task parallelism. Despite its relevance, an efficient implementation of ensemble algorithms is still challenging. For example, dynamic data\u00a0\u2026", "num_citations": "2\n", "authors": ["1069"]}
{"title": "AutoML for Stream k-Nearest Neighbors Classification\n", "abstract": " The last few decades have witnessed a significant evolution of technology in different domains, changing the way the world operates, which leads to an overwhelming amount of data generated in an open-ended way as streams. Over the past years, we observed the development of several machine learning algorithms to process big data streams. However, the accuracy of these algorithms is very sensitive to their hyper-parameters, which requires expertise and extensive trials to tune. Another relevant aspect is the high-dimensionality of data, which can causes degradation to computational performance. To cope with these issues, this paper proposes a stream k-nearest neighbors (kNN) algorithm that applies an internal dimension reduction to the stream in order to reduce the resource usage and uses an automatic monitoring system that tunes dynamically the configuration of the kNN algorithm and the output\u00a0\u2026", "num_citations": "2\n", "authors": ["1069"]}
{"title": "Unsupervised concept drift detection using a student\u2013teacher approach\n", "abstract": " Concept drift detection is a crucial task in data stream evolving environments. Most of the state of the art approaches designed to tackle this problem monitor the loss of predictive models. Accordingly, an alarm is launched when the loss increases significantly, which triggers some adaptation mechanism (e.g. retrain the model). However, this modus operandi falls short in many real-world scenarios, where the true labels are not readily available to compute the loss. These often take up\u00a0to several weeks to be available. In this context, there is increasing attention to approaches that perform concept drift detection in an unsupervised manner, i.e., without access to the true labels. We propose a novel approach to unsupervised concept drift detection, which is based on a student-teacher learning paradigm. Essentially, we create an auxiliary model (student) to mimic the behaviour of the main model (teacher). At\u00a0\u2026", "num_citations": "2\n", "authors": ["1069"]}
{"title": "ORSUM-Workshop on Online Recommender Systems and User Modeling\n", "abstract": " Modern online web-based systems continuously generate data at very fast rates. This continuous flow of data encompasses web content\u2013eg posts, news, products, comments\u2013, but also user feedback\u2013eg ratings, views, reads, clicks, thumbs up\u2013, as well as context information\u2013device used, geographic info, social network, current user activity, weather. This is potentially overwhelming for systems and algorithms design to train in offline batches, given the continuous and potentially fast change of content, context and user preferences. Therefore it is important to investigate online methods to be able to transparently adapt to the inherent dynamics of online systems. Incremental models that learn from data streams are gaining attention in the recommender systems community, given their natural ability to deal with data generated in dynamic, complex environments. User modeling and personalization can particularly\u00a0\u2026", "num_citations": "2\n", "authors": ["1069"]}
{"title": "Compressed k-nearest neighbors ensembles for evolving data streams\n", "abstract": " The unbounded and multidimensional nature, the evolution of data distributions with time, and the requirement of singlepass algorithms comprise the main challenges of data stream classification, which makes it impossible to infer learning models in the same manner as for batch scenarios. Data dimensionality reduction arises as a key factor to transform and select only the most relevant features from those streams in order to reduce algorithm space and time demands. In that context, Compressed Sensing (CS) encodes an input signal into lower-dimensional space, guaranteeing its reconstruction up to some distortion factor. This paper employs CS on data streams as a pre-processing step to support a k-Nearest Neighbors (kNN) classification algorithm, one of the most often used algorithms in the data stream mining area-all this while ensuring the key properties of CS hold. Based on topological properties, we show that our classification algorithm also preserves the neighborhood (withing an factor) of kNN after reducing the stream dimensionality with CS. As a consequence, end-users can set an acceptable error margin while performing such projections for kNN. For further improvements, we incorporate this method into an ensemble classifier, Leveraging Bagging, by combining a set of different CS matrices which increases the diversity inside the ensemble. An extensive set of experiments is performed on various datasets, and the results were compared against those yielded by current state-of-the-art approaches, confirming the good performance of our approaches.", "num_citations": "2\n", "authors": ["1069"]}
{"title": "Continuous Analytics of Web Streams\n", "abstract": " This half-day tutorial provides a comprehensive introduction to web stream processing, including the fundamental stream reasoning concepts, as well as an introduction to practical implementations and how to use them in concrete web applications. To this extent, we intend to (1) survey existing research outcomes from Stream Reasoning/RDF Stream Processing that arise in querying, reasoning on and learning from a variety of highly dynamic data,(2) introduce deductive and inductive stream reasoning techniques as powerful tools to use when addressing a data-centric problem characterized both by variety and velocity,(3) present a relevant use-case, which requires to address data velocity and variety simultaneously on the web, and guide the participants in developing a web stream processing application.", "num_citations": "2\n", "authors": ["1069"]}
{"title": "Scalable Model-Based Cascaded Imputation of Missing Data\n", "abstract": " Missing data is a common trait of real-world data that can negatively impact interpretability. In this paper, we present Cascade Imputation (CIM), an effective and scalable technique for automatic imputation of missing data. CIM is not restrictive on the characteristics of the data set, providing support for: Missing At Random and Missing Completely At Random data, numerical and nominal attributes, and large data sets including highly dimensional data sets. We compare CIM against well-established imputation techniques over a variety of data sets under multiple test configurations to measure the impact of imputation on the classification problem. Test results show that CIM outperforms other imputation methods over multiple test conditions. Additionally, we identify optimal performance and failure conditions for popular imputation techniques.", "num_citations": "2\n", "authors": ["1069"]}
{"title": "Deferral classification of evolving temporal dependent data streams\n", "abstract": " Data streams generated in real-time can be strongly temporally dependent. In this case, standard techniques where we suppose that class labels are not correlated may produce sub-optimal performance because the assumption is incorrect. To deal with this problem, we present in this paper a new algorithm to classify temporally correlated data based on deferral learning. This approach is suitable for learning over time-varying streams. We show how simple classifiers such as Naive Bayes can boost their performance using this new meta-learning methodology. We give an empirical validation of our new algorithm over several real and artificial datasets.", "num_citations": "2\n", "authors": ["1069"]}
{"title": "Mining big data streams with apache SAMOA\n", "abstract": " In this talk, we present Apache SAMOA, an open-source platform for mining big data streams with Apache Flink, Storm and Samza. Real time analytics is becoming the fastest and most efficient way to obtain useful knowledge from what is happening now, allowing organizations to react quickly when problems appear or to detect new trends helping to improve their performance. Apache SAMOA includes algorithms for the most common machine learning tasks such as classification and clustering. It provides a pluggable architecture that allows it to run on Apache Flink, but also with other several distributed stream processing engines such as Storm and Samza.", "num_citations": "2\n", "authors": ["1069"]}
{"title": "Data stream classification using random feature functions and novel method combinations\n", "abstract": " Data streams are being generated in a faster, bigger, and more commonplace manner. In this scenario, Hoeffding Trees are an established method for classification. Several extensions exist, including high-performing ensemble setups such as online and leveraging bagging. Also, k-nearest neighbours is a popular choice, with most extensions dealing with the inherent performance limitations over a potentially-infinite stream. At the same time, gradient descent methods are becoming increasingly popular, owing to the proliferation of interest and successes in deep learning. Although deep neural networks can learn incrementally, they have so far proved too sensitive to hyperparameter options and initial conditions to be considered an effective 'off-the-shelf' data streams solution. In this work, we look at combinations of Hoeffding trees, nearest neighbour, and gradient descent methods with a streaming preprocessing\u00a0\u2026", "num_citations": "2\n", "authors": ["1069"]}
{"title": "Improving the performance of bagging ensembles for data streams through mini-batching\n", "abstract": " Often, machine learning applications have to cope with dynamic environments where data are collected in the form of continuous data streams with potentially infinite length and transient behavior. Compared to traditional (batch) data mining, stream processing algorithms have additional requirements regarding computational resources and adaptability to data evolution. They must process instances incrementally because the data\u2019s continuous flow prohibits storing data for multiple passes. Ensemble learning achieved remarkable predictive performance in this scenario. Implemented as a set of (several) individual classifiers, ensembles are naturally amendable for task parallelism. However, the incremental learning and dynamic data structures used to capture the concept drift increase the cache misses and hinder the benefit of parallelism. This paper proposes a mini-batching strategy that can improve memory\u00a0\u2026", "num_citations": "1\n", "authors": ["1069"]}
{"title": "Studying and Exploiting the Relationship Between Model Accuracy and Explanation Quality\n", "abstract": " Many explanation methods have been proposed to reveal insights about the internal procedures of black-box models like deep neural networks. Although these methods are able to generate explanations for individual predictions, little research has been conducted to investigate the relationship of model accuracy and explanation quality, or how to use explanations to improve model performance. In this paper, we evaluate explanations using a metric based on area under the ROC curve (AUC), treating expert-provided image annotations as ground-truth explanations, and quantify the correlation between model accuracy and explanation quality when performing image classifications with deep neural networks. The experiments are conducted using two image datasets: the CUB-200-2011 dataset and a Kahikatea dataset that we publish with this paper. For each dataset, we compare and evaluate seven\u00a0\u2026", "num_citations": "1\n", "authors": ["1069"]}
{"title": "Confident Interpretations of Black Box Classifiers\n", "abstract": " Deep Learning models provide state of the art classification results, but are not human-interpretable. We propose a novel method to interpret the classification results of a black box model a posteriori. We emulate the complex classifier by surrogate decision trees. Each tree mimics the behavior of the complex classifier by overestimating one of the classes. This yields a global, interpretable approximation of the black box classifier. Our method provides interpretations that are at the same time general (applying to many data points), confident (generalizing well to other data points), faithful to the original model (making the same predictions), and simple (easy to understand). Our experiments show that our method beats competing methods in these desiderata, and our user study shows that users prefer this type of interpretations over others.", "num_citations": "1\n", "authors": ["1069"]}
{"title": "FARF: A Fair and Adaptive Random Forests Classifier\n", "abstract": " As Artificial Intelligence (AI) is used in more applications, the need to consider and mitigate biases from the learned models has followed. Most works in developing fair learning algorithms focus on the offline setting. However, in many real-world applications data comes in an online fashion and needs to be processed on the fly. Moreover, in practical application, there is a trade-off between accuracy and fairness that needs to be accounted for, but current methods often have multiple hyper-parameters with non-trivial interaction to achieve fairness. In this paper, we propose a flexible ensemble algorithm for fair decision-making in the more challenging context of evolving online settings. This algorithm, called FARF (Fair and Adaptive Random Forests), is based on using online component classifiers and updating them according to the current distribution, that also accounts for fairness and a single hyper-parameters\u00a0\u2026", "num_citations": "1\n", "authors": ["1069"]}
{"title": "Model Compression for Dynamic Forecast Combination\n", "abstract": " The predictive advantage of combining several different predictive models is widely accepted. Particularly in time series forecasting problems, this combination is often dynamic to cope with potential non-stationary sources of variation present in the data. Despite their superior predictive performance, ensemble methods entail two main limitations: high computational costs and lack of transparency. These issues often preclude the deployment of such approaches, in favour of simpler yet more efficient and reliable ones. In this paper, we leverage the idea of model compression to address this problem in time series forecasting tasks. Model compression approaches have been mostly unexplored for forecasting. Their application in time series is challenging due to the evolving nature of the data. Further, while the literature focuses on neural networks, we apply model compression to distinct types of methods. In an extensive set of experiments, we show that compressing dynamic forecasting ensembles into an individual model leads to a comparable predictive performance and a drastic reduction in computational costs. Further, the compressed individual model with best average rank is a rule-based regression model. Thus, model compression also leads to benefits in terms of model interpretability. The experiments carried in this paper are fully reproducible.", "num_citations": "1\n", "authors": ["1069"]}
{"title": "Green Accelerated Hoeffding Tree\n", "abstract": " State-of-the-art machine learning solutions mainly focus on creating highly accurate models without constraints on hardware resources. Stream mining algorithms are designed to run on resource-constrained devices, thus a focus on low power and energy and memory-efficient is essential. The Hoeffding tree algorithm is able to create energy-efficient models, but at the cost of less accurate trees in comparison to their ensembles counterpart. Ensembles of Hoeffding trees, on the other hand, create a highly accurate forest of trees but consume five times more energy on average. An extension that tried to obtain similar results to ensembles of Hoeffding trees was the Extremely Fast Decision Tree (EFDT). This paper presents the Green Accelerated Hoeffding Tree (GAHT) algorithm, an extension of the EFDT algorithm with a lower energy and memory footprint and the same (or higher for some datasets) accuracy levels. GAHT grows the tree setting individual splitting criteria for each node, based on the distribution of the number of instances over each particular leaf. The results show that GAHT is able to achieve the same competitive accuracy results compared to EFDT and ensembles of Hoeffding trees while reducing the energy consumption up to 70\\%.", "num_citations": "1\n", "authors": ["1069"]}
{"title": "Randomizing the Self-Adjusting Memory for Enhanced Handling of Concept Drift\n", "abstract": " Real-time learning from data streams in non-stationary environments gains ever more relevance due to the exponentially increasing amounts of generated data. Recently, the Self-Adjusting Memory (SAM) was proposed, an algorithm able to robustly handle heterogeneous types on the basis of two dedicated memories for the current and former concepts that continuously preserve consistency with explicit filtering. Yet, since the algorithm is restricted to one memory architecture, the variety of possible alternatives is limited by design in favor of an overall model consistency. Moreover, it does not actively detect drift, thus adapting with a relatively high delay in case of abrupt changes. We propose a dynamic ensemble on the basis of the SAM algorithm, which is triggered by both, the inherent passive adaptation of SAM and active drift detection. Further, since SAM is based on the stable k-Nearest-Neighbor algorithm, we\u00a0\u2026", "num_citations": "1\n", "authors": ["1069"]}
{"title": "CS-ARF: Compressed Adaptive Random Forests for Evolving Data Stream Classification\n", "abstract": " Ensemble-based methods are one of the most often used methods in the classification task that have been adapted to the stream setting because of their high learning performance achievement. For instance, Adaptive Random Forests (ARF) is a recent ensemble method for evolving data streams that proved to be of a good predictive performance but, as all ensemble methods, it suffers from a severe drawback related to the high computational demand which prevents it from being efficient and further exacerbates with high-dimensional data. In this context, the application of a dimensionality reduction technique is crucial while processing the Internet of Things (IoT) data stream with ultrahigh dimensionality. In this paper, we aim to alleviate this deficiency and improve ARF performance, so we introduce the CS-ARF approach that uses Compressed Sensing (CS) as an internal pre-processing task, to reduce the\u00a0\u2026", "num_citations": "1\n", "authors": ["1069"]}
{"title": "Discriminative Streaming Network Embedding\n", "abstract": " Many real-world networks (eg, friendship network among Facebook users) generate data (eg, friend requests) in a stream fashion. Recently, several network embedding methods are proposed to learn embeddings on such networks incrementally. However, these methods perform incremental updates in a heuristic manner and thus fail to quantitatively restrict the differences between incremental learning and direct learning on the entire network (ie, the batch learning). Moreover, they ignore the node labels (eg, interests) when learning node embeddings, which undermines the performance of network embeddings for applications such as node classification. To solve this problem, in this paper we propose a novel network embedding framework, Discriminative Streaming Network Embedding (DimSim). When an edge insertion/deletion occurs, DimSim fast learns node embeddings incrementally, which is desired for\u00a0\u2026", "num_citations": "1\n", "authors": ["1069"]}
{"title": "IoT Streams for Data-Driven Predictive Maintenance and IoT, Edge, and Mobile for Embedded Machine Learning: Second International Workshop, IoT Streams 2020, and First\u00a0\u2026\n", "abstract": " This book constitutes selected papers from the Second International Workshop on IoT Streams for Data-Driven Predictive Maintenance, IoT Streams 2020, and First International Workshop on IoT, Edge, and Mobile for Embedded Machine Learning, ITEM 2020, co-located with ECML/PKDD 2020 and held in September 2020. Due to the COVID-19 pandemic the workshops were held online. The 21 full papers and 3 short papers presented in this volume were thoroughly reviewed and selected from 35 submissions and are organized according to the workshops and their topics: IoT Streams 2020: Stream Learning; Feature Learning; ITEM 2020: Unsupervised Machine Learning; Hardware; Methods; Quantization.", "num_citations": "1\n", "authors": ["1069"]}
{"title": "Fifth special issue on knowledge discovery and business intelligence\n", "abstract": " [extract] Artificial Intelligence (AI) is impacting our world. In the 1970s and 1980s, Expert Systems (ES) consisted of AI systems that included explicit knowledge, often represented in a symbolic form (e.g., by using the Prolog language), that was extracted from human experts.", "num_citations": "1\n", "authors": ["1069"]}
{"title": "Real-time machine learning competition on data streams at the IEEE big data 2019\n", "abstract": " In this paper, we present the competition \u201cReal-time Machine Learning Competition on Data Streams a BigData Cup Challenge of the IEEE Big Data 2019 conference. Data streams, such as data originated from sensors, have increasingly gained the interest of researchers and companies and are currently widely studied in data science. Companies in the telecommunication and energy industries are trying to exploit these data and get real-time insights on their services and equipment. In order to extract valuable knowledge from data streams, one must be able to analyze the data as they arrive and make meaningful predictions. For this purpose, we use fast incremental learners. There already exists a great community that is organizing various competitions on machine learning tasks for batch learners. Our goal was to introduce the same approach to engage the whole community in solving essential problems in data\u00a0\u2026", "num_citations": "1\n", "authors": ["1069"]}
{"title": "Feature Scoring using Tree-Based Ensembles for Evolving Data Streams\n", "abstract": " Assigning scores to individual features is a popular method for estimating the relevance of features in supervised learning. An accurate feature score estimation provides essential insights in sensitive domains, which is decisive to explain how features influence a given decision, contributing to the interpretability of the model. Learning from streaming data adds several challenges to machine learning tasks, including limited resources and changes to the underlying data distribution (i.e., evolving data streams). In this work, we introduce and analyze methods to efficiently estimate the Mean Decrease in Impurity (MDI) and COVER measures using ensembles of incremental decision trees. To achieve current scores in evolving data streams, we employ tree-ensembles that incorporate active drift detection. Experimental results show how MDI and COVER can be used to track the feature scores when their importance to\u00a0\u2026", "num_citations": "1\n", "authors": ["1069"]}
{"title": "Resource-aware Elastic Swap Random Forest for Evolving Data Streams\n", "abstract": " Continual learning based on data stream mining deals with ubiquitous sources of Big Data arriving at high-velocity and in real-time. Adaptive Random Forest ({\\em ARF}) is a popular ensemble method used for continual learning due to its simplicity in combining adaptive leveraging bagging with fast random Hoeffding trees. While the default ARF size provides competitive accuracy, it is usually over-provisioned resulting in the use of additional classifiers that only contribute to increasing CPU and memory consumption with marginal impact in the overall accuracy. This paper presents Elastic Swap Random Forest ({\\em ESRF}), a method for reducing the number of trees in the ARF ensemble while providing similar accuracy. {\\em ESRF} extends {\\em ARF} with two orthogonal components: 1) a swap component that splits learners into two sets based on their accuracy (only classifiers with the highest accuracy are used to make predictions); and 2) an elastic component for dynamically increasing or decreasing the number of classifiers in the ensemble. The experimental evaluation of {\\em ESRF} and comparison with the original {\\em ARF} shows how the two new components contribute to reducing the number of classifiers up to one third while providing almost the same accuracy, resulting in speed-ups in terms of per-sample execution time close to 3x.", "num_citations": "1\n", "authors": ["1069"]}
{"title": "Metropolis-Hastings Algorithms for Estimating Betweenness Centrality Talel Abdessalem\n", "abstract": " Recently, an optimal probability distribution was proposed to sample vertices for estimating betweenness centrality, that yields the minimum approximation error. However, it is computation-ally expensive to directly use it. In this paper, we investigate exploiting Metropolis-Hastings technique to sample based on this distribution. As a result, first given a network G and a vertex r \u2208 V (G), we propose a Metropolis-Hastings MCMC algorithm that samples from the space V (G) and estimates betweenness score of r. The stationary distribution of our MCMC sampler is the optimal distribution. We also show that our MCMC sampler provides an (\u03f5, \u03b4)-approximation. Then, given a network G and a set R \u2282 V (G), we present a Metropolis-Hastings MCMC sam-pler that samples from the joint space R and V (G) and estimates relative betweenness scores of the vertices in R. We show that for any pair r i , r j \u2208 R, the ratio of the expected values of the estimated relative betweenness scores of r i and r j with respect to each other is equal to the ratio of their betweenness scores. We also show that our joint-space MCMC sampler provides an (\u03f5, \u03b4)-approximation of the relative betweenness score of r i with respect to r j .", "num_citations": "1\n", "authors": ["1069"]}
{"title": "Novel Adaptive Algorithms for Estimating Betweenness, Coverage and k-path Centralities\n", "abstract": " An important index widely used to analyze social and information networks is betweenness centrality. In this paper, first given a directed network  and a vertex , we present a novel adaptive algorithm for estimating betweenness score of . Our algorithm first computes two subsets of the vertex set of , called  and , that define the sample spaces of the start-points and the end-points of the samples. Then, it adaptively samples from  and  and stops as soon as some condition is satisfied. The stopping condition depends on the samples met so far,  and . We show that compared to the well-known existing methods, our algorithm gives a more efficient -approximation. Then, we propose a novel algorithm for estimating -path centrality of . Our algorithm is based on computing two sets  and . While  defines the sample space of the source vertices of the sampled paths,  defines the sample space of the other vertices of the paths. We show that in order to give a -approximation of the -path score of , our algorithm requires considerably less samples. Moreover, it processes each sample faster and with less memory. Finally, we empirically evaluate our proposed algorithms and show their superior performance. Also, we show that they can be used to efficiently compute centrality scores of a set of vertices.", "num_citations": "1\n", "authors": ["1069"]}
{"title": "5 Dealing with Change\n", "abstract": " A central feature of the data stream model is that streams evolve over time, and algorithms must react to the change. For example, let us consider email spam classifiers, which decide whether new incoming emails are or are not spam. As classifiers learn to improve their accuracy, spammers are going to modify their strategies to build spam messages, trying to fool the classifiers into believing they are not spam. Customer behavior prediction is another example: customers, actual or potential, change their preferences as prices rise or fall, as new products appear and others fall out of fashion, or simply as the time of the year changes. The predictors in these and other situations need to be adapted, revised, or replaced as time passes if they are to maintain reasonable accuracy.", "num_citations": "1\n", "authors": ["1069"]}
{"title": "8 Regression\n", "abstract": " Regression is a learning task whose goal is to predict a numeric value, instead of a categorical attribute as in classification. Some of the techniques that are used in classification can be used in regression, but not all. Decision trees, the Perceptron and lazy learning can be used for regression, with modifications.", "num_citations": "1\n", "authors": ["1069"]}
{"title": "10 Frequent Pattern Mining\n", "abstract": " Frequent pattern mining is an important unsupervised learning task in data mining, with multiple applications. It can be used purely as an unsupervised, exploratory approach to data, and as the basis for finding association rules. It can also be used to find discriminative features for classification or clustering.", "num_citations": "1\n", "authors": ["1069"]}
{"title": "Discriminative Distance-Based Network Indices and the Tiny-World Property\n", "abstract": " Distance-based indices, including closeness centrality, average path length, eccentricity and average eccentricity, are important tools for network analysis. In these indices, the distance between two vertices is measured by the size of shortest paths between them. However, this measure has shortcomings. A well-studied shortcoming is that extending it to disconnected graphs (and also directed graphs) is controversial. The second shortcoming is that when this measure is used in real-world networks, a huge number of vertices may have exactly the same closeness/eccentricity scores. This restricts the applicability of these indices as they cannot distinguish vertices. The third shortcoming is that in many applications, the distance between two vertices not only depends on the size of shortest paths, but also on the number of shortest paths between them. In this paper, we develop a new distance measure between vertices of a graph that yields discriminative distance-based centrality indices. This measure is proportional to the size of shortest paths and inversely proportional to the number of shortest paths. We present algorithms for exact computation of the proposed discriminative indices. We then develop randomized algorithms that precisely estimate average discriminative path length and average discriminative eccentricity and show that they give (\u03f5, \u03b4)-approximations of these indices (\u03f5\u2208 R+ and \u03b4\u2208(0, 1)). Finally, we preform extensive experiments over several real-world networks from different domains. We first show that compared to the traditional indices, discriminative indices have usually much more discriminability. We then show that our\u00a0\u2026", "num_citations": "1\n", "authors": ["1069"]}
{"title": "Data stream mining\n", "abstract": " BackgroundNowadays, the quantity of data that is created every day is growing fast. Moreover, it was estimated that 2007 was the first year in which it was not possible to store all the data that we are producing. This massive amount of data opens new challenging discovery tasks, and the goal of this paper is to discuss them.", "num_citations": "1\n", "authors": ["1069"]}
{"title": "Benchmarking Stream Clustering Algorithms within the MOA Framework\n", "abstract": " In today\u2019s applications, massive, evolving data streams are ubiquitous. To gain useful information from this data, real time clustering analysis for streams is needed. A multitude of stream clustering algorithms were introduced. However, assessing the effectiveness of such an algorithm is challenging, because up to now there is no tool that allows a direct comparison of these algorithms. We present a novel clustering evaluation framework for data streams. It is an extension of Massive Online Analysis (MOA), a software environment for implementation and evaluation of algorithms for online learning from evolving data streams. Our stream clustering algorithm evaluation framework includes a collection of online clustering methods and offers tools for extensive evaluation and visualization. Moreover, it allows for bidirectional interaction with WEKA, since it uses the same internal data structures. Our framework is designed for extensibility, allowing straightforward adding of more algorithms, evaluation measures, and data feeds. It is released under the GNU GPL license.", "num_citations": "1\n", "authors": ["1069"]}
{"title": "Learning Decision Trees Adaptively from Data Streams with Time Drift\n", "abstract": " Learning Decision Trees Adaptively from Data Streams with Time Drift Page 1 Introduction ADWIN-DT Decision Tree Experiments Conclusions Learning Decision Trees Adaptively from Data Streams with Time Drift Albert Bifet and Ricard Gavald\u00e0 LARCA: Laboratori d\u2019Algor\u00edsmica Relacional, Complexitat i Aprenentatge Departament de Llenguatges i Sistemes Inform\u00e0tics Universitat Polit\u00e8cnica de Catalunya September 2007 Page 2 Introduction ADWIN-DT Decision Tree Experiments Conclusions Introduction: Data Streams Data Streams Sequence is potentially infinite High amount of data: sublinear space High speed of arrival: sublinear time per example Once an element from a data stream has been processed it is discarded or archived Example Puzzle: Finding Missing Numbers Let \u03c0 be a permutation of {1,...,n}. Let \u03c0\u22121 be \u03c0 with one element missing. \u03c0\u22121[i] arrives in increasing order Task: Determine the missing \u2026", "num_citations": "1\n", "authors": ["1069"]}