{"title": "Modeling performance of hadoop applications: A journey from queueing networks to stochastic well formed nets\n", "abstract": " Nowadays, many enterprises commit to the extraction of actionable knowledge from huge datasets as part of their core business activities. Applications belong to very different domains such as fraud detection or one-to-one marketing, and encompass business analytics and support to decision making in both private and public sectors. In these scenarios, a central place is held by the MapReduce framework and in particular its open source implementation, Apache\u00a0Hadoop. In such environments, new challenges arise in the area of jobs performance prediction, with the needs to provide Service Level Agreement guarantees to the end-user and to avoid waste of computational resources. In this paper we provide performance analysis models to estimate MapReduce job execution times in Hadoop clusters governed by the YARN Capacity Scheduler. We propose models of increasing complexity and accuracy\u00a0\u2026", "num_citations": "35\n", "authors": ["716"]}
{"title": "Log2cloud: Log-based prediction of cost-performance trade-offs for cloud deployments\n", "abstract": " Numerous organisations are considering moving at least some of their existing applications to the cloud. A key motivating factor for this fast-paced adoption of cloud is the expectation of cost savings. Estimating what these cost savings might be requires comparing the known cost of running an application in-house with a predicted cost of its cloud deployment. A major problem with this approach is the lack of suitable techniques for predicting the cost of the virtual machines (VMs) that a cloud-deployed application requires in order to achieve a given service-level agreement. We introduce a technique that addresses this problem by using established results from queueing network theory to predict the minimum VM cost of cloud deployments starting from existing application logs. We describe how this formal technique can be used to predict the cost-performance trade-offs available for the cloud deployment of an\u00a0\u2026", "num_citations": "29\n", "authors": ["716"]}
{"title": "Performance evaluation of self-reconfigurable service-oriented software with stochastic petri nets\n", "abstract": " Open-world software is a paradigm which allows to develop distributed and heterogeneous software systems. They can be built by integrating already developed third-party services, which use to declare QoS values (e.g., related to performance). It is true that these QoS values are subject to some uncertainties. Consequently, the performance of the systems using these services may unexpectedly decrease. A challenge for this kind of software is to self-adapt its behavior as a response to changes in the availability or performance of the required services. In this paper, we develop an approach to model self-renconfigurable open-world software systems with stochastic Petri nets. Moreover, we develop strategies for a system to gain a new state where it can recover its availability or even improve its performance. Through an example, we apply these strategies and evaluate them to discover suitable reconfigurations for\u00a0\u2026", "num_citations": "20\n", "authors": ["716"]}
{"title": "Performance aware open-world software in a 3-layer architecture\n", "abstract": " Open-world software is a new paradigm that stresses the concept of software service as a pillar for building applications. Services are unceasingly deployed elsewhere in the open-world and are used on demand. Consequently, the performance of these open-world applications relies on the performance of definitely unknown third-parties. Another consequence is that performance prediction methods can no longer assume that service times for software activities are well-known all over the time. More feasible solutions defend that they should be inferred from the environment, for example monitoring current services executions. So, there is a need for new performance prediction methods, and it is likely that they have to be applied not only when developing, but also during software execution, so to learn from the environment and to adapt to it. In this paper, we build on a three layer architecture, taken from literature, to\u00a0\u2026", "num_citations": "14\n", "authors": ["716"]}
{"title": "Quantitative analysis of apache storm applications: the NewsAsset case study\n", "abstract": " The development of Information Systems today faces the era of Big Data. Large volumes of information need to be processed in real-time, for example, for Facebook or Twitter analysis. This paper addresses the redesign of NewsAsset, a commercial product that helps journalists by providing services, which analyzes millions of media items from the social network in real-time. Technologies like Apache Storm can help enormously in this context. We have quantitatively analyzed the new design of NewsAsset to assess whether the introduction of Apache Storm can meet the demanding performance requirements of this media product. Our assessment approach, guided by the Unified Modeling Language (UML), takes advantage, for performance analysis, of the software designs already used for development. In addition, we converted UML into a domain-specific modeling language (DSML) for Apache Storm\u00a0\u2026", "num_citations": "10\n", "authors": ["716"]}
{"title": "Quality Assessment in DevOps: Automated Analysis of a Tax Fraud Detection System\n", "abstract": " The paper presents an industrial application of a DevOps process for a Tax fraud detection system. In particular, we report the influence of the quality assessment during development iterations, with special focus on the fulfillment of performance requirements. We investigated how to guarantee quality requirements in a process iteration while new functionalities are added. The experience has been carried out by practitioners and academics in the context of a project for improving quality of data intensive applications.", "num_citations": "10\n", "authors": ["716"]}
{"title": "Performance sensitive self-adaptive service-oriented software using hidden markov models\n", "abstract": " Service Oriented Architecture (SOA) is a paradigm where applications are built on services offered by third party providers. Behavior of providers evolves and makes a challenge the performance prediction of SOA applications. A proper decision about when a provider should be substituted can dramatically improve the performance of the application. We propose hidden Markov models (HMM) to help service integrators to foretell the current state of third-parties. The paper leverages different algorithms that change providers based on predictions about their states. We also integrate these algorithms and HMMs in an architectural solution to coordinate them with other challenges in the SOA world.", "num_citations": "10\n", "authors": ["716"]}
{"title": "A systematic approach for performance assessment using process mining\n", "abstract": " Software performance engineering is a mature field that offers methods to assess system performance. Process mining is a promising research field applied to gain insight on system processes. The interplay of these two fields opens promising applications in the industry. In this work, we report our experience applying a methodology, based on process mining techniques, for the performance assessment of a commercial data-intensive software application. The methodology has successfully assessed the scalability of future versions of this system. Moreover, it has identified bottlenecks components and replication needs for fulfilling business rules. The system, an integrated port operations management system, has been developed by Prodevelop, a medium-sized software enterprise with high expertise in geospatial technologies. The performance assessment has been carried out by a team composed by\u00a0\u2026", "num_citations": "6\n", "authors": ["716"]}
{"title": "Architectural Concerns for Digital Twin of the Organization\n", "abstract": " Employing a Digital Twin of the Organization would help enterprises to change and innovate, thus enhancing their organization\u2019s sustainability. However, the lack of engineering best practices for developing and operating a Digital Twin of the Organization makes it difficult for enterprises to fully benefit from it. Many companies are currently investigating the potential use of it, but available solutions are often context-dependent or system-specific, and challenging to adapt, extend, and reuse. Therefore, digitalization is perceived as a slow, resource-demanding, and extremely expensive process whose outcome is uncertain. To this extent, enterprises seek solutions allowing them to gently introduce a Digital Twin of the Organization into their organization and to evolve it according to the changing needs and situations. This paper reports a first attempt on architecting a Digital Twin of an Organization, and\u00a0\u2026", "num_citations": "4\n", "authors": ["716"]}