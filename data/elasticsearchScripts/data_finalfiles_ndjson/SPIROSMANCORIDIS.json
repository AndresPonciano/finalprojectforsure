{"title": "Using automatic clustering to produce high-level system organizations of source code\n", "abstract": " We describe a collection of algorithms that we developed and implemented to facilitate the automatic recovery of the modular structure of a software system from its source code. We treat automatic modularization as an optimization problem. Our algorithms make use of traditional hill-climbing and genetic algorithms.", "num_citations": "687\n", "authors": ["342"]}
{"title": "Bunch: A clustering tool for the recovery and maintenance of software system structures\n", "abstract": " Software systems are typically modified in order to extend or change their functionality, improve their performance, port them to different platforms, and so on. For developers, it is crucial to understand the structure of a system before attempting to modify it. The structure of a system, however, may not be apparent to new developers, because the design documentation is non-existent or, worse, inconsistent with the implementation. This problem could be alleviated if developers were somehow able to produce high-level system decomposition descriptions from the low-level structures present in the source code. We have developed a clustering tool called Bunch that creates a system decomposition automatically by treating clustering as an optimization problem. The paper describes the extensions made to Bunch in response to feedback we received from users. The most important extension, in terms of the quality of\u00a0\u2026", "num_citations": "615\n", "authors": ["342"]}
{"title": "On the automatic modularization of software systems using the bunch tool\n", "abstract": " Since modern software systems are large and complex, appropriate abstractions of their structure are needed to make them more understandable and, thus, easier to maintain. Software clustering techniques are useful to support the creation of these abstractions by producing architectural-level views of a system's structure directly from its source code. This paper examines the Bunch clustering system which, unlike other software clustering tools, uses search techniques to perform clustering. Bunch produces a subsystem decomposition by partitioning a graph of the entities (e.g., classes) and relations (e.g., function calls) in the source code. Bunch uses a fitness function to evaluate the quality of graph partitions and uses search algorithms to find a satisfactory solution. This paper presents a case study to demonstrate how Bunch can be used to create views of the structure of significant software systems. This paper\u00a0\u2026", "num_citations": "545\n", "authors": ["342"]}
{"title": "Automatic clustering of software systems using a genetic algorithm\n", "abstract": " Large software systems tend to have a rich and complex structure. Designers typically depict the structure of software systems as one or more directed graphs. For example, a directed graph can be used to describe the modules (or classes) of a system and their static interrelationships using nodes and directed edges, respectively. We call such graphs \"module dependency graphs\" (MDGs). MDGs can be large and complex graphs. One way of making them more accessible is to partition them, separating their nodes (i.e. modules) into clusters (i.e. subsystems). In this paper, we describe a technique for finding \"good\" MDG partitions. Good partitions feature relatively independent subsystems that contain modules which are highly interdependent. Our technique treats finding a good partition as an optimization problem, and uses a genetic algorithm (GA) to search the extraordinarily large solution space of all possible\u00a0\u2026", "num_citations": "356\n", "authors": ["342"]}
{"title": "Comparing the decompositions produced by software clustering algorithms using similarity measurements\n", "abstract": " Decomposing source code components and relations into subsystem clusters is an active area of research. Numerous clustering approaches have been proposed in the reverse engineering literature, each one using a different algorithm to identify subsystems. Since different clustering techniques may not produce identical results when applied to the same system, mechanisms that can measure the extent of these differences are needed. Some work to measure the similarity between decompositions has been done, but this work considers the assignment of source code components to clusters as the only criterion for similarity. We argue that better similarity measurements can be designed if the relations between the components are considered. The authors propose two similarity measurements that overcome certain problems in existing measurements. We also provide some suggestions on how to identify and\u00a0\u2026", "num_citations": "166\n", "authors": ["342"]}
{"title": "Multi-channel change-point malware detection\n", "abstract": " A malware detection system and method detects changes in host behavior indicative of malware execution. The system uses linear discriminant analysis (LDA) for feature extraction, multi-channel change-point detection algorithms to infer malware execution, and a data fusion center (DFC) to combine local decisions into a host-wide diagnosis. The malware detection system includes sensors that monitor the status of a host computer being monitored for malware, a feature extractor that extracts data from the sensors corresponding to predetermined features, local detectors that perform malware detection on each stream of feature data from the feature extractor independently, and a data fusion center that uses the decisions from the local detectors to infer whether the host computer is infected by malware.", "num_citations": "123\n", "authors": ["342"]}
{"title": "Using heuristic search techniques to extract design abstractions from source code\n", "abstract": " As modern software systems are large and complex, appropriate abstractions of their structure are needed to make them more understandable and, thus, easier to maintain. Software clustering tools are useful to support the creation of these abstractions. In this paper we describe our search algorithms for software clustering, and conduct a case study to demonstrate how altering the clustering parameters impacts the behavior and performance of our algorithms.", "num_citations": "120\n", "authors": ["342"]}
{"title": "On the evaluation of the bunch search-based software modularization algorithm\n", "abstract": " The first part of this paper describes an automatic reverse engineering process to infer subsystem abstractions that are useful for a variety of software maintenance activities. This process is based on clustering the graph representing the modules and module-level dependencies found in the source code into abstract structures not in the source code called subsystems. The clustering process uses evolutionary algorithms to search through the enormous set of possible graph partitions, and is guided by a fitness function designed to measure the quality of individual graph partitions. The second part of this paper focuses on evaluating the results produced by our clustering technique. Our previous research has shown through both qualitative and quantitative studies that our clustering technique produces good results quickly and consistently. In this part of the paper we study the underlying structure of the\u00a0\u2026", "num_citations": "110\n", "authors": ["342"]}
{"title": "Using code metric histograms and genetic algorithms to perform author identification for software forensics\n", "abstract": " We have developed a technique to characterize software developers-styles using a set of source code metrics. This style fingerprint can be used to identify the likely author of a piece of code from a pool of candidates. Author identification has applications in criminal justice, corporate litigation, and plagiarism detection. Furthermore, we can identify candidate developers who share similar styles, making our technique useful for software maintenance as well. Our method involves measuring the differences in histogram distributions for code metrics. Identifying a combination of metrics that is effective in distinguishing developer styles is key to the utility of the technique. Our case study involves 18 metrics, and the time involved in exhaustive searching of the problem space prevented us from adding additional metrics. Using a genetic algorithm to perform the search, we were able to find good metric combinations in hours\u00a0\u2026", "num_citations": "87\n", "authors": ["342"]}
{"title": "A hierarchy of dynamic software views: From object-interactions to feature-interactions\n", "abstract": " This work presents a hierarchy of dynamic views that is constructed using tools that analyze program execution traces. At the highest-level of abstraction are the feature-interaction and implementation views, which track the inter-feature dependencies as well as the classes that implement these features. At the middle-level is the class-interaction view, which is an abstract view of the object-interactions. The object-interaction view is the base view for all the views, and captures the low-level runtime interactions between objects. Two case studies are used to demonstrate the effectiveness of our work.", "num_citations": "82\n", "authors": ["342"]}
{"title": "Detection, diagnosis, and mitigation of software faults\n", "abstract": " A computational geometry technique is utilized to detect, diagnose, and/or mitigate fault detection during the execution of a software application. Runtime measurements are collected and processed to generate a geometric enclosure that represents the normal, non-failing, operating space of the application being monitored. When collected runtime measurements are classified as being inside or on the perimeter of the geometric enclosure, the application is considered to be in a normal, non-failing, state. When collected runtime measurements are classified as being outside of the geometric enclosure, the application is considered to be in an anomalous, failing, state. In an example embodiment, the geometric enclosure is a convex hull generated in N-dimensional Euclidean space. Appropriate action (eg, restart the software, turn off access to a network port) can be taken depending on where the measurement\u00a0\u2026", "num_citations": "68\n", "authors": ["342"]}
{"title": "A probabilistic approach to source code authorship identification\n", "abstract": " There exists a need for tools to help identify the authorship of source code. This includes situations in which the ownership of code is questionable, such as in plagiarism or intellectual property infringement disputes. Authorship identification can also be used to assist in the apprehension of the creators of malware. In this paper we present an approach to identifying the authors of source code. We begin by computing a set of metrics to build profiles for a population of known authors using code samples that are verified to be authentic. We then compute metrics on unidentified source code to determine the closest matching profile. We demonstrate our approach on a case study that involves two kinds of software: one based on open source developers working on various projects, and another based on students working on assignments with the same requirements. In our case study we are able to determine authorship\u00a0\u2026", "num_citations": "62\n", "authors": ["342"]}
{"title": "Using program transformation to secure C programs against buffer overflows\n", "abstract": " Buffer overflows are the most common source of security vulnerabilities in C programs. This class of vulnerability, which is found in both legacy and modern software, costs the software industry hundreds of millions of dollars per year.The most common type of buffer overflow is the runtime stack overflow. It is common because programmers often use stack allocated arrays. This enables the attacker to change a program\u2019s control flow by writing beyond the boundary of an array onto a return address on the run-time stack. If the arrays are repositioned to the heap at compile time, none of these attacks succeed. Furthermore, repositioning buffers to the heap should perturb the heap memory enough to prevent many heap overflows as well. We have created a tool called Gemini that repositions stack allocated arrays at compile time using TXL. The transformation preserves the semantics of the program with a small performance penalty. This paper discusses the semantics-preserving transformation of stack allocated arrays to heap allocated \u201cpointers to arrays\u201d. A program that is amenable to a buffer overflow attack and several Linux programs are used as examples to demonstrate the effectiveness and overhead of our technique.", "num_citations": "60\n", "authors": ["342"]}
{"title": "System call-based detection of malicious processes\n", "abstract": " System call analysis is a behavioral malware detection technique that is popular due to its promising detection results and ease of implementation. This study describes a system that uses system call analysis to detect malware that evade traditional defenses. The system monitors executing processes to identify compromised hosts in production environments. Experimental results compare the effectiveness of multiple feature extraction strategies and detectors based on their detection accuracy at low false positive rates. Logistic regression and support vector machines consistently outperform log-likelihood ratio and signature detectors as processing and detection methods. A feature selection study indicates that a relatively small set of system call 3-grams provide detection accuracy comparable to that of more complex models. A case study indicates that the detection system performs well against a variety of\u00a0\u2026", "num_citations": "59\n", "authors": ["342"]}
{"title": "Characterizing the'security vulnerability likelihood'of software functions\n", "abstract": " Software maintainers and auditors would benefit from a tool to help them focus their attention on functions that are likely to be the source of security vulnerabilities. However, the existence of such a tool is predicated on the ability to characterize a function's 'security vulnerability likelihood'. Our hypothesis is that functions near a source of input are most likely to contain security vulnerability. These functions should be a small percentage of the total number of functions in the system. To validate this hypothesis, we performed an experiment involving thirty one vulnerabilities in thirty open source systems. This paper describes the experiment, its outcome, and the tools used to conduct it. It also describes the FLF (front line functions) finder, which is a tool that was developed using knowledge gathered from the outcome of the experiment. This tool automates the detection of high-risk functions. To demonstrate the\u00a0\u2026", "num_citations": "59\n", "authors": ["342"]}
{"title": "Craft: a framework for evaluating software clustering results in the absence of benchmark decompositions [clustering results analysis framework and tools]\n", "abstract": " Software clustering algorithms are used to create high-level views of a system's structure using source code-level artifacts. Software clustering is an active area of research that has produced many clustering algorithms. However, we have so far seen very little work that investigates how the results of these algorithms can be evaluated objectively in the absence of a benchmark decomposition or without the active participation of the original designers of the system. Ideally, for a given system, art agreed upon reference (benchmark) decomposition of the system's structure would exist, allowing the results of various clustering algorithms to be compared against it. Since such benchmarks seldom exist, we seek alternative methods to gain confidence in the quality of results produced by software clustering algorithms. In this paper, we present a tool that supports the evaluation of software clustering results in the absence of\u00a0\u2026", "num_citations": "59\n", "authors": ["342"]}
{"title": "An architecture for distributing the computation of software clustering algorithms\n", "abstract": " Collections of general purpose networked workstations offer processing capability that often rivals or exceeds supercomputers. Since networked workstations are readily available in most organizations, they provide an economic and scalable alternative to parallel machines. The authors discuss how individual nodes in a computer network can be used as a collection of connected processing elements to improve the performance of a software engineering tool that we developed. Our tool, called Bunch, automatically clusters the structure of software systems into a hierarchy of subsystems. Clustering helps developers understand complex systems by providing them with high-level abstract (clustered) views of the software structure. The algorithms used by Bunch are computationally intensive and, hence, we would like to improve our tool's performance in order to cluster very large systems. The paper describes how\u00a0\u2026", "num_citations": "59\n", "authors": ["342"]}
{"title": "Scenariographer: A tool for reverse engineering class usage scenarios from method invocation sequences\n", "abstract": " Typical documentation for object-oriented programs includes descriptions of the parameters and return types of each method in a class, but little or no information on valid method invocation sequences. Knowing the sequence with which methods of a class can be invoked is useful information especially for software engineers (e.g., developers, testers) who are actively involved in the maintenance of large software systems. This paper describes a new approach and a tool for generating class usage scenarios (i.e., how a class is used by other classes) from method invocations, which are collected during the execution of the software. Our approach is algorithmic and employs the notion of canonical sets to categorize method sequences into groups of similar sequences, where each group represents a usage scenario for a given class.", "num_citations": "57\n", "authors": ["342"]}
{"title": "Source code authorship attribution using long short-term memory based networks\n", "abstract": " Machine learning approaches to source code authorship attribution attempt to find statistical regularities in human-generated source code that can identify the author or authors of that code. This has applications in plagiarism detection, intellectual property infringement, and post-incident forensics in computer security. The introduction of features derived from the Abstract Syntax Tree (AST) of source code has recently set new benchmarks in this area, significantly improving over previous work that relied on easily obfuscatable lexical and format features of program source code. However, these AST-based approaches rely on hand-constructed features derived from such trees, and often include ancillary information such as function and variable names that may be obfuscated or manipulated.               In this work, we provide novel contributions to AST-based source code authorship attribution using deep\u00a0\u2026", "num_citations": "55\n", "authors": ["342"]}
{"title": "Spectral and meta-heuristic algorithms for software clustering\n", "abstract": " When large software systems are reverse engineered, one of the views that is produced is the system decomposition hierarchy. This hierarchy shows the system\u2019s subsystems, the contents of the subsystems (i.e., modules or other subsystems), and so on. Software clustering tools create the system decomposition automatically or semi-automatically with the aid of the software engineer.The Bunch software clustering tool shows how meta-heuristic search algorithms can be applied to the software clustering problem, successfully. Unfortunately, we do not know how close the solutions produced by Bunch are to the optimal solution. We can only obtain the optimal solution for trivial systems using an exhaustive search.This paper presents evidence that Bunch\u2019s solutions are within a known factor of the optimal solution. We show this by applying spectral methods to the software clustering problem. The advantage of using\u00a0\u2026", "num_citations": "54\n", "authors": ["342"]}
{"title": "A reverse engineering tool for extracting protocols of networked applications\n", "abstract": " Networked applications play a significant role in today's interconnected world. It is important for software engineers to be able to understand and model the behavior of these applications during software maintenance. Some networked applications use legacy protocols in ways they were not intended to be used. Others use newly created protocols that are designed in an ad hoc way to simply meet requirements. Protocol usage needs to be understood so that applications can be effectively tested and maintained. In this paper we propose the first step in achieving this goal by presenting a dynamic analysis tool, called PEXT, that can reverse engineer a networked application's underlying protocol by analyzing a collection of packets captured from the application at runtime. We demonstrate the effectiveness of this tool by extracting a protocol from an FTP application, and comparing the extracted protocol to the\u00a0\u2026", "num_citations": "49\n", "authors": ["342"]}
{"title": "Search based reverse engineering\n", "abstract": " In this paper we describe a two step process for reverse engineering the software architecture of a system directly from its source code. The first step involves clustering the modules from the source code into abstract structures called subsystems. The second step involves reverse engineering the subsystem-level relations using a formal (and visual) architectural constraint language. We use search techniques to accomplish both of these steps, and have implemented a suite of integrated tools to support the reverse engineering process. Through a case study, we demonstrate how our tools can be used to extract the software architecture of an open-source software package from its source code without having any a priori knowledge about its design.", "num_citations": "49\n", "authors": ["342"]}
{"title": "Form: A framework for creating views of program executions\n", "abstract": " Form is a framework used to construct tools for analyzing the runtime behavior of standalone and distributed software systems. The architecture of Form is based on the event broadcast and pipe and filter styles. In the implementation of this architecture, execution profiles may be generated from standalone or distributed systems. The profile data is subsequently broadcast by Form to one or more views. Each view is a tool used to support program understanding or other software development activities. The authors describe the Form architecture and implementation, as well as a tool that was built using Form. This tool profiles Java-based distributed systems and generates UML sequence diagrams to describe their execution. We also present a case study that shows how this tool was used to extract sequence diagrams from a three-tiered EJB-based distributed application.", "num_citations": "45\n", "authors": ["342"]}
{"title": "On computing the canonical features of software systems\n", "abstract": " Software applications typically have many features that vary in their similarity. We define a measurement of similarity between pairs of features based on their underlying implementations and use this measurement to compute a set of canonical features. The canonical features set (CFS) consists of a small number of features that are as dissimilar as possible to each other, yet are most representative of the features that are not in the CFS. The members of the CFS are distinguishing features and understanding their implementation provides the engineer with an overview of the system undergoing scrutiny. The members of the CFS can also be used as cluster centroids to partition the entire set of features. Partitioning the set of features can simplify the understanding of large and complex software systems. Additionally, when a specific feature must undergo maintenance, it is helpful to know which features are most\u00a0\u2026", "num_citations": "42\n", "authors": ["342"]}
{"title": "A tool for securely integrating legacy systems into a distributed environment\n", "abstract": " Legacy systems provide services that remain useful beyond the means of the technology in which they were originally implemented. Our Legacy Wrapper tool packages the services of a legacy application in order to redistribute as a distributed object. In this new environment, the wrapper provides its own layer of security between the security domain of the host and the distributed object system. This security layer includes a sandbox for the application that is designed to protect the application against malicious users and the host from malicious applications. In this paper we present the Legacy Access model and the Legacy Wrapper system. The Legacy Access model is an original system access model that presents a four tiered sandboxing model for wrapping legacy applications: complete encapsulation, shared sandbox, single sandbox, sandboxless operation. The Legacy Wrapper tool is an implementation of the\u00a0\u2026", "num_citations": "42\n", "authors": ["342"]}
{"title": "On the use of discretized source code metrics for author identification\n", "abstract": " Intellectual property infringement and plagiarism litigation involving source code would be more easily resolved using code authorship identification tools. Previous efforts in this area have demonstrated the potential of determining the authorship of a disputed piece of source code automatically. This was achieved by using source code metrics to build a database of developer profiles, thus characterizing a population of developers. These profiles were then used to determine the likelihood that the unidentified source code was authored by a given developer. In this paper we evaluate the effect of discretizing source code metrics for use in building developer profiles. It is well known that machine learning techniques perform better when using categorical variables as opposed to continuous ones. We present a genetic algorithm to discretize metrics to improve source code to author classification. We evaluate the\u00a0\u2026", "num_citations": "40\n", "authors": ["342"]}
{"title": "Toward an automatic, online behavioral malware classification system\n", "abstract": " Malware authors are increasingly using specialized toolkits and obfuscation techniques to modify existing malware and avoid detection by traditional antivirus software. The resulting proliferation of obfuscated malware variants poses a challenge to antivirus vendors, who must create signatures to detect each new malware variant. Although the many variants in a malware family have different static signatures, they share characteristic behavioral patterns resulting from their common function and heritage. We describe an automatic classification system that can be trained to accurately identify new variants within known malware families, using observed similarities in behavioral features extracted from sensors monitoring live computers hosts. We evaluate the accuracy of the classifier on a live testbed under a heavy computational load. The described classification system is intended to perform classification online\u00a0\u2026", "num_citations": "39\n", "authors": ["342"]}
{"title": "Reportal: A web-based portal site for reverse engineering\n", "abstract": " We present a Web-based portal site for the reverse engineering of software systems, called REportal (Reverse Engineering portal). REportal enables authorized users to upload their code to a secure Web site and then, through the guidance of wizards, to browse and analyze their code. Currently, the portal services include code analysis, browsing, querying and design extraction for C, C++ and Java programs. The REportal services are implemented by several reverse engineering tools that our team has developed over the years. With this work, we aim to assist professional software engineers, educators and other researchers who need to analyze code. Specifically, we present a technology that provides a simple and easily accessible user interface to a number of reverse engineering tools. More importantly, this technology saves the user from the time and effort required to install, administer and integrate these tools.", "num_citations": "38\n", "authors": ["342"]}
{"title": "Toward an environment for comprehending distributed systems.\n", "abstract": " Many modern software systems are often large, distributed, written in more than one programming language, and developed using pre-built components. This paper presents the results of the first phase of a project to develop an environment that supports the comprehension of distributed systems.The environment has a layered architecture consisting of three subsystems: data gathering, data repository, and modeling/visualization. The first phase of the project focuses on the design and implementation of the data gathering and data repository subsystems. The key requirements of the environment are to support:(a) static and dynamic analysis,(b) multiple languages,(c) distributed systems, and (d) component-based models.", "num_citations": "37\n", "authors": ["342"]}
{"title": "Applying spectral methods to software clustering\n", "abstract": " The application of spectral methods to the software clustering problem has the advantage of producing results that are within a known factor of the optimal solution. Heuristic search methods, such as those supported by the Bunch clustering tool, only guarantee local optimality which may be far from the global optimum. In this paper, we apply the spectral methods to the software clustering problem and make comparisons to Bunch using the same clustering criterion. We conducted a case study, involving 13 software systems, to draw our comparisons. There is a dual benefit to making these comparisons. Specifically, we gain insight into (1) the quality of the spectral methods solutions; and (2) the proximity of the results produced by Bunch to the optimal solution.", "num_citations": "36\n", "authors": ["342"]}
{"title": "Recovering the structure of software systems using tube graph interconnection clustering\n", "abstract": " An important product of the software design phase is the specification of the software structure at various levels of detail. Without reliable design documentation, significant software systems become less accessible to software engineers because structural information is buried in the intricate implementation source code. Reverse engineering techniques aim at recovering the structure of software systems, from the source code and mental models of developers, in order to make these systems more understandable to those maintaining them. Many reverse engineering techniques rely on creating a decomposition hierarchy by recursively clustering related software components (eg, variables, procedures, classes, modules) into composite components (eg, subsystems). Component clustering is necessary for managing complexity, and therefore is an important step in the reverse engineering process. In this paper, we\u00a0\u2026", "num_citations": "36\n", "authors": ["342"]}
{"title": "On the use of computational geometry to detect software faults at runtime\n", "abstract": " Despite advances in software engineering, software faults continue to cause system downtime. Software faults are difficult to detect before the system fails, especially since the first symptom of a fault is often system failure itself.", "num_citations": "34\n", "authors": ["342"]}
{"title": "Modeling the search landscape of metaheuristic software clustering algorithms\n", "abstract": " Software clustering techniques are useful for extracting architectural information about a system directly from its source code structure. This paper starts by examining the Bunch clustering system, which uses metaheuristic search techniques to perform clustering. Bunch produces a subsystem decomposition by partitioning a graph formed from the entities (e.g., modules) and relations (e.g., function calls) in the source code, and then uses a fitness function to evaluate the quality of the graph partition. Finding the best graph partition has been shown to be a NP-hard problem, thus Bunch attempts to find a sub-optimal result that is \u201cgood enough\u201d using search algorithms. Since the validation of software clustering results often is overlooked, we propose an evaluation technique based on the search landscape of the graph being clustered. By gaining insight into the search landscape, we can determine the quality\u00a0\u2026", "num_citations": "31\n", "authors": ["342"]}
{"title": "Behavioral anomaly detection of malware on home routers\n", "abstract": " The Internet of Things (IoT) introduced new targets and attack vectors for malicious actors who infect insecure devices with malware in order to form large botnets that can launch distributed denial of service (DDoS) attacks. These botnets comprise various infected devices such as Internet-connected cameras and home routers. This paper focuses on the unsolved problem of creating robust malware detection to secure home routers. This research compares the effectiveness of three different approaches to behavioral malware detection on home endpoint routers through the observation of kernel-level system calls on these routers: i) principal component analysis (PCA), ii) one-class support vector machines, and iii) a naive anomaly detector based on unseen n-grams.", "num_citations": "30\n", "authors": ["342"]}
{"title": "Run-time classification of malicious processes using system call analysis\n", "abstract": " This study presents a malware classification system designed to classify malicious processes at run-time on production hosts. The system monitors process-level system call activity and uses information extracted from system call traces as inputs to the classifier. The system is advantageous because it does not require the use of specialized analysis environments. Instead, a `lightweight' service application monitors process execution and classifies new malware samples based on their behavioral similarity to known malware. This study compares the effectiveness of multiple feature sets, ground truth labeling schemes, and machine learning algorithms for malware classification. The accuracy of the classification system is evaluated against processlevel system call traces of recently discovered malware samples collected from production environments. Experimental results indicate that accurate classification results\u00a0\u2026", "num_citations": "29\n", "authors": ["342"]}
{"title": "Gadget: A Tool for Extracting the Dynamic Structure of Java Programs.\n", "abstract": " Source code analysis and inspection does not provide enough information to describe the structure of an objectoriented program completely because there are components and relations that only exist during its runtime. This paper presents a tool, called Gadget, that helps software engineers extract the dynamic structure of objectoriented programs written in the Java programming language. The tool uses program profiling, filtering, and graph clustering techniques.In this work we show how Gadget is used to analyze a standard graphical user interface library for Java, called Swing. This library has a complex structure, part of which we expose using data gathered by Gadget during the execution of a simple Java program that uses Swing.", "num_citations": "29\n", "authors": ["342"]}
{"title": "Static security analysis based on input-related software faults\n", "abstract": " It is important to focus on security aspects during the development cycle to deliver reliable software. However, locating security faults in complex systems is difficult and there are only a few effective automatic tools available to help developers. In this paper we present an approach to help developers locate vulnerabilities by marking parts of the source code that involve user input. We focus on input-related code, since an attacker can usually take advantage of vulnerabilities by passing malformed input to the application. The main contributions of this work are two metrics to help locate faults during a code review, and algorithms to locate buffer overflow and format string vulnerabilities in C source code. We implemented our approach as a plug in to the Grammatech CodeSurfer tool. We tested and validated our technique on open source projects and we found faults in software that includes Pidgin and cyrus-imapd.", "num_citations": "23\n", "authors": ["342"]}
{"title": "Visualizing and analyzing software infrastructures\n", "abstract": " Companies frequently need to redesign their software infrastructures in response to marketplace changes, but they must do so carefully so that the new architecture will not disrupt existing operations or increase operating costs unnecessarily. To support these goals, system architects have long recognized the need to build a repository of information about all of their company's systems and their interfaces. Using this information, architects create system interface diagrams to help them study the existing architecture. The authors discuss their system, Enterprise Navigator, which lets users make ad hoc queries about an enterprise software architecture and then automatically generate the corresponding system interface diagram in real time on the Web.", "num_citations": "23\n", "authors": ["342"]}
{"title": "Reducing program comprehension effort in evolving software by recognizing feature implementation convergence\n", "abstract": " The implementations of software features evolve as an application matures. We define a measure of feature implementation overlap that determines how similar features are in their execution by examining their call graphs. We consider how this measure changes over time, and evaluate the hypothesis that over time and subsequent versions of a software application, the implementations of semantically similar features converge. As the features of an application converge in their implementation, we are able to more effectively determine groups of semantically similar features and to reduce the cost of program comprehension by selecting few key features that give an overview of the system. We present a case study analyzing the features of the Jext, Firefox, and Gaim software systems to support our hypothesis.", "num_citations": "21\n", "authors": ["342"]}
{"title": "Using interconnection style rules to infer software architecture relations\n", "abstract": " Software design techniques emphasize the use of abstractions to help developers deal with the complexity of constructing large and complex systems. These abstractions can also be used to guide programmers through a variety of maintenance, reengineering and enhancement activities. Unfortunately, recovering design abstractions directly from a system\u2019s implementation is a difficult task because the source code does not contain them. In this paper we describe an automatic process to infer architectural-level abstractions from the source code. The first step uses software clustering to aggregate the system\u2019s modules into abstract containers called subsystems. The second step takes the output of the clustering process, and infers architectural-level relations based on formal style rules that are specified visually. This two step process has been implemented using a set of integrated tools that employ search\u00a0\u2026", "num_citations": "21\n", "authors": ["342"]}
{"title": "Lightweight behavioral malware detection for windows platforms\n", "abstract": " We describe a lightweight behavioral malware detection technique that leverages Microsoft Windows prefetch files. We demonstrate that our malware detection achieves a high detection rate with a low false-positive rate of 1 \u00d7 10 -3 , and scales linearly for training samples. We demonstrate the generalization of our malware detection on two different Windows platforms with a different set of applications. We study the loss in performance of our malware detection in case of concept drift and its ability to adapt. Finally, we measure our malware detection against evasive malware and present an effective auxiliary defensive technique against such attacks.", "num_citations": "20\n", "authors": ["342"]}
{"title": "A conceptual framework for software development\n", "abstract": " Large scale software development is an intrinsically difficult task. Developers use a set of specialized tools to alleviate some of this difficulty. The problem is that most of these tools are not integrated and do little to help developers and managers maintain an overall view of the development by organizing the software entities, created by tools, in a consistent fashion.", "num_citations": "20\n", "authors": ["342"]}
{"title": "Behavioral malware classification using convolutional recurrent neural networks\n", "abstract": " Behavioral malware detection aims to improve on the performance of static signature-based techniques used by anti-virus systems, which are less effective against modern polymorphic and metamorphic malware. Behavioral malware classification aims to go beyond the detection of malware by also identifying a malware's family according to a naming scheme such as the ones used by anti-virus vendors. Behavioral malware classification techniques use run-time features, such as file system or network activities, to capture the behavioral characteristic of running processes. The increasing volume of malware samples, diversity of malware families, and the variety of naming schemes given to malware samples by anti-virus vendors present challenges to behavioral malware classifiers. We describe a behavioral classifier that uses a Convolutional Recurrent Neural Network and data from Microsoft Windows Prefetch\u00a0\u2026", "num_citations": "18\n", "authors": ["342"]}
{"title": "A \u201ccurriculum-cycle\u201d environment for teaching programming\n", "abstract": " There are a number of programming languages and tools available to educators for teaching programming to undergraduate computer science students. Although efforts have been made to integrate these languages and tools into programming environments, these environments generally do not have all of the ingredients that would make them useful at all levels of a typical undergraduate curriculum.Current technology used in most undergraduate courses is suitable for teaching students how to code in a particular programming language. Software development tools for activities other than coding such as requirements analysis, design, maintenance, and so on, are not provided. In addition, most programming languages in current use cannot be used in both beginner and advanced courses.", "num_citations": "18\n", "authors": ["342"]}
{"title": "Reverse engineering utility functions using genetic programming to detect anomalous behavior in software\n", "abstract": " Recent studies have shown the promise of using utility functions to detect anomalous behavior in software systems at runtime. However, it remains a challenge for software engineers to hand-craft a utility function that achieves both a high precision (i.e., few false alarms) and a high recall (i.e., few undetected faults). This paper describes a technique that uses genetic programming to automatically evolve a utility function for a specific system, set of resource usage metrics, and precision/recall preference. These metrics are computed using sensor values that monitor a variety of system resources (e.g., memory usage, processor usage, thread count). The technique allows users to specify the relative importance of precision and recall, and builds a utility function to meet those requirements. We evaluated the technique on the open source Jigsaw web server using ten resource usage metrics and five anomalous\u00a0\u2026", "num_citations": "14\n", "authors": ["342"]}
{"title": "Ga-based parameter tuning for multi-agent systems\n", "abstract": " Motivation. A MANET is a challenging environment for software system designers due to its dynamism and unpredictable nature. Network links can go up and down depending on a variety of physical factors, such as movement of hosts, terrain, weather, interference, or available battery power. Agent based systems, with their runtime flexibility, can adapt to such environments better than centralized systems [4]. On the other hand, the tuning and control of the agent based system is more complicated, due to the flexible and decentralized nature of Multi-Agent Systems (MAS). Since it is unlikely that the optimal agent population composition can be derived theoretically, a search based technique should be used to find acceptable suboptimal solutions rather then guaranteed optimal ones. Many references to the example of information dissemination or collection by agent based systems will be used throughout this paper\u00a0\u2026", "num_citations": "14\n", "authors": ["342"]}
{"title": "A program understanding environment based on the \u201cstar\u201d approach to tool integration\n", "abstract": " CSC'94: Proceedings of the 22nd annual ACM computer science conference on Scaling up: meeting the challenge of complexity in real-world computing applications: meeting the challenge of complexity in real-world computing applications March 1994 Pages 60\u201365 https://doi. org/10.1145/197530.197557", "num_citations": "14\n", "authors": ["342"]}
{"title": "Repairing software style using graph grammars\n", "abstract": " Often, software architects impose a particular style on the software systems they design. For large software systems, they would like to ensure that the design continues to conform to this style during the maintenance phase of the software-life cycle.We will assume that the architectural design of a software system is available; for instance, it may have been extracted from the source code of the system using a parser. We will also assume we have a set of stylistic constraints given by the architect. For example, the architect may want to ensure that if a module X is allowed to use a procedure in a module Y, then module Y needs to export that procedure. We define the Style Repair Problem as follows: If the current architectural design does not satisfy a set of stylistic constraints, how can we repair it so that it does? We choose to represent architectural designs as directed graphs; hence, repairing the style of these designs is equivalent to repairing the graph. We show how graph grammars can be used to automatically repair styles, and we show how this provides insight into the problem of style maintenance.", "num_citations": "13\n", "authors": ["342"]}
{"title": "Extending programming environments to support architectural design\n", "abstract": " As software systems grow in size and complexity, the demand for languages and tools to capture higher order abstractions than those supported by programming languages increases. One of these abstractions is the architectural design, which specifies a system's components, their interfaces, and their interrelationships using textual or visual notations. Although there have been significant advances in programming languages and environments, research into languages and tools for architectural design is still preliminary. Moreover there has been little emphasis on integrating design tools with existing programming environments. The paper describes how the Object Oriented Turing programming environment was extended to accommodate languages and tools for specifying and visualizing architectural designs.< >", "num_citations": "13\n", "authors": ["342"]}
{"title": "A case study on the automatic composition of network application mashups\n", "abstract": " MaxMash is a tool that can compose select features of networked application and generate the source code for application mashups that can integrate those features. This paper presents a case study that demonstrates how MaxMash is used to combine the Jabber chatting protocol and the Microsoft Maps Web application. The composed mashup is able to answer direction queries via a chat client.", "num_citations": "11\n", "authors": ["342"]}
{"title": "Multi-channel change-point malware detection\n", "abstract": " The complex computing systems employed by governments, corporations, and other institutions are frequently targeted by cyber-attacks designed for espionage and sabotage. The malicious software used in such attacks are typically custom-designed or obfuscated to avoid detection by traditional antivirus software. Our goal is to create a malware detection system that can quickly and accurately detect such otherwise difficult-to-detect malware. We pose the problem of malware detection as a multi-channel change-point detection problem, wherein the goal is to identify the point in time when a system changes from a known clean state to an infected state. We present a host-based malware detection system designed to run at the hypervisor level, monitoring hypervisor and guest operating system sensors and sequentially determining whether the host is infected. We present a case study wherein the detection system\u00a0\u2026", "num_citations": "10\n", "authors": ["342"]}
{"title": "Inoculation against malware infection using kernel-level software sensors\n", "abstract": " We present a technique for dynamic malware detection that relies on a set of sensors that monitor the interaction of applications with the underlying operating system. By monitoring the requests that each process makes to kernel-level operating system functions, we build a statistical model that describes both clean and infected systems in terms of the distribution of data collected from each sensor. The model parameters are learned from labeled training data gathered from machines infected with canonical samples of malware. We present a technique for detecting malware using the Neyman-Pearson test from classical detection theory. This technique classifies a system as either clean or infected at runtime as measurements are collected from the sensors. We provide experimental results that illustrate the effectiveness of this technique for a selection of malware samples. Additionally, we provide a performance\u00a0\u2026", "num_citations": "10\n", "authors": ["342"]}
{"title": "A survey of reverse engineering tools for the 32-bit Microsoft Windows environment\n", "abstract": " Reverse engineering is defined by Chikosfky and Cross as the process of analyzing a subject system to identify the system\u2019s components and their relationships, and to create representations of the system in another form or at a higher level of abstraction. The process of reverse engineering is accomplished using specific tools that, for the 32-bit Microsoft Windows environment, are categorized as hex editors, disassemblers/debuggers, decompilers, or related technologies such as code obfuscators, unpackers, and PE editors. An evaluation of each tool is provided that identifies its domain of applicability and usability.", "num_citations": "10\n", "authors": ["342"]}
{"title": "ISF: A visual formalism for specifying interconnection styles for software design\n", "abstract": " We have developed a framework for specifying high-level software designs. The core of the framework is a very simple visual notation.  This notation enables designers to document designs as labelled rectangles and directed edges.  In addition to the notation, our framework features a supporting formalism, called ISF (Interconnection Style Formalism).  This formalism enables designers to customize the simple design notation by specifying the type of entities, relations, legal configurations of entities and relations, as well as scoping rules of the custom notation.         In this paper we present the formal definition of ISF and use ISF to specify two custom design notations.  We also describe how ISF specifications, using deductive database technology, are used to generate supporting tools for these custom notations.", "num_citations": "10\n", "authors": ["342"]}
{"title": "Malware anomaly detection on virtual assistants\n", "abstract": " This work explores the application of anomaly detection techniques, specifically one-class support vector machine (SVM) and online change-point detection, to construct a model that can distinguish, in real-time, between the normal operation of an Amazon Alexa Virtual Assistant IoT device from anomalous operation due to malware infections. Despite the current absence of widespread malware for IoT devices, the anticipated rapid growth in deployment and use of IoT devices will likely attract many different malware attacks in the near future. Because of their highly specialized and, hence, predictable expected behavior, malware detection on IoT devices is not difficult given large training sets, long testing vectors, and extensive computational power. The challenge we address in this paper is to ascertain how quickly malware may be detected, i.e., the distribution on the number of system calls before a suitably high\u00a0\u2026", "num_citations": "9\n", "authors": ["342"]}
{"title": "Diagnosis of software failures using computational geometry\n", "abstract": " Complex software systems have become commonplace in modern organizations and are considered critical to their daily operations. They are expected to run on a diverse set of platforms while interoperating with a wide variety of other applications. Although there have been advances in the discipline of software engineering, software faults, and malicious attacks still regularly cause system downtime [1]. Downtime of critical applications can create additional work, cause delays, and lead to financial loss [2]. This paper presents a computational geometry technique to tackle the problem of timely failure diagnosis during the execution of a software application. Our approach to failure diagnosis involves collecting a set of software metrics and building a geometric enclosures corresponding to known classes of faults. The geometric enclosures are then used to partition the state space defined by the metrics.", "num_citations": "9\n", "authors": ["342"]}
{"title": "Controlling the interactions of architectural design components using scoping rules.\n", "abstract": " Degree: Ph. D.DegreeYear: 1996Institute: University of Toronto (Canada)Adviser: RC Holt.Software designers rely on programming language constructs and machine readable system models to specify dependencies between the components of a software system. In large software systems with hundreds of components, the overall system structure, called the software architecture or architectural design, may become obfuscated by the numerous components and their inter-dependencies. For this reason software designers often use informal diagrams to separately describe software architectures.", "num_citations": "9\n", "authors": ["342"]}
{"title": "thr2csp: Toward Transforming Threads into Communicating Sequential Processes\n", "abstract": " As multicore and heterogeneous multiprocessor platforms replace uniprocessor systems, software programs must be designed with a greater emphasis on concurrency. Threading has become the dominant paradigm of concurrent computation in the most popular programming languages. Large threaded programs are known to be difficult to implement correctly, comprehend, and maintain, while concurrent programs written in process algebraic paradigms of concurrency, such as communicating sequential processes, are known to be easier to analyze. This paper presents our initial work on reverse engineering threaded source code and transforming the code into functionally-equivalent message-passing code. The paper also explores future work needed to convert the message-passing code into communicating sequential processes.", "num_citations": "8\n", "authors": ["342"]}
{"title": "On evaluating the efficiency of software feature development using algebraic manifolds\n", "abstract": " Managers are often unable to explain objectively why or when effort was misplaced during the development process. In this paper, we present a formal technique to depict the expended effort during the life-cycle of a software feature using feature development manifolds (FDMs). Using the FDMs we can compute the preferred development path for a given feature. This development path includes the versions of a software feature that contributed to the final version of the feature in a positive way. The preferred development path excludes versions of the software feature that should have been skipped. Once the preferred development path is computed the amount of wasted effort can be quantified using the metric that we have developed. We demonstrate the effectiveness of our approach to compute wasted software feature development by applying our technique to two large open source software systems, Gaim and\u00a0\u2026", "num_citations": "8\n", "authors": ["342"]}
{"title": "A genetic algorithm for solving the binning problem in networked applications detection\n", "abstract": " Network administrators need a tool that detects the kind of applications running on their networks, in order to allocate resources and enforce security policies. Previous work shows that applications can be detected by analyzing packet size distributions. Detection by packet size distribution is more efficient and accurate if the distribution is binned. An unbinned packet size distribution considers the occurrences of each packet size individually. In contrast, a binned packet size distribution considers the occurrences of packets within packet size ranges. This paper reviews some of the common methods for binning distributions and presents an improved approach to binning using a genetic algorithms to assist the detection of network applications.", "num_citations": "8\n", "authors": ["342"]}
{"title": "A multi-dimensional taxonomy of software development environments\n", "abstract": " A Software Development Environment (SDE) is a set of tools that, at the very least, supports coding and possibly other software development activities. Related to SDEs are meta-SDEs, which are classes of SDEs that must be configured or populated by tools before they can be useful. We will use the generic term environment to refer to both SDEs and meta-SDEs. This paper presents a multi-dimensional taxonomy of environments. The primary dimensions of our taxonomy are scale and genericity. Scale distinguishes environments that are suitable for small-scale programming from those that are suitable for large-scale software development. Genericity differentiates monolithic environments from highly configurable and extendible ones. Secondary taxonomy dimensions include tool integration, which identifies the degree of interoperability and data sharing between tools, and the historical dimension, which gives\u00a0\u2026", "num_citations": "8\n", "authors": ["342"]}
{"title": "Using search methods for selecting and combining software sensors to improve fault detection in autonomic systems\n", "abstract": " Fault-detection approaches in autonomic systems typically rely on runtime software sensors to compute metrics for CPU utilization, memory usage, network throughput, and so on. One detection approach uses data collected by the runtime sensors to construct a convex-hull geometric object whose interior represents the normal execution of the monitored application. The approach detects faults by classifying the current application state as being either inside or outside of the convex hull. However, due to the computational complexity of creating a convex hull in multi-dimensional space, the convex-hull approach is limited to a few metrics. Therefore, not all sensors can be used to detect faults and so some must be dropped or combined with others. This paper compares the effectiveness of genetic-programming, genetic-algorithm, and random-search approaches in solving the problem of selecting sensors and\u00a0\u2026", "num_citations": "7\n", "authors": ["342"]}
{"title": "Task dependency of user perceived utility in autonomic VoIP systems\n", "abstract": " The transmission of voice-over-Internet protocol (VoIP) network traffic is used in an increasing variety of applications and settings. Many of these applications involve communications where VoIP systems are deployed under unpredictable conditions with poor network support. These conditions make it difficult for users to configure and optimize VoIP systems and this creates a need for self configuring and self optimizing systems. To build an autonomic system for VoIP communications, it is valuable to be able to measure the user perceived utility of a system. In this paper we identify factors important to the estimation of user perceived utility in task dependent VoIP communications.", "num_citations": "7\n", "authors": ["342"]}
{"title": "A collaborative bachelor's degree in software engineering\n", "abstract": " This paper discusses a new Bachelor of Science in Software Engineering (BSSE) that is offered via a collaboration of three departments of one university. The sponsors span the disciplinary areas of computer science, computer engineering, and information systems. The combination of disciplinary areas helps provide a broad foundation for the program. At the same time, while the idea of joint sponsorship may make sense, putting that idea into practice has its difficulties. Academic units have differences in organizational culture, and disciplines may vary in research tradition and curricular expectations. Administrative issues are also more complicated for a collaborative program. All of these factors may make it more difficult to achieve a satisfactory result. This paper begins with a discussion of the development of the degree program including the process used and issues that had to be addressed along the way. Next\u00a0\u2026", "num_citations": "7\n", "authors": ["342"]}
{"title": "Using tube graphs to model architectural designs of software systems\n", "abstract": " A tube graph is a mathematical entity that can be used for modelling architectural designs of software systems. It consists of a tree (representing containment) with a set of edges called tubes (representing dependencies) between the tree's vertices (representing components). Tubes, for example, can model the import relation at the module level and the use dependency relation at the subsystem level. Well-formedness constraints are imposed on tube edges to de ne restricted tube graphs that model particular styles of architectural designs. In this paper, the well-formedness constraints de ne tube graphs that model SIL language architectural designs. Di erent mathematical formulations of these well-formed tube graphs are given and proven to be equivalent. Algorithms for manipulating tube graphs, for possible use in a software development environment, are also given and their complexities are shown to be modest in most cases.", "num_citations": "7\n", "authors": ["342"]}
{"title": "A Framework For Specifying and Visualizing Architectural Designs\n", "abstract": " Architectural designs specify the components of a software system, their interfaces, and their interrelationships. Module Interconnection Languages (MILs) are useful for specifying architectural designs, but lack an intuitive visual representation similar to the visual design notations found in CASE tools. This paper presents a framework for formally de ning the syntax and semantics of languages for specifying and visualizing architectural designs. Also described are an instance and prototype implementation of this framework consisting of two languages: one for specifying designs and one for visualizing them.", "num_citations": "7\n", "authors": ["342"]}
{"title": "don\u2019t trust your router: Detecting compromised router\n", "abstract": " Safeguarding one\u2019s router has received very little attention despite a plethora of router-specific malware, which has emerged recently. Here, we propose a systematic approach to distinguish a router infected by malware from a healthy router. Our key novelty is that we analyze the behavior of the router, thus not relying on binary signatures (like anti-virus software for computers). Our contribution is two fold. First, we develop a non-trivial emulation capability, to observe the behavior of a router. This capability allows to instantiate a virtual router with or without malware and feed it a pre-recorded data trace. This setup monitors the behavior at multiple layers including: OS system calls, process information, and the network layer. Second, using the emulated environment, we provide initial evidence that a behavior-based method can distinguish between infected and healthy routers. We have collected 820 router-specific malware binaries and an initial set of real data traces. We find that infected routers exhibit:(a) an initial spike and an overall 50% increase in the number of system calls,(b) an initial spike and a modest increase in the number of active processes. Our preliminary work is a promising step towards understanding and securing routers against malware infections.", "num_citations": "6\n", "authors": ["342"]}
{"title": "Software analysis for security\n", "abstract": " This is a survey of the processes, practices, and technologies that can help software maintenance engineers improve the security of software systems. A particular emphasis is placed on validating security architectures, verifying that the implementation of an architecturepsilas constituent applications adhere to secure coding practices, and protecting software systems against malicious software. In addition to surveying the state-of-the-art, research challenges pertaining to software security are posed to the software maintenance research community.", "num_citations": "6\n", "authors": ["342"]}
{"title": "Toward a generic framework for computing subsystem interfaces\n", "abstract": " One aspect of reverse engineering deals with the recovery of high-level design information from source code. In this process, source code analysis is used to extract the components and dependencies that comprise a software system. Clustering algorithms are then used to group related software components into composite components, called subsystems. The product of this process is a subsystem decomposition hierarchy.This paper describes work in progress of a technique for computing the interfaces (eg, exported contents) of subsystems that were created during the reverse engineering process. These computed interfaces often reveal to software developers unexpected and undesirable interactions between software components, which is often the result of long-term ad hoc software maintenance practices. We realize that there is no single definition of a subsystem interface and that a variety of styles of\u00a0\u2026", "num_citations": "6\n", "authors": ["342"]}
{"title": "Clustering module dependency graphs of software systems using the bunch tool\n", "abstract": " In this demonstration we will show how our tool (Bunch), along with other tools for source code analysis and graph visualization, can be used to recover the high-level structure of a software system directly from its source code. We accomplish this task by first using a source code analysis system (eg, CIA, Acacia) to produce a module dependency graph that represents the system modules and module-level inter-relationships. We then use this graph as input to Bunch, which partitions the graph. The resultant clustered graph is displayed using a graph visualization tool (eg, dotty, Tom Sawyer).", "num_citations": "5\n", "authors": ["342"]}
{"title": "Customizable notations for software design\n", "abstract": " ISF is a visual formalism for creating custom notations for high-level software design. In this paper we describe the formal semantics of ISF in Datalog. The semantics provides useful insight on how deductive database technology can be used to generate tools to support our custom notations.", "num_citations": "5\n", "authors": ["342"]}
{"title": "Loosely integrating tools using the star system.\n", "abstract": " A developer must often be able to understand a software system at several levels of detail| from abstract system architecture down to source code| before being able to change system functionality or x bugs. Each of these levels might require its own notation and tools. For example, source code, written in a programming language, is viewed and manipulated using browsers and editors, whereas architectural designs, expressed as diagrams, are viewed and manipulated using visual editors. In this paper we show how a multi-view perspective of a software system can be realized, simply and economically, by loosely integrating a set of existing independent tools. Ideally, all environments would be designed a priori to allow their combination into tightly integrated systems. In actual fact, it is common that developers of individual tools cannot foresee the ways in which their tools will be used. People who integrate such tools must exploit what is available to them. The Star system demonstrates how loose integration among existing tools can rapidly and simply result in useful environments for software development. 1", "num_citations": "5\n", "authors": ["342"]}
{"title": "Multicolour programming and metamorphic programming: object oriented programming-in-the-large\n", "abstract": " A key difficulty in programming-in-the-large is the lack of a framework in which to place the rich set of concepts entailed in the software development cycle. We propose such a framework, one that supports separation of concerns (for example, among distinct phases such as analysis and design) in an integrated environment. Our multicolour programming approach splits programming into levels each of which is designated by a particular colour. The upper levels correspond to programming-in-the-large while the lowest levels correspond to programming-in-the-small. From high to low levels, these are: Brown Libraries and projects, Black Modules and relations among them, Green Interfaces to individual modules, Blue Specification and coding of individual modules. We also have Red programming, which corresponds to the making (compiling and linking) of individual software products or releases. Distinct from these\u00a0\u2026", "num_citations": "5\n", "authors": ["342"]}
{"title": "Automatically Transforming GNU C Source Code\n", "abstract": " To perform automated transformation techniques on production quality GNU C source code, non-trivial normalizations must occur. The syntax of GNU C contains inherent ambiguity that must be overcome. The techniques used by an automated transformation tool, Gemini, are presented.", "num_citations": "4\n", "authors": ["342"]}
{"title": "Demonstration of COSAK static analysis tools\n", "abstract": " A software vulnerability is a fault in the specification, implementation, or configuration of a software system whose execution can violate an explicit or implicit security policy. Users typically focus on the functionality of software rather than its security posture. Hence, vulnerabilities often escape their attention until the software is exploited by specially written malicious code. Code auditing is one solution which has been tried with some success in systems such as the OpenBSD operating system. Code audits involve the review of source code by experts in search of vulnerabilities. These audits are reoccurring, namely each revision of the software requires reexamination, and expensive because code audits are labor intensive. Auditors would benefit from a tool which enables them to focus their attention on high-risk areas, thus reducing the amount of code that needs to be audited. The article shows how the tools\u00a0\u2026", "num_citations": "4\n", "authors": ["342"]}
{"title": "Creating a jointly sponsored Master of Science in Software Engineering\n", "abstract": " This paper discusses a Master of Science in Software Engineering that is jointly sponsored by three colleges of Drexel University. The sponsors span the disciplinary areas of computer science, electrical and computer engineering, and information systems. The program is a product of the synergy that exists among these three colleges to support software engineering. The combination of disciplinary areas helps provide a broad and deep foundation for the program. The paper begins with a discussion of the development of the degree program including the process used and issues that had to be addressed along the way. Next, the curricular content of the degree is outlined, with particular attention to contribution and perspective provided by each of the degree sponsors. Finally, the paper discusses developments that will affect the development of software engineering programs today.", "num_citations": "4\n", "authors": ["342"]}
{"title": "Algorithms for Managing the Evolution of Software Designs\n", "abstract": " Most software systems evolve, with time, in response to changing functional and non-function requirements. This evolution a ects both the dynamic behavior of the system as well as the form of the static software artifacts (eg, source code, user documentation, requirements speci cations, design diagrams, test scripts) that comprise the system.A major challenge facing software practitioners is being able to evolve software artifacts to support new system features without corrupting these artifacts. For example, some questions that may arise during the design evolution process are: How does the introduction of a new module or subsystem a ect the structure of a software design? Does the addition of such components violate any constraints imposed by the design notation? Can the design be\\repaired\" automatically, if necessary, to satisfy the design constraints? Have any redundant components or relations been added by mistake?", "num_citations": "4\n", "authors": ["342"]}
{"title": "A visual programming environment for Object-Oriented Turing.\n", "abstract": " Degree: M. Sc.DegreeYear: 1992Institute: University of Toronto (Canada)In this thesis we propose a programming environment for the Object-Oriented Turing programming language, called OOT, that has tools to support both small and large scale programming. OOT is a window based programming environment for the Object-Oriented Turing programming language. It supports tools for small scale programming such as editing, compiling, running, error viewing and directory browsing. It also supports features such as implicit selective compilation which is especially useful for large scale programming.", "num_citations": "4\n", "authors": ["342"]}
{"title": "Perception of Utility in Autonomic VoIP Systems\n", "abstract": " The transmission of voice-over-Internet protocol (VoIP) network traffic is used in an increasing variety of applications and settings. Many of these applications involve communications where VoIP systems are deployed under unpredictable conditions with poor network support. These conditions make it difficult for users to configure and optimize VoIP systems and this creates a need for self configuring and self optimizing systems. To build an autonomic system for VoIP communications, it is valuable to be able to measure the user perceived utility of a system. In this paper we identify factors important to the estimation of user perceived utility in task dependent VoIP communications.", "num_citations": "3\n", "authors": ["342"]}
{"title": "An Approach to Comprehending Networked Applications through Analogy\n", "abstract": " Distributed applications rely on packet-switched networks to connect their various elements. This paper describes a technique that can help software engineers and network administrators characterize unfamiliar networked applications by matching them to a single, or a combination of several, analogous and familiar networked applications). This matching is based on the size distribution of the packets sent and received by the application undergoing scrutiny.", "num_citations": "3\n", "authors": ["342"]}
{"title": "On Controlling the Interactions of Software Components: An Evolutionary Perspective\n", "abstract": " Being able to control the interactions among software components is essential for the successful initial development and subsequent maintenance of large and complex software systems. The control of these interactions occurs at various levels of a software speci cation. In this paper we describe how notations are used to specify constraints on the interactions of software components at the architectural, module, and block structure level. Our presentation gives an evolutionary perspective by showing that scoping rules for constraining architectural interactions have evolved from rules for constraining module interactions, which in turn have evolved from scoping rules for controlling the interactions that occur between components in block structured language.", "num_citations": "3\n", "authors": ["342"]}
{"title": "Exploring Paraphrasing Techniques on Formal Language for Generating Semantics Preserving Source Code Transformations\n", "abstract": " Automatically identifying and generating equivalent semantic content to a word, phrase, or sentence is an important part of natural language processing (NLP). The research done so far in paraphrases in NLP has been focused exclusively on textual data, but has significant potential if it is applied to formal languages like source code. In this paper, we present a novel technique for generating source code transformations via the use of paraphrases. We explore how to extract and validate source code paraphrases. The transformations can be used for stylometry tasks and processes like refactoring. A machine learning method of identifying valid transformations has the advantage of avoiding the generation of transformations by hand and is more likely to have more valid transformations. Our dataset is comprised by 27,300 C++ source code files, consisting of 273topics each with 10 parallel files. This generates\u00a0\u2026", "num_citations": "2\n", "authors": ["342"]}
{"title": "On the Detection of Malware on Virtual Assistants Based on Behavioral Anomalies\n", "abstract": " The Internet of Things (IoT) refers to the growing network of``smart objects.\" The increase in popularity of IoT devices, due to their efficiency and convenience, has given rise to new security concerns. The variety and novelty of IoT devices provide a corpus of malware that is of insufficient size to employ classic machine learning algorithms. This makes anomaly detection methods for IoT device security more attractive, especially in the short term, until there are enough behavioral signatures for malware to train more sophisticated machine learning detection models for these devices. This thesis explores some of the security concerns pertaining to running software similar to Amazon Alexa home assistant on IoT-like platforms. We implement a behavioral-based malware detector and compare the effectiveness of different system attributes that are used in detecting malware, ie, system calls, network traffic, and the\u00a0\u2026", "num_citations": "2\n", "authors": ["342"]}
{"title": "On the effectiveness of application characteristics in the automatic classification of malware on smartphones\n", "abstract": " The increase in smartphone usage is providing impetus to malicious actors to target these devices via malware injection. This can be seen in the increasing number of malware identified in the past few years. Android, being the most commonly used platform and one that provides an open architecture, makes it the most common target for malware developers. One possible method to identify malicious code is to use the characteristics of an application such as permissions to identify an application's disposition. This paper describes a method for using application characteristics to classify sample applications as either benign or malware. Both binary and familial classification of malware samples is performed to determine whether each sample is malware or not (i.e., binary classification) and what is the familial provenance of the malware sample (i.e. familial classification). The results compare the effectiveness of the\u00a0\u2026", "num_citations": "2\n", "authors": ["342"]}
{"title": "On the maintenance of UI-integrated mashup applications\n", "abstract": " This paper describes the maintenance of long-lived mashup applications that are integrated at the user interface (UI) layer. It presents techniques that help mashup developers to maintain applications by identifying when and how the original applications' UIs change. It describes a novel mashup editing environment that can be used to create, share, and edit mashups. This paper also presents an experiment that demonstrates our approach's ability to track UI changes as an application evolves and a demonstration of the effort expanded by developers to maintain mashups as the applications used by the mashups evolve.", "num_citations": "2\n", "authors": ["342"]}
{"title": "Malware Detection using Behavioral Whitelisting of Computer Systems\n", "abstract": " Malware detection has been an active area of research for a long time. With the rapid growth of self-mutating malware, many malware-detection tools fail quickly or have a high rate of false positives. Our work tackles the problem differently by creating anomaly detectors for computer systems. Since the number of potential malware far exceeds the number of benign software on any given computer system, our thesis is that it is possible to efficiently detect malware as anomalies in the expected behavior of computer systems hosting only benign software. This is in contrast to traditional approaches that attempt to construct behavioral models for every possible instance or type of malware.", "num_citations": "1\n", "authors": ["342"]}
{"title": "Automatic Malware Detection on an Alexa-Pi IoT Device\n", "abstract": " This work explores some of the security concerns pertaining to running software similar to Amazon Alexa home assistant on IoT-like platforms. We implement a behavioral-based malware detector and compare the effectiveness of different system attributes that are used in detecting malware, ie, system calls, network traffic, and the integration of system call and network traffic features. Given the small number of malware samples for IoT devices, we create a parameterizable malware sample that mimics Alexa behavior to varying degrees, while exfiltrating data from the device to a remote host. The performance of our anomaly detector is evaluated based on how well it determines the presence of our parameterized malware on an Alexa-enabled IoT device.", "num_citations": "1\n", "authors": ["342"]}
{"title": "Action Languages and the Mitigation of Malware\n", "abstract": " Automating malware mitigation requires taking into account potentially intricate dependencies among the system\u2019s components, understanding potential side-effects of the possible actions, and ensuring that required system functionalities are preserved. Answers still need to be found for fundamental questions: What does it mean to mitigate malware? When can one claim that malware has been mitigated? What are the side-effects of a mitigation strategy? This paper aims to demonstrate that techniques from reasoning about actions and change can provide the means to create a precise characterization of the notion of mitigation and by defining corresponding algorithms. The key observation underlying our work is that a computer system can be viewed as a dynamic system, ie, a system whose state changes over time. Taking this perspective makes it possible to leverage the techniques for reasoning about actions and change to model, and reason about, system components and malware. Furthermore, efficient computation can be achieved by relying on encodings based on Answer Set Programming.", "num_citations": "1\n", "authors": ["342"]}
{"title": "On the automatic recovery of style-specific architectural relations in software systems\n", "abstract": " The cost of maintaining a software system over a long period of time far exceeds its initial development cost. Much of the maintenance cost is attributed to the time required by new developers to understand legacy systems. High-level structural information helps maintainers navigate through the numerous low-level components and relations present in the source code. Modularization tools can be used to produce subsystem decompositions from the source code but do not typically produce high-level architectural relations between the newly found subsystems. Controlling subsystem interactions is one important way in which the overall complexity of software maintenance can be reduced.               We have developed a tool, called ARIS (Architecture Relation Inference System), that enables software engineers to define rules and relations for regulating subsystem interactions. These rules and relations are\u00a0\u2026", "num_citations": "1\n", "authors": ["342"]}
{"title": "Using networked general-purpose workstations to improve the performance of software clustering algorithms\n", "abstract": " Collections of general purpose networked workstations o er processing capability that often rivals or exceeds supercomputers. Since networked workstations are readily available in most organizations, they provide and economic and scalable alternative to parallel machines. In this paper we discuss how individual nodes in a computer network can be used as a collection of connected processing elements to improve the performance of a software engineering tool that we developed. Our tool, called Bunch, automatically clusters the structure of software systems into a hierarchy of subsystems. Clustering helps developers understand the structure of complex systems by providing them with highlevel abstract (clustered) views of the software structure. The algorithms used by Bunch are computationally intensive and, hence, we would like to improve our tool's performance for clustering large systems. This paper\u00a0\u2026", "num_citations": "1\n", "authors": ["342"]}