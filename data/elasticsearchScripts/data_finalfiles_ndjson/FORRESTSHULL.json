{"title": "Investigating the impact of design debt on software quality\n", "abstract": " Technical debt is a metaphor describing situations where developers accept sacrifices in one dimension of development (eg software quality) in order to optimize another dimension (eg implementing necessary features before a deadline). Approaches, such as code smell detection, have been developed to identify particular kinds of debt, eg design debt. What has not yet been understood is the impact design debt has on the quality of a software product. Answering this question is important for understanding how growing debt affects a software product and how it slows down development, eg though introducing rework such as fixing bugs. In this case study we investigate how design debt, in the form of god classes, affects the maintainability and correctness of software products by studying two sample applications of a small-size software development company. The results show that god classes are changed more\u00a0\u2026", "num_citations": "173\n", "authors": ["159"]}
{"title": "To game or not to game?\n", "abstract": " One challenge in software engineering education is to give students sufficient hands-on experience in actually building software. This is necessary so that students can understand which practices and techniques are useful in various situations. Some researchers have advocated alternative teaching methods to help in this regard. If successful, such methods could give students some experience with different approaches' effects in a shorter, more constrained time period. We examine one such approach, game-based learning, here.", "num_citations": "133\n", "authors": ["159"]}
{"title": "Building empirical support for automated code smell detection\n", "abstract": " Identifying refactoring opportunities in software systems is an important activity in today's agile development environments. The concept of code smells has been proposed to characterize different types of design shortcomings in code. Additionally, metric-based detection algorithms claim to identify the\" smelly\" components automatically. This paper presents results for an empirical study performed in a commercial environment. The study investigates the way professional software developers detect god class code smells, then compares these results to automatic classification. The results show that, even though the subjects perceive detecting god classes as an easy task, the agreement for the classification is low. Misplaced methods are a strong driver for letting subjects identify god classes as such. Earlier proposed metric-based detection approaches performed well compared to the human classification. These\u00a0\u2026", "num_citations": "123\n", "authors": ["159"]}
{"title": "Prioritizing design debt investment opportunities\n", "abstract": " Technical debt is the technical work developers owe a system, typically caused by speeding up development, eg before a software release. Approaches, such as code smell detection, have been developed to identify particular kinds of debt, eg design debt. Up until now, code smell detection has been used to help point to components that need to be freed from debt by refactoring. To date, a number of methods have been described for finding code smells in a system. However, typical debt properties, such as the value of the debt and interest rate to be paid, have not been well established. This position paper proposes an approach to using cost/benefit analysis to prioritize technical debt reduction work by ranking the value and interest of design debt caused by god classes. The method is based on metric analysis and software repository mining and is demonstrated on a commercial software application at a mid-size\u00a0\u2026", "num_citations": "95\n", "authors": ["159"]}
{"title": "Defect categorization: making use of a decade of widely varying historical data\n", "abstract": " This paper describes our experience in aggregating a number of historical datasets containing inspection defect data using different categorization schemes. Our goal was to make use of the historical data by creating models to guide future development projects. We describe our approach to reconciling the different choices used in the historical datasets to categorize defects, and the challenges we faced. We also present a set of recommendations for others involved in classifying defects.", "num_citations": "69\n", "authors": ["159"]}
{"title": "Practical considerations, challenges, and requirements of tool-support for managing technical debt\n", "abstract": " Developing a software product with a high level of quality that also meets budget and schedule is the main goal of any organization. This usually implies making tradeoffs among conflicting aspects like number of features to implement, user perceived quality, time-to-market, and the ability of the company to maintain and improve the system in a feasible way in the future (aka, managing Technical Debt (TD)). In this paper we present a fresh perspective on TD from a CMMI Maturity Level 5 company. Examples, practical considerations, and challenges in dealing with TD are presented along with ten requirements of a tool for managing TD.", "num_citations": "58\n", "authors": ["159"]}
{"title": "A practical approach for quality-driven inspections\n", "abstract": " Software inspection is a rigorous process for validating software work products that's both efficient and cost effective. However, this process presents challenges that might keep software developers from continuing to implement inspections. From our experience, major reasons for this are poor or missing customization of inspections for given context characteristics and insufficient stakeholder involvement. The TAQtIC (Tailoring Approach for Quality-Driven Inspections) inspection approach lets organizations implement inspections in a sustainable way in a given organizational context. Practitioners can use TAQtIC's underlying concepts to customize inspections for their environment. Experiences from projects using this approach demonstrate how organizations can tailor inspections of their software products", "num_citations": "53\n", "authors": ["159"]}
{"title": "A family of experiments to investigate the influence of context on the effect of inspection techniques\n", "abstract": " For a growing population of researchers in software engineering, empirical studies have become a key approach of research. Empirical studies may be used, for example, to evaluate technologies and help to direct further research by revealing what problems and difficulties people have in practice. Without empirical studies, we have to rely only on intuition or educated opinion.Individual empirical studies often yield interesting results for their particular context, but typically this context is not described in sufficient detail to decide whether another context is similar enough to apply the conclusions of the study also there. We argue instead that families of experiments with a common framework for collecting context data are necessary in order to abstract conclusions at a useful level of detail.", "num_citations": "52\n", "authors": ["159"]}
{"title": "Domain-specific tailoring of code smells: an empirical study\n", "abstract": " Code smells refer to commonly occurring patterns in source code that indicate poor programming practices or code decay. Detecting code smells helps developers find design problems that can cause trouble in future maintenance. Detection rules for code smells, based on software metrics, have been proposed, but they do not take domain-specific characteristics into consideration. In this study we investigate whether such generic heuristics can be tailored to include domain-specific factors. Input into these domain-specific heuristics comes from an iterative empirical field study in a software maintenance project. The results yield valuable insight into code smell detection.", "num_citations": "48\n", "authors": ["159"]}
{"title": "A snapshot of the state of practice in software development for medical devices\n", "abstract": " The medical device industry is a constantly growing domain which makes use of more and more software products. Given the importance to this industry of dependable software components, rigorous software engineering techniques would seem to have an important role to play. However, in a recent survey of the industry we found a lower than expected rate of adoption of certain sound software engineering practices. To ensure and improve the quality of developed software products, whether they are standalone applications or embedded in complex systems, both the medical device industry as well as software engineers have to take action. Our survey is a first step in this direction and may help in identifying appropriate programs and future research topics.", "num_citations": "46\n", "authors": ["159"]}
{"title": "Software inspections, reviews & walkthroughs\n", "abstract": " While software has become one of the most valuable products of the past decades, its growing complexity and size is responsible for making it one of the most challenging ones to build and maintain. The challenge stems from the fact that software development belongs to the most labor-and, at the same time, knowledge-intensive processes of today's world. The heavy dependence on knowledgeable human beings may be one reason why software development is often compared to an art or craft rather than to an engineering discipline. However, it has almost become impossible nowadays for a craftsman to produce large software systems according to a given schedule, to a limited budget, and to the quality requirements of a customer at delivery. Hence, researchers as well as practitioners are increasingly obliged to address the question of how to integrate engineering principles into software development. An\u00a0\u2026", "num_citations": "45\n", "authors": ["159"]}
{"title": "Exploring language support for immutability\n", "abstract": " Programming languages can restrict state change by preventing it entirely (immutability) or by restricting which clients may modify state (read-only restrictions). The benefits of immutability and read-only restrictions in software structures have been long-argued by practicing software engineers, researchers, and programming language designers. However, there are many proposals for language mechanisms for restricting state change, with a remarkable diversity of techniques and goals, and there is little empirical data regarding what practicing software engineers want in their tools and what would benefit them. We systematized the large collection of techniques used by programming languages to help programmers prevent undesired changes in state. We interviewed expert software engineers to discover their expectations and requirements, and found that important requirements, such as expressing immutability\u00a0\u2026", "num_citations": "41\n", "authors": ["159"]}
{"title": "Inspecting the history of inspections: An example of evidence-based technology diffusion\n", "abstract": " If we're going to have a column about evidence in software engineering, we're going to need to talk about inspections sooner or later. Inspections are among the most mature and perhaps best-studied practices in software engineering. In short, software inspection was one of those rare software engineering innovations that had the ability to effect real process change.", "num_citations": "35\n", "authors": ["159"]}
{"title": "Building theories from multiple evidence sources\n", "abstract": " As emphasized in other chapters of this book, useful results in empirical software engineering require a variety of data to be collected through different studies \u2013 focusing on a single context or single metric rarely tells a useful story. But, in each study, the requirements of the local context are liable to impose different constraints on study design, the metrics to be collected, and other factors. Thus, even when all the studies focus on the same phenomenon (say, software quality), such studies can validly collect a number of different measures that are not at all compatible (say, number of defects required to be fixed during development, number of problem reports received from the customer, total amount of effort that needed to be spent on rework). Can anything be done to build a useful body of knowledge from these disparate pieces? This chapter addresses strategies that have been applied to date to draw conclusions\u00a0\u2026", "num_citations": "35\n", "authors": ["159"]}
{"title": "Impact of research on practice in the field of inspections, reviews and walkthroughs: learning from successful industrial uses\n", "abstract": " Software inspections, reviews, and walkthroughs have become a standard process component in many software development domains. Maturity level 3 of the CMM-I requires establishment of peer reviews [12] and substantial sustained improvements in quality and productivity have been reported as a result of using reviews ([16], [21], [22], [27]). The NSF Impact project identifies the degree to which these industrial success cases have been instigated and improved by research in software engineering. This research identifies that there is widespread adoption of inspections, reviews or walkthroughs but that companies do not generally exploit their full potential. However there exist sustained industrial success cases with respect to the wide-spread and measurably successful application of them. It also identifies research in software engineering that can be credibly documented as having influenced the industrial\u00a0\u2026", "num_citations": "32\n", "authors": ["159"]}
{"title": "An empirical approach to best practice identification and selection: the US Department of Defense acquisition best practices clearinghouse\n", "abstract": " Recommendations concerning best practices are ubiquitous in academic literature as well as audit reports and institutional policy. However, information on the effectiveness of recommended practices, or the context within which that effectiveness has been observed, is not readily available. In the rare occasions this information is available, it is not packaged in such a way as to support practitioners. Most organizations and projects do not possess sufficient resources to implement even a fraction of the practices espoused, so often choose to adopt none. There are also few if any tools to support practice selection. This paper describes an approach to identify, analyze, synthesize, and package empirical information about practice effectiveness, and evaluates the knowledge gained from the development of a prototype tool designed to implement that approach.", "num_citations": "26\n", "authors": ["159"]}
{"title": "Victor R. Basili's contributions to software quality\n", "abstract": " This article deals with Victor R. Basili's contributions to software quality. Basili's contributions cover three broad areas: research in the 1970s and early 1980s on software measurement and the Goal Question Metric (GQM) model, research in the 1980s and 1990s on these measurement ideas' maturation into a software engineering model of empirical studies, including the development of the Quality Improvement Paradigm (QIP) and the influence of the NASA Goddard Space Flight Center Software Engineering Laboratory, and research since 1990 in the Experience Factory as a model for creating learning organizations for continuous software process improvement. Some of Basili's most important contributions are in measuring software development processes and products and gifted the community with an invaluable tool: the GQM approach. The GQM approach is based on the assumption that for an organization\u00a0\u2026", "num_citations": "25\n", "authors": ["159"]}
{"title": "A hybrid threat modeling method\n", "abstract": " In FY 2016, the research team evaluated Security Cards, STRIDE (Spoofing identity, Tampering with data, Repudiation, Information disclosure, Denial of service, Elevation of privilege), and persona non grata (PnG) for effectiveness in threat identification. Security Cards is an approach that emphasizes creativity and brainstorming over more structured approaches such as checklists. STRIDE involves modeling a system and subsystem and related data flows. PnGs represent archetypal users who behave in unwanted, possibly nefarious ways. The team used two scenarios: an aircraft maintenance scenario and a drone swarm scenario, both described in this technical note in detail, along with the project outcomes. No individual threat modeling method included all identified threats.The research team subsequently developed the Hybrid Threat Modeling Method (hTMM), considering the desirable characteristics for a Threat Modeling Method. At a high level, the hTMM includes the following steps, described in detail in the technical note:(1) Identify the system you will be threat modeling.(2) Apply Security Cards according to developers\u2019 suggestions.(3) Prune PnGs that are unlikely or for which no realistic attack vectors could be identified.(4) Summarize results from the above steps, utilizing tool support.(5) Continue with a formal risk assessment method.", "num_citations": "24\n", "authors": ["159"]}
{"title": "Getting an intuition for big data\n", "abstract": " IEEE Software Editor-in-Chief Forrest Shull discusses the importance of building reliable systems to interpret big data. In addition, he discusses the IBM Impact 2013 Unconference; the Software Engineering Institute's SATURN 2013 conference in which the IEEE Software Architecture in Practice Award went to Simon Brown of Coding the Architecture, for his presentation titled \"The Conflict between Agile and Architecture: Myth or Reality\" and the IEEE Software New Directions Award went to Darryl Nelson of Raytheon for his presentation titled, \"Next-Gen Web Architecture for the Cloud Era.\" He also welcomes Professor Rafael Prikladnicki of the Computer Science School at PUCRS, Brazil, and Chief Software Economist Walker Royce of IBM's Software Group to the IEEE Software Advisory Board. The first Web extra at http://youtu.be/JrQorWS5m6w is a video interview in which IEEE Software editor in chief Forrest Shull\u00a0\u2026", "num_citations": "23\n", "authors": ["159"]}
{"title": "Perfectionists in a world of finite resources\n", "abstract": " The metaphor of technical debt, originally coined by Ward Cunninghamhas helped me recently get a handle on this type of issue. Almost invariably in software projects, developers can be so focused on accomplishing the needed functionality that the software itself grows less understandable, more complex, and harder to modify. Since this system deterioration usually reflects a lack of activity spent in refactoring, documentation, and other aspects of the project infrastructure, we can view it as a kind of debt that developers owe the system. Ward Cunningham's metaphor helps make clear an important trade-off: although a little debt can speed up software development in the short run, this benefit is achieved at the cost of extra work in the future, as if paying interest on the debt. Technical debt gives us a frame work for thinking about the fact that not doing some good things today, no matter how valuable they seem on\u00a0\u2026", "num_citations": "20\n", "authors": ["159"]}
{"title": "NetDyn revisited: A replicated study of network dynamics\n", "abstract": " In 1992 and 1993, a series of experiments using the NetDyn tool was run at the University of Maryland to characterize network behavior. These studies identified multiple design and implementation faults in the Internet. Since that time, there has been a wide array of changes to the Internet. During the Spring of 1996, we conducted a replication of the NetDyn experiments in order to characterize end-to-end behavior in the current environment. In this paper, we present and discuss the latest results obtained during this study. Although the network seems to be stabilizing with respect to transit times, our current results are similar to the results from past experiments. That is, networks often exhibit unexpected behavior. The data suggest that, while there has been some improvement, there are still problem areas that need to be addressed.", "num_citations": "19\n", "authors": ["159"]}
{"title": "The role of MPI in development time: a case study\n", "abstract": " There is widespread belief in the computer science community that MPI is a difficult and time-intensive approach to developing parallel software. Nevertheless, MPI remains the dominant programming model for HPC systems, and many projects have made effective use of it. It remains unknown how much impact the use of MPI truly has on the productivity of computational scientists. In this paper, we examine a mature, ongoing HPC project, the Flash Center at the University of Chicago, to understand how MPI is used and to estimate the time that programmers spend on MPI-related issues during development. Our analysis is based on an examination of the source code, version control history, and regression testing history of the software. Based on our study, we estimate that about 20% of the development effort is related to MPI. This implies a maximum productivity improvement of 25% for switching to an alternate\u00a0\u2026", "num_citations": "18\n", "authors": ["159"]}
{"title": "A survey of software engineering techniques in medical device development\n", "abstract": " A wide variety of the functions provided by today's medical devices relies heavily on software. Most of these capabilities could not be offered without the underlying integrated software solutions. As a result, the medical device industry has become highly interdisciplinary. Medical device manufacturers are finding an increasing need to incorporate the research ideas and results from traditionally disconnected research areas such as medicine, software and system engineering, and mechanical engineering. In 2006, we conducted a survey with more than 100 companies from Europe and the USA to shine some light on the current status of the integration of software engineering technologies into the medical device domain. The initial results of this survey are presented in this paper. Both software engineers and the medical device industry can use these findings to better understand current challenges and future\u00a0\u2026", "num_citations": "17\n", "authors": ["159"]}
{"title": "Using the ISO/IEC 9126 product quality model to classify defects: A controlled experiment\n", "abstract": " Background: Existing software defect classification schemes support multiple tasks, such as root cause analysis and process improvement guidance. However, existing schemes do not assist in assigning defects to a broad range of high level software goals, such as software quality characteristics like functionality, maintainability, and usability. Aim: We investigate whether a classification based on the ISO/IEC 9126 software product quality model is reliable and useful to link defects to quality aspects impacted. Method: Six different subjects, divided in two groups with respect to their expertise, classified 78 defects from an industrial web application using the ISO/IEC 9126 quality main characteristics and sub-characteristics, and a set of proposed extended guidelines. Results: The ISO/IEC 9126 model is reasonably reliable when used to classify defects, even using incomplete defect reports. Reliability and variability is\u00a0\u2026", "num_citations": "15\n", "authors": ["159"]}
{"title": "Software reading techniques\n", "abstract": " Software reading is defined as the process by which a developer gains an understanding of the information encoded in a work product sufficient to accomplish a particular task. Software reading is an important skill in software development, occurring potentially many times throughout a system\u2019s life cycle, applied to a number of different work products for a number of different tasks. One example of a process that supports software reading is stepwise abstraction, a procedure by which sequences of steps in an algorithmic document are abstracted to descriptions of functionality at various levels. Stepwise abstraction helps a developer to understand a code document in order to complete a particular task, namely, checking whether it conforms to the specifications set for it. In the current state of the practice, support for software reading is often not available or not used; developers may be taught how to write intermediate\u00a0\u2026", "num_citations": "14\n", "authors": ["159"]}
{"title": "Decision support for using software inspections\n", "abstract": " In support of decision-making for planning the effort to be allocated to inspections in different software development phases, we propose combining empirical studies with process modeling and simulation. We present the simulator developed for answering questions and running \"what-if\" scenarios specific to NASA software development projects.", "num_citations": "13\n", "authors": ["159"]}
{"title": "Research 2.0?\n", "abstract": " IEEE Software Editor in Chief Forrest Shull discuss the state of research in software engineering, focusing on empirical software engineering (ESE) and the expanded goal\u2014question\u2014metric strategies (GQM+Strategies) to tie specific measurements to the technical goals that they address. He also welcomes Girish Suryanarayana as the magazine's newest member of its Industry Advisory Board.", "num_citations": "12\n", "authors": ["159"]}
{"title": "An examination of change profiles in reusable and non\u2010reusable software systems\n", "abstract": " This paper reports on an industrial case study in a large Norwegian Oil and Gas company (StatoilHydro ASA) involving a reusable Java\u2010class framework and two applications that use that framework. We analyzed software changes from three releases of the reusable framework, called Java Enterprise Framework (JEF), and two applications reusing the framework, called Digital Cargo File (DCF) and Shipment and Allocation (S&A). On the basis of our analysis, we found the following: (1) Profiles of change types for the reused framework and the applications are similar, specifically, perfective changes dominate significantly. (2) Although on observing the mean value adaptive changes are more frequent and are active longer in JEF and S&A, these systems went through less refactoring than DCF. For DCF, we saw that preventive changes were more frequent and were active longer. (3) Finally, we found that designing\u00a0\u2026", "num_citations": "12\n", "authors": ["159"]}
{"title": "Early identification of SE-related program risks\n", "abstract": " The mission of the DoD Systems Engineering Research Center SERC is to perform research leading to transformational SE methods, processes, and tools MPTs that enable DoD and Intelligence Community IC systems to achieve significantly improved mission successes. An elevator speech for the capabilities delivered by the Department of Defense DoD Systems Engineering Research Center SERC Systems Engineering SE Effectiveness Measurement EM task reads as follows for the DoD, whose Major Defense Acquisition Programs MDAPs frequently and significantly overrun their budgets and schedules and deliver incomplete systems, the SERC SE EM framework, operational concepts, and tools will empower MDAP sponsors and performers to collaboratively determine their early SE shortfalls and enable the development of successful systems within their resource constraints. Unlike traditional schedule-based and event-based reviews, the SERC SE EM technology enables sponsors and performers to agree on the nature and use of more effective evidence-based reviews. These enable early detection of missing SE capabilities or personnel competencies with respect to a framework of Goals, Critical Success Factors CSFs, and Questions determined by the EM task from the leading DoD early-SE CSF analyses. The EM tools enable risk-based prioritization of corrective actions, as shortfalls in evidence for each question are early uncertainties, which when combined with the relative system impact of a negative answer to the question, translates into the degree of risk that needs to be managed to avoid system overruns and incomplete\u00a0\u2026", "num_citations": "12\n", "authors": ["159"]}
{"title": "Fully employing software inspections data\n", "abstract": " Software inspections provide a proven approach to quality assurance for software products of all kinds, including requirements, design, code, test plans, among others. Common to all inspections is the aim of finding and fixing defects as early as possible, and thereby providing cost savings by minimizing the amount of rework necessary later in the life cycle. Measurement data, such as the number and type of found defects and the effort spent by the inspection team, provide not only direct feedback about the software product to the project team, but are also valuable for process improvement activities. In this paper, we discuss NASA\u2019s use of software inspections and the rich set of data that has resulted. In particular, we present results from analysis of inspection data that illustrate the benefits of fully utilizing that data for process improvement at several levels. Examining such data across multiple inspections or\u00a0\u2026", "num_citations": "11\n", "authors": ["159"]}
{"title": "Can knowledge of technical debt help identify software vulnerabilities?\n", "abstract": " Software vulnerabilities originating from design decisions are hard to find early and time consuming to fix later. We investigated whether the problematic design decisions themselves might be relatively easier to find, based on the concept of \u201ctechnical debt,\u201d ie, design or implementation constructs that are expedient in the short term but make future changes and fixes more costly. If so, can knowing which components contain technical debt help developers identify and manage certain classes of vulnerabilities? This paper provides our approach for using knowledge of technical debt to identify software vulnerabilities that are difficult to find using only static analysis of the code. We present initial findings from a study of the Chromium open source project that motivates the need to examine a combination of evidence: quantitative static analysis of anomalies in code, qualitative classification of design consequences in issue trackers, and software development indicators in the commit history.", "num_citations": "10\n", "authors": ["159"]}
{"title": "Measuring developers: Aligning perspectives and other best practices\n", "abstract": " The paper discusses the software metrics programs. Software metrics programs might rank among the all-time touchiest subjects in software development. Done well, a measurement program can prove an effective tool for keeping on top of development effort-especially for large, distributed projects. It can help developers feel that they have a fair and objective way of communicating their progress and getting resources allocated where they're needed most. An effective metrics program must address both products and processes. Product-related metrics can be tied to customer satisfaction. They often represent objectively verifiable properties such as the number of bugs or defects found in a product. Process metrics, include measures of effort and effectiveness and can give quick feedback about the status of defect containment, productivity, and other desirable properties at many points during development. However\u00a0\u2026", "num_citations": "10\n", "authors": ["159"]}
{"title": "Experience report on the effect of software development characteristics on change distribution\n", "abstract": " This paper reports on an industrial case study in a large Norwegian Oil and Gas company (StatoilHydro ASA) involving a reusable Java-class framework and an application that uses that framework. We analyzed software changes from three releases of the framework and the application. On the basis of our analysis of the data, we found that perfective and corrective changes account for the majority of changes in both the reusable framework and the non-reusable application. Although adaptive changes are more frequent and has longer active time in the reusable framework, it went through less refactoring compared to the non-reusable application. For the non-reusable application we saw preventive changes as more frequent and with longer active time. We also found that designing for reuse seems to lead to fewer changes, as well as we saw a positive effect on doing refactoring.", "num_citations": "10\n", "authors": ["159"]}
{"title": "The computational research and engineering acquisition tools and environments (CREATE) program\n", "abstract": " Physics-based high-performance computing (HPC) engineering software applications are proving to highly effective for the development of complex innovative products such as automobiles, airplanes, and microprocessors. Over the next year and a half, CiSE will feature three issues describing the US Department of Defense (DoD) High Performance Computing Modernization Program (HPCMP) Computational Research and Engineering Acquisition Tools and Environments (CREATE) program. CREATE was launched in 2006 to develop and deploy a set of multiphysics HPC software applications to help the DoD acquisition community (government and industry) develop innovative military air vehicle, naval vessel, and radio frequency antenna systems.", "num_citations": "9\n", "authors": ["159"]}
{"title": "Managing Montezuma: Handling All the Usual Challenges of Software Development, and Making It Fun: An Interview with Ed Beach\n", "abstract": " EIC Forrest Shull interviews AI lead programmer Ed Beach to investigate the software engineering practices employed to create computer gaming software that is both high quality and fun. This interview focuses on the context of the best-selling Civilization series of games created by Firaxis Inc.", "num_citations": "8\n", "authors": ["159"]}
{"title": "The quest for convincing evidence\n", "abstract": " What makes evidence elegant, valid, useful, and convincing? This is the question the authors of this chapter have been pursuing for the past two decades. Between us, we have published over 200 papers on empirical software engineering and organized seemingly countless panel discussions, workshops, conferences, and special journal issues about the topic.", "num_citations": "7\n", "authors": ["159"]}
{"title": "Evaluating the effectiveness of systems and software engineering methods, processes and tools for use in defense programs\n", "abstract": " The systems engineering research center university affiliated research center (SERC-UARC) at Stevens Institute has been tasked to evaluate the effectiveness of the systems and software engineering processes, methods and tools (MPTs) used in US department of defense acquisition and development programs. This paper presents the selection and evaluation process, describes its evolution based on changing sponsor needs, and presents additional information on the characterization of MPTs for evaluation.", "num_citations": "7\n", "authors": ["159"]}
{"title": "Decision support for best practices: Lessons learned on bridging the gap between research and applied pratice\n", "abstract": " Today, everyone is looking at best practices for developing a system or making the right choice in acquiring system components. If the right best practices are applied, they help to avoid common problems and improve quality, cost, or both. However, finding and selecting an appropriate best practice is not always an easy endeavor. In most cases guidance, based on sound experience, is missing; often the best practice is too new, still under study, or the existing experiences do not fit the user's context. This article reports on a program that tries to bridge the gap between rigorous empirical research and practical needs for guiding practitioners in selecting appropriate best practices.", "num_citations": "7\n", "authors": ["159"]}
{"title": "On failure classification: the impact of\" getting it wrong\"\n", "abstract": " Bug classification is a well-established practice which supports important activities such as enhancing verification and validation (V&V) efficiency and effectiveness. The state of the practice is manual and hence classification errors occur. This paper investigates the sensitivity of the value of bug classification (specifically, failure type classification) to its error rate; ie, the degree to which misclassified historic bugs decrease the V&V effectiveness (ie, the ability to find bugs of a failure type of interest). Results from the analysis of an industrial database of more than 3,000 bugs show that the impact of classification error rate on V&V effectiveness significantly varies with failure type. Specifically, there are failure types for which a 5% classification error can decrease the ability to find them by 66%. Conversely, there are failure types for which the V&V effectiveness is robust to very high error rates. These results show the utility\u00a0\u2026", "num_citations": "6\n", "authors": ["159"]}
{"title": "I believe!\n", "abstract": " Many studies have shown that important factors and key relationships often don't hold up well when transferred from one project to another. To deal with this seeming lack of global truisms in software engineering, it helps to develop a healthy skepticism and find ways to test our beliefs in key development practices against measures collected within the project context.", "num_citations": "6\n", "authors": ["159"]}
{"title": "The true cost of mobility?\n", "abstract": " IEEE Software Editor-in-Chief Forrest Shull discusses privacy implications for mobile and cloud computing with the John Howie, chief operating officer of the Cloud Security Alliance. He also looks at the upcoming Software Experts Summit scheduled for 30 May 2014 in Bangalore, India, and discusses the 200th episode of Software Engineering Radio. The Web extra at http://youtu.be/12w2q6BirV8 is an audio interview in which IEEE Software editor-in-chief Forrest Shull discusses the privacy implications of mobile and cloud computing with John Howie, chief operating officer of the Cloud Security Alliance.", "num_citations": "5\n", "authors": ["159"]}
{"title": "Progression, Regression, or Stasis?\n", "abstract": " IEEE Software Editor in Chief Forrest Shull discusses the challenges of delivering quality software systems on time and on schedule, while considering the many challenges faced by the Health Insurance Marketplace system. He also introduces new Editorial Board and Advisory Board members Jeromy Carriere, Davide Falessi, Evelyn Tian, and Grigori Melnik.", "num_citations": "5\n", "authors": ["159"]}
{"title": "A lifetime guarantee\n", "abstract": " IEEE Software editor-in-chief Forrest Shull discusses the software sustainability and his interview with Girish Seshagiri, the CEO of AIS, an organization that offers \"firm fixed-price contracting with performance guarantees, including a lifetime warranty on software defects\" in government contracts. In addition, he discusses the best paper award at the 21st Annual IEEE International Requirements Engineering Conference and the best research paper award at the Agile Conference. The first Web extra at http://youtu.be/L1XN0R4koRk is an audio interview highlighting IEEE Software editor in chief Forrest Shull's discussion with Girish Seshagiri, the CEO of AIS, about the organization's philosophy of offering \"firm fixed-price contracting with performance guarantees, including a lifetime warranty on software defects\" in government contracts. The second Web extra at http://youtu.be/iFsZlrhSM9E is the complete audio\u00a0\u2026", "num_citations": "5\n", "authors": ["159"]}
{"title": "Software vulnerabilities, defects, and design flaws: A technical debt perspective\n", "abstract": " Technical debt describes a universal software development phenomenon: Quick and easy design or implementation choices that linger in the system will cause ripple effects that make future changes more costly. Although DoD software sustainment organizations have routine practices to manage other kinds of software issues, such as defects and vulnerabilities, the same cannot be said for technical debt. In this work, we discuss the relationships among these three kinds of software anomalies and their impact on software assurance and sustainable development and delivery. Defects are directly linked to external quality, and vulnerabilities are linked to more specific security concerns, but technical debt concerns internal quality and has a significant economic impact on the cost of sustaining and evolving software systems. Emerging research results and industry input demonstrate there are clear distinctions that call for different detection and management methods for defects, vulnerabilities, and technical debt. We draw from concrete examples and experience to offer software development practices to improve the management of technical debt and its impact on security.", "num_citations": "4\n", "authors": ["159"]}
{"title": "Disbanding the\" Process Police\": New Visions for Assuring Compliance\n", "abstract": " This article presents a vision of future techniques and approaches for software assurance, based on interviews with researchers for NASA's Office of Safety and Mission Assurance, and their sense of research trends and future directions. Key components of this vision include a more constructive role for software assurance, based upon early and effective collaboration with software developers, techniques that aim to ensure the quality of software as it is being built rather than after the fact, and earlier feedback loops of the assessments of software quality.", "num_citations": "4\n", "authors": ["159"]}
{"title": "Security engineering fy17 systems aware cybersecurity\n", "abstract": " Resilience features that sustain operator control of weapon systems and assure the validity of the most critical data elements required for weapon control. The decision support tool research focused on integrating historical threat considerations as well as risk considerations into the planning for defenses. Specifically, research investigated the threat analysis aspects of the integrated riskthreat decision support process and included the development of new threat analysis methods focused on mission-aware security.Descriptors:", "num_citations": "3\n", "authors": ["159"]}
{"title": "Emperor: A method for collaborative experience management\n", "abstract": " Software developers have an array of methods, tools, and techniques\u2014generically,\u201cpractices\u201d\u2014to choose from in tackling the complexities of engineering good software. Practice choices can range from large-scope issues that will impact almost all the other decisions on a project (such as whether to use an iterative life cycle model to organize the work) to fine-grained decisions such as whether to use a specific testing approach. In addition to the suite of well-tested practices already in existence, new practices are being created all the time in an attempt to keep up with the ever-growing needs of contemporary software teams. Software developers are bombarded by these practices all the time in books, magazines, podcasts, and even in the scientific literature, yet remarkably little information is easily available that describes the usefulness of a particular practice in any given situation.The difficulty in choosing the \u201cright\u00a0\u2026", "num_citations": "3\n", "authors": ["159"]}
{"title": "Assuring the future? A look at validating climate model software\n", "abstract": " The scientific community studying climate change uses a variety of strategies to assess the correctness of their models. These software systems represent large, sophisticated, fine-grained scientific tools. The validation practices described are thus tailored to a domain in which software and software engineering practices are useful but cannot be allowed to get in the way of the science. In audio interviews, two scientists--Robert Jacob, a computational climate scientist at Argonne National Laboratory, and Gavin Schmidt, a climatologist and climate modeler at the NASA Goddard Institute for Space Studies--discuss what it means to develop and communicate ground-breaking results.", "num_citations": "3\n", "authors": ["159"]}
{"title": "Assessing the Quality Impact of Design Inspections\n", "abstract": " Inspections are widely used and studies have found them to be effective in uncovering defects. However, there is less data available regarding the impact of inspections on different defect types and almost no data quantifying the link between inspections and desired end product qualities. This paper addresses this issue by investigating whether design inspection checklists can be tailored so as to effectively target certain defect types without impairing the overall defect detection rate. The results show that the design inspection approach used here does uncover useful design quality issues and that the checklists can be effectively tailored for some types of defects.", "num_citations": "3\n", "authors": ["159"]}
{"title": "Who Needs Evidence, Anyway?\n", "abstract": " In daily life, we face any number of problems and what seems like an ever-increasing list of possible solutions. The process of building and acquiring software is equally fraught with momentous decisions. However, the infrastructure to help people make informed decisions isn't as well developed. There's certainly no dearth of advertisements for new and improved ways of doing things, nor do we lack vendors' testimonials that their tool, approach, or methodology will solve the big problems. And a lot of people make decisions by hearing what solutions have worked for other people, whether around the water cooler or on discussion boards. Still, we usually can't easily access a helpful set of evidence from other folks, covering many of the decisions that we'd like to make.", "num_citations": "3\n", "authors": ["159"]}
{"title": "Cyber security requirements methodology\n", "abstract": " This report addresses the DoDArmySERC-sponsored, UVA-led 9 month research effort to develop a methodology for establishing cyber security requirements at the preliminary design phase of new physical systems programs. The requirements addressed include the integration of cyber attack defense and resilience solutions, as well as security-related software engineering solutions. Referred to as Cyber Security Requirements Methodology CSRM, the developed process includes six sequential steps conducted by three teams an operationally focused team, a cybersecurity focused team and a systems engineering team. Model-based engineering tools were utilized to support each of the steps. A trial weapon system use case was conducted to gain an initial evaluation of the methodology. The use case system, referred to as Silverfish, was hypothetical, but deemed as a reasonable representation of a possible weapon system. Results of the trial were promising and point to a number of possible paths for follow-on research including implementing the methodology on a real system and building the necessary tools to scale up the methodology to a real system.Descriptors:", "num_citations": "2\n", "authors": ["159"]}
{"title": "3.2. 2 Enhancing the System Development Process Performance: a Value\u2010Based Approach\n", "abstract": " When planning or controlling the system development process, a project leader needs to make decisions which take into account a number of aspects, including: availability of assets and competences, previously enacted processes in the organization, certifications the system is required to obtain, standards to comply with, interactions among process activities, contextual factors and constraints, and allocated budget and schedule. In this paper we propose a value\u2010based approach for supporting decision making. The aim is to provide supportive information for decisions related to the system verification process. This would in turn enhance the performance of system development process by supporting the decision making process for complex systems. We report both academic and industrial empirical evaluations, which demonstrate the feasibility and effectiveness of our proposal, and thus prompt us to refine and\u00a0\u2026", "num_citations": "2\n", "authors": ["159"]}
{"title": "A Brave New World of Testing? An Interview with Google\n", "abstract": " The increasing pervasiveness of cloud computing is changing the state of the practice in software testing. In an interview with James Whittaker, an engineering director at Google, editor in chief Forrest Shull explores some of the important trends in cloud computing and their implications. The conversation covers key technology changes, such as more pervasive access to monitoring frameworks, the ability to aggregate and act on feedback directly from massive user communities (the\" crowdsourcing\" of quality assurance), and the ability to know the exact machine configuration when bugs are discovered. All of these changes are having concrete impacts on which skills are important\u2014and which no longer so\u2014for software testers. An accompanying audio interview provides a complete recording of the conversation and more details on points such as privacy testing.", "num_citations": "2\n", "authors": ["159"]}
{"title": "How Do You Keep Up to Date?\n", "abstract": " Keeping up to date with new software engineering methods, practices, and tools is challenging in the best of times, and made even more urgent by today's tough economic climate. This article discusses a survey of software developers and describes high-level themes related to the types of media that were deemed useful for staying up to date. Based on these themes, some important thrusts for IEEE Software digital content are described.", "num_citations": "2\n", "authors": ["159"]}
{"title": "Decision support with Emperor\n", "abstract": " Selecting the right practice or technology for a given task should be based on the project goals and context. The selection process should be supported by empirical evidence and experiences with the practice in similar contexts [1]. Practitioners, however, usually do not have the time to search for existing empirical studies and to evaluate them in detail. A more centralized and practice-oriented approach is necessary to provide the required information in a convenient way. EMPEROR (experience management portal using empirical results as organizational resources) was developed to address this issue.", "num_citations": "2\n", "authors": ["159"]}
{"title": "The Policies and Economics of Software Sustainment: DoD's Software Sustainment Ecosystem\n", "abstract": " Software is the foundational building material for the engineering of systems, enabling almost 100% of the integrated functionality of cyber physical systems especially mission- and safety-critical software reliant systems to the extent that these systems cannot function without software. As a result, it is imperative that the DoD has the capability and capacity to affordably sustain software-reliant systems and to continually operate and achieve mission success in a dynamic threat, cybersecurity, and net-centric environment.  The Carnegie Mellon University (CMU) Software Engineering Institute (SEI) has been performing studies to inform Departmental decisions regarding software sustainment policies and programs regarding complex weapon systems. These studies were based on interviews and discussions with sustainment centers across all of the Services, case studies on selected programs, and a literature review.  In this paper we present an overview of our initial study regarding the DoD's organic software sustainment infrastructure and its key components related to complex weapon systems, and a selection of key themes from our analysis of sustainment practices. There are two key takeaway messages. First, software sustainment is not effectively described with a model based on hardware (where sustainment can be treated as a discrete series of activities intended to restore form, fit, and function). Secondly, software sustainment is really about continuous engineering in which the software undergoes a series of engineering activities intended to deliver the latest capability to the warfighter, a task which is never done.", "num_citations": "1\n", "authors": ["159"]}
{"title": "Our Best Hope\n", "abstract": " IEEE Software editor in chief Forrest Shull talks with author and consultant Linda Rising about the power of retrospectives for software teams. The Web extra at http://youtu.be/2Tgui-qr2AQ is an audio recording of IEEE Software editor in chief Forrest Shull talking with author and consultant Linda Rising about the power of retrospectives for software teams. The second Web extra at http://www.se-radio.net/2014/01/episode-200-markus-volter-on-language-design-and-domain-specific-languages/ is an audio recording of Markus Voelter talking to Linda Rising about retrospectives and the logistics of making them work for software projects.", "num_citations": "1\n", "authors": ["159"]}
{"title": "Towards flexible automated support to improve the quality of computational science and engineering software\n", "abstract": " Continual evolution of the available hardware (e.g. in terms of increasing size, architecture, and computing power) and software (e.g. reusable libraries) is the norm rather than exception. Our goal is to enable CSE developers to spend more of their time finding scientific results by capitalizing on these evolutions instead of being stuck in fixing software engineering (SE) problems such as porting the application to new hardware, debugging, reusing (unreliable) code, and integrating open source libraries. In this paper we sketch a flexible automated solution supporting scientists and engineers in developing accurate and reliable CSE applications. This solution, by collecting and analyzing product and process metrics, enables the application of well-established software engineering best practices (e.g., separation of concerns, regression testing and inspections) and it is based upon the principles of automation\u00a0\u2026", "num_citations": "1\n", "authors": ["159"]}
{"title": "Sharing your story\n", "abstract": " IEEE Software Editor-in-Chief Forrest Shull discusses the value of experience reports and how they can bring practical advice and perspective that simple metrics are not always able to provide. In addition, he discusses Software Experts Summit 2013 and announces that the magazine is seeking a new multimedia editor. The first Web extra at http://youtu.be/KTUHr-1S_wo is a video preview of Software Experts Summit 2013, which will focus on Smart Data Science: Harnessing Data for Intelligent Decision Making. Scheduled for 17 July at the Microsoft Campus in Redmond, Washington, speakers include James Whittaker of Microsoft, Paul Zikopoulos of IBM, Wolfram Schulte of Microsoft Research, Ayse Bener of Ryerson University, and Forrest Shull of the Fraunhofer Center for Experimental Software Engineering.", "num_citations": "1\n", "authors": ["159"]}
{"title": "Engineering Values: From Architecture Games to Agile Requirements\n", "abstract": " IEEE Software Editor-in-Chief Forrest Shull discusses the importance of having and applying professional principles in all facets of software development while also keeping them in perspective. He cites work by Philippe Kruchten, Ellen Gottesdiener and Mary Gorman to support his position. In addition, he welcomes Dr. Adam Welc to the IEEE Software Editorial Board and discusses the 2012 African Conference on Software Engineering and Applied Computing. The first Web extra at http://youtu.be/R5zUHUFEB7k is an audio interview of IEEE Software editor-in-chief Forrest Shull speaking with Ellen Gottesdiener and Mary Gorman about requirements management in an agile context. The second Web extra at http://youtu.be/SSO6td0xzkI is an audio interview of IEEE Software editor-in-chief Forrest Shull speaking with Philippe Kruchten about how software engineers can be misled by their own cognitive biases\u00a0\u2026", "num_citations": "1\n", "authors": ["159"]}
{"title": "Protection from wishful thinking\n", "abstract": " One of the problems of software development is accurately assessing progress and the cost remaining to completion. Without a good sense of where the project is and how far it still has to go, it's just not possible to consistently manage people and resources well. And miscommunicating progress to stakeholders is among the surest ways to lose trust and buy-in. Even with the best of intentions and good processes in place, it's possible for projects to end up in serious trouble as a result of misassessing or miscomunicating progress. The paper mentions that optimism is a virtue in many cases, but there is a fine distinction between optimism and wishful thinking.", "num_citations": "1\n", "authors": ["159"]}
{"title": "Evidence-based best practices collections\n", "abstract": " For capturing and transferring knowledge between different projects and organizations, the concept of a Best Practice is commonly used. A similar but more general concept for knowledge capturing is often referred to as a Lesson Learned. Both best practices and lessons learned are frequently organized in the form of knowledge collections. Such collections exist in many forms and flavours: From simple notes on a white board, to paper file collections on a shelf, to electronic versions filed in a common folder or shared drive, to systematically archived and standardized versions in experience and databases, or even specific knowledge management systems. In the past few decades, many organizations have invested much time and effort in such specific knowledge collections (eg, databases, experience repositories) for best practices and/or lessons learned. The driving force behind all these activities is to\u00a0\u2026", "num_citations": "1\n", "authors": ["159"]}
{"title": "Early Identification of SE-Related Program Risks: Opportunities for DoD Systems Engineering (SE) Transformation via SE Effectiveness Measures (EMs) and Evidence-Based Reviews\n", "abstract": " DoD programs need effective systems engineering SE to succeed. DoD program managers need early warning of any risks to achieving effective SE. This SERC project has synthesized analyses of DoD SE effectiveness risk sources into a lean framework and toolset for early identification of SE-related program risks. Three important points need to be made about these risks. They are generally not indicators of bad SE. Although SE can be done badly, more often the risks are consequences of inadequate program funding SE is the first victim of an underbudgeted program, of misguided contract provisions when a program manager is faced with the choice between allocating limited SE resources toward producing contract-incentivized functional specifications vs. addressing key performance parameter risks, the path of least resistance is to obey the contract, or of management temptations to show early progress on the easy parts while deferring the hard parts till later. Analyses have shown that unaddressed risk generally leads to serious budget and schedule overruns. Risks are not necessarily bad. If an early capability is needed, and the risky solution has been shown to be superior to the alternatives, accepting and focusing on mitigating the risk is generally better than waiting for a better alternative to show up. Unlike traditional schedule-based and event-based reviews, the SERC SE EM technology enables sponsors and performers to agree on the nature and use of more effective evidence-based reviews. These enable early detection of missing SE capabilities or personnel competencies with respect to a framework of Goals, Critical Success\u00a0\u2026", "num_citations": "1\n", "authors": ["159"]}
{"title": "Making Use of a Decade of Widely Varying Historical Data: SARP Project\n", "abstract": " \u25aa Motivation: Better knowledge of inspection\u2019s strengths & weaknesses could be used to better allocate resources among V&V activities.\u25aa Issue: Defects that slip through inspections aren\u2019t found until much later; different defect type descriptors mean they often are hard to compare.\u25aa Action: Compare test and inspection defect profiles (on the same projects or within the same domain)", "num_citations": "1\n", "authors": ["159"]}
{"title": "Study Guide to Accompany Shari Lawrence Pfleeger's Software Engineering: Theory and Practice\n", "abstract": " This course is organized so as to, first, provide a general introduction to software development and identify the important phases of any software project. Then, each of the phases is examined in detail, in order to give the reader a picture of the current state of our understanding of software development. Chapter 1 provides a general introduction to the field in order to give some sense of the magnitude and importance of software in today's world, t he kinds of problems that make software development difficult, and an outline of how software development is undertaken. Chapter 2 provides more detail on the idea of a \u201csoftware process\u201d, that is, on the various stages software goes through, from the planning stages to its delivery to the customer and beyond. Different models of the process are introduced, and the types of project features for which each is most appropriate are discussed. Chapters 3 through 10 follow, in order, the major phases in the life of a software system. Chapter 3 deals with the planning stages: how resources and cost are estimated, how risks are identified and planned, and how schedules are created. Chapter 4 details how the problem to be solved by the system (not the system itself) is defined. This chapter concentrates on the methods that are necessary to fully capture the customer's requirements for the system, and how to specify them in a way that will be useful for future needs. Once the problem is sufficiently well understood, the system that solves it can be designed. Chapter 5 discusses the design of the software, introducing broad architectural styles that may be useful for different types of systems as well as more specific\u00a0\u2026", "num_citations": "1\n", "authors": ["159"]}