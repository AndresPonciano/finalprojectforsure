{"title": "Data reverse engineering using system dependency graphs\n", "abstract": " Data reverse engineering (DRE) is a complex and costly process that requires a deep understanding of large data-intensive software systems. This process can be made easier with the use of program understanding methods and tools. In this paper, we focus on the program slicing technique and we show how it can be adapted to support DRE. We present a DML-independent SDG construction approach involving the analysis of database operations as a first stage. We describe a tool based upon this approach and we report on two industrial DRE projects", "num_citations": "63\n", "authors": ["134"]}
{"title": "Understanding database schema evolution: A case study\n", "abstract": " Database reverse engineering (DRE) has traditionally been carried out by considering three main information sources: (1) the database schema, (2) the stored data, and (3) the application programs. Not all of these information sources are always available, or of sufficient quality to inform the DRE process. For example, getting access to real-world data is often extremely problematic for information systems that maintain private data. In recent years, the analysis of the evolution history of software programs have gained an increasing role in reverse engineering in general, but comparatively little such research has been carried out in the context of database reverse engineering. The goal of this paper is to contribute to narrowing this gap and exploring the use of the database evolution history as an additional information source to aid database schema reverse engineering. We present a tool-supported method for\u00a0\u2026", "num_citations": "51\n", "authors": ["134"]}
{"title": "Co-transformations in database applications evolution\n", "abstract": " The paper adresses the problem of consistency preservation in data intensive applications evolution. When the database structure evolves, the application programs must be changed to interface with the new schema. The latter modification can prove very complex, error prone and time consuming. We describe a comprehensive transformation/generative approach according to which automated program transformation can be derived from schema transformation. The proposal is illustrated in the particular context of database reengineering, for which a specific methodology and a prototype tool are presented. Some results of two case studies are described.", "num_citations": "49\n", "authors": ["134"]}
{"title": "Migration of legacy information systems\n", "abstract": " This chapter addresses the problem of platform migration of large business applications, that is, complex software systems built around a database and comprising thousands of programs. More specifically, it studies the substitution of a modern data management technology for a legacy one. Platform migration raises two major issues. The first one is the conversion of the database to a new data management paradigm. Recent results have shown that automated lossless database migration can be achieved, both at the schema and data levels. The second problem concerns the adaptation of the application programs to the migrated database schema and to the target data management system. This chapter first poses the problem and describes the State of the Art in information system migration. Then, it develops a two-dimensional reference framework that identifies six representative migration strategies. The\u00a0\u2026", "num_citations": "42\n", "authors": ["134"]}
{"title": "Towards dynamic software product lines: unifying design and runtime adaptations\n", "abstract": " In the recent years, we have witnessed major advances in mobile computing. Modern devices are equipped with a variety of sensors and network interfaces that make them quite versatile. In order to take advantage of all the hardware capabilities and provide a better user experience, software has to be context aware, i.e. it has to monitor the events and information coming from its environment and react accordingly. At the same time, we notice that an important number of such mobile applications share several characteristics regarding its architecture, communication, storage and interfaces. This leads us to consider that context-aware systems can also benefit from the Software Product Line (SPL) paradigm. SPLs were defined to take advantage of commonalities through the definition of reusable artifacts, in order to automate the derivation of multiple products. Nevertheless, SPLs are limited regarding the runtime modifications implied by context awareness. This dissertation investigates on Dynamic Software Product Lines (DSPL). A DSPL extends a classic SPLs by providing mechanisms to adapt products at runtime to cope with dynamic changes imposed by context awareness. Our main goal is to unify design and runtime adaptations under the same definition through high-level artifacts. Such artifacts can then be used to implement DSPLs by defining the processes required to map them into concrete products at design time and at runtime. Concretely, as the first contribution of this dissertation, we introduce both: a simple - yet complete - variability model, and a composition model that realizes variability. With the variability model we aim at\u00a0\u2026", "num_citations": "37\n", "authors": ["134"]}
{"title": "Dynamic analysis of SQL statements for data-intensive applications reverse engineering\n", "abstract": " Analyzing SQL statements brings invaluable information that can be used in various applications such as program understanding and database reverse engineering. While static SQL statements are fairly easy to analyze, dynamic SQL statements most often require dynamic analysis techniques that may prove more difficult to implement. This paper addresses the problem of dynamic SQL query analysis in the context of software and database reverse engineering. It explores the use of dynamic analysis techniques such as aspect-based tracing and SQL trace analysis for extracting implicit information about both the program behavior and the database structure.", "num_citations": "37\n", "authors": ["134"]}
{"title": "How clean is your sandbox?\n", "abstract": " Bidirectional transformations (bx) constitute an emerging mechanism for maintaining the consistency of interdependent sources of information in software systems. Researchers from many different communities have recently investigated the use of bxto solve a large variety of problems, including relational view update, schema evolution, data exchange, database migration, and model co-evolution, just to name a few. Each community leveraged and extended different theoretical frameworks and tailored their use for specific sub-problems. Unfortunately, the question of how these approaches actually relate to and differ from each other remains unanswered. This question should be addressed to reduce replicated efforts among and even within communities, enabling more effective collaboration and fostering cross-fertilization. To effectively move forward, a systematization of these many theories and systems\u00a0\u2026", "num_citations": "35\n", "authors": ["134"]}
{"title": "Dynamic program analysis for database reverse engineering\n", "abstract": " The maintenance and evolution of data-intensive systems should ideally rely on a complete and accurate database documentation. Unfortunately, this documentation is often missing, or, at best, outdated. Database redocumentation, a process also known as database reverse engineering, then comes to the rescue. This process typically involves the elicitation of implicit schema constructs, that is, data structures and constraints that have been incompletely translated into the operational database schema. In this context, the SQL statements executed by the programs may be a particularly rich source of information. SQL APIs come in two variants, namely static and dynamic. The latter is intensively used in object-oriented and web applications, notably through ODBC and JDBC APIs. While the static analysis of SQL queries has long been studied, coping with automatically generated SQL statements requires\u00a0\u2026", "num_citations": "34\n", "authors": ["134"]}
{"title": "Static analysis of dynamic database usage in java systems\n", "abstract": " Understanding the links between application programs and their database is useful in various contexts such as migrating information systems towards a new database platform, evolving the database schema, or assessing the overall system quality. In the case of Java systems, identifying which portion of the source code accesses which portion of the database may prove challenging. Indeed, Java programs typically access their database in a dynamic way. The queries they send to the database server are built at runtime, through String concatenations, or Object-Relational Mapping frameworks like Hibernate and JPA. This paper presents a static analysis approach to program-database links recovery, specifically designed for Java systems. The approach allows developers to automatically identify the source code locations accessing given database tables and columns. It focuses on the combined analysis\u00a0\u2026", "num_citations": "31\n", "authors": ["134"]}
{"title": "A conceptual approach to database applications evolution\n", "abstract": " Data-intensive systems are subject to continuous evolution that translates ever-changing business and technical requirements. System evolution usually constitutes a highly complex, expensive and risky process. This holds, in particular, when the evolution involves database schema changes, which in turn impact on data instances and application programs. This paper presents a comprehensive approach that supports the rapid development and the graceful evolution of data-intensive applications. The approach combines the automated derivation of a relational database from a conceptual schema, and the automated generation of a data manipulation API providing programs with a conceptual view of the relational database. The derivation of the database is achieved through a systematic transformation process, keeping track of the mapping between the successive versions of the schema. The generation\u00a0\u2026", "num_citations": "30\n", "authors": ["134"]}
{"title": "Using the Meta-Environment for maintenance and renovation\n", "abstract": " The meta-environment is a flexible framework for language development, source code analysis and source code transformation. We highlight new features and demonstrate how the system supports key functionalities for software evolution: fact extraction, software analysis, visualization, and software transformation", "num_citations": "30\n", "authors": ["134"]}
{"title": "Database semantics recovery through analysis of dynamic SQL statements\n", "abstract": " The documentation of a database includes its conceptual schema, that formalizes the semantics of the data, and its logical schema that translates the former according to an operational database model. Important engineering processes such as database and program evolution rely on a complete and accurate database documentation. In many cases, however, these schemas are missing, or, at best, incomplete and outdated. Their reconstruction, a process called database reverse engineering, requires DDL code analysis but, more important, the elicitation of implicit constructs (data structures and constraints), that is, constructs that have been incompletely translated into the operational database schema. The most powerful discovery technique of these implicit constructs is the static analysis of application program source code, and, in particular of embedded SQL statements. Unfortunately, the increasing\u00a0\u2026", "num_citations": "27\n", "authors": ["134"]}
{"title": "Program analysis and transformation for data-intensive system evolution\n", "abstract": " Data-intensive software systems are generally made of a database and a collection of application programs in strong interaction with the former. They constitute critical assets in most enterprises, since they support business activities in all production and management domains. Data-intensive systems form most of the so-called legacy systems: they typically are one or more decades old, they are very large, heterogeneous and highly complex. Many of them significantly resist modifications and change due to the lack of documentation, to the use of aging technologies and to inflexible architectures. Therefore, the evolution of data-intensive systems clearly calls for automated support. This thesis explores the use of automated program analysis and transformation techniques in support to the evolution of the database component of the system. The program analysis techniques aim to ease the database evolution\u00a0\u2026", "num_citations": "27\n", "authors": ["134"]}
{"title": "Reverse engineering user interfaces for interactive database conceptual analysis\n", "abstract": " The first step of most database design methodologies consists in eliciting part of the user requirements from various sources such as user interviews and corporate documents. These requirements formalize into a conceptual schema of the application domain, that has proved to be difficult to validate, especially since the visual representation of the ER model has shown understandability limitations from the end-users standpoint. In contrast, we claim that prototypical user interfaces can be used as a two-way channel to efficiently express, capture and validate data requirements. Considering these interfaces as a possibly populated physical view on the database to be developed, reverse engineering techniques can be applied to derive their underlying conceptual schema. We present an interactive tool-supported approach to derive data requirements from user interfaces. This approach, based on an intensive\u00a0\u2026", "num_citations": "26\n", "authors": ["134"]}
{"title": "Where was this SQL query executed? a static concept location approach\n", "abstract": " Concept location in software engineering is the process of identifying where a specific concept is implemented in the source code of a software system. It is a very common task performed by developers during development or maintenance, and many techniques have been studied by researchers to make it more efficient. However, most of the current techniques ignore the role of a database in the architecture of a system, which is also an important source of concepts or dependencies among them. In this paper, we present a concept location technique for data-intensive systems, as systems with at least one database server in their architecture which is intensively used by its clients. Specifically, we present a static technique for identifying the exact source code location from where a given SQL query was sent to the database. We evaluate our technique by collecting and locating SQL queries from testing scenarios of\u00a0\u2026", "num_citations": "25\n", "authors": ["134"]}
{"title": "Managing technical debt in database schemas of critical software\n", "abstract": " The metaphor of technical debt (TD) has been used to characterize and quantify issues arising from software evolution and maintenance actions taken to modify the functionality or behaviour of a system while compromising on certain \"under the hood\" quality attributes in order to save cost and effort. The majority of research in this area has concentrated on software program code and architecture. Fewer research considers TD in the context of database applications, particularly TD related to database schemas, which is the focus of this paper. Managing TD in database schemas provides considerable and unique challenges, in particular for applications of safety and security critical nature. We discuss these challenges, point out potential solutions and present an industrial case study in this area.", "num_citations": "25\n", "authors": ["134"]}
{"title": "Hecataeus: A what-if analysis tool for database schema evolution\n", "abstract": " Databases are continuously evolving environments, where design constructs are added, removed or updated rather often. Small changes in the database configurations might impact a large number of applications and data stores around the system: queries and data entry forms can be invalidated, application programs might crash. HECATAEUS is a tool, which represents the database schema along with its dependent workload, mainly queries and views, as a uniform directed graph. The tool enables the user to create hypothetical evolution events and examine their impact over the overall graph as well as to define rules so that both syntactical and semantic correctness of the affected workload is retained.", "num_citations": "24\n", "authors": ["134"]}
{"title": "Detecting and preventing program inconsistencies under database schema evolution\n", "abstract": " Nowadays, data-intensive applications tend to access their underlying database in an increasingly dynamic way. The queries that they send to the database server are usually built at runtime, through String concatenation, or Object-Relational-Mapping (ORM) frameworks. This level of dynamicity significantly complicates the task of adapting application programs to database schema changes. Failing to correctly adapt programs to an evolving database schema results in program inconsistencies, which in turn may cause program failures. In this paper, we present a tool-supported approach, that allows developers to (1) analyze how the source code and database schema co-evolved in the past and (2) simulate a database schema change and automatically determine the set of source code locations that would be impacted by this change. Developers are then provided with recommendations about what they should\u00a0\u2026", "num_citations": "22\n", "authors": ["134"]}
{"title": "Dahlia: A visual analyzer of database schema evolution\n", "abstract": " In a continuously changing environment, software evolution becomes an unavoidable activity. The mining software repositories (MSR) field studies the valuable data available in software repositories such as source code version-control systems, issue/bug-tracking systems, or communication archives. In recent years, many researchers have used MSR techniques as a way to support software understanding and evolution. While many software systems are data-intensive, i.e., their central artefact is a database, little attention has been devoted to the analysis of this important system component in the context of software evolution. The goal of our work is to reduce this gap by considering the database evolution history as an additional information source to aid software evolution. We present DAHLIA (Database ScHema EvoLutIon Analysis), a visual analyzer of database schema evolution. Our tool mines the database\u00a0\u2026", "num_citations": "22\n", "authors": ["134"]}
{"title": "Co-transformations in information system reengineering\n", "abstract": " Database reengineering consists of deriving a new database from a legacy database and adapting the software components accordingly. This migration process involves three main steps, namely schema conversion, data conversion and program conversion. This paper explores the feasibility of transforming the application programs through code transformation patterns that are automatically derived from the database transformations. It presents the principles of a new transformational approach coupling database and program transformations and it describes a prototype CASE tool based on this approach.", "num_citations": "22\n", "authors": ["134"]}
{"title": "Visual tracing for the eclipse java debugger\n", "abstract": " In contrast to stepping, tracing is a debugging technique that does not suspend the execution. This technique is more suitable for debugging programs whose correctness is compromised by the suspension of execution. In this work we present a tool for visually tracing Java programs in Eclipse. Trace point hits are collected on a per-instance basis. This enables finding out which trace points were hit for which objects at which time. The interactive visualization provides detailed information about the hits such as thread, stack trace, and assigned values. We implemented the tool as an Eclipse plug in that integrates with other features of Eclipse Java debugger. In an informal evaluation, developers appreciated the utility of our method as a solution in the middle between full tracing and stop-and-go debugging. They suggested scenarios in which our tool can help them in debugging and understanding their programs.", "num_citations": "19\n", "authors": ["134"]}
{"title": "A static code smell detector for SQL queries embedded in Java code\n", "abstract": " A database plays a central role in the architecture of an information system, and the way it stores the data delimits its main features. However, it is not just the data that matters. The way it is handled, i.e., how the application communicates with the database is of critical importance too. Therefore the implementation of such a communication layer has to be reliable and efficient. SQL is a popular language to query a database, and modern technologies rely on it (or its dialects) as query strings embedded in the application code. In many languages (e.g. in Java), an embedded query is typically constructed through several string operations that obstruct developers in understanding the statement finally sent to the database. It is a potential source of fault-prone and inefficient database usage, i.e., code smells. In our paper, we present a tool for the identification of code smells in SQL queries embedded in Java code. Our tool\u00a0\u2026", "num_citations": "16\n", "authors": ["134"]}
{"title": "Multidirectional Transformations and Synchronisations (Dagstuhl Seminar 18491)\n", "abstract": " Bidirectional transformations (bx) are a mechanism for maintaining the consistency of two (or more) related sources of information, such as models in model-driven development, database schemas, or programs. Bx technologies have been developed for practical engineering purposes in many diverse fields. Different disciplines such as programming languages, graph transformations, software engineering, and databases have contributed to the concepts and theory of bx. However, so far, most efforts have been focused on the case where exactly two information sources must be kept consistent; the case of more than two has usually been considered as an afterthought. In many practical scenarios, it is essential to work with more than two information sources, but the community has hardly started to identify and address the research challenges that this brings. Driven by the practical needs and usage scenarios from industry, this Dagstuhl Seminar aimed to identify the challenges, issues and open research problems for multidirectional model transformations and synchronisations and sketch a road map for developing relevant concepts, theories and tools. The report contains an executive summary of the seminar, reports from its working groups, as well as descriptions of industrial and academic case studies that motivated the discussions.", "num_citations": "14\n", "authors": ["134"]}
{"title": "Establishing referential integrity in legacy information systems-reality bites!\n", "abstract": " Most modern relational DBMS have the ability to monitor and enforce referential integrity constraints (RICs). In contrast to new applications, however, heavily evolved legacy information systems may not make use of this important feature, if their design predates its availability. The detection of RICs in legacy systems has been a long-term research topic in the DB reengineering community and a variety of different methods have been proposed, analyzing schema, application code and data. However, empirical evidence on their application for reengineering large-scale industrial systems is scarce and all too often \"problems\" (case studies) are carefully selected to fit a particular \"solution\" (method), rather than the other way around. This paper takes a different approach. We analyze in detail the issues posed in reengineering a complex, mission-critical information system to support RICs. In our analysis, we find that\u00a0\u2026", "num_citations": "14\n", "authors": ["134"]}
{"title": "Understanding schema evolution as a basis for database reengineering\n", "abstract": " Software repositories can provide valuable information for facilitating software reengineering efforts. In recent years, many researchers have started to follow a holistic approach, considering diverse software artifacts and the links existing between them. However, when analyzing data-intensive systems, comparatively little attention has been devoted to the analysis of an important system artifact: the database. Even fewer approaches attempt to uncover facts about the evolution history of database schemas. We have developed a tool-supported method for analyzing and visualizing database schema history. This paper reports early results of applying and validating this method. We discuss our experiences to date and point out several novel research perspectives in this domain.", "num_citations": "14\n", "authors": ["134"]}
{"title": "Progesterone receptor antagonists\n", "abstract": " The present invention relates to progesterone receptor antagonists of general formula I: formula I", "num_citations": "14\n", "authors": ["134"]}
{"title": "Document and schema XML updates\n", "abstract": " Purpose of this chapter is to describe the different research proposals and the facilities of main enabled and native XML DBMSs to handle XML updates at document and schema level, and their versions. Specifically, the chapter will provide a review of various proposals for XML document updates, their different semantics and their handling of update sequences, with a focus on the XQuery Update proposal. Approaches and specific issues concerned with schema updates will then be reviewed. Document and schema versioning will be considered. Finally, a review of the degree and limitations of update support in existing DBMSs will be discussed.", "num_citations": "14\n", "authors": ["134"]}
{"title": "Mining Stack Overflow for discovering error patterns in SQL queries\n", "abstract": " Constructing complex queries in SQL sometimes necessitates the use of language constructs and the invocation of internal functions which inexperienced developers find hard to comprehend or which are unknown to them. In the worst case, bad usage of these constructs might lead to errors, to ineffective queries, or hamper developers in their tasks. This paper presents a mining technique for Stack Overflow to identify error-prone patterns in SQL queries. Identifying such patterns can help developers to avoid the use of error-prone constructs, or if they have to use such constructs, the Stack Overflow posts can help them to properly utilize the language. Hence, our purpose is to provide the initial steps towards a recommendation system that supports developers in constructing SQL queries. Our current implementation supports the MySQL dialect, and Stack Overflow has over 300,000 questions tagged with the MySQL\u00a0\u2026", "num_citations": "13\n", "authors": ["134"]}
{"title": "Visualizing arrays in the Eclipse Java IDE\n", "abstract": " The Eclipse Java debugger uses an indented list to view arrays at runtime. This visualization provides limited insight into the array. Also, it is cumbersome and time-consuming to search for certain values at an unknown index. We present a new Eclipse plug in for visualizing large arrays and collections while debugging Java programs. The plug in provides three views to visualize the data. These views are designed to support different tasks more efficiently. A tabular view gives detailed information about the elements in the array, such as the value of their field variables. A line chart aims to depict the values of a numerical field over the array. Lastly, bar charts and histograms show how the values of a field are distributed. We show how these views can be used to explore linear data structures and hashes from the Collections Framework. The plug in features tight integration with the Eclipse IDE, and is freely available\u00a0\u2026", "num_citations": "12\n", "authors": ["134"]}
{"title": "Large-scale data reengineering: Return from experience\n", "abstract": " This paper reports on a recent data reengineering project, the goal of which was to migrate a large CODASYL database towards a relational platform. The legacy system, made of one million lines of COBOL code, is in use in a Belgian federal administration. The approach followed combines program transformation, data reverse engineering, data analysis, wrapping and code generation techniques.", "num_citations": "12\n", "authors": ["134"]}
{"title": "An industrial experience report on legacy data-intensive system migration\n", "abstract": " This paper presents an experience report on the migration of a COBOL system of over 2 million lines of code. The main goal of this project was to migrate a legacy CODA-SYL database towards a relational platform, while preserving the functionalities of the legacy application programs.", "num_citations": "11\n", "authors": ["134"]}
{"title": "Supporting schema evolution in schema-less NoSQL data stores\n", "abstract": " NoSQL data stores are becoming popular due to their schema-less nature. They offer a high level of flexibility, since they do not require to declare a global schema. Thus, the data model is maintained within the application source code. However, due to this flexibility, developers have to struggle with a growing data structure entropy and to manage legacy data. Moreover, support to schema evolution is lacking, which may lead to runtime errors or irretrievable data loss, if not properly handled. This paper presents an approach to support the evolution of a schema-less NoSQL data store by analyzing the application source code and its history. We motivate this approach on a subject system and explain how useful it is to understand the present database structure and facilitate future developments.", "num_citations": "10\n", "authors": ["134"]}
{"title": "Understanding the database manipulation behavior of programs\n", "abstract": " Due to the lack of (up-do-date) documentation, software maintenance and evolution processes often necessitate the recovery of a sucient understanding of the software system, before the latter can be adapted to new or changing requirements. To address this problem, several program comprehension techniques have been proposed to support this preliminary phase of software maintenance and evolution. Nevertheless, those techniques generally fail in gaining a complete and accurate understanding in the case of modern data-intensive systems, which are characterized by complex, dynamic and continuous interactions between the application programs and their database. In particular, understanding the database manipulation behavior of a given program involves dierent levels of comprehension ranging from identifying to relating and interpreting the successive database access operations. In this paper, we\u00a0\u2026", "num_citations": "10\n", "authors": ["134"]}
{"title": "Wrapper-based system evolution application to codasyl to relational migration\n", "abstract": " The paper presents a tool-supported approach to legacy data-intensive systems migration. It particularly elaborates on the program conversion phase, for which the use of wrapping techniques is suggested. The approach is applied to a popular specific application, that is, the migration of a large COBOL system using a CODASYL database towards a relational database platform. The use of the methodology and tools is evaluated in the context of a real-size industrial migration project.", "num_citations": "10\n", "authors": ["134"]}
{"title": "Variability management in database applications\n", "abstract": " Complex software products are often subject to application context specific configuration and variations. Variability management is a critical aspect of engineering software product families efficiently. However, the variability management on the data aspect of systems has received less attention. In this paper, we present SVL Tool, a plug-in for the Case Tool DB-Main which enables software engineers to model feature models, map them to database schema elements, and finally produce a new database schema including only the selected features. We present the Simple Variability Language, a language designed on the basis of the Common Variability Language. We also present our results of applying SVL Tool to a case study, an Electronic Medical Records software program widely used in Canadian primary health care.", "num_citations": "9\n", "authors": ["134"]}
{"title": "A comparison of taxonomies for model transformation languages\n", "abstract": " Since the introduction of the MDE/MDA/MDD ideas for software systems development several years ago, a number of different (meta)modeling and model transformation languages have been proposed. Although the OMG's QVT standard specification has somewhat stabilized the number of new model transformation languages, it is likely that new ones will continue to appear, following different paradigms and approaches. However, the evolution towards the consolidation of models as a unifying, foundational and consistent concept for software-intensive system development, requires the realization of a set of ideal characteristics for the specification of model transformation languages. Several works have been proposed for defining characterization and classification schemes, and the set of these ideal characteristics. In this paper we present a comparison of characterization and classification schemes for model transformation languages, and an analysis of the OMG's QVT specification with respect to this set of ideal characteristics. Although the MOF 2 QVT specification presents a good coverage of these characteristics, it still lacks a formal foundation, from which it could obtain several benefits.", "num_citations": "9\n", "authors": ["134"]}
{"title": "Adapting Queries to Database Schema Changes in Hybrid Polystores\n", "abstract": " Database schema change has long been recognized as a complex, time-consuming and risky process. It requires not only the modification of database structures and contents, but also the joint evolution of related application programs. This coevolution process mainly consists in converting database queries expressed on the source database schema, into equivalent queries expressed on the target database schema. Several approaches, techniques and tools have been proposed to address this problem, by considering software systems relying on a single database. In this paper, we propose an automated approach to query adaptation for schema changes in hybrid polystores, i.e., data-intensive systems relying on several, possibly heterogeneous, databases. The proposed approach takes advantage of a conceptual modeling language for representing the polystore schema, and considers a generic query\u00a0\u2026", "num_citations": "8\n", "authors": ["134"]}
{"title": "SQLInspect: A static analyzer to inspect database usage in Java applications\n", "abstract": " We present SQLInspect, a tool intended to assist developers who deal with SQL code embedded in Java applications. It is integrated into Eclipse as a plug-in that is able to extract SQL queries from Java code through static string analysis. It parses the extracted queries and performs various analyses on them. As a result, one can readily explore the source code which accesses a given part of the database, or which is responsible for the construction of a given SQL query. SQL-related metrics and common coding mistakes are also used to spot inefficiently or defectively performing SQL statements and to identify poorly designed classes, like those that construct many queries via complex control-flow paths. SQLInspect is a novel tool that relies on recent query extraction approaches. It currently supports Java applications working with JDBC and SQL code written for MySQL or Apache Impala. Check out the live demo of\u00a0\u2026", "num_citations": "8\n", "authors": ["134"]}
{"title": "Bidirectional Transformations in Database Evolution: A Case Study\" At Scale\".\n", "abstract": " Bidirectional transformations (BX) play an important role in database schema/application co-evolution. In earlier work, Terwilliger introduced the theoretical concept of a Channel as a BX-based mechanism to de-couple \u201cvirtual databases\u201d used by the application code from the actual representation of the data maintained within the DBMS. In this paper, we report on considerations and experiences implementing such Channels in practice in the context of a complex real-world application, and with generative tool support. We focus on Channels implementing Pivot/Unpivot transformations. We present different alternatives for generating such Channels and discuss their performance characteristics at scale. We also present a transformational tool to generate these Channels.", "num_citations": "8\n", "authors": ["134"]}
{"title": "Feature-based adaptation of database schemas\n", "abstract": " A recognized quality of a modern software system is its ability to adapt to changing user needs, user tasks, user skills and context of operation. While recent research results have been achieved in the domain of architectures, requirements and user interfaces, very little attention has been devoted to the adaptation of the data manipulation aspects of the system. However, mobile and pervasive applications impose constraints over the amount of data that can be supported, thus making necessary to adapt the schema to the current context in order to provide only relevant data to the system user.             This paper presents a feature-based approach to adapt database schemas according to the changing context conditions. The method allows the derivation of a consistent and sufficient sub-schema starting from the current context. We define and formalize a generic framework, considering two levels of abstraction\u00a0\u2026", "num_citations": "8\n", "authors": ["134"]}
{"title": "Inverse wrappers for legacy information systems migration\n", "abstract": " The paper studies some problems that arise when a technology change induces the migration of a data-centered application. In particular, it addresses the difficult problem of migrating application programs from a legacy data manager, such as a COBOL file system, to a modern DBMS, such as a relational database management system. The approach suggested in this paper relies on the concept of inverse wrappers, that is, wrappers that simulate the legacy API on top of the new database. This architecture allows (1) the design of a fully normalized database rid of the anomalies of the legacy data,(2) future programs to be developed on a sound basis and (3) legacy programs to work on the new database with minimum transformation, and therefore at low cost.", "num_citations": "8\n", "authors": ["134"]}
{"title": "Carbene\u2013Metal\u2013Amide Polycrystalline Materials Feature Blue Shifted Energy yet Unchanged Kinetics of Emission\n", "abstract": " The nature of carbene\u2013metal\u2013amide (CMA) photoluminescence in the solid state is explored through spectroscopic and quantum-chemical investigations on a representative Au-centered molecule. The crystalline phase offers well-defined coplanar geometries\u2014enabling the link between molecular conformations and photophysical properties to be unravelled. We show that a combination of restricted torsional distortion and molecular electronic polarization blue shift the charge-transfer emission by around 400 meV in the crystalline versus the amorphous phase, through energetically raising the less-dipolar S1 state relative to S0. This blue shift brings the lowest charge-transfer states very close to the localized carbazole triplet state, whose structured emission is observable at low temperature in the polycrystalline phase. Moreover, we discover that the rate of intersystem crossing and emission kinetics are unaffected\u00a0\u2026", "num_citations": "7\n", "authors": ["134"]}
{"title": "Dahlia 2.0: A visual analyzer of database usage in dynamic and heterogeneous systems\n", "abstract": " Understanding the links between application programs and their database is useful in various contexts such as migrating information systems towards a new database platform, evolving the database schema, or assessing the overall system quality. However, data-intensive applications nowadays tend to access their underlying database in an increasingly dynamic way. The queries that they send to the database server are usually built at runtime, through String concatenation, or Object-Relational-Mapping (ORM) frameworks. This level of dynamicity significantly complicates the task of adapting programs to an evolving database schema. In this paper, we present DAHLIA 2.0, an interactive visualization tool that allows developers to analyze the database usage in order to support data-intensive software evolution and more precisely, program-database co-evolution.", "num_citations": "7\n", "authors": ["134"]}
{"title": "L\u2019int\u00e9gration des adaptations interfaces utilisateur dans une approche de d\u00e9veloppement logiciel orient\u00e9e contexte\n", "abstract": " A l\u2019avenir, les smart cities se d\u00e9velopperont et cr\u00e9eront ainsi des villes dites connect\u00e9es. Quoi de mieux, que de pouvoir b\u00e9n\u00e9ficier de telles informations collect\u00e9es par les senseurs, pour adapter le comportement des applications des utilisateurs. Cependant, avec les technologies d\u2019aujourd\u2019hui, il est fastidieux de changer le comportement de telles applications \u00e0 l\u2019ex\u00e9cution. Ce travail a pour but de d\u00e9velopper un framework permettant de cr\u00e9er plus ais\u00e9ment ce type d\u2019applications. Celles-ci devront pouvoir s\u2019 adapter en fonction de leur environnement courant, c\u2019est-\u00e0-dire que leur comportement changera selon la nouvelle situation qui a \u00e9t\u00e9 rep\u00e9r\u00e9e. La d\u00e9tection d\u2019une situation peut venir soit d\u2019un \u00e9v\u00e8nement externe, comme la m\u00e9t\u00e9o, soit d\u2019un \u00e9v\u00e8nement interne, tel que le niveau de la batterie du smartphone, mais devra \u00e9galement prendre en compte les t\u00e2ches de l\u2019utilisateur, comme par exemple, le fait que ce dernier donne ses pr\u00e9f\u00e9rences. Cette solution devra pouvoir g\u00e9rer de mani\u00e8re uniformis\u00e9e les aspects sur les interfaces utilisateur, sur les comportements ainsi que sur les donn\u00e9es. Cependant, dans ce m\u00e9moire, la perspective \u00e9tudi\u00e9e concernera les interfaces utilisateur, \u00e0 savoir comment les adaptations pourront rendre le meilleur visuel possible \u00e0 l\u2019utilisateur tout en consid\u00e9rant son environnement global. Enfin, un outil web sera \u00e9galement d\u00e9velopp\u00e9 pour simuler de tels environnements afin de pouvoir cr\u00e9er, tester et mieux comprendre le fonctionnement de ce type d\u2019applications.", "num_citations": "7\n", "authors": ["134"]}
{"title": "Towards highly adaptive data-intensive systems: A research agenda\n", "abstract": " Data-intensive software systems work in different contexts for different users with the aim of supporting heterogeneous tasks in heterogeneous environments. Most of the operations carried out by data-intensive systems are interactions with data. Managing these complex systems means focusing the attention to the huge amount of data that have to be managed despite limited capacity devices where data are accessed. This rises the need of introducing adaptivity in accessing data as the key element for data-intensive systems to become reality. Currently, these systems are not supported during their lifecycle by a complete process starting from design to implementation and execution while taking into account the variability of accessing data. In this paper, we introduce the notion of data-intensive self-adaptive (DISA) systems as data-intensive systems able to perform context-dependent data accesses. We\u00a0\u2026", "num_citations": "7\n", "authors": ["134"]}
{"title": "The Role of Implicit Schema Constructs in Data Quality.\n", "abstract": " This paper presents a comprehensive approach to legacy data* uality assessment and improvement. It is based on an initial database reverse engineering phase that allows to recover 0,)-060% constructs, that is, structures and cone straints that have not been explicitly declared in the datae base schema, using program analysis techni* ues. These cone structs then serve as a basis for detecting data inconsistene cies, identifying unsafe program fragments, and proposing necessary improvements at both the database and program sides.", "num_citations": "7\n", "authors": ["134"]}
{"title": "Automating program conversion in database reengineering: A wrapper-based approach\n", "abstract": " Database reengineering consists in deriving a new database from a legacy database and adapting associated software components accordingly. This migration process typically involves three main steps, namely schema conversion, data conversion and program conversion. This paper presents a wrapper-based approach to automating the program conversion step. The proposed approach combines program transformations and code generation, which are derived from schema transformations", "num_citations": "7\n", "authors": ["134"]}
{"title": "From pattern-based user interfaces to conceptual schemas and back\n", "abstract": " Since the necessity to associate end-users of the future system with its specification and development steps has proven indisputable, it is crucial to define accessible means to express and communicate conceptual requirements between end-users and analysts in the context of Information Systems engineering. For this purpose, we present a simple form-based interface model offering an understandable and expressive graphical counterpart to a rich subset of the Generic Entity-Relationship model (GER). We describe how the elements of the proposed interface model can be translated into GER schema constructs, bearing the semantic characterisation of inter-concept relationships. The exposed principles support the automated translation of user-drawn form-based interfaces into a conceptual schema. The same principles can also serve as a basis to support the inverse translation process that aims at\u00a0\u2026", "num_citations": "6\n", "authors": ["134"]}
{"title": "Conceptual interpretation of SQL execution traces for program comprehension\n", "abstract": " Modern data-intensive software systems manipulate an increasing amount of heterogeneous data usually stored in a database. Maintaining such systems became a crucial and complex task, which is especially true due to the lack of sufficient documentation. In this context, program comprehension became a primary and an important step in this task. Unfortunately, the highly dynamic nature of interactions between a system and its database makes it hard to analyze these interactions with static analysis techniques. To this end, we propose a novel approach that combines dynamic analysis techniques and visualization to ease understanding data-intensive systems, by focusing on their database manipulation behavior. The approach consists of defining the conceptual interpretation of SQL execution traces in terms of a domain-specific, platform-independent model.", "num_citations": "5\n", "authors": ["134"]}
{"title": "Data-centered applications conversion using program transformations\n", "abstract": " This Master's thesis is not a personal work at all. It is the result of an active collaboration with two research teams from two distinct scientific communities: the LIBD laboratory of the University of Namur and the SEN1 group of the CWI of Amsterdam. The LIBD team is specialized in database engineering, database reengineering and database reverse engineering. The SEN1 group does research in the area of software reverse engineering, software reengineering and software renovation. I would like to acknowledge all the people working in those two teams for providing me a pleasant and fruitful research environment.In particular, I would like to thank Professor Jean-Luc Hainaut, my promotor, and Professor Arie van Deursen, my CWI supervisor, for their precious ideas, feedback and support. I also express my gratitude to Jean Henrard for his patience and time to answer my questions and to read early versions of this work. Working at the CWI has been an excellent experience. I would like to thank all the people of the SEN1 group for being so nice colleagues. Many thanks to Paul Klint, Mark van den Brand, Ralf L\u0414mmel, Jurgen Vinju and Magiel Bruntink for the interest they expressed throughout the realization of this project. I also thank Steven Klusener and Niels Veerman, from the Free University of Amsterdam, for their precious collaboration.", "num_citations": "5\n", "authors": ["134"]}
{"title": "Using Sign language corpora as bilingual corpora for data mining. Contrastive linguistics and computer-assisted annotation\n", "abstract": " More and more sign languages nowadays are now documented by large scale digital corpora. But exploiting sign language (SL) corpus data remains subject to the time consuming and expensive manual task of annotating. In this paper, we present an ongoing research that aims at testing a new approach to better mine SL data. It relies on the methodology of corpus-based contrastive linguistics, exploiting SL corpora as bilingual corpora. We present and illustrate the main improvements we foresee in developing such an approach: downstream, for the benefit of the linguistic description and the bilingual (signed-spoken) competence of teachers, learners and the users; and upstream, in order to enable the automatisation of the annotation process of sign language data. We also describe the methodology we are using to develop a concordancer able to turn SL corpora into searchable translation corpora, and to derive from it a tool support to annotation.", "num_citations": "4\n", "authors": ["134"]}
{"title": "Dynamic analysis of SQL statements in data-intensive programs\n", "abstract": " SQL statements control the bi-directional data flow between application programs and a database through a high-level, declarative and semantically rich data manipulation language. Analyzing these statements brings invaluable information that can be used in such applications as program understanding, database reverse engineering, intrusion detection, program behaviour analysis, program refactoring, traffic monitoring, performance analysis and tuning, to mention some of them. SQL APIs come in two variants, namely static and dynamic. While static SQL statements are fairly easy to process, dynamic SQL statements most often require dynamic analysis techniques that may prove more difficult to implement. The goal of the paper is to identify and evaluate the most effective techniques for dynamic SQL statement analysis in data-intensive application programs. First, it describes the SQL API variants from the program architecture point of view. Then, it discusses some of the most important software engineering applications to which SQL statement understanding can be a significant contribution. A large range of analysis and processing techniques are proposed and the properties of each of them are evaluated. Finally, the applicability of these techniques to the software engineering applications is established. Two practical applications are presented and discussed.", "num_citations": "4\n", "authors": ["134"]}
{"title": "researchportal. unamur. be\n", "abstract": " Restoration of perfusion and reoxygenation of ischemic tissues restores aerobic metabo\u2010lism and supports postischemic functional recovery but also generates significant dam\u2010age related to the ischemia/reperfusion (I/R) phenomenon. At the level of a blood vessel, lesions of I/R are mainly characterized by the perturbation of vasomotion and endothe\u2010lial dysfunction. Moreover, despite the fact that ischemia occurs in a sterile environment, reperfusion induces a significant activation of innate and adaptive immune responses: massive reactive oxygen species (ROS) production; activation of pattern-recognition re\u2010ceptors or toll-like receptors (TLRs); activation of complement, coagulation, cytokine and chemokine production; and inflammatory cell trafficking into the diseased organ. 1 I/R ac\u2010tivates different programs of cell death (necrosis, apoptosis or autophagy-associated cell death) and generates a systemic inflammatory response that lasts several days and that can lead, in some cases, to multi-organ failure and death.[2-4]", "num_citations": "3\n", "authors": ["134"]}
{"title": "What Do Foreign Keys Actually Mean?\n", "abstract": " Foreign keys form a major structuring construct in relational databases and in standard files. In reverse engineering processes, they have long been interpreted as the implementation of many-to-one relationship types, not only in relational databases but also in legacy hierarchical and network databases. Besides the standard version of foreign key, according to which a set of columns (fields) in a table (file) is used to designate rows (records) in another table, a careful analysis of existing databases puts into light a surprisingly large variety of non standard forms of foreign keys. Most of them are quite correct, and perfectly fitted to the requirements the developer had in mind. However, their conceptual interpretation can prove more difficult to formalize than the standard forms. This paper classifies, analyzes and interprets some outstanding variants of foreign keys that were observed in operational files and databases\u00a0\u2026", "num_citations": "3\n", "authors": ["134"]}
{"title": "Modelling a parallel corpus of french and french belgian sign language\n", "abstract": " The overarching objective underlying this research is to develop an online tool, based on a parallel corpus of French Belgian Sign Language (LSFB) and written Belgian French. This tool is aimed to assist various set of tasks related to the comparison of LSFB and French, to the benefit of general users as well as teachers in bilingual schools, translators and interpreters, as well as linguists. These tasks include (1) the comprehension of LSFB or French texts,(2) the production of LSFB or French texts,(3) the translation between LSFB and French in both directions and (4) the contrastive analysis of these languages. The first step of investigation aims at creating an unidirectional French-LSFB concordancer, able to align a one-or multiple-word expression from the French translated text with its corresponding expressions in the videotaped LSFB productions. We aim at testing the efficiency of this concordancer for the extraction of a dictionary of meanings in context. In this paper, we will present the modelling of the different data sources at our disposal and specifically the way they interact with one another.", "num_citations": "2\n", "authors": ["134"]}
{"title": "Mining SQL Execution Traces for Data Manipulation Behavior Recovery.\n", "abstract": " Modern data-intensive software systems manipulate an increasing amount of heterogeneous data in order to support users in various execution contexts. Maintaining and evolving activities of such systems rely on an accurate documentation of their behavior which is often missing or outdated. Unfortunately, standard program analysis techniques are not always suitable for extracting the behavior of dataintensive systems which rely on more and more dynamic data access mechanisms which mainly consist in run-time interactions with a database. This paper proposes a framework to extract behavioral models from dataintensive program executions. The framework makes use of dynamic analysis techniques to capture and analyze SQL execution traces. It applies clustering techniques to identify data manipulation functions from such traces. Process mining techniques are then used to synthesize behavioral models.", "num_citations": "2\n", "authors": ["134"]}
{"title": "A stability-aware approach to continuous self-adaptation of data-intensive systems\n", "abstract": " Nowadays data-intensive software systems have to meet user expectations in ever-changing execution environments. The increasing space of possible context states and the limited capacity of mobile devices make no longer possible to incorporate all necessary software functionalities and data in the system. Instead, the system database has to be adapted to successive context changes, in order to include all the information required at each stage. This adaptation process may translate into frequent and costly reconfigurations, in turn affecting negatively system stability and performance. This paper presents an approach to context-dependent database reconfiguration that aims to improve system stability by anticipating future information needs. The latter are specified by means of an annotated probabilistic task model, where each state is associated with a database subset. Experiments suggest that this\u00a0\u2026", "num_citations": "2\n", "authors": ["134"]}
{"title": "Model Co-evolution and Consistency Management (MCCM\u201908)\n", "abstract": " The goal of the workshop was to exchange ideas and experiences related to Model (Co-)evolution and Consistency Management (MCCM) in the context of Model-Driven Engineering (MDE). Contemporary MDE practices typically include the manipulation and transformation of a large and heterogeneous set of models. This heterogeneity exhibits itself in different guises ranging from notational differences to semantic content-wise variations. These differences need to be carefully managed in order to arrive at a consistent specification that is adaptable to change. This requires a dedicated activity in the development process and a rigourous adoption of techniques such as model differencing, model comparison, model refactoring, model (in)consistency management, model versioning, and model merging. The workshop invited submissions from both academia and industry on these topics, as well as\u00a0\u2026", "num_citations": "2\n", "authors": ["134"]}
{"title": "An empirical study of (multi-) database models in open-source projects\n", "abstract": " Managing data-intensive systems has long been recognized as an expensive and error-prone process. This is mainly due to the often implicit consistency relationships that hold between applications and their database. As new technologies emerged for specialized purposes (e.g., graph databases, document stores), the joint use of database models has also become popular. There are undeniable benefits of such multi-database models where developers combine various technologies. However, the side effects on design, querying, and maintenance are not well-known yet. In this paper, we study multi-database models in software systems by mining major open-source repositories. We consider four years of history, from 2017 to 2020, of a total number of 40,609 projects with databases. Our results confirm the emergence of hybrid data-intensive systems as we found (multi-) database models (e.g\u00a0\u2026", "num_citations": "1\n", "authors": ["134"]}
{"title": "An End-to-End Framework for Integrating and Publishing Linked Open Government Data\n", "abstract": " Linked Government Statistical Data on the Web are significantly increasing in terms of variety, which makes challenging to determine the quality of this data that explicitly become accessible to data consumers. Converting and publishing such data involves several challenges, e.g., data unifying, design decisions, knowledge extraction. In this paper, we aim to address this issue by proposing an end-to-end framework, based on linked open data technologies. This framework assists the user to produce and publish structured Linked Data in an e-Government context. Our aim is to enable data consumers to access to an end-to-end solution, allowing them to produce and publish high-quality data on the web, that meet their needs. To assess our framework, we use a real case study, relying on a local government catalogue.", "num_citations": "1\n", "authors": ["134"]}
{"title": "An empirical study on the usage of SQL execution traces for program comprehension\n", "abstract": " Several studies have investigated dynamic analysis in the context of software maintenance and evolution, and most of them confirmed the positive impact of such analysis on program comprehension tasks. In this paper, we focus on the understanding of the database access behavior of a program, which has become an important (yet largely ignored) aspect of program comprehension. We empirically assess how developers/students are (not) able to understand interactions between the database and the application program. To this end, we used DAViS, a tool for Dynamic Analysis and Visualization of SQL execution traces. We present a controlled experiment that quantitatively evaluates to what extent DAViS can influence program comprehension in terms of duration and correctness of the tasks. The results of the study indicate that DAViS does reduce the response time and increases the correctness (with a large\u00a0\u2026", "num_citations": "1\n", "authors": ["134"]}
{"title": "Extracting data manipulation processes from SQL execution traces\n", "abstract": " Modern data-intensive software systems manipulate an increasing amount of data in order to support users in various execution contexts. Maintaining and evolving activities of such systems rely on an accurate documentation of their behavior which is often missing or outdated. Unfortunately, standard program analysis techniques are not always suitable for extracting the behavior of data-intensive systems which rely on more and more dynamic data access mechanisms which mainly consist in run-time interactions with a database. This paper proposes a framework to extract behavioral models from data-intensive program executions. The framework makes use of dynamic analysis techniques to capture and analyze SQL execution traces. It applies clustering techniques to identify data manipulation functions from such traces. Process mining techniques are then used to synthesize behavioral models.", "num_citations": "1\n", "authors": ["134"]}
{"title": "A Framework to Support the Development and Evolution of Self-adaptive Data-intensive Systems\n", "abstract": " Nowadays ubiquitous software systems have to meet user expectations while considering an ever-changing environment. The increasing space of possible contexts and the limited capacity of mobile devices make no longer possible to incorporate all necessary software alternatives and the required data for all possible contexts. Thus, upon variations to user task, role, preferences or physical environment, the current software alternative and the required data have to be reconfigured. This talk supports the variation of the current software alternative and data by defining the mapping between user requirements and database excerpts. We support the creation of the most suitable, consistent and sufficient excerpt of data taking into account the current context and its possible future changes. The aim of this research is to create a theoretical and practical framework that supports the development and the evolution of adaptive data-intensive software systems for ubiquitous environments.I. SETTING THE CONTEXT Nowadays software systems have to meet user expectations taking into account heterogeneous environments and changing user needs. In this context, the literature of self-adaptive systems [13],[3],[6] provides the theoretical and methodological support for managing the variability of system behavior as a consequence of context variations. At design-time, software engineers define different software alternatives that provide the means for satisfying user requirements in different contexts. At run-time the adaptivity is supported by switching to the most suitable software alternative based on the current context. Different software alternatives\u00a0\u2026", "num_citations": "1\n", "authors": ["134"]}