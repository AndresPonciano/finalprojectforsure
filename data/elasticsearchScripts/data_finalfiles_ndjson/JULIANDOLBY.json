{"title": "Associating synchronization constraints with data in an object-oriented language\n", "abstract": " Concurrency-related bugs may happen when multiple threads access shared data and interleave in ways that do not correspond to any sequential execution. Their absence is not guaranteed by the traditional notion of \"data race\" freedom. We present a new definition of data races in terms of 11 problematic interleaving scenarios, and prove that it is complete by showing that any execution not exhibiting these scenarios is serializable for a chosen set of locations. Our definition subsumes the traditional definition of a data race as well as high-level data races such as stale-value errors and inconsistent views. We also propose a language feature called atomic sets of locations, which lets programmers specify the existence of consistency properties between fields in objects, without specifying the properties themselves. We use static analysis to automatically infer those points in the code where synchronization is\u00a0\u2026", "num_citations": "295\n", "authors": ["288"]}
{"title": "Building an efficient RDF store over a relational database\n", "abstract": " Efficient storage and querying of RDF data is of increasing importance, due to the increased popularity and widespread acceptance of RDF on the web and in the enterprise. In this paper, we describe a novel storage and query mechanism for RDF which works on top of existing relational representations. Reliance on relational representations of RDF means that one can take advantage of 35+ years of research on efficient storage and querying, industrial-strength transaction support, locking, security, etc. However, there are significant challenges in storing RDF in relational, which include data sparsity and schema variability. We describe novel mechanisms to shred RDF into relational, and novel query translation techniques to maximize the advantages of this shredded representation. We show that these mechanisms result in consistently good performance across multiple RDF benchmarks, even when compared\u00a0\u2026", "num_citations": "244\n", "authors": ["288"]}
{"title": "CLAP: recording local executions to reproduce concurrency failures\n", "abstract": " We present CLAP, a new technique to reproduce concurrency bugs. CLAP has two key steps. First, it logs thread local execution paths at runtime. Second, offline, it computes memory dependencies that accord with the logged execution and are able to reproduce the observed bug. The second step works by combining constraints from the thread paths and constraints based on a memory model, and computing an execution with a constraint solver. CLAP has four major advantages. First, logging purely local execution of each thread is substantially cheaper than logging memory interactions, which enables CLAP to be efficient compared to previous approaches. Second, our logging does not require any synchronization and hence with no added memory barriers or fences; this minimizes perturbation and missed bugs due to extra synchronizations foreclosing certain racy behaviors. Third, since it uses no\u00a0\u2026", "num_citations": "136\n", "authors": ["288"]}
{"title": "Efficient Construction of Approximate Call Graphs for JavaScript IDE Services\n", "abstract": " The rapid rise of JavaScript as one of the most popular programming languages of the present day has led to a demand for sophisticated IDE support similar to what is available for Java or C#. However, advanced tooling is hampered by the dynamic nature of the language, which makes any form of static analysis very difficult. We single out efficient call graph construction as a key problem to be solved in order to improve development tools for JavaScript. To address this problem, we present a scalable field-based flow analysis for constructing call graphs. Our evaluation on large real-world programs shows that the analysis, while in principle unsound, produces highly accurate call graphs in practice. Previous analyses do not scale to these programs, but our analysis handles them in a matter of seconds, thus proving its suitability for use in an interactive setting.", "num_citations": "124\n", "authors": ["288"]}
{"title": "Race detection for web applications\n", "abstract": " Modern web pages are becoming increasingly full-featured, and this additional functionality often requires greater use of asynchrony. Unfortunately, this asynchrony can trigger unexpected concurrency errors, even though web page scripts are executed sequentially. We present the first formulation of a happens-before relation for common web platform features. Developing this relation was a non-trivial task, due to complex feature interactions and browser differences. We also present a logical memory access model for web applications that abstracts away browser implementation details. Based on the above, we implemented WebRacer, the first dynamic race detector for web applications. WebRacer is implemented atop the production-quality WebKit engine, enabling testing of full-featured web sites. WebRacer can also simulate certain user actions, exposing more races. We evaluated WebRacer by testing a large\u00a0\u2026", "num_citations": "120\n", "authors": ["288"]}
{"title": "An automatic object inlining optimization and its evaluation\n", "abstract": " Automatic object inlining [19, 20] transforms heap data structures by fusing parent and child objects together. It can improve runtime by reducing object allocation and pointer dereference costs. We report continuing work studying object inlining optimizations. In particular, we present a new semantic derivation of the correctness conditions for object inlining, and program analysis which extends our previous work. And we present an object inlining transformation, focusing on a new algorithm which optimizes class field layout to minimize code expansion. Finally, we detail a fuller evaluation on eleven programs and libraries (including Xpdf, the 25,000 line Portable Document Format (PDF) file browser) that utilizes hardware measures of impact on the memory system.", "num_citations": "118\n", "authors": ["288"]}
{"title": "Dynamic detection of atomic-set-serializability violations\n", "abstract": " Previously we presented atomic sets, memory locations that share some consistency property, and units of work, code fragments that preserve consistency of atomic sets on which they are declared. We also proposed atomic-set serializability as a correctness criterion for concurrent programs, stating that units of work must be serializable for each atomic set. We showed that a set of problematic data access patterns characterize executions that are not atomic-set serializable. Our criterion subsumes data races (single-location atomic sets) and serializability (all locations in one set).", "num_citations": "109\n", "authors": ["288"]}
{"title": "Matching patient records to clinical trials using ontologies\n", "abstract": " This paper describes a large case study that explores the applicability of ontology reasoning to problems in the medical domain. We investigate whether it is possible to use such reasoning to automate common clinical tasks that are currently labor intensive and error prone, and focus our case study on improving cohort selection for clinical trials. An obstacle to automating such clinical tasks is the need to bridge the semantic gulf between raw patient data, such as laboratory tests or specific medications, and the way a clinician interprets this data. Our key insight is that matching patients to clinical trials can be formulated as a problem of semantic retrieval. We describe the technical challenges to building a realistic case study, which include problems related to scalability, the integration of large ontologies, and dealing with noisy, inconsistent data. Our solution is based on the SNOMED CT\u00ae ontology, and\u00a0\u2026", "num_citations": "106\n", "authors": ["288"]}
{"title": "Scalable semantic retrieval through summarization and refinement\n", "abstract": " Query processing of OWL-DL ontologies is intractable in the worst case, but we present a novel technique that in practice allows for efficient querying of ontologies with large Aboxes in secondary storage. We focus on the processing of instance retrieval queries, ie, queries that retrieve individuals in the Abox which are instances of a given concept C. Our technique uses summarization and refinement to reduce instance retrieval to a small relevant subset of the original Abox. We demonstrate the effectiveness of this technique in Aboxes with up to 7 million assertions. Our results are applicable to the very expressive description logic SHIN, which corresponds to OWL-DL minus nominals and datatypes.", "num_citations": "99\n", "authors": ["288"]}
{"title": "MemSAT: checking axiomatic specifications of memory models\n", "abstract": " Memory models are hard to reason about due to their complexity, which stems from the need to strike a balance between ease-of-programming and allowing compiler and hardware optimizations. In this paper, we present an automated tool, MemSAT, that helps in debugging and reasoning about memory models. Given an axiomatic specification of a memory model and a multi-threaded test program containing assertions, MemSAT outputs a trace of the program in which both the assertions and the memory model axioms are satisfied, if one can be found. The tool is fully automatic and is based on a SAT solver. If it cannot find a trace, it outputs a minimal subset of the memory model and program constraints that are unsatisfiable. We used MemSAT to check several existing memory models against their published test cases, including the current Java Memory Model by Manson et al. and a revised version of it by\u00a0\u2026", "num_citations": "97\n", "authors": ["288"]}
{"title": "Scalable and precise taint analysis for android\n", "abstract": " We propose a type-based taint analysis for Android. Concretely, we present DFlow, a context-sensitive information flow type system, and DroidInfer, the corresponding type inference analysis for detecting privacy leaks in Android apps. We present novel techniques for error reporting based on CFL-reachability, as well as novel techniques for handling of Android-specific features, including libraries, multiple entry points and callbacks, and inter-component communication. Empirical results show that our approach is scalable and precise. DroidInfer scales well in terms of time and memory and has false-positive rate of 15.7%. It detects privacy leaks in apps from the Google Play Store and in known malware.", "num_citations": "94\n", "authors": ["288"]}
{"title": "Efficiently refactoring Java applications to use generic libraries\n", "abstract": " Java 1.5 generics enable the creation of reusable container classes with compiler-enforced type-safe usage. This eliminates the need for potentially unsafe down-casts when retrieving elements from containers. We present a refactoring that replaces raw references to generic library classes with parameterized references. The refactoring infers actual type parameters for allocation sites and declarations using an existing framework of type constraints, and removes casts that have been rendered redundant. The refactoring was implemented in Eclipse, a popular open-source development environment for Java, and laid the grounds for a similar refactoring in the forthcoming Eclipse 3.1 release. We evaluated our work by refactoring several Java programs that use the standard collections framework to use Java 1.5\u2019s generic version instead. In these benchmarks, on average, 48.6% of the casts are removed, and\u00a0\u2026", "num_citations": "93\n", "authors": ["288"]}
{"title": "Automatic inline allocation of objects\n", "abstract": " Object-oriented languages like Java and Smalltalk provide a uniform object model that simplifies programming by providing a consistent, abstract model of object behavior. But direct implementations introduce overhead, removal of which requires aggressive implementation techniques (eg type inference, function specialization); in this paper, we introduce object inlining, an optimization that automatically inline allocates objects within containers (as is done by hand in C++) within a uniform model. We present our technique, which includes novel program analyses that track how inlinable objects are used throughout the program. We evaluated object inlining on several object-oriented benchmarks. It produces performance up to three times as fast as a dynamic model without inlining and roughly equal to that of manually-inlined codes.", "num_citations": "92\n", "authors": ["288"]}
{"title": "Using atomic sets of memory locations\n", "abstract": " A system and method includes steps or acts of receiving and examining a computer program written in an object-oriented language; receiving sequences of accesses that form logical operations on a set of memory locations used by the program; receiving definitions of atomic sets of memory locations, each consisting of pieces of data; providing a message indicating where the synchronization is required.", "num_citations": "91\n", "authors": ["288"]}
{"title": "Finding bugs efficiently with a SAT solver\n", "abstract": " We present an approach for checking code against rich specifications, based on existing work that consists of encoding the program in a relational logic and using a constraint solver to find specification violations. We improve the efficiency of this approach with a new encoding of the program that effectively slices it at the logical level with respect to the specification. We also present new encodings for integer values and arrays, enabling the verification of realistic fragments of code that manipulate both. Our technique can handle integers of much larger ranges than previously possible, and permits large sparse arrays to be handled efficiently.", "num_citations": "86\n", "authors": ["288"]}
{"title": "ICC++-a C++ dialect for high performance parallel computing\n", "abstract": " ICC++ is a new concurrent C++ dialect which supports a single source code for sequential and parallel program versions, the construction of concurrent data abstractions, convenient expression of irregular and fine-grained concurrency, and high performance implementations. ICC++ programs are annotated with potential concurrency, facilitating both sharing source with sequential programs and automatic grain size tuning for efficient execution. Concurrency control is at the object level; each object ensures the consistency of its own state. This consistency can be extended over larger data abstractions. Finally, ICC++ integrates arrays into the object system and the concurrency model. In short, ICC++ addresses concurrency and its relation to abstractions \u2014 whether they are implemented by single objects, several objects, or object collections. The design of the language, its rationale, and current status are all\u00a0\u2026", "num_citations": "85\n", "authors": ["288"]}
{"title": "Dynamic determinacy analysis\n", "abstract": " We present an analysis for identifying determinate variables and expressions that always have the same value at a given program point. This information can be exploited by client analyses and tools to, e.g., identify dead code or specialize uses of dynamic language constructs such as eval, replacing them with equivalent static constructs. Our analysis is completely dynamic and only needs to observe a single execution of the program, yet the determinacy facts it infers hold for any execution. We present a formal soundness proof of the analysis for a simple imperative language, and a prototype implementation that handles full JavaScript. Finally, we report on two case studies that explored how static analysis for JavaScript could leverage the information gathered by dynamic determinacy analysis. We found that in some cases scalability of static pointer analysis was improved dramatically, and that many uses of\u00a0\u2026", "num_citations": "84\n", "authors": ["288"]}
{"title": "Automatic customization of classes\n", "abstract": " A method and computer readable medium for automatic replacement of object classes in a library with custom classes to improve program efficiency. The method begins with static analysis preformed on a program containing a plurality of objects in order to determine type-correctness constraints and to detect unused functionality in one or more of the objects to be replaced. The plurality of objects is instrumented to detect usage patterns of functionality in one or more objects. Customized classes are generated based upon the static analysis and usage patterns detected. Bytecode is rewritten which is used for generating classes. The present invention provides transparency in the replacement of the objects.", "num_citations": "70\n", "authors": ["288"]}
{"title": "Refactoring Java programs for flexible locking\n", "abstract": " Recent versions of the Java standard library offer flexible locking constructs that go beyond the language's built-in monitor locks in terms of features, and that can be fine-tuned to suit specific application scenarios. Under certain conditions, the use of these constructs can improve performance significantly, by reducing lock contention. However, the code transformations needed to convert between locking constructs are non-trivial, and great care must be taken to update lock usage throughout the program consistently. We present Relocker, an automated tool that assists programmers with refactoring synchronized blocks into ReentrantLocks and ReadWriteLocks, to make exploring the performance tradeoffs among these constructs easier. In experiments on a collection of real-world Java applications, Relocker was able to refactor over 80% of built-in monitors into ReentrantLocks. Additionally, in most cases the tool\u00a0\u2026", "num_citations": "63\n", "authors": ["288"]}
{"title": "Correct refactoring of concurrent java code\n", "abstract": " Automated refactorings as implemented in modern IDEs for Java usually make no special provisions for concurrent code. Thus, refactored programs may exhibit unexpected new concurrent behaviors. We analyze the types of such behavioral changes caused by current refactoring engines and develop techniques to make them behavior-preserving, ranging from simple techniques to deal with concurrency-related language constructs to a framework that computes and tracks synchronization dependencies. By basing our development directly on the Java Memory Model, we can state and prove precise correctness results about refactoring concurrent programs. We show that a broad range of refactorings are not influenced by concurrency at all, whereas other important refactorings can be made behavior-preserving for correctly synchronized programs by using our framework. Experience with a prototype\u00a0\u2026", "num_citations": "62\n", "authors": ["288"]}
{"title": "Scalable highly expressive reasoner (SHER)\n", "abstract": " In this paper, we describe scalable highly expressive reasoner (SHER), a breakthrough technology that provides semantic querying of large relational datasets using OWL ontologies. SHER relies on a unique algorithm based on ontology summarization and combines a traditional in-memory description logic reasoner with a database backed RDF Store to scale reasoning to very large Aboxes. In our latest experiments, SHER is able to do sound and complete conjunctive query answering up to 7 million triples in seconds, and scales to datasets with 60 million triples, responding to queries in minutes. We describe the SHER system architecture, discuss the underlying components and their functionality, and briefly highlight two concrete use-cases of scalable OWL reasoning based on SHER in the Health Care and Life Science space. The SHER system, with the source code, is available for download (free for\u00a0\u2026", "num_citations": "61\n", "authors": ["288"]}
{"title": "Scalable grounded conjunctive query evaluation over large and expressive knowledge bases\n", "abstract": " Grounded conjunctive query answering over OWL-DL ontologies is intractable in the worst case, but we present novel techniques which allow for efficient querying of large expressive knowledge bases in secondary storage. In particular, we show that we can effectively answer grounded conjunctive queries without building a full completion forest for a large Abox (unlike state of the art tableau reasoners). Instead we rely on the completion forest of a dramatically reduced summary of the Abox. We demonstrate the effectiveness of this approach in Aboxes with up to 45 million assertions.", "num_citations": "54\n", "authors": ["288"]}
{"title": "HybriDroid: static analysis framework for Android hybrid applications\n", "abstract": " Mobile applications (apps) have long invaded the realm of desktop apps, and hybrid apps become a promising solution for supporting multiple mobile platforms. Providing both platform-specific functionalities via native code like native apps and user interactions via JavaScript code like web apps, hybrid apps help developers build multiple apps for different platforms without much duplicated efforts. However, most hybrid apps are developed in multiple programming languages with different semantics, which may be vulnerable to programmer errors. Moreover, because untrusted JavaScript code may access device-specific features via native code, hybrid apps may be vulnerable to various security attacks. Unfortunately, no existing tools can help hybrid app developers by detecting errors or security holes. In this paper, we present HybriDroid, a static analysis framework for Android hybrid apps. We investigate the\u00a0\u2026", "num_citations": "50\n", "authors": ["288"]}
{"title": "Using atomic sets of memory locations\n", "abstract": " A system and method for ensuring consistency of data and preventing data races, including steps of: receiving and examining a computer program written in an object-oriented language; receiving sequences of accesses that form logical operations on a set of memory locations used by the program; receiving definitions of atomic sets of data from the memory locations, wherein said atomic sets are sets of data that indicate an existence of a consistency property without requiring the consistency property itself; inferring which code blocks of the computer program must be synchronized in order to prevent one or more data races in the computer program, wherein synchronization is inferred by determining by analysis for each unit of work, what atomic sets are read and written by the unit of work; and providing a message indicating where synchronization is required.", "num_citations": "46\n", "authors": ["288"]}
{"title": "A type system for data-centric synchronization\n", "abstract": " Data-centric synchronization groups fields of objects into atomic sets to indicate they must be updated atomically. Each atomic set has associated units of work, code fragments that preserve the consistency of that atomic set. We present a type system for data-centric synchronization that enables separate compilation and supports atomic sets that span multiple objects, thus allowing recursive data structures to be updated atomically. The type system supports full encapsulation for more efficient code generation. We evaluate our proposal using AJ, which extends the Java programming language with data-centric synchronization. We report on the implementation of a compiler and on refactoring classes from standard libraries and a multi-threaded benchmark to use atomic sets. Our results suggest that data-centric synchronization enjoys low annotation overhead while preventing high-level data races.", "num_citations": "46\n", "authors": ["288"]}
{"title": "An evaluation of automatic object inline allocation techniques\n", "abstract": " Object-oriented languages such as Java and Smalltalk provide a uniform object reference model, allowing objects to be conveniently shared. If implemented directly, these uniform reference models can suffer in efficiency due to additional memory dereferences and memory management operations. Automatic inline allocation of child objects within parent objects can reduce overheads of heap-allocated pointer-referenced objects.We present three compiler analyses to identify inlinable fields by tracking accesses to heap objects. These analyses span a range from local data flow to adaptive whole-program, flow-sensitive inter-procedural analysis. We measure their cost and effectiveness on a suite of moderate-sized C++ programs (up to 30,000 lines including libraries). We show that aggressive interprocedural analysis is required to enable object inlining, and our adaptive inter-procedural analysis [23] computes\u00a0\u2026", "num_citations": "44\n", "authors": ["288"]}
{"title": "Fault detection and localization in dynamic software applications requiring user inputs and persistent states\n", "abstract": " The present invention provides a system, computer program product and a computer implemented method for prioritizing code fragments based on the use of a software oracle and on a correlation between the executed code fragments and the output they produce. Also described is a computer-implemented method generates additional user inputs based on execution information associated with path constraints and based on information from the oracle. Advantageously, the embodiment is useful in a test generation tool that generated many similar inputs when a failure-inducing input is found, in order to enhance fault localization. Further, described is a computer-implemented flow for extending the existing idea of concolic testing to applications that interact with persistent state.", "num_citations": "41\n", "authors": ["288"]}
{"title": "A data-centric approach to synchronization\n", "abstract": " Concurrency-related errors, such as data races, are frustratingly difficult to track down and eliminate in large object-oriented programs. Traditional approaches to preventing data races rely on protecting instruction sequences with synchronization operations. Such control-centric approaches are inherently brittle, as the burden is on the programmer to ensure that all concurrently accessed memory locations are consistently protected. Data-centric synchronization is an alternative approach that offloads some of the work on the language implementation. Data-centric synchronization groups fields of objects into atomic sets to indicate that these fields must always be updated atomically. Each atomic set has associated units of work, that is, code fragments that preserve the consistency of that atomic set. Synchronization operations are added automatically by the compiler. We present an extension to the Java programming\u00a0\u2026", "num_citations": "39\n", "authors": ["288"]}
{"title": "Optimizing sparse schema-less data in data stores\n", "abstract": " Various embodiments of the invention relate to optimizing storage of schema-less data. At least one of a schema-less dataset including a plurality of resources one or more query workloads associated with the plurality of resources is received. Each resource is associated with at least a plurality of properties. At least one set of co-occurring properties from the plurality of properties is identified. A graph including a plurality of nodes is generated. Each of the nodes represents a unique property in the set of co-occurring properties. The graph further includes an edge connecting each node representing a pair of co-occurring properties. A schema is generated based on the graph that assigns a column identifier from a table to each unique property represented by one of the nodes in the graph.", "num_citations": "37\n", "authors": ["288"]}
{"title": "Method and apparatus for optimizing the evaluation of semantic web queries\n", "abstract": " A semantic query over an RDF database is received with RDF database statistics and access methods for evaluating triple patterns in the query. The semantic query is expressed as a parse tree containing triple patterns and logical relationships among the triple patterns. The parse tree and access methods create a data flow graph containing a plurality of triple pattern and access method pair nodes connected by a plurality of edges, and an optimal flow tree through the data flow graph is determined such that costs are minimized and all triple patterns in the semantic query are contained in the optimal flow tree. A structure independent execution tree defining a sequence of evaluation through the optimal flow tree is created and is transformed into a database structure dependent query plan. This is used to create an SQL query that is used to evaluate the semantic query over the RDF database.", "num_citations": "34\n", "authors": ["288"]}
{"title": "Method and apparatus for detecting vulnerabilities and bugs in software applications\n", "abstract": " In one embodiment, the present invention is a method and apparatus for detecting vulnerabilities and bugs in software applications. One embodiment of a method for detecting a vulnerability in a computer software application comprising a plurality of variables that have respective values and include data and functions includes detecting at least one piece of data that is tainted, tracking the propagation of the tainted data through the software application, and identifying functions that are security sensitive and that are reached by the tainted data its the propagation.", "num_citations": "34\n", "authors": ["288"]}
{"title": "Fault detection and localization in dynamic software applications\n", "abstract": " The present invention provides a system, computer program product and a computer implemented method for prioritizing code fragments based on the use of a software oracle and on a correlation between the executed code fragments and the output they produce. Also described is a computer-implemented method generates additional user inputs based on execution information associated with path constraints and based on information from the oracle. Advantageously, the embodiment is useful in a test generation tool that generated many similar inputs when a failure-inducing input is found, in order to enhance fault localization. Further, described is a computer-implemented flow for extending the existing idea of concolic testing to applications that interact with persistent state.", "num_citations": "33\n", "authors": ["288"]}
{"title": "Statically checking web API requests in JavaScript\n", "abstract": " Many JavaScript applications perform HTTP requests to web APIs, relying on the request URL, HTTP method, and request data to be constructed correctly by string operations. Traditional compile-time error checking, such as calling a non-existent method in Java, are not available for checking whether such requests comply with the requirements of a web API. In this paper, we propose an approach to statically check web API requests in JavaScript. Our approach first extracts a request's URL string, HTTP method, and the corresponding request data using an inter-procedural string analysis, and then checks whether the request conforms to given web API specifications. We evaluated our approach by checking whether web API requests in JavaScript files mined from GitHub are consistent or inconsistent with publicly available API specifications. From the 6575 requests in scope, our approach determined whether the\u00a0\u2026", "num_citations": "32\n", "authors": ["288"]}
{"title": "Practically tunable static analysis framework for large-scale JavaScript applications (T)\n", "abstract": " We present a novel approach to analyze large-scale JavaScript applications statically by tuning the analysis scalability possibly giving up its soundness. For a given sound static baseline analysis of JavaScript programs, our framework allows users to define a sound approximation of selected executions that they are interested in analyzing, and it derives a tuned static analysis that can analyze the selected executions practically. The selected executions serve as parameters of the framework by taking trade-off between the scalability and the soundness of derived analyses. We formally describe our framework in abstract interpretation, and implement two instances of the framework. We evaluate them by analyzing large-scale real-world JavaScript applications, and the evaluation results show that the framework indeed empowers users to experiment with different levels of scalability and soundness. Our\u00a0\u2026", "num_citations": "32\n", "authors": ["288"]}
{"title": "Opportunities in software engineering research for web API consumption\n", "abstract": " Nowadays, invoking third party code increasingly involves calling web services via their web APIs, as opposed to the more traditional scenario of downloading a library and invoking the library's API. However, there are also new challenges for developers calling these web APIs. In this paper, we highlight abroad set of these challenges and argue for resulting opportunities for software engineering research to support developers in consuming web APIs. We outline two specific research threads in this context: (1) web API specification curation, which enables us to know the signatures of web APIs, and (2) static analysis that is capable of extracting URLs, HTTP methods etc. of web API calls. Furthermore, we present new work on how we combine (1) and (2) to provide IDE support for application developers consuming web APIs. As web APIs are used broadly, research in supporting the consumption of web APIs offers\u00a0\u2026", "num_citations": "30\n", "authors": ["288"]}
{"title": "Detecting deadlock in programs with data-centric synchronization\n", "abstract": " Previously, we developed a data-centric approach to concurrency control in which programmers specify synchronization constraints declaratively, by grouping shared locations into atomic sets. We implemented our ideas in a Java extension called AJ, using Java locks to implement synchronization. We proved that atomicity violations are prevented by construction, and demonstrated that realistic Java programs can be refactored into AJ without significant loss of performance. This paper presents an algorithm for detecting possible deadlock in AJ programs by ordering the locks associated with atomic sets. In our approach, a type-based static analysis is extended to handle recursive data structures by considering programmer-supplied, compiler-verified lock ordering annotations. In an evaluation of the algorithm, all 10 AJ programs under consideration were shown to be deadlock-free. One program needed 4 ordering\u00a0\u2026", "num_citations": "30\n", "authors": ["288"]}
{"title": "Chianti: A prototype change impact analysis tool for Java\n", "abstract": " This paper reports on the design and implementation of Chianti, a change impact analysis tool for Java that is implemented in the context of the Eclipse environment. Chianti analyzes two versions of an application and decomposes their difference into a set of atomic changes. Change impact is reported in terms of affected tests whose execution behavior may have been modified by the applied changes. For each affected test, Chianti also determines a set of affecting changes that were responsible for the test\u2019s modified behavior. We evaluated Chianti on 6 months of data from M. Ernst\u2019s Daikon system, and found that, on average, 62.4% of the tests is affected. Furthermore, each affected test, on average, is affected by only 5.6% of the atomic changes. These findings suggest that change impact analysis is a promising technique for assisting developers with program understanding and debugging.", "num_citations": "30\n", "authors": ["288"]}
{"title": "Supporting high level programming with high performance: The Illinois Concert system\n", "abstract": " Programmers of concurrent applications are faced with a complex performance space in which data distribution and concurrency management exacerbate the difficulty of building large, complex applications. To address these challenges, the Illinois Concert system provides a global names-pace, implicit concurrency control and granularity management, implicit storage management, and object oriented programming features. These features are embodied in a language ICC++ (derived from C++) which has been used to build a number of kernels and applications. As high level features can potentially incur overhead, the Concert system employs a range of compiler and runtime optimization techniques to efficiently support the high level programming model. The compiler techniques include type inference, inlining and specialization; and the runtime techniques include caching, prefetching and hybrid stack/heap\u00a0\u2026", "num_citations": "30\n", "authors": ["288"]}
{"title": "Customization of Java library classes using type constraints and profile information\n", "abstract": " The use of class libraries increases programmer productivity by allowing programmers to focus on the functionality unique to their application. However, library classes are generally designed with some typical usage pattern in mind, and performance may be suboptimal if the actual usage differs. We present an approach for rewriting applications to use customized versions of library classes that are generated using a combination of static analysis and profile information. Type constraints are used to determine where customized classes may be used, and profile information is used to determine where customization is likely to be profitable. We applied this approach to a number of Java applications by customizing various standard container classes and the omnipresent StringBuffer class, and measured speedups up to 78% and memory footprint reductions up to 46%. The increase in application size due to\u00a0\u2026", "num_citations": "29\n", "authors": ["288"]}
{"title": "Scaleable ontology reasoning to explain inferences made by a tableau reasoner\n", "abstract": " Methods and apparatus, including computer program products, for scalable ontology reasoning. A method of generating a summarized ontology includes loading an ontology from a store, eliminating relationships in the ontology, the eliminating relationships including an insertion of new relationships that simplify the ontology, eliminating individuals in the ontology, the eliminating individuals including insertion of new individuals to simplify the ontology, eliminating concepts in the ontology including insertion of new concepts to simplify the ontology, and generating the summarized ontology from the eliminating relationships, eliminating individuals and eliminating concepts.", "num_citations": "28\n", "authors": ["288"]}
{"title": "Optimizing sparse schema-less data in relational stores\n", "abstract": " Various embodiments of the invention relate to optimizing storage of schema-less data. A schema-less dataset including a plurality of resources is received. Each resource is associated with at least a plurality of properties. At least one set of co-occurring properties from the plurality of properties is identified. A graph including a plurality of nodes is generated. Each of the nodes represents a unique property in the set of co-occurring properties. The graph further includes an edge connecting each node representing a pair of co-occurring properties. A graph coloring operation is performed on the graph. The graph coloring operation includes assigning each of nodes to a color, where nodes connected by an edge are assigned different colors. A schema is generated that assigns a column identifier from a table to each unique property represented by one of the nodes in the graph based on the color assigned to the node.", "num_citations": "26\n", "authors": ["288"]}
{"title": "Finding incorrect compositions of atomicity\n", "abstract": " In object-oriented code, atomicity is ideally isolated in a library which encapsulates shared program state and provides atomic APIs for access. The library provides a convenient way for programmers to reason about the needed synchronization. However, as the library exports a limited set of APIs, it cannot satisfy every unplanned atomicity demand; therefore, clients may have to compose invocations of the library APIs to obtain new atomic functionality. This process is error-prone due to the complexity of reasoning required, hence tool support for uncovering incorrect compositions (ie, atomic compositions that are implemented incorrectly) would be very helpful. A key difficulty is how to determine the intended atomic compositions, which are rarely documented. Existing inference techniques cannot be used to infer the atomic compositions because they cannot recognize the library and the client, which requires\u00a0\u2026", "num_citations": "25\n", "authors": ["288"]}
{"title": "Compile-time analysis and specialization of clocks in concurrent programs\n", "abstract": " Clocks are a mechanism for providing synchronization barriers in concurrent programming languages. They are usually implemented using primitive communication mechanisms and thus spare the programmer from reasoning about low-level implementation details such as remote procedure calls and error conditions.             Clocks provide flexibility, but programs often use them in specific ways that do not require their full implementation. In this paper, we describe a tool that mitigates the overhead of general-purpose clocks by statically analyzing how programs use them and choosing optimized implementations when available.             We tackle the clock implementation in the standard library of the X10 programming language\u2014a parallel, distributed object-oriented language. We report our findings for a small set of analyses and benchmarks. Our tool only adds a few seconds to analysis time, making it\u00a0\u2026", "num_citations": "25\n", "authors": ["288"]}
{"title": "Correct refactoring of concurrent software\n", "abstract": " Automated refactorings as implemented in modern IDEs for Java usually make no special provisions for concurrent code. Thus, refactored programs may exhibit unexpected new concurrent behaviors. We analyze the types of such behavioral changes caused by current refactoring engines and develop techniques to make them behavior-preserving, ranging from simple techniques to deal with concurrency-related language constructs to a framework that computes and tracks synchronization dependencies. By basing our development directly on the Java Memory Model we can state and prove precise correctness results about refactoring concurrent programs. We show that a broad range of refactorings are not influenced by concurrency at all, whereas other important refactorings can be made behavior-preserving for correctly synchronized programs by using our framework. Experience with a prototype\u00a0\u2026", "num_citations": "24\n", "authors": ["288"]}
{"title": "Dynamic detection of atomic-set-serializability violations\n", "abstract": " A method, an information processing system, and a computer readable medium, are used to detect atomic-set serializability violations in an execution of a program. A set of classes associated with a program to be analyzed is identified. The set of classes include a set of fields. At least one subset of fields in the set of fields in the identified classes is selected. A set of code fragments associated with an execution of the program is selected. Data accesses in the selected set of code fragments are observed. It is determined if the selected set of code fragments is serializable for each selected subset of fields.", "num_citations": "21\n", "authors": ["288"]}
{"title": "Generating additional user inputs for fault detection and localization in dynamic software applications\n", "abstract": " The present invention provides a system, computer program product and a computer implemented method for prioritizing code fragments based on the use of a software oracle and on a correlation between the executed code fragments and the output they produce. Also described is a computer-implemented method generates additional user inputs based on execution information associated with path constraints and based on information from the oracle. Advantageously, the embodiment is useful in a test generation tool that generated many similar inputs when a failure-inducing input is found, in order to enhance fault localization. Further, described is a computer-implemented flow for extending the existing idea of concolic testing to applications that interact with persistent state.", "num_citations": "19\n", "authors": ["288"]}
{"title": "Refactoring techniques for migrating applications to generic Java container classes\n", "abstract": " Version 1.5 of the Java programming language will include generics, a language construct for associating type parameters with classes and methods. Generics are particularly useful for creating statically type-safe, reusable container classes such that a store of an inappropriate type causes a compile-time error, and that no down-casting is needed when retrieving elements. The standard libraries released with Java 1.5 will include generic versions of popular container classes such as HashMap and ArrayList. This paper presents a method for refactoring Java programs that use current container classes into equivalent Java 1.5 programs that use their new generic counterparts. Our method uses a variation on an existing model of type constraints to infer the element types of container objects, and it is parameterized by how much, if any, context sensitivity to exploit when generating these type constraints. We present both a context-insensitive instantiation of the framework and one using a low-cost variation on Agesen\u2019s Cartesian Product Algorithm. The method has been implemented in Eclipse, a popular open-source development environment for Java. We evaluated our approach on several small benchmark programs, and found that, in all but one case, between 40% and 100% of all casts can be removed.", "num_citations": "19\n", "authors": ["288"]}
{"title": "Refactoring programs for flexible locking\n", "abstract": " Disclosed is a novel computer implemented system, on demand service, computer program product and a method that provides a set of lock usages that improves concurrency resulting in execution performance of the software application by reducing lock contention through refactoring. More specifically, disclosed is a method to refactor a software application. The method starts with accessing at least a portion of a software application that can execute in an operating environment where there are more two or more threads of execution. Next, a determination is made if there is at least one lock used in the software application to enforce limits on accessing a resource. In response to determining that there is a lock with a first type of construct with a given set of features, the software application is refactored with the lock to preserve behavior of the software application.", "num_citations": "18\n", "authors": ["288"]}
{"title": "Jcrypt: Towards computation over encrypted data\n", "abstract": " Cloud computing allows clients to upload data and computation to untrusted servers, which leads to potential violations to the confidentiality of client data. We propose JCrypt, a static program analysis which transforms a Java program into an equivalent one, so that it performs computation over encrypted data and preserves data confidentiality. JCrypt minimizes computation over encrypted data. It consists of two stages. The first stage is a type-based information flow analysis which partitions the program so that only sensitive parts need to be encrypted. The second stage is an inter-procedural data-flow analysis, similar to the classical Available Expressions. It deduces the appropriate encryption scheme for sensitive variables. We implemented JCrypt for Java and showed that our analysis is effective and practical using five benchmark suites. JCrypt encrypts a significantly larger percentage of benchmarks compared\u00a0\u2026", "num_citations": "17\n", "authors": ["288"]}
{"title": "Effective smart completion for JavaScript\n", "abstract": " Many modern IDEs offer a smart completion feature: when the programmer enters an expression e followed by a dot character, the IDE offers a list of properties available on e to complete the expression. For statically typed languages, this list is easily computed from the type of e and the class hierarchy. In JavaScript, the same problem is much more challenging, since expressions are not statically typed and object properties may change over time. The problem is exacerbated by the common use of complex framework libraries like jQuery and native libraries like the browser DOM. We present a novel approach to JavaScript smart completion based on combining static and dynamic analysis. Completions are computed using a static pointer analysis enhanced with support for usage-based property inference. Frameworks are handled using a fully automatic dynamic analysis which infers API models based on the\u00a0\u2026", "num_citations": "17\n", "authors": ["288"]}
{"title": "Static code analysis for packaged application customization\n", "abstract": " A method for static code analyzing customizations to a pre-packaged computing solution can include establishing a communicative connection from a recommendation generation module to a pre-packaged computing solution and authenticating into the pre-packaged computing solution. Customized program code can be extracted from the pre-packaged computing solution and a call graph of the customized program code can be constructed such that the call graph indicates method calls to different interfaces for program code of the pre-packaged computing solution. Finally, a report can be generated identifying customized program code to be adapted to a new version of the pre-packaged computing solution based upon changes in the different interfaces shown by the call graph to be used in the new version of the pre-packaged computing solution and modifications required for the customized program code to\u00a0\u2026", "num_citations": "16\n", "authors": ["288"]}
{"title": "Efficient handling of string-number conversion\n", "abstract": " String-number conversion is an important class of constraints needed for the symbolic execution of string-manipulating programs. In particular solving string constraints with string-number conversion is necessary for the analysis of scripting languages such as JavaScript and Python, where string-number conversion is a part of the definition of the core semantics of these languages. However, solving this type of constraint is very challenging for the state-of-the-art solvers. We propose in this paper an approach that can efficiently support both string-number conversion and other common types of string constraints. Experimental results show that it significantly outperforms other state-of-the-art tools on benchmarks that involves string-number conversion.", "num_citations": "15\n", "authors": ["288"]}
{"title": "Ariadne: analysis for machine learning programs\n", "abstract": " Machine learning has transformed domains like vision and translation, and is now increasingly used in science, where the correctness of such code is vital. Python is popular for machine learning, in part because of its wealth of machine learning libraries, and is felt to make development faster; however, this dynamic language has less support for error detection at code creation time than tools like Eclipse. This is especially problematic for machine learning: given its statistical nature, code with subtle errors may run and produce results that look plausible but are meaningless. This can vitiate scientific results. We report on: applying a static framework, WALA, to machine learning code that uses TensorFlow. We have created static analysis for Python, a type system for tracking tensors\u2014Tensorflow\u2019s core data structures\u2014and a data flow analysis to track their usage. We report on how it was built and present some early\u00a0\u2026", "num_citations": "15\n", "authors": ["288"]}
{"title": "Revamping JavaScript static analysis via localization and remediation of root causes of imprecision\n", "abstract": " Static analysis is challenged by the dynamic language constructs of JavaScript which often lead to unacceptable performance and/or precision results. We describe an approach that focuses on improving the practicality and accuracy of points-to analysis and call graph construction for JavaScript programs. The approach first identifies program constructs which are sources of imprecision (ie, root causes) through monitoring the static analysis process. We then examine and suggest specific context-sensitive analyses to apply. Our technique is able to to find that the root causes comprise less than 2% of the functions in JavaScript library applications. Moreover, the specialized analysis derived by our approach finishes within a few seconds, even on programs which can not complete within 10 minutes with the original analysis.", "num_citations": "15\n", "authors": ["288"]}
{"title": "Using static analysis for IDE\u2019s for dynamic languages\n", "abstract": " Modern IDE\u2019s for languages such as Java exploit the static type system of the language to provide shortcuts and hints to aid programmers with common programming tasks. An example of this help is auto-completion of field and method names when a programmer types a \u201c.\u201d operator. In dynamic languages, such as scripting languages like JavaScript [1] and programming languages like Scheme [3], there is no such static type system that can be exploited in this manner. However, a deeper analysis of the source program may be able to provide equivalent information that allows similar levels of functionality in an IDE for dynamic languages. To enable this, we discuss various uses to which analysis data could be put, and we argue for a layered approach that exploits the same analysis infrastructure across multiple languagesModern IDE\u2019s provide a range of shortcuts and wizards to ease, and to some extent automate, many tedious aspects of programming. While some of these shortcuts are straightforward insertion of boilerplate text\u2014such as a \u201cnew class\u201d operation for Java that simply inserts some syntax\u2014there are many that are context-sensitive in some manner\u2014such as auto-completion of field and method names. In staticallytypes languages such as Java and C++, much of this context information can be derived from declared type information in a relatively straightforward manner. For instance, the explicit type information for the left-hand side expression of a \u201c.\u201d or \u201c->\u201d operator can be used to generate a fairly good list of possible completions for constants on the right hand side. Similarly, IDEs often indicate visually when some method\u00a0\u2026", "num_citations": "14\n", "authors": ["288"]}
{"title": "Constructing call graphs of Scala programs\n", "abstract": " As Scala gains popularity, there is growing interest in programming tools for it. Such tools often require call graphs. However, call graph construction algorithms in the literature do not handle Scala features, such as traits and abstract type members. Applying existing call graph construction algorithms to the JVM bytecodes generated by the Scala compiler produces very imprecise results due to type information being lost during compilation. We adapt existing call graph construction algorithms, Name-Based Resolution (RA) and Rapid Type Analysis (RTA), for Scala, and present a formalization based on Featherweight Scala. We evaluate our algorithms on a collection of Scala programs. Our results show that careful handling of complex Scala constructs greatly helps precision and that our most precise analysis generates call graphs with 1.1-3.7 times fewer nodes and 1.5-18.7 times fewer edges than a\u00a0\u2026", "num_citations": "13\n", "authors": ["288"]}
{"title": "Static Analysis of Shape in TensorFlow Programs\n", "abstract": " Machine learning has been widely adopted in diverse science and engineering domains, aided by reusable libraries and quick development patterns. The TensorFlow library is probably the best-known representative of this trend and most users employ the Python API to its powerful back-end. TensorFlow programs are susceptible to several systematic errors, especially in the dynamic typing setting of Python. We present Pythia, a static analysis that tracks the shapes of tensors across Python library calls and warns of several possible mismatches. The key technical aspects are a close modeling of library semantics with respect to tensor shape, and an identification of violations and error-prone patterns. Pythia is powerful enough to statically detect (with 84.62% precision) 11 of the 14 shape-related TensorFlow bugs in the recent Zhang et al. empirical study-an independent slice of real-world bugs.", "num_citations": "12\n", "authors": ["288"]}
{"title": "Who you gonna call?: analyzing web requests in Android applications\n", "abstract": " Relying on ubiquitous Internet connectivity, applications on mobile devices frequently perform web requests during their execution. They fetch data for users to interact with, invoke remote functionalities, or send user-generated content or meta-data. These requests collectively reveal common practices of mobile application development, like what external services are used and how, and they point to possible negative effects like security and privacy violations, or impacts on battery life. In this paper, we assess different ways to analyze what web requests Android applications make. We start by presenting dynamic data collected from running 20 randomly selected Android applications and observing their network activity. Next, we present a static analysis tool, Stringoid, that analyzes string concatenations in Android applications to estimate constructed URL strings. Using Stringoid, we extract URLs from 30, 000\u00a0\u2026", "num_citations": "12\n", "authors": ["288"]}
{"title": "Finding optimal query plans\n", "abstract": " Systems and methods for optimizing a query, and more particularly, systems and methods for finding optimal plans for graph queries by casting the task of finding the optimal plan as an integer programming (ILP) problem. A method for optimizing a query, comprises building a data structure for a query, the data structure including a plurality of components, wherein each of the plurality of components corresponds to at least one graph pattern, determining a plurality of flows of query variables between the plurality of components, and determining a combination of the plurality of flows between the plurality of components that results in a minimum cost to execute the query.", "num_citations": "12\n", "authors": ["288"]}
{"title": "Extracting enterprise vocabularies using linked open data\n", "abstract": " A common vocabulary is vital to smooth business operation, yet codifying and maintaining an enterprise vocabulary is an arduous, manual task. We describe a process to automatically extract a domain specific vocabulary (terms and types) from unstructured data in the enterprise guided by term definitions in Linked Open Data (LOD). We validate our techniques by applying them to the IT (Information Technology) domain, taking 58 Gartner analyst reports and using two specific LOD sources \u2013 DBpedia and Freebase. We show initial findings that address the generalizability of these techniques for vocabulary extraction in new domains, such as the energy industry.", "num_citations": "12\n", "authors": ["288"]}
{"title": "Method and apparatus for storing sparse graph data as multi-dimensional cluster\n", "abstract": " A system for storing graph data as a multi-dimensional cluster having a database with a graph dataset containing data and relationships between data pairs and a schema list of storage methods that use a table with columns and rows associated with data or relationships. An analyzer module to collect statistics of a graph dataset and a dimension identification module to identify a plurality of dimensions that each represent a column in the table. A schema creation and loading module creates a modified storage method and having a plurality of distinct table blocks and a plurality of table block indexes, one index for each table block and arranges the data and relationships in the given graph dataset in accordance with the modified storage method to create the multi-dimensional cluster.", "num_citations": "11\n", "authors": ["288"]}
{"title": "Efficient reasoning on large SHIN Aboxes in relational databases\n", "abstract": " As applications based on semantic web technologies enter the mainstream, there is a need to provide highly efficient ontology reasoning over large Aboxes. A common approach to achieving scalability is to build reasoners for DL subsets (eg, the EL family of languages, DL-Lite, DLP, or OWL-Prime). However, the proliferation of DL subsets runs counter to standardization efforts. In this paper, we present a hybrid approach which combines a fast, incomplete reasoning algorithm with a slower complete reasoning algorithm to handle the more expressive features of DL. Our approach works for SHIN. We demonstrate the effectiveness of this approach on large datasets (30-60 million assertions), where we show that performance on this hybrid approach provides significant performance gains (an average of 15 mins per query compared to 100 mins) without sacrificing completeness or expressivity.", "num_citations": "11\n", "authors": ["288"]}
{"title": "Automatically running tests against WEB APIs based on specifications\n", "abstract": " A method and system of determining whether a specification is an accurate representation of an application program interface (API) is provided. The specification is received electronically over a network. Service calls to be tested are identified based on the specification. A test case is created for each of the identified service calls. A sequence is created for the test cases. A test plan is generated based on the created sequence. The generated test plan is executed. Upon identifying an error in response to the executed test plan, a notification is generated, indicating that the specification is not an accurate representation of the API.", "num_citations": "10\n", "authors": ["288"]}
{"title": "Validating structural properties of nested objects\n", "abstract": " Frameworks are widely used to facilitate software reuse and accelerate development time. However, there are currently no systematic mechanisms to enforce the explicit and implicit rules of these frameworks. This paper focuses on a class of framework rules that place restrictions on the properties of data structures in framework applications. We present a mechanism to enforce these rules by the use of a generic\" bad store template\" which can be customized for different rule instances. We demonstrate the use of this template to validate specific bad store rules within J2EE framework applications. Violations of these rules cause subtle defects which manifest themselves at runtime as data loss, data corruption, or race conditions. Our algorithm to detect\" bad stores\" is implemented in the Smart Analysis-Based Error Reduction (SABER) validation tool, where we pay special attention to facilitating problem understanding\u00a0\u2026", "num_citations": "10\n", "authors": ["288"]}
{"title": "A Study of Call Graph Construction for JVM-Hosted Languages\n", "abstract": " Call graphs have many applications in software engineering, including bug-finding, security analysis, and code navigation in IDEs. However, the construction of call graphs requires significant investment in program analysis infrastructure. An increasing number of programming languages compile to the Java Virtual Machine (JVM), and program analysis frameworks such as WALA and SOOT support a broad range of program analysis algorithms by analyzing JVM bytecode. This approach has been shown to work well when applied to bytecode produced from Java code. In this paper, we show that it also works well for diverse other JVM-hosted languages: dynamically-typed functional Scheme, statically-typed object-oriented Scala, and polymorphic functional OCaml. Effectively, we get call graph construction for these languages for free, using existing analysis infrastructure for Java, with only minor challenges to soundness. This, in turn, suggests that bytecode-based analysis could serve as an implementation vehicle for bug-finding, security analysis, and IDE features for these languages. We present qualitative and quantitative analyses of the soundness and precision of call graphs constructed from JVM bytecodes for these languages, and also for Groovy, Clojure, Python, and Ruby. However, we also show that implementation details matter greatly. In particular, the JVM-hosted implementations of Groovy, Clojure, Python, and Ruby produce very unsound call graphs, due to the pervasive use of reflection, invokedynamic instructions, and run-time code generation. Interestingly, the dynamic translation schemes employed by these languages, which\u00a0\u2026", "num_citations": "9\n", "authors": ["288"]}
{"title": "Analysis to check web api code usage and specification\n", "abstract": " A debugging tool and method for statically verifying programs that invoke web-based services through API calls is provided. The tool receives source code that comprises one or more invocation of web APIs for requesting web-based services. The tool also receives a set of web API specifications. The tool extracts a set of request information for each web API invocation in the source code, the set of request information including a usage string of an URL endpoint. The tool verifies whether the set of request information complies with the received web API specifications and reports a result of the verification.", "num_citations": "9\n", "authors": ["288"]}
{"title": "SYSTEM AND METHOD FOR DEBUGGING MEMORY CONSISTENCY MODELS\n", "abstract": " A system and method for analyzing a test program with respect to a memory model includes preprocessing a test program into an intermediate form and translating the intermediate form of the test program into a relational logic representation. The relational logic representation is combined with a memory model to produce a legality formula. A set of bounds are computed on a space to be searched for the memory model or on a core of the legality formula. A relational satisfiability problem is solved, which is defined by the legality formula and the set of bounds to determine a legal trace of the test program or debug the memory model.", "num_citations": "9\n", "authors": ["288"]}
{"title": "Static detection of atomic-set-serializability violations\n", "abstract": " Vaziri et al. propose a data-centric approach to synchronization. The key underlying concept of their work is the atomic set, which specifies the existence of an invariant that holds on a set of fields of an object type. In addition, they formalize a set of eleven data-access scenarios that completely specify the set of non-serializable interleaving patterns that can lead to an atomic-set serializability violation of the expressed invariant.  We present an algorithm that uses state-space exploration techniques to statically detect atomic-set serializability violations. The key idea is that the data-access scenarios can be used as a property specification for a software model checker. We tested our technique on programs with known serialiability violations from the concurrency-testing benchmark created by Eytani et al. Of the ten programs analyzed, our tool reported eight atomic-set serializability violations, with seven of them being true bugs.", "num_citations": "9\n", "authors": ["288"]}
{"title": "Type-based call graph construction algorithms for Scala\n", "abstract": " Call graphs have many applications in software engineering. For example, they serve as the basis for code navigation features in integrated development environments and are at the foundation of static analyses performed in verification tools. While many call graph construction algorithms have been presented in the literature, we are not aware of any that handle Scala features such as traits and abstract type members. Applying existing algorithms to the JVM bytecodes generated by the Scala compiler produces very imprecise results because type information is lost during compilation. We adapt existing type-based call graph construction algorithms to Scala and present a formalization based on Featherweight Scala. An experimental evaluation shows that our most precise algorithm generates call graphs with 1.1--3.7 times fewer nodes and 1.5--17.3 times fewer edges than a bytecode-based RTA analysis.", "num_citations": "8\n", "authors": ["288"]}
{"title": "A Case Study in Irregular Parallel Programming.\n", "abstract": " Massively-parallel machines have achieved impressive performance on a variety of regular parallel applications. However, for these machines to gain broader acceptance, they must achieve high performance on irregular parallel applications as well. We compare two approaches for programming irregular applications, describing the resulting program structures, our programming experience, and the achieved performance. We use an application program, the\" Box of Rocks,\" to compare the data parallel (vector) and concurrent object oriented models. The Box of Rocks is an irregular, discrete physical simulation involving piles of dirt clumps. For each model, we describe the steps required to get good performance. From this study it is clear that it is quite difficult to tune an irregular, parallel program for the data parallel model. In contrast mapping irregular parallelism into the concurrent object-oriented model is straightforward. A frequently cited drawback of the concurrent object-oriented model is that the programmer must deal with explicit concurrency, but actually programmers are already dealing with explicit concurrency when they insert vectorization directives into their data parallel programs. Performance numbers for both systems are also presented.", "num_citations": "8\n", "authors": ["288"]}
{"title": "Python 3 types in the wild: a tale of two type systems\n", "abstract": " Python 3 is a highly dynamic language, but it has introduced a syntax for expressing types with PEP484. This paper explores how developers use these type annotations, the type system semantics provided by type checking and inference tools, and the performance of these tools. We evaluate the types and tools on a corpus of public GitHub repositories. We review MyPy and PyType, two canonical static type checking and inference tools, and their distinct approaches to type analysis. We then address three research questions:(i) How often and in what ways do developers use Python 3 types?(ii) Which type errors do developers make?(iii) How do type errors from different tools compare?", "num_citations": "7\n", "authors": ["288"]}
{"title": "Race detection for web applications\n", "abstract": " A method of executing a rendering engine including executing a web application including at least two operations a single thread of execution, generating an auxiliary map for instrumentation accesses of the web application, and detecting and reporting concurrent memory accesses of the web application as a race.", "num_citations": "7\n", "authors": ["288"]}
{"title": "Evaluating high level parallel programming support for irregular applications in ICC++\n", "abstract": " Object\u2010oriented techniques have been proffered as aids for managing complexity, enhancing reuse, and improving readability of irregular parallel applications. However, as performance is the major reason for employing parallelism, programmability and high performance must be delivered together. Using a suite of seven challenging irregular applications and the mature Illinois Concert system (a high\u2010level concurrent object\u2010oriented programming model) and an aggressive implementation (whole program compilation plus microsecond threading and communication primitives in the runtime), we evaluate what programming efforts are required to achieve high performance. For all seven applications, we achieve performance comparable to the best achievable via low\u2010level programming means on large\u2010scale parallel systems. In general, a high\u2010level concurrent object\u2010oriented programming model supported by\u00a0\u2026", "num_citations": "7\n", "authors": ["288"]}
{"title": "Graph4code: A machine interpretable knowledge graph for code\n", "abstract": " Knowledge graphs have proven extremely useful in powering diverse applications in semantic search and natural language understanding. Graph4Code is a knowledge graph about program code that can similarly power diverse applications such as program search, code understanding, refactoring, bug detection, and code automation. The graph uses generic techniques to capture the semantics of Python code: the key nodes in the graph are classes, functions and methods in popular Python modules. Edges indicate function usage (e.g., how data flows through function calls, as derived from program analysis of real code), and documentation about functions (e.g., code documentation, usage documentation, or forum discussions such as StackOverflow). We make extensive use of named graphs in RDF to make the knowledge graph extensible by the community. We describe a set of generic extraction techniques that we applied to over 1.3M Python files drawn from GitHub, over 2,300 Python modules, as well as 47M forum posts to generate a graph with over 2 billion triples. We also provide a number of initial use cases of the knowledge graph in code assistance, enforcing best practices, debugging and type inference. The graph and all its artifacts are available to the community for use.", "num_citations": "5\n", "authors": ["288"]}
{"title": "SecureMR: secure mapreduce computation using homomorphic encryption and program partitioning\n", "abstract": " In cloud computing customers upload data and computation to cloud providers. As they upload their data to the cloud provider, they typically give up data confidentiality. We develop SecureMR, a system that analyzes and transforms MapReduce programs to operate over encrypted data. SecureMR makes use of partially homomorphic encryption and a trusted client. We evaluate SecureMR on a set of complex computation-intensive MapReduce benchmarks.", "num_citations": "5\n", "authors": ["288"]}
{"title": "Method and apparatus for identifying the optimal schema to store graph data in a relational store\n", "abstract": " A system for identifying a schema for storing graph data includes a database containing a graph dataset of data and relationships between data pairs and a list of storage methods that each are a distinct structural arrangement of the data and relationships from the graph data set. An analyzer module collects statistics for the graph dataset, and a data classification module uses the collected statistics to calculate metrics describing the data and relationships in the graph dataset, uses the calculated metrics to group the data and relationships into a plurality of graph dataset subsets and. associates each graph dataset subset with one of the plurality of storage methods. The resulting group of storage methods associated with the plurality of graph dataset subsets includes a unique storage method for each graph dataset subset. The data and relationships in each graph dataset subset are arranged in accordance with\u00a0\u2026", "num_citations": "5\n", "authors": ["288"]}
{"title": "Automated discovery of programmatic resources\n", "abstract": " A method for business process to customized program code mapping in a pre-packaged computing solution can include establishing a connection from a business process to code mapping module executing in memory by a processor of a computer to a source pre-packaged computing solution, extracting both a business process hierarchy (BPH) and customized program code from the source pre-packaged computing solution, storing the BPH and the extracted customized program code in storage coupled to the computer, selecting a business process in the BPH and mapping the selected business process to corresponding portions of the customized program code, and generating and displaying in the computer a dependency graph indicating dependency relationships of the mapped portions of the customized program code for the selected business process of the BPH of the source pre-packaged computing\u00a0\u2026", "num_citations": "5\n", "authors": ["288"]}
{"title": "Evaluating high level parallel programming support for irregular applications in ICC++\n", "abstract": " 2 BackgroundIn evaluating a programming approach, our goal is to identify the concerns which must be addressed by a programmer to achieve good performance and how those concerns are managed. Two high-Ievel concerns are of most interest: concurrency and synchronization specification, and data locality and load balance. The former is required to express the parallelism structure, while the latter ensures efficient execution. We describe interfaces in the Concert system which aid in the management of these two concerns. 1Basic Programming Interlace Our programming interface is an object-oriented programming model augmented with simple extensions for concurrency. The model is based on C++(ICC++[14]), providing a single namespace, scalars and objects, and single inheritance. Simple extensions for concurrency include annotating standard blocks (compound statements) and loops with the conc", "num_citations": "5\n", "authors": ["288"]}
{"title": "Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks\n", "abstract": " Advancements in deep learning and machine learning algorithms have enabled breakthrough progress in computer vision, speech recognition, natural language processing and beyond. In addition, over the last several decades, software has been built into the fabric of every aspect of our society. Together, these two trends have generated new interest in the fast-emerging research area of AI for Code. As software development becomes ubiquitous across all industries and code infrastructure of enterprise legacy applications ages, it is more critical than ever to increase software development productivity and modernize legacy applications. Over the last decade, datasets like ImageNet, with its large scale and diversity, have played a pivotal role in algorithmic advancements from computer vision to language and speech understanding. In this paper, we present Project CodeNet, a first-of-its-kind, very large scale, diverse, and high-quality dataset to accelerate the algorithmic advancements in AI for Code. It consists of 14M code samples and about 500M lines of code in 55 different programming languages. Project CodeNet is not only unique in its scale, but also in the diversity of coding tasks it can help benchmark: from code similarity and classification for advances in code recommendation algorithms, and code translation between a large variety programming languages, to advances in code performance (both runtime, and memory) improvement techniques. CodeNet also provides sample input and output test sets for over 7M code samples, which can be critical for determining code equivalence in different languages. As a usability feature, we\u00a0\u2026", "num_citations": "4\n", "authors": ["288"]}
{"title": "QED: out-of-the-box datasets for SPARQL query evaluation\n", "abstract": " In this paper, we present SPARQL QED, a system generating out-of-the-box datasets for SPARQL queries over linked data. QED distinguishes the queries according to the different SPARQL features and creates, for each query, a small but exhaustive dataset comprising linked data and the query answers over this data. These datasets can support the development of applications based on SPARQL query answering in various ways. For instance, they may serve as SPARQL compliance tests or can be used for learning in query-by-example systems. We ensure that the created datasets are diverse and cover various practical use cases and, of course, that the sets of answers included are the correct ones. Example tests generated based on queries and data from DBpedia have shown bugs in Jena and Virtuoso.", "num_citations": "4\n", "authors": ["288"]}
{"title": "Automatically Extracting Web API Specifications from HTML Documentation\n", "abstract": " Web API specifications are machine-readable descriptions of APIs. These specifications, in combination with related tooling, simplify and support the consumption of APIs. However, despite the increased distribution of web APIs, specifications are rare and their creation and maintenance heavily relies on manual efforts by third parties. In this paper, we propose an automatic approach and an associated tool called D2Spec for extracting specifications from web API documentation pages. Given a seed online documentation page on an API, D2Spec first crawls all documentation pages on the API, and then uses a set of machine learning techniques to extract the base URL, path templates, and HTTP methods, which collectively describe the endpoints of an API. We evaluated whether D2Spec can accurately extract endpoints from documentation on 120 web APIs. The results showed that D2Spec achieved a precision of 87.5% in identifying base URLs, a precision of 81.3% and a recall of 80.6% in generating path templates, and a precision of 84.4% and a recall of 76.2% in extracting HTTP methods. In addition, we found that D2Spec was useful when applied to APIs with pre-existing API specifications: D2Spec revealed many inconsistencies between web API documentation and their corresponding publicly available specifications. Thus, D2Spec can be used by web API providers to keep documentation and specifications in synchronization.", "num_citations": "4\n", "authors": ["288"]}
{"title": "High level parallel programming: the illinois concert system\n", "abstract": " Programmers of concurrent applications are faced with complex performance trade-o s, since data distribution and concurrency management exacerbate the di culty of building large, complex applications. To address these challenges, the Illinois Concert system provides a global namespace, implicit concurrency control and granularity management, implicit storage management, and object-oriented programming features. These features are embodied in a language ICC++(derived from C++) which has been used to build a number of kernels and applications. As high level features can potentially incur overhead, the Concert system employs a range of compiler and runtime optimization techniques to e ciently support the high level programming model. The compiler techniques include type inference, inlining and specialization; and the runtime techniques include caching, prefetching and hybrid stack/heap multithreading. The e ectiveness of these techniques permits the construction of complex parallel applications that are portable and exible, enabling convenient application modi cation or tuning. We present performance results for a number of application programs which attain good speedups and absolute performance.", "num_citations": "4\n", "authors": ["288"]}
{"title": "SWAN: a static analysis framework for swift\n", "abstract": " Swift is an open-source programming language and Apple's recommended choice for app development. Given the global widespread use of Apple devices, the ability to analyze Swift programs has significant impact on millions of users. Although static analysis frameworks exist for various computing platforms, there is a lack of comparable tools for Swift. While LLVM and Clang support some analyses for Swift, they are either primarily dynamic analyses or not suitable for deeper analyses of Swift programs such as taint tracking. Moreover, other existing tools for Swift only help enforce code styles and best practices.", "num_citations": "3\n", "authors": ["288"]}
{"title": "Cognitive virtual detector\n", "abstract": " Aspects of the present invention disclose a method, computer program product, and system for detecting and mitigating adversarial virtual interactions. The method includes one or more processors detecting a user communication that is interacting with a virtual agent. The method further includes one or more processors determining a risk level associated with the detected user communication based on one or more actions performed by the detected user while interacting with the virtual agent. The method further includes one or more processors in response to determining that the determined risk level associated with the detected user communication exceeds a risk level threshold, initiating, a mitigation protocol on interactions between the detected user and the virtual agent, where the mitigation protocol is based on the actions performed by the detected user while interacting with the virtual agent.", "num_citations": "3\n", "authors": ["288"]}
{"title": "Merging datasets through deep learning\n", "abstract": " Merging datasets is a key operation for data analytics. A frequent requirement for merging is joining across columns that have different surface forms for the same entity (e.g., the name of a person might be represented as \"Douglas Adams\" or \"Adams, Douglas\"). Similarly, ontology alignment can require recognizing distinct surface forms of the same entity, especially when ontologies are independently developed. However, data management systems are currently limited to performing merges based on string equality, or at best using string similarity. We propose an approach to performing merges based on deep learning models. Our approach depends on (a) creating a deep learning model that maps surface forms of an entity into a set of vectors such that alternate forms for the same entity are closest in vector space, (b) indexing these vectors using a nearest neighbors algorithm to find the forms that can be potentially joined together. To build these models, we had to adapt techniques from metric learning due to the characteristics of the data; specifically we describe novel sample selection techniques and loss functions that work for this problem. To evaluate our approach, we used Wikidata as ground truth and built models from datasets with approximately 1.1M people's names (200K identities) and 130K company names (70K identities). We developed models that allow for joins with precision@1 of .75-.81 and recall of .74-.81. We make the models available for aligning people or companies across multiple datasets.", "num_citations": "3\n", "authors": ["288"]}
{"title": "Extending SPARQL for Data Analytic Tasks\n", "abstract": " SPARQL has many nice features for accessing data integrated across different data sources, which is an important step in any data analysis task. We report on the use of SPARQL for two real data analytic use cases from the healthcare and life sciences domains, which exposed certain weaknesses in the current specification of SPARQL, specifically when the data being integrated is most conveniently accessed via RESTful services and in formats beyond RDF, such as XML. We therefore extended SPARQL with generalized service, constructs for accessing services beyond the SPARQL endpoints supported by service. For efficiency, our constructs support posting data, which is also not supported by service. We provide an open source implementation of this SPARQL endpoint in an RDF store called Quetzal, and evaluate its use in the two data analytic scenarios over real datasets.", "num_citations": "3\n", "authors": ["288"]}
{"title": "d2c: Deterministic, deadlock-free concurrency\n", "abstract": " The advent of multicore processors has made concurrent programming languages mandatory. However, most concurrent programming models come with two major pitfalls: non-determinism and deadlocks. By determinism, we mean the output behavior of the program is independent of the scheduling choices (eg, the operating system) and depends only on the input behavior. A few concurrent programming models provide deterministic behavior by providing constructs that impose additional synchronization, but the improper (or the out of order) use of these constructs leads to problems like deadlocks.In this paper, we argue for both determinism and deadlock-freedom and provide a deterministic, deadlockfree concurrent model. The model can be implemented either as programming language constructs or as a library. Any program that uses this model is guaranteed to produce the same output for a given input. Additionally, the program will never deadlock: the program will either terminate or run for ever.", "num_citations": "3\n", "authors": ["288"]}
{"title": "Using the concert debugger\n", "abstract": " Concurrent programming introduces whole new classes of programming errors in addition to the many already inherent in all programming: among these are synchronization errors and deadlocks. To facilitate nding both the new kinds of bugs and the old ones, the Concert debugger allows one to observe the state of a CA program at various stages of its execution. The Concert Debugger is built atop the Gnu Debugger and Emacs. The interface is through Emacs-Gdb mode, with a set of\\concert\" commands to make debugging CA programs easier.", "num_citations": "3\n", "authors": ["288"]}
{"title": "Generating web api specification from online documentation\n", "abstract": " A tool that automatically generates a web API specification from a web API documentation is provided. The tool extracts a base uniform resource locator (URL) string from the received documentation by identifying URL strings in the documentation that are valid web application programming interface (API) calls. The tool infers path templates by identifying and clustering path expressions in the documentation that invoke the same URL endpoints. The tool extracts hypertext transfer protocol (HTTP) request type and query parameters associated with the inferred path templates. The tool generates a specification that includes the extracted base URL, the inferred path templates, the extracted HTTP request types, and the extracted query parameters.", "num_citations": "2\n", "authors": ["288"]}
{"title": "Protecting chatbots from toxic content\n", "abstract": " There is a paradigm shift in web-based services towards conversational user interfaces. Companies increasingly offer conversational interfaces, or chatbots, to let their customers or employees interact with their services in a more flexible and mobile manner. Unfortunately, this new paradigm faces a major problem, namely toxic content in user inputs. Toxic content in user inputs to chatbots may cause privacy concerns, may be adversarial or malicious, and can cause the chatbot provider substantial economic, reputational, or legal harm. We address this problem with an interdisciplinary approach, drawing upon programming languages, cloud computing, and other disciplines to build protections for chatbots. Our solution, called BotShield, is non-intrusive in that it does not require changes to existing chatbots or underlying conversational platforms. This paper introduces novel security mechanisms, articulates their\u00a0\u2026", "num_citations": "2\n", "authors": ["288"]}
{"title": "Evaluating Call Graph Construction for JVM-hosted Language Implementations\n", "abstract": " An increasing number of programming languages compile to the Java Virtual Machine (JVM), and program analysis frameworks such as WALA and SOOT support a broad range of program analysis algorithms by analyzing bytecode. While this approach works well when applied to bytecode produced from Java code, its efficacy when applied to other bytecode has not been studied until now. We present qualitative and quantitative analysis of the soundness and precision of call graphs constructed from JVM bytecodes produced for Python, Ruby, Clojure, Groovy, Scala, and OCaml applications. We show that, for Python, Ruby, Clojure, and Groovy, the call graphs are unsound due to use of reflection, invokedynamic instructions, and run-time code generation, and imprecise due to how function calls are translated. For Scala and OCaml, all unsoundness comes from rare, complex uses of reflection and proxies, and the translation of first-class features in Scala incurs a significant loss of precision.", "num_citations": "2\n", "authors": ["288"]}
{"title": "Parasight: A debugger for concurrent object-oriented programs\n", "abstract": " Parallel machines, from supercomputers to networks of workstations to multiprocessor servers have vastly more potential computing power than even the most powerful sequential machines. Programming languages from data-parallel Fortran to messaging libraries to concurrent object-oriented models have been developed to exploit this power. However, to realize the potential of parallel machines, fully-edged programming systems are required to help the programmer handle the complexity inherent in parallelism. The Illinois Concert System is such a system, based upon a concurrent object-oriented programming model. Debugging support is a crucial feature of any programming system; it is especially important for parallel programming as the errors are more complex and harder to trace. This thesis describes the design and implementation of ParaSight, the Concert debugger. ParaSight is a source-level debugger providing a uni ed view of the execution of concurrent Concert programs. ParaSight includes two novel concurrent debugging mechanisms. The rst, called abstraction tracing, is based upon the notion of abstractions, that is, encapsulated portions of a program that interact with the rest in a controlled fashion. We provide a mechanism that allows such abstractions to be debugged in isolation. This paradigm can raise the level at which debugging is undertaken, from built in primitives up to the level of user-de ned abstractions. The other mechanism, called coherent printing, makes the most common form of sequential debugging, print statements, more useful on parallel systems. Coherent printing compensates for non-determinism and\u00a0\u2026", "num_citations": "2\n", "authors": ["288"]}
{"title": "Concert tutorial\n", "abstract": " Before Concert is installed anywhere, you should make sure that you are using a machine powerful enough to run the system. The Concert System will run on Sparc2s and and Sparc10s, and it requires at least 32MB of RAM to run the compiler comfortably; even more memory will help performance when compiling large programs.", "num_citations": "2\n", "authors": ["288"]}
{"title": "CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks\n", "abstract": " Over the last several decades, software has been woven into the fabric of every aspect of our society. As software development surges and code infrastructure of enterprise applications ages, it is now more critical than ever to increase software development productivity and modernize legacy applications. Advances in deep learning and machine learning algorithms have enabled breakthroughs in computer vision, speech recognition, natural language processing and beyond, motivating researchers to leverage AI techniques to improve software development efficiency. Thus, the fast-emerging research area of \u201cAI for Code\u201d has garnered new interest and gathered momentum. In this paper, we present a large-scale dataset\\textit {CodeNet}, consisting of over 14 million code samples and about 500 million lines of code in 55 different programming languages, which is aimed at teaching AI to code. In addition to its large scale, CodeNet has a rich set of high-quality annotations to benchmark and help accelerate research in AI techniques for a variety of critical coding tasks, including code similarity and classification, code translation between a large variety of programming languages, and code performance (runtime and memory) improvement techniques. Additionally, CodeNet provides sample input and output test sets for 98.5\\% of the code samples, which can be used as an oracle for determining code correctness and potentially guide reinforcement learning for code quality improvements. As a usability feature, we provide several pre-processing tools in CodeNet to transform source code into representations that can be readily used as inputs into\u00a0\u2026", "num_citations": "1\n", "authors": ["288"]}
{"title": "Extracting Hyperparameter Constraints from Code\n", "abstract": " Machine-learning operators often have correctness constraints that cut across multiple hyperparameters and/or data. Violating these constraints causes runtime exceptions, but they are usually documented only informally or not at all. This paper presents a weakest precondition analysis for Python code. We demonstrate our analysis by extracting hyperparameter constraints for 45 sklearn operators. Our analysis is a step towards safer and more robust machine learning.", "num_citations": "1\n", "authors": ["288"]}
{"title": "A Demonstration of CodeBreaker: A Machine Interpretable Knowledge Graph for Code\n", "abstract": " Knowledge graphs have been extremely useful in powering diverse applications like natural language understanding. CodeBreaker attempts to construct machine interpretable knowledge graphs about program code to similarly power diverse applications such as code search, code understanding, and code automation. We have built such a 1.98 billion edges knowledge graph by a detailed analysis of function usage in 1.3 million Python programs in GitHub, documentation about the functions in 2300+ modules, forum discussions with more than 47 million posts, class hierarchy information, etc. In this work, we will demonstrate one application of this knowledge graph, which is a code recommendation engine for programmers within an IDE. All user interactions within the application get translated into SPARQL queries, which have quite different characteristics than queries against traditional knowledge graphs such as DBpedia or Wikidata. Aspects of code such as data flow are inherently transitive, hence the SPARQL is complex and requires property paths. One of our goals is to provide these queries as a basis for graph querying benchmarks, while allowing users the ability to interact with a real application built on top of a large graph database.", "num_citations": "1\n", "authors": ["288"]}
{"title": "An Executable Specification for SPARQL\n", "abstract": " Linked Data on the web consists of over 1000 datasets from a variety of domains. They are queried with the SPARQL query language. There exist many implementations of SPARQL, and this rich ecosystem has demanded a precise specification and compliance tests. However, the SPARQL specification has grown in complexity, and it is increasingly difficult for developers to validate their implementations. In this paper, we present a declarative specification for SPARQL, based on relational logic. It describes SPARQL with just a few operators, and is executable: queries written in it can be directly executed against real datasets.", "num_citations": "1\n", "authors": ["288"]}
{"title": "Integrating asynchronous task parallelism and data-centric atomicity\n", "abstract": " Processor design has turned toward parallelism and heterogeneous cores to achieve performance and energy efficiency. Developers find high-level languages attractive as they use abstraction to offer productivity and portability over these hardware complexities. Over the past few decades, researchers have developed increasingly advanced mechanisms to deliver performance despite the overheads naturally imposed by this abstraction. Recent work has demonstrated that such mechanisms can be exploited to attack overheads that arise in emerging high-level languages, which provide strong abstractions over parallelism. However, current implementation of existing popular high-level languages, such as Java, offer little by way of abstractions that allow the developer to achieve performance in the face of extensive hardware parallelism.", "num_citations": "1\n", "authors": ["288"]}
{"title": "Program analysis for mobile: how and why to run WALA on your phone\n", "abstract": " As mobile devices become ubiquitous, security of such devices has become a serious concern. Attacks on the devices themselves are a danger, as is theft of data they contain. Static analysis of the devices' software is one approach to verifying the absence of security, and several tools have been created to analyze apps for potential attacks and vulnerabilities. Many tools focus on single apps, but there are starting to be tools that look for possible vulnerabilities or attacks due to multiple apps on a single device that can communicate. Such analysis depends on having access to the relevant apps, and hence has been proposed to be performed on app stores. One challenge in the Android environment is that apps are often installed from multiple sources, such as development builds of apps installed from developer sites, eg Mozilla Aurora pre-released of Firefox. Ultimately, sometimes the device itself is the only place\u00a0\u2026", "num_citations": "1\n", "authors": ["288"]}
{"title": "An Offline Optimal SPARQL Query Planning Approach to Evaluate Online Heuristic Planners\n", "abstract": " In graph databases, a given graph query can be executed in a large variety of semantically equivalent ways. Each such execution plan produces the same results, but at different computation costs. The query planning problem consists of finding, for a given query, an execution plan with the minimum cost. The traditional greedy or heuristic cost-based approaches addressing the query planning problem do not guarantee by design the optimality of the chosen execution plan. In this paper, we present a principled framework to solve the query planning problem by casting it into an Integer Linear Programming problem, and discuss its applications to testing and improving heuristic-based query planners.", "num_citations": "1\n", "authors": ["288"]}
{"title": "Analysis of clocks in x10 programs (extended)\n", "abstract": " Clocks are a mechanism for providing synchronization barriers in concurrent programming languages. They are usually implemented using primitive communication mechanisms and thus spare the programmer from reasoning about low-level implementation details such as remote procedure calls and error conditions. Clocks provide flexibility, but programs often use them in specific ways that do not require their full implementation. In this paper, we describe a tool that mitigates the overhead of general-purpose clocks by statically analyzing how programs use them and choosing optimized implementations when available.We tackle the clock implementation in the standard library of the X10 programming language\u2014a parallel, distributed object-oriented language. We report our findings for a small set of analyses and benchmarks. Our tool only adds a few seconds to analysis time, making it practical to use as part of a compilation chain.", "num_citations": "1\n", "authors": ["288"]}
{"title": "Extracting enterprise vocabulary using linked open data\n", "abstract": " A common vocabulary is vital to smooth business operation, yet codifying and maintaining an enterprise vocabulary is an arduous, manual task. We present a fully automated process for creating an enterprise vocabulary, by extracting terms from a domain-specific corpus, and extracting their types from LOD (Linked Open Data). We applied this process to create a vocabulary for the IT industry, using 58 Gartner analyst reports as a corpus, and the LOD subset consisting of DBpedia and Freebase. We present novel techniques for linking, cleansing, and extending the types in this LOD subset, resulting in an improvement of 55% for our IT domain results. We further improved our results through NER over the corpus. Our NER training is completely automated, exploiting Wikipedia and DBpedia. Altogether, we achieved 46.3% recall and 78.1% precision.", "num_citations": "1\n", "authors": ["288"]}