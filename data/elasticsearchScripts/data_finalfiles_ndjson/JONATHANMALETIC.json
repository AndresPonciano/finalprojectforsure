{"title": "A survey and taxonomy of approaches for mining software repositories in the context of software evolution\n", "abstract": " A comprehensive literature survey on approaches for mining software repositories (MSR) in the context of software evolution is presented. In particular, this survey deals with those investigations that examine multiple versions of software artifacts or other temporal information. A taxonomy is derived from the analysis of this literature and presents the work via four dimensions: the type of software repositories mined (what), the purpose (why), the adopted/invented methodology used (how), and the evaluation method (quality). The taxonomy is demonstrated to be expressive (i.e., capable of representing a wide spectrum of MSR investigations) and effective (i.e., facilitates similarities and comparisons of MSR investigations). Lastly, a number of open research issues in MSR that require further investigation are identified. Copyright \u252c\u2310 2007 John Wiley & Sons, Ltd.", "num_citations": "543\n", "authors": ["59"]}
{"title": "Supporting source code difference analysis\n", "abstract": " The paper describes an approach to easily conduct analysis of source-code differences. The approach is termed meta-differencing to reflect the fact that additional knowledge of the differences can be automatically derived. Meta-differencing is supported by an underlying source-code representation developed by the authors. The representation, srcML, is an XML format that explicitly embeds abstract syntax within the source code while preserving the documentary structure as dictated by the developer. XML tools are leveraged together with standard differencing utilities (i.e., diff,) to generate a meta-difference. The meta-difference is also represented in an XML format called srcDiff. The meta-difference contains specific syntactic information regarding the source-code changes. In turn this can be queried and searched with XML tools for the purpose of extracting information about the specifics of the changes. A case\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "129\n", "authors": ["59"]}
{"title": "The impact of identifier style on effort and comprehension\n", "abstract": " A family of studies investigating the impact of program identifier style on human comprehension is presented. Two popular identifier styles are examined, namely camel case and underscore. The underlying hypothesis is that identifier style affects the speed and accuracy of comprehending source code. To investigate this hypothesis, five studies were designed and conducted. The first study, which investigates how well humans read identifiers in the two different styles, focuses on low-level readability issues. The remaining four studies build on the first to focus on the semantic implications of identifier style. The studies involve 150 participants with varied demographics from two different universities. A range of experimental methods is used in the studies including timed testing, read aloud, and eye tracking. These methods produce a broad set of measurements and appropriate statistical methods, such as\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "126\n", "authors": ["59"]}
{"title": "Automatic software clustering via latent semantic analysis\n", "abstract": " The paper describes the initial results of applying Latent Semantic Analysis (LSA) to program source code and associated documentation. Latent Semantic Analysis is a corpus based statistical method for inducing and representing aspects of the meanings of words and passages (of natural language) reflective in their usage. This methodology is assessed for application to the domain of software components (i.e., source code and its accompanying documentation). The intent of applying Latent Semantic Analysis to software components is to automatically induce a specific semantic meaning of a given component. Here LSA is used as the basis to cluster software components. Results of applying this method to the LEDA library and MINIX operating system are given. Applying Latent Semantic Analysis to the domain of source code and internal documentation for the support of software reuse is a new application of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "105\n", "authors": ["59"]}
{"title": "An eye tracking study on the effects of layout in understanding the role of design patterns\n", "abstract": " The effect of layout in the comprehension of design pattern roles in UML class diagrams is assessed. This work replicates and extends a previous study using questionnaires but uses an eye tracker to gather additional data. The purpose of the replication is to gather more insight into the eye gaze behavior not evident from questionnaire-based methods. Similarities and differences between the studies are presented. Four design patterns are examined in two layout schemes in the context of three open source systems. Fifteen participants answered a series of eight design pattern role detection questions. Results show a significant improvement in role detection accuracy and visual effort with a certain layout for the Strategy and Observer patterns and a significant improvement in role detection time for all four patterns. Eye gaze data indicates classes participating in a design pattern act like visual beacons when they\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "94\n", "authors": ["59"]}
{"title": "Who can help me with this source code change?\n", "abstract": " An approach to recommend a ranked list of developers to assist in performing software changes to a particular file is presented. The ranking is based on change expertise, experience, and contributions of developers, as derived from the analysis of the previous commits involving the specific file in question. The commits are obtained from a software systempsilas version control repositories (e.g., Subversion). The basic premise is that a developer who has substantially contributed changes to specific files in the past is likely to best assist for their current or future change. Evaluation of the approach on a number of open source systems such as koffice, Apache httpd, and GNU gcc is also presented. The results show that the accuracy of the correctly recommended developers is between 43% and 82%. New developers to a long-lived software project, or project managers, can use this approach to assist them in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "90\n", "authors": ["59"]}
{"title": "Tracelab: An experimental workbench for equipping researchers to innovate, synthesize, and comparatively evaluate traceability solutions\n", "abstract": " TraceLab is designed to empower future traceability research, through facilitating innovation and creativity, increasing collaboration between researchers, decreasing the startup costs and effort of new traceability research projects, and fostering technology transfer. To this end, it provides an experimental environment in which researchers can design and execute experiments in TraceLab's visual modeling environment using a library of reusable and user-defined components. TraceLab fosters research competitions by allowing researchers or industrial sponsors to launch research contests intended to focus attention on compelling traceability challenges. Contests are centered around specific traceability tasks, performed on publicly available datasets, and are evaluated using standard metrics incorporated into reusable TraceLab components. TraceLab has been released in beta-test mode to researchers at seven\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "83\n", "authors": ["59"]}
{"title": "Recovering UML class models from C++: A detailed explanation\n", "abstract": " An approach to recovering design-level UML class models from C++ source code to support program comprehension is presented. A set of mappings are given that focus on accurately identifying such elements as relationship types, multiplicities, and aggregation semantics. These mappings are based on domain knowledge of the C++ language and common programming conventions and idioms. Additionally, formal concept analysis is used to detect design-level attributes of UML classes. An application implementing these mappings is used to reverse engineer a moderately sized, open-source application and the resultant class model is compared against those produced by other UML reverse engineering tools. This comparison shows that the presented mapping rules effectively produce meaningful and semantically accurate UML models.", "num_citations": "58\n", "authors": ["59"]}
{"title": "Applying dynamic change impact analysis in component-based architecture design\n", "abstract": " Change impact analysis plays an important role in maintenance and evolution of component-based software architecture. Viewing component replacement as a change to composition-based software architecture, this paper proposes a component interaction trace based approach to support dynamic change impact analysis at software architecture level. Given an architectural change, our approach determines the architecture elements causing the change and impacted by the change. Firstly, component-based software architecture and component interaction trace are defined. An algorithm for generating component interaction trace from static structure model of software architecture and UML sequence diagram is provided. Secondly, the taxonomy of changes on composition-based software architecture is presented, according to which a set of impact rules are suggested to determine the transfer of the changes in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "52\n", "authors": ["59"]}
{"title": "Improving feature location by enhancing source code with stereotypes\n", "abstract": " A novel approach to improve feature location by enhancing the corpus (i.e., source code) with static information is presented. An information retrieval method, namely Latent Semantic Indexing (LSI), is used for feature location. Adding stereotype information to each method/function enhances the corpus. Stereotypes are terms that describe the abstract role of a method, for example get, set, and predicate are well-known method stereotypes. Each method in the system is automatically stereotyped via a static-analysis approach. Experimental comparisons of using LSI for feature location with, and without, stereotype information are conducted on a set of open-source systems. The results show that the added information improves the recall and precision in the context of feature location. Moreover, the use of stereotype information decreases the total effort that a developer would need to expend to locate relevant methods\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "49\n", "authors": ["59"]}
{"title": "Comparing approaches to mining source code for call-usage patterns\n", "abstract": " Two approaches for mining function-call usage patterns from source code are compared The first approach, itemset mining, has recently been applied to this problem. The other approach, sequential-pattern mining, has not been previously applied to this problem. Here, a call-usage pattern is a composition of function calls that occur in a function definition. Both approaches look for frequently occurring patterns that represent standard usage of functions and identify possible errors. Itemset mining produces unordered patterns, i.e., sets of function calls, whereas, sequential-pattern mining produces partially ordered patterns, i.e., sequences of function calls. The trade-off between the additional ordering context given by sequential-pattern mining and the efficiency of itemset mining is investigated. The two approaches are applied to the Lima kernel v2.6.14 and results show that mining ordered patterns is worth the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "42\n", "authors": ["59"]}
{"title": "Automatically identifying changes that impact code-to-design traceability during evolution\n", "abstract": " An approach is presented that automatically determines if a given source code change impacts the design (i.e., UML class diagram) of the system. This allows code-to-design traceability to be consistently maintained as the source code evolves. The approach uses lightweight analysis and syntactic differencing of the source code changes to determine if the change alters the class diagram in the context of abstract design. The intent is to support both the simultaneous updating of design documents with code changes and bringing old design documents up to date with current code given the change history. An efficient tool was developed to support the approach and is applied to an open source system. The results are evaluated and compared against manual inspection by human experts. The tool performs better than (error prone) manual inspection. The developed approach and tool were used to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "39\n", "authors": ["59"]}
{"title": "A lightweight transformational approach to support large scale adaptive changes\n", "abstract": " An approach to automate adaptive maintenance changes on large-scale software systems is presented. This approach uses lightweight parsing and lightweight on-the-fly static analysis to support transformations that make corrections to source code in response to adaptive maintenance changes, such as platform changes. SrcML, an XML source code representation, is used and transformations can be performed using either XSLT or LINQ. A number of specific adaptive changes are presented, based on recent adaptive maintenance needs from products at ABB Inc. The transformations are described in detail and then demonstrated on a number of examples from the production systems. The results are compared with manual adaptive changes that were done by professional developers. The approach performed better than the manual changes, as it successfully transformed instances missed by the developers\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "39\n", "authors": ["59"]}
{"title": "Mappings for accurately reverse engineering UML class models from C++\n", "abstract": " The paper introduces a number of mapping rules for reverse engineering UML class models from C++ source code. The mappings focus on accurately identifying such elements as relationship types, multiplicities, and aggregation semantics. These mappings are based on domain knowledge of the C++ language and common programming conventions and idioms. An application implementing these heuristics is used to reverse engineer a moderately sized open source, C++ application, and the resultant class model is compared against those produced by other UML reverse engineering applications. A comparison shows that these presented mapping rules effectively produce meaningful, semantically accurate UML models", "num_citations": "39\n", "authors": ["59"]}
{"title": "An empirical study of debugging patterns among novices programmers\n", "abstract": " Students taking introductory computer science courses often have difficulty with the debugging process. This work investigates a number of different logical errors that novice programmers encounter and the associated debugging behaviors. Data is collected and analyzed data in two different experiments from 142 subjects. The results show some errors are more difficult than others. Different types of bugs and novices' debugging behaviors are identified. Years of experience showed a significant role in the process of debugging in terms of correctness level and time required for debugging", "num_citations": "34\n", "authors": ["59"]}
{"title": "srcSlice: very efficient and scalable forward static slicing\n", "abstract": " A highly efficient lightweight forward static slicing approach is presented and evaluated. The approach does not compute the program/system dependence graph but instead dependence and control information is computed as needed while computing the slice on a variable. The result is a list of line numbers, dependent variables, aliases, and function calls that are part of the slice for all variables (both local and global) for the entire system. The method is implemented as a tool, called srcSlice, on top of srcML, an XML representation of source code. The approach is highly scalable and can generate the slices for all variables of the Linux kernel in approximately 20\u0393\u00c7\u00ebmin on a typical desktop. Benchmark results are compared with the CodeSurfer slicing tool from GrammaTech Inc., and the approach compares well with regard to accuracy of slices. Copyright \u252c\u2310 2014 John Wiley & Sons, Ltd.", "num_citations": "30\n", "authors": ["59"]}
{"title": "The use of version space controlled genetic algorithms to solve the boole problem\n", "abstract": " The Version Space Controlled Genetic Algorithms (VGA) uses the structure of the version space to cache generalizations about the performance history of chromosomes in the genetic algorithm. This cached experience is used to constrain the generation of new members of the genetic algorithms population. The VGA is shown to be a specific instantiation of a more general framework, Autonomous Learning Elements (ALE). The capabilities of the VGA system are demonstrated using the Boole problem suggested by Wilson [Wilson 1987]. The performance of the VGA is compared to that of decision trees and genetic algorithms. The results suggest that the VGA is able to exploit a certain set of symbiotic relationships between its components, so that the resulting system performs better than either component individually.", "num_citations": "28\n", "authors": ["59"]}
{"title": "How we manage portability and configuration with the C preprocessor\n", "abstract": " An in-depth investigation of C preprocessor usage for portability and configuration management is presented. Three heavily-ported and widely used C++ libraries are examined. A core set of header files responsible for configuration management is identified in each system. Then macro usage is extracted and analyzed both manually and with the help of program analysis tools. The configuration structure of each library is discussed in details and commonalities between the systems, including conventions and patterns are discussed. A common configuration architecture for managing portability concerns is derived and presented.", "num_citations": "21\n", "authors": ["59"]}
{"title": "Developer reading behavior while summarizing java methods: Size and context matters\n", "abstract": " An eye-tracking study of 18 developers reading and summarizing Java methods is presented. The developers provide a written summary for methods assigned to them. In total, 63 methods are used from five different systems. Previous studies on this topic use only short methods presented in isolation usually as images. In contrast, this work presents the study in the Eclipse IDE allowing access to all the source code in the system. The developer can navigate via scrolling and switching files while writing the summary. New eye-tracking infrastructure allows for this improvement in the study environment. Data collected includes eye gazes on source code, written summaries, and time to complete each summary. Unlike prior work that concluded developers focus on the signature the most, these results indicate that they tend to focus on the method body more than the signature. Moreover, both experts and novices tend to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["59"]}
{"title": "Lexical categories for source code identifiers\n", "abstract": " A set of lexical categories, analogous to part-of-speech categories for English prose, is defined for source-code identifiers. The lexical category for an identifier is determined from its declaration in the source code, syntactic meaning in the programming language, and static program analysis. Current techniques for assigning lexical categories to identifiers use natural-language part-of-speech taggers. However, these NLP approaches assign lexical tags based on how terms are used in English prose. The approach taken here differs in that it uses only source code to determine the lexical category. The approach assigns a lexical category to each identifier and stores this information along with each declaration. srcML is used as the infrastructure to implement the approach and so the lexical information is stored directly in the srcML markup as an additional XML element for each identifier. These lexical-category\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "18\n", "authors": ["59"]}
{"title": "CFB: A Call for Benchmarks-for Software Visualization.\n", "abstract": " The paper argues for the need of a benchmark, or suite of benchmarks, to exercise and evaluate software visualization methods, tools, and research. The intent of the benchmark (s) must be to further and motivate research in the field of using visualization methods to support understanding and analysis of real world and/or large scale software systems undergoing development or evolution. The paper points to other software engineering sub-fields that have recently benefited from benchmarks and explains how these examples can assist in the development of a benchmark for software visualization.", "num_citations": "15\n", "authors": ["59"]}
{"title": "A slice-based estimation approach for maintenance effort\n", "abstract": " Program slicing is used as a basis for an approach to estimate maintenance effort. A case study of the GNU Linux kernel with over 900 versions spanning 17 years of history is presented. For each version a system dictionary is built using a lightweight slicing approach and encodes the forward decomposition static slice profiles for all variables in all the files in the system. Changes to the system are then modeled at the behavioral level using the difference between the system dictionaries of two versions. The three different granularities of slice (i.e., line, function, and file) are analyzed. We use a direct extension of srcML to represent computed change information. The retrieved information reflects the fact that additional knowledge of the differences can be automatically derived to help maintainers understand code changes. We consider the hypotheses: (1) The structured format helps create traceability links between\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["59"]}
{"title": "A tracelab-based solution for identifying traceability links using LSI\n", "abstract": " An information retrieval technique, latent semantic indexing (LSI), is used to automatically identify traceability links from system documentation to program source code. The experiment is performed in the TraceLab framework. The solution provides templates and components for building and querying LSI space and datasets (corpora) that can be used as inputs for these components. The proposed solution is evaluated on traceability links already discovered by mining adaptive commits of the open source system KDE/Koffice. The results show that the approach can identify of traceability links with high precision using TraceLab components.", "num_citations": "14\n", "authors": ["59"]}
{"title": "Identification of idiom usage in C++ generic libraries\n", "abstract": " A tool supporting the automatic identification of programming idioms specific to the construction of C++ generic libraries is presented. The goal is to assist developers in understanding the complex syntactic elements of these libraries. Large C++ generic libraries are notorious for being extremely difficult to comprehend due to their use of advanced language features and idiomatic nature. To facilitate automated identification, the idioms are equated to micropatterns, which can be evaluated by a fact extractor. These micropattern instances act as beacons for the idioms being identified. The method is applied to study a number of widely used open source C++ generic libraries.", "num_citations": "13\n", "authors": ["59"]}
{"title": "An empirical examination of the prevalence of inhibitors to the parallelizability of open source software systems\n", "abstract": " An empirical study is presented that examines the potential to parallelize general-purpose software systems. The study is conducted on 13 open source systems comprising over 14 MLOC. Each for-loop is statically analyzed to determine if it can be parallelized or not. A for-loop that can be parallelized is termed a free-loop. Free-loops can be easily parallelized using tools such as OpenMP. For the loops that cannot be parallelized, the various inhibitors to parallelization are determined and tabulated. The data shows that the most prevalent inhibitor by far, is functions called within for-loops that have side effects. This single inhibitor poses the greatest challenge in adapting and re-engineering systems to better utilize modern multi-core architectures. This fact is somewhat contradictory to the literature, which is primarily focused on the removal of data dependencies within loops. Results of this paper also show\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["59"]}
{"title": "Factoring differences for iterative change management\n", "abstract": " An approach for factoring source-code differences is presented. A single large difference between two versions of a program is decomposed into factors (i.e., smaller changes). The application of all the factors is equivalent to the application of the single large difference. The factors are obtained by user-defined criteria. They include changes that are limited to a specific syntactic construct or ones that occur throughout a file. The factors can be applied individually or in combination to generate intermediate forms of the large change. This directly supports iterative software change management by decomposing large changes into smaller factors. The approach uses srcDiff, an XML representation of multiple versions of a source-code file and their differences. XML transformations are used to factor a change according to an XPath expression. The approach is applied to changes made to a large opensource system. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["59"]}
{"title": "Stepwise refinement and problem solving\n", "abstract": " The possibility of reexpressing the traditional notion of stepwise refinement as a combination of general problem-solving activities that are based on paradigms taken from artificial intelligence research is discussed. This reexpression can form the basis for a more explicit view of programming as a problem-solving activity. Experiments in which each step of the refinement process is encoded into problem solving activities are described. 26 examples of code implementation using the stepwise refinement of pseudocode have been analyzed. The presence of certain combinations of activities suggest that programmers are implicitly emulating certain paradigms that have proved useful in solving complex problems. Also, a particular paradigm and its associated activities seem to be applied often throughout the refinement sequence for a given problem. The nature of the problem to be solved influences the type of activities\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["59"]}
{"title": "we must keep going i guess\n", "abstract": " PM: a system to support the automatic acquisition of programming knowledge 1) Storage and retrieval of heterogeneous knowledge and structures. Programming knowledge comes in many forms and the system should support the acquisition and utilization of each. For example, a professional programmer must possess knowledge of algorithms and data structures, knowledge about an application domain, and knowledge of how to plan and implement application software.", "num_citations": "10\n", "authors": ["59"]}
{"title": "A tool for efficiently reverse engineering accurate UML class diagrams\n", "abstract": " A tool that reverse engineers UML class diagrams from C++ source code is presented. The tool takes srcML as input and produces yUML as output. srcML is an XML representation of the abstract syntactic information of source code. The srcML parser (srcML.org) is highly scalable, efficient, and robust. yUML is a textual format for UML class diagrams that can be easily rendered into a graphical diagram via a web service (yUML.me) or a tool such as Graphvis. The approach utilizes efficient SAX (Simple API for XML) parsing to collect the information needed to construct the class diagram. Currently it supports the following UML features: differentiating between class, data type, or interface, identifying design level attributes, multiplicity and type, determining parameter direction, and identification of the relationships aggregation, composition, generalization, and realization. The tool produces yUML for all of Calligra (~1\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["59"]}
{"title": "Automatically identifying C++ 0x concepts in function templates\n", "abstract": " An automated approach to the identification of C++0x concepts in function templates is described. Concepts are part of a new language feature appearing in the next standard for C++ (i.e., C++0x). Concept identification is the enumeration of constraints on the sets of types over which templates can be instantiated. The approach analyzes template source code and computes a set of viable concept instances describing the implied data abstraction of the template parameters. The approach is evaluated on generic algorithms defined in the C++ Standard Template Library (STL). The evaluation demonstrates the effectiveness of the approach. The approach can be used to assist in reengineering existing generic libraries to C++0x. Additionally, it has the potential to assist in the validation of concept hierarchies and interface definition in generic libraries.", "num_citations": "9\n", "authors": ["59"]}
{"title": "Factors influencing dwell time during source code reading: a large-scale replication experiment\n", "abstract": " The paper partially replicates and extends a previous study by Busjahn et al.[4] on the factors influencing dwell time during source code reading, where source code element type and frequency of gaze visits are studied as factors. Unlike the previous study, this study focuses on analyzing eye movement data in large open source Java projects. Five experts and thirteen novices participated in the study where the main task is to summarize methods. The results examine semantic line-level information that developers view during summarization. We find no correlation between the line length and the total duration of time spent looking on the line even though it exists between a token's length and the total fixation time on the token reported in prior work. The first fixations inside a method are more likely to be on a method's signature, a variable declaration, or an assignment compared to the other fixations inside a method\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["59"]}
{"title": "Using developer eye movements to externalize the mental model used in code summarization tasks\n", "abstract": " Eye movements of developers are used to speculate the mental cognition model (ie, bottom-up or top-down) applied during program comprehension tasks. The cognition models examine how programmers understand source code by describing the temporary information structures in the programmer's short term memory. The two types of models that we are interested in are top-down and bottom-up. The top-down model is normally applied as-needed (ie, the domain of the system is familiar). The bottom-up model is typically applied when a developer is not familiar with the domain or the source code. An eye-tracking study of 18 developers reading and summarizing Java methods is used as our dataset for analyzing the mental cognition model. The developers provide a written summary for methods assigned to them. In total, 63 methods are used from five different systems. The results indicate that on average\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["59"]}
{"title": "A study on developer perception of transformation languages for refactoring\n", "abstract": " Although there is much research advancing state-of-art of program transformation tools, their application in industry source code change problems has not yet been gauged. In this context, the purpose of this paper is to better understand developer familiarity and comfort with these languages by conducting a survey. It poses, and answers, four research questions to understand how frequently source code transformation languages are applied to refactoring tasks, how well-known these languages are in industry, what developers think are obstacles to adoption, and what developer refactoring habits tell us about their current use, or underuse, of transformation languages. The results show that while source code transformation languages can fill a needed niche in refactoring, research must motivate their application. We provide explanations and insights based on data, aimed at the program transformation and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["59"]}
{"title": "A tool to support knowledge based software maintenance: the Software Service Bay\n", "abstract": " A software maintenance methodology, The Software Service Bay, is introduced. This methodology is analogous to the automotive service bay which employs a number of experts for particular maintenance problems. Problems in maintenance are reformulated so they may be solved with current AI tools and technologies.< >", "num_citations": "8\n", "authors": ["59"]}
{"title": "An introduction to refinement metrics: Assessing a programming language's support of the stepwise refinement process\n", "abstract": " 1. Introduction. Much of the software life cycle is realized through ProgrammQ~ guages. The structure of the particular programming language used to implement a system can influence the effort expended at some of the different phases of the life cycle. This impact is particularly evident in phases such as maintenance that deal with completed code. For example, the presence of certain code structuring units in a language is generally thought to improve the readability of programs that employ them [l]. However, the impact that language structure has on the design process is less clear cut. The stepwise refmement of pseudocode is a common mechanism for program design in a variety of languages. Yet, the effects of a language\u0393\u00c7\u00d6s structure on the refinement process has not been quantitatively documented. The need for such assessment is particularly acute in light of the fact that many new program design languages\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["59"]}
{"title": "srcDiff: A syntactic differencing approach to improve the understandability of deltas\n", "abstract": " An efficient and scalable rule\u0393\u00c7\u00c9based syntactic differencing approach is presented. The tool srcDiff is built upon the srcML infrastructure. srcML adds abstract syntactic information into the code via an XML format. A syntactic difference of srcML documents is then taken. During this process, the differences are further refined using a set of rules that model typical editing patterns of source code by developers. Thus, the resulting deltas model edits that are programmer centric versus a purely syntactic tree edit view. Other syntactic differencing approaches focus on obtaining an optimal tree edit distance with the assumption that this will produce an accurate difference. While this may work well for small or simple changes, the differences quickly become unreadable for more complex changes. By contrast, the approach presented here purposely deviates from an optimal tree edit difference in order to create a delta that is\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["59"]}
{"title": "The use of version space controlled genetic algorithms to solve the Boole problem\n", "abstract": " It is demonstrated that the VGA (version space guided genetic algorithm) is a particular instantiation of a more general class of systems, termed autonomous learning elements (ALEs). The basic components of an ALEs are discussed. The Boole problem posed by SW Wilson (1987) is introduced, and its expression in terms of the VGA framework is discussed. The details of the VGA system are given followed by a discussion of results. In particular, the performances of the VGA on two versions of the Boole problem are described and compared with those of classifier systems and decision trees.<>", "num_citations": "7\n", "authors": ["59"]}
{"title": "Mining evolutionary dependencies from web\u0393\u00c7\u00c9localization repositories\n", "abstract": " An approach to mining repositories of web\u0393\u00c7\u00c9based user documentation for patterns of evolutionary change in the context of internationalization and localization is presented. Localized web documents that are frequently co\u0393\u00c7\u00c9changed (i.e., an evolutionary dependency) during the natural language translation process are uncovered to support the future evolution of the system. A sequential\u0393\u00c7\u00c9pattern mining technique is used to uncover patterns from version histories. Characteristics of the uncovered patterns such as size, frequency, and occurrence within a single natural language or across multiple languages are discussed. Such patterns help provide an insight into the effort required in retranslation due to a change in the documentation. The approach is validated on the open source K Desktop Environment (KDE) system. KDE maintains documentation for over 50 different natural languages and presents a prime\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["59"]}
{"title": "Utilizing association rules for identification of possible errors in data sets\n", "abstract": " The paper analyzes the application of association rules to the problem of data cleansing and automatically identifying potential errors in data sets. Association rules are a fundamental class of patterns that exist in data. These patterns have been widely utilized (eg, market basket analysis) and extensive studies exist to find efficient association rule mining algorithms. Special attention is given in literature to the extension of binary association rules (eg, ratio, quantitative, generalized, multiple-level, constrained-based, distance-based, composite association rules). A new extension of the boolean association rules, ordinal association rules, that incorporates ordinal relationships among data items, is introduced. An algorithm that finds these rules and identifies potential errors in data is proposed. A prototype tool is described and the results of applying it to a real-world data set are given. The tool is designed to be domain independent and constitutes the first part in a proposed framework for automated data cleansing. Other approaches to data cleansing are described and compared.", "num_citations": "6\n", "authors": ["59"]}
{"title": "Progress report on automated data cleansing\n", "abstract": " CiteSeerX \u0393\u00c7\u00f6 Progress Report on Automated Data Cleansing Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA Progress Report on Automated Data Cleansing Cached Download as a PDF Download Links [www.cs.kent.edu] [www.sdml.info] Save to List Add to Collection Correct Errors Monitor Changes by Jonathan I. Maletic , Andrian Marcus Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract Keyphrases progress report automated data cleansing Powered by: Apache Solr About CiteSeerX Submit and Index Documents Privacy Policy Help Data Source Contact Us Developed at and hosted by The College of Information Sciences and Technology \u252c\u2310 2007-2019 The State \u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["59"]}
{"title": "The Software Service Bay: A knowledge-based software maintenance methodology\n", "abstract": " The maintenance of software has been found to be the most costly part of the software life cycle. Little work explicitly addresses a framework for software maintenance problems solving. A software maintenance methodology, the Software Service Bay, is introduced which focuses on the different type of problem solving required in software maintenance. The Software Service Bay is a knowledge based maintenance methodology analogous to the automotive service bay. Within an automotive service bay there are many different auto mechanic experts, each with specific knowledge on different maintenance problem, be they transmission, brakes, engine, etc. Each mechanic solves problem with their domain using different knowledge, tools, and methods. Software maintenance is much the same way. The variety of problem types (perfect, corrective, etc.) lends to a specific set knowledge, specific set of tools, and specific\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["59"]}
{"title": "Slice-based cognitive complexity metrics for defect prediction\n", "abstract": " Researchers have identified several quality metrics to predict defects, relying on different information however, these approaches lack metrics to estimate the effort of program understandability of system artifacts. In this paper, novel metrics to compute the cognitive complexity based on program slicing are introduced. These metrics help identify code that is more likely to have defects due to being challenging to comprehension. The metrics include such measures as the total number of slices in a file, the size, the average number of identifiers, and the average spatial distance of a slice. A scalable lightweight slicing tool is used to compute the necessary slicing data. A thorough empirical investigation into how cognitive complexity correlates with and predicts defects in the version histories of 10 datasets of 7 open source systems is performed. The results show that the increase of cognitive complexity significantly\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["59"]}
{"title": "Simplifying the construction of source code transformations via automatic syntactic restructurings\n", "abstract": " A set of restructurings to systematically normalize selective syntax in C++ is presented. The objective is to convert variations in syntax of specific portions of code into a single form to simplify the construction of large, complex program transformation rules. Current approaches to constructing transformations require developers to account for a large number of syntactic cases, many of which are syntactically different but semantically equivalent. The work identifies classes of such syntactic variations and presents normalizing restructurings to simplify each variation to a single, consistent syntactic form. The normalizing restructurings for C++ are presented and applied to two open source systems for evaluation. The evaluation uses the system's test cases to validate that the normalizing restructurings do not affect the systems' tested behavior. In addition, a set of example transformations that benefit from the prior\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["59"]}
{"title": "Studying developer gaze to empower software engineering research and practice\n", "abstract": " A new research paradigm is proposed that leverages developer eye gaze to improve the state of the art in software engineering research and practice. The vision of this new paradigm for use on software engineering tasks such as code summarization, code recommendations, prediction, and continuous traceability is described. Based on this new paradigm, it is foreseen that new benchmarks will emerge based on developer gaze. The research borrows from cognitive psychology, artificial intelligence, information retrieval, and data mining. It is hypothesized that new algorithms will be discovered that work with eye gaze data to help improve current IDEs, thus improving developer productivity. Conducting empirical studies using an eye tracker will lead to inventing, evaluating, and applying innovative methods and tools that use eye gaze to support the developer. The implications and challenges of this paradigm for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["59"]}
{"title": "Mining for co-changes in the context of web localization\n", "abstract": " An approach for mining repositories of Web-based user documentation for patterns of evolutionary change in the context of internationalization and localization is presented. Sets of documents that are changed together during the translation process are uncovered and documented to support future evolution of the system. A sequential-pattern mining technique is used to uncover the patterns from Subversion repositories. The approach is applied to the open source KDE system. KDE maintains documentation for over fifty different natural languages and presents a prime example of the problem. Characteristics of the uncovered patterns such as size, frequency, and occurrences within a single language or across multiple languages are discussed. Such patterns help provide insight as to the effort required in retranslation due to a change in the documentation and help user communities estimated the progress of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["59"]}
{"title": "Using dynamic slicing to analyze change impact on role type based component composition model\n", "abstract": " Slicing is an effective technique for narrowing the focus of attention to the relevant parts of a system. Applying dynamic slicing on component interaction trace, this paper presents a change impact analysis approach to support the maintenance and evolution of role type based component composition model. Our approach determines the potential consequences of a given change to role type based component composition model and generates corresponding regression test case suite", "num_citations": "5\n", "authors": ["59"]}
{"title": "PM: a metrics-driven plan compiler\n", "abstract": " The goal of the Partial Metrics (PM) project is to investigate the metrics-driven acquisition of planning knowledge. The approach taken here can be thought of as reverse compilation. Abstract plans are generated from source programs in a given target language. This process involves the integration of traditional compiler techniques with certain AI paradigms. This paper describes the current prototype's design. The prototype's capabilities are demonstrated by taking an Ada target program generated by a plan in the Programmer's Apprentice system and creating another plan using the PM knowledge compiler. The plan produced by the prototype possesses many of the original plan's features.", "num_citations": "5\n", "authors": ["59"]}
{"title": "The evaluation of an approach for automatic generated documentation\n", "abstract": " Two studies are conducted to evaluate an approach to automatically generate natural language documentation summaries for C++ methods. The documentation approach relies on a method's stereotype information. First, each method is automatically assigned a stereotype(s) based on static analysis and a set of heuristics. Then, the approach uses the stereotype information, static analysis, and predefined templates to generate a natural-language summary/documentation for each method. This documentation is automatically added to the code base as a comment for each method. The result of the first study reveals that the generated documentation is accurate, does not include unnecessary information, and does a reasonable job describing what the method does. Based on statistical analysis of the second study, the most important part of the documentation is the short description as it describes the intended\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["59"]}
{"title": "Can the ez reader model predict eye movements over code? towards a model of eye movements over source code\n", "abstract": " Studies of eye movements during source code reading have supported the idea that reading source code differs fundamentally from reading natural text. The paper analyzed an existing data set of natural language and source code eye movement data using the EZ reader model of eye movement control. The results show that the EZ reader model can be used with natural text and with source code where it provides good predictions of eye movement duration. This result is confirmed by comparing model predictions to eye-movement data from this experiment and calculating the correlation score for each metric. Finally, it was found that gaze duration is influenced by token frequency in code and in natural text. The frequency effect is less pronounced on first fixation duration and single fixation duration. An eye movement control model for source code reading may open the door for tools in education and the industry\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["59"]}
{"title": "Emulating C++ 0x concepts\n", "abstract": " A library for the emulation of C++0x concepts developed using the emerging C++11 programming language is presented. The library integrates existing techniques for concept checking and template metaprogramming to provide a uniform interface to defining and using concepts. The purpose of this work is to establish a concrete foundation for experimentation of design techniques for concepts and to motivate and evaluate language design. The viability of the approach is demonstrated by applying it to characterize a number of previously identified usability problems with concepts in the proposed C++0x language. In particular, issues related to the use of explicit and automatic concepts in generic library design from the perspective of this experiment are examined. Issues related to concept refinement, default implementations of requirements, and the generation of error messages are also discussed.", "num_citations": "3\n", "authors": ["59"]}
{"title": "Abstracting the template instantiation relation in c++\n", "abstract": " A source code model that supports the static analysis of C++ templates and template metaprograms is presented. Analogous to techniques for object-oriented and procedural software (e.g., the abstraction of call graphs, inheritance hierarchies, etc.), this model provides a basis for maintenance concerns such as program comprehension, fact extraction, and impact analysis of generic code. The source code model is used to derive the template instantiation graph, and potential applications of this model discussed. An application to reverse engineer this model from source code is described.", "num_citations": "3\n", "authors": ["59"]}
{"title": "Identification of Test Cases from Business Requirements of Software Systems\n", "abstract": " Many organizations are struggling between the fast delivery of new software and quality assurance. Software testing play a key part in the quality assurance of software systems. Formal testing techniques increase software quality and, at the same time, reduce software development cycle time. This article presents a methodology for the identification and definition of black box test cases based on the functional requirements of a software system. The methodology is applied during the initial phases of software development. The method involves analyzing system requirements and constructing a functional description graph to organize these requirements.", "num_citations": "3\n", "authors": ["59"]}
{"title": "Operationalizing software reuse as a problem in machine learning\n", "abstract": " RG Reynolds, JI Maletic & E. Zannoni, Operationalizing software reuse as a problem in machine learning - PhilPapers Sign in | Create an account PhilPapers PhilPeople PhilArchive PhilEvents PhilJobs PhilPapers home Syntax Advanced Search Syntax Advanced Search Syntax Advanced Search Operationalizing software reuse as a problem in machine learning RG Reynolds, JI Maletic & E. Zannoni Proceedings of the Fourth Florida Artificial Intelligence Research Symposium, Florida Ai Research Society (forthcoming) Authors Ronald Reynolds University of Nevada, Reno Abstract This article has no associated abstract. (fix it) Keywords No keywords specified (fix it) Categories Software in Philosophy of Computing and Information (categorize this paper) Options Edit this record Mark as duplicate Export citation Find it on Scholar Request removal from index Revision history Download options PhilArchive copy a copy .\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["59"]}
{"title": "A Fine-grained Assessment on Novice Programmers\u0393\u00c7\u00d6 Gaze Patterns on Pseudocode Problems\n", "abstract": " To better understand code comprehension and problem solving strategies, we conducted an eye tracking study that includes 51 undergraduate computer science students solving six pseudocode program comprehension tasks. Each task required students to order a sequence of pseudocode statements necessary to correctly solve a programming problem. We compare the viewing patterns of computer science students to evaluate changes in behavior while participants solve problems of varying difficulty. The intent is to find out if gaze patterns are similar prior to solving the task and if this pattern changes as the problems get more difficult. The findings show that as the difficulty increases regressions between areas of interest also tend to increase. Furthermore, an analysis of clusters of participants\u0393\u00c7\u00d6 common viewing patterns was performed to identify groups of participants\u0393\u00c7\u00d6 sharing similar gaze patterns prior to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["59"]}
{"title": "Adding Structure to Unstructured Text\n", "abstract": " An overview of the authors\u0393\u00c7\u00d6 research program in document engineering is presented. Underlying techniques are being developed for agile parsing of unstructured and semi-structured text to extract metadata. XML technologies are leveraged in novel ways to support complex querying, analysis, and transformation of large text bases. New methods for difference analysis are being developed to support document evolution and maintenance. Additionally, advanced information retrieval methods, namely latent semantic indexing, in conjunction with clustering techniques are used to extract high level features and concepts from large corpora.", "num_citations": "2\n", "authors": ["59"]}
{"title": "An empirical study of the use of problem reduction as a paradigm for problem solving in software engineering\n", "abstract": " Marvin Minsky [8] states that the most powerful way to solve a hard problem is to find a method that splits it into several simpler problems that can be solved by them self, This generic paradigm for solving problems is called problem reduction. In this scenario, a problem in a given domain is decomposed into a structured set of subproblems in that domain. Each subproblem is subject to further decomposition until the subproblems produced are solvable with given techniques. This problem reduction paradigm has been successfully applied to solve problems in every phase of the software development cycle where a top-down decision-making strategy is applicable.In this paper, the focus is on utilization of the problem reduction paradigm at the code implementation level in the software development life cycle. The production of code in a program development language (PDL) using a variation on the problem reduction\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["59"]}
{"title": "Determining Differences in Reading Behavior Between Experts and Novices by Investigating Eye Movement on Source Code Constructs During a Bug Fixing Task\n", "abstract": " This research compares the eye movement of expert and novice programmers working on a bug fixing task. This comparison aims at investigating which source code elements programmers focus on when they review Java source code. Programmer code reading behaviors at the line and term levels are used to characterize the differences between experts and novices. The study analyzes programmers\u0393\u00c7\u00d6 eye movements over identified source code areas using an existing eye tracking dataset of 12 experts and 10 novices. The results show that the difference between experts and novices is significant in source code element coverage. Specifically, novices read more method signatures, variable declarations, identifiers, and keywords compared to experts. However, experts are better at finishing the task using fewer source code elements when compared to novices. Moreover, programmers tend to focus on the method\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["59"]}
{"title": "On the Naming of Methods: A Survey of Professional Developers\n", "abstract": " This paper describes the results of a large (+1100 responses) survey of professional software developers concerning standards for naming source code methods. The various standards for source code method names are derived from and supported in the software engineering literature. The goal of the survey is to determine if there is a general consensus among developers that the standards are accepted and used in practice. Additionally, the paper examines factors such as years of experience and programming language knowledge in the context of survey responses. The survey results show that participants very much agree about the importance of various standards and how they apply to names and that years of experience and the programming language has almost no effect on their responses. The results imply that the given standards are both valid and to a large degree complete. The work provides a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["59"]}
{"title": "Automated recording and semantics-aware replaying of high-speed eye tracking and interaction data to support cognitive studies of software engineering tasks\n", "abstract": " The paper introduces a fundamental technological problem with collecting high-speed eye tracking data while studying software engineering tasks in an integrated development environment. The use of eye trackers is quickly becoming an important means to study software developers and how they comprehend source code and locate bugs. High quality eye trackers can record upwards of 120 to 300 gaze points per second. However, it is not possible to map each of these points to a line and column position in a source code file (in the presence of scrolling and file switching) in real time at data rates over 60 gaze points per second without data loss. Unfortunately, higher data rates are more desirable as they allow for finer granularity and more accurate study analyses. To alleviate this technological problem, a novel method for eye tracking data collection is presented. Instead of performing gaze analysis in real time\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["59"]}
{"title": "3D Representations for Software Visualization\n", "abstract": " The paper presents a new 3D representation for visualizing large software systems. The origins of this representation can be directly traced to the SeeSoft metaphor. This work extends these visualization mechanisms by utilizing the third dimension, texture, abstraction mechanism, and by supporting new manipulation techniques and user interfaces. By utilizing a 3D representation we can better represent higher dimensional data than previous 2D views. An overview of our prototype tool and its basic functionality is given. Applications of this method to particular software engineering tasks are also discussed. visual representation easy to understand; yielding high levels of trust on behalf of the user. Color and pixel maps are used to show relationships between elements of a software system (rather than graph-based representations). This allows the representation of large amounts of source code, the non-trivial relationships, and data on a standard 2D visualization medium (eg, monitor or screen). Many other software visualization tools use graph-based representations that suffer from scalability, layout, and mapping problems.", "num_citations": "1\n", "authors": ["59"]}
{"title": "A Method for 3D Visualization of Microarray Data\n", "abstract": " Microarray based technologies allow researchers to simultaneously measure the expression levels of thousands, and even tens of thousands of genes, providing a new and powerful means of discovering, characterizing, and analyzing genes and their expression patterns. The need for new types of analysis tools is warranted by the exponential increase in the number of gene expression experiments being conducted, and thus concomitantly by the amount of data generated. Existing visualization tools do not provide the analyst a simple means for viewing the complex relationships present in the data. We present a novel visualization method that aims to provide such a mechanism in a way flexible enough to allow the user to choose a combination of visual metaphors that provide the most explanatory view of the data for the question at hand. Our approach utilizes a method for 3D visualization originally developed\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["59"]}
{"title": "Operationalizing software reuse as a problem in inductive learning\n", "abstract": " Biggerstaff and Richter suggest that there are four fundamental subtasks associated with operationalizing the reuse process [1]: finding reusable components, understanding these components, modifying these components, and composing components. Each of these subproblems can be re-expressed as a knowledge acquisition problem relative to producing a new representation able to facilitate the reuse process. In the current implementation of the Partial Metrics (PM), the focus is on operationalizing the first two subtasks.             This paper describes how the PM System performs the extraction of reusable procedural knowledge. An explanation of how PM works is carried out thorough the paper using as example the PASCAL system written by Goldberg [4] to implement the Holland's Genetic Algorithm.", "num_citations": "1\n", "authors": ["59"]}