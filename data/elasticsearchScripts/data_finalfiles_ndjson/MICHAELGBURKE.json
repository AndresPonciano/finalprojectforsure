{"title": "Interprocedural dependence analysis and parallelization\n", "abstract": " We present a method that combines a deep analysis of program dependences with a broad analysis of the interaction among procedures. The method is more efficient than existing methods: we reduce many tests, performed separately by existing methods, to a single test. The method is more precise than existing methods with respect to references to multi-dimensional arrays and dependence information hidden by procedure calls. The method is more general than existing methods: we accommodate potentially aliased variables and structures of differing shapes that share storage. We accomplish the above through a unified approach that integrates subscript analysis with aliasing and interprocedural information.", "num_citations": "441\n", "authors": ["296"]}
{"title": "An overview of the PTRAN analysis system for multiprocessing\n", "abstract": " PTRAN (Parallel TRANslator) is a system for automatically restructuring sequential FORTRAN programs for execution on parallel architectures. This paper describes PTRAN-A: the currently operational analysis phase of PTRAN. The analysis is both broad and deep, incorporating interprocedural information into dependence analysis. The system is organized around a persistent database of program and procedure information. PTRAN incorporates several new, fast algorithms in a pragmatic design.", "num_citations": "367\n", "authors": ["296"]}
{"title": "An interval-based approach to exhaustive and incremental interprocedural data-flow analysis\n", "abstract": " We reformulate interval analysis so that it can he applied to any monotone data-flow problem, including the nonfast problems of flow-insensitive interprocedural analysis. We then develop an incremental interval analysis technique that can be applied to the same class of problems. When applied to flow-insensitive interprocedural data-flow problems, the resulting algorithms are simple, practical, and efficient. With a single update, the incremental algorithm can accommodate any sequence of program changes that does not alter the structure of the program call graph. It can also accommodate a large class of structural changes. For alias analysis, we develop an incremental algorithm that obtains the exact solution as computed by an exhaustive algorithm. Finally, we develop a transitive closure algorithm that is particularly well suited to the very sparse matrices associated with the problems we address.", "num_citations": "209\n", "authors": ["296"]}
{"title": "A framework for determining useful parallelism\n", "abstract": " An approach to finding and forming parallel processes for both sequential and parallel programs is presented. The approach is presented in a framework that can create useful parallelism for a variety of parallel architectures. The framework makes use of a control dependence graph to capture maximal parallelism, a process tree to expose useful parallelism, renaming and storage segregation to reduce data dependencies, and an architecture-specific cost analyzer to evaluate the effectiveness of the potential processes. The framework is currently being implemented.", "num_citations": "93\n", "authors": ["296"]}
{"title": "A practical method for LR and LL syntactic error diagnosis and recovery\n", "abstract": " This paper presents a powerful, practical, and essentially language-independent syntactic error diagnosis and recovery method that is applicable within the frameworks of LR and LL parsing. The method generally issues accurate diagnoses even where multiple errors occur within close proximity, yet seldom issues spurious error messages. It employs a new technique, parse action deferral, that allows the most appropriate recovery in cases where this would ordinarily be precluded by late detection of the error. The method is practical in that it does not impose substantial space or time overhead on the parsing of correct programs, and in that its time efficiency in processing an error allows for its incorporation in a production compiler. The method is language independent, but it does allow for tuning with respect to particular languages and implementations through the setting of language-specific parameters.", "num_citations": "86\n", "authors": ["296"]}
{"title": "Interprocedural optimization: eliminating unnecessary recompilation\n", "abstract": " While efficient new algorithms for interprocedural data-flow analysis have made these techniques practical for use in production compilation systems, a new problem has arisen: collecting and using interprocedural information in a compiler introduces subtle dependence amongthe proceduresof a program. If the compiler dependson interprocedural information to optimize a given module, a subsequentediting changeto another module in the program may changethe interprocedural information and necessitaterecompilation. To avoid having to recompile every module in a program in responseto a single editing changeto one module, we have developed techniquesto more precisely determine which compilations have actually beeninvalidated by a changeto the program\u2019s source. This paper presents a general recoi-npzlatton test to determine which proceduresmust be compiledin responseto a seriesof editing\u00a0\u2026", "num_citations": "77\n", "authors": ["296"]}
{"title": "A critical analysis of incremental iterative data flow analysis algorithms\n", "abstract": " A model of data flow analysis and fixed point iteration solution procedures is presented. The faulty incremental iterative algorithm is introduced. Examples of the imprecision of restarting iteration from the intraprocedural and interprocedural domains are given. Some incremental techniques which calculate precise data flow information are summarized.< >", "num_citations": "49\n", "authors": ["296"]}
{"title": "Automatic generation of nested, fork-join parallelism\n", "abstract": " This paper presents an efficient algorithm that automatically generates a parallel program from a dependence-based representation of a sequential program. The resulting parallel program consists of nested fork-join constructs, composed from the loops and statements of the sequential program. Data dependences are handled by two techniques. One technique implicitly satisfies them by sequencing, thereby reducing parallelism. Where increased parallelism results, the other technique eliminates them by privatization: the introduction of process-specific private instances of variables. Additionally, the algorithm determines when copying values of such instances in and out of nested parallel constructs results in greater parallelism. This is the first algorithm for automatically generating parallelism for such a general model. The algorithm generates as much parallelism as is possible in our model while\u00a0\u2026", "num_citations": "47\n", "authors": ["296"]}
{"title": "Detecting an integrity constraint violation in a database by analyzing database schema, application and mapping and inserting a check into the database and application\n", "abstract": " A system (and method) of detecting an error in a database interaction, includes providing information about at least one of at least first and second software systems, and a mapping between at least a portion of said at least first and second software systems; and examining said at least one of said first and second software systems and said mapping to determine an error in an interaction between said at least first and second software systems.", "num_citations": "38\n", "authors": ["296"]}
{"title": "A practical method for syntactic error diagnosis and recovery\n", "abstract": " Our goal is to develop a practical syntactic error recovery method applicable within the general framework of viable prefix parsing. Our method represents an attempt to accurately diagnose and report all syntax errors without reporting errors that are not actually present. Successful recovery depends upon accurate diagnosis of errors together with sensible \u201ccorrection\u201d or alteration of the text to put the parse back on track. The issuing of accurate and helpful diagnostics is achieved by indicating the nature of the recovery made for each error encountered. The error recovery is prior to and independent of any semantic analysis of the program. However, the method does not exclude the invocation of semantic actions while parsing or preclude the use of semantic information for error recovery.", "num_citations": "35\n", "authors": ["296"]}
{"title": "Displaying and refactoring programs that include database statements\n", "abstract": " Embodiments of the invention provide techniques for displaying and refactoring of programs, including database statements. In one embodiment, database statements embedded in the program source are evaluated to identify statements of the source code affected by, or affecting, the changed element of the database. An indication of the statements of source code affected by or affecting the changed element of the database may be presented to a user.", "num_citations": "32\n", "authors": ["296"]}
{"title": "Defining flow sensitivity in data flow problems\n", "abstract": " Since Banning rst introduced ow sensitivity in 1978, the term has been used to indicate hard or complex data ow problems, but there is no consensus as to its precise meaning. We look at Banning's original uses of the term and some interpretations they have generated. Then we consider the multiplicity of meanings in more recent interprocedural analyses, categorizing a number of data ow problems. We also classify several recent interprocedural approximation techniques with respect to properties related to sensitivity and discuss additional data ow problem properties. Finally, we propose a de nition for ow sensitivity that appears to capture much of the original intent and current use.", "num_citations": "29\n", "authors": ["296"]}
{"title": "PTRAN II-a compiler for High Performance Fortran\n", "abstract": " New languages for distributed memory compilation, including High Performance Fortran (HPF), provide user directives for partitioning data among processors to specify parallelism. We believe that it will be important for distributed memory compilers for these languages to provide not only e ciency close to a hand-written message passing program, but also exibility, so that the user is not always required to provide directives nor specify the number of physical processors. We describe distributed-memory compilation techniques for both optimization and exibility.", "num_citations": "25\n", "authors": ["296"]}
{"title": "C++ ojbect model alternatives\n", "abstract": " Alternatives for laying out objects and/or virtual function tables (VFTs), sharing a virtual function table pointer with a direct, virtual base, eliminating the need for an early cast by increasing the number of virtual function table entries, and providing a smart virtual dispatch that eliminates late casts where possible. An object model can be constructed by choosing one option from each of several alternatives. Interaction of alternatives include (1) sharing VFT pointers with a direct, virtual base requires that virtual base pointers are not stored in objects and (2) sharing VFT pointers affects the sharing of virtual base pointers with base classes and placement of late cast adjustment arguments and determining virtual function table entries.", "num_citations": "22\n", "authors": ["296"]}
{"title": "Interprocedural dependence analysis and parallelization\n", "abstract": " The area of dependence analysis has served as grounds for fruitful research as well as practical implementation. Compilers and tools that utilize dependence information can generate code that takes advantage of parallel resources and storage hierarchies on modern architectures. Here, we offer some historical background on the context and thinking that fostered our 1986 paper. We also attempt to summarize the direction research in this area has taken since the paper's appearance.We present a method that combines a deep analysis of program dependences with a broad analysis of the interaction among procedures. The method is more efficient than existing methods: we reduce many tests, performed separately by existing methods, to a single test. The method is more precise than existing methods with respect to references to multi-dimensional arrays and dependence information hidden by procedure calls\u00a0\u2026", "num_citations": "20\n", "authors": ["296"]}
{"title": "The NYU Ada translator and interpreter\n", "abstract": " The NYU-Ada project is engaged in the design and implementation of a translator-interpreter for the Ada language. The objectives of this project are twofold: a) to provide an executable semantic model for the full Ada language, that can be used for teaching, and serve as a starting point for the design of an efficient Ada compiler; b) to serve as a testing ground for the software methodology that has emerged from our experience with the very-high level language SETL. In accordance with these objectives, the NYU-Ada system is written in a particularly high-level, abstract SETL style that emphasizes clarity of design and user interface over speed and efficiency. A number of unusual design features of the translator and interpreter follow from this emphasis. Some of these features are described below. We also discuss the question of semantic specification of programming languages, and the general methodology of\u00a0\u2026", "num_citations": "19\n", "authors": ["296"]}
{"title": "Disentangled relational representations for explaining and learning from demonstration\n", "abstract": " Learning from demonstration is an effective method for human users to instruct desired robot behaviour. However, for most non-trivial tasks of practical interest, efficient learning from demonstration depends crucially on inductive bias in the chosen structure for rewards/costs and policies. We address the case where this inductive bias comes from an exchange with a human user. We propose a method in which a learning agent utilizes the information bottleneck layer of a high-parameter variational neural model, with auxiliary loss terms, in order to ground abstract concepts such as spatial relations. The concepts are referred to in natural language instructions and are manifested in the high-dimensional sensory input stream the agent receives from the world. We evaluate the properties of the latent space of the learned model in a photorealistic synthetic environment and particularly focus on examining its usability for downstream tasks. Additionally, through a series of controlled table-top manipulation experiments, we demonstrate that the learned manifold can be used to ground demonstrations as symbolic plans, which can then be executed on a PR2 robot.", "num_citations": "17\n", "authors": ["296"]}
{"title": "De ning Flow Sensitivity in Data Flow Problems\n", "abstract": " Since Banning rst introduced ow sensitivity in 1978, the term has been used to indicate hard or complex data ow problems, but there is no consensus as to its precise meaning. We look at Banning's original uses of the term and some interpretations they have generated. Then we consider the multiplicity of meanings in more recent interprocedural analyses, categorizing a number of data ow problems. We also classify several recent interprocedural approximation techniques with respect to properties related to sensitivity and discuss additional data ow problem properties. Finally, we propose a de nition for ow sensitivity that appears to capture much of the original intent and current use.", "num_citations": "17\n", "authors": ["296"]}
{"title": "Identifying impact of database changes on an application\n", "abstract": " A technique is disclosed for identifying impact of database changes on an application. Given a proposed database change, affected source code is identified that is affected by the proposed database change. References to the affected source code are created and organized into a hierarchy. A user can ascertain the extent of the impact by viewing the hierarchy and by using the hierarchy to access and view the affected source code in a visually distinct manner.", "num_citations": "14\n", "authors": ["296"]}
{"title": "Physics-as-inverse-graphics: Unsupervised physical parameter estimation from video\n", "abstract": " We propose a model that is able to perform unsupervised physical parameter estimation of systems from video, where the differential equations governing the scene dynamics are known, but labeled states or objects are not available. Existing physical scene understanding methods require either object state supervision, or do not integrate with differentiable physics to learn interpretable system parameters and states. We address this problem through a physics-as-inverse-graphics approach that brings together vision-as-inverse-graphics and differentiable physics engines, enabling objects and explicit state and velocity representations to be discovered. This framework allows us to perform long term extrapolative video prediction, as well as vision-based model-predictive control. Our approach significantly outperforms related unsupervised methods in long-term future frame prediction of systems with interacting objects (such as ball-spring or 3-body gravitational systems), due to its ability to build dynamics into the model as an inductive bias. We further show the value of this tight vision-physics integration by demonstrating data-efficient learning of vision-actuated model-based control for a pendulum system. We also show that the controller's interpretability provides unique capabilities in goal-driven control and physical reasoning for zero-data adaptation.", "num_citations": "12\n", "authors": ["296"]}
{"title": "The tuning language for concurrent collections\n", "abstract": " Concurrent Collections (CnC) is a programming model for parallel systems. A novel aspect of this model is that there is a clear separation of the specification of the application semantics, called the domain specification, and the tuning specification, which maps the domain spec to a platform. The domain spec is declarative specification that indicates the constraints on execution. These correspond to data dependences and control dependences. The domain spec can be written by a domain expert who does not necessarily have knowledge of the target architecture. The separation of concerns isolates the domain expert from the tuning facility. This isolation permits the tuning language to provide strong capabilities for control and flexibility for a tuning expert. In addition, the separation means that a given domain spec can have multiple tuning specs. There might be multiple targets, and within the same target there might be distinct goals, such as performance vs. power. This paper introduces the CnC language for tuning specifications in some detail and show examples of its use. The goal of the tuning language is to provide capability for mapping the parallelism implicit in the domain spec for high-performance execution on a target platform. The focus of this activity is locality. The basic concept in the tuning language is the affinity group, a set of computations that the tuner suggests executing close in time and space. Hierarchical affinity groups allow the specification of relative levels of affinity. They provide a mechanism that allows the programmer to specify locality, while allowing but not requiring him to distinguish between spatial and temporal locality\u00a0\u2026", "num_citations": "12\n", "authors": ["296"]}
{"title": "Quantifying and evaluating the space overhead for alternative C++ memory layouts\n", "abstract": " This paper develops a formalism that precisely characterizes when class tables are required for C++ memory layouts. A memory layout is a particular choice of data structures for implementing run\u2010time support for object\u2010oriented languages. We use this formalism to quantify and evaluate, on a set of benchmarks, the space overhead for a set of C++ memory layouts. In particular, this paper studies the space overhead due to three language features: virtual dispatch, virtual inheritance, and dynamic typing. To date, there has been no scientific quantification or evaluation of C++ memory layouts. Our approach can help C++ implementors. This work has already influenced the memory layout design choices in IBM's Visual Age C++ V5 compiler. Applying our approach to a set of five benchmarks, we demonstrate that the impact of object\u2010oriented space overhead can vary dramatically between applications (ranging from 0\u00a0\u2026", "num_citations": "12\n", "authors": ["296"]}
{"title": "Bias remediation in driver drowsiness detection systems using generative adversarial networks\n", "abstract": " Datasets are crucial when training a deep neural network. When datasets are unrepresentative, trained models are prone to bias because they are unable to generalise to real world settings. This is particularly problematic for models trained in specific cultural contexts, which may not represent a wide range of races, and thus fail to generalise. This is a particular challenge for driver drowsiness detection, where many publicly available datasets are unrepresentative as they cover only certain ethnicity groups. Traditional augmentation methods are unable to improve a model's performance when tested on other groups with different facial attributes, and it is often challenging to build new, more representative datasets. In this paper, we introduce a novel framework that boosts the performance of detection of drowsiness for different ethnicity groups. Our framework improves Convolutional Neural Network (CNN) trained for\u00a0\u2026", "num_citations": "11\n", "authors": ["296"]}
{"title": "Composing diverse policies for temporally extended tasks\n", "abstract": " Robot control policies for temporally extended and sequenced tasks are often characterized by discontinuous switches between different local dynamics. These change-points are often exploited in hierarchical motion planning to build approximate models and to facilitate the design of local, region-specific controllers. However, it becomes combinatorially challenging to implement such a pipeline for complex temporally extended tasks, especially when the sub-controllers work on different information streams, time scales and action spaces. In this letter, we introduce a method that can automatically compose diverse policies comprising motion planning trajectories, dynamic motion primitives and neural network controllers. We introduce a global goal scoring estimator that uses local, per-motion primitive dynamics models and corresponding activation state-space sets to sequence diverse policies in a locally optimal\u00a0\u2026", "num_citations": "10\n", "authors": ["296"]}
{"title": "Computation of impacted and affected code due to database schema changes\n", "abstract": " The disclosed technology enables assistance to software developers by identifying the software application code that needs to be changed or behaves differently, as a consequence of a database schema change. The disclosed technology also enables a provision of automatic transformations, or at least hints for the transformation of the code, to accommodate the database schema change.", "num_citations": "9\n", "authors": ["296"]}
{"title": "Hybrid system identification using switching density networks\n", "abstract": " Behaviour cloning is a commonly used strategy for imitation learning and can be extremely effective in constrained domains. However, in cases where the dynamics of an environment may be state dependent and varying, behaviour cloning places a burden on model capacity and the number of demonstrations required. This paper introduces switching density networks, which rely on a categorical reparametrisation for hybrid system identification. This results in a network comprising a classification layer that is followed by a regression layer. We use switching density networks to predict the parameters of hybrid control laws, which are toggled by a switching layer to produce different controller outputs, when conditioned on an input state. This work shows how switching density networks can be used for hybrid system identification in a variety of tasks, successfully identifying the key joint angle goals that make up manipulation tasks, while simultaneously learning image-based goal classifiers and regression networks that predict joint angles from images. We also show that they can cluster the phase space of an inverted pendulum, identifying the balance, spin and pump controllers required to solve this task. Switching density networks can be difficult to train, but we introduce a cross entropy regularisation loss that stabilises training.", "num_citations": "8\n", "authors": ["296"]}
{"title": "Method and system for detection of integrity constraint violations\n", "abstract": " A system (and method) of detecting an error in a database interaction, includes providing information about at least one of at least first and second software systems, and a mapping between at least a portion of the at least first and second software systems, and examining the at least one of the first and second software systems and the mapping to determine an error in an interaction between the at least first and second software systems.", "num_citations": "8\n", "authors": ["296"]}
{"title": "A practical method for LR and LL syntactic error diagnosis and recovery\n", "abstract": " A powerful, practical, and language-independent method for diagnosing and recovering from syntactic errors within the LR and LL parsing frameworks is described. The method proceeds in three phases. The simple recovery phase attempts a single token modification of the source text, scope recovery attempts a multiple token insertion to close one or more open scopes, and secondary recovery involves a multiple deletion of tokens surrounding the error point.", "num_citations": "8\n", "authors": ["296"]}
{"title": "Vid2param: Modeling of dynamics parameters from video\n", "abstract": " Sensors are routinely mounted on robots to acquire various forms of measurements in spatio-temporal fields. Locating features within these fields and reconstruction (mapping) of the dense fields can be challenging in resource-constrained situations, such as when trying to locate the source of a gas leak from a small number of measurements. In such cases, a model of the underlying complex dynamics can be exploited to discover informative paths within the field. We use a fluid simulator as a model, to guide inference for the location of a gas leak. We perform localization via minimization of the discrepancy between observed measurements and gas concentrations predicted by the simulator. Our method is able to account for dynamically varying parameters of wind flow (e.g., direction and strength), and its effects on the observed distribution of gas. We develop algorithms for off-line inference as well as for on-line\u00a0\u2026", "num_citations": "7\n", "authors": ["296"]}
{"title": "Detecting inter-sectional accuracy differences in driver drowsiness detection algorithms\n", "abstract": " Convolutional Neural Networks (CNNs) have been used successfully across a broad range of areas including data mining, object detection, and in business. The dominance of CNNs follows a breakthrough by Alex Krizhevsky which showed improvements by dramatically reducing the error rate obtained in a general image classification task from 26.2% to 15.4%. In road safety, CNNs have been applied widely to the detection of traffic signs, obstacle detection, and lane departure checking. In addition, CNNs have been used in data mining systems that monitor driving patterns and recommend rest breaks when appropriate. This paper presents a driver drowsiness detection system and shows that there are potential social challenges regarding the application of these techniques, by highlighting problems in detecting dark-skinned drivers faces. Unfortunately, publicly available datasets are often captured in different\u00a0\u2026", "num_citations": "6\n", "authors": ["296"]}
{"title": "Residual learning from demonstration: adapting dynamic movement primitives for contact-rich insertion tasks\n", "abstract": " Contacts and friction are inherent to nearly all robotic manipulation tasks. Through the motor skill of insertion, we study how robots can learn to cope when these attributes play a salient role. In this work we study ways for adapting dynamic movement primitives (DMP) to improve their performance in the context of contact rich insertion. We propose a framework we refer to as residual learning from demonstration (rLfD) that combines dynamic movement primitives (DMP) that rely on behavioural cloning with a reinforcement learning (RL) based residual correction policy. Our evaluation suggests that applying residual learning directly in task space and operating on the full pose of the robot can significantly improve the overall performance of DMPs. We show that rLfD outperforms alternatives and improves the generalisation abilities of DMPs. We evaluate this approach by training an agent to successfully perform both simulated and real world insertions of pegs, gears and plugs into respective sockets.", "num_citations": "3\n", "authors": ["296"]}
{"title": "Action sequencing using visual permutations\n", "abstract": " Humans can easily reason about the sequence of high level actions needed to complete tasks, but it is particularly difficult to instill this ability in robots trained from relatively few examples. This work considers the task of neural action sequencing conditioned on a single reference visual state. This task is extremely challenging as it is not only subject to the significant combinatorial complexity that arises from large action sets, but also requires a model that can perform some form of symbol grounding, mapping high dimensional input data to actions, while reasoning about action relationships. This letter takes a permutation perspective and argues that action sequencing benefits from the ability to reason about both permutations and ordering concepts. Empirical analysis shows that neural models trained with latent permutations outperform standard neural architectures in constrained action sequencing tasks. Results\u00a0\u2026", "num_citations": "2\n", "authors": ["296"]}
{"title": "NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces\n", "abstract": " Learning low-dimensional latent state space dynamics models has proven powerful for enabling vision-based planning and learning for control. We introduce a latent dynamics learning framework that is uniquely designed to induce proportional controlability in the latent space, thus enabling the use of simple and well-known PID controllers. We show that our learned dynamics model enables proportional control from pixels, dramatically simplifies and accelerates behavioural cloning of vision-based controllers, and provides interpretable goal discovery when applied to imitation learning of switching controllers from demonstration. Notably, such proportional controlability also allows for robust path following from visual demonstrations using Dynamic Movement Primitives in the learned latent space.", "num_citations": "2\n", "authors": ["296"]}
{"title": "Surfing on an uncertain edge: Precision cutting of soft tissue using torque-based medium classification\n", "abstract": " Precision cutting of soft-tissue remains a challenging problem in robotics, due to the complex and unpredictable mechanical behaviour of tissue under manipulation. Here, we consider the challenge of cutting along the boundary between two soft mediums, a problem that is made extremely difficult due to visibility constraints, which means that the precise location of the cutting trajectory is typically unknown. This paper introduces a novel strategy to address this task, using a binary medium classifier trained using joint torque measurements, and a closed loop control law that relies on an error signal compactly encoded in the decision boundary of the classifier. We illustrate this on a grapefruit cutting task, successfully modulating a nominal trajectory t using dynamic movement primitives to follow the boundary between grapefruit pulp and peel using torque based medium classification. Results show that this control\u00a0\u2026", "num_citations": "2\n", "authors": ["296"]}
{"title": "A plea for thresholds, ie, maximal allowed levels for prohibited substances, to prevent questionable doping convictions\n", "abstract": " With the development of highly sensitive drug testing technologies that can detect a minute quantity of a prohibited substance in an athlete's body, accidental contamination through contact with publicly circulated materials can more readily result in a \u201cpositive\u201d reading. To discharge the burden of a positive finding, the athlete must show the \u201cfactual circumstances\u201d in which the prohibited substance entered his/her system. In cases of accidental contamination, the athlete generally cannot even know how it occurred, as there are many known and unknown possible sources of contamination. When an athlete does give an account, it cannot generally be proven or disproven.Outside the realm of sports anti-doping, the use of scientifically established thresholds for drug testing is standard practice. Basic logic dictates that thresholds would enable one to differentiate between relevant and irrelevant amounts in the context of\u00a0\u2026", "num_citations": "2\n", "authors": ["296"]}
{"title": "Quantifying and evaluating the space overhead in C++ memory layouts\n", "abstract": " This paper develops a formalism that precisely characterizes when class tables are required for C++ memory layouts. A memory layout is a particular choice of data structures for implementing run-time support for object-oriented languages. We use this formalism and other techniques to quantify and evaluate, on a set of benchmarks, the space overhead for a set of C++ memory layouts. In particular, this paper studies the space overhead due to three language features: virtual dispatch, virtual inheritance, and dynamic typing. To date, there has been no scientific quantification or evaluation of C++ memory layouts. Our approach can help C++ implementors. This work has already influenced the memory layout design choices in the next version of IBM\u2019s Visual Age C++ compiler.Applying our approach to a set of five benchmarks, we demonstrate that the impact of object-oriented space overhead can vary dramatically between applications (ranging from 0.42% to 99.79% for our benchmarks). In particular, applications whose object space is dominated by objects that heavily use object-oriented language features will be significantly impacted by the choice of a memory layout. The two end points in the spectrum of our memory layouts differ by at least 95% for each of our benchmarks.", "num_citations": "2\n", "authors": ["296"]}
{"title": "Learning wih Modular Representations for Long-Term Multi-Agent Motion Predictions\n", "abstract": " Context plays a significant role in the generation of motion for dynamic agents in interactive environments. This work proposes a modular method that utilises a model of the environment to aid motion prediction of tracked agents. This paper shows that modelling the spatial and dynamic aspects of a given environment alongside the local per agent behaviour results in more accurate and informed long-term motion prediction. Further, we observe that this decoupling of dynamics and environment models allows for better adaptation to unseen environments, requiring that only a spatial representation of a new environment be learned. We highlight the model\u2019s prediction capability using a benchmark pedestrian tracking problem and by tracking a robot arm performing a tabletop manipulation task. The proposed approach allows for robust and data efficient forward modelling, and relaxes the need for full model re-training in new environments. We evaluate this through an ablation study which shows better performance gain when utilising both representation modules in addition to improved generalisation on tasks with dynamics unseen at training time.", "num_citations": "1\n", "authors": ["296"]}
{"title": "XJ: robust XML processing in Java\u2122\n", "abstract": " XML has emerged as an important standard for data integration. Despite the importance of XML, the development of XML processing applications in object-oriented programming languages such as Java can be tedious. Programmers use low-level APIs such as DOM, which provide minimal support for ensuring that programs that process XML are correct with respect to the XML Schemas governing the XML data. Furthermore, the runtime performance of XML processing is a limitation of XML. Runtime libraries do not take advantage of static analysis of programs or XML Schema information to optimize accesses to XML data. XJ [2] is a research language that integrates XML as a first-class construct into Java (a prototype is available for download from http://www. alphaworks. ibm. com/tech/xj). Salient features of XJ include:", "num_citations": "1\n", "authors": ["296"]}