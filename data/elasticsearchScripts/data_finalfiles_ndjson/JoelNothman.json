{"title": "Learning multilingual named entity recognition from Wikipedia\n", "abstract": " We automatically create enormous, free and multilingual silver-standard training annotations for named entity recognition (ner) by exploiting the text and structure of Wikipedia. Most ner systems rely on statistical models of annotated data to identify and classify names of people, locations and organisations in text. This dependence on expensive annotation is the knowledge bottleneck our work overcomes. We first classify each Wikipedia article into named entity (ne) types, training and evaluating on 7200 manually-labelled Wikipedia articles across nine languages. Our cross-lingual approach achieves up to 95% accuracy. We transform the links between articles into ne annotations by projecting the target article\u02bcs classifications onto the anchor text. This approach yields reasonable annotations, but does not immediately compete with existing gold-standard data. By inferring additional links and heuristically tweaking\u00a0\u2026", "num_citations": "363\n", "authors": ["913"]}
{"title": "Evaluating entity linking with wikipedia\n", "abstract": " Abstract Named Entity Linking (nel) grounds entity mentions to their corresponding node in a Knowledge Base (kb). Recently, a number of systems have been proposed for linking entity mentions in text to Wikipedia pages. Such systems typically search for candidate entities and then disambiguate them, returning either the best candidate or nil. However, comparison has focused on disambiguation accuracy, making it difficult to determine how search impacts performance. Furthermore, important approaches from the literature have not been systematically compared on standard data sets. We reimplement three seminal nel systems and present a detailed evaluation of search strategies. Our experiments find that coreference and acronym handling lead to substantial improvement, and search strategies account for much of the variation between systems. This is an interesting finding, because these aspects of the\u00a0\u2026", "num_citations": "325\n", "authors": ["913"]}
{"title": "Cross-lingual name tagging and linking for 282 languages\n", "abstract": " The ambitious goal of this work is to develop a cross-lingual name tagging and linking framework for 282 languages that exist in Wikipedia. Given a document in any of these languages, our framework is able to identify name mentions, assign a coarse-grained or fine-grained type to each mention, and link it to an English Knowledge Base (KB) if it is linkable. We achieve this goal by performing a series of new KB mining methods: generating \u201csilver-standard\u201d annotations by transferring annotations from English to other languages through cross-lingual links and KB properties, refining annotations through self-training and topic selection, deriving language-specific morphology features from anchor links, and mining word translation pairs from cross-lingual links. Both name tagging and linking results for 282 languages are promising on Wikipedia data and on-Wikipedia data.", "num_citations": "194\n", "authors": ["913"]}
{"title": "Overview of tac-kbp2014 entity discovery and linking tasks\n", "abstract": " In this paper we give an overview of the Entity Discovery and Linking tasks at the Knowledge Base Population track at TAC 2014. In this year we introduced a new end-to-end English entity discovery and linking task which requires a system to take raw texts as input, automatically extract entity mentions, link them to a knowledge base, and cluster NIL mentions. In this paper we provide an overview of the task definition, annotation issues, successful methods and research challenges associated with this new task. This new task has attracted a lot of participants and has intrigued many interesting research problems and potential approaches. We believe it\u2019s a promising task to be extended to a tri-lingual setting in KBP2015.", "num_citations": "126\n", "authors": ["913"]}
{"title": "Transforming Wikipedia into named entity training data\n", "abstract": " Statistical named entity recognisers require costly hand-labelled training data and, as a result, most existing corpora are small. We exploit Wikipedia to create a massive corpus of named entity annotated text. We transform Wikipedia\u2019s links into named entity annotations by classifying the target articles into common entity types (eg person, organisation and location). Comparing to MUC, CONLL and BBN corpora, Wikipedia generally performs better than other cross-corpus train/test pairs.", "num_citations": "125\n", "authors": ["913"]}
{"title": "Named entity recognition in wikipedia\n", "abstract": " Named entity recognition (NER) is used in many domains beyond the newswire text that comprises current gold-standard corpora. Recent work has used Wikipedia\u2019s link structure to automatically generate near gold-standard annotations. Until now, these resources have only been evaluated on newswire corpora or themselves.We present the first NER evaluation on a Wikipedia gold standard (WG) corpus. Our analysis of cross-corpus performance on WG shows that Wikipedia text may be a harder NER domain than newswire. We find that an automatic annotation of Wikipedia has high agreement with WG and, when used as training data, outperforms newswire models by up to 7.7%.", "num_citations": "110\n", "authors": ["913"]}
{"title": "Overview of TAC-KBP2015 Tri-lingual Entity Discovery and Linking.\n", "abstract": " In this paper we give an overview of the Tri-lingual Entity Discovery and Linking task at the Knowledge Base Population (KBP) track at TAC2015. In this year we introduced a new end-to-end Tri-lingual entity discovery and linking task which requires a system to take raw texts from three languages (English, Chinese and Spanish) as input, automatically extract entity mentions, link them to an English knowledge base, and cluster NIL mentions across languages. More entity types and mention types were also added into some languages. In this paper we provide an overview of the task definition, annotation issues, successful methods and research challenges associated with this new task. This new task has attracted a lot of participants and has intrigued many interesting research problems and potential approaches. We believe it\u2019sa promising task to be combined with Tri-lingual slot filling to form a new Tri-lingual cool-start KBP track in TAC2016.", "num_citations": "98\n", "authors": ["913"]}
{"title": "Analysing Wikipedia and gold-standard corpora for NER training\n", "abstract": " Named entity recognition (NER) for English typically involves one of three gold standards: MUC, CoNLL, or BBN, all created by costly manual annotation. Recent work has used Wikipedia to automatically create a massive corpus of named entity annotated text.We present the first comprehensive crosscorpus evaluation of NER. We identify the causes of poor cross-corpus performance and demonstrate ways of making them more compatible. Using our process, we develop a Wikipedia corpus which outperforms gold standard corpora on crosscorpus evaluation by up to 11%.", "num_citations": "73\n", "authors": ["913"]}
{"title": "Overview of TAC-KBP2016 Tri-lingual EDL and its impact on end-to-end Cold-Start KBP\n", "abstract": " In this paper we give an overview of the Tri-lingual Entity Discovery and Linking (EDL) task at the Knowledge Base Population (KBP) track at TAC2016. We will summarize several new and effective research directions including cross-lingual knowledge transfer and exploiting language-specific resources. In this year we make the EDL task as part of end-to-end Tri-lingual cold-start knowledge base construction on a very large document collection (90,003 documents). So we will also measure and investigate the impact of EDL on cold-start slot filling.", "num_citations": "47\n", "authors": ["913"]}
{"title": "Stop word lists in free open-source software packages\n", "abstract": " Open-source software packages for language processing often include stop word lists. Users may apply them without awareness of their surprising omissions (eg \u201chasn\u2019t\u201d but not \u201chadn\u2019t\u201d) and inclusions (\u201ccomputer\u201d), or their incompatibility with a particular tokenizer. Motivated by issues raised about the Scikit-learn stop list, we investigate variation among and consistency within 52 popular English-language stop lists, and propose strategies for mitigating these issues.", "num_citations": "44\n", "authors": ["913"]}
{"title": "Cheap and easy entity evaluation\n", "abstract": " The AIDA-YAGO dataset is a popular target for whole-document entity recognition and disambiguation, despite lacking a shared evaluation tool. We review evaluation regimens in the literature while comparing the output of three approaches, and identify research opportunities. This utilises our open, accessible evaluation tool. We exemplify a new paradigm of distributed, shared evaluation, in which evaluation software and standardised, versioned system outputs are provided online.", "num_citations": "40\n", "authors": ["913"]}
{"title": "Overview of TAC-KBP2017 13 Languages Entity Discovery and Linking.\n", "abstract": " In this paper we give an overview of the Tri-lingual Entity Discovery and Linking (EDL) task at the Knowledge Base Population (KBP) track at TAC2017, and of the Ten Low Resource Language EDL Pilot. We will summarize several new and effective research directions including multi-lingual common space construction for cross-lingual knowledge transfer, rapid approaches for silver-standard training data generation and joint entity and word representation. We will also sketch out remaining challenges and future research directions.", "num_citations": "33\n", "authors": ["913"]}
{"title": "Event linking: Grounding event reference in a news archive\n", "abstract": " Interpreting news requires identifying its constituent events. Events are complex linguistically and ontologically, so disambiguating their reference is challenging. We introduce event linking, which canonically labels an event reference with the article where it was first reported. This implicitly relaxes coreference to co-reporting, and will practically enable augmenting news archives with semantic hyperlinks. We annotate and analyse a corpus of 150 documents, extracting 501 links to a news archive with reasonable inter-annotator agreement.", "num_citations": "23\n", "authors": ["913"]}
{"title": "Evaluating a statistical CCG parser on Wikipedia\n", "abstract": " The vast majority of parser evaluation is conducted on the 1984 Wall Street Journal (WSJ). In-domain evaluation of this kind is important for system development, but gives little indication about how the parser will perform on many practical problems.", "num_citations": "20\n", "authors": ["913"]}
{"title": "Naive but effective NIL clustering baselines\u2013CMCRC at TAC 2011\n", "abstract": " This paper describes the CMCRC systems entered in the TAC 2011 entity linking challenge. We used our best-performing system from TAC 2010 to link queries, then clustered NIL links. We focused on na\u0131ve baselines that group by attributes of the top entity candidate. All three systems performed strongly at 75.4% B3 F1, above the 71.6% median score.", "num_citations": "19\n", "authors": ["913"]}
{"title": "Learning named entity recognition from Wikipedia\n", "abstract": " We present a method to produce free, enormous corpora to train taggers for Named Entity Recognition (NER), the task of identifying and classifying names in text, often solved by statistical learning systems. Our approach utilises the text of Wikipedia, a free online encyclopedia, transforming links between Wikipedia articles into entity annotations. Having derived a baseline corpus, we found that altering Wikipedia\u2019s links and identifying classes of capitalised non-entity terms would enable the corpus to conform more closely to gold-standard annotations, increasing performance by up to 32% F score. The evaluation of our method is novel since the training corpus is not usually a variable in NER experimentation. We therefore develop a number of methods for analysing and comparing training corpora. Gold-standard training corpora for NER perform poorly (F score up to 32% lower) when evaluated on test data from a different gold-standard corpus. Our Wikipedia-derived data can outperform manually-annotated corpora on this cross-corpus evaluation task by up to 7% on held-out test data. These experimental results show that Wikipedia is viable as a source of automatically-annotated training corpora, which have wide domain coverage applicable to a broad range of NLP applications. ii", "num_citations": "19\n", "authors": ["913"]}
{"title": "Document-level Entity Linking: CMCRC at TAC 2010.\n", "abstract": " This paper describes the CMCRC systems entered in the TAC 2010 entity linking challenge. The best performing system we describe implements the document-level entity linking system from Cucerzan (2007), with several additions that exploit global information. Our implementation of Cucerzan\u2019s method achieved a score of 74.9% in development experiments. Additional global information improves performance to 78.4%. On the TAC 2010 test data, our best system achieves a score of 84.4%, which is second in the overall rankings of submitted systems.", "num_citations": "18\n", "authors": ["913"]}
{"title": "Analysing recall loss in named entity slot filling\n", "abstract": " State-of-the-art fact extraction is heavily constrained by recall, as demonstrated by recent performance in TAC Slot Filling. We isolate this recall loss for NE slots by systematically analysing each stage of the slot filling pipeline as a filter over correct answers. Recall is critical as candidates never generated can never be recovered, whereas precision can always be increased in downstream processing.We provide precise, empirical confirmation of previously hypothesised sources of recall loss in slot filling. While NE type constraints substantially reduce the search space with only a minor recall penalty, we find that 10% to 39% of slot fills will be entirely ignored by most systems. One in six correct answers are lost if coreference is not used, but this can be mostly retained by simple name matching rules.", "num_citations": "17\n", "authors": ["913"]}
{"title": "SYDNEY CMCRC at TAC 2013.\n", "abstract": " We use a supervised whole-document approach to English Entity Linking with simple clustering approaches. The system extends our TAC 2012 system (Radford et al., 2012), introducing new features for modelling local entity description and type-specific matching as well type-specific supervised models and supervised NIL classification. Our rule-based clustering takes advantage of local description and topics to split NIL clusters. The best system uses supervised entity linking and local description type clustering and scores 72.7% B3+ F1 score. Our KB clustering score is competitive with the top system at 71.4%.", "num_citations": "14\n", "authors": ["913"]}
{"title": "English event detection with translated language features\n", "abstract": " We propose novel radical features from automatic translation for event extraction. Event detection is a complex language processing task for which it is expensive to collect training data, making generalisation challenging. We derive meaningful subword features from automatic translations into target language. Results suggest this method is particularly useful when using languages with writing systems that facilitate easy decomposition into subword features, eg, logograms and Cangjie. The best result combines logogram features from Chinese and Japanese with syllable features from Korean, providing an additional 3.0 points f-score when added to state-of-the-art generalisation features on the TAC KBP 2015 Event Nugget task.", "num_citations": "8\n", "authors": ["913"]}
{"title": "Classifying articles in english and german wikipedia\n", "abstract": " Named Entity (NE) information is critical for Information Extraction (IE) tasks. However, the cost of manually annotating sufficient data for training purposes, especially for multiple languages, is prohibitive, meaning automated methods for developing resources are crucial. We investigate the automatic generation of NE annotated data in German from Wikipedia. By incorporating structural features of Wikipedia, we can develop a German corpus which accurately classifies Wikipedia articles into NE categories to within 1% F-score of the state-of-the-art process in English.", "num_citations": "8\n", "authors": ["913"]}
{"title": "Unsupervised biographical event extraction using wikipedia traffic\n", "abstract": " Biographical summarisation can provide succinct and meaningful answers to the question \u201cWho is X?\u201d. Current supervised summarisation approaches extract sentences from documents using features from textual context.In this paper, we explore a novel approach to biographical summarisation, by extracting important sentences from an entity\u2019s Wikipedia page based on internet traffic to the page over time. Using a pilot data set, we found that it is feasible to extract key sentences about people\u2019s notability without the need for a large annotated corpus.", "num_citations": "7\n", "authors": ["913"]}
{"title": "Grounding event references in news\n", "abstract": " Events are frequently discussed in natural language, and their accurate identification is central to language understanding. Yet they are diverse and complex in ontology and reference; computational processing hence proves challenging. News provides a shared basis for communication by reporting events. We perform several studies into news event reference. One annotation study characterises each news report in terms of its update and topic events, but finds that topic is better consider through explicit references to background events. In this context, we propose the event linking task which\u2014analogous to named entity linking or disambiguation\u2014models the grounding of references to notable events. It defines the disambiguation of an event reference as a link to the archival article that first reports it. When two references are linked to the same article, they need not be references to the same event. Event linking hopes to provide an intuitive approximation to coreference, erring on the side of over-generation in contrast with the literature. The task is also distinguished in considering event references from multiple perspectives over time. We diagnostically evaluate the task by first linking references to past, newsworthy events in news and opinion pieces to an archive of the Sydney Morning Herald. The intensive annotation results in only a small corpus of 229 distinct links. However, we observe that a number of hyperlinks targeting online news correspond to event links. We thus acquire two large corpora of hyperlinks at very low cost. From these we learn weights for temporal and term overlap features in a retrieval system. These noisy data lead to\u00a0\u2026", "num_citations": "7\n", "authors": ["913"]}
{"title": "A distributed architecture for interactive parse annotation\n", "abstract": " In this paper we describe a modular system architecture for distributed parse annotation using interactive correction. This involves interactively adding constraints to an existing parse until the returned parse is correct. Using a mixed initiative approach, human annotators interact live with distributed ccg parser servers through an annotation gui. The examples presented to each annotator are selected by an active learning framework to maximise the value of the annotated corpus for machine learners. We report on an initial implementation based on a distributed workflow architecture.", "num_citations": "7\n", "authors": ["913"]}
{"title": "Article segmentation in digitised newspapers with a 2D Markov model\n", "abstract": " Document analysis and recognition is increasingly used to digitise collections of historical books, newspapers and other periodicals. In the digital humanities, it is often the goal to apply information retrieval (IR) and natural language processing (NLP) techniques to help researchers analyse and navigate these digitised archives. The lack of article segmentation is impairing many IR and NLP systems, which assume text is split into ordered, error-free documents. We define a document analysis and image processing task for segmenting digitised newspapers into articles and other content, e.g. adverts, and we automatically create a dataset of 11602 articles. Using this dataset, we develop and evaluate an innovative 2D Markov model that encodes reading order and substantially outperforms the current state-of-the-art, reaching similar accuracy to human annotators.", "num_citations": "6\n", "authors": ["913"]}
{"title": "Using mention accessibility to improve coreference resolution\n", "abstract": " Modern coreference resolution systems require linguistic and general knowledge typically sourced from costly, manually curated resources. Despite their intuitive appeal, results have been mixed. In this work, we instead implement fine-grained surface-level features motivated by cognitive theory. Our novel fine-grained feature specialisation approach significantly improves the performance of a strong baseline, achieving state-of-the-art results of 65.29 and 61.13% on CoNLL-2012 using gold and automatic preprocessing, with system extracted mentions.", "num_citations": "5\n", "authors": ["913"]}
{"title": "The Computable News project: research in the newsroom\n", "abstract": " We report on a four year academic research project to build a natural language processing platform in support of a large media company. The Computable News platform processes news stories, producing a layer of structured data that can be used to build rich applications. We describe the underlying platform and the research tasks that we explored building it. The platform supports a wide range of prototype applications designed to support different newsroom functions. We hope that this qualitative review provides some insight into the challenges involved in this type of project.", "num_citations": "3\n", "authors": ["913"]}
{"title": "Trading accuracy for faster named entity linking\n", "abstract": " Named entity linking (NEL) can be applied to documents such as financial reports, web pages and news articles, but state of the art disambiguation techniques are currently too slow for web-scale applications because of a high complexity with respect to the number of candidates. In this paper, we accelerate NEL by taking two successful disambiguation features (popularity and context comparability) and use them to reduce the number of candidates before further disambiguation takes place. Popularity is measured by in-link score, and context similarity is measured by locality sensitive hashing.We present a novel approach to locality sensitive hashing which embeds the projection matrix into a smaller array and extracts columns of the projection matrix using feature hashing, resulting in a lowmemory approximation. We run the linker on a test set in 63% of the baseline time with an accuracy loss of 0.72%.", "num_citations": "3\n", "authors": ["913"]}
{"title": "Command-line utilities for managing and exploring annotated corpora\n", "abstract": " Users of annotated corpora frequently perform basic operations such as inspecting the available annotations, filtering documents, formatting data, and aggregating basic statistics over a corpus. While these may be easily performed over flat text files with stream-processing UNIX tools, similar tools for structured annotation require custom design. Dawborn and Curran (2014) have developed a declarative description and storage for structured annotation, on top of which we have built generic command-line utilities. We describe the most useful utilities\u2013some for quick data exploration, others for high-level corpus management\u2013with reference to comparable UNIX utilities. We suggest that such tools are universally valuable for working with structured corpora; in turn, their utility promotes common storage and distribution formats for annotated text.", "num_citations": "3\n", "authors": ["913"]}
{"title": "Overview of TAC-KBP 2019 Fine-grained Entity Extraction.\n", "abstract": " In the past several years TAC KBP Entity Discovery and Linking (EDL) track has only focused on five major coarse-grained entity types: person (PER), geo-political entity (GPE), location (LOC), organization (ORG) and facility (FAC). However, many real-world applications in scenarios such as disaster relief and technical support require us to significantly extend EDL capabilities to a wider variety of fine-grained entity types (eg, technical terms, lawsuits, disease, crisis, vehicles, food, biomedical entities). In this overview paper we give an outline of the Ultra-Fine-Grained Name Tagging for Entity Types task (along with system participation) at the Knowledge Base Population (KBP) track at TAC 2019. We will also sketch out remaining challenges and future research directions.", "num_citations": "1\n", "authors": ["913"]}
{"title": "Understanding engagement with insurgents through retweet rhetoric\n", "abstract": " Organisations\u2014including insurgent movements\u2014harness social media to engage potential consumers. They evoke sympathetic (and antipathic) response; content sharing engenders affinity and community. We report on a pilot study of presumed rhetorical intent for statuses retweeted by a set of suspected Islamic State-sympathetic Twitter accounts. This annotation is orthogonal to prior opinion mining work focused on sentiment or stance expressed in a debate, and suggests a parallel to dialogue act classification applied to retweeting. By exploring the distribution of rhetoric among Islamic State-sympathetic and general users, we also hope to identify trends in IS social media use and user roles.", "num_citations": "1\n", "authors": ["913"]}