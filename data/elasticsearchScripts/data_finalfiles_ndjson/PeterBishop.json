{"title": "A methodology for safety case development\n", "abstract": " A safety case is a rcquircmelll in many safety standards. Expli<.: il safely cases arc required fur military systems, the off shore oil industry, rail transport and the nucle< tr industry. r011hermorc. equivalent requirements can be found in other industry standards, such as IHC l.'iOK (which requires a\" functional safely as~ ssmcm\") the EN 292 Machinery Directive (which n:<) Uircs a\" technical file\") and DO 17118 for avionics (which requires an\" accomplishnu: nl sumnmry\").It is important that an adequate safety case is produced for a system. In ll! I (Uiatcd industries such as the nuclear industry, the need to de1111mstmtc\u2022 saldy lu a regulator can he a majur commercial risk. For example the computer-based Darlington Reactor Prulcction System in Canada required around 50 mim years of software asscssnu: nt effort which w< ts prub< tbly more than the effort required to develop the software. In addition the< tssessmelll dcl<\u00a0\u2026", "num_citations": "375\n", "authors": ["477"]}
{"title": "Safety and assurance cases: Past, present and possible future\u2013an Adelard perspective\n", "abstract": " This paper focuses on the approaches used in safety cases for software based systems. We outline the history of approaches for assuring the safety of software-based systems, the current uptake of safety and assurance cases and the current practice on structured safety cases. Directions for further development are discussed.", "num_citations": "190\n", "authors": ["477"]}
{"title": "PODS\u2014A project on diverse software\n", "abstract": " A review of the Project on Diverse Software (PODS), a collaborative software reliability research project, is presented. The purpose of the project was to determine the effect of a number of different software development techniques on software reliability. The main objectives were to evaluate the merits of using diverse software, evaluate the specification language X-SPEX, and compare the productivity and reliability associated with high-level and low-level languages. A secondary objective was to monitor the software development process, with particular reference to the creation and detection of software faults. To achieve these objectives, an experiment was performed which simulated a normal software development process to produce three diverse programs to the same requirement. The requirement was for a reactor over-power protection (trip) system. After careful independent development and testing, the\u00a0\u2026", "num_citations": "124\n", "authors": ["477"]}
{"title": "The variation of software survival time for different operational input profiles (or why you can wait a long time for a big bug to fail)\n", "abstract": " Experimental and theoretical evidence for the existence of contiguous failure regions in the program input space (blob defects) is provided. For real-time systems where successive input values tend to be similar, blob defects can have a major impact on the software survival time because the failure probability is not constant. For example, with a random walk input sequence, the probability of failure decreases as the time from the last failure increases. It is shown that the key factors affecting the survival time are the input trajectory, the rate of change of the input values, and the surface area of the defect (rather than its volume).", "num_citations": "109\n", "authors": ["477"]}
{"title": "The future of goal-based assurance cases\n", "abstract": " Most regulations and guidelines for critical systems require a documented case that the system will meet its critical requirements, which we call an assurance case. Increasingly, the case is made using a goal-based approach, where claims are made (or goals are set) about the system and arguments and evidence are presented to support those claims. In this paper we describe Adelard\u2019s approach to safety cases in particular, and assurance cases more generally, and discuss some possible future directions to improve frameworks for goal-based assurance cases.", "num_citations": "91\n", "authors": ["477"]}
{"title": "PODS revisited-a study of software failure behaviour\n", "abstract": " A description is given of an empirical study of the failure characteristics of software defects detected in the programs developed in the Project on Diverse Software (PODS). The results are interpreted in the context of a state machine model of software failure. The results of the empirical study case doubts on the general validity of the assumption of constant software failure probability and the assumption of constant software failure probability and the assumption that all defects have similar failure rates. In addition, an analysis of failure dependency lends support to the use of diversity as a means of minimizing the impact of design-level faults. Here, nonidentical faults exhibited coincident failure characteristics approximately in accord with the independence assumption, and some of the observed positive and negative correlation effects could be explained by failure masking effects, which can be removed by suitable\u00a0\u2026", "num_citations": "63\n", "authors": ["477"]}
{"title": "The practicalities of goal-based safety regulation\n", "abstract": " \u201cGoal-based regulation\u201d does not specify the means of achieving compliance but sets goals that allow alternative ways of achieving compliance, e.g. \u201cPeople shall be prevented from falling over the edge of the cliff\u201d. In \u201cprescriptive regulation\u201d the specific means of achieving compliance is mandated, e.g. \u201cYou shall install a 1 meter high rail at the edge of the cliff\u201d.", "num_citations": "59\n", "authors": ["477"]}
{"title": "A conservative theory for long-term reliability-growth prediction [of software]\n", "abstract": " This paper describes a different approach to software reliability growth modeling which enables long-term predictions. Using relatively common assumptions, it is shown that the average value of the failure rate of the program, after a particular use-time, t, is bounded by N/(e/spl middot/t), where N is the initial number of faults. This is conservative since it places a worst-case bound on the reliability rather than making a best estimate. The predictions might be relatively insensitive to assumption violations over the longer term. The theory offers the potential for making long-term software reliability growth predictions based solely on prior estimates of the number of residual faults. The predicted bound appears to agree with a wide range of industrial and experimental reliability data. Less pessimistic results can be obtained if additional assumptions are made about the failure rate distribution of faults.", "num_citations": "55\n", "authors": ["477"]}
{"title": "Dependability of Critical Computer Systems v. 3\n", "abstract": " Dependability of Critical Computer Systems v. 3 | Guide books ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksDependability of Critical Computer Systems v. ABSTRACT No abstract available. Comments Login options Check if you have access through your login credentials or your institution to get full access on this article. Sign in Full Access Get this Publication Information Contributors Published in Guide books cover image Dependability of Critical Computer Systems v. 3 October 2007 250 pages ISBN:1851665447 Author: Peter G Bishop profile image PG Bishop Copyright \u00a9 2007 Publisher Springer Publishing Company, : 12 \u2026", "num_citations": "48\n", "authors": ["477"]}
{"title": "Diversity for security: a study with off-the-shelf antivirus engines\n", "abstract": " We have previously reported [1] the results of an exploratory analysis of the potential gains in detection capability from using diverse AntiVirus products. The analysis was based on 1599 malware samples collected from a distributed honey pot deployment over a period of 178 days. The malware samples were sent to the signature engines of 32 different AntiVirus products hosted by the Virus Total service. The analysis suggested significant gains in detection capability from using more than one AntiVirus product in a one-out-of-two intrusion-tolerant setup. In this paper we present new analysis of this dataset to explore the detection gains that can be achieved from using more diversity (i.e. more than two AntiVirus products), how diversity may help to reduce the \"at risk time\" of a system and a preliminary model-fitting using the hyper-exponential distribution.", "num_citations": "44\n", "authors": ["477"]}
{"title": "Software fault tolerance by design diversity\n", "abstract": " CiteSeerX \u2014 Software fault tolerance by design diversity Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA Software fault tolerance by design diversity (1995) Cached Download as a PDF Download Links [www.cse.cuhk.edu.hk] Save to List Add to Collection Correct Errors Monitor Changes by Peter Bishop Citations: 12 - 0 self Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract Keyphrases pitfall design n-version experiment Powered by: Apache Solr About CiteSeerX Submit and Index Documents Privacy Policy Help Data Source Contact Us Developed at and hosted by The College of Information Sciences and Technology \u00a9 2007-2019 The Pennsylvania State University \u2026", "num_citations": "38\n", "authors": ["477"]}
{"title": "Software criticality analysis of cots/soup\n", "abstract": " This paper describes the Software Criticality Analysis (SCA) approach that was developed to support the justification of commercial off-the-shelf software (COTS) used in a safety-related system. The primary objective of SCA is to assess the importance to safety of the software components within the COTS and to show there is segregation between software components with different safety importance. The approach taken was a combination of Hazops based on design documents and on a detailed analysis of the actual code (100kloc). Considerable effort was spent on validation and ensuring the conservative nature of the results. The results from reverse engineering from the code showed that results based only on architecture and design documents would have been misleading.", "num_citations": "37\n", "authors": ["477"]}
{"title": "The risk assessment of ERTMS-based railway systems from a cyber security perspective: Methodology and lessons learned\n", "abstract": " The impact that cyber issues might have on the safety and resilience of railway systems has been studied for more than five years by industry specialists and government agencies. This paper presents some of the work done by Adelard in this area, ranging from an analysis of potential vulnerabilities in the ERTMS specifications through to a high-level cyber security risk assessment of a national ERTMS implementation and detailed analysis of particular ERTMS systems on behalf of the GB rail industry. The focus of the paper is on our overall methodology for security-informed safety and hazard analysis. Lessons learned will be presented but of course our detailed results remain proprietary or sensitive and cannot be published.", "num_citations": "36\n", "authors": ["477"]}
{"title": "Justifying the use of software of uncertain pedigree (SOUP) in safety-related applications\n", "abstract": " This short paper is intended to serve as an introduction to a publicly available research study undertaken by Adelard for the UK Health and Safety Executive [1]. The main focus for this project was \u201csoftware of uncertain pedigree\u201d(SOUP) used in safetyrelated applications. It outlines an overall safety justification approach and ways in which the use of SOUP can be incorporated within that approach. The full report is available from the HSE web site.", "num_citations": "34\n", "authors": ["477"]}
{"title": "The SHIP safety case approach\n", "abstract": " This paper presents a safety case approach to the justification of safety-related systems. It combines methods used for handling software design faults with approaches used for hazardous plant. The general structure of the safety argument is presented together with the underlying models for system failure that can be used as the basis for quantified reliability estimates. The approach is illustrated using plant and computer based examples.", "num_citations": "33\n", "authors": ["477"]}
{"title": "Using reversible computing to achieve fail-safety\n", "abstract": " This paper describes a fail-safe design approach that can be used to achieve a high level of fail-safety with conventional computing equipment which may contain design flaws. The method is based on the well-established concept of reversible computing. Conventional programs destroy information and hence cannot be reversed. However it is easy to define a virtual machine that preserves sufficient intermediate information to permit reversal. Any program implemented on this virtual machine is inherently reversible. The integrity of a calculation can therefore be checked by reversing back from the output values and checking for the equivalence of intermediate values and original input values. By using different machine instructions on the forward and reverse paths, errors in any single instruction execution can be revealed. Random corruptions in data values are also detected. An assessment of the performance of\u00a0\u2026", "num_citations": "32\n", "authors": ["477"]}
{"title": "An exploration of software faults and failure behaviour in a large population of programs\n", "abstract": " A large part of software engineering research suffers from a major problem-there are insufficient data to test software hypotheses, or to estimate parameters in models. To obtain statistically significant results, a large set of programs is needed, each set comprising many programs built to the same specification. We have gained access to such a large body of programs (written in C, C++, Java or Pascal) and in this paper we present the results of an exploratory analysis of around 29,000 C programs written to a common specification. The objectives of this study were to characterise the types of fault that are present in these programs; to characterise how programs are debugged during development; and to assess the effectiveness of diverse programming. The findings are discussed, together with the potential limitations on the realism of the findings.", "num_citations": "30\n", "authors": ["477"]}
{"title": "Error masking: A source of failure dependency in multi-version programs\n", "abstract": " This paper presents some empirical measurements of failure dependencies between the known faults detected in an earlier software diversity experiment (PODS). The results showed that some apparently unrelated pairs of faults had high (and very similar) levels of dependency. This has been explained in terms of a error masking process. It is shown that this process is likely to occur in many software applications, including the missile launcher application used in the Knight and Leveson experiment. Error masking behaviour can be predicted from the specification (prior to implementation), and simple modifications to the program design can minimize the error masking effect and hence the observed dependency.", "num_citations": "29\n", "authors": ["477"]}
{"title": "Project on diverse software\u2014an experiment in software reliability\n", "abstract": " The Project on Diverse Software (PODS) is an experiment which has attempted to quantify ehe impact of a nunber of commonly used software development techniques on software reliability. The main objectives of the project were to:\u2013Provide a measure of the merits of using diverse software.\u2013Evaluate the formal specification language X.\u2013Compare the time and effort expended using high-and low-level languages and using formal and informal specification techniques.\u2013Compare the reliability of software written in high-and low-level languages.\u2013Evaluate testing methodologies.\u2013Observe the software development processThis paper describes the experimental design, organisation and docunentatlon of the project and presents an analysis of the results. Some of the major conclusions of the experiment are that:\u2013Diverse implementation was effective in revealing faults not discovered by normal development methods\u00a0\u2026", "num_citations": "29\n", "authors": ["477"]}
{"title": "Using a log-normal failure rate distribution for worst case bound reliability prediction\n", "abstract": " Prior research has suggested that the failure rates of faults follow a log normal distribution. We propose a specific model where distributions close to a log normal arise naturally from the program structure. The log normal distribution presents a problem when used in reliability growth models as it is not mathematically tractable. However we demonstrate that a worst case bound can be estimated that is less pessimistic than our earlier worst case bound theory.", "num_citations": "25\n", "authors": ["477"]}
{"title": "Worst case reliability prediction based on a prior estimate of residual defects\n", "abstract": " In this paper we extend an earlier worst case bound reliability theory to derive a worst case reliability function R(t), which gives the worst case probability of surviving a further time t given an estimate of residual defects in the software N and a prior test time T. The earlier theory and its extension are presented and the paper also considers the case where there is a low probability of any defect existing in the program. For the \"fractional defect\" case, there can be a high probability of surviving any subsequent time t. The implications of the theory are discussed and compared with alternative reliability models.", "num_citations": "20\n", "authors": ["477"]}
{"title": "Methods for Assessing the Safety Integrity of Safety-related Software of Uncertain Pedigree (SOUP).\n", "abstract": " Methods for assessing the safety integrity of safety-related software of uncertain pedigree (SOUP) - OpenGrey fra | eng OpenGrey Open System for Information on Grey literature in Europe Home Search Subjects Partners Export Help Search XML To cite or link to this reference: http://hdl.handle.net/10068/624513 Title : Methods for assessing the safety integrity of safety-related software of uncertain pedigree (SOUP) Authors : Jones, C. ; Bloomfield, R. ; Froome, P. ; Corporate author : Health and Safety Executive, London (United Kingdom) ; Publication year : 2001 Language : English ; Pagination/Size : 38 p. ; SIGLE classification : 09H - Computer software, programming ; Keyword(s) : STANDARDS ; RELIABILITY ; Document type : R - Report ; ISBN : ISBN 0-7176-2011-5 ; Report number : HSE-CRR--337/2001 ; Other identifier : GB_ 2001:4628 ; GB ; handle : http://hdl.handle.net/10068/624513 Provenance : SIGLE ; : : a -\u2026", "num_citations": "19\n", "authors": ["477"]}
{"title": "Stem\u2014A Project on Software Test and Evaluation Methods\n", "abstract": " STEM is an acronym for Software Test and Evaluation Methodologies. The objective of the STEM project was to evaluate a number of fault detection, fault prediction and failure estimation methods by applying them to the documented programs produced in PODS (Project on Diverse Software, IEEE Trans. S. Eng. Sept 1986) which contained known faults. At the time this paper was written, the STEM project was still in progress, but some of the results obtained have been surprizing. For the particular programs used in this project it was found that:                                     Of the many types of test data examined, comparison testing with uniform random data seemed to be the most effective (even when compared with specially designed tests).                                                     Non-identical bugs in diverse programs generally conformed with the independent failure assumption and around 5% were negatively correlated\u00a0\u2026", "num_citations": "19\n", "authors": ["477"]}
{"title": "Rescaling reliability bounds for a new operational profile\n", "abstract": " One of the main problems with reliability testing and prediction is that the result is specific to a particular operational profile. This paper extends an earlier reliability theory for computing a worst case reliability bound. The extended theory derives a re-scaled reliability bound based on the change in execution rates of the code segments in the program. In some cases it is possible to derive a maximum failure rate bound that applies to any change in the profile. It also predicts that (in principle) a\" fair\" test profile can be derived where the reliability bounds are relatively insensitive to the operational profile. In addition the theory allows unit and module test coverage measures to be incorporated into an operational reliability bound prediction. The implications of the theory are discussed, and the theory is evaluated by applying it to two example programs with known faults.", "num_citations": "16\n", "authors": ["477"]}
{"title": "Software diversity: way to enhance safety?\n", "abstract": " The topic of the paper is the use of diversely produced programs to enhance the safety of computer-based systems applied in safety-critical areas. The paper starts with a survey of scientific investigations on the impact of software redundancy made at various institutions around the world. Main emphasis will, however, be put on the PODS/STEM projects, which have been performed at the OECD Halden Project in cooperation with the Technical Research Center of Finland, the Safety and Reliability Directorate, AEA Technology, UK, and Central Electricity Research Laboratory (now National Power Technology and Environment Centre), UK. In these projects, three program versions were made independently by three different teams, all based on the same specification. The three programs were tested back-to-back with a large amount of test data. The experience and results from this process were carefully logged and\u00a0\u2026", "num_citations": "15\n", "authors": ["477"]}
{"title": "Deriving a frequentist conservative confidence bound for probability of failure per demand for systems with different operational and test profiles\n", "abstract": " Reliability testing is typically used in demand-based systems (such as protection systems) to derive a confidence bound for a specific operational profile. To be realistic, the number of tests for each class of demand should be proportional to the demand frequency of the class. In practice, however, the actual operational profile may differ from that used during testing. This paper provides a means for estimating the confidence bound when the test profile differs from the profile used in actual operation. Based on this analysis the paper examines what bound can be claimed for different types of profile uncertainty and options for dealing with this uncertainty. We also show that the same conservative bound estimation equations can be applied to cases where different measures of software test coverage and operational profile are used.", "num_citations": "14\n", "authors": ["477"]}
{"title": "Combining testing and proof to gain high assurance in software: a case study\n", "abstract": " Dynamic software test methods are generally easy to use, but the results only apply to the specific input values tested. Static analysis produces results which are more general, but can require more effort to perform. There are potential benefits in combining both types of techniques because the results obtained can be more general than standalone dynamic testing but less resource-intensive than standalone static analysis. This paper presents a specific example of this approach applied to the verification of continuous monotonic functions. This approach combines a monotonicity analysis with a defined set of tests to demonstrate the accuracy of a software function over its entire input range. Unlike \u201cstandalone\u201d dynamic methods, our approach provides full coverage, and guarantees a maximal error. We present a case study of the application of our approach to the analysis and testing of the software-implemented\u00a0\u2026", "num_citations": "14\n", "authors": ["477"]}
{"title": "Justification of smart sensors for nuclear applications\n", "abstract": " This paper describes the results of a research study sponsored by the UK nuclear industry into methods of justifying smart sensors. Smart sensors are increasingly being used in the nuclear industry; they have potential benefits such as greater accuracy and better noise filtering, and in many cases their analogue counterparts are no longer manufactured. However, smart sensors (as it is the case for most COTS) are sold as black boxes despite the fact that their safety justification might require knowledge of their internal structure and development process. The study covered both management aspects of interacting with manufacturers to obtain the information needed, and the technical aspects of designing an appropriate safety justification approach and assessing feasibility of a range of technical analyses. The analyses performed include the methods we presented at Safecomp 2002 and 2003.", "num_citations": "14\n", "authors": ["477"]}
{"title": "Safety case framework to provide justifiable reliability numbers for software systems\n", "abstract": " Very high reliability figures cannot be formally justified for a piece of software. Failure probabilities lower than 1E-4 are rarely claimed or justified even in a highly diversified software system, and there is not an accepted approach for the use of quantitative evaluation for software reliability between the different countries. The situation is even more difficult concerning figures for software CCF. The current state of the art for the quantification of software reliability relies mostly on holistic approaches, such as conformance to appropriate safety standards such as IEC 61508, or statistical testing. The EU Euratom FP7 project HARMONICS (Harmonised Assessment of Reliability of Modern Nuclear I&C Software) will tackle the problem of software reliability quantification using analytical and Bayesian approaches that take into consideration all the information available, in particular evidence obtained by V&V. Key to these approaches is how different pieces of evidence are interpreted in a probability model context and how their interrelationships are assessed. This can be combined with other approaches that model the development process and use development fault data to estimate the number of residual faults. This information can then be used to estimate worst-case bounds on the software reliability. The justification of the reliability estimated will follow the concept of a structured safety case, which is a solution to get a coherent process for the quantitative reliability assessment of software-based systems.", "num_citations": "13\n", "authors": ["477"]}
{"title": "MC/DC based estimation and detection of residual faults in PLC logic networks\n", "abstract": " A logic coverage measure related to MC/DC testing is used to estimate residual faults. The residual fault prediction method is evaluated on an industrial PLC logic example. A randomized form of MC/DC testing is used to maximize coverage growth and fault detection efficiency.", "num_citations": "13\n", "authors": ["477"]}
{"title": "Estimating residual faults from code coverage\n", "abstract": " Many reliability prediction techniques require an estimate for the number of residual faults. In this paper, a new theory is developed for using test coverage to estimate the number of residual faults. This theory is applied to a specific example with known faults and the results agree well with the theory. The theory is used to justify the use of linear extrapolation to estimate residual faults. It is also shown that it is important to establish the amount of unreachable code in order to make a realistic residual fault estimate.", "num_citations": "12\n", "authors": ["477"]}
{"title": "Probabilistic modelling of software failure characteristics\n", "abstract": " This paper describes an empirical study of the failure characteristics of software defects detected in the programs developed in the Project on Diverse Software (PODS).The results of the empirical study cast doubts on the general validity of the following assumptions:--similar failure rates for a population of defects,-a constant software failure rate.The observed deviations from these assumptions are explained in terms of a state machine model of software behaviour in conjunction with an assumption that software defects occur as \u2018blobs\u2019 in the program state space.", "num_citations": "12\n", "authors": ["477"]}
{"title": "The ship safety case approach: a combination of system and software methods\n", "abstract": " This paper presents a safety case approach to the justification of safety-related systems. It combines methods used for handling software design faults with approaches used for hazardous plant. The general structure of the safety argument is presented together with the underlying models for system failure that can be used as the basis for quantified reliability estimates. The approach is illustrated using plant and computer based examples.", "num_citations": "11\n", "authors": ["477"]}
{"title": "The PODS diversity experiment\n", "abstract": " A high integrity system typically has a number of redundant components operating in parallel to reduce the probability of a system failure. If the component failures were random, then the probability of several components failing simultaneously would be much smaller than the failure probability of any single component. However, should the components contain common design flaws, then more than one component could fail simultaneously due to a common cause (a common mode failure). This would increase the probability of a system failure. For a computer-based system where the same software \u201ccomponent\u201d is being run in each processor, any software fault is a potential cause of common mode failure. One method of reducing common software faults is to use diverse software in each processor (n-version programming [Avi\u017eienis 1975]).", "num_citations": "11\n", "authors": ["477"]}
{"title": "Does software have to be ultra reliable in safety critical systems?\n", "abstract": " It is difficult to demonstrate that safety-critical software is completely free of dangerous faults. Prior testing can be used to demonstrate that the unsafe failure rate is below some bound, but in practice, the bound is not low enough to demonstrate the level of safety performance required for critical software-based systems like avionics. This paper argues higher levels of safety performance can be claimed by taking account of: 1) external mitigation to prevent an accident: 2) the fact that software is corrected once failures are detected in operation. A model based on these concepts is developed to derive an upper bound on the number of expected failures and accidents under different assumptions about fault fixing, diagnosis, repair and accident mitigation. A numerical example is used to illustrate the approach. The implications and potential applications of the theory are discussed.", "num_citations": "10\n", "authors": ["477"]}
{"title": "An empirical exploration of the difficulty function\n", "abstract": " The theory developed by Eckhardt and Lee (and later extended by Littlewood and Miller) utilises the concept of a \u201cdifficulty function\u201d to estimate the expected gain in reliability of fault tolerant architectures based on diverse programs. The \u201cdifficulty function\u201d is the likelihood that a randomly chosen program will fail for any given input value. To date this has been an abstract concept that explains why dependent failures are likely to occur. This paper presents an empirical measurement of the difficulty function based on an analysis of over six thousand program versions implemented to a common specification. The study derived a \u201cscore function\u201d for each version. It was found that several different program versions produced identical score functions, which when analysed, were usually found to be due to common programming faults. The score functions of the individual versions were combined to derive an\u00a0\u2026", "num_citations": "10\n", "authors": ["477"]}
{"title": "SILs and software\n", "abstract": " The SIL (safety integrity level) concept was introduced in the HSE (Health and Safety Executive) PES (programmable electronic system) guidelines and subsequently extended in the development of IEC 61508. This article sets out our diagnosis of some of the issues associated with the SIL concept and provides some ideas for the improvement of IEC 61508.", "num_citations": "8\n", "authors": ["477"]}
{"title": "Assessment and qualification of smart sensors\n", "abstract": " This paper describes research work done on approaches to justifying smart instruments, and in particular, how some of this research has successfully been applied to the safety substantiation of such instruments. From a management perspective, we examine both the issues involved gaining access to information required for this justification and the necessity for a sustainable long-term approach for the justification of smart sensors that is acceptable to both suppliers and customers. From a technical perspective, we examine both overall safety justification approaches and specific techniques that can be used in the justification of the instruments\u2019 software. Our smart device assessment work covered both management and technical issues. Many of the approaches that were initially developed in research projects have now been applied in practice to smart devices that will be used in nuclear applications. We anticipate that further analysis techniques developed in our research programme will be deployed in future device assessments.", "num_citations": "7\n", "authors": ["477"]}
{"title": "Safety justification frameworks: Integrating rule-based, goal-based and risk-informed approaches\n", "abstract": " The reliability and safety of the digital I&C systems that implement safety functions are critical issues. In particular, software defects could result in common cause failures that defeat redundancy and defence-in-depth mechanisms. Unfortunately, the differences in current safety justification principles and methods for digital I&C restrict international co-operation and hinder the emergence of widely accepted best practices. These differences also prevent cost sharing and reduction, and unnecessarily increase licensing uncertainties, thus creating a very difficult operating environment for utilities, vendors and regulatory bodies. The European project HARMONICS (Harmonised Assessment of Reliability of MOdern Nuclear I&C Software) is seeking to develop a more harmonised approach to the justification of software-based I&C systems important to safety. This paper outlines the justification framework we intend to develop in HARMONICS. It will integrate three strategies commonly used in safety justifications of I&C system and its software: rule-based-evidence of compliance to accepted standards; goal-based-evidence that the intended behaviour and other claimed properties has been achieved; and risk-informed-evidence that unintended behaviour is unlikely. The paper will present general forms of safety case that can be adapted to a variety of specific topics.", "num_citations": "6\n", "authors": ["477"]}
{"title": "Security-informed safety: Supporting stakeholders with codes of practice\n", "abstract": " Codes of practice provide principles and guidance on how organizations can incorporate security considerations into their safety engineering lifecycle and become more security minded.", "num_citations": "5\n", "authors": ["477"]}
{"title": "Overcoming non-determinism in testing smart devices: a case study\n", "abstract": " This paper presents a case study in \u201dblack-box\u201d assessment of a \u201dsmart\u201d device where, based only on the user manuals and the instrument itself, we try to build confidence in smart device reliability. To perform the black-box assessment, we developed a test environment which automates the generation of test data, their execution and interpretation of the results. The assessment was made more complex by the inherent non-determinism of the device. For example, non-determinism can arise due to inaccuracy in an analogue measurement made by the device when two alternative actions are possible depending on the measured value. This non-determinism makes it difficult to predict the output values that are expected from a test sequence of analogue input values. The paper presents two approaches to dealing with this difficulty: (1) based on avoidance of test values that could have multiple responses, (2\u00a0\u2026", "num_citations": "5\n", "authors": ["477"]}
{"title": "A random walk through software reliability theory\n", "abstract": " A random walk through software reliability theory | Mathematical structures for software engineering ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksMathematical structures for software engineeringA random walk through software reliability theory chapter A random walk through software reliability theory Share on Authors: Peter G Bishop profile image PG Bishop View Profile , FD Pullen profile image FD Pullen View Profile Authors Info & Affiliations Publication: Mathematical structures for software engineeringJune 1991 Pages 83\u2013111 1citation 0 Downloads Metrics Total Citations1 Total Downloads0 Last 12 Months0 Last 6 weeks\u2026", "num_citations": "5\n", "authors": ["477"]}
{"title": "Using an assurance case framework to develop security strategy and policies\n", "abstract": " Assurance cases have been developed to reason and communicate about the trustworthiness of systems. Recently we have also been using them to support the development of policy and to assess the impact of security issues on safety regulation. In the example we present in this paper, we worked with a safety regulator (anonymised as A Regulatory Organisation (ARO) in this paper) to investigate the impact of cyber-security on safety regulation.", "num_citations": "3\n", "authors": ["477"]}
{"title": "Estimating PLC logic program reliability\n", "abstract": " In earlier research we developed a theory for predicting the reliability of conventional sequential programs based on an estimate of residual faults. This paper describes how the theory was applied to a realistic industrial example containing a known number of faults. The industrial example was implemented in a PLC application language where the program is represented by a network of logic gates (e.g. AND and OR gates). To make a residual fault estimate, our fault estimation method had to be adapted to apply to logic networks. The previous estimation method relied on a measurement of code coverage, and this had to be replaced by a measurement of logic network coverage. Several different measures of logic coverage were evaluated, including coverage of input values, output values, and input-output pair values. Using the residual fault estimate and information about the testing applied, a reliability\u00a0\u2026", "num_citations": "3\n", "authors": ["477"]}
{"title": "Diverse protection systems for improving security: a study with AntiVirus engines\n", "abstract": " Diverse \u201cbarriers\u201d or \u201cprotection systems\u201d are very common in many industries, especially in safety-critical ones where the designers must use \u201cdefense in depth\u201d techniques to prevent safety failures. Similar techniques are also commonly prescribed for security systems: using multiple, diverse detection systems to prevent security breaches. However empirical evidence of the effectiveness of diversity is rare. We present results of an empirical study which uses a large-scale dataset to assess the benefits of diversity with an important category of security systems: AntiVirus products. The analysis was based on 1599 malware samples collected from a distributed honeypot deployment over a period of 178 days. The malware samples were sent to the signature engines of 32 different AntiVirus products hosted by the VirusTotal service. We also present an exploratory model which shows that the number of diverse protection layers that are needed to achieve \u201cperfect\u201d detection with our dataset follows an exponential power-law distribution. If this distribution is shown to be generic with other datasets, it would be a cost-effective means for predicting the probability of perfect detection for systems that use a large number of barriers based on measurements made with systems that are composed of fewer (say 2, 3) barriers.", "num_citations": "2\n", "authors": ["477"]}
{"title": "HSE Learning from incidents D1-Review of methods and industry practice\n", "abstract": " HSE Learning from incidents D1 - Review of methods and industry practice \u2014 TU Delft Research Portal Skip to main navigation Skip to search Skip to main content TU Delft Research Portal Logo Help & FAQ Home Researchers Research Units Research output Activities Datasets Press / Media Prizes Projects Search by expertise, name or affiliation HSE Learning from incidents D1 - Review of methods and industry practice PG Bishop, RE Bloomfield, LO Emmet, A Johnson, W Black, V Hamilton, F Koornneef Safety and Security Science Research output: Book/Report \u203a Report \u203a Professional Overview Original language Undefined/Unknown Place of Publication Londen Publisher Adelard Number of pages 90 Publication status Published - 2003 Publication series Name Publisher Adelard Keywords official reports Geen BTA classificatie Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Bishop, PG., , RE., , ., .\u2026", "num_citations": "2\n", "authors": ["477"]}
{"title": "A conservative confidence bound for the probability of failure on demand of a software-based system based on failure-free tests of its components\n", "abstract": " The standard approach to deriving the confidence bound for the probability of failure on demand (pfd) of a software-based system is to perform statistical tests on the whole system as a \u201cblack-box\u201d. In practice, performing tests on the entire system may be infeasible for logistical reasons, such as lack of availability of all component subsystems at the same time during implementation. This paper presents a general method for deriving a confidence bound for the overall system from successful independent tests on individual system components. In addition, a strategy is presented for optimizing the number of tests allocated to system components for an arbitrary system architecture that minimizes the confidence bound for the system pfd. For some system architectures, we show that an optimum allocation of component tests is as effective as tests on the complete system for demonstrating a given confidence bound. The\u00a0\u2026", "num_citations": "1\n", "authors": ["477"]}
{"title": "The Variation of Software Survival Time for Different Operational Input Profiles. Fault-Tolerant Computing, 1993. FTCS-23\n", "abstract": " This paper provides experimental and theoretical evidence for the existence of contiguous failure regions in the program input space (\u2018blob\u2019defects). For real-time systems where successive input values tend to be similar, blob defects can have a major impact on the software survival time because the failure probability is not constant. For example, with a \u2018random walk\u2019input sequence, the probability of failure decreases as the time from the last failure increases. It is shown that the key factors affecting the survival time are the input \u2018trajectory\u2019, the rate of change of the input values and the \u2018surface area\u2019of the defect (rather than its volume).", "num_citations": "1\n", "authors": ["477"]}
{"title": "STEM-software test and evaluation methods. A study of failure dependency in diverse software\n", "abstract": " [en] STEM is a collaborative software reliability project undertaken in partnership with Halden Reactor Project, UKAEA, and the Finnish Technical Research Centre. The objective of STEM is to evaluate a number of fault detection and fault estimation methods which can be applied to high integrity software. This Report presents a study of the observed failure dependencies between faults in diversely produced software.(author)", "num_citations": "1\n", "authors": ["477"]}