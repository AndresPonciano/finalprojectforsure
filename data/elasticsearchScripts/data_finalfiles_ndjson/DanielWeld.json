{"title": "Open information extraction from the web\n", "abstract": " Targeted IE methods are transforming into open-ended techniques.", "num_citations": "2757\n", "authors": ["2065"]}
{"title": "Wrapper induction for information extraction\n", "abstract": " The Internet presents numerous sources of useful information--telephone directories, product catalogs, stock quotes, weather forecasts, etc. Recently, many systems have been built that automatically gather and manipulate such information on a user's behalf. However, these resources are usually formatted for use by people (eg, the relevant content is embedded in HTML pages), so extracting their content is difficult.", "num_citations": "1702\n", "authors": ["2065"]}
{"title": "Unsupervised named-entity extraction from the web: An experimental study\n", "abstract": " The KnowItAll system aims to automate the tedious process of extracting large collections of facts (eg, names of scientists or politicians) from the Web in an unsupervised, domain-independent, and scalable manner. The paper presents an overview of KnowItAll's novel architecture and design principles, emphasizing its distinctive ability to extract information without any hand-labeled training examples. In its first major run, KnowItAll extracted over 50,000 class instances, but suggested a challenge: How can we improve KnowItAll's recall and extraction rate without sacrificing precision? This paper presents three distinct ways to address this challenge and evaluates their performance. Pattern Learning learns domain-specific extraction rules, which enable additional extractions. Subclass Extraction automatically identifies sub-classes in order to boost recall (eg,\u201cchemist\u201d and \u201cbiologist\u201d are identified as sub-classes of\u00a0\u2026", "num_citations": "1476\n", "authors": ["2065"]}
{"title": "Web-scale information extraction in knowitall: (preliminary results)\n", "abstract": " Manually querying search engines in order to accumulate a large bodyof factual information is a tedious, error-prone process of piecemealsearch. Search engines retrieve and rank potentially relevantdocuments for human perusal, but do not extract facts, assessconfidence, or fuse information from multiple documents. This paperintroduces KnowItAll, a system that aims to automate the tedious process ofextracting large collections of facts from the web in an autonomous, domain-independent, and scalable manner. The paper describes preliminary experiments in which an instance of KnowItAll, running for four days on a single machine, was able to automatically extract 54,753 facts. KnowItAll associates a probability with each fact enabling it to trade off precision and recall. The paper analyzes KnowItAll's architecture and reports on lessons learned for the design of large-scale information extraction systems.", "num_citations": "1123\n", "authors": ["2065"]}
{"title": "UCPOP: A Sound, Complete, Partial Order Planner for ADL.\n", "abstract": " We describe the UCPOP partial order planning algorithm which handles a subset of Pednault's ADL action representation. In particular, UCPOP operates with actions that have conditional effects, universally quan-tified preconditions and effects, and with uni-versally quantified goals. We prove UCPOP is both sound and complete for this representation and describe a practical implementa-tion that succeeds on all of Pednault's and", "num_citations": "1121\n", "authors": ["2065"]}
{"title": "Knowledge-based weak supervision for information extraction of overlapping relations\n", "abstract": " Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web\u2019s natural language text. Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors. Recently, researchers have developed multiinstance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume relations are disjoint\u2014for example they cannot extract the pair Founded (Jobs, Apple) and CEO-of (Jobs, Apple).This paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extraction model with a simple, corpus-level component for aggregating the individual facts. We apply our model to learn extractors for NY Times text using weak supervision from Freebase. Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level.", "num_citations": "979\n", "authors": ["2065"]}
{"title": "A scalable comparison-shopping agent for the world-wide web\n", "abstract": " The World-Wide-Web is less agent-friendly than we might hope. Most information on the Web is presented in loosely structured natural language text with no agent-readable semantics. HTML annotations structure the display of Web pages, but provide virtually no insight into their content. Thus, the designers of intelligent Web agents need to address the following ques-tions:(1) To what extent can an agent understand information published at Web sites?(2) Is the agent's understanding sufficient to provide genuinely useful assistance to users?(3) Is site-specific hand-coding necessary, or can the agent automatically extract information from unfamiliar Web sites?(4) What aspects of the Web facilitate this competence? In this paper we investigate these issues with a case study using ShopBot, a fully-implemented, domain-independent comparison-shopping agent. Given the home pages of several online stores\u00a0\u2026", "num_citations": "928\n", "authors": ["2065"]}
{"title": "An introduction to least commitment planning\n", "abstract": " Recent developments have clarified the process of generating partially ordered, partially specified sequences of actions whose execution will achieve an agent's goal. This article summarizes a progression of least commitment planners, starting with one that handles the simple STRIPS representation and ending with UCPOP, a planner that manages actions with disjunctive precondition, conditional effects, and universal quantification over dynamic universes. Along the way, I explain how Chapman's formulation of the modal truth criterion is misleading and why his NP-completeness result for reasoning about plans with conditional effects does not apply to UCPOP.", "num_citations": "905\n", "authors": ["2065"]}
{"title": "Readings in qualitative reasoning about physical systems\n", "abstract": " Readings in Qualitative Reasoning about Physical Systems describes the automated reasoning about the physical world using qualitative representations. This text is divided into nine chapters, each focusing on some aspect of qualitative physics. The first chapter deal with qualitative physics, which is concerned with representing and reasoning about the physical world. The goal of qualitative physics is to capture both the commonsense knowledge of the person on the street and the tacit knowledge underlying the quantitative knowledge used by engineers and scientists. The succeeding chapter discusses the qualitative calculus and its role in constructing an envisionment that includes behavior over both mythical time and elapsed time. These topics are followed by reviews of the mathematical aspects of qualitative reasoning, history-based simulation and temporal reasoning, as well as the intelligence in scientific computing. The final chapters are devoted to automated modeling for qualitative reasoning and causal explanations of behavior. These chapters also examine the qualitative kinematics of reasoning about shape and space. This book will prove useful to psychologists and psychiatrists.", "num_citations": "853\n", "authors": ["2065"]}
{"title": "A softbot-based interface to the internet\n", "abstract": " The Internet Softbot (software robot) is a fullyimplemented AI agent developed at the University of Washington (Etzioni, Lesh, & Segal 1993). The softbot uses a UNIX shell and the World-Wide Web to interact with a wide range of internet resources. The softbot's e ectors include ftp, telnet, mail, and numerous le manipulation commands. Its sensors include internet facilities such as archie, gopher, netfind, and many more. The softbot is designed to incorporate new facilities into its repertoire as they become available. The softbot's\\added value\" is three-fold. First, it provides an integrated and expressive interface to the internet. Second, the softbot dynamically chooses which facilities to invoke, and in what sequence. For example, the softbot might use netfind to determine David McAllester's e-mail address. Since it knows that netfind requires a person's institution as input, the softbot would rst search bibliographic databases for a technical report by McAllester which would reveal his institution, and then feed that information to netfind. Third, the softbot uidly backtracks from one facility to another based on information collected at run time. As a result, the softbot's behavior changes in response to transient system conditions (eg, the UUCP gateway is down). In this article, we focus on the ideas underlying the softbot-based interface.", "num_citations": "824\n", "authors": ["2065"]}
{"title": "Open information extraction using wikipedia\n", "abstract": " Information-extraction (IE) systems seek to distill semantic relations from naturallanguage text, but most systems use supervised learning of relation-specific examples and are thus limited by the availability of training data. Open IE systems such as TextRunner, on the other hand, aim to handle the unbounded number of relations found on the Web. But how well can these open systems perform?This paper presents WOE, an open IE system which improves dramatically on TextRunner\u2019s precision and recall. The key to WOE\u2019s performance is a novel form of self-supervised learning for open extractors\u2014using heuristic matches between Wikipedia infobox attribute values and corresponding sentences to construct training data. Like TextRunner, WOE\u2019s extractor eschews lexicalized features and handles an unbounded set of semantic relations. WOE can operate in two modes: when restricted to POS tag features, it runs as quickly as TextRunner, but when set to use dependency-parse features its precision and recall rise even higher.", "num_citations": "818\n", "authors": ["2065"]}
{"title": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension\n", "abstract": " We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- http://nlp.cs.washington.edu/triviaqa/", "num_citations": "799\n", "authors": ["2065"]}
{"title": "Scaling question answering to the web\n", "abstract": " The wealth of information on the web makes it an attractive resource for seeking quick answers to simple, factual questions such as\" who was the first American in space?\" or\" what is the second tallest mountain in the world?\" Yet today's most advanced web search services (eg, Google and AskJeeves) make it surprisingly tedious to locate answers to such questions. In this paper, we extend question-answering techniques, first studied in the information retrieval literature, to the web and experimentally evaluate their performance. First we introduce \u0625\u0638\u0630 \u0636, which we believe to be the first general-purpose, fully-automated questionanswering system available on the web. Second, we describe \u0625\u0638\u0630 \u0636's architecture, which relies on multiple search-engine queries, natural-language parsing, and a novel voting procedure to yield reliable answers coupled with high recall. Finally, we compare \u0625\u0638\u0630 \u0636's performance to that of\u00a0\u2026", "num_citations": "774\n", "authors": ["2065"]}
{"title": "Spanbert: Improving pre-training by representing and predicting spans\n", "abstract": " We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERTlarge, our single model obtains 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6% F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE.", "num_citations": "726\n", "authors": ["2065"]}
{"title": "An adaptive query execution system for data integration\n", "abstract": " Query processing in data integration occurs over network-bound, autonomous data sources. This requires extensions to traditional optimization and execution techniques for three reasons: there is an absence of quality statistics about the data, data transfer rates are unpredictable and bursty, and slow or unavailable data sources can often be replaced by overlapping or mirrored sources. This paper presents the Tukwila data integration system, designed to support adaptivity at its core using a two-pronged approach. Interleaved planning and execution with partial optimization allows Tukwila to quickly recover from decisions based on inaccurate estimates. During execution, Tukwila uses adaptive query operators such as the double pipelined hash join, which produces answers quickly, and the dynamic collector, which robustly and efficiently computes unions across overlapping data sources. We demonstrate that\u00a0\u2026", "num_citations": "645\n", "authors": ["2065"]}
{"title": "Intelligent agents on the internet: Fact, fiction, and forecast\n", "abstract": " In the future, intelligent software agents will help us navigate the information superhighway by serving as backseat drivers or taxi drivers. Better yet, they have the potential to act as sophisticated concierges who make it unnecessary for us to approach the highway at all. We describe a selective sample of software agents currently under development, to give an idea of the agents that will emerge in the next few years. We organize our discussion by the agents' sophistication and the degree to which they make the information superhighway disappear. Following the information superhighway metaphor, an intelligent agent may be a backseat driver who makes suggestions at every rum, a taxi driver who takes you to your destination, or even a concierge whose knowledge and skills make it unnecessary for you to approach the superhighway at all. The Internet Softbot is discussed as a case study.< >", "num_citations": "604\n", "authors": ["2065"]}
{"title": "Autonomously semantifying wikipedia\n", "abstract": " Berners-Lee's compelling vision of a Semantic Web is hindered by a chicken-and-egg problem, which can be best solved by a bootstrapping method-creating enough structured data to motivate the development of applications. This paper argues that autonomously\" Semantifying Wikipedia\" is the best way to solve the problem. We choose Wikipedia as an initial data source, because it is comprehensive, not too large, high-quality, and contains enough manually-derived structure to bootstrap an autonomous, self-supervised process. We identify several types of structures which can be automatically enhanced in Wikipedia (eg, link structure, taxonomic data, infoboxes, etc.), and we describea prototype implementation of a self-supervised, machine learning system which realizes our vision. Preliminary experiments demonstrate the high precision of our system's extracted data-in one case equaling that of humans.", "num_citations": "546\n", "authors": ["2065"]}
{"title": "An algorithm for probabilistic planning\n", "abstract": " We define the probabilistic planning problem in terms of a probability distribution over initial world states, a boolean combination of propositions representing the goal, a probability threshold, and actions whose effects depend on the execution-time state of the world and on random chance. Adopting a probabilistic model complicates the definition of plan success: instead of demanding a plan that provably achieves the goal, we seek plans whose probability of success exceeds the threshold.In this paper, we present buridan, an implemented least-commitment planner that solves problems of this form. We prove that the algorithm is both sound and complete. We then explore buridan's efficiency by contrasting four algorithms for plan evaluation, using a combination of analytic methods and empirical experiments. We also describe the interplay between generating plans and evaluating them, and discuss the role of\u00a0\u2026", "num_citations": "528\n", "authors": ["2065"]}
{"title": "SUPPLE: automatically generating user interfaces\n", "abstract": " In order to give people ubiquitous access to software applications, device controllers, and Internet services, it will be necessary to automatically adapt user interfaces to the computational devices at hand (eg, cell phones, PDAs, touch panels, etc.). While previous researchers have proposed solutions to this problem, each has limitations. This paper proposes a novel solution based on treating interface adaptation as an optimization problem. When asked to render an interface on a specific device, our supple system searches for the rendition that meets the device's constraints and minimizes the estimated effort for the user's expected interface actions. We make several contributions: 1) precisely defining the interface rendition problem, 2) demonstrating how user traces can be used to customize interface rendering to particular user's usage pattern, 3) presenting an efficient interface rendering algorithm, 4) performing\u00a0\u2026", "num_citations": "516\n", "authors": ["2065"]}