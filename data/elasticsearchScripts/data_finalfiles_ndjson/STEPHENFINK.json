{"title": "Design, implementation and evaluation of adaptive recompilation with on-stack replacement\n", "abstract": " Modern virtual machines often maintain multiple compiled versions of a method. An on-stack replacement (OSR) mechanism enables a virtual machine to transfer execution between compiled versions, even while a method runs. Relying on this mechanism, the system can exploit powerful techniques to reduce compile time and code space, dynamically de-optimize code, and invalidate speculative optimizations. The paper presents a new, simple, mostly compiler-independent mechanism to transfer execution into compiled code. Additionally, we present enhancements to an analytic model for recompilation to exploit OSR for more aggressive optimization. We have implemented these techniques in Jikes RVM and present a comprehensive evaluation, including a study of fully automatic, online, profile-driven deferred compilation.", "num_citations": "185\n", "authors": ["280"]}
{"title": "Compiling a high-level language for GPUs: (via language support for architectures and compilers)\n", "abstract": " Languages such as OpenCL and CUDA offer a standard interface for general-purpose programming of GPUs. However, with these languages, programmers must explicitly manage numerous low-level details involving communication and synchronization. This burden makes programming GPUs difficult and error-prone, rendering these powerful devices inaccessible to most programmers. We desire a higher-level programming model that makes GPUs more accessible while also effectively exploiting their computational power. This paper presents features of Lime, a new Java-compatible language targeting heterogeneous systems, that allow an optimizing compiler to generate high quality GPU code. The key insight is that the language type system enforces isolation and immutability invariants that allow the compiler to optimize for a GPU without heroic compiler analysis. Our compiler attains GPU speedups\u00a0\u2026", "num_citations": "139\n", "authors": ["280"]}
{"title": "Efficient implementation of Java interfaces: Invokeinterface considered harmless\n", "abstract": " Single superclass inheritance enables simple and efficient table-driven virtual method dispathc. However, virtual method table dispatch does not handle multiple inheritance and interfaces. This complication has led to a widespread misimpression that interface method dispatch is inherently inefficient. This paper argues that with proper implementation techniques, Java interfaces need not be a source of significant performance degradation.", "num_citations": "127\n", "authors": ["280"]}
{"title": "Space-and time-efficient implementation of the Java object model\n", "abstract": " While many object-oriented languages impose space overhead of only one word per object to support features like virtual method dispatch, Java\u2019s richer functionality has led to implementations that require two or three header words per object. This space overhead increases memory usage and attendant garbage collection costs, reduces cache locality, and constrains programmers who might naturally solve a problem by using large numbers of small objects.               In this paper, we show that with careful engineering, a high-performance virtual machine can instantiate most Java objects with only a single-word object header. The single header word provides fast access to the virtual method table, allowing for quick method invocation. The implementation represents other per-object data (lock state, hash code, and garbage collection flags) using heuristic compression techniques. The heuristic retains two\u00a0\u2026", "num_citations": "121\n", "authors": ["280"]}
{"title": "Efficient run-time support for irregular block-structured applications\n", "abstract": " Parallel implementations of scientific applications often rely on elaborate dynamic data structures with complicated communication patterns. We describe a set of intuitive geometric programming abstractions that simplify coordination of irregular block-structured scientific calculations without sacrificing performance. We have implemented these abstractions in KeLP, a C++ run-time library. KeLP's abstractions enable the programmer to express complicated communication patterns for dynamic applications and to tune communication activity with a high-level, abstract interface. We show that KeLP's flexible communication model effectively manages elaborate data motion patterns arising in structured adaptive mesh refinement and achieves performance comparable to hand-coded message-passing on several structured numerical kernels.", "num_citations": "103\n", "authors": ["280"]}
{"title": "A compiler and runtime for heterogeneous computing\n", "abstract": " Heterogeneous systems show a lot of promise for extracting highperformance by combining the benefits of conventional architectures with specialized accelerators in the form of graphics processors (GPUs) and reconfigurable hardware (FPGAs). Extracting this performance often entails programming in disparate languages and models, making it hard for a programmer to work equally well on all aspects of an application. Further, relatively little attention is paid to co-execution - the problem of orchestrating program execution using multiple distinct computational elements that work seamlessly together. We present Liquid Metal, a comprehensive compiler and runtime system for a new programming language called Lime. Our work enables the use of a single language for programming heterogeneous computing platforms, and the seamless co-execution of the resultant programs on CPUs and accelerators that include\u00a0\u2026", "num_citations": "75\n", "authors": ["280"]}
{"title": "Flexible communication mechanisms for dynamic structured applications\n", "abstract": " Irregular scientific applications are often difficult to parallelize due to elaborate dynamic data structures with complicated communication patterns. We describe flexible data orchestration abstractions that enable the programmer to express customized communication patterns arising in an important class of irregular computations\u2014adaptive finite difference methods for partial differential equations. These abstractions are supported by KeLP, a c++ run-time library. KeLP enables the programmer to manage spatial data dependence patterns and express data motion handlers as first-class mutable objects. Using two finite difference applications, we show that KeLP's flexible communication model effectively manages elaborate data motion arising in semi-structured adaptive methods.", "num_citations": "67\n", "authors": ["280"]}
{"title": "Communication overlap in multi-tier parallel algorithms.\n", "abstract": " Hierarchically organized multicomputers such as SMP clusters o er new opportunities and new challenges for high-performance computation, but realizing their full potential remains a formidable task. We present a hierarchical model of communication targeted to block-structured, bulk-synchronous applications running on dedicated clusters of symmetric multiprocessors. Our model supports node-level rather processor-level communication as the fundamental operation, and is optimized for aggregate patterns of regular section moves rather than point-topoint messages. These two capabilities work synergistically. They provide exibility in overlapping communication and overcome de ciencies in the underlying communication layer on systems where internode communication bandwidth is at a premium. We have implemented our communication model in the KeLP2. 0 run time library. We present empirical results for ve applications running on a cluster of Digital AlphaServer 2100's. Four of the applications were able to overlap communication on a system which does not support overlap via non-blocking message passing using MPI. Overall performance improvements due to our overlap strategy ranged from 12% to 28%.", "num_citations": "56\n", "authors": ["280"]}
{"title": "Translating imperative code to MapReduce\n", "abstract": " We present an approach for automatic translation of sequential, imperative code into a parallel MapReduce framework. Automating such a translation is challenging: imperative updates must be translated into a functional MapReduce form in a manner that both preserves semantics and enables parallelism. Our approach works by first translating the input code into a functional representation, with loops succinctly represented by fold operations. Then, guided by rewrite rules, our system searches a space of equivalent programs for an effective MapReduce implementation. The rules include a novel technique for handling irregular loop-carried dependencies using group-by operations to enable greater parallelism. We have implemented our technique in a tool called Mold. It translates sequential Java code into code targeting the Apache Spark runtime. We evaluated Mold on several real-world kernels and found that\u00a0\u2026", "num_citations": "46\n", "authors": ["280"]}
{"title": "A programming methodology for dual-tier multicomputers\n", "abstract": " Hierarchically organized ensembles of shared memory multiprocessors possess a richer and more complex model of locality than previous generation multicomputers with single processor nodes. These dual-tier computers introduce many new factors into the programmer's performance model. We present a methodology for implementing block-structured numerical applications on dual-tier computers and a run-time infrastructure, called KeLP2, that implements the methodology. KeLP2 supports two levels of locality and parallelism via hierarchical SPMD control flow, run-time geometric meta-data, and asynchronous collective communication. KeLP applications can effectively overlap communication with computation under conditions where nonblocking point-to-point message passing fails to do so. KeLP's abstractions hide considerable detail without sacrificing performance and dual-tier applications written in KeLP\u00a0\u2026", "num_citations": "46\n", "authors": ["280"]}
{"title": "Java server benchmarks\n", "abstract": " The Java\u2122 platform has the potential to revolutionize computing, with its promise of \u201cwrite once, run anywhere\u201d\u2122 development. However, in order to realize this potential, Java applications must demonstrate satisfactory performance. Rapid progress has been made in addressing Java performance, although most of the initial efforts have targeted Java client applications. To make a significant impact in network computing, server applications written in the Java language, or those using Java extensions, frameworks, or components, must exhibit a competitive level of performance. One obstacle to obtaining this goal has been the lack of well-defined, server-specific, Java benchmarks. This paper helps address this shortcoming by defining representative Java server benchmarks. These benchmarks represent server application areas, including Web-based dynamic content delivery (servlets), business object frameworks\u00a0\u2026", "num_citations": "44\n", "authors": ["280"]}
{"title": "The complexity of Andersen\u2019s analysis in practice\n", "abstract": " While the tightest proven worst-case complexity for Andersen\u2019s points-to analysis is nearly cubic, the analysis seems to scale better on real-world codes. We examine algorithmic factors that help account for this gap. In particular, we show that a simple algorithm can compute Andersen\u2019s analysis in worst-case quadratic time as long as the input program is k-sparse, i.e. it has at most k statements dereferencing each variable and a sparse flow graph. We then argue that for strongly-typed languages like Java, typical structure makes programs likely to be k-sparse, and we give empirical measurements across a suite of Java programs that confirm this hypothesis. We also discuss how various standard implementation techniques yield further constant-factor speedups.", "num_citations": "40\n", "authors": ["280"]}
{"title": "Runtime support for multi-tier programming of block-structured applications on SMP clusters\n", "abstract": " We present a small set of programming abstractions to simplify efficient implementations for block-structured scientific calculations on SMP clusters. We have implemented these abstractions in KeLP 2.0, a C++ class library. KeLP 2.0 provides hierarchical SMPD control flow to manage two levels of parallelism and locality. Additionally, to tolerate slow inter-node communication costs, KeLP 2.0 combines inspector/executor communication analysis with overlap of communication and computation. We illustrate how these programming abstractions hide the low-level details of thread management, scheduling, synchronization, and message-passing, but allow the programmer to express efficient algorithms with intuitive geometric primitives.", "num_citations": "36\n", "authors": ["280"]}
{"title": "Method and apparatus to provide concurrency control over objects without atomic operations on non-shared objects\n", "abstract": " A lock is implemented by assigning the lock to a thread that creates the associated object. The \u201cowning thread\u201d can thereafter acquire and release the lock without any atomic operations. If another thread attempts to acquire the lock, the non-owning thread sends a message to the owning thread, requesting permission to acquire the lock. At some point, the owning thread receives the message from the non-owning thread and changes the state of the lock such that future lock/unlock operations use atomic operations that support object sharing.", "num_citations": "32\n", "authors": ["280"]}
{"title": "Programming with LPARX\n", "abstract": " LPARX is a software development tool for implementing dynamic irregular scienti c applications on high performance MIMD parallel computers. LPARX, implemented as a C++ class library, supports coarse grain data parallelism arising in particle methods and adaptive nite di erence methods. It provides structural abstraction, which enables data decompositions to exist as rst-class objects. LPARX is currently running on diverse MIMD including the Intel Paragon, the Cray C-90, and networks of workstations running under PVM, and software may be developed on a single processor workstation.", "num_citations": "26\n", "authors": ["280"]}
{"title": "A programming model for block-structured scientific calculations on SMP clusters\n", "abstract": " Multi-tier parallel computers such as clusters of symmetric multiprocessors (SMPs) offer both new opportunities and new challenges for high-performance computation. Although these computer platforms can potentially deliver unprecedented performance for computationally intensive scientific calculations, realizing the hardware's potential remains a formidable task. To achieve high performance, the programmer must coordinate several levels of parallelism and locality to match the hardware's capabilities. Current programming languages and software tools do not directly facilitate this task, and the resultant difficulties hinder efficient implementations of scientific calculations on SMP clusters.", "num_citations": "24\n", "authors": ["280"]}
{"title": "System and method for dynamically optimizing executing activations\n", "abstract": " A system and method for dynamically optimizing a code sequence of a program while executing in a computer system comprises: identifying one or more program yield points in an original code sequence at which a run-time representation of the original code sequence may be optimized in an executing program; generating a prologue of instructions for setting up program state associated with the original code sequence at a particular yield point; adding the prologue of instructions to an intermediate representation of the original code sequence code for generating a specialized code sequence; and, compiling the specialized code sequence with a compiler for generating a run-time representation of the specialized code sequence, the run-time representation being further optimized for execution on a target computer system.", "num_citations": "20\n", "authors": ["280"]}
{"title": "Multiple data parallelism with HPF and KeLP\n", "abstract": " High Performance Fortran (HPF) is an effective language for implementing regular data parallel applications on distributed memory architectures, but it is not well suited to irregular, block-structured applications such as multiblock and adaptive mesh methods.A solution to this problem is to use an SPMD program to coordinate multiple concurrent HPF tasks, each operating on a regular subgrid of the multiblock domain. This paper presents such a system, in which the coordination layer is provided by the C++ class library KeLP. We describe the KeLP\u2013HPF implementation and programming model, and show an example KeLP\u2013HPF multiblock solver together with performance results.", "num_citations": "20\n", "authors": ["280"]}
{"title": "The Data Mover: A machine-independent abstraction for managing customized data motion\n", "abstract": " This paper discusses the Data Mover, an abstraction for expressing machine-independent customized communication algorithms arising in block-structured computations. The Data Mover achieves performance that is competitive with hand-coding in MPI, but enables application-specific optimization to be expressed using intuitive geometric set operations that encapsulate low-level details.", "num_citations": "17\n", "authors": ["280"]}
{"title": "Multiple data parallelism with HPF and KeLP\n", "abstract": " High Performance Fortran (HPF) is an effective language for implementing regular data parallel applications on distributed memory architectures, but it is not well suited to irregular, block-structured applications such as multiblock and adaptive mesh methods. A solution to this problem is to use a non-HPF SPMD program to coordinate multiple concurrent HPF tasks, each operating on a regular subgrid of an irregular data domain. To this end we have developed an interface between the C++ class library KeLP, which supports irregular, dynamic block-structured applications on distributed systems, and an HPF compiler, SHPF. This allows KeLP to handle the data layout and inter-block communications, and to invoke HPF concurrently on each block. There are a number of advantages to this approach: it combines the strengths of both KeLP and HPF; it is relatively easy to implement; and it involves no\u00a0\u2026", "num_citations": "16\n", "authors": ["280"]}
{"title": "Run-time data distribution for block-structured applications on distributed memory computers\n", "abstract": " In many scienti c applications running on parallel computers, e cient data decompositions exhibit a block structure which can be regular or irregular. We present a run-time strategy for both regular and irregular block-structured applications which provides a three-stage mapping including alignment, rst-class decomposition objects, and block data movement. We have implemented this strategy using a small set of primitive operations, and present these along with performance results. In contrast to data parallel Fortran dialects, our system executes completely at run-time and supports coarse-grained user-de ned data decompositions.", "num_citations": "12\n", "authors": ["280"]}
{"title": "Predicting application performance on hardware accelerators\n", "abstract": " Predicting program performance on hardware devices, in one aspect, may comprise obtaining a set of existing applications and observed performance on a target hardware device. The set of existing applications are run on one or more general purpose computer processors and application features are extracted from the existing application. A machine learning technique is employed to train a predictive model based on the extracted application features and the observed performance for predicting application performance on the target hardware device.", "num_citations": "11\n", "authors": ["280"]}
{"title": "Automatic conversion of sequential array-based programs to parallel map-reduce programs\n", "abstract": " The present disclosure relates generally to the field of automatic conversion of sequential array-based programs to parallel MapReduce programs. In various examples, automatic conversion of sequential array-based programs to parallel MapReduce programs may be implemented in the form of systems, methods and/or algorithms.", "num_citations": "10\n", "authors": ["280"]}
{"title": "Detecting program phases with periodic call-stack sampling during garbage collection\n", "abstract": " A computer readable storage medium for associating a phase with an activation of a computer program that supports garbage collection include: a plurality of stacks, each stack including at least one stack frame that includes an activation count; and a processor with logic for performing steps of: zeroing the activation count whenever the program creates a new stack frame and after garbage collection is performed; determining whether an interval has transpired during program execution; examining each stack frame's content and incrementing the activation count for each frame of the stacks once the interval has transpired; detecting the phase whose activation count is non-zero and associating the phase with the activation; and ensuring that when the phase ends, an action is immediately performed.", "num_citations": "9\n", "authors": ["280"]}
{"title": "Visualizing serverless cloud application logs for program understanding\n", "abstract": " A cloud platform records a wealth of information regarding program execution. Most cloud service providers offer dashboard monitoring tools that visualize resource usage and billing information, and support debugging. In this paper, we present a tool that visualizes cloud execution logs for a different goal - to facilitate program understanding and generate documentations for an application using runtime data. Our tool introduces a new timeline visualization, a new method and user interface to summarize multiple JSON objects and present the result, and interaction techniques that facilitate navigating among functions. Together, these features explain a serverless cloud application's composition, performance, dataflow and data schema. We report some initial user feedback from several expert developers that were involved in the tool's design and development process.", "num_citations": "7\n", "authors": ["280"]}
{"title": "Non-uniform Partitioning of Finite Difference Methods Running on SMP Clusters\n", "abstract": " A multicomputer or workstation cluster with multiprocessor nodes introduces significant need and opportunity for overlapping communication with computation. We evaluate partitioning strategies for an important application class, finite difference methods, running on clusters of symmetric multiprocessors. Our results show that even for a regular, uniform finite difference method, a non-uniform partitioning can give the best performance by judiciously overlapping communication and computation. We present an analytic performance model and experimental results to evaluate partitioning strategies, and discuss implications for SMP cluster programming models. 1 Introduction Clusters of symmetric multiprocessors (SMPs) offer promising opportunities for high-performance scientific computation [1]. An SMP cluster, with several levels of locality and parallelism, presents a more complex non-uniform memory hierarchy than a cluster with uniprocessor nodes. Within an SMP node, multiple processors sh...", "num_citations": "7\n", "authors": ["280"]}
{"title": "Programming language requirements for the next millennium\n", "abstract": " The needs of the high-performance programming community will be met by combining the best features of programming languages and layered libraries. In particular, we believe that the community can bene t from a small, stable programming language with abstraction features that support the development of self-tuning, optimizing, easily adaptable, integrable layered systems.", "num_citations": "7\n", "authors": ["280"]}
{"title": "Serverless composition of functions into applications\n", "abstract": " A processor may receive a query from a user. The query may include one or more portions. The processor may identify a primary function. The processor may determine to segment the primary function into two or more subsidiary functions. The processor may process a first portion of the query with a first subsidiary function. The processor may display a processed outcome of the query to the user.", "num_citations": "5\n", "authors": ["280"]}
{"title": "Extracting stream graph structure in a computer language by pre-executing a deterministic subset\n", "abstract": " Compile-time recognition of graph structure where graph has arbitrary connectivity and is constructed using recursive computations is provided. In one aspect, the graph structure recognized at compile time may be duplicated at runtime and can then operate on runtime values not known at compile time.", "num_citations": "5\n", "authors": ["280"]}
{"title": "An introduction to programming with X10\n", "abstract": " The X10 programming language was introduced to try to bring to programming in the high-performance world the latest in programming language technology. This text is an attempt to start programmers who have some experience with an object-oriented language on the road to writing X10. We\u2019ll talk about Java and C++ from time to time, since they seem to be the most broadly taught for the likely audience for this book, but we\u2019ll do so only to help translate from these languages into X10, not to explain X10 itself. So, even familiarity with \u201cscripting\u201d languages like JavaScript, Python, Ruby, or Smalltalk will give you the background we need, although you may find that the X10 syntax takes some getting used to if come from as far away as Python or Smalltalk. Those familiar with C, but not C++, may find this book useful, but some minimal knowledge of object-oriented programming really is a must\u2014eg classes, objects, class methods versus instance methods, and so on, but nothing deep.", "num_citations": "5\n", "authors": ["280"]}
{"title": "Parallel cluster identification for multidimensional lattices\n", "abstract": " The cluster identification problem is a variant of connected component labeling that arises in cluster algorithms for spin models in statistical physics. We present a multidimensional version of K.P. Belkhale and P. Banerjee's quad algorithm (1992) for connected component labeling on distributed memory parallel computers. Our extension abstracts away extraneous spatial connectivity information in more than two dimensions, simplifying implementation for higher dimensionality. We identify two types of locality present in cluster configurations, and present optimizations to exploit locality for better performance. Performance results from 2D, 3D, and 4D Ising model simulations with Swendson-Wang dynamics show that the optimizations improve performance by 20-80 percent.", "num_citations": "5\n", "authors": ["280"]}
{"title": "Facilitating debugging serverless applications via graph rewriting\n", "abstract": " Techniques that facilitate re-hosting a subset of a serverless application are provided. In one example, a system includes an interface component, a rewriter component and a broker component. The interface component receives identifier data from a computing device that identifies a portion of a serverless application to be re-hosted by the computing device. The computing device is in communication with the serverless computing system via a network device. The rewriter component rewrites the serverless application to allow the first portion of the serverless application to be executed by the computing device and another portion of the serverless application to be executed by the serverless computing system. The interface component re-routes the first portion of the serverless application to the computing device to facilitate a debugging session for the first portion of the serverless application that is performed by the\u00a0\u2026", "num_citations": "4\n", "authors": ["280"]}
{"title": "Managing external feeds in an event-based computing system\n", "abstract": " At a cloud platform, a class of feed is received for an external feed corresponding to an information source, as are an instruction corresponding to a create operation for the external feed, and a dictionary input corresponding to parameters expected by the information source. The external feed produces a corresponding class of events. At the cloud platform, a handler is selected based on the received class of feed and the received create operation; the input dictionary is transferred to the handler; and the handler generates a unique destination to receive events for the class of events. The handler on the cloud platform generates a unique request to the information source to generate events of the class of feed to the unique destination and sends the request to the information source. Events generated from the information source responsive to the unique request are received at the unique destination.", "num_citations": "4\n", "authors": ["280"]}
{"title": "Method for detecting program phases with periodic call-stack sampling\n", "abstract": " A system and method for detecting phases in a running computer program, creates an activation count associated with each stack frame. The activation count is zeroed whenever a new frame is created in a stack and incremented for each frame encountered during periodic intervals. A phase is detected with an activation whose activation count is non-zero.", "num_citations": "4\n", "authors": ["280"]}
{"title": "An evaluation of java system services with microbenchmarks\n", "abstract": " The Java programming language and standard libraries provide a portable interface to traditional operating system services. We present a set of microbenchmarks to help evaluate the performance of these services. Building on previous work on operating system microbenchmarks, we present an\\apples-to-apples\" comparison of Java and C performance of memory bandwidth, le system, thread, and network performance tests. We present experimental results for several Java runtime systems on Windows NT, which show both strengths and weaknesses of current Java implementations. 1", "num_citations": "4\n", "authors": ["280"]}
{"title": "Protocol for communication of data structures\n", "abstract": " A system and method are provided for communicating information in a data structure between applications. According to the method, a description of a data structure is sent from a first application to a second application, and there is received from the second application an identification of at least one portion of the data structure that is requested by the second application. The first application marshals a subset of the data structure consisting of the at least one portion that was identified, and there is sent from the first application to the second application the marshalled subset of the data structure.", "num_citations": "3\n", "authors": ["280"]}
{"title": "Thin slicing\n", "abstract": " personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission.", "num_citations": "3\n", "authors": ["280"]}
{"title": "Optimization of communication of data structures using program analysis\n", "abstract": " A method for regulating communication of information in a data structure between components of a computer program is disclosed. The method includes reading a computer program and automatically identifying fields of a data structure that must be transmitted from a first component of the computer program to a second component of the computer program. The method further includes generating a routine that indicates the fields of the data structure that were identified. The method further includes requiring the first component to execute the routine when sending the data structure to the second component, such that the first component only marshals the fields of the data structure that are indicated by the routine.", "num_citations": "3\n", "authors": ["280"]}
{"title": "Optimizing transactions based on a set of persistent objects\n", "abstract": " A method, apparatus, system, and signal-bearing medium that, in an embodiment, identify a component that initiates a transaction in an application, calculate a set of persistent objects potentially enlisted by the transaction, and calculate a set of potential operations that the transaction may perform with respect to each of the potentially enlisted persistent objects. The transaction is then optimized based on a configuration, where the configuration is based on the set of potential operations that the transaction may perform with respect to each of the potentially enlisted persistent objects. If a single invocation of the component initiates more than one transaction, actions taken by the set of persistent objects are collapsed into a collapsed set, and policies relevant to the collapsed set are associated with a unique task name.", "num_citations": "3\n", "authors": ["280"]}
{"title": "Cluster identification on a distributed memory multiprocessor\n", "abstract": " The cluster identification step is often the bottleneck in multiprocessor simulations of spin models for statistical mechanics. We have applied a connected component labeling algorithm originally developed for VLSI circuit extraction to the cluster identification problem. The algorithm is extended to more than two dimensions, abstracting away unnecessary spatial information to simplify implementation in higher dimensions. We identify two types of spatial locality in cluster configurations, and present optimizations to exploit each type of locality. Performance results are presented from two and three-dimensional Ising model simulations.< >", "num_citations": "3\n", "authors": ["280"]}
{"title": "Language-independent program composition using containers\n", "abstract": " An action sequence including a plurality of actions and a corresponding input dictionary are obtained. A first container, running an image for a first one of the actions implemented in a first programming language, is instantiated, and the image is executed on the input dictionary to obtain a first action result; the input dictionary is then updated with the first action result to obtain an updated input dictionary. A second container, running an image for a second one of the actions implemented in a second programming language, different than the first programming language, is instantiated, and the image is executed on the updated input dictionary to obtain a second action result; the updated input dictionary is further updated with the second action result to obtain a further updated input dictionary. The input dictionaries are independent of the programming languages.", "num_citations": "2\n", "authors": ["280"]}
{"title": "Space-efficient object models for object-oriented programming languages\n", "abstract": " A method for implementing an object model for an object-oriented programming language. Also contemplated is a method whereby some object state is materialized directly in those objects deemed likely to use such a state, but is externalized for those objects deemed unlikely to use the state.", "num_citations": "2\n", "authors": ["280"]}
{"title": "The liquid metal ip bridge\n", "abstract": " Programmers are increasingly turning to heterogeneous systems to achieve performance. Examples include FPGA-based systems that integrate reconfigurable architectures with conventional processors. However, the burden of managing the coding complexity that is intrinsic to these systems falls entirely on the programmer. This limits the proliferation of these systems as only highly-skilled programmers and FPGA developers can unlock their potential. The goal of the Liquid Metal project at IBM Research is to address the programming complexity attributed to heterogeneous FPGA-based systems. A feature of this work is a vertically integrated development lifecycle that appeals to skilled software developers. A primary enabler for this work is a canonical IP bridge, designed to offer a uniform communication methodology between software and hardware, and that is applicable across a wide range of platforms\u00a0\u2026", "num_citations": "1\n", "authors": ["280"]}
{"title": "A Preliminary Evaluation of HPF.\n", "abstract": " HPF is a data parallel Fortran dialect currently implemented on diverse hardware platforms ranging from workstations to massively parallel processors. To date, performance data are sparse. We will present preliminary measurements of selected benchmarks, comparing HPF applications against equivalent SPMD implementations and the same HPF implementation running with di erent compilers on di erent hardware. We will discuss software issues in light of how they a ect performance.", "num_citations": "1\n", "authors": ["280"]}